@article{JMLR:v10:abeel09a,
 abstract = {Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classifiers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license.},
 author = {Thomas Abeel and Yves Van de Peer and Yvan Saeys},
 code = {http://java-ml.sourceforge.net},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W1760882240},
 pages = {931--934},
 pdf = {/papers/volume10/abeel09a/abeel09a.pdf},
 title = {Java-ML: A Machine Learning Library},
 url = {http://jmlr.org/papers/v10/abeel09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:king09a,
 abstract = {There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.},
 author = {Davis E. King},
 code = {https://sourceforge.net/projects/dclib/},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2115252128},
 pages = {1755--1758},
 pdf = {/papers/volume10/king09a/king09a.pdf},
 title = {Dlib-ml: A Machine Learning Toolkit},
 url = {http://jmlr.org/papers/v10/king09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:lehmann09a,
 abstract = {In this paper, we introduce DL-Learner, a framework for learning in description logics and OWL. OWL is the official W3C standard ontology language for the Semantic Web. Concepts in this language can be learned for constructing and maintaining OWL ontologies or for solving problems similar to those in Inductive Logic Programming. DL-Learner includes several learning algorithms, support for different OWL formats, reasoner interfaces, and learning problems. It is a cross-platform framework implemented in Java. The framework allows easy programmatic access and provides a command line interface, a graphical interface as well as a WSDL-based web service.},
 author = {Jens Lehmann},
 code = {https://dl-learner.org},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2160605923},
 pages = {2639--2642},
 pdf = {/papers/volume10/lehmann09a/lehmann09a.pdf},
 title = {DL-Learner: Learning Concepts in Description Logics},
 url = {http://jmlr.org/papers/v10/lehmann09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:maes09a,
 abstract = {In this paper we introduce NIEME, a machine learning library for large-scale classification, regression and ranking. NIEME, relies on the framework of energy-based models (LeCun et al., 2006) which unifies several learning algorithms ranging from simple perceptrons to recent models such as the pegasos support vector machine or l1-regularized maximum entropy models. This framework also unifies batch and stochastic learning which are both seen as energy minimization problems. NIEME, can hence be used in a wide range of situations, but is particularly interesting for large-scale learning tasks where both the examples and the features are processed incrementally. Being able to deal with new incoming features at any time within the learning process is another original feature of the NIEME, toolbox. NIEME, is released under the GPL license. It is efficiently implemented in C++, it works on Linux, Mac OS X and Windows and provides interfaces for C++, Java and Python.},
 author = {Francis Maes},
 code = {nieme-source-2009-03-06.tgz},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W114702462},
 pages = {743--746},
 pdf = {/papers/volume10/maes09a/maes09a.pdf},
 title = {Nieme: Large-Scale Energy-Based Models},
 url = {http://jmlr.org/papers/v10/maes09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:raeder09a,
 author = {Troy Raeder and Nitesh V. Chawla},
 code = {https://mloss.org/software/view/137/},
 journal = {Journal of Machine Learning Research},
 number = {47},
 pages = {1387--1390},
 pdf = {/papers/volume10/raeder09a/raeder09a.pdf},
 title = {Model Monitor (M2): Evaluating, Comparing, and Monitoring Models},
 url = {http://jmlr.org/papers/v10/raeder09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:shah09a,
 abstract = {In this paper, we introduce pebl, a Python library and application for learning Bayesian network structure from data and prior knowledge that provides features unmatched by alternative software packages: the ability to use interventional data, flexible specification of structural priors, modeling with hidden variables and exploitation of parallel processing.},
 author = {Abhik Shah and Peter Woolf},
 code = {https://github.com/abhik/pebl},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2109182932},
 pages = {159--162},
 pdf = {/papers/volume10/shah09a/shah09a.pdf},
 title = {Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data.},
 url = {http://jmlr.org/papers/v10/shah09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:tanner09a,
 abstract = {RL-Glue is a standard, language-independent software package for reinforcement-learning experiments. The standardization provided by RL-Glue facilitates code sharing and collaboration. Code sharing reduces the need to re-engineer tasks and experimental apparatus, both common barriers to comparatively evaluating new ideas in the context of the literature. Our software features a minimalist interface and works with several languages and computing platforms. RL-Glue compatibility can be extended to any programming language that supports network socket communication. RL-Glue has been used to teach classes, to run international competitions, and is currently used by several other open-source software and hardware projects.},
 author = {Brian Tanner and Adam White},
 code = {https://glue.rl-community.org},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W2103048296},
 pages = {2133--2136},
 pdf = {/papers/volume10/tanner09a/tanner09a.pdf},
 title = {RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments},
 url = {http://jmlr.org/papers/v10/tanner09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v11:bifet10a,
 abstract = {Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA includes a collection of offline and ...},
 author = {Albert Bifet and Geoff Holmes and Richard Kirkby and Bernhard Pfahringer},
 code = {http://moa.cs.waikato.ac.nz/},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W3003253354},
 pages = {1601--1604},
 pdf = {/papers/volume11/bifet10a/bifet10a.pdf},
 title = {MOA: Massive Online Analysis},
 url = {http://jmlr.org/papers/v11/bifet10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v11:escalera10a,
 author = {Sergio Escalera and Oriol Pujol and Petia Radeva},
 code = {Code.zip},
 journal = {Journal of Machine Learning Research},
 number = {20},
 pages = {661--664},
 pdf = {/papers/volume11/escalera10a/escalera10a.pdf},
 title = {Error-Correcting Output Codes Library},
 url = {http://jmlr.org/papers/v11/escalera10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v11:gorissen10a,
 abstract = {An exceedingly large number of scientific and engineering fields are confronted with the need for computer simulations to study complex, real world phenomena or solve challenging design problems. H...},
 author = {Dirk Gorissen and Ivo Couckuyt and Piet Demeester and Tom Dhaene and Karel Crombecq},
 code = {http://www.sumo.intec.ugent.be/},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W3004784263},
 pages = {2051--2055},
 pdf = {/papers/volume11/gorissen10a/gorissen10a.pdf},
 title = {A Surrogate Modeling and Adaptive Sampling Toolbox for Computer Based Design},
 url = {http://jmlr.org/papers/v11/gorissen10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v11:hothorn10a,
 abstract = {We describe version 2.0 of the R add-on package mboost. The package implements boosting for optimizing general risk functions using component-wise (penalized) least squares estimates or regression trees as base-learners for fitting generalized linear, additive and interaction models to potentially high-dimensional data.},
 author = {Torsten Hothorn and Peter B{{\"u}}hlmann and Thomas Kneib and Matthias Schmid and Benjamin Hofner},
 code = {https://cran.r-project.org/web/packages/mboost/},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2104235537},
 pages = {2109--2113},
 pdf = {/papers/volume11/hothorn10a/hothorn10a.pdf},
 title = {Model-based Boosting 2.0},
 url = {http://jmlr.org/papers/v11/hothorn10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v11:jaimovich10a,
 abstract = {The FastInf C++ library is designed to perform memory and time efficient approximate inference in large-scale discrete undirected graphical models. The focus of the library is propagation based approximate inference methods, ranging from the basic loopy belief propagation algorithm to propagation based on convex free energies. Various message scheduling schemes that improve on the standard synchronous or asynchronous approaches are included. Also implemented are a clique tree based exact inference, Gibbs sampling, and the mean field algorithm. In addition to inference, FastInf provides parameter estimation capabilities as well as representation and learning of shared parameters. It offers a rich interface that facilitates extension of the basic classes to other inference and learning methods.},
 author = {Ariel Jaimovich and Ofer Meshi and Ian McGraw and Gal Elidan},
 code = {http://compbio.cs.huji.ac.il/FastInf/},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W2117776955},
 pages = {1733--1736},
 pdf = {/papers/volume11/jaimovich10a/jaimovich10a.pdf},
 title = {FastInf: An Efficient Approximate Inference Library},
 url = {http://jmlr.org/papers/v11/jaimovich10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v11:krause10a,
 abstract = {In recent years, a fundamental problem structure has emerged as very useful in a variety of machine learning applications: Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Similarly to convexity, submodularity allows one to efficiently find provably (near-) optimal solutions for large problems. We present SFO, a toolbox for use in MATLAB or Octave that implements algorithms for minimization and maximization of submodular functions. A tutorial script illustrates the application of submodularity to machine learning and AI problems such as feature selection, clustering, inference and optimized information gathering.},
 author = {Andreas Krause},
 code = {https://las.inf.ethz.ch/sfo/index.html},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W2118936174},
 pages = {1141--1144},
 pdf = {/papers/volume11/krause10a/krause10a.pdf},
 title = {SFO: A Toolbox for Submodular Function Optimization},
 url = {http://jmlr.org/papers/v11/krause10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v11:mooij10a,
 abstract = {This paper describes the software package libDAI, a free & open source C++ library that provides implementations of various exact and approximate inference methods for graphical models with discrete-valued variables. libDAI supports directed graphical models (Bayesian networks) as well as undirected ones (Markov random fields and factor graphs). It offers various approximations of the partition sum, marginal probability distributions and maximum probability states. Parameter learning is also supported. A feature comparison with other open source software packages for approximate inference is given. libDAI is licensed under the GPL v2+ license and is available at http://www.libdai.org.},
 author = {Joris M. Mooij},
 code = {https://github.com/dbtsai/libDAI},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W2169447051},
 pages = {2169--2173},
 pdf = {/papers/volume11/mooij10a/mooij10a.pdf},
 title = {libDAI: A Free and Open Source C++ Library for Discrete Approximate Inference in Graphical Models},
 url = {http://jmlr.org/papers/v11/mooij10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v11:rasmussen10a,
 abstract = {The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.},
 author = {Carl Edward Rasmussen and Hannes Nickisch},
 code = {http://gaussianprocess.org/gpml/code/matlab/doc/},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2107386393},
 pages = {3011--3015},
 pdf = {/papers/volume11/rasmussen10a/rasmussen10a.pdf},
 title = {Gaussian Processes for Machine Learning (GPML) Toolbox},
 url = {http://jmlr.org/papers/v11/rasmussen10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v11:shelton10a,
 abstract = {We present a continuous time Bayesian network reasoning and learning engine (CTBN-RLE). A continuous time Bayesian network (CTBN) provides a compact (factored) description of a continuous-time Markov process. This software provides libraries and programs for most of the algorithms developed for CTBNs. For learning, CTBN-RLE implements structure and parameter learning for both complete and partial data. For inference, it implements exact inference and Gibbs and importance sampling approximate inference for any type of evidence pattern. Additionally, the library supplies visualization methods for graphically displaying CTBNs or trajectories of evidence.},
 author = {Christian R. Shelton and Yu Fan and William Lam and Joon Lee and Jing Xu},
 code = {https://github.com/eudoxia0/ctbnrle},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W1645398407},
 pages = {1137--1140},
 pdf = {/papers/volume11/shelton10a/shelton10a.pdf},
 title = {Continuous Time Bayesian Network Reasoning and Learning Engine},
 url = {http://jmlr.org/papers/v11/shelton10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v11:sonnenburg10a,
 abstract = {We have developed a machine learning toolbox, called SHOGUN, which is designed for unified large-scale learning for a broad range of feature types and learning settings. It offers a considerable number of machine learning models such as support vector machines, hidden Markov models, multiple kernel learning, linear discriminant analysis, and more. Most of the specific algorithms are able to deal with several different data classes. We have used this toolbox in several applications from computational biology, some of them coming with no less than 50 million training examples and others with 7 billion test examples. With more than a thousand installations worldwide, SHOGUN is already widely adopted in the machine learning community and beyond.

SHOGUN is implemented in C++ and interfaces to MATLABTM, R, Octave, Python, and has a stand-alone command line interface. The source code is freely available under the GNU General Public License, Version 3 at http://www.shogun-toolbox.org.},
 author = {S{{\"o}}ren Sonnenburg and Gunnar R&#228;tsch and Sebastian Henschel and Christian Widmer and Jonas Behr and Alexander Zien and Fabio de Bona and Alexander Binder and Christian Gehl and Vojt{{\ve}}ch Franc},
 code = {https://github.com/shogun-toolbox/shogun},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W1571024744},
 pages = {1799--1802},
 pdf = {/papers/volume11/sonnenburg10a/sonnenburg10a.pdf},
 title = {The SHOGUN Machine Learning Toolbox},
 url = {http://jmlr.org/papers/v11/sonnenburg10a.html},
 volume = {11},
 year = {2010}
}

@article{JMLR:v12:gashler11a,
 abstract = {We present a breadth-oriented collection of cross-platform command-line tools for researchers in machine learning called Waffles. The Waffles tools are designed to offer a broad spectrum of functionality in a manner that is friendly for scripted automation. All functionality is also available in a C++ class library. Waffles is available under the GNU Lesser General Public License.},
 author = {Michael Gashler},
 code = {https://github.com/mikegashler/waffles},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W2171934181},
 pages = {2383--2387},
 pdf = {/papers/volume12/gashler11a/gashler11a.pdf},
 title = {Waffles : A Machine Learning Toolkit},
 url = {http://jmlr.org/papers/v12/gashler11a.html},
 volume = {12},
 year = {2011}
}

@article{JMLR:v12:hahsler11a,
 abstract = {This paper describes the ecosystem of R add-on packages developed around the infrastructure provided by the package arules. The packages provide comprehensive functionality for analyzing interestin...},
 author = {Michael Hahsler and Sudheer Chelluboina and Kurt Hornik and Christian Buchta},
 code = {https://cran.r-project.org/web/packages/arules/},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W3015150373},
 pages = {2021--2025},
 pdf = {/papers/volume12/hahsler11a/hahsler11a.pdf},
 title = {The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets},
 url = {http://jmlr.org/papers/v12/hahsler11a.html},
 volume = {12},
 year = {2011}
}

@article{JMLR:v12:lauer11a,
 abstract = {This paper describes MSVMpack, an open source software package dedicated to our generic model of multi-class support vector machine. All four multi-class support vector machines (M-SVMs) proposed so far in the literature appear as instances of this model. MSVMpack provides for them the first unified implementation and offers a convenient basis to develop other instances. This is also the first parallel implementation for M-SVMs. The package consists in a set of command-line tools with a callable library. The documentation includes a tutorial, a user's guide and a developer's guide.},
 author = {Fabien Lauer and Yann Guermeur},
 code = {https://members.loria.fr/FLauer/files/MSVMpack/MSVMpack.html},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2151922881},
 pages = {2293--2296},
 pdf = {/papers/volume12/lauer11a/lauer11a.pdf},
 title = {MSVMpack: a Multi-Class Support Vector Machine Package},
 url = {http://jmlr.org/papers/v12/lauer11a.html},
 volume = {12},
 year = {2011}
}

@article{JMLR:v12:lichtenwalter11a,
 abstract = {LPmade is a complete cross-platform software solution for multi-core link prediction and related tasks and analysis. Its first principal contribution is a scalable network library supporting high-performance implementations of the most commonly employed unsupervised link prediction methods. Link prediction in longitudinal data requires a sophisticated and disciplined procedure for correct results and fair evaluation, so the second principle contribution of LPmade is a sophisticated GNU make architecture that completely automates link prediction, prediction evaluation, and network analysis. Finally, LPmade streamlines and automates the procedure for creating multivariate supervised link prediction models with a version of WEKA modified to operate effectively on extremely large data sets. With mere minutes of manual work, one may start with a raw stream of records representing a network and progress through hundreds of steps to generate plots, gigabytes or terabytes of output, and actionable or publishable results.},
 author = {Ryan N. Lichtenwalter and Nitesh V. Chawla},
 code = {https://github.com/rlichtenwalter/LPmade},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W1487695060},
 pages = {2489--2492},
 pdf = {/papers/volume12/lichtenwalter11a/lichtenwalter11a.pdf},
 title = {LPmade: Link Prediction Made Easy},
 url = {http://jmlr.org/papers/v12/lichtenwalter11a.html},
 volume = {12},
 year = {2011}
}

@article{JMLR:v12:melnykov11a,
 abstract = {This paper presents the CLUSTERING ALGORITHMS' REFEREE PACKAGE or CARP, an open source GNU GPL-licensed C package for evaluating clustering algorithms. Calibrating performance of such algorithms is important and CARP addresses this need by generating datasets of different clustering complexity and by assessing the performance of the concerned algorithm in terms of its ability to classify each dataset relative to the true grouping. This paper briefly describes the software and its capabilities.},
 author = {Volodymyr Melnykov and Ranjan Maitra},
 code = {https://maitra.public.iastate.edu/Software/CARP.html},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W115463415},
 pages = {69--73},
 pdf = {/papers/volume12/melnykov11a/melnykov11a.pdf},
 title = {CARP: Software for Fishing Out Good Clustering Algorithms},
 url = {http://jmlr.org/papers/v12/melnykov11a.html},
 volume = {12},
 year = {2011}
}

@article{JMLR:v12:mueller11a,
 abstract = {The Stationary Subspace Analysis (SSA) algorithm linearly factorizes a high-dimensional time series into stationary and non-stationary components. The SSA Toolbox is a platform-independent efficient stand-alone implementation of the SSA algorithm with a graphical user interface written in Java, that can also be invoked from the command line and from Matlab. The graphical interface guides the user through the whole process; data can be imported and exported from comma separated values (CSV) and Matlab's .mat files.},
 author = {Jan Saputra M{{\"u}}ller and Paul von B{{\"u}}nau and Frank C. Meinecke and Franz J. Kir{{\'a}}ly and Klaus-Robert M{{\"u}}ller},
 code = {https://github.com/paulbuenau/SSA-Toolbox},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W2167291268},
 pages = {3065--3069},
 pdf = {/papers/volume12/mueller11a/mueller11a.pdf},
 title = {The Stationary Subspace Analysis Toolbox},
 url = {http://jmlr.org/papers/v12/mueller11a.html},
 volume = {12},
 year = {2011}
}

@article{JMLR:v12:pedregosa11a,
 abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.},
 author = {Fabian Pedregosa and Ga{{\"e}}l Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and {{\'E}}douard Duchesnay},
 code = {https://github.com/scikit-learn/scikit-learn},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W2101234009},
 pages = {2825--2830},
 pdf = {/papers/volume12/pedregosa11a/pedregosa11a.pdf},
 title = {Scikit-learn: Machine Learning in Python},
 url = {http://jmlr.org/papers/v12/pedregosa11a.html},
 volume = {12},
 year = {2011}
}

@article{JMLR:v12:tsoumakas11a,
 abstract = {MULAN is a Java library for learning from multi-label data. It offers a variety of classification, ranking, thresholding and dimensionality reduction algorithms, as well as algorithms for learning ...},
 author = {Grigorios Tsoumakas and Eleftherios Spyromitros-Xioufis and Jozef Vilcek and Ioannis Vlahavas},
 code = {https://github.com/tsoumakas/mulan},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W3008214279},
 pages = {2411--2414},
 pdf = {/papers/volume12/tsoumakas11a/tsoumakas11a.pdf},
 title = {MULAN: A Java Library for Multi-Label Learning},
 url = {http://jmlr.org/papers/v12/tsoumakas11a.html},
 volume = {12},
 year = {2011}
}

@article{JMLR:v13:benbouzid12a,
 abstract = {The MULTIBOOST package provides a fast C++ implementation of multi-class/multi-label/multitask boosting algorithms. It is based on ADABOOST.MH but it also implements popular cascade classifiers and FILTERBOOST. The package contains common multi-class base learners (stumps, trees, products, Haar filters). Further base learners and strong learners following the boosting paradigm can be easily implemented in a flexible framework.},
 author = {Djalel Benbouzid and R{{\'o}}bert Busa-Fekete and Norman Casagrande and Fran{\c{c}}ois-David Collin and Bal{{\'a}}zs K{{\'e}}gl},
 code = {multiboost.org},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W2184241488},
 pages = {549--553},
 pdf = {/papers/volume13/benbouzid12a/benbouzid12a.pdf},
 title = {MULTIBOOST: a multi-purpose boosting package},
 url = {http://jmlr.org/papers/v13/benbouzid12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:chen12a,
 abstract = {In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative filtering. SVDFeature is designed to efficiently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efficient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years.},
 author = {Tianqi Chen and Weinan Zhang and Qiuxia Lu and Kailong Chen and Zhao Zheng and Yong Yu},
 code = {https://mloss.org/software/view/333/},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W21207210},
 pages = {3619--3622},
 pdf = {/papers/volume13/chen12a/chen12a.pdf},
 title = {SVDFeature: a toolkit for feature-based collaborative filtering},
 url = {http://jmlr.org/papers/v13/chen12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:desmedt12a,
 abstract = {Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classifiers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern.},
 author = {Tom De Smedt and Walter Daelemans},
 code = {http://www.clips.ua.ac.be/pages/pattern},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2110453538},
 pages = {2063--2067},
 pdf = {/papers/volume13/desmedt12a/desmedt12a.pdf},
 title = {Pattern for python},
 url = {http://jmlr.org/papers/v13/desmedt12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:fortin12a,
 abstract = {DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license.},
 author = {F{{\'e}}lix-Antoine Fortin and Fran{\c{c}}ois-Michel De Rainville and Marc-Andr{{\'e}} Gardner and Marc Parizeau and Christian Gagn{{\'e}}},
 code = {https://github.com/DEAP/deap},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W2109042184},
 pages = {2171--2175},
 pdf = {/papers/volume13/fortin12a/fortin12a.pdf},
 title = {DEAP: evolutionary algorithms made easy},
 url = {http://jmlr.org/papers/v13/fortin12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:gould12a,
 abstract = {We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data flows.},
 author = {Stephen Gould},
 code = {https://github.com/sgould/drwn},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W148361626},
 pages = {3533--3537},
 pdf = {/papers/volume13/gould12a/gould12a.pdf},
 title = {DARWIN: a framework for machine learning and computer vision research and development},
 url = {http://jmlr.org/papers/v13/gould12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:grau12a,
 abstract = {Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classifiers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples.},
 author = {Jan Grau and Jens Keilwagen and Andr{{\'e}} Gohr and Berit Haldemann and Stefan Posch and Ivo Grosse},
 code = {http://www.jstacs.de},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W771098},
 pages = {1967--1971},
 pdf = {/papers/volume13/grau12a/grau12a.pdf},
 title = {Jstacs: a java framework for statistical analysis and classification of biological sequences},
 url = {http://jmlr.org/papers/v13/grau12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:lee12b,
 abstract = {Recommendation systems are important business applications with significant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms.},
 author = {Joonseok Lee and Mingxuan Sun and Guy Lebanon},
 code = {https://mloss.org/software/view/420/},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W2132555563},
 pages = {2699--2703},
 pdf = {/papers/volume13/lee12b/lee12b.pdf},
 title = {PREA: personalized recommendation algorithms toolkit},
 url = {http://jmlr.org/papers/v13/lee12b.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:nickisch12a,
 abstract = {The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean field. Scalable and efficient inference in fully-connected undirected graphical models or Markov random fields with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX files to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classification as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit.},
 author = {Hannes Nickisch},
 code = {https://mloss.org/software/view/269/},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W2165748376},
 pages = {1699--1703},
 pdf = {/papers/volume13/nickisch12a/nickisch12a.pdf},
 title = {glm-ie: generalised linear models inference & estimation toolbox},
 url = {http://jmlr.org/papers/v13/nickisch12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:park12a,
 abstract = {This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper.},
 author = {Chiwoo Park and Jianhua Z. Huang and Yu Ding},
 code = {http://mloss.org/revision/download/990/},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W38425204},
 pages = {775--779},
 pdf = {/papers/volume13/park12a/park12a.pdf},
 title = {GPLP: a local and parallel computation toolbox for Gaussian process regression},
 url = {http://jmlr.org/papers/v13/park12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:piccolo12a,
 abstract = {Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classification analyses in a systematic yet flexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net.},
 author = {Stephen R. Piccolo and Lewis J. Frey},
 code = {http://mlflex.sourceforge.net},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2118017676},
 pages = {555--559},
 pdf = {/papers/volume13/piccolo12a/piccolo12a.pdf},
 title = {ML-Flex: a flexible toolbox for performing classification analyses in parallel},
 url = {http://jmlr.org/papers/v13/piccolo12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:rieck12a,
 abstract = {Strings and sequences are ubiquitous in many areas of data analysis. However, only few learning methods can be directly applied to this form of data. We present Sally, a tool for embedding strings in vector spaces that allows for applying a wide range of learning methods to string data. Sally implements a generalized form of the bag-of-words model, where strings are mapped to a vector space that is spanned by a set of string features, such as words or n-grams of words. The implementation of Sally builds on efficient string algorithms and enables processing millions of strings and features. The tool supports several data formats and is capable of interfacing with common learning environments, such as Weka, Shogun, Matlab, or Pylab. Sally has been successfully applied for learning with natural language text, DNA sequences and monitored program behavior.},
 author = {Konrad Rieck and Christian Wressnegger and Alexander Bikadorov},
 code = {https://github.com/rieck/sally},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W2131182686},
 pages = {3247--3251},
 pdf = {/papers/volume13/rieck12a/rieck12a.pdf},
 title = {Sally: a tool for embedding strings in vector spaces},
 url = {http://jmlr.org/papers/v13/rieck12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:verstraeten12a,
 abstract = {Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several crossvalidation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http: //organic.elis.ugent.be/oger.},
 author = {David Verstraeten and Benjamin Schrauwen and Sander Dieleman and Philemon Brakel and Pieter Buteneers and Dejan Pecevski},
 code = {http://organic.elis.ugent.be/oger},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W165885910},
 pages = {2995--2998},
 pdf = {/papers/volume13/verstraeten12a/verstraeten12a.pdf},
 title = {Oger: modular learning architectures for large-scale sequential processing},
 url = {http://jmlr.org/papers/v13/verstraeten12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zeng12a,
 abstract = {Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models. The current version includes BP algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future. Interested users may also extend BP algorithms for learning more complicated topic models. The source codes are freely available under the GNU General Public Licence, Version 1.0 at https://mloss.org/software/view/399/.},
 author = {Jia Zeng},
 code = {https://mloss.org/software/view/399/},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2951681883},
 pages = {2233--2236},
 pdf = {/papers/volume13/zeng12a/zeng12a.pdf},
 title = {A Topic Modeling Toolbox Using Belief Propagation},
 url = {http://jmlr.org/papers/v13/zeng12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zhao12a,
 abstract = {We describe an R package named huge which provides easy-to-use functions for estimating high dimensional undirected graphs from data. This package implements recent results in the literature, including Friedman et al. (2007), Liu et al. (2009, 2012) and Liu et al. (2010). Compared with the existing graph estimation package glasso, the huge package provides extra features: (1) instead of using Fortan, it is written in C, which makes the code more portable and easier to modify; (2) besides fitting Gaussian graphical models, it also provides functions for fitting high dimensional semiparametric Gaussian copula models; (3) more functions like data-dependent model selection, data generation and graph visualization; (4) a minor convergence problem of the graphical lasso algorithm is corrected; (5) the package allows the user to apply both lossless and lossy screening rules to scale up large-scale problems, making a tradeoff between computational and statistical efficiency.},
 author = {Tuo Zhao and Han Liu and Kathryn Roeder and John Lafferty and Larry Wasserman},
 code = {https://cran.r-project.org/web/packages/huge/},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2125156589},
 pages = {1059--1062},
 pdf = {/papers/volume13/zhao12a/zhao12a.pdf},
 title = {The huge Package for High-dimensional Undirected Graph Estimation in R},
 url = {http://jmlr.org/papers/v13/zhao12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zitnik12a,
 abstract = {NIMFA is an open-source Python library that provides a unified interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA's component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks.},
 author = {Marinka {\v{Z}}itnik and Bla{\v{z}} Zupan},
 code = {https://github.com/mims-harvard/nimfa},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W1823022514},
 pages = {849--853},
 pdf = {/papers/volume13/zitnik12a/zitnik12a.pdf},
 title = {NIMFA: a python library for nonnegative matrix factorization},
 url = {http://jmlr.org/papers/v13/zitnik12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v14:curtin13a,
 abstract = {MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning library released in late 2011 offering both a simple, consistent API accessible to novice users and high performance and flexibility to expert users by leveraging modern features of C++. MLPACK provides cutting-edge algorithms whose benchmarks exhibit far better performance than other leading machine learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available at http://www.mlpack.org.},
 author = {Ryan R. Curtin and James R. Cline and N. P. Slagle and William B. March and Parikshit Ram and Nishant A. Mehta and Alexander G. Gray},
 code = {https://github.com/mlpack/mlpack},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W4301584875},
 pages = {801--805},
 pdf = {/papers/volume14/curtin13a/curtin13a.pdf},
 title = {MLPACK: A Scalable C++ Machine Learning Library},
 url = {http://jmlr.org/papers/v14/curtin13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:demsar13a,
 abstract = {Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.},
 author = {Janez Dem{\v{s}}ar and Toma{\v{z}} Curk and Ale{\v{s}} Erjavec and {\v{C}}rt Gorup and Toma{\v{z}} Ho{\v{c}}evar and Mitar Milutinovi{\v{c}} and Martin Mo{\v{z}}ina and Matija Polajnar and Marko Toplak and An{\v{z}}e Stari{\v{c}} and Miha {\v{S}}tajdohar and Lan Umek and Lan {\v{Z}}agar and Jure {\v{Z}}bontar and Marinka {\v{Z}}itnik and Bla{\v{z}} Zupan},
 code = {https://github.com/biolab/orange3},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2131850886},
 pages = {2349--2353},
 pdf = {/papers/volume14/demsar13a/demsar13a.pdf},
 title = {Orange: data mining toolbox in python},
 url = {http://jmlr.org/papers/v14/demsar13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:djuric13a,
 abstract = {We present BudgetedSVM, an open-source C++ toolbox comprising highly-optimized implementations of recently proposed algorithms for scalable training of Support Vector Machine (SVM) approximators: Adaptive Multi-hyperplane Machines, Low-rank Linearization SVM, and Budgeted Stochastic Gradient Descent. BudgetedSVM trains models with accuracy comparable to LibSVM in time comparable to LibLinear, solving non-linear problems with millions of high-dimensional examples within minutes on a regular computer. We provide command-line and Matlab interfaces to BudgetedSVM, an efficient API for handling large-scale, high-dimensional data sets, as well as detailed documentation to help developers use and further extend the toolbox.},
 author = {Nemanja Djuric and Liang Lan and Slobodan Vucetic and Zhuang Wang},
 code = {https://github.com/djurikom/BudgetedSVM},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W2141642784},
 pages = {3813--3817},
 pdf = {/papers/volume14/djuric13a/djuric13a.pdf},
 title = {BudgetedSVM: a toolbox for scalable SVM approximations},
 url = {http://jmlr.org/papers/v14/djuric13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:frezza-buet13a,
 abstract = {This paper introduces the rllib as an original C++ template-based library oriented toward value function estimation. Generic programming is promoted here as a way of having a good fit between the mathematics of reinforcement learning and their implementation in a library. The main concepts of rllib are presented, as well as a short example.},
 author = {HervÃ© Frezza-Buet and Matthieu Geist},
 code = {https://github.com/HerveFrezza-Buet/RLlib},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W153509885},
 pages = {625--628},
 pdf = {/papers/volume14/frezza-buet13a/frezza-buet13a.pdf},
 title = {A C++ Template-Based Reinforcement Learning Library: Fitting the Code to the Mathematics},
 url = {http://jmlr.org/papers/v14/frezza-buet13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:lewis13a,
 abstract = {Divvy is an application for applying unsupervised machine learning techniques (clustering and dimensionality reduction) to the data analysis process. Divvy provides a novel UI that allows researchers to tighten the action-perception loop of changing algorithm parameters and seeing a visualization of the result. Machine learning researchers can use Divvy to publish easy to use reference implementations of their algorithms, which helps themachine learning field have a greater impact on research practices elsewhere.},
 author = {Joshua M. Lewis and Virginia R. de Sa and Laurens van der Maaten},
 code = {https://github.com/jmlewis/divvy},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W2164959980},
 pages = {3159--3163},
 pdf = {/papers/volume14/lewis13a/lewis13a.pdf},
 title = {Divvy: fast and intuitive exploratory data analysis},
 url = {http://jmlr.org/papers/v14/lewis13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:lisitsyn13a,
 abstract = {We present Tapkee, a C++ template library that provides efficient implementations of more than 20 widely used dimensionality reduction techniques ranging from Locally Linear Embedding (Roweis and Saul, 2000) and Isomap (de Silva and Tenenbaum, 2002) to the recently introduced Barnes-Hut-SNE (van der Maaten, 2013). Our library was designed with a focus on performance and flexibility. For performance, we combine efficient multi-core algorithms, modern data structures and state-of-the-art low-level libraries. To achieve flexibility, we designed a clean interface for applying methods to user data and provide a callback API that facilitates integration with the library. The library is freely available as open-source software and is distributed under the permissive BSD 3-clause license. We encourage the integration of Tapkee into other open-source toolboxes and libraries. For example, Tapkee has been integrated into the codebase of the Shogun toolbox (Sonnenburg et al., 2010), giving us access to a rich set of kernels, distance measures and bindings to common programming languages including Python, Octave, Matlab, R, Java, C#, Ruby, Perl and Lua. Source code, examples and documentation are available at http://tapkee.lisitsyn.me.},
 author = {Sergey Lisitsyn and Christian Widmer and Fernando J. Iglesias Garcia},
 code = {https://github.com/lisitsyn/tapkee},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W2131142674},
 pages = {2355--2359},
 pdf = {/papers/volume14/lisitsyn13a/lisitsyn13a.pdf},
 title = {Tapkee: an efficient dimension reduction library},
 url = {http://jmlr.org/papers/v14/lisitsyn13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:picard13a,
 abstract = {JKernelMachines is a Java library for learning with kernels. It is primarily designed to deal with custom kernels that are not easily found in standard libraries, such as kernels on structured data. These types of kernels are often used in computer vision or bioinformatics applications. We provide several kernels leading to state of the art classification performances in computer vision, as well as various kernels on sets. The main focus of the library is to be easily extended with new kernels. Standard SVM optimization algorithms are available, but also more sophisticated learning-based kernel combination methods such as Multiple Kernel Learning (MKL), and a recently published algorithm to learn powered products of similarities (Product Kernel Learning).},
 author = {David Picard and Nicolas Thome and Matthieu Cord},
 code = {https://github.com/davidpicard/jkernelmachines},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W2133046540},
 pages = {1417--1421},
 pdf = {/papers/volume14/picard13a/picard13a.pdf},
 title = {JKernelMachines: a simple framework for kernel machine},
 url = {http://jmlr.org/papers/v14/picard13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:salleb-aouissi13a,
 abstract = {In this paper, we propose QUANTMINER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers good intervals in association rules by optimizing both the support and the confidence. The experiments on real and artificial databases have shown the usefulness of QUANTMINER as an interactive, exploratory data mining tool.},
 author = {Ansaf Salleb-Aouissi and Christel Vrain and Cyril Nortet and Xiangrong Kong and Vivek Rathod and Daniel Cassard},
 code = {https://github.com/QuantMiner/QuantMiner},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W2138358572},
 pages = {3153--3157},
 pdf = {/papers/volume14/salleb-aouissi13a/salleb-aouissi13a.pdf},
 title = {QuantMiner for Mining Quantitative Association Rules},
 url = {http://jmlr.org/papers/v14/salleb-aouissi13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:tacchetti13a,
 abstract = {We present GURLS, a least squares, modular, easy-to-extend software library for efficient supervised learning. GURLS is targeted to machine learning practitioners, as well as non-specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efficient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available. The package is distributed under the BSD licence and is available for download at https://github.com/CBCL/GURLS.},
 author = {Andrea Tacchetti and Pavan K. Mallapragada and Matteo Santoro and Lorenzo Rosasco},
 code = {https://github.com/LCSL/GURLS},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2137592396},
 pages = {3201--3205},
 pdf = {/papers/volume14/tacchetti13a/tacchetti13a.pdf},
 title = {GURLS: a Least Squares Library for Supervised Learning},
 url = {http://jmlr.org/papers/v14/tacchetti13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:vanhatalo13a,
 abstract = {The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods.},
 author = {Jarno Vanhatalo and Jaakko RiihimÃ¤ki and Jouni Hartikainen and Pasi JylÃ¤nki and Ville Tolvanen and Aki Vehtari},
 code = {https://github.com/gpstuff-dev/gpstuff},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W136174036},
 pages = {1175--1179},
 pdf = {/papers/volume14/vanhatalo13a/vanhatalo13a.pdf},
 title = {GPstuff: Bayesian modeling with Gaussian processes},
 url = {http://jmlr.org/papers/v14/vanhatalo13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:wang13d,
 abstract = {We describe a R-Java CAM (convex analysis of mixtures) package that provides comprehensive analytic functions and a graphic user interface (GUI) for blindly separating mixed nonnegative sources. This open-source multiplatform software implements recent and classic algorithms in the literature including Chan et al. (2008), Wang et al. (2010), Chen et al. (2011a) and Chen et al. (2011b). The CAM package offers several attractive features: (1) instead of using proprietary MATLAB, its analytic functions are written in R, which makes the codes more portable and easier to modify; (2) besides producing and plotting results in R, it also provides a Java GUI for automatic progress update and convenient visual monitoring; (3) multi-thread interactions between the R and Java modules are driven and integrated by a Java GUI, assuring that the whole CAM software runs responsively; (4) the package offers a simple mechanism to allow others to plug-in additional R-functions.},
 author = {Niya Wang and Fan Meng and Li Chen and Subha Madhavan and Robert Clarke and Eric P. Hoffman and Jianhua Xuan and Yue Wang},
 code = {https://mloss.org/software/view/437/},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W2130705250},
 pages = {2899--2903},
 pdf = {/papers/volume14/wang13d/wang13d.pdf},
 title = {The CAM software for nonnegative blind source separation in R-Java},
 url = {http://jmlr.org/papers/v14/wang13d.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v15:boumal14a,
 abstract = {Optimization on manifolds is a rapidly developing branch of nonlinear optimization. Its focus is on problems where the smooth geometry of the search space can be leveraged to design efficient numerical algorithms. In particular, optimization on manifolds is well-suited to deal with rank and orthogonality constraints. Such structured constraints appear pervasively in machine learning applications, including low-rank matrix completion, sensor network localization, camera network registration, independent component analysis, metric learning, dimensionality reduction and so on. The Manopt toolbox, available at www.manopt.org, is a user-friendly, documented piece of software dedicated to simplify experimenting with state of the art Riemannian optimization algorithms. We aim particularly at reaching practitioners outside our field.},
 author = {Nicolas Boumal and Bamdev Mishra and P.-A. Absil and Rodolphe Sepulchre},
 code = {https://www.manopt.org},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W2167623372},
 pages = {1455--1459},
 pdf = {/papers/volume15/boumal14a/boumal14a.pdf},
 title = {Manopt, a Matlab toolbox for optimization on manifolds},
 url = {http://jmlr.org/papers/v15/boumal14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:claesen14a,
 abstract = {EnsembleSVM is a free software package containing efficient routines to perform ensemble learning with support vector machine (SVM) base models. It currently offers ensemble methods based on binary SVM models. Our implementation avoids duplicate storage and evaluation of support vectors which are shared between constituent models. Experimental results show that using ensemble approaches can drastically reduce training complexity while maintaining high predictive accuracy. The EnsembleSVM software package is freely available online at http://esat.kuleuven.be/stadius/ensemblesvm.},
 author = {Marc Claesen and Frank De Smet and Johan A.K. Suykens and Bart De Moor},
 code = {https://github.com/claesenm/EnsembleSVM},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2166619467},
 pages = {141--145},
 pdf = {/papers/volume15/claesen14a/claesen14a.pdf},
 title = {EnsembleSVM: A Library for Ensemble Learning Using Support Vector Machines},
 url = {http://jmlr.org/papers/v15/claesen14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:couckuyt14a,
 abstract = {When analyzing data from computationally expensive simulation codes, surrogate modeling methods are firmly established as facilitators for design space exploration, sensitivity analysis, visualization and optimization. Kriging is a popular surrogate modeling technique used for the Design and Analysis of Computer Experiments (DACE). Hence, the past decade Kriging has been the subject of extensive research and many extensions have been proposed, e.g., co-Kriging, stochastic Kriging, blind Kriging, etc. However, few Kriging implementations are publicly available and tailored towards scientists and engineers. Furthermore, no Kriging toolbox exists that unifies several Kriging flavors. This paper addresses this need by presenting an efficient object-oriented Kriging implementation and several Kriging extensions, providing a flexible and easily extendable framework to test and implement new Kriging flavors while reusing as much code as possible.},
 author = {Ivo Couckuyt and Tom Dhaene and Piet Demeester},
 code = {https://cran.r-project.org/web/packages/DiceKriging/index.html},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2151794096},
 pages = {3183--3186},
 pdf = {/papers/volume15/couckuyt14a/couckuyt14a.pdf},
 title = {ooDACE toolbox: a flexible object-oriented Kriging implementation},
 url = {http://jmlr.org/papers/v15/couckuyt14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:cuong14a,
 abstract = {Dependencies among neighboring labels in a sequence are important sources of information for sequence labeling and segmentation. However, only first-order dependencies, which are dependencies between adjacent labels or segments, are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we give effcient inference algorithms to handle high-order dependencies between labels or segments in conditional random fields, under the assumption that the number of distinct label patterns used in the features is small. This leads to effcient learning algorithms for these conditional random fields. We show experimentally that exploiting high-order dependencies can lead to substantial performance improvements for some problems, and we discuss conditions under which high-order features can be effective.},
 author = {Nguyen Viet Cuong and Nan Ye and Wee Sun Lee and Hai Leong Chieu},
 code = {https://github.com/nvcuong/HOSemiCRF},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2165073960},
 pages = {981--1009},
 pdf = {/papers/volume15/cuong14a/cuong14a.pdf},
 title = {Conditional random field with high-order dependencies for sequence labeling and segmentation},
 url = {http://jmlr.org/papers/v15/cuong14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:fournierviger14a,
 abstract = {We present SPMF, an open-source data mining library offering implementations of more than 55 data mining algorithms. SPMF is a cross-platform library implemented in Java, specialized for discovering patterns in transaction and sequence databases such as frequent itemsets, association rules and sequential patterns. The source code can be integrated in other Java programs. Moreover, SPMF offers a command line interface and a simple graphical interface for quick testing. The source code is available under the GNU General Public License, version 3. The website of the project offers several resources such as documentation with examples of how to run each algorithm, a developer's guide, performance comparisons of algorithms, data sets, an active forum, a FAQ and a mailing list.},
 author = {Philippe Fournier-Viger and Antonio Gomariz and Ted Gueniche and Azadeh Soltani and Cheng-Wei Wu and Vincent S. Tseng},
 code = {https://www.philippe-fournier-viger.com/spmf/},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W2126046032},
 pages = {3569--3573},
 pdf = {/papers/volume15/fournierviger14a/fournierviger14a.pdf},
 title = {SPMF: a Java open-source pattern mining library},
 url = {http://jmlr.org/papers/v15/fournierviger14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:gillian14a,
 abstract = {The Gesture Recognition Toolkit is a cross-platform open-source C++ library designed to make real-time machine learning and gesture recognition more accessible for non-specialists. Emphasis is placed on ease of use, with a consistent, minimalist design that promotes accessibility while supporting flexibility and customization for advanced users. The toolkit features a broad range of classification and regression algorithms and has extensive support for building real-time systems. This includes algorithms for signal processing, feature extraction and automatic gesture spotting.},
 author = {Nicholas Gillian and Joseph A. Paradiso},
 code = {https://github.com/nickgillian/grt},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W2151889438},
 pages = {3483--3487},
 pdf = {/papers/volume15/gillian14a/gillian14a.pdf},
 title = {The Gesture Recognition Toolkit},
 url = {http://jmlr.org/papers/v15/gillian14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:hoi14a,
 abstract = {LIBOL is an open-source library for large-scale online learning, which consists of a large family of efficient and scalable state-of-the-art online learning algorithms for large-scale online classification tasks. We have offered easy-to-use command-line tools and examples for users and developers, and also have made comprehensive documents available for both beginners and advanced users. LIBOL is not only a machine learning toolbox, but also a comprehensive experimental platform for conducting online learning research.},
 author = {Steven C.H. Hoi and Jialei Wang and Peilin Zhao},
 code = {https://github.com/LIBOL/LIBOL},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2119393863},
 pages = {495--499},
 pdf = {/papers/volume15/hoi14a/hoi14a.pdf},
 title = {LIBOL: a library for online learning algorithms},
 url = {http://jmlr.org/papers/v15/hoi14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:martinezcantin14a,
 abstract = {BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization is sample efficient by building a posterior distribution to capture the evidence and prior knowledge for the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave.},
 author = {Ruben Martinez-Cantin},
 code = {https://github.com/rmcantin/bayesopt},
 journal = {Journal of Machine Learning Research},
 number = {115},
 openalex = {W4295725036},
 pages = {3915--3919},
 pdf = {/papers/volume15/martinezcantin14a/martinezcantin14a.pdf},
 title = {BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits},
 url = {http://jmlr.org/papers/v15/martinezcantin14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:mueller14a,
 abstract = {Structured prediction methods have become a central tool for many machine learning applications. While more and more algorithms are developed, only very few implementations are available.

PyStruct aims at providing a general purpose implementation of standard structured prediction methods, both for practitioners and as a baseline for researchers. It is written in Python and adapts paradigms and types from the scientific Python community for seamless integration with other projects.},
 author = {Andreas C. M{{\"u}}ller and Sven Behnke},
 code = {https://pystruct.github.io},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W181529752},
 pages = {2055--2060},
 pdf = {/papers/volume15/mueller14a/mueller14a.pdf},
 title = {PyStruct: learning structured prediction in python},
 url = {http://jmlr.org/papers/v15/mueller14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:pang14a,
 abstract = {We develop an R package FASTCLIME for solving a family of regularized linear programming (LP) problems. Our package efficiently implements the parametric simplex algorithm, which provides a scalabl...},
 author = {Haotian Pang and Han Liu and Robert V and erbei},
 code = {http://cran.nexr.com/web/packages/fastclime/},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W3114411530},
 pages = {489--493},
 pdf = {/papers/volume15/pang14a/pang14a.pdf},
 title = {The fastclime package for linear programming and large-scale precision matrix estimation in R},
 url = {http://jmlr.org/papers/v15/pang14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:szabo14a,
 abstract = {We present ITE (information theoretical estimators) a free and open source, multi-platform, Matlab/Octave toolbox that is capable of estimating many different variants of entropy, mutual information, divergence, association measures, cross quantities, and kernels on distributions. Thanks to its highly modular design, ITE supports additionally (i) the combinations of the estimation techniques, (ii) the easy construction and embedding of novel information theoretical estimators, and (iii) their immediate application in information theoretical optimization problems. ITE also includes a prototype application in a central problem class of signal processing, independent subspace analysis and its extensions.},
 author = {Zolt{{\'a}}n Szab{{\'o}}},
 code = {https://bitbucket.org/szzoli/ite/},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W1522345630},
 pages = {283--287},
 pdf = {/papers/volume15/szabo14a/szabo14a.pdf},
 title = {Information Theoretical Estimators Toolbox},
 url = {http://jmlr.org/papers/v15/szabo14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v16:cano15a,
 abstract = {JCLEC-Classification is a usable and extensible open source library for genetic programming classification algorithms. It houses implementations of rule-based methods for classification based on genetic programming, supporting multiple model representations and providing to users the tools to implement any classifier easily. The software is written in Java and it is available from http://jclec.sourceforge.net/classification under the GPL license.},
 author = {Alberto Cano and Jos{{\'e}} Mar{{\'i}}a Luna and Amelia Zafra and Sebasti{{\'a}}n Ventura},
 code = {http://samoa.incubator.apache.org},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2100683190},
 pages = {491--494},
 pdf = {/papers/volume16/cano15a/cano15a.pdf},
 title = {A classification module for genetic programming algorithms in JCLEC},
 url = {http://jmlr.org/papers/v16/cano15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:geramifard15a,
 abstract = {RLPy is an object-oriented reinforcement learning software package with a focus on value-function-based methods using linear function approximation and discrete actions. The framework was designed for both educational and research purposes. It provides a rich library of fine-grained, easily exchangeable components for learning agents (e.g., policies or representations of value functions), facilitating recently increased specialization in reinforcement learning. RLPy is written in Python to allow fast prototyping, but is also suitable for large-scale experiments through its built-in support for optimized numerical libraries and parallelization. Code profiling, domain visualizations, and data analysis are integrated in a self-contained package available under the Modified BSD License at http://github.com/rlpy/rlpy. All of these properties allow users to compare various reinforcement learning algorithms with little effort.},
 author = {Alborz Geramifard and Christoph Dann and Robert H. Klein and William Dabney and Jonathan P. How},
 code = {https://github.com/rlpy/rlpy/},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W2130935555},
 pages = {1573--1578},
 pdf = {/papers/volume16/geramifard15a/geramifard15a.pdf},
 title = {RLPy: a value-function-based reinforcement learning framework for education and research},
 url = {http://jmlr.org/papers/v16/geramifard15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:heaton15a,
 abstract = {This paper introduces the Encog library for Java and C#, a scalable, adaptable, multiplatform machine learning framework that was 1st released in 2008. Encog allows a variety of machine learning models to be applied to datasets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient multithreaded code to reduce training time by exploiting modern multicore processors. The current version of Encog can be downloaded from http://www.encog.org.},
 author = {Jeff Heaton},
 code = {http://www.heatonresearch.com/download/},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W4300124873},
 pages = {1243--1247},
 pdf = {/papers/volume16/heaton15a/heaton15a.pdf},
 title = {Encog: Library of Interchangeable Machine Learning Models for Java and C#},
 url = {http://jmlr.org/papers/v16/heaton15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:hothorn15a,
 abstract = {The R package partykit provides a flexible toolkit for learning, representing, summarizing, and visualizing a wide range of tree-structured regression and classification models. The functionality encompasses: (a) basic infrastructure for representing trees (inferred by any algorithm) so that unified print/plot/predict methods are available; (b) dedicated methods for trees with constant fits in the leaves (or terminal nodes) along with suitable coercion functions to create such trees (e.g., by rpart, RWeka, PMML); (c) a reimplementation of conditional inference trees (ctree, originally provided in the party package); (d) an extended reimplementation of model-based recursive partitioning (mob, also originally in party) along with dedicated methods for trees with parametric models in the leaves. Here, a brief overview of the package and its design is given while more detailed discussions of items (a)--(d) are available in vignettes accompanying the package.},
 author = {Torsten Hothorn and Achim Zeileis},
 code = {https://cran.r-project.org/web/packages/partykit/index.html},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W3121876541},
 pages = {3905--3909},
 pdf = {/papers/volume16/hothorn15a/hothorn15a.pdf},
 title = {partykit: A Modular Toolkit for Recursive Partytioning in R},
 url = {http://jmlr.org/papers/v16/hothorn15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:li15a,
 abstract = {This paper describes an R package named flare, which implements a family of new high dimensional regression methods (LAD Lasso, SQRT Lasso, lq Lasso, and Dantzig selector) and their extensions to s...},
 author = {Xingguo Li and Tuo Zhao and Xiaoming Yuan and Han Liu},
 code = {http://cran.r-project.org/src/contrib/flare_1.5.0.tar.gz},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W3008266953},
 pages = {553--557},
 pdf = {/papers/volume16/li15a/li15a.pdf},
 title = {The flare package for high dimensional linear regression and precision matrix estimation in R},
 url = {http://jmlr.org/papers/v16/li15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:lowd15a,
 abstract = {The Libra Toolkit is a collection of algorithms for learning and inference with discrete probabilistic models, including Bayesian networks, Markov networks, dependency networks, and sum-product networks. Compared to other toolkits, Libra places a greater emphasis on learning the structure of tractable models in which exact inference is efficient. It also includes a variety of algorithms for learning graphical models in which inference is potentially intractable, and for performing exact and approximate inference. Libra is released under a 2-clause BSD license to encourage broad use in academia and industry.},
 author = {Daniel Lowd and Amirmohammad Rooshenas},
 code = {http://libra.cs.uoregon.edu},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W2963458184},
 pages = {2459--2463},
 pdf = {/papers/volume16/lowd15a/lowd15a.pdf},
 title = {The Libra Toolkit for Probabilistic Models},
 url = {http://jmlr.org/papers/v16/lowd15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:morales15a,
 abstract = {SAMOA (SCALABLE ADVANCED MASSIVE ONLINE ANALYSIS) is a platform for mining big data streams. It provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Storm, S4, and Samza. samoa is written in Java, is open source, and is available at http://samoa-project.net under the Apache Software License version 2.0.},
 author = {Gianmarco De Francisci Morales and Albert Bifet},
 code = {http://jclec.sourceforge.net},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2104564392},
 pages = {149--153},
 pdf = {/papers/volume16/morales15a/morales15a.pdf},
 title = {SAMOA: scalable advanced massive online analysis},
 url = {http://jmlr.org/papers/v16/morales15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:neumann15a,
 abstract = {We introduce pyGPs, an object-oriented implementation of Gaussian processes (gps) for machine learning. The library provides a wide range of functionalities reaching from simple gp specification via mean and covariance and gp inference to more complex implementations of hyperparameter optimization, sparse approximations, and graph based learning. Using Python we focus on usability for both users and researchers. Our main goal is to offer a user-friendly and flexible implementation of GPs for machine learning.},
 author = {Marion Neumann and Shan Huang and Daniel E. Marthaler and Kristian Kersting},
 code = {https://github.com/PMBio/pygp},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W2277766262},
 pages = {2611--2616},
 pdf = {/papers/volume16/neumann15a/neumann15a.pdf},
 title = {pyGPs: a Python library for Gaussian process regression and classification},
 url = {http://jmlr.org/papers/v16/neumann15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:weninger15a,
 abstract = {In this article, we introduce CURRENNT, an open-source parallel implementation of deep recurrent neural networks (RNNs) supporting graphics processing units (GPUs) through NVIDIA's Computed Unified Device Architecture (CUDA). CURRENNT supports uni- and bidirectional RNNs with Long Short-Term Memory (LSTM) memory cells which overcome the vanishing gradient problem. To our knowledge, CURRENNT is the first publicly available parallel implementation of deep LSTM-RNNs. Benchmarks are given on a noisy speech recognition task from the 2013 2nd CHiME Speech Separation and Recognition Challenge, where LSTM-RNNs have been shown to deliver best performance. In the result, double digit speedups in bidirectional LSTM training are achieved with respect to a reference single-threaded CPU implementation. CURRENNT is available under the GNU General Public License from http://sourceforge.net/p/currennt.},
 author = {Felix Weninger},
 code = {http://sourceforge.net/projects/currennt/},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W304834817},
 pages = {547--551},
 pdf = {/papers/volume16/weninger15a/weninger15a.pdf},
 title = {Introducing CURRENNT: the Munich open-source CUDA recurrent neural network toolkit},
 url = {http://jmlr.org/papers/v16/weninger15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:zhang15a,
 abstract = {CEKA is a software package for developers and researchers to mine the wisdom of crowds. It makes the entire knowledge discovery procedure much easier, including analyzing qualities of workers, simulating labeling behaviors, inferring true class labels of instances, filtering and correcting mislabeled instances (noise), building learning models and evaluating them. It integrates a set of state-of-the-art inference algorithms, a set of general noise handling algorithms, and abundant functions for model training and evaluation. CEKA is written in Java with core classes being compatible with the well-known machine learning tool WEKA, which makes the utilization of the functions in WEKA much easier.},
 author = {Jing Zhang and Victor S. Sheng and Bryce A. Nicholson and Xindong Wu},
 code = {http://ceka.sourceforge.net},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W2285341298},
 pages = {2853--2858},
 pdf = {/papers/volume16/zhang15a/zhang15a.pdf},
 title = {CEKA: a tool for mining the wisdom of crowds},
 url = {http://jmlr.org/papers/v16/zhang15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v17:12-164,
 abstract = {Multi-label classification has rapidly attracted interest in the machine learning literature, and there are now a large number and considerable variety of methods for this type of learning. We present MEKA: an open-source Java framework based on the well-known WEKA library. MEKA provides interfaces to facilitate practical application, and a wealth of multi-label classifiers, evaluation metrics, and tools for multi-label experiments and development. It supports multi-label and multi-target data, including in incremental and semi-supervised contexts.},
 author = {Jesse Read and Peter Reutemann and Bernhard Pfahringer and Geoff Holmes},
 code = {https://github.com/Waikato/meka},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2343252634},
 pages = {1--5},
 pdf = {/papers/volume17/12-164/12-164.pdf},
 title = {Meka: a multi-label/multi-target extension to weka},
 url = {http://jmlr.org/papers/v17/12-164.html},
 volume = {17},
 year = {2016}
}

@article{JMLR:v17:15-237,
 abstract = {Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.},
 author = {Xiangrui Meng and Joseph Bradley and Burak Yavuz and Evan Sparks and Shivaram Venkataraman and Davies Liu and Jeremy Freeman and DB Tsai and Manish Amde and Sean Owen and Doris Xin and Reynold Xin and Michael J. Franklin and Reza Zadeh and Matei Zaharia and Ameet Talwalkar},
 code = {http://spark.apache.org/downloads.html},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W2963288913},
 pages = {1--7},
 pdf = {/papers/volume17/15-237/15-237.pdf},
 title = {MLlib: Machine Learning in Apache Spark},
 url = {http://jmlr.org/papers/v17/15-237.html},
 volume = {17},
 year = {2016}
}

@article{JMLR:v17:15-347,
 abstract = {Active Learning has become an important area of research owing to the increasing number of real-world problems which contain labelled and unlabelled examples at the same time. JCLAL is a Java Class Library for Active Learning which has an architecture that follows strong principles of object-oriented design. It is easy to use, and it allows the developers to adapt, modify and extend the framework according to their needs. The library offers a variety of active learning methods that have been proposed in the literature. The software is available under the GPL license.},
 author = {Oscar Reyes and Eduardo P{{\'e}}rez and Mar{{\'i}}a del Carmen Rodr{{\'i}}guez-Hern{{\'a}}ndez and Habib M. Fardoun and Sebasti{{\'a}}n Ventura},
 code = {https://sourceforge.net/projects/jclal/},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W2511186734},
 pages = {1--5},
 pdf = {/papers/volume17/15-347/15-347.pdf},
 title = {JCLAL: a Java framework for active learning},
 url = {http://jmlr.org/papers/v17/15-347.html},
 volume = {17},
 year = {2016}
}

@article{JMLR:v17:15-355,
 abstract = {Factorization Machines (FM) are only used in a narrow range of applications and are not part of the standard toolbox of machine learning models. This is a pity, because even though FMs are recognized as being very successful for recommender system type applications they are a general model to deal with sparse and high dimensional features. Our Factorization Machine implementation provides easy access to many solvers and supports regression, classification and ranking tasks. Such an implementation simplifies the use of FM's for a wide field of applications. This implementation has the potential to improve our understanding of the FM model and drive new development.},
 author = {Immanuel Bayer},
 code = {https://github.com/ibayer/fastFM},
 journal = {Journal of Machine Learning Research},
 number = {184},
 openalex = {W4299536598},
 pages = {1--5},
 pdf = {/papers/volume17/15-355/15-355.pdf},
 title = {fastFM: A Library for Factorization Machines},
 url = {http://jmlr.org/papers/v17/15-355.html},
 volume = {17},
 year = {2016}
}

@article{JMLR:v17:15-408,
 abstract = {CVXPY is a domain-specific language for convex optimization embedded in Python. It allows the user to express convex optimization problems in a natural syntax that follows the math, rather than in the restrictive standard form required by solvers. CVXPY makes it easy to combine convex optimization with high-level features of Python such as parallelism and object-oriented design. CVXPY is available at this http URL under the GPL license, along with documentation and examples.},
 author = {Steven Diamond and Stephen Boyd},
 code = {https://github.com/cvxgrp/cvxpy},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W2949979136},
 pages = {1--5},
 pdf = {/papers/volume17/15-408/15-408.pdf},
 title = {CVXPY: A Python-Embedded Modeling Language for Convex Optimization},
 url = {http://jmlr.org/papers/v17/15-408.html},
 volume = {17},
 year = {2016}
}

@article{JMLR:v17:15-471,
 abstract = {Matrix factorization (MF) plays a key role in many applications such as recommender systems and computer vision, but MF may take long running time for handling large matrices commonly seen in the big data era. Many parallel techniques have been proposed to reduce the running time, but few parallel MF packages are available. Therefore, we present an open source library, LIBMF, based on recent advances of parallel MF for shared-memory systems. LIBMF includes easy-to-use command-line tools, interfaces to C/C++ languages, and comprehensive documentation. Our experiments demonstrate that LIBMF outperforms state of the art packages. LIBMF is BSD-licensed, so users can freely use, modify, and redistribute the code.},
 author = {Wei-Sheng Chin and Bo-Wen Yuan and Meng-Yuan Yang and Yong Zhuang and Yu-Chin Juan and Chih-Jen Lin},
 code = {https://www.csie.ntu.edu.tw/~cjlin/libmf/},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W2416951723},
 pages = {1--5},
 pdf = {/papers/volume17/15-471/15-471.pdf},
 title = {LIBMF: a library for parallel matrix factorization in shared-memory systems},
 url = {http://jmlr.org/papers/v17/15-471.html},
 volume = {17},
 year = {2016}
}

@article{JMLR:v17:15-535,
 abstract = {We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.},
 author = {Jure {\v{Z}}bontar and Yann LeCun},
 code = {https://github.com/jzbontar/mc-cnn},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W4299328286},
 pages = {1--32},
 pdf = {/papers/volume17/15-535/15-535.pdf},
 title = {Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches},
 url = {http://jmlr.org/papers/v17/15-535.html},
 volume = {17},
 year = {2016}
}

@article{JMLR:v17:16-109,
 abstract = {Manifold Learning (ML) is a class of algorithms seeking a low-dimensional non-linear representation of high-dimensional data. Thus, ML algorithms are most applicable to high-dimensional data and require large sample sizes to accurately estimate the manifold. Despite this, most existing manifold learning implementations are not particularly scalable. Here we present a Python package that implements a variety of manifold learning algorithms in a modular and scalable fashion, using fast approximate neighbors searches and fast sparse eigendecompositions. The package incorporates theoretical advances in manifold learning, such as the unbiased Laplacian estimator introduced by Coifman and Lafon (2006) and the estimation of the embedding distortion by the Riemannian metric method introduced by Perrault-Joncas and Meila (2013). In benchmarks, even on a single-core desktop computer, our code embeds millions of data points in minutes, and takes just 200 minutes to embed the main sample of galaxy spectra from the Sloan Digital Sky Survey-- consisting of 0.6 million samples in 3750-dimensions--a task which has not previously been possible.},
 author = {James McQueen and Marina Meil{\u{a}} and Jacob VanderPlas and Zhongyue Zhang},
 code = {https://github.com/mmp2/megaman},
 journal = {Journal of Machine Learning Research},
 number = {148},
 openalex = {W2518916044},
 pages = {1--5},
 pdf = {/papers/volume17/16-109/16-109.pdf},
 title = {Megaman: scalable manifold learning in python},
 url = {http://jmlr.org/papers/v17/16-109.html},
 volume = {17},
 year = {2016}
}

@article{JMLR:v17:rieck16a,
 abstract = {Comparing strings and assessing their similarity is a basic operation in many application domains of machine learning, such as in information retrieval, natural language processing and bioinformatics. The practitioner can choose from a large variety of available similarity measures for this task, each emphasizing different aspects of the string data. In this article, we present Harry, a small tool specifically designed for measuring the similarity of strings. Harry implements over 20 similarity measures, including common string distances and string kernels, such as the Levenshtein distance and the Subsequence kernel. The tool has been designed with efficiency in mind and allows for multi-threaded as well as distributed computing, enabling the analysis of large data sets of strings. Harry supports common data formats and thus can interface with analysis environments, such as Matlab, Pylab and Weka.},
 author = {Konrad Rieck and Christian Wressnegger},
 code = {https://github.com/rieck/harry},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W2298909997},
 pages = {1--5},
 pdf = {/papers/volume17/rieck16a/rieck16a.pdf},
 title = {Harry: a tool for measuring string similarity},
 url = {http://jmlr.org/papers/v17/rieck16a.html},
 volume = {17},
 year = {2016}
}

@article{JMLR:v18:15-441,
 abstract = {We introduce Refinery, an open source platform for exploring large text document collections with topic models. Refinery is a standalone web application driven by a graphical interface, so it is usable by those without machine learning or programming expertise. Users can interactively organize articles by topic and also refine this organization with phrase-level analysis. Under the hood, we train Bayesian nonparametric topic models that can adapt model complexity to the provided data with scalable learning algorithms. The project website http://daeilkim.github.io/refinery/ contains Python code and further documentation.},
 author = {Daeil Kim and Benjamin F. Swanson and Michael C. Hughes and Erik B. Sudderth},
 code = {https://github.com/daeilkim/refinery},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W2608020720},
 pages = {1--5},
 pdf = {/papers/volume18/15-441/15-441.pdf},
 title = {Refinery: an open source topic modeling web platform},
 url = {http://jmlr.org/papers/v18/15-441.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-492,
 abstract = {SnapVX is a high-performance solver for convex optimization problems defined on networks. For problems of this form, SnapVX provides a fast and scalable solution with guaranteed global convergence. It combines the capabilities of two open source software packages: Snap.py and CVXPY. Snap.py is a large scale graph processing library, and CVXPY provides a general modeling framework for small-scale subproblems. SnapVX offers a customizable yet easy-to-use Python interface with "out-of-the-box" functionality. Based on the Alternating Direction Method of Multipliers (ADMM), it is able to efficiently store, analyze, parallelize, and solve large optimization problems from a variety of different applications. Documentation, examples, and more can be found on the SnapVX website at http://snap.stanford.edu/snapvx.},
 author = {David Hallac and Christopher Wong and Steven Diamond and Abhijit Sharang and Rok Sosi{\v{c}} and Stephen Boyd and Jure Leskovec},
 code = {http://snap.stanford.edu/snapvx/#install},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2964051704},
 pages = {1--5},
 pdf = {/papers/volume18/15-492/15-492.pdf},
 title = {SnapVX: A Network-Based Convex Optimization Solver.},
 url = {http://jmlr.org/papers/v18/15-492.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-087,
 author = {Simone Filice and Giuseppe Castellucci and Giovanni Da San Martino and Aless and ro Moschitti and Danilo Croce and Roberto Basili},
 code = {https://github.com/SAG-KeLP},
 journal = {Journal of Machine Learning Research},
 number = {191},
 openalex = {W2808660397},
 pages = {1--5},
 pdf = {/papers/volume18/16-087/16-087.pdf},
 title = {KELP: a Kernel-based Learning Platform},
 url = {http://jmlr.org/papers/v18/16-087.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-131,
 abstract = {Java Statistical Analysis Tool (JSAT) is a Machine Learning library written in pure Java. It works to fill a void in the Java ecosystem for a general purpose library that is relatively high performance and flexible, which is not adequately fulfilled by Weka (Hall et al., 2009) and Java-ML (Abeel et al., 2009). Almost all of the algorithms are independently implemented using an Object-Oriented framework. JSAT is made available under the GNU GPL license here: https://github.com/EdwardRaff/JSAT.},
 author = {Edward Raff},
 code = {https://github.com/EdwardRaff/JSAT},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2608296545},
 pages = {1--5},
 pdf = {/papers/volume18/16-131/16-131.pdf},
 title = {JSAT: Java statistical analysis tool, a library for machine learning},
 url = {http://jmlr.org/papers/v18/16-131.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-261,
 abstract = {Many different machine learning algorithms exist; taking into account each algorithm’s hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider feature selection techniques and all machine learning approaches implemented in WEKA’s standard distribution, spanning 2 ensemble methods, 10 meta-methods, 28 base learners, and hyperparameter settings for each learner. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
 author = {Lars Kotthoff and Chris Thornton and Holger H. Hoos and Frank Hutter and Kevin Leyton-Brown},
 code = {https://github.com/automl/autoweka},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W2608595939},
 pages = {1--5},
 pdf = {/papers/volume18/16-261/16-261.pdf},
 title = {Auto-WEKA: Automatic Model Selection and Hyperparameter Optimization in WEKA},
 url = {http://jmlr.org/papers/v18/16-261.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-300,
 abstract = {POMDPs.jl is an open-source framework for solving Markov decision processes (MDPs) and partially observable MDPs (POMDPs). POMDPs.jl allows users to specify sequential decision making problems with minimal effort without sacrificing the expressive nature of POMDPs, making this framework viable for both educational and research purposes. It is written in the Julia language to allow flexible prototyping and large-scale computation that leverages the high-performance nature of the language. The associated JuliaPOMDP community also provides a number of state-of-the-art MDP and POMDP solvers and a rich library of support tools to help with implementing new solvers and evaluating the solution results. The most recent version of POMDPs.jl, the related packages, and documentation can be found at https://github.com/JuliaPOMDP/POMDPs.jl.},
 author = {Maxim Egorov and Zachary N. Sunberg and Edward Balaban and Tim A. Wheeler and Jayesh K. Gupta and Mykel J. Kochenderfer},
 code = {https://github.com/JuliaPOMDP/POMDPs.jl},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2607868186},
 pages = {1--5},
 pdf = {/papers/volume18/16-300/16-300.pdf},
 title = {POMDPs.jl: a framework for sequential decision making under uncertainty},
 url = {http://jmlr.org/papers/v18/16-300.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-365,
 abstract = {Imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub: https://github.com/scikit-learn-contrib/imbalanced-learn.},
 author = {Guillaume Lema{{\^i}}tre and Fernando Nogueira and Christos K. Aridas},
 code = {https://github.com/scikit-learn-contrib/imbalanced-learn},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W4293713156},
 pages = {1--5},
 pdf = {/papers/volume18/16-365/16-365.pdf},
 title = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},
 url = {http://jmlr.org/papers/v18/16-365.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-509,
 abstract = {The R package GFA provides a full pipeline for factor analysis of multiple data sources that are represented as matrices with co-occurring samples. It allows learning dependencies between subsets of the data sources, decomposed into latent factors. The package also implements sparse priors for the factorization, providing interpretable biclusters of the multi-source data},
 author = {Eemeli Lepp{{\"a}}aho and Muhammad Ammad-ud-din and Samuel Kaski},
 code = {https://cran.r-project.org/web/packages/GFA/index.html},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W2952874200},
 pages = {1--5},
 pdf = {/papers/volume18/16-509/16-509.pdf},
 title = {GFA: Exploratory Analysis of Multiple Data Sources with Group Factor Analysis},
 url = {http://jmlr.org/papers/v18/16-509.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-537,
 abstract = {GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware.},
 author = {Alexander G. de G. Matthews and Mark van der Wilk and Tom Nickson and Keisuke Fujii and Alexis Boukouvalas and Pablo Le{\'o}n-Villagr{\'a} and Zoubin Ghahramani and James Hensman},
 code = {https://github.com/GPflow/GPflow},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W4301091646},
 pages = {1--6},
 pdf = {/papers/volume18/16-537/16-537.pdf},
 title = {GPflow: A Gaussian process library using TensorFlow},
 url = {http://jmlr.org/papers/v18/16-537.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-113,
 abstract = {We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e.g., acoustic or visual low-level descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed subbags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool are exemplified in two sample scenarios: time-continuous speech-based emotion recognition and sentiment analysis in tweets where improved results over other feature representation forms were observed.},
 author = {Maximilian Schmitt and Bj{{\"o}}rn Schuller},
 code = {https://github.com/openXBOW/openXBOW},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W4293571337},
 pages = {1--5},
 pdf = {/papers/volume18/17-113/17-113.pdf},
 title = {openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit},
 url = {http://jmlr.org/papers/v18/17-113.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-156,
 author = {Frans A. Oliehoek and Matthijs T. J. Spaan and Bas Terwijn and Philipp Robbel and Jo\~{a}o V. Messias},
 code = {https://github.com/MADPToolbox/MADP},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W2903219079},
 pages = {1--5},
 pdf = {/papers/volume18/17-156/17-156.pdf},
 title = {The MADP Toolbox: An Open-Source Library for Planning and Learning in (Multi-)Agent Systems.},
 url = {http://jmlr.org/papers/v18/17-156.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-228,
 abstract = {We introduce \texttt{pycobra}, a Python library devoted to ensemble learning (regression and classification) and visualisation. Its main assets are the implementation of several ensemble learning algorithms, a flexible and generic interface to compare and blend any existing machine learning algorithm available in Python libraries (as long as a \texttt{predict} method is given), and visualisation tools such as Voronoi tessellations. \texttt{pycobra} is fully \texttt{scikit-learn} compatible and is released under the MIT open-source license. \texttt{pycobra} can be downloaded from the Python Package Index (PyPi) and Machine Learning Open Source Software (MLOSS). The current version (along with Jupyter notebooks, extensive documentation, and continuous integration tests) is available at \href{https://github.com/bhargavvader/pycobra}{https://github.com/bhargavvader/pycobra} and official documentation website is \href{https://modal.lille.inria.fr/pycobra}{https://modal.lille.inria.fr/pycobra}.},
 author = {Benjamin Guedj and Bhargav Srinivasa Desikan},
 code = {https://github.com/bhargavvader/pycobra},
 journal = {Journal of Machine Learning Research},
 number = {190},
 openalex = {W2732215177},
 pages = {1--5},
 pdf = {/papers/volume18/17-228/17-228.pdf},
 title = {Pycobra: A Python Toolbox for Ensemble Learning and Visualisation},
 url = {http://jmlr.org/papers/v18/17-228.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-381,
 author = {Emmanuel Bacry and Martin Bompaire and Philip Deegan and St{{\'e}}phane Ga{{\"i}}ffas and S{{\o}}ren V. Poulsen},
 code = {https://github.com/X-DataInitiative/tick},
 journal = {Journal of Machine Learning Research},
 number = {214},
 openalex = {W2808127013},
 pages = {1--5},
 pdf = {/papers/volume18/17-381/17-381.pdf},
 title = {tick: a Python Library for Statistical Learning, with an emphasis on Hawkes Processes and Time-Dependent Models},
 url = {http://jmlr.org/papers/v18/17-381.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-434,
 author = {Andrew C. Heusser and Kirsten Ziman and Lucy L. W. Owen and Jeremy R. Manning},
 code = {https://github.com/ContextLab/hypertools},
 journal = {Journal of Machine Learning Research},
 number = {152},
 openalex = {W2801900386},
 pages = {1--6},
 pdf = {/papers/volume18/17-434/17-434.pdf},
 title = {HyperTools: a Python Toolbox for Gaining Geometric Insights into High-Dimensional Data},
 url = {http://jmlr.org/papers/v18/17-434.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-632,
 author = {Hiroyuki Kasai},
 code = {https://github.com/hiroyuki-kasai/SGDLibrary},
 journal = {Journal of Machine Learning Research},
 number = {215},
 openalex = {W2808678810},
 pages = {1--5},
 pdf = {/papers/volume18/17-632/17-632.pdf},
 title = {SGDLibrary: A MATLAB library for stochastic optimization algorithms},
 url = {http://jmlr.org/papers/v18/17-632.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v19:17-374,
 abstract = {Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features.},
 author = {Jarno Lintusaari and Henri Vuollekoski and Antti Kangasr{{\"a}}{{\"a}}si{{\"o}} and Kusti SkytÃ©n and Marko J{{\"a}}rvenp{{\"a}}{{\"a}} and Pekka Marttinen and Michael U. Gutmann and Aki Vehtari and Jukka Corander and Samuel Kaski},
 code = {https://github.com/elfi-dev/elfi},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2953009816},
 pages = {1--7},
 pdf = {/papers/volume19/17-374/17-374.pdf},
 title = {ELFI: Engine for Likelihood-Free Inference},
 url = {http://jmlr.org/papers/v19/17-374.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-740,
 author = {Zeyi Wen and Jiashuai Shi and Qinbin Li and Bingsheng He and Jian Chen},
 code = {https://github.com/Xtra-Computing/thundersvm},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2782213427},
 pages = {1--5},
 pdf = {/papers/volume19/17-740/17-740.pdf},
 title = {ThunderSVM: A Fast SVM Library on GPUs and CPUs},
 url = {http://jmlr.org/papers/v19/17-740.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-100,
 author = {Tom Ronan and Shawn Anastasio and Zhijie Qi and Pedro Henrique S. Vieira Tavares and Roman Sloutsky and Kristen M. Naegle},
 code = {https://github.com/NaegleLab/OpenEnsembles},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2892560236},
 pages = {1--6},
 pdf = {/papers/volume19/18-100/18-100.pdf},
 title = {OpenEnsembles: A Python Resource for Ensemble Clustering},
 url = {http://jmlr.org/papers/v19/18-100.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-160,
 abstract = {Seglearn is an open-source python package for machine learning time series or sequences using a sliding window segmentation approach. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. This package is compatible with scikit-learn and is listed under scikit-learn Related Projects. The package depends on numpy, scipy, and scikit-learn. Seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage.},
 author = {David M. Burns and Cari M. Whyne},
 code = {https://github.com/dmbee/seglearn},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W2962970382},
 pages = {1--7},
 pdf = {/papers/volume19/18-160/18-160.pdf},
 title = {Seglearn: A Python Package for Learning Sequences and Time Series},
 url = {http://jmlr.org/papers/v19/18-160.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-251,
 abstract = {Scikit-multiflow is a multi-output/multi-label and stream data mining framework for the Python programming language. Conceived to serve as a platform to encourage democratization of stream learning research, it provides multiple state of the art methods for stream learning, stream generators and evaluators. scikit-multiflow builds upon popular open source frameworks including scikit-learn, MOA and MEKA. Development follows the FOSS principles and quality is enforced by complying with PEP8 guidelines and using continuous integration and automatic testing. The source code is publicly available at https://github.com/scikit-multiflow/scikit-multiflow.},
 author = {Jacob Montiel and Jesse Read and Albert Bifet and Talel Abdessalem},
 code = {https://github.com/scikit-multiflow/scikit-multiflow},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W2847284300},
 pages = {1--5},
 pdf = {/papers/volume19/18-251/18-251.pdf},
 title = {Scikit-Multiflow: A Multi-output Streaming Framework},
 url = {http://jmlr.org/papers/v19/18-251.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v20:17-100,
 author = {Piotr Szyma{{\'n}}ski and Tomasz Kajdanowicz},
 code = {http://scikit.ml/},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2912218607},
 pages = {1--22},
 pdf = {/papers/volume20/17-100/17-100.pdf},
 title = {scikit-multilearn: A Python library for Multi-Label Classification},
 url = {http://jmlr.org/papers/v20/17-100.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-722,
 abstract = {We describe a new library named picasso, which implements a unified framework of pathwise coordinate optimization for a variety of sparse learning problems (e.g., sparse linear regression, sparse logistic regression, sparse Poisson regression and scaled sparse linear regression) combined with efficient active set selection strategies. Besides, the library allows users to choose different sparsity-inducing regularizers, including the convex $\ell_1$, nonconvex MCP and SCAD regularizers. The library is coded in C++ and has user-friendly R and Python wrappers. Numerical experiments demonstrate that picasso can scale up to large problems efficiently.},
 author = {Jason Ge and Xingguo Li and Haoming Jiang and Han Liu and Tong Zhang and Mengdi Wang and Tuo Zhao},
 code = {https://github.com/jasonge27/picasso},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W2935670023},
 pages = {1--5},
 pdf = {/papers/volume20/17-722/17-722.pdf},
 title = {Picasso: A sparse learning library for high dimensional data analysis in R and python},
 url = {http://jmlr.org/papers/v20/17-722.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:17-743,
 author = {Enrique G. Rodrigo and Juan A. Aledo and Jos{{\'e}} A. G{{\'a}}mez},
 code = {https://github.com/enriquegrodrigo/spark-crowd},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2913859482},
 pages = {1--5},
 pdf = {/papers/volume20/17-743/17-743.pdf},
 title = {spark-crowd: A Spark Package for Learning from Crowdsourced Big Data},
 url = {http://jmlr.org/papers/v20/17-743.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-277,
 abstract = {Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly},
 author = {Jean Kossaifi and Yannis Panagakis and Anima Anandkumar and Maja Pantic},
 code = {https://github.com/tensorly/tensorly},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2544822491},
 pages = {1--6},
 pdf = {/papers/volume20/18-277/18-277.pdf},
 title = {TensorLy: tensor learning in python},
 url = {http://jmlr.org/papers/v20/18-277.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-349,
 abstract = {Ordinal regression, also named ordinal classification, studies classification problems where there exist a natural order between class labels. This structured order of the labels is crucial in all steps of the learning process in order to take full advantage of the data. ORCA (Ordinal Regression and Classification Algorithms) is a Matlab/Octave framework that implements and integrates different ordinal classification algorithms and specifically designed performance metrics. The framework simplifies the task of experimental comparison to a great extent, allowing the user to: (i) describe experiments by simple configuration files; (ii) automatically run different data partitions; (iii) parallelize the executions; (iv) generate a variety of performance reports and (v) include new algorithms by using its intuitive interface. Source code, binaries, documentation, descriptions and links to data sets and tutorials (including examples of educational purpose) are available at https://github.com/ayrna/orca.},
 author = {Javier S{{\'a}}nchez-Monedero and Pedro A. Guti{{\'e}}rrez and Mar{{\'i}}a P{{\'e}}rez-Ortiz},
 code = {https://github.com/ayrna/orca},
 journal = {Journal of Machine Learning Research},
 number = {125},
 openalex = {W2974818429},
 pages = {1--5},
 pdf = {/papers/volume20/18-349/18-349.pdf},
 title = {ORCA: A Matlab/Octave Toolbox for Ordinal Regression},
 url = {http://jmlr.org/papers/v20/18-349.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-403,
 abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large datasets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
 author = {Eli Bingham and Jonathan P. Chen and Martin Jankowiak and Fritz Obermeyer and Neeraj Pradhan and Theofanis Karaletsos and Rohit Singh and Paul Szerlip and Paul Horsfall and Noah D. Goodman},
 code = {https://github.com/pyro-ppl/pyro},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2964321317},
 pages = {1--6},
 pdf = {/papers/volume20/18-403/18-403.pdf},
 title = {Pyro: Deep Universal Probabilistic Programming},
 url = {http://jmlr.org/papers/v20/18-403.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-450,
 author = {Felipe Bravo-Marquez and Eibe Frank and Bernhard Pfahringer and Saif M. Mohammad},
 code = {https://github.com/felipebravom/AffectiveTweets},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W2950151462},
 pages = {1--6},
 pdf = {/papers/volume20/18-450/18-450.pdf},
 title = {AffectiveTweets: a Weka package for analyzing affect in tweets},
 url = {http://jmlr.org/papers/v20/18-450.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-540,
 abstract = {In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.},
 author = {Maximilian Alber and Sebastian Lapuschkin and Philipp Seegerer and Miriam H{{\"a}}gele and Kristof T. Sch{{\"u}}tt and Gr{{\'e}}goire Montavon and Wojciech Samek and Klaus-Robert M{{\"u}}ller and Sven D{{\"a}}hne and Pieter-Jan Kindermans},
 code = {https://github.com/albermax/innvestigate},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W4289699961},
 pages = {1--8},
 pdf = {/papers/volume20/18-540/18-540.pdf},
 title = {iNNvestigate neural networks!},
 url = {http://jmlr.org/papers/v20/18-540.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:18-859,
 abstract = {SMART is an open source web application designed to help data scientists and research teams efficiently build labeled training data sets for supervised machine learning tasks. SMART provides users with an intuitive interface for creating labeled data sets, supports active learning to help reduce the required amount of labeled data, and incorporates inter-rater reliability statistics to provide insight into label quality. SMART is designed to be platform agnostic and easily deployable to meet the needs of as many different research teams as possible. The project website contains links to the code repository and extensive user documentation.},
 author = {Rob Chew and Michael Wenger and Caroline Kery and Jason Nance and Keith Richards and Emily Hadley and Peter Baumgartner},
 code = {https://rtiinternational.github.io/SMART/},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W4382132564},
 pages = {1--5},
 pdf = {/papers/volume20/18-859/18-859.pdf},
 title = {SMART: An Open Source Data Labeling Platform for Supervised Learning},
 url = {http://jmlr.org/papers/v20/18-859.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v20:19-011,
 abstract = {PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI) or https://github.com/yzhao062/pyod.},
 author = {Yue Zhao and Zain Nasrullah and Zheng Li},
 code = {https://github.com/yzhao062/pyod},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W2963445059},
 pages = {1--7},
 pdf = {/papers/volume20/19-011/19-011.pdf},
 title = {PyOD: A Python Toolbox for Scalable Outlier Detection},
 url = {http://jmlr.org/papers/v20/19-011.html},
 volume = {20},
 year = {2019}
}

@article{JMLR:v21:18-008,
 abstract = {Tensor Train decomposition is used across many branches of machine learning. We present T3F -- a library for Tensor Train decomposition based on TensorFlow. T3F supports GPU execution, batch processing, automatic differentiation, and versatile functionality for the Riemannian optimization framework, which takes into account the underlying manifold structure to construct efficient optimization methods. The library makes it easier to implement machine learning papers that rely on the Tensor Train decomposition. T3F includes documentation, examples and 94% test coverage.},
 author = {Alexander Novikov and Pavel Izmailov and Valentin Khrulkov and Michael Figurnov and Ivan Oseledets},
 code = {https://github.com/Bihaqo/t3f},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W3013294074},
 pages = {1--7},
 pdf = {/papers/volume21/18-008/18-008.pdf},
 title = {Tensor Train decomposition on TensorFlow (T3F)},
 url = {http://jmlr.org/papers/v21/18-008.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-370,
 abstract = {The problem of accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. Graph kernels have recently emerged as a promising approach to this problem. There are now many kernels, each focusing on different structural aspects of graphs. Here, we present GraKeL, a library that unifies several graph kernels into a common framework. The library is written in Python and adheres to the scikit-learn interface. It is simple to use and can be naturally combined with scikit-learn's modules to build a complete machine learning pipeline for tasks such as graph classification and clustering. The code is BSD licensed and is available at: https://github.com/ysig/ GraKeL.},
 author = {Giannis Siglidis and Giannis Nikolentzos and Stratis Limnios and Christos Giatsidis and Konstantinos Skianis and Michalis Vazirgiannis},
 code = {https://github.com/ysig/GraKeL},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W3028283557},
 pages = {1--5},
 pdf = {/papers/volume21/18-370/18-370.pdf},
 title = {GraKeL: A Graph Kernel Library in Python},
 url = {http://jmlr.org/papers/v21/18-370.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-402,
 author = {Eugenio Bargiacchi and Diederik M. Roijers and Ann NowÃ©},
 code = {https://github.com/Svalorzen/AI-Toolbox},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W3045084777},
 pages = {1--12},
 pdf = {/papers/volume21/18-402/18-402.pdf},
 title = {AI-toolbox: A C++ library for reinforcement learning and planning (with Python Bindings)},
 url = {http://jmlr.org/papers/v21/18-402.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-800,
 abstract = {Apache Mahout is a library for scalable machine learning (ML) on distributed dataflow systems, offering various implementations of classification, clustering, dimensionality reduction and recommendation algorithms. Mahout was a pioneer in large-scale machine learning in 2008, when it started and targeted MapReduce, which was the predominant abstraction for scalable computing in industry at that time. Mahout has been widely used by leading web companies and is part of several commercial cloud offerings. In recent years, Mahout migrated to a general framework enabling a mix of dataflow programming and linear algebraic computations on backends such as Apache Spark and Apache Flink. This design allows users to execute data preprocessing and model training in a single, unified dataflow system, instead of requiring a complex integration of several specialized systems. Mahout is maintained as a community-driven open source project at the Apache Software Foundation, and is available under https://mahout.apache.org.},
 author = {Robin Anil and Gokhan Capan and Isabel Drost-Fromm and Ted Dunning and Ellen Friedman and Trevor Grant and Shannon Quinn and Paritosh Ranjan and Sebastian Schelter and ÃzgÃ¼r YÄ±lmazel},
 code = {https://mahout.apache.org},
 journal = {Journal of Machine Learning Research},
 number = {127},
 openalex = {W3047908100},
 pages = {1--6},
 pdf = {/papers/volume21/18-800/18-800.pdf},
 title = {Apache Mahout: Machine Learning on Distributed Dataflow Systems},
 url = {http://jmlr.org/papers/v21/18-800.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-027,
 abstract = {We introduce Geomstats, an open-source Python toolbox for computations and statistics on nonlinear manifolds, such as hyperbolic spaces, spaces of symmetric positive definite matrices, Lie groups of transformations, and many more. We provide object-oriented and extensively unit-tested implementations. Among others, manifolds come equipped with families of Riemannian metrics, with associated exponential and logarithmic maps, geodesics and parallel transport. Statistics and learning algorithms provide methods for estimation, clustering and dimension reduction on manifolds. All associated operations are vectorized for batch computation and provide support for different execution backends, namely NumPy, PyTorch and TensorFlow, enabling GPU acceleration. This paper presents the package, compares it with related libraries and provides relevant code examples. We show that Geomstats provides reliable building blocks to foster research in differential geometry and statistics, and to democratize the use of Riemannian geometry in machine learning applications. The source code is freely available under the MIT license at \url{geomstats.ai}.},
 author = {Nina Miolane and Nicolas Guigui and Alice Le Brigant and Johan Mathe and Benjamin Hou and Yann Thanwerdas and Stefan Heyder and Olivier Peltre and Niklas Koep and Hadi Zaatiti and Hatem Hajri and Yann Cabanes and Thomas Gerald and Paul Chauchat and Christian Shewmake and Daniel Brooks and Bernhard Kainz and Claire Donnat and Susan Holmes and Xavier Pennec},
 code = {https://github.com/geomstats/geomstats},
 journal = {Journal of Machine Learning Research},
 number = {223},
 openalex = {W2804935370},
 pages = {1--9},
 pdf = {/papers/volume21/19-027/19-027.pdf},
 title = {Geomstats: A Python Package for Riemannian Geometry in Machine Learning},
 url = {http://jmlr.org/papers/v21/19-027.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-047,
 abstract = {The wavelet scattering transform is an invariant signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks. All transforms may be executed on a GPU (in addition to CPU), offering a considerable speed up over CPU implementations. The package also has a small memory footprint, resulting inefficient memory usage. The source code, documentation, and examples are available undera BSD license at this https URL},
 author = {Mathieu Andreux and TomÃ¡s Angles and Georgios Exarchakis and Roberto Leonarduzzi and Gaspar Rochette and Louis Thiry and John Zarka and StÃ©phane Mallat and Joakim AndÃ©n and Eugene Belilovsky and Joan Bruna and Vincent Lostanlen and Muawiz Chaudhary and Matthew J. Hirn and Edouard Oyallon and Sixin Zhang and Carmine Cella and Michael Eickenberg},
 code = {https://github.com/kymatio/kymatio},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2908414239},
 pages = {1--6},
 pdf = {/papers/volume21/19-047/19-047.pdf},
 title = {Kymatio: Scattering Transforms in Python},
 url = {http://jmlr.org/papers/v21/19-047.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-095,
 author = {Zeyi Wen and Hanfeng Liu and Jiashuai Shi and Qinbin Li and Bingsheng He and Jian Chen},
 code = {https://github.com/xtra-computing/thundergbm},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W2912265134},
 pages = {1--5},
 pdf = {/papers/volume21/19-095/19-095.pdf},
 title = {ThunderGBM: Fast GBDTs and Random Forests on GPUs},
 url = {http://jmlr.org/papers/v21/19-095.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-1035,
 author = {Vijay Arya and Rachel K. E. Bellamy and Pin-Yu Chen and Amit Dhurandhar and Michael Hind and Samuel C. Hoffman and Stephanie Houde and Q. Vera Liao and Ronny Luss and Aleksandra MojsiloviÄ and Sami Mourad and Pablo Pedemonte and Ramya Raghavendra and John T. Richards and Prasanna Sattigeri and Karthikeyan Shanmugam and Moninder Singh and Kush R. Varshney and Dennis Wei and Yunfeng Zhang},
 code = {http://aix360.mybluemix.net},
 journal = {Journal of Machine Learning Research},
 number = {130},
 openalex = {W3048102985},
 pages = {1--6},
 pdf = {/papers/volume21/19-1035/19-1035.pdf},
 title = {AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models},
 url = {http://jmlr.org/papers/v21/19-1035.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-348,
 author = {Edesio AlcobaÃ§a and Felipe Siqueira and Adriano Rivolli and LuÃ­s P. F. Garcia and Jefferson T. Oliva and AndrÃ© C. P. L. F. de Carvalho},
 code = {https://github.com/ealcobaca/pymfe},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W3044965819},
 pages = {1--5},
 pdf = {/papers/volume21/19-348/19-348.pdf},
 title = {MFE: Towards reproducible meta-feature extraction},
 url = {http://jmlr.org/papers/v21/19-348.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-467,
 abstract = {We present apricot, an open source Python package for selecting representative subsets from large data sets using submodular optimization. The package implements an efficient greedy selection algorithm that offers strong theoretical guarantees on the quality of the selected set. Two submodular set functions are implemented in apricot: facility location, which is broadly applicable but requires memory quadratic in the number of examples in the data set, and a feature-based function that is less broadly applicable but can scale to millions of examples. Apricot is extremely efficient, using both algorithmic speedups such as the lazy greedy algorithm and code optimizers such as numba. We demonstrate the use of subset selection by training machine learning models to comparable accuracy using either the full data set or a representative subset thereof. This paper presents an explanation of submodular selection, an overview of the features in apricot, and an application to several data sets. The code and tutorial Jupyter notebooks are available at https://github.com/jmschrei/apricot},
 author = {Jacob Schreiber and Jeffrey Bilmes and William Stafford Noble},
 code = {https://github.com/jmschrei/apricot},
 journal = {Journal of Machine Learning Research},
 number = {161},
 openalex = {W2948852427},
 pages = {1--6},
 pdf = {/papers/volume21/19-467/19-467.pdf},
 title = {apricot: Submodular selection for data summarization in Python},
 url = {http://jmlr.org/papers/v21/19-467.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-470,
 author = {Davide Bacciu and Federico Errica and Alessio Micheli},
 code = {https://github.com/diningphil/CGMM},
 journal = {Journal of Machine Learning Research},
 number = {134},
 openalex = {W3047991353},
 pages = {1--39},
 pdf = {/papers/volume21/19-470/19-470.pdf},
 title = {Probabilistic Learning on Graphs via Contextual Architectures},
 url = {http://jmlr.org/papers/v21/19-470.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-678,
 abstract = {metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT licence.},
 author = {William de Vazelhes and CJ Carey and Yuan Tang and Nathalie Vauquier and AurÃ©lien Bellet},
 code = {https://github.com/scikit-learn-contrib/metric-learn},
 journal = {Journal of Machine Learning Research},
 number = {138},
 openalex = {W3047712904},
 pages = {1--6},
 pdf = {/papers/volume21/19-678/19-678.pdf},
 title = {metric-learn: Metric Learning Algorithms in Python},
 url = {http://jmlr.org/papers/v21/19-678.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-763,
 abstract = {pyts is an open-source Python package for time series classification. This versatile toolbox provides implementations of many algorithms published in the literature, preprocessing functionalities, and data set loading utilities. pyts relies on the standard scientific Python packages numpy, scipy, scikit-learn, joblib, and numba, and is distributed under the BSD-3-Clause license. Documentation contains installation instructions, a detailed user guide, a full API description, and concrete self-contained examples. Source code and documentation can be downloaded from https://github.com/johannfaouzi/pyts.},
 author = {Johann Faouzi and Hicham Janati},
 code = {https://github.com/johannfaouzi/pyts},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W3014052783},
 pages = {1--6},
 pdf = {/papers/volume21/19-763/19-763.pdf},
 title = {pyts: A Python Package for Time Series Classification},
 url = {http://jmlr.org/papers/v21/19-763.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-773,
 abstract = {In this report we describe a tool for comparing the performance of graphical causal structure learning algorithms implemented in the TETRAD freeware suite of causal analysis methods. Currently the tool is available as package in the TETRAD source code (written in Java). Simulations can be done varying the number of runs, sample sizes, and data modalities. Performance on this simulated data can then be compared for a number of algorithms, with parameters varied and with performance statistics as selected, producing a publishable report. The package presented here may also be used to compare structure learning methods across platforms and programming languages, i.e., to compare algorithms implemented in TETRAD with those implemented in MATLAB, Python, or R.},
 author = {Joseph D. Ramsey and Daniel Malinsky and Kevin V. Bui},
 code = {https://github.com/bd2kccd/causal-compare},
 journal = {Journal of Machine Learning Research},
 number = {238},
 openalex = {W3116013208},
 pages = {1--6},
 pdf = {/papers/volume21/19-773/19-773.pdf},
 title = {algcomparison: Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD},
 url = {http://jmlr.org/papers/v21/19-773.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-805,
 author = {Aghiles Salah and Quoc-Tuan Truong and Hady W. Lauw},
 code = {https://github.com/PreferredAI/cornac},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W3040516071},
 pages = {1--5},
 pdf = {/papers/volume21/19-805/19-805.pdf},
 title = {Cornac: A Comparative Framework for Multimodal Recommender Systems},
 url = {http://jmlr.org/papers/v21/19-805.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-820,
 author = {Alexander Alexandrov and Konstantinos Benidis and Michael Bohlke-Schneider and Valentin Flunkert and Jan Gasthaus and Tim Januschowski and Danielle C. Maddix and Syama Rangapuram and David Salinas and Jasper Schulz and Lorenzo Stella and Ali Caner TÃ¼rkmen and Yuyang Wang},
 code = {https://github.com/awslabs/gluon-ts},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W3042623101},
 pages = {1--6},
 pdf = {/papers/volume21/19-820/19-820.pdf},
 title = {GluonTS: Probabilistic and Neural Time Series Modeling in Python},
 url = {http://jmlr.org/papers/v21/19-820.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-864,
 author = {Juan Luis SuÃ¡rez and Salvador GarcÃ­a and Francisco Herrera},
 code = {https://github.com/jlsuarezdiaz/pyDML},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W3041284003},
 pages = {1--7},
 pdf = {/papers/volume21/19-864/19-864.pdf},
 title = {pyDML: A Python Library for Distance Metric Learning},
 url = {http://jmlr.org/papers/v21/19-864.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-091,
 abstract = {tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn.},
 author = {Romain Tavenard and Johann Faouzi and Gilles Vandewiele and Felix Divo and Guillaume Androz and Chester Holtz and Marie Payne and Roman Yurchak and Marc RuÃwurm and Kushal Kolar and Eli Woods},
 code = {https://github.com/tslearn-team/tslearn},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W3039352388},
 pages = {1--6},
 pdf = {/papers/volume21/20-091/20-091.pdf},
 title = {Tslearn, A Machine Learning Toolkit for Time Series Data},
 url = {http://jmlr.org/papers/v21/20-091.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-412,
 abstract = {Scikit-network is a Python package inspired by scikit-learn for the analysis of large graphs. Graphs are represented by their adjacency matrix in the sparse CSR format of SciPy. The package provides state-of-the-art algorithms for ranking, clustering, classifying, embedding and visualizing the nodes of a graph. High performance is achieved through a mix of fast matrix-vector products (using SciPy), compiled code (using Cython) and parallel processing. The package is distributed under the BSD license, with dependencies limited to NumPy and SciPy. It is compatible with Python 3.6 and newer. Source code, documentation and installation instructions are available online.},
 author = {Thomas Bonald and Nathan de Lara and Quentin Lutz and Bertrand Charpentier},
 code = {https://github.com/sknetwork-team/scikit-network},
 journal = {Journal of Machine Learning Research},
 number = {185},
 openalex = {W3086706245},
 pages = {1--6},
 pdf = {/papers/volume21/20-412/20-412.pdf},
 title = {Scikit-network: Graph Analysis in Python},
 url = {http://jmlr.org/papers/v21/20-412.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-729,
 author = {Sebastian PÃ¶lsterl},
 code = {https://github.com/sebp/scikit-survival},
 journal = {Journal of Machine Learning Research},
 number = {212},
 openalex = {W3097349486},
 pages = {1--6},
 pdf = {/papers/volume21/20-729/20-729.pdf},
 title = {scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn},
 url = {http://jmlr.org/papers/v21/20-729.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v22:18-056,
 abstract = {MushroomRL is an open-source Python library developed to simplify the process of implementing and running Reinforcement Learning (RL) experiments. Compared to other available libraries, MushroomRL has been created with the purpose of providing a comprehensive and flexible framework to minimize the effort in implementing and testing novel RL methodologies. Indeed, the architecture of MushroomRL is built in such a way that every component of an RL problem is already provided, and most of the time users can only focus on the implementation of their own algorithms and experiments. The result is a library from which RL researchers can significantly benefit in the critical phase of the empirical analysis of their works. MushroomRL stable code, tutorials and documentation can be found at https://github.com/MushroomRL/mushroom-rl.},
 author = {Carlo D'Eramo and Davide Tateo and Andrea Bonarini and Marcello Restelli and Jan Peters},
 code = {https://github.com/MushroomRL/mushroom-rl},
 journal = {Journal of Machine Learning Research},
 number = {131},
 openalex = {W2998544442},
 pages = {1--5},
 pdf = {/papers/volume22/18-056/18-056.pdf},
 title = {MushroomRL: Simplifying Reinforcement Learning Research},
 url = {http://jmlr.org/papers/v22/18-056.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-433,
 abstract = {Pykg2vec is an open-source Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 16 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of TensorFlow and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, mean rank evaluation, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at https://github.com/Sujit-O/pykg2vec.},
 author = {Shih-Yuan Yu and Sujit Rokka Chhetri and Arquimedes Canedo and Palash Goyal and Mohammad Abdullah Al Faruque},
 code = {https://github.com/Sujit-O/pykg2vec},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W4288336232},
 pages = {1--6},
 pdf = {/papers/volume22/19-433/19-433.pdf},
 title = {Pykg2vec: A Python Library for Knowledge Graph Embedding},
 url = {http://jmlr.org/papers/v22/19-433.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:19-920,
 abstract = {OpenML is an online platform for open science collaboration in machine learning, used to share datasets and results of machine learning experiments. In this paper we introduce OpenML-Python, a client API for Python, opening up the OpenML platform for a wide range of Python-based tools. It provides easy access to all datasets, tasks and experiments on OpenML from within Python. It also provides functionality to conduct machine learning experiments, upload the results to OpenML, and reproduce results which are stored on OpenML. Furthermore, it comes with a scikit-learn plugin and a plugin mechanism to easily integrate other machine learning libraries written in Python into the OpenML ecosystem. Source code and documentation is available at this https URL.},
 author = {Matthias Feurer and Jan N. van Rijn and Arlind Kadra and Pieter Gijsbers and Neeratyoy Mallik and Sahithya Ravi and Andreas MÃ¼ller and Joaquin Vanschoren and Frank Hutter},
 code = {https://github.com/openml/openml-python/},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2983758708},
 pages = {1--5},
 pdf = {/papers/volume22/19-920/19-920.pdf},
 title = {OpenML-Python: an extensible Python API for OpenML},
 url = {http://jmlr.org/papers/v22/19-920.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1364,
 author = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
 code = {https://github.com/DLR-RM/stable-baselines3},
 journal = {Journal of Machine Learning Research},
 number = {268},
 openalex = {W3216772467},
 pages = {1--8},
 pdf = {/papers/volume22/20-1364/20-1364.pdf},
 title = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
 url = {http://jmlr.org/papers/v22/20-1364.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1370,
 abstract = {As data are generated more and more from multiple disparate sources, multiview data sets, where each sample has features in distinct views, have ballooned in recent years. However, no comprehensive package exists that enables non-specialists to use these methods easily. mvlearn is a Python library which implements the leading multiview machine learning methods. Its simple API closely follows that of scikit-learn for increased ease-of-use. The package can be installed from Python Package Index (PyPI) and the conda package manager and is released under the MIT open-source license. The documentation, detailed examples, and all releases are available at https://mvlearn.github.io/.},
 author = {Ronan Perry and Gavin Mischler and Richard Guo and Theodore Lee and Alexander Chang and Arman Koul and Cameron Franz and Hugo Richard and Iain Carmichael and Pierre Ablin and Alexandre Gramfort and Joshua T. Vogelstein},
 code = {https://github.com/mvlearn/mvlearn},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W4287775886},
 pages = {1--7},
 pdf = {/papers/volume22/20-1370/20-1370.pdf},
 title = {mvlearn: Multiview Machine Learning in Python},
 url = {http://jmlr.org/papers/v22/20-1370.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1380,
 abstract = {River is a machine learning library for dynamic data streams and continual learning. It provides multiple state-of-the-art learning methods, data generators/transformers, performance metrics and evaluators for different stream learning problems. It is the result from the merger of the two most popular packages for stream learning in Python: Creme and scikit-multiflow. River introduces a revamped architecture based on the lessons learnt from the seminal packages. River's ambition is to be the go-to library for doing machine learning on streaming data. Additionally, this open source package brings under the same umbrella a large community of practitioners and researchers. The source code is available at https://github.com/online-ml/river.},
 author = {Jacob Montiel and Max Halford and Saulo Martiello Mastelini and Geoffrey Bolmier and Raphael Sourty and Robin Vaysse and Adil Zouitine and Heitor Murilo Gomes and Jesse Read and Talel Abdessalem and Albert Bifet},
 code = {https://github.com/online-ml/river},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W3111828510},
 pages = {1--8},
 pdf = {/papers/volume22/20-1380/20-1380.pdf},
 title = {River: machine learning for streaming data in Python},
 url = {http://jmlr.org/papers/v22/20-1380.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-1473,
 abstract = {The increasing amount of available data, computing power, and the constant pursuit for higher performance results in the growing complexity of predictive models. Their black-box nature leads to opaqueness debt phenomenon inflicting increased risks of discrimination, lack of reproducibility, and deflated performance due to data drift. To manage these risks, good MLOps practices ask for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity of deeper model transparency appears not only from scientific and social domains, but also emerging laws and regulations on artificial intelligence. To facilitate the development of responsible machine learning models, we showcase dalex, a Python package which implements the model-agnostic interface for interactive model exploration. It adopts the design crafted through the development of various tools for responsible machine learning; thus, it aims at the unification of the existing solutions. This library's source code and documentation are available under open license at https://python.drwhy.ai/.},
 author = {Hubert Baniecki and Wojciech Kretowicz and Piotr PiÄ
tyszek and Jakub WiÅniewski and PrzemysÅaw Biecek},
 code = {https://github.com/ModelOriented/DALEX},
 journal = {Journal of Machine Learning Research},
 number = {214},
 openalex = {W3206100932},
 pages = {1--7},
 pdf = {/papers/volume22/20-1473/20-1473.pdf},
 title = {dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python},
 url = {http://jmlr.org/papers/v22/20-1473.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-225,
 abstract = {TensorHive is a tool for organizing work of research and engineering teams that use servers with GPUs for machine learning workloads. In a comprehensive web interface, it supports reservation of GPUs for exclusive usage, hardware monitoring, as well as configuring, executing and queuing distributed computational jobs. Focusing on easy installation and simple configuration, the tool automatically detects the available computing resources and monitors their utilization. Reservations granted on the basis of flexible access control settings are protected by pluggable violation hooks. The job execution module includes auto-configuration templates for distributed neural network training jobs in frameworks such as TensorFlow and PyTorch. Documentation, source code, usage examples and issue tracking are available at the project page: https://github.com/roscisz/TensorHive/},
 author = {PaweÅ RoÅciszewski and MichaÅ Martyniak and Filip Schodowski},
 code = {https://github.com/roscisz/TensorHive/},
 journal = {Journal of Machine Learning Research},
 number = {215},
 openalex = {W3205622964},
 pages = {1--5},
 pdf = {/papers/volume22/20-225/20-225.pdf},
 title = {TensorHive: Management of Exclusive GPU Access for Distributed Machine Learning Workloads},
 url = {http://jmlr.org/papers/v22/20-225.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-275,
 abstract = {The KeOps library provides a fast and memory-efficient GPU support for tensors whose entries are given by a mathematical formula, such as kernel and distance matrices. KeOps alleviates the major bottleneck of tensor-centric libraries for kernel and geometric applications: memory consumption. It also supports automatic differentiation and outperforms standard GPU baselines, including PyTorch CUDA tensors or the Halide and TVM libraries. KeOps combines optimized C++/CUDA schemes with binders for high-level languages: Python (Numpy and PyTorch), Matlab and GNU R. As a result, high-level "quadratic" codes can now scale up to large data sets with millions of samples processed in seconds. KeOps brings graphics-like performances for kernel methods and is freely available on standard repositories (PyPi, CRAN). To showcase its versatility, we provide tutorials in a wide range of settings online at \url{www.kernel-operations.io}.},
 author = {Benjamin Charlier and Jean Feydy and Joan Alexis GlaunÃ¨s and FranÃ§ois-David Collin and Ghislain Durif},
 code = {https://github.com/getkeops/keops/},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W3020430839},
 pages = {1--6},
 pdf = {/papers/volume22/20-275/20-275.pdf},
 title = {Kernel Operations on the GPU, with Autodiff, without Memory Overflows},
 url = {http://jmlr.org/papers/v22/20-275.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-325,
 abstract = {We introduce giotto-tda, a Python library that integrates high-performance topological data analysis with machine learning via a scikit-learn-compatible API and state-of-the-art C++ implementations. The library's ability to handle various types of data is rooted in a wide range of preprocessing techniques, and its strong focus on data exploration and interpretability is aided by an intuitive plotting API. Source code, binaries, examples, and documentation can be found at https://github.com/giotto-ai/giotto-tda.},
 author = {Guillaume Tauzin and Umberto Lupo and Lewis Tunstall and Julian Burella PÃ©rez and Matteo Caorsi and Anibal M. Medina-Mardones and Alberto Dassatti and Kathryn Hess},
 code = {https://github.com/giotto-ai/giotto-tda},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W4287817216},
 pages = {1--6},
 pdf = {/papers/volume22/20-325/20-325.pdf},
 title = {giotto-tda: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration},
 url = {http://jmlr.org/papers/v22/20-325.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-376,
 abstract = {In this paper, we introduce ChainerRL, an open-source deep reinforcement learning (DRL) library built using Python and the Chainer deep learning framework. ChainerRL implements a comprehensive set of DRL algorithms and techniques drawn from state-of-the-art research in the field. To foster reproducible research, and for instructional purposes, ChainerRL provides scripts that closely replicate the original papers' experimental settings and reproduce published benchmark results for several algorithms. Lastly, ChainerRL offers a visualization tool that enables the qualitative inspection of trained agents. The ChainerRL source code can be found on GitHub: https://github.com/chainer/chainerrl.},
 author = {Yasuhiro Fujita and Prabhat Nagarajan and Toshiki Kataoka and Takahiro Ishikawa},
 code = {https://github.com/chainer/chainerrl},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W2994240296},
 pages = {1--14},
 pdf = {/papers/volume22/20-376/20-376.pdf},
 title = {ChainerRL: A Deep Reinforcement Learning Library},
 url = {http://jmlr.org/papers/v22/20-376.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-416,
 abstract = {We overview the ensmallen numerical optimization library, which provides a flexible C++ framework for mathematical optimization of user-supplied objective functions. Many types of objective functions are supported, including general, differentiable, separable, constrained, and categorical. A diverse set of pre-built optimizers is provided, including Quasi-Newton optimizers and many variants of Stochastic Gradient Descent. The underlying framework facilitates the implementation of new optimizers. Optimization of an objective function typically requires supplying only one or two C++ functions. Custom behavior can be easily specified via callback functions. Empirical comparisons show that ensmallen outperforms other frameworks while providing more functionality. The library is available at https://ensmallen.org and is distributed under the permissive BSD license.},
 author = {Ryan R. Curtin and Marcus Edel and Rahul Ganesh Prabhu and Suryoday Basak and Zhihao Lou and Conrad Sanderson},
 code = {https://github.com/mlpack/ensmallen},
 journal = {Journal of Machine Learning Research},
 number = {166},
 openalex = {W3196826520},
 pages = {1--6},
 pdf = {/papers/volume22/20-416/20-416.pdf},
 title = {The ensmallen library for flexible numerical optimization},
 url = {http://jmlr.org/papers/v22/20-416.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-451,
 author = {RÃ©mi Flamary and Nicolas Courty and Alexandre Gramfort and Mokhtar Z. Alaya and AurÃ©lie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and LÃ©o Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
 code = {https://github.com/PythonOT/POT},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W4287245321},
 pages = {1--8},
 pdf = {/papers/volume22/20-451/20-451.pdf},
 title = {POT : Python Optimal Transport},
 url = {http://jmlr.org/papers/v22/20-451.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:20-815,
 author = {Yang Liu and Tao Fan and Tianjian Chen and Qian Xu and Qiang Yang},
 code = {https://github.com/FederatedAI/FATE},
 journal = {Journal of Machine Learning Research},
 number = {226},
 openalex = {W3206887799},
 pages = {1--6},
 pdf = {/papers/volume22/20-815/20-815.pdf},
 title = {FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection},
 url = {http://jmlr.org/papers/v22/20-815.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0017,
 author = {Janis Klaise and Arnaud Van Looveren and Giovanni Vacanti and Alexandru Coca},
 code = {https://github.com/SeldonIO/alibi},
 journal = {Journal of Machine Learning Research},
 number = {181},
 openalex = {W3190528386},
 pages = {1--7},
 pdf = {/papers/volume22/21-0017/21-0017.pdf},
 title = {Alibi Explain: Algorithms for Explaining Machine Learning Models},
 url = {http://jmlr.org/papers/v22/21-0017.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0029,
 abstract = {The sklvq package is an open-source Python implementation of a set of learning vector quantization (LVQ) algorithms. In addition to providing the core functionality for the GLVQ, GMLVQ, and LGMLVQ algorithms, sklvq is distinctive by putting emphasis on its modular and customizable design. Not only resulting in a feature-rich implementation for users but enabling easy extensions of the algorithms for researchers. The theory behind this design is described in this paper. To facilitate adoptions and inspire future contributions, sklvq is publicly available on Github (under the BSD license) and can be installed through the Python package index (PyPI). Next to being well-covered by automated testing to ensure code quality, it is accompanied by detailed online documentation. The documentation covers usage examples and provides an in-depth API including theory and scientific references.},
 author = {Rick van Veen and Michael Biehl and Gert-Jan de Vries},
 code = {https://github.com/rickvanveen/sklvq},
 journal = {Journal of Machine Learning Research},
 number = {231},
 openalex = {W3206436985},
 pages = {1--6},
 pdf = {/papers/volume22/21-0029/21-0029.pdf},
 title = {sklvq : Scikit Learning Vector Quantization},
 url = {http://jmlr.org/papers/v22/21-0029.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0281,
 author = {Martin Binder and Florian Pfisterer and Michel Lang and Lennart Schneider and Lars Kotthoff and Bernd Bischl},
 code = {https://github.com/mlr-org/mlr3pipelines},
 journal = {Journal of Machine Learning Research},
 number = {184},
 openalex = {W3189984135},
 pages = {1--7},
 pdf = {/papers/volume22/21-0281/21-0281.pdf},
 title = {mlr3pipelines - Flexible Machine Learning Pipelines in R},
 url = {http://jmlr.org/papers/v22/21-0281.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v22:21-0343,
 abstract = {Although there exist several libraries for deep learning on graphs, they are aiming at implementing basic operations for graph deep learning. In the research community, implementing and benchmarking various advanced tasks are still painful and time-consuming with existing libraries. To facilitate graph deep learning research, we introduce DIG: Dive into Graphs, a turnkey library that provides a unified testbed for higher level, research-oriented graph deep learning tasks. Currently, we consider graph generation, self-supervised learning on graphs, explainability of graph neural networks, and deep learning on 3D graphs. For each direction, we provide unified implementations of data interfaces, common algorithms, and evaluation metrics. Altogether, DIG is an extensible, open-source, and turnkey library for researchers to develop new methods and effortlessly compare with common baselines using widely used datasets and evaluation metrics. Source code is available at https://github.com/divelab/DIG.},
 author = {Meng Liu and Youzhi Luo and Limei Wang and Yaochen Xie and Hao Yuan and Shurui Gui and Haiyang Yu and Zhao Xu and Jingtun Zhang and Yi Liu and Keqiang Yan and Haoran Liu and Cong Fu and Bora M Oztekin and Xuan Zhang and Shuiwang Ji},
 code = {https://github.com/divelab/DIG},
 journal = {Journal of Machine Learning Research},
 number = {240},
 openalex = {W3206541895},
 pages = {1--9},
 pdf = {/papers/volume22/21-0343/21-0343.pdf},
 title = {DIG: A Turnkey Library for Diving into Graph Deep Learning Research},
 url = {http://jmlr.org/papers/v22/21-0343.html},
 volume = {22},
 year = {2021}
}

@article{JMLR:v23:21-0174,
 author = {Å imon MandlÃ­k and MatÄj RaÄinskÃ½ and Viliam LisÃ½ and TomÃ¡Å¡ PevnÃ½},
 code = {https://github.com/CTUAvastLab/JsonGrinder.jl},
 journal = {Journal of Machine Learning Research},
 number = {298},
 pages = {1--5},
 pdf = {/papers/volume23/21-0174/21-0174.pdf},
 title = {JsonGrinder.jl: automated differentiable neural architecture for embedding arbitrary JSON data},
 url = {http://jmlr.org/papers/v23/21-0174.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-0738,
 author = {Xuhong Li and Haoyi Xiong and Xingjian Li and Xuanyu Wu and Zeyu Chen and Dejing Dou},
 code = {https://github.com/PaddlePaddle/InterpretDL},
 journal = {Journal of Machine Learning Research},
 number = {197},
 pages = {1--6},
 pdf = {/papers/volume23/21-0738/21-0738.pdf},
 title = {InterpretDL: Explaining Deep Models in PaddlePaddle},
 url = {http://jmlr.org/papers/v23/21-0738.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-0791,
 author = {Dominique Benielli and Baptiste Bauvin and Sokol KoÃ§o and Riikka Huusari and CÃ©cile Capponi and Hachem Kadri and FranÃ§ois Laviolette},
 code = {https://github.com/dbenielli/scikit-multimodallearn},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W4226092505},
 pages = {1--7},
 pdf = {/papers/volume23/21-0791/21-0791.pdf},
 title = {Toolbox for Multimodal Learn (scikit-multimodallearn)},
 url = {http://jmlr.org/papers/v23/21-0791.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-0862,
 abstract = {DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org.},
 author = {Philipp Bach and Victor Chernozhukov and Malte S. Kurz and Martin Spindler},
 code = {https://github.com/DoubleML/doubleml-for-py},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W4287237145},
 pages = {1--6},
 pdf = {/papers/volume23/21-0862/21-0862.pdf},
 title = {DoubleML -- An Object-Oriented Implementation of Double Machine Learning in Python},
 url = {http://jmlr.org/papers/v23/21-0862.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-0888,
 abstract = {Algorithm parameters, in particular hyperparameters of machine learning algorithms, can substantially impact their performance. To support users in determining well-performing hyperparameter configurations for their algorithms, datasets and applications at hand, SMAC3 offers a robust and flexible framework for Bayesian Optimization, which can improve performance within a few evaluations. It offers several facades and pre-sets for typical use cases, such as optimizing hyperparameters, solving low dimensional continuous (artificial) global optimization problems and configuring algorithms to perform well across multiple problem instances. The SMAC3 package is available under a permissive BSD-license at https://github.com/automl/SMAC3.},
 author = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and AndrÃ© Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and RenÃ© Sass and Frank Hutter},
 code = {https://github.com/automl/SMAC3},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W4286970057},
 pages = {1--9},
 pdf = {/papers/volume23/21-0888/21-0888.pdf},
 title = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
 url = {http://jmlr.org/papers/v23/21-0888.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-1060,
 abstract = {We introduce a new library named abess that implements a unified framework of best-subset selection for solving diverse machine learning problems, e.g., linear regression, classification, and principal component analysis. Particularly, the abess certifiably gets the optimal solution within polynomial times with high probability under the linear model. Our efficient implementation allows abess to attain the solution of best-subset selection problems as fast as or even 20x faster than existing competing variable (model) selection toolboxes. Furthermore, it supports common variants like best group subset selection and $\ell_2$ regularized best-subset selection. The core of the library is programmed in C++. For ease of use, a Python library is designed for conveniently integrating with scikit-learn, and it can be installed from the Python library Index. In addition, a user-friendly R library is available at the Comprehensive R Archive Network. The source code is available at: https://github.com/abess-team/abess.},
 author = {Jin Zhu and Xueqin Wang and Liyuan Hu and Junhao Huang and Kangkang Jiang and Yanhang Zhang and Shiyun Lin and Junxian Zhu},
 code = {https://github.com/abess-team/abess},
 journal = {Journal of Machine Learning Research},
 number = {202},
 openalex = {W3206459514},
 pages = {1--7},
 pdf = {/papers/volume23/21-1060/21-1060.pdf},
 title = {abess: A Fast Best Subset Selection Library in Python and R},
 url = {http://jmlr.org/papers/v23/21-1060.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-1124,
 abstract = {We present ktrain, a low-code Python library that makes machine learning more accessible and easier to apply. As a wrapper to TensorFlow and many other libraries (e.g., transformers, scikit-learn, stellargraph), it is designed to make sophisticated, state-of-the-art machine learning models simple to build, train, inspect, and apply by both beginners and experienced practitioners. Featuring modules that support text data (e.g., text classification, sequence tagging, open-domain question-answering), vision data (e.g., image classification), graph data (e.g., node classification, link prediction), and tabular data, ktrain presents a simple unified interface enabling one to quickly solve a wide range of tasks in as little as three or four commands or lines of code.},
 author = {Arun S. Maiya},
 code = {https://github.com/amaiya/ktrain},
 journal = {Journal of Machine Learning Research},
 number = {158},
 openalex = {W3017852832},
 pages = {1--6},
 pdf = {/papers/volume23/21-1124/21-1124.pdf},
 title = {ktrain: A Low-Code Library for Augmented Machine Learning},
 url = {http://jmlr.org/papers/v23/21-1124.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-1127,
 abstract = {In this paper, we present Tianshou, a highly modularized Python library for deep reinforcement learning (DRL) that uses PyTorch as its backend. Tianshou intends to be research-friendly by providing a flexible and reliable infrastructure of DRL algorithms. It supports online and offline training with more than 20 classic algorithms through a unified interface. To facilitate related research and prove Tianshou's reliability, we have released Tianshou's benchmark of MuJoCo environments, covering eight classic algorithms with state-of-the-art performance. We open-sourced Tianshou at https://github.com/thu-ml/tianshou/.},
 author = {Jiayi Weng and Huayu Chen and Dong Yan and Kaichao You and Alexis Duburcq and Minghao Zhang and Yi Su and Hang Su and Jun Zhu},
 code = {https://github.com/thu-ml/tianshou/},
 journal = {Journal of Machine Learning Research},
 number = {267},
 openalex = {W3183892796},
 pages = {1--6},
 pdf = {/papers/volume23/21-1127/21-1127.pdf},
 title = {Tianshou: a Highly Modularized Deep Reinforcement Learning Library},
 url = {http://jmlr.org/papers/v23/21-1127.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-1155,
 abstract = {This paper presents solo-learn, a library of self-supervised methods for visual representation learning. Implemented in Python, using Pytorch and Pytorch lightning, the library fits both research and industry needs by featuring distributed training pipelines with mixed-precision, faster data loading via Nvidia DALI, online linear evaluation for better prototyping, and many additional training tricks. Our goal is to provide an easy-to-use library comprising a large amount of Self-supervised Learning (SSL) methods, that can be easily extended and fine-tuned by the community. solo-learn opens up avenues for exploiting large-budget SSL solutions on inexpensive smaller infrastructures and seeks to democratize SSL by making it accessible to all. The source code is available at https://github.com/vturrisi/solo-learn.},
 author = {Victor Guilherme Turrisi da Costa and Enrico Fini and Moin Nabi and Nicu Sebe and Elisa Ricci},
 code = {https://github.com/vturrisi/solo-learn},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W4287028988},
 pages = {1--6},
 pdf = {/papers/volume23/21-1155/21-1155.pdf},
 title = {Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning},
 url = {http://jmlr.org/papers/v23/21-1155.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-1177,
 abstract = {We present Darts, a Python machine learning library for time series, with a focus on forecasting. Darts offers a variety of models, from classics such as ARIMA to state-of-the-art deep neural networks. The emphasis of the library is on offering modern machine learning functionalities, such as supporting multidimensional series, meta-learning on multiple series, training on large datasets, incorporating external data, ensembling models, and providing a rich support for probabilistic forecasting. At the same time, great care goes into the API design to make it user-friendly and easy to use. For instance, all models can be used using fit()/predict(), similar to scikit-learn.},
 author = {Julien Herzen and Francesco LÃ¤ssig and Samuele Giuliano Piazzetta and Thomas Neuer and LÃ©o Tafti and Guillaume Raille and Tomas Van Pottelbergh and Marek Pasieka and Andrzej Skrodzki and Nicolas Huguenin and Maxime Dumonal and Jan KoÅcisz and Dennis Bader and FrÃ©dÃ©rick Gusset and Mounir Benheddi and Camila Williamson and Michal Kosinski and Matej Petrik and GaÃ«l Grosch},
 code = {https://github.com/unit8co/darts},
 journal = {Journal of Machine Learning Research},
 number = {124},
 openalex = {W4286908664},
 pages = {1--6},
 pdf = {/papers/volume23/21-1177/21-1177.pdf},
 title = {Darts: User-Friendly Modern Machine Learning for Time Series},
 url = {http://jmlr.org/papers/v23/21-1177.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:21-1342,
 abstract = {CleanRL is an open-source library that provides high-quality single-file implementations of Deep Reinforcement Learning algorithms. It provides a simpler yet scalable developing experience by having a straightforward codebase and integrating production tools to help interact and scale experiments. In CleanRL, we put all details of an algorithm into a single file, making these performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, videos of an agent's gameplay, dependencies, and more to the cloud. Despite succinct implementations, we have also designed tools to help scale, at one point orchestrating experiments on more than 2000 machines simultaneously via Docker and cloud providers. Finally, we have ensured the quality of the implementations by benchmarking against a variety of environments. The source code of CleanRL can be found at https://github.com/vwxyzjn/cleanrl},
 author = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga and Dipam Chakraborty and Kinal Mehta and JoÃ£o G.M. AraÃºjo},
 code = {https://github.com/vwxyzjn/cleanrl},
 journal = {Journal of Machine Learning Research},
 number = {274},
 openalex = {W3213774989},
 pages = {1--18},
 pdf = {/papers/volume23/21-1342/21-1342.pdf},
 title = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},
 url = {http://jmlr.org/papers/v23/21-1342.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:22-0017,
 abstract = {In this paper, we introduce d3rlpy, an open-sourced offline deep reinforcement learning (RL) library for Python. d3rlpy supports a set of offline deep RL algorithms as well as off-policy online algorithms via a fully documented plug-and-play API. To address a reproducibility issue, we conduct a large-scale benchmark with D4RL and Atari 2600 dataset to ensure implementation quality and provide experimental scripts and full tables of results. The d3rlpy source code can be found on GitHub: \url{https://github.com/takuseno/d3rlpy}.},
 author = {Takuma Seno and Michita Imai},
 code = {https://github.com/takuseno/d3rlpy},
 journal = {Journal of Machine Learning Research},
 number = {315},
 openalex = {W3211444363},
 pages = {1--20},
 pdf = {/papers/volume23/22-0017/22-0017.pdf},
 title = {d3rlpy: An Offline Deep Reinforcement Learning Library},
 url = {http://jmlr.org/papers/v23/22-0017.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:22-0185,
 abstract = {Deep reinforcement learning (RL) is a powerful framework to train decision-making models in complex environments. However, RL can be slow as it requires repeated interaction with a simulation of the environment. In particular, there are key system engineering bottlenecks when using RL in complex environments that feature multiple agents with high-dimensional state, observation, or action spaces. We present WarpDrive, a flexible, lightweight, and easy-to-use open-source RL framework that implements end-to-end deep multi-agent RL on a single GPU (Graphics Processing Unit), built on PyCUDA and PyTorch. Using the extreme parallelization capability of GPUs, WarpDrive enables orders-of-magnitude faster RL compared to common implementations that blend CPU simulations and GPU models. Our design runs simulations and the agents in each simulation in parallel. It eliminates data copying between CPU and GPU. It also uses a single simulation data store on the GPU that is safely updated in-place. WarpDrive provides a lightweight Python interface and flexible environment wrappers that are easy to use and extend. Together, this allows the user to easily run thousands of concurrent multi-agent simulations and train on extremely large batches of experience. Through extensive experiments, we verify that WarpDrive provides high-throughput and scales almost linearly to many agents and parallel environments. For example, WarpDrive yields 2.9 million environment steps/second with 2000 environments and 1000 agents (at least 100x higher throughput compared to a CPU implementation) in a benchmark Tag simulation. As such, WarpDrive is a fast and extensible multi-agent RL platform to significantly accelerate research and development.},
 author = {Tian Lan and Sunil Srinivasa and Huan Wang and Stephan Zheng},
 code = {https://github.com/salesforce/warp-drive},
 journal = {Journal of Machine Learning Research},
 number = {316},
 openalex = {W3196764258},
 pages = {1--6},
 pdf = {/papers/volume23/22-0185/22-0185.pdf},
 title = {WarpDrive: Extremely Fast End-to-End Deep Multi-Agent Reinforcement Learning on a GPU},
 url = {http://jmlr.org/papers/v23/22-0185.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:22-0277,
 abstract = {The optimization and machine learning toolkit (OMLT) is an open-source software package incorporating neural network and gradient-boosted tree surrogate models, which have been trained using machine learning, into larger optimization problems. We discuss the advances in optimization technology that made OMLT possible and show how OMLT seamlessly integrates with the algebraic modeling language Pyomo. We demonstrate how to use OMLT for solving decision-making problems in both computer science and engineering.},
 author = {Francesco Ceccon and Jordan Jalving and Joshua Haddad and Alexander Thebelt and Calvin Tsay and Carl D Laird and Ruth Misener},
 code = {https://github.com/cog-imperial/OMLT},
 journal = {Journal of Machine Learning Research},
 number = {349},
 openalex = {W4226523564},
 pages = {1--8},
 pdf = {/papers/volume23/22-0277/22-0277.pdf},
 title = {OMLT: Optimization &amp; Machine Learning Toolkit},
 url = {http://jmlr.org/papers/v23/22-0277.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:22-0281,
 abstract = {This paper presents Deepchecks, a Python library for comprehensively validating machine learning models and data. Our goal is to provide an easy-to-use library comprising of many checks related to various types of issues, such as model predictive performance, data integrity, data distribution mismatches, and more. The package is distributed under the GNU Affero General Public License (AGPL) and relies on core libraries from the scientific Python ecosystem: scikit-learn, PyTorch, NumPy, pandas, and SciPy. Source code, documentation, examples, and an extensive user guide can be found at \url{https://github.com/deepchecks/deepchecks} and \url{https://docs.deepchecks.com/}.},
 author = {Shir Chorev and Philip Tannor and Dan Ben Israel and Noam Bressler and Itay Gabbay and Nir Hutnik and Jonatan Liberman and Matan Perlmutter and Yurii Romanyshyn and Lior Rokach},
 code = {https://github.com/deepchecks/deepchecks},
 journal = {Journal of Machine Learning Research},
 number = {285},
 openalex = {W4221145303},
 pages = {1--6},
 pdf = {/papers/volume23/22-0281/22-0281.pdf},
 title = {Deepchecks: A Library for Testing and Validating Machine Learning Models and Data},
 url = {http://jmlr.org/papers/v23/22-0281.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v23:22-0611,
 abstract = {We introduce ReservoirComputing.jl, an open source Julia library for reservoir computing models. The software offers a great number of algorithms presented in the literature, and allows to expand on them with both internal and external tools in a simple way. The implementation is highly modular, fast and comes with a comprehensive documentation, which includes reproduced experiments from literature. The code and documentation are hosted on Github under an MIT license https://github.com/SciML/ReservoirComputing.jl.},
 author = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora},
 code = {https://github.com/SciML/ReservoirComputing.jl},
 journal = {Journal of Machine Learning Research},
 number = {288},
 openalex = {W4224120179},
 pages = {1--8},
 pdf = {/papers/volume23/22-0611/22-0611.pdf},
 title = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models},
 url = {http://jmlr.org/papers/v23/22-0611.html},
 volume = {23},
 year = {2022}
}

@article{JMLR:v24:20-1355,
 author = {Haifeng Jin and FranÃ§ois Chollet and Qingquan Song and Xia Hu},
 code = {https://github.com/keras-team/autokeras},
 journal = {Journal of Machine Learning Research},
 number = {6},
 pages = {1--6},
 pdf = {/papers/volume24/20-1355/20-1355.pdf},
 title = {AutoKeras: An AutoML Library for Deep Learning},
 url = {http://jmlr.org/papers/v24/20-1355.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-0321,
 author = {Takashi Ikeuchi and Mayumi Ide and Yan Zeng and Takashi Nicholas Maeda and Shohei Shimizu},
 code = {https://github.com/cdt15/lingam},
 journal = {Journal of Machine Learning Research},
 number = {14},
 pages = {1--8},
 pdf = {/papers/volume24/21-0321/21-0321.pdf},
 title = {Python package for causal discovery based on LiNGAM},
 url = {http://jmlr.org/papers/v24/21-0321.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1436,
 author = {Adrien Pavao and Isabelle Guyon and Anne-Catherine Letournel and Dinh-Tuan Tran and Xavier Baro and Hugo Jair Escalante and Sergio Escalera and Tyler Thomas and Zhen Xu},
 code = {https://github.com/codalab/codalab-competitions/},
 journal = {Journal of Machine Learning Research},
 number = {198},
 openalex = {W4295682935},
 pages = {1--6},
 pdf = {/papers/volume24/21-1436/21-1436.pdf},
 title = {CodaLab Competitions: An open source platform to organize scientific challenges},
 url = {http://jmlr.org/papers/v24/21-1436.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1441,
 abstract = {Much research effort has been devoted to explaining the success of deep learning. Random Matrix Theory (RMT) provides an emerging way to this end: spectral analysis of large random matrices involved in a trained deep neural network (DNN) such as weight matrices or Hessian matrices with respect to the stochastic gradient descent algorithm. To have more comprehensive understanding of weight matrices spectra, we conduct extensive experiments on weight matrices in different modules, e.g., layers, networks and data sets. Following the previous work of \cite{martin2018implicit}, we classify the spectra in the terminal stage into three main types: Light Tail (LT), Bulk Transition period (BT) and Heavy Tail(HT). These different types, especially HT, implicitly indicate some regularization in the DNNs. A main contribution from the paper is that we identify the difficulty of the classification problem as a driving factor for the appearance of heavy tail in weight matrices spectra. Higher the classification difficulty, higher the chance for HT to appear. Moreover, the classification difficulty can be affected by the signal-to-noise ratio of the dataset, or by the complexity of the classification problem (complex features, large number of classes) as well. Leveraging on this finding, we further propose a spectral criterion to detect the appearance of heavy tails and use it to early stop the training process without testing data. Such early stopped DNNs have the merit of avoiding overfitting and unnecessary extra training while preserving a much comparable generalization ability. These findings from the paper are validated in several NNs, using Gaussian synthetic data and real data sets (MNIST and CIFAR10).},
 author = {XuranMeng and JeffYao},
 code = {https://github.com/juve-xx/watchtheweight},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W4226320345},
 pages = {1--40},
 pdf = {/papers/volume24/21-1441/21-1441.pdf},
 title = {Impact of classification difficulty on the weight matrices spectra in Deep Learning and application to early-stopping},
 url = {http://jmlr.org/papers/v24/21-1441.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:21-1518,
 abstract = {HiClass is an open-source Python library for local hierarchical classification entirely compatible with scikit-learn. It contains implementations of the most common design patterns for hierarchical machine learning models found in the literature, that is, the local classifiers per node, per parent node and per level. Additionally, the package contains implementations of hierarchical metrics, which are more appropriate for evaluating classification performance on hierarchical data. The documentation includes installation and usage instructions, examples within tutorials and interactive notebooks, and a complete description of the API. HiClass is released under the simplified BSD license, encouraging its use in both academic and commercial environments. Source code and documentation are available at https://github.com/scikit-learn-contrib/hiclass.},
 author = {FÃ¡bio M. Miranda and Niklas KÃ¶hnecke and Bernhard Y. Renard},
 code = {https://github.com/scikit-learn-contrib/hiclass},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W4226083381},
 pages = {1--17},
 pdf = {/papers/volume24/21-1518/21-1518.pdf},
 title = {HiClass: a Python library for local hierarchical classification compatible with scikit-learn},
 url = {http://jmlr.org/papers/v24/21-1518.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0142,
 abstract = {The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus -- a comprehensive, evaluation toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/).},
 author = {Anna HedstrÃ¶m and Leander Weber and Daniel Krakowczyk and Dilyara Bareeva and Franz Motzkus and Wojciech Samek and Sebastian Lapuschkin and Marina M.-C. HÃ¶hne},
 code = {https://github.com/understandable-machine-intelligence-lab/Quantus/},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W4221159886},
 pages = {1--11},
 pdf = {/papers/volume24/22-0142/22-0142.pdf},
 title = {Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond},
 url = {http://jmlr.org/papers/v24/22-0142.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0169,
 abstract = {Population-based multi-agent reinforcement learning (PB-MARL) refers to the series of methods nested with reinforcement learning (RL) algorithms, which produces a self-generated sequence of tasks arising from the coupled population dynamics. By leveraging auto-curricula to induce a population of distinct emergent strategies, PB-MARL has achieved impressive success in tackling multi-agent tasks. Despite remarkable prior arts of distributed RL frameworks, PB-MARL poses new challenges for parallelizing the training frameworks due to the additional complexity of multiple nested workloads between sampling, training and evaluation involved with heterogeneous policy interactions. To solve these problems, we present MALib, a scalable and efficient computing framework for PB-MARL. Our framework is comprised of three key components: (1) a centralized task dispatching model, which supports the self-generated tasks and scalable training with heterogeneous policy combinations; (2) a programming architecture named Actor-Evaluator-Learner, which achieves high parallelism for both training and sampling, and meets the evaluation requirement of auto-curriculum learning; (3) a higher-level abstraction of MARL training paradigms, which enables efficient code reuse and flexible deployments on different distributed computing paradigms. Experiments on a series of complex tasks such as multi-agent Atari Games show that MALib achieves throughput higher than 40K FPS on a single machine with $32$ CPU cores; 5x speedup than RLlib and at least 3x speedup than OpenSpiel in multi-agent training tasks. MALib is publicly available at https://github.com/sjtu-marl/malib.},
 author = {Ming Zhou and Ziyu Wan and Hanjing Wang and Muning Wen and Runzhe Wu and Ying Wen and Yaodong Yang and Yong Yu and Jun Wang and Weinan Zhang},
 code = {https://github.com/sjtu-marl/malib},
 journal = {Journal of Machine Learning Research},
 number = {150},
 openalex = {W3196792970},
 pages = {1--12},
 pdf = {/papers/volume24/22-0169/22-0169.pdf},
 title = {MALib: A Parallel Framework for Population-based Multi-agent Reinforcement Learning},
 url = {http://jmlr.org/papers/v24/22-0169.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0189,
 abstract = {We present L0Learn: an open-source package for sparse linear regression and classification using $\ell_0$ regularization. L0Learn implements scalable, approximate algorithms, based on coordinate descent and local combinatorial optimization. The package is built using C++ and has user-friendly R and Python interfaces. L0Learn can address problems with millions of features, achieving competitive run times and statistical performance with state-of-the-art sparse learning packages. L0Learn is available on both CRAN and GitHub (https://cran.r-project.org/package=L0Learn and https://github.com/hazimehh/L0Learn).},
 author = {Hussein Hazimeh and Rahul Mazumder and Tim Nonet},
 code = {https://github.com/hazimehh/L0Learn},
 journal = {Journal of Machine Learning Research},
 number = {205},
 openalex = {W4221152715},
 pages = {1--8},
 pdf = {/papers/volume24/22-0189/22-0189.pdf},
 title = {L0Learn: A Scalable Package for Sparse Learning using L0 Regularization},
 url = {http://jmlr.org/papers/v24/22-0189.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0347,
 author = {Baijiong Lin and Yu Zhang},
 code = {https://github.com/median-research-group/LibMTL},
 journal = {Journal of Machine Learning Research},
 number = {209},
 pages = {1--7},
 pdf = {/papers/volume24/22-0347/22-0347.pdf},
 title = {LibMTL: A Python Library for Deep Multi-Task Learning},
 url = {http://jmlr.org/papers/v24/22-0347.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0440,
 abstract = {Federated learning (FL) is a machine learning field in which researchers try to facilitate model learning process among multiparty without violating privacy protection regulations. Considerable effort has been invested in FL optimization and communication related researches. In this work, we introduce \texttt{FedLab}, a lightweight open-source framework for FL simulation. The design of \texttt{FedLab} focuses on FL algorithm effectiveness and communication efficiency. Also, \texttt{FedLab} is scalable in different deployment scenario. We hope \texttt{FedLab} could provide flexible API as well as reliable baseline implementations, and relieve the burden of implementing novel approaches for researchers in FL community.},
 author = {Dun Zeng and Siqi Liang and Xiangjing Hu and Hui Wang and Zenglin Xu},
 code = {https://github.com/SMILELab-FL/FedLab},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W3184328775},
 pages = {1--7},
 pdf = {/papers/volume24/22-0440/22-0440.pdf},
 title = {FedLab: A Flexible Federated Learning Framework},
 url = {http://jmlr.org/papers/v24/22-0440.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-0809,
 author = {Aadyot Bhatnagar and Paul Kassianik and Chenghao Liu and Tian Lan and Wenzhuo Yang and Rowan Cassius and Doyen Sahoo and Devansh Arpit and Sri Subramanian and Gerald Woo and Amrita Saha and Arun Kumar Jagota and Gokulakrishnan Gopalakrishnan and Manpreet Singh and K C Krithika and Sukumar Maddineni and Daeki Cho and Bo Zong and Yingbo Zhou and Caiming Xiong and Silvio Savarese and Steven Hoi and Huan Wang},
 code = {https://github.com/salesforce/Merlion},
 journal = {Journal of Machine Learning Research},
 number = {226},
 pages = {1--6},
 pdf = {/papers/volume24/22-0809/22-0809.pdf},
 title = {Merlion: End-to-End Machine Learning for Time Series},
 url = {http://jmlr.org/papers/v24/22-0809.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1021,
 abstract = {Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of > 20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.},
 author = {Paul Pu Liang and Yiwei Lyu and Xiang Fan and Arav Agarwal and Yun Cheng and Louis-Philippe Morency and Ruslan Salakhutdinov},
 code = {https://github.com/pliang279/MultiBench},
 journal = {Journal of Machine Learning Research},
 number = {234},
 openalex = {W4382619411},
 pages = {1--7},
 pdf = {/papers/volume24/22-1021/22-1021.pdf},
 title = {MultiZoo &amp; MultiBench: A Standardized Toolkit for Multimodal Deep Learning},
 url = {http://jmlr.org/papers/v24/22-1021.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:22-1047,
 author = {Jun Zhou and Ke Zhang and Lin Wang and Hua Wu and Yi Wang and ChaoChao Chen},
 code = {https://github.com/sql-machine-learning/sqlflow},
 journal = {Journal of Machine Learning Research},
 number = {116},
 pages = {1--9},
 pdf = {/papers/volume24/22-1047/22-1047.pdf},
 title = {SQLFlow: An Extensible Toolkit Integrating DB and AI},
 url = {http://jmlr.org/papers/v24/22-1047.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0112,
 abstract = {skrl is an open-source modular library for reinforcement learning written in Python and designed with a focus on readability, simplicity, and transparency of algorithm implementations. In addition to supporting environments that use the traditional interfaces from OpenAI Gym and DeepMind, it provides the facility to load, configure, and operate NVIDIA Isaac Gym and NVIDIA Omniverse Isaac Gym environments. Furthermore, it enables the simultaneous training of several agents with customizable scopes (subsets of environments among all available ones), which may or may not share resources, in the same run. The library's documentation can be found at https://skrl.readthedocs.io and its source code is available on GitHub at https://github.com/Toni-SM/skrl.},
 author = {Antonio Serrano-MuÃ±oz and Dimitrios Chrysostomou and Simon BÃ¸gh and Nestor Arana-Arexolaleiba},
 code = {https://github.com/Toni-SM/skrl},
 journal = {Journal of Machine Learning Research},
 number = {254},
 openalex = {W4226449706},
 pages = {1--9},
 pdf = {/papers/volume24/23-0112/23-0112.pdf},
 title = {skrl: Modular and Flexible Library for Reinforcement Learning},
 url = {http://jmlr.org/papers/v24/23-0112.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0130,
 abstract = {Continual learning is the problem of learning from a nonstationary stream of data, a fundamental issue for sustainable and efficient training of deep neural networks over time. Unfortunately, deep learning libraries only provide primitives for offline training, assuming that model's architecture and data are fixed. Avalanche is an open source library maintained by the ContinualAI non-profit organization that extends PyTorch by providing first-class support for dynamic architectures, streams of datasets, and incremental training and evaluation methods. Avalanche provides a large set of predefined benchmarks and training algorithms and it is easy to extend and modular while supporting a wide range of continual learning scenarios. Documentation is available at \url{https://avalanche.continualai.org}.},
 author = {Antonio Carta and Lorenzo Pellegrini and Andrea Cossu and Hamed Hemati and Vincenzo Lomonaco},
 code = {https://avalanche.continualai.org/},
 journal = {Journal of Machine Learning Research},
 number = {363},
 openalex = {W4319323699},
 pages = {1--6},
 pdf = {/papers/volume24/23-0130/23-0130.pdf},
 title = {Avalanche: A PyTorch Library for Deep Continual Learning},
 url = {http://jmlr.org/papers/v24/23-0130.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0191,
 abstract = {Recent years have witnessed the booming of various differentiable optimization algorithms. These algorithms exhibit different execution patterns, and their execution needs massive computational resources that go beyond a single CPU and GPU. Existing differentiable optimization libraries, however, cannot support efficient algorithm development and multi-CPU/GPU execution, making the development of differentiable optimization algorithms often cumbersome and expensive. This paper introduces TorchOpt, a PyTorch-based efficient library for differentiable optimization. TorchOpt provides a unified and expressive differentiable optimization programming abstraction. This abstraction allows users to efficiently declare and analyze various differentiable optimization programs with explicit gradients, implicit gradients, and zero-order gradients. TorchOpt further provides a high-performance distributed execution runtime. This runtime can fully parallelize computation-intensive differentiation operations (e.g. tensor tree flattening) on CPUs / GPUs and automatically distribute computation to distributed devices. Experimental results show that TorchOpt achieves $5.2\times$ training time speedup on an 8-GPU server. TorchOpt is available at: https://github.com/metaopt/torchopt/.},
 author = {Jie Ren* and Xidong Feng* and Bo Liu* and Xuehai Pan* and Yao Fu and Luo Mai and Yaodong Yang},
 code = {https://github.com/metaopt/torchopt},
 journal = {Journal of Machine Learning Research},
 number = {367},
 openalex = {W4309131813},
 pages = {1--14},
 pdf = {/papers/volume24/23-0191/23-0191.pdf},
 title = {TorchOpt: An Efficient Library for Differentiable Optimization},
 url = {http://jmlr.org/papers/v24/23-0191.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0300,
 abstract = {Hyperdimensional computing (HD), also known as vector symbolic architectures (VSA), is a framework for computing with distributed representations by exploiting properties of random high-dimensional vector spaces. The commitment of the scientific community to aggregate and disseminate research in this particularly multidisciplinary area has been fundamental for its advancement. Joining these efforts, we present Torchhd, a high-performance open source Python library for HD/VSA. Torchhd seeks to make HD/VSA more accessible and serves as an efficient foundation for further research and application development. The easy-to-use library builds on top of PyTorch and features state-of-the-art HD/VSA functionality, clear documentation, and implementation examples from well-known publications. Comparing publicly available code with their corresponding Torchhd implementation shows that experiments can run up to 100x faster. Torchhd is available at: https://github.com/hyperdimensional-computing/torchhd.},
 author = {Mike Heddes and Igor Nunes and Pere VergÃ©s and Denis Kleyko and Danny Abraham and Tony Givargis and Alexandru Nicolau and Alexander Veidenbaum},
 code = {https://github.com/hyperdimensional-computing/torchhd},
 journal = {Journal of Machine Learning Research},
 number = {255},
 openalex = {W4281260224},
 pages = {1--10},
 pdf = {/papers/volume24/23-0300/23-0300.pdf},
 title = {Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures},
 url = {http://jmlr.org/papers/v24/23-0300.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0378,
 abstract = {A significant challenge facing researchers in the area of multi-agent reinforcement learning (MARL) pertains to the identification of a library that can offer fast and compatible development for multi-agent tasks and algorithm combinations, while obviating the need to consider compatibility issues. In this paper, we present MARLlib, a library designed to address the aforementioned challenge by leveraging three key mechanisms: 1) a standardized multi-agent environment wrapper, 2) an agent-level algorithm implementation, and 3) a flexible policy mapping strategy. By utilizing these mechanisms, MARLlib can effectively disentangle the intertwined nature of the multi-agent task and the learning process of the algorithm, with the ability to automatically alter the training strategy based on the current task's attributes. The MARLlib library's source code is publicly accessible on GitHub: \url{https://github.com/Replicable-MARL/MARLlib}.},
 author = {Siyi Hu and Yifan Zhong and Minquan Gao and Weixun Wang and Hao Dong and Xiaodan Liang and Zhihui Li and Xiaojun Chang and Yaodong Yang},
 code = {https://github.com/Replicable-MARL/MARLlib},
 journal = {Journal of Machine Learning Research},
 number = {315},
 openalex = {W4307416208},
 pages = {1--23},
 pdf = {/papers/volume24/23-0378/23-0378.pdf},
 title = {MARLlib: A Scalable and Efficient Multi-agent Reinforcement Learning Library},
 url = {http://jmlr.org/papers/v24/23-0378.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0389,
 abstract = {Fairlearn is an open source project to help practitioners assess and improve fairness of artificial intelligence (AI) systems. The associated Python library, also named fairlearn, supports evaluation of a model's output across affected populations and includes several algorithms for mitigating fairness issues. Grounded in the understanding that fairness is a sociotechnical challenge, the project integrates learning resources that aid practitioners in considering a system's broader societal context.},
 author = {Hilde Weerts and Miroslav DudÃ­k and Richard Edgar and Adrin Jalali and Roman Lutz and Michael Madaio},
 code = {https://github.com/fairlearn/fairlearn},
 journal = {Journal of Machine Learning Research},
 number = {257},
 openalex = {W4361806824},
 pages = {1--8},
 pdf = {/papers/volume24/23-0389/23-0389.pdf},
 title = {Fairlearn: Assessing and Improving Fairness of AI Systems},
 url = {http://jmlr.org/papers/v24/23-0389.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v24:23-0795,
 abstract = {Recent neural network-based language models have benefited greatly from scaling up the size of training datasets and the number of parameters in the models themselves. Scaling can be complicated due to various factors including the need to distribute computation on supercomputer clusters (e.g., TPUs), prevent bottlenecks when infeeding data, and ensure reproducible results. In this work, we present two software libraries that ease these issues: $\texttt{t5x}$ simplifies the process of building and training large language models at scale while maintaining ease of use, and $\texttt{seqio}$ provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines. These open-source libraries have been used to train models with hundreds of billions of parameters on datasets with multiple terabytes of training data. Along with the libraries, we release configurations and instructions for T5-like encoder-decoder models as well as GPT-like decoder-only architectures. $\texttt{t5x}$ and $\texttt{seqio}$ are open source and available at https://github.com/google-research/t5x and https://github.com/google/seqio, respectively.},
 author = {Adam Roberts and Hyung Won Chung and Gaurav Mishra and Anselm Levskaya and James Bradbury and Daniel Andor and Sharan Narang and Brian Lester and Colin Gaffney and Afroz Mohiuddin and Curtis Hawthorne and Aitor Lewkowycz and Alex Salcianu and Marc van Zee and Jacob Austin and Sebastian Goodman and Livio Baldini Soares and Haitang Hu and Sasha Tsvyashchenko and Aakanksha Chowdhery and Jasmijn Bastings and Jannis Bulian and Xavier Garcia and Jianmo Ni and Andrew Chen and Kathleen Kenealy and Kehang Han and Michelle Casbon and Jonathan H. Clark and Stephan Lee and Dan Garrette and James Lee-Thorp and Colin Raffel and Noam Shazeer and Marvin Ritter and Maarten Bosma and Alexandre Passos and Jeremy Maitin-Shepard and Noah Fiedel and Mark Omernick and Brennan Saeta and Ryan Sepassi and Alexander Spiridonov and Joshua Newlan and Andrea Gesmundo},
 code = {https://github.com/google-research/t5x},
 journal = {Journal of Machine Learning Research},
 number = {377},
 openalex = {W4224442590},
 pages = {1--8},
 pdf = {/papers/volume24/23-0795/23-0795.pdf},
 title = {Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$},
 url = {http://jmlr.org/papers/v24/23-0795.html},
 volume = {24},
 year = {2023}
}

@article{JMLR:v25:22-0891,
 abstract = {Invariant and equivariant networks are useful in learning data with symmetry, including images, sets, point clouds, and graphs. In this paper, we consider invariant and equivariant networks for symmetries of finite groups. Invariant and equivariant networks have been constructed by various researchers using Reynolds operators. However, Reynolds operators are computationally expensive when the order of the group is large because they use the sum over the whole group, which poses an implementation difficulty. To overcome this difficulty, we consider representing the Reynolds operator as a sum over a subset instead of a sum over the whole group. We call such a subset a Reynolds design, and an operator defined by a sum over a Reynolds design a reductive Reynolds operator. For example, in the case of a graph with $n$ nodes, the computational complexity of the reductive Reynolds operator is reduced to $O(n^2)$, while the computational complexity of the Reynolds operator is $O(n!)$. We construct learning models based on the reductive Reynolds operator called equivariant and invariant Reynolds networks (ReyNets) and prove that they have universal approximation property. Reynolds designs for equivariant ReyNets are derived from combinatorial observations with Young diagrams, while Reynolds designs for invariant ReyNets are derived from invariants called Reynolds dimensions defined on the set of invariant polynomials. Numerical experiments show that the performance of our models is comparable to state-of-the-art methods.},
 author = {Akiyoshi Sannai and Makoto Kawano and Wataru Kumagai},
 code = {https://github.com/makora9143/ReyNet},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W3205662036},
 pages = {1--36},
 pdf = {/papers/volume25/22-0891/22-0891.pdf},
 title = {Equivariant and Invariant Reynolds Networks},
 url = {http://jmlr.org/papers/v25/22-0891.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0347,
 abstract = {In this article we consider the estimation of static parameters for partially observed diffusion process with discrete-time observations over a fixed time interval. In particular, we assume that one must time-discretize the partially observed diffusion process and work with the model with bias and consider maximizing the resulting log-likelihood. Using a novel double randomization scheme, based upon Markovian stochastic approximation we develop a new method to unbiasedly estimate the static parameters, that is, to obtain the maximum likelihood estimator with no time discretization bias. Under assumptions we prove that our estimator is unbiased and investigate the method in several numerical examples, showing that it can empirically out-perform existing unbiased methodology.},
 author = {Jeremy Heng and Jeremie Houssineau and Ajay Jasra},
 code = {https://github.com/jeremyhengjm/UnbiasedScore},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W4386908032},
 pages = {1--66},
 pdf = {/papers/volume25/23-0347/23-0347.pdf},
 title = {Unbiased Parameter Estimation for Partially Observed Diffusions},
 url = {http://jmlr.org/papers/v25/23-0347.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0572,
 author = {Runzhong Wang and Ziao Guo and Wenzheng Pan and Jiale Ma and Yikai Zhang and Nan Yang and Qi Liu and Longxuan Wei and Hanxue Zhang and Chang Liu and Zetian Jiang and Xiaokang Yang and Junchi Yan},
 code = {https://github.com/Thinklab-SJTU/pygmtools},
 journal = {Journal of Machine Learning Research},
 number = {33},
 pages = {1--7},
 pdf = {/papers/volume25/23-0572/23-0572.pdf},
 title = {Pygmtools: A Python Graph Matching Toolkit},
 url = {http://jmlr.org/papers/v25/23-0572.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0636,
 author = {Moritz Wolter and Felix Blanke and Jochen Garcke and Charles Tapley Hoyt},
 code = {https://github.com/v0lta/PyTorch-Wavelet-Toolbox},
 journal = {Journal of Machine Learning Research},
 number = {80},
 pages = {1--7},
 pdf = {/papers/volume25/23-0636/23-0636.pdf},
 title = {ptwt - The PyTorch Wavelet Toolbox},
 url = {http://jmlr.org/papers/v25/23-0636.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0970,
 abstract = {Causal discovery aims at revealing causal relations from observational data, which is a fundamental task in science and engineering. We describe $\textit{causal-learn}$, an open-source Python library for causal discovery. This library focuses on bringing a comprehensive collection of causal discovery methods to both practitioners and researchers. It provides easy-to-use APIs for non-specialists, modular building blocks for developers, detailed documentation for learners, and comprehensive methods for all. Different from previous packages in R or Java, $\textit{causal-learn}$ is fully developed in Python, which could be more in tune with the recent preference shift in programming languages within related communities. The library is available at https://github.com/py-why/causal-learn.},
 author = {Yujia Zheng and Biwei Huang and Wei Chen and Joseph Ramsey and Mingming Gong and Ruichu Cai and Shohei Shimizu and Peter Spirtes and Kun Zhang},
 code = {https://github.com/py-why/causal-learn},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W4385474105},
 pages = {1--8},
 pdf = {/papers/volume25/23-0970/23-0970.pdf},
 title = {Causal-learn: Causal Discovery in Python},
 url = {http://jmlr.org/papers/v25/23-0970.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1027,
 abstract = {QDax is an open-source library with a streamlined and modular API for Quality-Diversity (QD) optimization algorithms in Jax. The library serves as a versatile tool for optimization purposes, ranging from black-box optimization to continuous control. QDax offers implementations of popular QD, Neuroevolution, and Reinforcement Learning (RL) algorithms, supported by various examples. All the implementations can be just-in-time compiled with Jax, facilitating efficient execution across multiple accelerators, including GPUs and TPUs. These implementations effectively demonstrate the framework's flexibility and user-friendliness, easing experimentation for research purposes. Furthermore, the library is thoroughly documented and tested with 95\% coverage.},
 author = {Felix Chalumeau and Bryan Lim and Rapha{{\"e}}l Boige and Maxime Allard and Luca Grillotti and Manon Flageat and Valentin Mac{{\'e}} and Guillaume Richard and Arthur Flajolet and Thomas Pierrot and Antoine Cully},
 code = {https://github.com/adaptive-intelligent-robotics/QDax},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W4385682158},
 pages = {1--16},
 pdf = {/papers/volume25/23-1027/23-1027.pdf},
 title = {QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration},
 url = {http://jmlr.org/papers/v25/23-1027.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v9:corani08b,
 abstract = {JNCC2 implements the naive credal classifier 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classifications also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classifications (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license.},
 author = {Giorgio Corani and Marco Zaffalon},
 code = {https://people.idsia.ch/~giorgio/jncc2.html},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W2163913262},
 pages = {2695--2698},
 pdf = {/papers/volume9/corani08b/corani08b.pdf},
 title = {JNCC2: The Java Implementation Of Naive Credal Classifier 2},
 url = {http://jmlr.org/papers/v9/corani08b.html},
 volume = {9},
 year = {2008}
}

@article{JMLR:v9:fan08a,
 abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library ...},
 author = {Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin},
 code = {https://www.csie.ntu.edu.tw/~cjlin/liblinear/},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W3001645704},
 pages = {1871--1874},
 pdf = {/papers/volume9/fan08a/fan08a.pdf},
 title = {LIBLINEAR: A Library for Large Linear Classification},
 url = {http://jmlr.org/papers/v9/fan08a.html},
 volume = {9},
 year = {2008}
}

@article{JMLR:v9:igel08a,
 abstract = {Shark is a new data analysis system that marries query processing with complex analytics on large clusters. It leverages a novel distributed memory abstraction to provide a unified engine that can run SQL queries and sophisticated analytics functions (e.g. iterative machine learning) at scale, and efficiently recovers from failures mid-query. This allows Shark to run SQL queries up to 100X faster than Apache Hive, and machine learning programs more than 100X faster than Hadoop. Unlike previous systems, Shark shows that it is possible to achieve these speedups while retaining a MapReduce-like execution engine, and the fine-grained fault tolerance properties that such engine provides. It extends such an engine in several ways, including column-oriented in-memory storage and dynamic mid-query replanning, to effectively execute SQL. The result is a system that matches the speedups reported for MPP analytic databases over MapReduce, while offering fault tolerance properties and complex analytics capabilities that they lack.},
 author = {Christian Igel and Verena Heidrich-Meisner and Tobias Glasmachers},
 code = {https://github.com/Shark-ML/Shark/},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W2139072600},
 pages = {993--996},
 pdf = {/papers/volume9/igel08a/igel08a.pdf},
 title = {Shark},
 url = {http://jmlr.org/papers/v9/igel08a.html},
 volume = {9},
 year = {2008}
}

@article{JMLR:v9:klanke08a,
 abstract = {In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As t...},
 author = {Stefan Klanke and Sethu Vijayakumar and Stefan Schaal},
 code = {http://www.ipab.inf.ed.ac.uk/slmc/software/lwpr},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W3004439174},
 pages = {623--626},
 pdf = {/papers/volume9/klanke08a/klanke08a.pdf},
 title = {A Library for Locally Weighted Projection Regression},
 url = {http://jmlr.org/papers/v9/klanke08a.html},
 volume = {9},
 year = {2008}
}
