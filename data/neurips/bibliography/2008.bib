@inproceedings{NIPS2008_00411460,
 abstract = {This paper describes a recursive estimation procedure for multivariate binary densities (probability distributions of vectors of Bernoulli random variables) using orthogonal expansions. For $d$ covariates, there are $2^d$ basis coefficients to estimate, which renders conventional approaches computationally prohibitive when $d$ is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for flexible control of the trade-off between mean-squared error and computational complexity.},
 author = {Raginsky, Maxim and Lazebnik, Svetlana and Willett, Rebecca and Silva, Jorge},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/00411460f7c92d2124a67ea0f4cb5f85-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/00411460f7c92d2124a67ea0f4cb5f85-Metadata.json},
 openalex = {W2949377188},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Near-minimax recursive density estimation on the binary hypercube},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0060ef47,
 abstract = {This paper investigates a new machine learning strategy called translated learning. Unlike many previous learning tasks, we focus on how to use labeled data from one feature space to enhance the classification of other entirely different learning spaces. For example, we might wish to use labeled text data to help learn a model for classifying image data, when the labeled images are difficult to obtain. An important aspect of translated learning is to build a bridge to link one feature space (known as the space) to another space (known as the space) through a translator in order to migrate the knowledge from source to target. The translated learning solution uses a language model to link the class labels to the features in the source spaces, which in turn is translated to the features in the target spaces. Finally, this chain of linkages is completed by tracing back to the instances in the target spaces. We show that this path of linkage can be modeled using a Markov chain and risk minimization. Through experiments on the text-aided image classification and cross-language classification tasks, we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods.},
 author = {Dai, Wenyuan and Chen, Yuqiang and Xue, Gui-rong and Yang, Qiang and Yu, Yong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0060ef47b12160b9198302ebdb144dcf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0060ef47b12160b9198302ebdb144dcf-Metadata.json},
 openalex = {W2156940638},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0060ef47b12160b9198302ebdb144dcf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Translated Learning: Transfer Learning across Different Feature Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_006f52e9,
 abstract = {With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufficiently expressive for generalization while also allowing for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overfitting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modifications and that is polynomial in the size of the graph and the treewidth bound. At the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model's treewidth by at most one. We demonstrate the effectiveness of our treewidth-friendly method on several real-life datasets. Importantly, we also show that by using global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth.},
 author = {Elidan, Gal and Gould, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/006f52e9102a8d3be2fe5614f42ba989-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/006f52e9102a8d3be2fe5614f42ba989-Metadata.json},
 openalex = {W2220813089},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/006f52e9102a8d3be2fe5614f42ba989-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning Bounded Treewidth Bayesian Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/006f52e9102a8d3be2fe5614f42ba989-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_01161aaa,
 abstract = {Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian Process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other nonparametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and nu-SVR.},
 author = {Nguyen-tuong, Duy and Peters, Jan and Seeger, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Metadata.json},
 openalex = {W2172114485},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Local Gaussian Process Regression for Real Time Online Model Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/01161aaa0b6d1345dd8fe4e481144d84-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_013a006f,
 abstract = {Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To find matches for contours, we rely only on shape, which applies directly to all three modalities without modification, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identified to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a final grouping on contours in the original image while simultaneously finding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts.},
 author = {Srinivasan, Praveen and Wang, Liming and Shi, Jianbo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/013a006f03dbc5392effeb8f18fda755-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/013a006f03dbc5392effeb8f18fda755-Metadata.json},
 openalex = {W2101221075},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Grouping Contours Via a Related Image},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0141a8ae,
 abstract = {Sequential optimal design methods hold great promise for improving the efficiency of neurophysiology experiments. However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive field). Here we describe how to use stronger prior information, in the form of parametric models of the receptive field, in order to construct optimal stimuli and further improve the efficiency of our experiments. For example, if we believe that the receptive field is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc.) using as few experimental trials as possible. More generally, we may believe a priori that the receptive field lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. Applications to simulated and real data indicate that these methods may in many cases improve the experimental efficiency.},
 author = {Lewi, Jeremy and Butera, Robert and Schneider, David and Woolley, Sarah and Paninski, Liam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0141a8aedb1b53970fac7c81dac79fbe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0141a8aedb1b53970fac7c81dac79fbe-Metadata.json},
 openalex = {W2120583184},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0141a8aedb1b53970fac7c81dac79fbe-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0141a8aedb1b53970fac7c81dac79fbe-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_02a32ad2,
 abstract = {Almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem. We draw on recent work in nonparametric Bayesian statistics to define a rational model of human feature learning that forms a featural representation from raw sensory data without pre-specifying the number of features. By comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations, we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features.},
 author = {Griffiths, Thomas and Austerweil, Joseph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/02a32ad2669e6fe298e607fe7cc0e1a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/02a32ad2669e6fe298e607fe7cc0e1a0-Metadata.json},
 openalex = {W2126848884},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/02a32ad2669e6fe298e607fe7cc0e1a0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Analyzing human feature learning as nonparametric Bayesian inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/02a32ad2669e6fe298e607fe7cc0e1a0-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0336dcba,
 abstract = {We present cutoff averaging, a technique for converting any conservative online learning algorithm into a batch learning algorithm. Most online-to-batch conversion techniques work well with certain types of online learning algorithms and not with others, whereas cutoff averaging explicitly tries to adapt to the characteristics of the online algorithm being converted. An attractive property of our technique is that it preserves the efficiency of the original online algorithm, making it appropriate for large-scale learning problems. We provide a statistical analysis of our technique and back our theoretical claims with experimental results.},
 author = {Dekel, Ofer},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0336dcbab05b9d5ad24f4333c7658a0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0336dcbab05b9d5ad24f4333c7658a0e-Metadata.json},
 openalex = {W2134969063},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0336dcbab05b9d5ad24f4333c7658a0e-Supplemental.zip},
 title = {From Online to Batch Learning with Cutoff-Averaging},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_03c6b069,
 abstract = {Ranking is at the heart of many information retrieval applications. Unlike standard regression or classification in which we predict outputs independently, in ranking we are interested in predicting structured outputs so that misranking one object can significantly affect whether we correctly rank the other objects. In practice, the problem of ranking involves a large number of objects to be ranked and either approximate structured prediction methods are required, or assumptions of independence between object scores must be made in order to make the problem tractable. We present a probabilistic method for learning to rank using the graphical modelling framework of cumulative distribution networks (CDNs), where we can take into account the structure inherent to the problem of ranking by modelling the joint cumulative distribution functions (CDFs) over multiple pairwise preferences. We apply our framework to the problem of document retrieval in the case of the OHSUMED benchmark dataset. We will show that the RankNet, ListNet and ListMLE probabilistic models can be viewed as particular instances of CDNs and that our proposed framework allows for the exploration of a broad class of flexible structured loss functionals for learning to rank.},
 author = {Huang, Jim and Frey, Brendan J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/03c6b06952c750899bb03d998e631860-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/03c6b06952c750899bb03d998e631860-Metadata.json},
 openalex = {W2099994538},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/03c6b06952c750899bb03d998e631860-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Structured ranking learning using cumulative distribution networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/03c6b06952c750899bb03d998e631860-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_06997f04,
 abstract = {Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning algorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors independently perform Gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard LDA and HDP samplers, but with significant improvements in computation time and memory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processors. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced.},
 author = {Smyth, Padhraic and Welling, Max and Asuncion, Arthur},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/06997f04a7db92466a2baa6ebc8b872d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/06997f04a7db92466a2baa6ebc8b872d-Metadata.json},
 openalex = {W2107469355},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/06997f04a7db92466a2baa6ebc8b872d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Asynchronous Distributed Learning of Topic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/06997f04a7db92466a2baa6ebc8b872d-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_072b030b,
 abstract = {One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difficult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classification Models (CCM), where repeated instantiations of these classifiers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited black box interface with the models, allowing us to use very sophisticated, state-of-the-art classifiers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction.},
 author = {Heitz, Geremy and Gould, Stephen and Saxena, Ashutosh and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/072b030ba126b2f4b2374f342be9ed44-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/072b030ba126b2f4b2374f342be9ed44-Metadata.json},
 openalex = {W2116445618},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Cascaded Classification Models: Combining Models for Holistic Scene Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_07563a3f,
 abstract = {Identification and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many fields, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods.},
 author = {Calderhead, Ben and Girolami, Mark and Lawrence, Neil},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07563a3fe3bbe7e3ba84431ad9d055af-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07563a3fe3bbe7e3ba84431ad9d055af-Metadata.json},
 openalex = {W2104911495},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0777d5c1,
 abstract = {We provide a new analysis of an efficient margin-based algorithm for selective sampling in classification problems. Using the so-called Tsybakov low noise condition to parametrize the instance distribution, we show bounds on the convergence rate to the Bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm. Our analysis reveals that, excluding logarithmic factors, the average risk of the selective sampler converges to the Bayes risk at rate N-(1+α)(2+α)/2(3+α) where N denotes the number of queried labels, and α > 0 is the exponent in the low noise condition. For all α > √ - 1 ≈ 0.73 this convergence rate is asymptotically faster than the rate N-(1+α)/(2+α) achieved by the fully supervised version of the same classifier, which queries all labels, and for α → ∞ the two rates exhibit an exponential gap. Experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efficient competitors.},
 author = {Cavallanti, Giovanni and Cesa-bianchi, Nicol\`{o} and Gentile, Claudio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0777d5c17d4066b82ab86dff8a46af6f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0777d5c17d4066b82ab86dff8a46af6f-Metadata.json},
 openalex = {W2101132006},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0777d5c17d4066b82ab86dff8a46af6f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Linear Classification and Selective Sampling Under Low Noise Conditions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0777d5c17d4066b82ab86dff8a46af6f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_077e29b1,
 abstract = {Bartlett et al (2006) recently proved that a ground condition for convex surrogates, classification calibration, ties up the minimization of the surrogates and classification risks, and left as an important problem the algorithmic questions about the minimization of these surrogates. In this paper, we propose an algorithm which provably minimizes any classification calibrated surrogate strictly convex and differentiable — a set whose losses span the exponential, logistic and squared losses —, with boosting-type guaranteed convergence rates under a weak learning assumption. A particular subclass of these surrogates, that we call balanced convex surrogates, has a key rationale that ties it to maximum likelihood estimation, zero-sum games and the set of losses that satisfy some of the most common requirements for losses in supervised learning. We report experiments on more than 50 readily available domains of 11 flavors of the algorithm, that shed light on new surrogates, and the potential of data dependent strategies to tune surrogates.},
 author = {Nock, Richard and Nielsen, Frank},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/077e29b11be80ab57e1a2ecabb7da330-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/077e29b11be80ab57e1a2ecabb7da330-Metadata.json},
 openalex = {W2155964055},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/077e29b11be80ab57e1a2ecabb7da330-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On the Efficient Minimization of Classification Calibrated Surrogates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/077e29b11be80ab57e1a2ecabb7da330-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_07871915,
 author = {Singh, Aarti and Nowak, Robert and Zhu, Jerry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07871915a8107172b3b5dc15a6574ad3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07871915a8107172b3b5dc15a6574ad3-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Unlabeled data: Now it helps, now it doesn\textquotesingle t},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/07871915a8107172b3b5dc15a6574ad3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_07a96b1f,
 abstract = {Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a unified model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereffects. This unified model also makes the surprising prediction that force field adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We confirm this prediction with an experiment.},
 author = {Haith, Adrian and Jackson, Carl and Miall, R. and Vijayakumar, Sethu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07a96b1f61097ccb54be14d6a47439b0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07a96b1f61097ccb54be14d6a47439b0-Metadata.json},
 openalex = {W2145804579},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07a96b1f61097ccb54be14d6a47439b0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Unifying the Sensory and Motor Components of Sensorimotor Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/07a96b1f61097ccb54be14d6a47439b0-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_07c5807d,
 abstract = {Accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to define a Gaussian process model of human function learning that combines the strengths of both approaches.},
 author = {Griffiths, Thomas and Lucas, Chris and Williams, Joseph and Kalish, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07c5807d0d927dcd0980f86024e5208b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07c5807d0d927dcd0980f86024e5208b-Metadata.json},
 openalex = {W2166851712},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07c5807d0d927dcd0980f86024e5208b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Modeling human function learning with Gaussian processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/07c5807d0d927dcd0980f86024e5208b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_07cdfd23,
 abstract = {Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require data-structures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way.},
 author = {Nessler, Bernhard and Pfeiffer, Michael and Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07cdfd23373b17c6b337251c22b7ea57-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07cdfd23373b17c6b337251c22b7ea57-Metadata.json},
 openalex = {W2148623408},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Hebbian Learning of Bayes Optimal Decisions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/07cdfd23373b17c6b337251c22b7ea57-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_08740852,
 abstract = {We study the problem of domain transfer for a supervised classification task in mRNA splicing. We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. We find that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classification performance.},
 author = {Schweikert, Gabriele and R\"{a}tsch, Gunnar and Widmer, Christian and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/087408522c31eeb1f982bc0eaf81d35f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/087408522c31eeb1f982bc0eaf81d35f-Metadata.json},
 openalex = {W2129416674},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/087408522c31eeb1f982bc0eaf81d35f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/087408522c31eeb1f982bc0eaf81d35f-Supplemental.zip},
 title = {An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/087408522c31eeb1f982bc0eaf81d35f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_08b255a5,
 abstract = {We introduce a kernel-based method for change-point analysis within a sequence of temporal observations. Change-point analysis of an unlabelled sample of observations consists in, first, testing whether a change in the distribution occurs within the sample, and second, if a change occurs, estimating the change-point instant after which the distribution of the observations switches from one distribution to another different distribution. We propose a test statistic based upon the maximum kernel Fisher discriminant ratio as a measure of homogeneity between segments. We derive its limiting distribution under the null hypothesis (no change occurs), and establish the consistency under the alternative hypothesis (a change occurs). This allows to build a statistical hypothesis testing procedure for testing the presence of a change-point, with a prescribed false-alarm probability and detection probability tending to one in the large-sample setting. If a change actually occurs, the test statistic also yields an estimator of the change-point location. Promising experimental results in temporal segmentation of mental tasks from BCI data and pop song indexation are presented.},
 author = {Harchaoui, Za\"{\i}d and Moulines, Eric and Bach, Francis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/08b255a5d42b89b0585260b6f2360bdd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/08b255a5d42b89b0585260b6f2360bdd-Metadata.json},
 openalex = {W2112724113},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Kernel Change-point Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/08b255a5d42b89b0585260b6f2360bdd-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_08c5433a,
 abstract = {Most algorithms for solving Markov decision processes rely on a discount factor, which ensures their convergence. It is generally assumed that using an artificially low discount factor will improve the convergence rate, while sacrificing the solution quality. We however demonstrate that using an artificially low discount factor may significantly improve the solution quality, when used in approximate dynamic programming. We propose two explanations of this phenomenon. The first justification follows directly from the standard approximation error bounds: using a lower discount factor may decrease the approximation error bounds. However, we also show that these bounds are loose, thus their decrease does not entirely justify the improved solution quality. We thus propose another justification: when the rewards are received only sporadically (as in the case of Tetris), we can derive tighter bounds, which support a significant improvement in the solution quality with a decreased discount factor.},
 author = {Petrik, Marek and Scherrer, Bruno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/08c5433a60135c32e34f46a71175850c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/08c5433a60135c32e34f46a71175850c-Metadata.json},
 openalex = {W2124169341},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/08c5433a60135c32e34f46a71175850c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Biasing Approximate Dynamic Programming with a Lower Discount Factor},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/08c5433a60135c32e34f46a71175850c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0a113ef6,
 author = {Kim, Jooseuk and Scott, Clayton},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0a113ef6b61820daa5611c870ed8d5ee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0a113ef6b61820daa5611c870ed8d5ee-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Performance analysis for L\textbackslash \_2 kernel classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0a348ede,
 abstract = {Graph clustering methods such as spectral clustering are defined for general weighted graphs. In machine learning, however, data often is not given in form of a graph, but in terms of similarity (or distance) values between points. In this case, first a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph. In this paper we investigate the influence of the construction of the similarity graph on the clustering results. We first study the convergence of graph clustering criteria such as the normalized cut (Ncut) as the sample size tends to infinity. We find that the limit expressions are different for different types of graph, for example the r-neighborhood graph or the k-nearest neighbor graph. In plain words: Ncut on a kNN graph does something systematically different than Ncut on an r-neighborhood graph! This finding shows that graph clustering criteria cannot be studied independently of the kind of graph they are applied to. We also provide examples which show that these differences can be observed for toy and real data already for rather small sample sizes.},
 author = {Maier, Markus and Luxburg, Ulrike and Hein, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0a348ede8ac3768875037baca5de6e26-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0a348ede8ac3768875037baca5de6e26-Metadata.json},
 openalex = {W2120189049},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0a348ede8ac3768875037baca5de6e26-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Influence of graph construction on graph-based clustering measures},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0a348ede8ac3768875037baca5de6e26-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0c74b7f7,
 abstract = {Markov Decision Processes (MDPs) have been extensively studied and used in the context of planning and decision-making, and many methods exist to find the optimal policy for problems modelled as MDPs. Although finding the optimal policy is sufficient in many domains, in certain applications such as decision support systems where the policy is executed by a human (rather than a machine), finding all possible near-optimal policies might be useful as it provides more flexibility to the person executing the policy. In this paper we introduce the new concept of non-deterministic MDP policies, and address the question of finding near-optimal non-deterministic policies. We propose two solutions to this problem, one based on a Mixed Integer Program and the other one based on a search algorithm. We include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system.},
 author = {Fard, M. and Pineau, Joelle},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Metadata.json},
 openalex = {W2161717678},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {MDPs with Non-Deterministic Policies.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0d0871f0,
 abstract = {The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classification. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classification accuracy becomes a challenge. We introduce Semi-supervised Learning with Weakly-Related Unlabeled Data (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classification tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal word-correlation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of state-of-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a significant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.},
 author = {Yang, Liu and Jin, Rong and Sukthankar, Rahul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0d0871f0806eae32d30983b62252da50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0d0871f0806eae32d30983b62252da50-Metadata.json},
 openalex = {W2169415597},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0d0871f0806eae32d30983b62252da50-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0d0871f0806eae32d30983b62252da50-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0d7de1ac,
 abstract = {We present a simple new Monte Carlo algorithm for evaluating probabilities of observations in complex latent variable models, such as Deep Belief Networks. While the method is based on Markov chains, estimates based on short runs are formally unbiased. In expectation, the log probability of a test set will be underestimated, and this could form the basis of a probabilistic bound. The method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest Monte Carlo methods. We give examples of the new method substantially improving simple variational bounds at modest extra cost.},
 author = {Murray, Iain and Salakhutdinov, Russ R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0d7de1aca9299fe63f3e0041f02638a3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0d7de1aca9299fe63f3e0041f02638a3-Metadata.json},
 openalex = {W2157002241},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0d7de1aca9299fe63f3e0041f02638a3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Evaluating probabilities under high-dimensional latent variable models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0d7de1aca9299fe63f3e0041f02638a3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0dbb3fb9,
 abstract = {We show that an important and computationally challenging solution space feature of the graph coloring problem (COL), namely the number of clusters of solutions, can be accurately estimated by a technique very similar to one for counting the number of solutions. This cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the COL instance. Using a variant of the Belief Propagation inference framework, we can efficiently approximate cluster counts in random COL problems over a large range of graph densities. We illustrate the algorithm on instances with up to 100, 000 vertices. Moreover, we supply a methodology for computing the number of clusters exactly using advanced techniques from the knowledge compilation literature. This methodology scales up to several hundred variables.},
 author = {Kroc, Lukas and Sabharwal, Ashish and Selman, Bart},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0dbb3fb9a5cd1d5f8a9075b5bb8070aa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0dbb3fb9a5cd1d5f8a9075b5bb8070aa-Metadata.json},
 openalex = {W2103672714},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0dbb3fb9a5cd1d5f8a9075b5bb8070aa-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0dbb3fb9a5cd1d5f8a9075b5bb8070aa-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0e01938f,
 abstract = {We formulate the problem of bipartite graph inference as a supervised learning problem, and propose a new method to solve it from the viewpoint of distance metric learning. The method involves the learning of two mappings of the heterogeneous objects to a unified Euclidean space representing the network topology of the bipartite graph, where the graph is easy to infer. The algorithm can be formulated as an optimization problem in a reproducing kernel Hilbert space. We report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data.},
 author = {Yamanishi, Yoshihiro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0e01938fc48a2cfb5f2217fbfb00722d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0e01938fc48a2cfb5f2217fbfb00722d-Metadata.json},
 openalex = {W2111811203},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0e01938fc48a2cfb5f2217fbfb00722d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Supervised Bipartite Graph Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0e01938fc48a2cfb5f2217fbfb00722d-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0e65972d,
 abstract = {This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ∊ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions benefit from favorable theoretical guarantees. Our main result shows that, remarkably, for any fixed target function, there exists a distribution weighted combining rule that has a loss of at most ∊ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3∊. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.},
 author = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Metadata.json},
 openalex = {W2105523772},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Domain Adaptation with Multiple Sources},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0e65972dce68dad4d52d063967f0a705-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0efe3284,
 abstract = {Randomized neural networks are immortalized in this AI Koan:

In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.

What are you doing? asked Minsky. I am training a randomly wired neural net to play tic-tac-toe, Sussman replied. Why is the net wired randomly? asked Minsky. Sussman replied, I do not want it to have any preconceptions of how to play.

Minsky then shut his eyes. Why do you close your eyes? Sussman asked his teacher. So that the room will be empty, replied Minsky. At that moment, Sussman was enlightened.

We analyze shallow random networks with the help of concentration of measure inequalities. Specifically, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classification performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities.},
 author = {Rahimi, Ali and Recht, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Metadata.json},
 openalex = {W2123395972},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_0f840be9,
 abstract = {For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature's weight is penalized by the amount it differs from the average weight of its neighbors. For text classification, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge significantly improve prediction accuracy.},
 author = {Sandler, Ted and Blitzer, John and Talukdar, Partha and Ungar, Lyle},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0f840be9b8db4d3fbd5ba2ce59211f55-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0f840be9b8db4d3fbd5ba2ce59211f55-Metadata.json},
 openalex = {W2108423015},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Regularized Learning with Networks of Features},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/0f840be9b8db4d3fbd5ba2ce59211f55-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_12092a75,
 abstract = {CAPTCHAs are computer-generated tests that humans can pass but current computer systems cannot. CAPTCHAs provide a method for automatically distinguishing a human from a computer program, and therefore can protect Web services from abuse by so-called bots. Most CAPTCHAs consist of distorted images, usually text, for which a user must provide some description. Unfortunately, visual CAPTCHAs limit access to the millions of visually impaired people using the Web. Audio CAPTCHAs were created to solve this accessibility issue; however, the security of audio CAPTCHAs was never formally tested. Some visual CAPTCHAs have been broken using machine learning techniques, and we propose using similar ideas to test the security of audio CAPTCHAs. Audio CAPTCHAs are generally composed of a set of words to be identified, layered on top of noise. We analyzed the security of current audio CAPTCHAs from popular Web sites by using AdaBoost, SVM, and k-NN, and achieved correct solutions for test samples with accuracy up to 71%. Such accuracy is enough to consider these CAPTCHAs broken. Training several different machine learning algorithms on different types of audio CAPTCHAs allowed us to analyze the strengths and weaknesses of the algorithms so that we could suggest a design for a more robust audio CAPTCHA.},
 author = {Tam, Jennifer and Simsa, Jiri and Hyde, Sean and Ahn, Luis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/12092a75caa75e4644fd2869f0b6c45a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/12092a75caa75e4644fd2869f0b6c45a-Metadata.json},
 openalex = {W2171973097},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/12092a75caa75e4644fd2869f0b6c45a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Breaking Audio CAPTCHAs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/12092a75caa75e4644fd2869f0b6c45a-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_1385974e,
 abstract = {We address the problem of estimating the ratio of two probability density functions (a.k.a. the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efficient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efficient than competing approaches.},
 author = {Kanamori, Takafumi and Hido, Shohei and Sugiyama, Masashi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1385974ed5904a438616ff7bdb3f7439-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1385974ed5904a438616ff7bdb3f7439-Metadata.json},
 openalex = {W2170935389},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/1385974ed5904a438616ff7bdb3f7439-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_1458e750,
 abstract = {Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1) that promotes sparsity. We show how smoother priors can preserve the benefits of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efficiently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and find that online optimization of the parameters of the KL-regularized model can significantly improve prediction performance.},
 author = {Bagnell, J. and Bradley, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1458e7509aa5f47ecfb92536e7dd1dc7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1458e7509aa5f47ecfb92536e7dd1dc7-Metadata.json},
 openalex = {W2132283655},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1458e7509aa5f47ecfb92536e7dd1dc7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1458e7509aa5f47ecfb92536e7dd1dc7-Supplemental.zip},
 title = {Differentiable Sparse Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/1458e7509aa5f47ecfb92536e7dd1dc7-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_147ebe63,
 abstract = {Motivated by applications like elections, web-page ranking, revenue maximization etc., we consider the question of inferring popular rankings using constrained data. More specifically, we consider the problem of inferring a probability distribution over the group of permutations using its first order marginals. We first prove that it is not possible to recover more than O(n) permutations over n elements with the given information. We then provide a simple and novel algorithm that can recover up to O(n) permutations under a natural stochastic model; in this sense, the algorithm is optimal. In certain applications, the interest is in recovering only the most popular (or mode) ranking. As a second result, we provide an algorithm based on the Fourier Transform over the symmetric group to recover the mode under a natural majority condition; the algorithm turns out to be a maximum weight matching on an appropriately defined weighted bipartite graph. The questions considered are also thematically related to Fourier Transforms over the symmetric group and the currently popular topic of compressed sensing.},
 author = {Jagabathula, Srikanth and Shah, Devavrat},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/147ebe637038ca50a1265abac8dea181-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/147ebe637038ca50a1265abac8dea181-Metadata.json},
 openalex = {W2570779488},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/147ebe637038ca50a1265abac8dea181-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Inferring rankings under constrained sensing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/147ebe637038ca50a1265abac8dea181-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_149e9677,
 abstract = {We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network.},
 author = {Alvarez, Mauricio and Lawrence, Neil},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/149e9677a5989fd342ae44213df68868-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/149e9677a5989fd342ae44213df68868-Metadata.json},
 openalex = {W2143672530},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/149e9677a5989fd342ae44213df68868-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/149e9677a5989fd342ae44213df68868-Supplemental.zip},
 title = {Sparse Convolved Gaussian Processes for Multi-output Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/149e9677a5989fd342ae44213df68868-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_1595af64,
 author = {Gupta, Abhinav and Shi, Jianbo and Davis, Larry S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1595af6435015c77a7149e92a551338e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1595af6435015c77a7149e92a551338e-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1595af6435015c77a7149e92a551338e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A \textasciigrave \textasciigrave Shape Aware\textquotesingle \textquotesingle Model for semi-supervised Learning of Objects and its Context},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/1595af6435015c77a7149e92a551338e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_15d4e891,
 abstract = {The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneficial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks.},
 author = {Williams, Christopher and Klanke, Stefan and Vijayakumar, Sethu and Chai, Kian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/15d4e891d784977cacbfcbb00c48f133-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/15d4e891d784977cacbfcbb00c48f133-Metadata.json},
 openalex = {W2125650133},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multi-task Gaussian Process Learning of Robot Inverse Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/15d4e891d784977cacbfcbb00c48f133-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_1651cf0d,
 abstract = {Accurate and efficient inference in evolutionary trees is a central problem in computational biology. While classical treatments have made unrealistic site independence assumptions, ignoring insertions and deletions, realistic approaches require tracking insertions and deletions along the phylogenetic tree—a challenging and unsolved computational problem. We propose a new ancestry resampling procedure for inference in evolutionary trees. We evaluate our method in two problem domains—multiple sequence alignment and reconstruction of ancestral sequences—and show substantial improvement over the current state of the art.},
 author = {Bouchard-c\^{o}t\'{e}, Alexandre and Klein, Dan and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1651cf0d2f737d7adeab84d339dbabd3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1651cf0d2f737d7adeab84d339dbabd3-Metadata.json},
 openalex = {W2124613178},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1651cf0d2f737d7adeab84d339dbabd3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Inference in Phylogenetic InDel Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/1651cf0d2f737d7adeab84d339dbabd3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_1679091c,
 abstract = {We introduce a novel framework for estimating vector fields using sparse basis field expansions (S-FLEX). The notion of basis fields, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector field, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that significantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art.},
 author = {Haufe, Stefan and Nikulin, Vadim and Ziehe, Andreas and M\"{u}ller, Klaus-Robert and Nolte, Guido},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1679091c5a880faf6fb5e6087eb1b2dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1679091c5a880faf6fb5e6087eb1b2dc-Metadata.json},
 openalex = {W2155431465},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1679091c5a880faf6fb5e6087eb1b2dc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Estimating vector fields using sparse basis field expansions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/1679091c5a880faf6fb5e6087eb1b2dc-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_170c9449,
 abstract = {We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are averaged over fixed intervals whereas the events causing data artifacts may occur at any time and often have durations significantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables detection of artifacts and accurate estimation of the underlying blood pressure values. Our model's performance identifying artifacts is superior to two other classifiers' and about as good as a physician's.},
 author = {Aleks, Norm and Russell, Stuart J and Madden, Michael and Morabito, Diane and Staudenmayer, Kristan and Cohen, Mitchell and Manley, Geoffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/170c944978496731ba71f34c25826a34-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/170c944978496731ba71f34c25826a34-Metadata.json},
 openalex = {W2120141618},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/170c944978496731ba71f34c25826a34-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Probabilistic detection of short events, with application to critical care monitoring},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/170c944978496731ba71f34c25826a34-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_185e65bc,
 abstract = {Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance.},
 author = {Huang, Ling and Yan, Donghui and Taft, Nina and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/185e65bc40581880c4f2c82958de8cfe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/185e65bc40581880c4f2c82958de8cfe-Metadata.json},
 openalex = {W2146407935},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/185e65bc40581880c4f2c82958de8cfe-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/185e65bc40581880c4f2c82958de8cfe-Supplemental.zip},
 title = {Spectral Clustering with Perturbed Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/185e65bc40581880c4f2c82958de8cfe-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_19b65066,
 abstract = {The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number, location, and orientation of these sources remains a challenging task, one that is significantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efficacy of this approach.},
 author = {Owen, Julia and Attias, Hagai and Sekihara, Kensuke and Nagarajan, Srikantan and Wipf, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/19b650660b253761af189682e03501dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/19b650660b253761af189682e03501dd-Metadata.json},
 openalex = {W2156802517},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/19b650660b253761af189682e03501dd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/19b650660b253761af189682e03501dd-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_1be3bc32,
 abstract = {We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem:

• Heuristic methods such as gradient descent that only find a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution.

• Convex relaxation such as L1-regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality.

This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-L1 regularization. Our performance bound shows that the procedure is superior to the standard L1 convex relaxation for learning sparse targets. Experiments confirm the effectiveness of this method on some simulation and real data.},
 author = {Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1be3bc32e6564055d5ca3e5a354acbef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1be3bc32e6564055d5ca3e5a354acbef-Metadata.json},
 openalex = {W2121275167},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multi-stage Convex Relaxation for Learning with Sparse Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/1be3bc32e6564055d5ca3e5a354acbef-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_1ce927f8,
 abstract = {We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (≈ 2mm cube) of brain tissue. Our model, which we call the V-SPAM model, is based on the reasonable assumption that fMRI measurements reflect the (possibly nonlinearly) pooled, rectified output of a large population of simple and complex cells in V1. It has a hierarchical filtering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive fields, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the V-SPAM model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities.},
 author = {Vu, Vincent Q and Yu, Bin and Naselaris, Thomas and Kay, Kendrick and Gallant, Jack and Ravikumar, Pradeep},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1ce927f875864094e3906a4a0b5ece68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1ce927f875864094e3906a4a0b5ece68-Metadata.json},
 openalex = {W2114000517},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/1ce927f875864094e3906a4a0b5ece68-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_1e056d2b,
 abstract = {Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.},
 author = {Mnih, Andriy and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1e056d2b0ebd5c878c550da6ac5d3724-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1e056d2b0ebd5c878c550da6ac5d3724-Metadata.json},
 openalex = {W2131462252},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Scalable Hierarchical Distributed Language Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_1fc21400,
 abstract = {Recently, supervised dimensionality reduction has been gaining attention, owing to the realization that data labels are often available and indicate important underlying structure in the data. In this paper, we present a novel convex supervised dimensionality reduction approach based on exponential family PCA, which is able to avoid the local optima of typical EM learning. Moreover, by introducing a sample-based approximation to exponential family models, it overcomes the limitation of the prevailing Gaussian assumptions of standard PCA, and produces a kernelized formulation for nonlinear supervised dimensionality reduction. A training algorithm is then devised based on a subgradient bundle method, whose scalability can be gained using a coordinate descent procedure. The advantage of our global optimization approach is demonstrated by empirical results over both synthetic and real data.},
 author = {Guo, Yuhong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1fc214004c9481e4c8073e85323bfd4b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1fc214004c9481e4c8073e85323bfd4b-Metadata.json},
 openalex = {W2136337599},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/1fc214004c9481e4c8073e85323bfd4b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Supervised Exponential Family Principal Component Analysis via Convex Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/1fc214004c9481e4c8073e85323bfd4b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_2291d2ec,
 abstract = {Stochastic relational models (SRMs) [15] provide a rich family of choices for learning and predicting dyadic data between two sets of entities. The models generalize matrix factorization to a supervised learning problem that utilizes attributes of entities in a hierarchical Bayesian framework. Previously variational Bayes inference was applied for SRMs, which is, however, not scalable when the size of either entity set grows to tens of thousands. In this paper, we introduce a Markov chain Monte Carlo (MCMC) algorithm for equivalent models of SRMs in order to scale the computation to very large dyadic data sets. Both superior scalability and predictive accuracy are demonstrated on a collaborative filtering problem, which involves tens of thousands users and half million items.},
 author = {Zhu, Shenghuo and Yu, Kai and Gong, Yihong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Metadata.json},
 openalex = {W2139828520},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Stochastic Relational Models for Large-scale Dyadic Data using MCMC},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_23ce1851,
 abstract = {We describe a new content publishing system that selects articles to serve to a user, choosing from an editorially programmed pool that is frequently refreshed. It is now deployed on a major Yahoo! portal, and selects articles to serve to hundreds of millions of user visits per day, significantly increasing the number of user clicks over the original manual approach, in which editors periodically selected articles to display. Some of the challenges we face include a dynamic content pool, short article lifetimes, non-stationary click-through rates, and extremely high traffic volumes. The fundamental problem we must solve is to quickly identify which items are popular (perhaps within different user segments), and to exploit them while they remain current. We must also explore the underlying pool constantly to identify promising alternatives, quickly discarding poor performers. Our approach is based on tracking per article performance in near real time through online models. We describe the characteristics and constraints of our application setting, discuss our design choices, and show the importance and effectiveness of coupling online models with a randomization procedure. We discuss the challenges encountered in a production online content-publishing environment and highlight issues that deserve careful attention. Our analysis of this application also suggests a number of future research avenues.},
 author = {Agarwal, Deepak and Chen, Bee-chung and Elango, Pradheep and Motgi, Nitin and Park, Seung-taek and Ramakrishnan, Raghu and Roy, Scott and Zachariah, Joe},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/23ce1851341ec1fa9e0c259de10bf87c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/23ce1851341ec1fa9e0c259de10bf87c-Metadata.json},
 openalex = {W2099586447},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Online Models for Content Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/23ce1851341ec1fa9e0c259de10bf87c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_24681928,
 abstract = {Lasso, or <i xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">l</i> <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">1</sup> regularized least squares, has been explored extensively for its remarkable sparsity properties. In this paper it is shown that the solution to Lasso, in addition to its sparsity, has robustness properties: it is the solution to a robust optimization problem. This has two important consequences. First, robustness provides a connection of the regularizer to a physical property, namely, protection from noise. This allows a principled selection of the regularizer, and in particular, generalizations of Lasso that also yield convex optimization problems are obtained by considering different uncertainty sets. Second, robustness can itself be used as an avenue for exploring different properties of the solution. In particular, it is shown that robustness of the solution explains why the solution is sparse. The analysis as well as the specific results obtained differ from standard sparsity results, providing different geometric intuition. Furthermore, it is shown that the robust optimization formulation is related to kernel density estimation, and based on this approach, a proof that Lasso is consistent is given, using robustness directly. Finally, a theorem is proved which states that sparsity and algorithmic stability contradict each other, and hence Lasso is not stable.},
 author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/24681928425f5a9133504de568f5f6df-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/24681928425f5a9133504de568f5f6df-Metadata.json},
 openalex = {W2117173631},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/24681928425f5a9133504de568f5f6df-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Robust Regression and Lasso},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/24681928425f5a9133504de568f5f6df-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_24b16fed,
 abstract = {We develop new techniques for time series classification based on hierarchical Bayesian generative models (called mixed-effect models) and the Fisher kernel derived from them. A key advantage of the new formulation is that one can compute the Fisher information matrix despite varying sequence lengths and varying sampling intervals. This avoids the commonly-used ad hoc replacement of the Fisher information matrix with the identity which destroys the geometric invariance of the kernel. Our construction retains the geometric invariance, resulting in a kernel that is properly invariant under change of coordinates in the model parameter space. Experiments on detecting cognitive decline show that classifiers based on the proposed kernel out-perform those based on generative models and other feature extraction routines, and on Fisher kernels that use the identity in place of the Fisher information.},
 author = {Lu, Zhengdong and Kaye, Jeffrey and Leen, Todd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/24b16fede9a67c9251d3e7c7161c83ac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/24b16fede9a67c9251d3e7c7161c83ac-Metadata.json},
 openalex = {W2108598236},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/24b16fede9a67c9251d3e7c7161c83ac-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Hierarchical Fisher Kernels for Longitudinal Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/24b16fede9a67c9251d3e7c7161c83ac-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_26e359e8,
 abstract = {We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efficient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%-8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically significant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically significant.},
 author = {Tam, Yik-cheung and Schultz, Tanja},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/26e359e83860db1d11b6acca57d8ea88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/26e359e83860db1d11b6acca57d8ea88-Metadata.json},
 openalex = {W2113500964},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/26e359e83860db1d11b6acca57d8ea88-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Correlated Bigram LSA for Unsupervised Language Model Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/26e359e83860db1d11b6acca57d8ea88-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_2723d092,
 abstract = {We introduce a new probability distribution over a potentially infinite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the infinite factorial hidden Markov model can be used for blind source separation.},
 author = {Gael, Jurgen and Teh, Yee and Ghahramani, Zoubin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2723d092b63885e0d7c260cc007e8b9d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2723d092b63885e0d7c260cc007e8b9d-Metadata.json},
 openalex = {W2122418603},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2723d092b63885e0d7c260cc007e8b9d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Infinite Factorial Hidden Markov Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/2723d092b63885e0d7c260cc007e8b9d-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_287e03db,
 abstract = {Compressive Sensing (CS) combines sampling and compression into a single sub-Nyquist linear measurement process for sparse and compressible signals. In this paper, we extend the theory of CS to include signals that are concisely represented in terms of a graphical model. In particular, we use Markov Random Fields (MRFs) to represent sparse signals whose nonzero coefficients are clustered. Our new model-based recovery algorithm, dubbed Lattice Matching Pursuit (LaMP), stably recovers MRF-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms.},
 author = {Cevher, Volkan and Duarte, Marco and Hegde, Chinmay and Baraniuk, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/287e03db1d99e0ec2edb90d079e142f3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/287e03db1d99e0ec2edb90d079e142f3-Metadata.json},
 openalex = {W2170844819},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sparse Signal Recovery Using Markov Random Fields},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/287e03db1d99e0ec2edb90d079e142f3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_288cc0ff,
 abstract = {A novel center-based clustering algorithm is proposed in this paper. We first formulate clustering as an NP-hard linear integer program and we then use linear programming and the duality theory to derive the solution of this optimization problem. This leads to an efficient and very general algorithm, which works in the dual domain, and can cluster data based on an arbitrary set of distances. Despite its generality, it is independent of initialization (unlike EM-like methods such as K-means), has guaranteed convergence, can automatically determine the number of clusters, and can also provide online optimality bounds about the quality of the estimated clustering solutions. To deal with the most critical issue in a center-based clustering algorithm (selection of cluster centers), we also introduce the notion of stability of a cluster center, which is a well defined LP-based quantity that plays a key role to our algorithm's success. Furthermore, we also introduce, what we call, the margins (another key ingredient in our algorithm), which can be roughly thought of as dual counterparts to stabilities and allow us to obtain computationally efficient approximations to the latter. Promising experimental results demonstrate the potentials of our method.},
 author = {Komodakis, Nikos and Paragios, Nikos and Tziritas, Georgios},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/288cc0ff022877bd3df94bc9360b9c5d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/288cc0ff022877bd3df94bc9360b9c5d-Metadata.json},
 openalex = {W2168407575},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/288cc0ff022877bd3df94bc9360b9c5d-Supplemental.zip},
 title = {Clustering via LP-based Stabilities},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_2a271795,
 abstract = {This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer.},
 author = {Yang, Zhi and Zhao, Qi and Liu, Wentai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2a2717956118b4d223ceca17ce3865e2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2a2717956118b4d223ceca17ce3865e2-Metadata.json},
 openalex = {W2168129892},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2a2717956118b4d223ceca17ce3865e2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Spike Feature Extraction Using Informative Samples},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/2a2717956118b4d223ceca17ce3865e2-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_2a9d121c,
 abstract = {Multi-level hierarchical models provide an attractive framework for incorporating correlations induced in a response variable that is organized hierarchically. Model fitting is challenging, especially for a hierarchy with a large number of nodes. We provide a novel algorithm based on a multi-scale Kalman filter that is both scalable and easy to implement. For Gaussian response, we show our method provides the maximum a-posteriori (MAP) parameter estimates; for non-Gaussian response, parameter estimation is performed through a Laplace approximation. However, the Laplace approximation provides biased parameter estimates that is corrected through a parametric bootstrap procedure. We illustrate through simulation studies and analyses of real world data sets in health care and online advertising.},
 author = {Zhang, Liang and Agarwal, Deepak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Metadata.json},
 openalex = {W2140160814},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fast Computation of Posterior Mode in Multi-Level Hierarchical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_2ab56412,
 abstract = {A crucial part of developing mathematical models of information processing in the brain is the quantification of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modification of the traditional formula that significantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overfitting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and find that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise.},
 author = {Haefner, Ralf and Cumming, Bruce},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2ab56412b1163ee131e1246da0955bd1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2ab56412b1163ee131e1246da0955bd1-Metadata.json},
 openalex = {W2136012330},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {An improved estimator of Variance Explained in the presence of noise.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/2ab56412b1163ee131e1246da0955bd1-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_2bf283c0,
 abstract = {We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis.},
 author = {Rai, Piyush and Daume, Hal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2bf283c05b601f21364d052ca0ec798d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2bf283c05b601f21364d052ca0ec798d-Metadata.json},
 openalex = {W2166230915},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2bf283c05b601f21364d052ca0ec798d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Infinite Hierarchical Factor Regression Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/2bf283c05b601f21364d052ca0ec798d-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_2dace78f,
 abstract = {In this paper we aim to train deep neural networks for rapid visual recognition. The task is highly challenging, largely due to the lack of a meaningful regular-izer on the functions realized by the networks. We propose a novel regularization method that takes advantage of kernel methods, where an oracle kernel function represents prior knowledge about the recognition task of interest. We derive an efficient algorithm using stochastic gradient descent, and demonstrate encouraging results on a wide range of recognition tasks, in terms of both accuracy and speed.},
 author = {Yu, Kai and Xu, Wei and Gong, Yihong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2dace78f80bc92e6d7493423d729448e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2dace78f80bc92e6d7493423d729448e-Metadata.json},
 openalex = {W2102116870},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Deep Learning with Kernel Regularization for Visual Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/2dace78f80bc92e6d7493423d729448e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_2e2c080d,
 abstract = {We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the first layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the first layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in VI and MT. In addition, the model demonstrates how feedback from higher levels can influence representations at lower levels as a by-product of inference in a graphical model.},
 author = {Cadieu, Charles and Olshausen, Bruno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2e2c080d5490760af59d0baf5acbb84e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2e2c080d5490760af59d0baf5acbb84e-Metadata.json},
 openalex = {W2163202312},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2e2c080d5490760af59d0baf5acbb84e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/2e2c080d5490760af59d0baf5acbb84e-Supplemental.zip},
 title = {Learning Transformational Invariants from Natural Movies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/2e2c080d5490760af59d0baf5acbb84e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_30bb3825,
 abstract = {This paper is devoted to thoroughly investigating how to the ROC curve, a widely used visual tool for evaluating the accuracy of test/scoring statistics in the bipartite setup. The issue of confidence bands for the ROC curve is considered and a resampling procedure based on a smooth version of the empirical distribution called the smoothed bootstrap is introduced. Theoretical arguments and simulation results are presented to show that the smoothed bootstrap is preferable to a naive in order to construct accurate confidence bands.},
 author = {Bertail, Patrice and Cl\'{e}men\c{c}con, St\'{e}phan and Vayatis, Nicolas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/30bb3825e8f631cc6075c0f87bb4978c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/30bb3825e8f631cc6075c0f87bb4978c-Metadata.json},
 openalex = {W2139892284},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/30bb3825e8f631cc6075c0f87bb4978c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On Bootstrapping the ROC Curve},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/30bb3825e8f631cc6075c0f87bb4978c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_31b3b31a,
 abstract = {The Singular Value Decomposition is a key operation in many machine learning methods. Its computational cost, however, makes it unscalable and impractical for applications involving large datasets or real-time responsiveness, which are becoming increasingly common. We present a new method, QUIC-SVD, for fast approximation of the whole-matrix SVD based on a new sampling mechanism called the cosine tree. Our empirical tests show speedups of several orders of magnitude over exact SVD. Such scalability should enable QUIC-SVD to accelerate and enable a wide array of SVD-based methods and applications.},
 author = {Holmes, Michael and Jr. Isbell and Lee, Charles and Gray, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/31b3b31a1c2f8a370206f111127c0dbd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/31b3b31a1c2f8a370206f111127c0dbd-Metadata.json},
 openalex = {W2168430922},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {QUIC-SVD: Fast SVD Using Cosine Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/31b3b31a1c2f8a370206f111127c0dbd-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_31fefc0e,
 abstract = {We present a new, massively parallel architecture for accelerating machine learning algorithms, based on arrays of vector processing elements (VPEs) with variable-resolution arithmetic. Groups of VPEs operate in SIMD (single instruction multiple data) mode, and each group is connected to an independent memory bank. The memory bandwidth thus scales with the number of VPEs, while the main data flows are local, keeping power dissipation low. With 256 VPEs, implemented on two FPGAs (field programmable gate array) chips, we obtain a sustained speed of 19 GMACS (billion multiply-accumulate per sec.) for SVM training, and 86 GMACS for SVM classification. This performance is more than an order of magnitude higher than that of any FPGA implementation reported so far. The speed on one FPGA is similar to the fastest speeds published on a Graphics Processor for the MNIST problem, despite a clock rate that is an order of magnitude lower. Tests with Convolutional Neural Networks show similar compute performances. This massively parallel architecture is particularly attractive for embedded applications, where low power dissipation is critical.},
 author = {Graf, Hans and Cadambi, Srihari and Jakkula, Venkata and Sankaradass, Murugan and Cosatto, Eric and Chakradhar, Srimat and Dourdanovic, Igor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/31fefc0e570cb3860f2a6d4b38c6490d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/31fefc0e570cb3860f2a6d4b38c6490d-Metadata.json},
 openalex = {W2153387583},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Massively Parallel Digital Learning Processor},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_322f6246,
 abstract = {We explore a recently proposed mixture model approach to understanding interactions between conflicting sensory cues. Alternative model formulations, differing in their sensory noise models and inference methods, are compared based on their fit to experimental data. Heavy-tailed sensory likelihoods yield a better description of the subjects’ response behavior than standard Gaussian noise models. We study the underlying cause for this result, and then present several testable predictions of these models.},
 author = {Natarajan, Rama and Murray, Iain and Shams, Ladan and Zemel, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/322f62469c5e3c7dc3e58f5a4d1ea399-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/322f62469c5e3c7dc3e58f5a4d1ea399-Metadata.json},
 openalex = {W2169801915},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/322f62469c5e3c7dc3e58f5a4d1ea399-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Characterizing response behavior in multisensory perception with conflicting cues},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/322f62469c5e3c7dc3e58f5a4d1ea399-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_335f5352,
 abstract = {We present the Gaussian Process Density Sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a fixed density function that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We can also infer the hyperparameters of the Gaussian process. We compare this density modeling technique to several existing techniques on a toy problem and a skull-reconstruction task.},
 author = {Murray, Iain and MacKay, David and Adams, Ryan P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/335f5352088d7d9bf74191e006d8e24c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/335f5352088d7d9bf74191e006d8e24c-Metadata.json},
 openalex = {W2141742840},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/335f5352088d7d9bf74191e006d8e24c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Gaussian Process Density Sampler},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/335f5352088d7d9bf74191e006d8e24c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_3416a75f,
 abstract = {Detecting underlying clusters from large-scale data plays a central role in machine learning research. In this paper, we tackle the problem of clustering complex data of multiple distributions and multiple scales. To this end, we develop an algorithm named Zeta l-links (Zell) which consists of two parts: Zeta merging with a similarity graph and an initial set of small clusters derived from local l-links of samples. More specifically, we propose to structurize a cluster using cycles in the associated subgraph. A new mathematical tool, Zeta function of a graph, is introduced for the integration of all cycles, leading to a structural descriptor of a cluster in determinantal form. The popularity character of a cluster is conceptualized as the global fusion of variations of such a structural descriptor by means of the leave-one-out strategy in the cluster. Zeta merging proceeds, in the hierarchical agglomerative fashion, according to the maximum incremental popularity among all pairwise clusters. Experiments on toy data clustering, imagery pattern clustering, and image segmentation show the competitive performance of Zell. The 98.1% accuracy, in the sense of the normalized mutual information (NMI), is obtained on the FRGC face data of 16028 samples and 466 facial clusters.},
 author = {Zhao, Deli and Tang, Xiaoou},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3416a75f4cea9109507cacd8e2f2aefc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3416a75f4cea9109507cacd8e2f2aefc-Metadata.json},
 openalex = {W2139437550},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Cyclizing Clusters via Zeta Function of a Graph},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_37bc2f75,
 abstract = {In many domains, data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset. While there are asymptotically correct, informative algorithms for discovering causal relationships from a single dataset, even with missing values and hidden variables, there have been no such reliable procedures for distributed data with overlapping variables. We present a novel, asymptotically correct procedure that discovers a minimal equivalence class of causal DAG structures using local independence information from distributed data of this form and evaluate its performance using synthetic and real-world data against causal discovery algorithms for single datasets and applying Structural EM, a heuristic DAG structure learning procedure for data with missing values, to the concatenated data.},
 author = {Danks, David and Glymour, Clark and Tillman, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/37bc2f75bf1bcfe8450a1a41c200364c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/37bc2f75bf1bcfe8450a1a41c200364c-Metadata.json},
 openalex = {W2154323970},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Integrating Locally Learned Causal Structures with Overlapping Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_38913e1d,
 abstract = {We address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets. Recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent. Models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientific interest in species such as human and mouse. We present Brownian Factor Phylogenetic Analysis, a statistical model that makes a number of significant extensions to previous models to enable characterization of changes in expression among highly complex organisms. We demonstrate the efficacy of our method on a microarray dataset profiling diverse tissues from multiple vertebrate species. We anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects.},
 author = {Quon, Gerald and Teh, Yee and Chan, Esther and Hughes, Timothy and Brudno, Michael and Morris, Quaid},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/38913e1d6a7b94cb0f55994f679f5956-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/38913e1d6a7b94cb0f55994f679f5956-Metadata.json},
 openalex = {W2171170496},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/38913e1d6a7b94cb0f55994f679f5956-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A mixture model for the evolution of gene expression in non-homogeneous datasets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/38913e1d6a7b94cb0f55994f679f5956-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_38af8613,
 abstract = {It has been shown that the problem of l1-penalized least-square regression commonly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that are sparse and therefore achieves model selection. We propose in this paper RecLasso, an algorithm to solve the Lasso with online (sequential) observations. We introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point. We compare our method to Lars and Coordinate Descent, and present an application to compressive sensing with sequential observations. Our approach can easily be extended to compute an homotopy from the current solution to the solution that corresponds to removing a data point, which leads to an efficient algorithm for leave-one-out cross-validation. We also propose an algorithm to automatically update the regularization parameter after observing a new data point.},
 author = {Garrigues, Pierre and Ghaoui, Laurent},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/38af86134b65d0f10fe33d30dd76442e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/38af86134b65d0f10fe33d30dd76442e-Metadata.json},
 openalex = {W2115314650},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {An Homotopy Algorithm for the Lasso with Online Observations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/38af86134b65d0f10fe33d30dd76442e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_38b3eff8,
 abstract = {In recent work Long and Servedio [LS05] presented a algorithm that works by constructing a branching program over weak classifiers and has a simple analysis based on elementary properties of random walks. [LS05] showed that this martingale booster can tolerate random classification noise when it is run with a noise-tolerant weak learner; however, a drawback of the algorithm is that it is not adaptive, i.e. it cannot effectively take advantage of variation in the quality of the weak classifiers it receives.

We present an adaptive variant of the martingale boosting algorithm. This adaptiveness is achieved by modifying the original algorithm so that the random walks that arise in its analysis have different step size depending on the quality of the weak learner at each stage. The new algorithm inherits the desirable properties of the original [LS05] algorithm, such as random classification noise tolerance, and has other advantages besides adaptiveness: it requires polynomially fewer calls to the weak learner than the original algorithm, and it can be used with confidence-rated weak hypotheses that output real values rather than Boolean predictions.},
 author = {Long, Phil and Servedio, Rocco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/38b3eff8baf56627478ec76a704e9b52-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/38b3eff8baf56627478ec76a704e9b52-Metadata.json},
 openalex = {W2151097972},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/38b3eff8baf56627478ec76a704e9b52-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Martingale Boosting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/38b3eff8baf56627478ec76a704e9b52-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_39059724,
 abstract = {We propose a new fast Gaussian summation algorithm for high-dimensional datasets with high accuracy. First, we extend the original fast multipole-type methods to use approximation schemes with both hard and probabilistic error. Second, we utilize a new data structure called subspace tree which maps each data point in the node to its lower dimensional mapping as determined by any linear dimension reduction method such as PCA. This new data structure is suitable for reducing the cost of each pairwise distance computation, the most dominant cost in many kernel methods. Our algorithm guarantees probabilistic relative error on each kernel sum, and can be applied to high-dimensional Gaussian summations which are ubiquitous inside many kernel methods as the key computational bottleneck. We provide empirical speedup results on low to high-dimensional datasets up to 89 dimensions.},
 author = {Lee, Dongryeol and Gray, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/39059724f73a9969845dfe4146c5660e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/39059724f73a9969845dfe4146c5660e-Metadata.json},
 openalex = {W2128900799},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/39059724f73a9969845dfe4146c5660e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/39059724f73a9969845dfe4146c5660e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_3dc4876f,
 abstract = {In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-of-the-art techniques.},
 author = {Faivishevsky, Lev and Goldberger, Jacob},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3dc4876f3f08201c7c76cb71fa1da439-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3dc4876f3f08201c7c76cb71fa1da439-Metadata.json},
 openalex = {W2121359844},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3dc4876f3f08201c7c76cb71fa1da439-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {ICA based on a Smooth Estimation of the Differential Entropy},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/3dc4876f3f08201c7c76cb71fa1da439-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_3df1d4b9,
 abstract = {In this paper, the problem of implementing the reject option in support vector machines (SVMs) is addressed. We started by observing that methods proposed so far simply apply a reject threshold to the outputs of a trained SVM. We then showed that, under the framework of the structural risk minimisation principle, the rejection region must be determined during the training phase of a classifier. By applying this concept, and by following Vapnik's approach, we developed a maximum margin classifier with reject option. This led us to a SVM whose rejection region is determined during the training phase, that is, a SVM with embedded reject option. To implement such a SVM, we devised a novel formulation of the SVM training problem and developed a specific algorithm to solve it. Preliminary results on a character recognition problem show the advantages of the proposed SVM in terms of the achievable error-reject trade-off.},
 author = {Grandvalet, Yves and Rakotomamonjy, Alain and Keshet, Joseph and Canu, St\'{e}phane},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3df1d4b96d8976ff5986393e8767f5b2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3df1d4b96d8976ff5986393e8767f5b2-Metadata.json},
 openalex = {W1824259148},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Support Vector Machines with Embedded Reject Option},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/3df1d4b96d8976ff5986393e8767f5b2-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_3e77a146,
 abstract = {Neuroimaging datasets often have a very large number of voxels and a very small number of training cases, which means that overfitting of models for this data can become a very serious problem. Working with a set of fMRI images from a study on stroke recovery, we consider a classification task for which logistic regression performs poorly, even when L1- or L2- regularized. We show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data. We compare discriminative training of exactly the same set of models, and we also consider convex blends of generative and discriminative training.},
 author = {Schmah, Tanya and Hinton, Geoffrey E and Small, Steven and Strother, Stephen and Zemel, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3e77a14629775492504515dc4b23deda-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3e77a14629775492504515dc4b23deda-Metadata.json},
 openalex = {W2162747531},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/3e77a14629775492504515dc4b23deda-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Generative versus discriminative training of RBMs for classification of fMRI images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/3e77a14629775492504515dc4b23deda-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_42e77b63,
 abstract = {Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency.},
 author = {Coquelin, Pierre-arnaud and Deguest, Romain and Munos, R\'{e}mi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/42e77b63637ab381e8be5f8318cc28a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/42e77b63637ab381e8be5f8318cc28a2-Metadata.json},
 openalex = {W2120545133},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/42e77b63637ab381e8be5f8318cc28a2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/42e77b63637ab381e8be5f8318cc28a2-Supplemental.zip},
 title = {Particle filter-based policy gradient for pomdps},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/42e77b63637ab381e8be5f8318cc28a2-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_49ae49a2,
 abstract = {We consider multi-armed bandit problems where the number of arms is larger than the possible number of experiments. We make a stochastic assumption on the mean-reward of a new selected arm which characterizes its probability of being a near-optimal arm. Our assumption is weaker than in previous works. We describe algorithms based on upper-confidence-bounds applied to a restricted set of randomly selected arms and provide upper-bounds on the resulting expected regret. We also derive a lower-bound which matches (up to a logarithmic factor) the upper-bound in some cases.},
 author = {Wang, Yizao and Audibert, Jean-yves and Munos, R\'{e}mi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/49ae49a23f67c759bf4fc791ba842aa2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/49ae49a23f67c759bf4fc791ba842aa2-Metadata.json},
 openalex = {W2158858912},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/49ae49a23f67c759bf4fc791ba842aa2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/49ae49a23f67c759bf4fc791ba842aa2-Supplemental.zip},
 title = {Algorithms for Infinitely Many-Armed Bandits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/49ae49a23f67c759bf4fc791ba842aa2-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_4a533591,
 abstract = {In this theoretical contribution we provide mathematical proof that two of the most important classes of network learning - correlation-based differential Heb-bian learning and reward-based temporal difference learning - are asymptotically equivalent when timing the learning with a local modulatory signal. This opens the opportunity to consistently reformulate most of the abstract reinforcement learning framework from a correlation based perspective that is more closely related to the biophysics of neurons.},
 author = {Kolodziejski, Christoph and Porr, Bernd and Tamosiunaite, Minija and W\"{o}rg\"{o}tter, Florentin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4a533591763dfa743a13affab1a85793-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4a533591763dfa743a13affab1a85793-Metadata.json},
 openalex = {W2125444740},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4a533591763dfa743a13affab1a85793-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/4a533591763dfa743a13affab1a85793-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_4b0a59dd,
 author = {Minka, Tom and Winn, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4b0a59ddf11c58e7446c9df0da541a84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4b0a59ddf11c58e7446c9df0da541a84-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4b0a59ddf11c58e7446c9df0da541a84-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4b0a59ddf11c58e7446c9df0da541a84-Supplemental.zip},
 title = {Gates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/4b0a59ddf11c58e7446c9df0da541a84-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_4e0928de,
 abstract = {We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the category-learner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classifier based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classifier is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent flag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort.},
 author = {Vijayanarasimhan, Sudheendra and Grauman, Kristen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4e0928de075538c593fbdabb0c5ef2c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4e0928de075538c593fbdabb0c5ef2c3-Metadata.json},
 openalex = {W2109832419},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multi-Level Active Prediction of Useful Image Annotations for Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/4e0928de075538c593fbdabb0c5ef2c3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_500e75a0,
 abstract = {We propose a multiplicative approximation scheme (MAS) for inference problems in graphical models, which can be applied to various inference algorithms. The method uses ∊-decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error ∊. MAS translates these local approximations into bounds on the accuracy of the results. We show how to optimize ∊-decompositions and provide a fast closed-form solution for an L2 approximation. Applying MAS to the Variable Elimination inference algorithm, we introduce an algorithm we call DynaDecomp which is extremely fast in practice and provides guaranteed error bounds on the result. The superior accuracy and efficiency of DynaDecomp is demonstrated.},
 author = {Wexler, Ydo and Meek, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/500e75a036dc2d7d2fec5da1b71d36cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/500e75a036dc2d7d2fec5da1b71d36cc-Metadata.json},
 openalex = {W2100926206},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {MAS: a multiplicative approximation scheme for probabilistic inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/500e75a036dc2d7d2fec5da1b71d36cc-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_53e3a716,
 abstract = {Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difficult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure.},
 author = {He, Xuming and Zemel, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/53e3a7161e428b65688f14b84d61c610-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/53e3a7161e428b65688f14b84d61c610-Metadata.json},
 openalex = {W2151862441},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/53e3a7161e428b65688f14b84d61c610-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning Hybrid Models for Image Annotation with Partially Labeled Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/53e3a7161e428b65688f14b84d61c610-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_5487315b,
 abstract = {Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efficient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by minimizing an objective function. We demonstrate the algorithm on regression and classification problems and we use it to estimate the parameters of a differential equation model of gene regulation.},
 author = {Lawrence, Neil and Rattray, Magnus and Titsias, Michalis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5487315b1286f907165907aa8fc96619-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5487315b1286f907165907aa8fc96619-Metadata.json},
 openalex = {W2127972252},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5487315b1286f907165907aa8fc96619-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5487315b1286f907165907aa8fc96619-Supplemental.zip},
 title = {Efficient Sampling for Gaussian Process Inference using Control Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/5487315b1286f907165907aa8fc96619-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_5751ec3e,
 abstract = {We present an algorithm for solving a broad class of online resource allocation problems. Our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule. We assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v, τ), where τ is the time invested in activity v. Under this assumption, our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time T, for some fixed deadline T > 0, and (ii) the average time required to complete each job. We evaluate our algorithm experimentally by using it to learn, online, a schedule for allocating CPU time among solvers entered in the 2007 SAT solver competition.},
 author = {Streeter, Matthew and Golovin, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5751ec3e9a4feab575962e78e006250d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5751ec3e9a4feab575962e78e006250d-Metadata.json},
 openalex = {W2121671791},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {An Online Algorithm for Maximizing Submodular Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_5b69b9cb,
 abstract = {This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simplified proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a unified analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2.},
 author = {Kakade, Sham M and Sridharan, Karthik and Tewari, Ambuj},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Metadata.json},
 openalex = {W2160354932},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/5b69b9cb83065d403869739ae7f0995e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_5f0f5e5f,
 abstract = {Principal Components Analysis (PCA) has become established as one of the key tools for dimensionality reduction when dealing with real valued data. Approaches such as exponential family PCA and non-negative matrix factorisation have successfully extended PCA to non-Gaussian data types, but these techniques fail to take advantage of Bayesian inference and can suffer from problems of over-fitting and poor generalisation. This paper presents a fully probabilistic approach to PCA, which is generalised to the exponential family, based on Hybrid Monte Carlo sampling. We describe the model which is based on a factorisation of the observed data matrix, and show performance of the model on both synthetic and real data.},
 author = {Mohamed, Shakir and Ghahramani, Zoubin and Heller, Katherine A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5f0f5e5f33945135b874349cfbed4fb9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5f0f5e5f33945135b874349cfbed4fb9-Metadata.json},
 openalex = {W2148495331},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5f0f5e5f33945135b874349cfbed4fb9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Exponential Family PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/5f0f5e5f33945135b874349cfbed4fb9-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_5f2c22cb,
 abstract = {In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reflect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential filtering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential filter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities.},
 author = {Yu, Angela J and Cohen, Jonathan D},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5f2c22cb4a5380af7ca75622a6426917-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5f2c22cb4a5380af7ca75622a6426917-Metadata.json},
 openalex = {W2107564936},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/5f2c22cb4a5380af7ca75622a6426917-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sequential effects: Superstition or rational behavior?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/5f2c22cb4a5380af7ca75622a6426917-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_605ff764,
 abstract = {In this work, we consider the problem of learning a positive semidefinite matrix. The critical issue is how to preserve positive semidefiniteness during the course of learning. Our algorithm is mainly inspired by LPBoost [1] and the general greedy convex optimization framework of Zhang [2]. We demonstrate the essence of the algorithm, termed PSDBoost (positive semidefinite Boosting), by focusing on a few different applications in machine learning. The proposed PSDBoost algorithm extends traditional Boosting algorithms in that its parameter is a positive semidefinite matrix with trace being one instead of a classifier. PSDBoost is based on the observation that any trace-one positive semidefinite matrix can be decomposed into linear convex combinations of trace-one rank-one matrices, which serve as base learners of PSDBoost. Numerical experiments are presented.},
 author = {Shen, Chunhua and Welsh, Alan and Wang, Lei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/605ff764c617d3cd28dbbdd72be8f9a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/605ff764c617d3cd28dbbdd72be8f9a2-Metadata.json},
 openalex = {W2164370641},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/605ff764c617d3cd28dbbdd72be8f9a2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/605ff764c617d3cd28dbbdd72be8f9a2-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_61b4a64b,
 abstract = {Using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose. First, these tools allow patients to interact with their environment through a Brain-Machine Interface (BMI). Second, analyzing the characteristics of such methods can reveal the relative significance of various features of neural activity, task stimuli, and behavior. In this study we adapted, implemented and tested a machine learning method called Kernel Auto-Regressive Moving Average (KARMA), for the task of inferring movements from neural activity in primary motor cortex. Our version of this algorithm is used in an online learning setting and is updated after a sequence of inferred movements is completed. We first used it to track real hand movements executed by a monkey in a standard 3D reaching task. We then applied it in a closed-loop BMI setting to infer intended movement, while the monkey's arms were comfortably restrained, thus performing the task using the BMI alone. KARMA is a recurrent method that learns a nonlinear model of output dynamics. It uses similarity functions (termed kernels) to compare between inputs. These kernels can be structured to incorporate domain knowledge into the method. We compare KARMA to various state-of-the-art methods by evaluating tracking performance and present results from the KARMA based BMI experiments.},
 author = {Shpigelman, Lavi and Lalazar, Hagai and Vaadia, Eilon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/61b4a64be663682e8cb037d9719ad8cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/61b4a64be663682e8cb037d9719ad8cd-Metadata.json},
 openalex = {W2106580610},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/61b4a64be663682e8cb037d9719ad8cd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/61b4a64be663682e8cb037d9719ad8cd-Supplemental.zip},
 title = {Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/61b4a64be663682e8cb037d9719ad8cd-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_61f2585b,
 author = {Raskutti, Garvesh and Yu, Bin and Wainwright, Martin J and Ravikumar, Pradeep},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/61f2585b0ebcf1f532c4d1ec9a7d51aa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/61f2585b0ebcf1f532c4d1ec9a7d51aa-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/61f2585b0ebcf1f532c4d1ec9a7d51aa-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \textbackslash boldmath\textbackslash ell\_1-regularized MLE},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/61f2585b0ebcf1f532c4d1ec9a7d51aa-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_62e7f2e0,
 abstract = {Suppose we train an animal in a conditioning experiment. Can one predict how a given animal, under given experimental conditions, would perform the task? Since various factors such as stress, motivation, genetic background, and previous errors in task performance can influence animal behaviour, this appears to be a very challenging aim. Reinforcement learning (RL) models have been successful in modeling animal (and human) behaviour, but their success has been limited because of uncertainty as to how to set meta-parameters (such as learning rate, exploitation-exploration balance and future reward discount factor) that strongly influence model performance. We show that a simple RL model whose meta-parameters are controlled by an artificial neural network, fed with inputs such as stress, affective phenotype, previous task performance, and even neuromodulatory manipulations, can successfully predict mouse behaviour in the hole-box - a simple conditioning task. Our results also provide important insights on how stress and anxiety affect animal learning, performance accuracy, and discounting of future rewards, and on how noradrenergic systems can interact with these processes.},
 author = {Sandi, Carmen and Gerstner, Wulfram and Luk\v{s}ys, Gediminas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/62e7f2e090fe150ef8deb4466fdc81b3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/62e7f2e090fe150ef8deb4466fdc81b3-Metadata.json},
 openalex = {W2149632162},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/62e7f2e090fe150ef8deb4466fdc81b3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/62e7f2e090fe150ef8deb4466fdc81b3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_63923f49,
 abstract = {Many human interactions involve pieces of information being passed from one person to another, raising the question of how this process of information transmission is affected by the capacities of the agents involved. In the 1930s, Sir Frederic Bartlett explored the influence of memory biases in of information, in which one person's reconstruction of a stimulus from memory becomes the stimulus seen by the next person. These experiments were done using relatively uncontrolled stimuli such as pictures and stories, but suggested that serial reproduction would transform information in a way that reflected the biases inherent in memory. We formally analyze serial reproduction using a Bayesian model of reconstruction from memory, giving a general result characterizing the effect of memory biases on information transmission. We then test the predictions of this account in two experiments using simple one-dimensional stimuli. Our results provide theoretical and empirical justification for the idea that serial reproduction reflects memory biases.},
 author = {Xu, Jing and Griffiths, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/63923f49e5241343aa7acb6a06a751e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/63923f49e5241343aa7acb6a06a751e7-Metadata.json},
 openalex = {W2123588095},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/63923f49e5241343aa7acb6a06a751e7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {How memory biases affect information transmission: A rational analysis of serial reproduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/63923f49e5241343aa7acb6a06a751e7-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_647bba34,
 abstract = {This paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace. The problem of solving for the mapping is transformed into one of solving for an Eulerian flow field which we compute using ideas from kernel methods. We demonstrate the efficacy of our approach on various real world data sets.},
 author = {Walder, Christian and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/647bba344396e7c8170902bcf2e15551-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/647bba344396e7c8170902bcf2e15551-Metadata.json},
 openalex = {W2122165674},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/647bba344396e7c8170902bcf2e15551-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/647bba344396e7c8170902bcf2e15551-Supplemental.zip},
 title = {Diffeomorphic Dimensionality Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/647bba344396e7c8170902bcf2e15551-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_65658fde,
 abstract = {Motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning. Recent impressive results range from humanoid robot movement generation to timing models of human motions. The automatic generation of skill libraries containing multiple motion templates is an important step in robot learning. Such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system. In this paper, we show how human trajectories captured as multi-dimensional time-series can be clustered using Bayesian mixtures of linear Gaussian state-space models based on the similarity of their dynamics. The appropriate number of templates is automatically determined by enforcing a parsimonious parametrization. As the resulting model is intractable, we introduce a novel approximation method based on variational Bayes, which is especially designed to enable the use of efficient inference algorithms. On recorded human Balero movements, this method is not only capable of finding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic SARCOS arm.},
 author = {Chiappa, Silvia and Kober, Jens and Peters, Jan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/65658fde58ab3c2b6e5132a39fae7cb9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/65658fde58ab3c2b6e5132a39fae7cb9-Metadata.json},
 openalex = {W2160162867},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Using Bayesian Dynamical Systems for Motion Template Libraries},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/65658fde58ab3c2b6e5132a39fae7cb9-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_65cc2c82,
 abstract = {We propose an efficient sequential Monte Carlo inference scheme for the recently proposed coalescent clustering model [1]. Our algorithm has a quadratic runtime while those in [1] is cubic. In experiments, we were surprised to find that in addition to being more efficient, it is also a better sequential Monte Carlo sampler than the best in [1], when measured in terms of variance of estimated likelihood and effective sample size.},
 author = {Gorur, Dilan and Teh, Yee},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/65cc2c8205a05d7379fa3a6386f710e1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/65cc2c8205a05d7379fa3a6386f710e1-Metadata.json},
 openalex = {W2117287544},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {An Efficient Sequential Monte Carlo Algorithm for Coalescent Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/65cc2c8205a05d7379fa3a6386f710e1-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_6602294b,
 abstract = {We define a metric for measuring behavior similarity between states in a Markov decision process (MDP), which takes action similarity into account. We show that the kernel of our metric corresponds exactly to the classes of states defined by MDP homomorphisms (Ravindran & Barto, 2003). We prove that the difference in the optimal value function of different states can be upper-bounded by the value of this metric, and that the bound is tighter than previous bounds provided by bisimulation metrics (Ferns et al. 2004, 2005). Our results hold both for discrete and for continuous actions. We provide an algorithm for constructing approximate homomorphisms, by using this metric to identify states that can be grouped together, as well as actions that can be matched. Previous research on this topic is based mainly on heuristics.},
 author = {Taylor, Jonathan and Precup, Doina and Panagaden, Prakash},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6602294be910b1e3c4571bd98c4d5484-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6602294be910b1e3c4571bd98c4d5484-Metadata.json},
 openalex = {W2105960367},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6602294be910b1e3c4571bd98c4d5484-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bounding Performance Loss in Approximate MDP Homomorphisms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/6602294be910b1e3c4571bd98c4d5484-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_66368270,
 author = {Graves, Alex and Schmidhuber, J\"{u}rgen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/66368270ffd51418ec58bd793f2d9b1b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/66368270ffd51418ec58bd793f2d9b1b-Metadata.json},
 openalex = {W2170942820},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Offline Arabic Handwriting Recognition with Multidimensional Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_66f041e1,
 abstract = {Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the assumption. However, real shapes from image datasets, even when expected to be related by almost isometric transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times.},
 author = {Smola, Alex and Mcauley, Julian and Caetano, Tib\'{e}rio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/66f041e16a60928b05a7e228a89c3799-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/66f041e16a60928b05a7e228a89c3799-Metadata.json},
 openalex = {W2101900221},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/66f041e16a60928b05a7e228a89c3799-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Robust Near-Isometric Matching via Structured Learning of Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/66f041e16a60928b05a7e228a89c3799-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_677e0972,
 abstract = {Given an n-vertex weighted tree with structural diameter S and a subset of m vertices, we present a technique to compute a corresponding m × m Gram matrix of the pseudoinverse of the graph Laplacian in O(n + m2 + mS) time. We discuss the application of this technique to fast label prediction on a generic graph. We approximate the graph with a spanning tree and then we predict with the kernel perceptron. We address the approximation of the graph with either a minimum spanning tree or a shortest path tree. The fast computation of the pseudoinverse enables us to address prediction problems on large graphs. We present experiments on two web-spam classification tasks, one of which includes a graph with 400,000 vertices and more than 10,000,000 edges. The results indicate that the accuracy of our technique is competitive with previous methods using the full graph information.},
 author = {Herbster, Mark and Pontil, Massimiliano and Galeano, Sergio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/677e09724f0e2df9b6c000b75b5da10d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/677e09724f0e2df9b6c000b75b5da10d-Metadata.json},
 openalex = {W2170535396},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/677e09724f0e2df9b6c000b75b5da10d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fast Prediction on a Tree},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/677e09724f0e2df9b6c000b75b5da10d-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_68ce199e,
 abstract = {Confidence-weighted (CW) learning [6], an online learning method for linear classifiers, maintains a Gaussian distributions over weight vectors, with a covariance matrix that represents uncertainty about weights and correlations. Confidence constraints ensure that a weight vector drawn from the hypothesis distribution correctly classifies examples with a specified probability. Within this framework, we derive a new convex form of the constraint and analyze it in the mistake bound model. Empirical evaluation with both synthetic and text data shows our version of CW learning achieves lower cumulative and out-of-sample errors than commonly used first-order and second-order online methods.},
 author = {Crammer, Koby and Dredze, Mark and Pereira, Fernando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/68ce199ec2c5517597ce0a4d89620f55-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/68ce199ec2c5517597ce0a4d89620f55-Metadata.json},
 openalex = {W2104554891},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/68ce199ec2c5517597ce0a4d89620f55-Supplemental.zip},
 title = {Exact Convex Confidence-Weighted Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/68ce199ec2c5517597ce0a4d89620f55-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_6a10bbd4,
 abstract = {Bandpass filtering, orientation selectivity, and contrast gain control are prominent features of sensory coding at the level of V1 simple cells. While the effect of bandpass filtering and orientation selectivity can be assessed within a linear model, contrast gain control is an inherently nonlinear computation. Here we employ the class of $L_p$ elliptically contoured distributions to investigate the extent to which the two features---orientation selectivity and contrast gain control---are suited to model the statistics of natural images. Within this framework we find that contrast gain control can play a significant role for the removal of redundancies in natural images. Orientation selectivity, in contrast, has only a very limited potential for redundancy reduction.},
 author = {Sinz, Fabian and Bethge, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6a10bbd480e4c5573d8f3af73ae0454b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6a10bbd480e4c5573d8f3af73ae0454b-Metadata.json},
 openalex = {W2134931916},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6a10bbd480e4c5573d8f3af73ae0454b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6a10bbd480e4c5573d8f3af73ae0454b-Supplemental.zip},
 title = {The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/6a10bbd480e4c5573d8f3af73ae0454b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_6aab1270,
 abstract = {By attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, Co-clustering algorithms often demonstrate surprisingly impressive performance improvements over traditional one-sided row clustering techniques. A good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa). In many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering. In this paper, we develop two novel semi-supervised multi-class classification algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation formulations for co-clustering. These algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical Representer theorem applied to regularization problems posed on a collection of Reproducing Kernel Hilbert Spaces. Empirical results demonstrate the effectiveness and utility of our algorithms.},
 author = {Sindhwani, Vikas and Hu, Jianying and Mojsilovic, Aleksandra},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6aab1270668d8cac7cef2566a1c5f569-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6aab1270668d8cac7cef2566a1c5f569-Metadata.json},
 openalex = {W2162215411},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Regularized Co-Clustering with Dual Supervision},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/6aab1270668d8cac7cef2566a1c5f569-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_6bc24fc1,
 abstract = {Large-margin structured estimation methods minimize a convex upper bound of loss functions. While they allow for efficient optimization algorithms, these convex formulations are not tight and sacrifice the ability to accurately model the true loss. We present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classification to structured estimation. We show that a small modification of existing optimization algorithms suffices to solve this modified problem. On structured prediction tasks such as protein sequence alignment and web page ranking, our algorithm leads to improved accuracy.},
 author = {Chapelle, Olivier and B., Chuong and Teo, Choon and Le, Quoc and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6bc24fc1ab650b25b4114e93a98f1eba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6bc24fc1ab650b25b4114e93a98f1eba-Metadata.json},
 openalex = {W2158460069},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6bc24fc1ab650b25b4114e93a98f1eba-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Tighter Bounds for Structured Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/6bc24fc1ab650b25b4114e93a98f1eba-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_6c3cf77d,
 abstract = {In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. The multi-agent filtering problem is to efficiently represent and update these beliefs through time as the agents act in the world. In this paper, we formally define an infinite sequence of nested beliefs about the state of the world at the current time t, and present a filtering algorithm that maintains a finite representation which can be used to generate these beliefs. In some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex. In experiments, we demonstrate efficient filtering in a range of multi-agent domains.},
 author = {Zettlemoyer, Luke and Milch, Brian and Kaelbling, Leslie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6c3cf77d52820cd0fe646d38bc2145ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6c3cf77d52820cd0fe646d38bc2145ca-Metadata.json},
 openalex = {W2124930238},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multi-Agent Filtering with Infinitely Nested Beliefs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/6c3cf77d52820cd0fe646d38bc2145ca-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_6c9882bb,
 abstract = {Unexpected stimuli are a challenge to any machine learning algorithm. Here, we identify distinct types of unexpected events when general-level and specific-level classifiers give conflicting predictions. We define a formal framework for the representation and processing of incongruent events: Starting from the notion of label hierarchy, we show how partial order on labels can be deduced from such hierarchies. For each event, we compute its probability in different ways, based on adjacent levels in the label hierarchy. An incongruent event is an event where the probability computed based on some more specific level is much smaller than the probability computed based on some more general level, leading to conflicting predictions. Algorithms are derived to detect incongruent events from different types of hierarchies, different applications, and a variety of data types. We present promising results for the detection of novel visual and audio objects, and new patterns of motion in video. We also discuss the detection of Out-Of- Vocabulary words in speech recognition, and the detection of incongruent events in a multimodal audiovisual scenario.},
 author = {Weinshall, Daphna and Hermansky, Hynek and Zweig, Alon and Luo, Jie and Jimison, Holly and Ohl, Frank and Pavel, Misha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6c9882bbac1c7093bd25041881277658-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6c9882bbac1c7093bd25041881277658-Metadata.json},
 openalex = {W2106837342},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Novelty Detection: Incongruent Events, When General and Specific Classifiers Disagree},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/6c9882bbac1c7093bd25041881277658-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_6e16656a,
 abstract = {Is accurate classification possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction—where the probability of class membership increases monotonically with the MF's value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions.

We argue that MFs arise naturally in a broad range of textual classification applications. On the classic data set, a learner given an MF and unlabeled data achieves classification accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data.},
 author = {Downey, Doug and Etzioni, Oren},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6e16656a6ee1de7232164767ccfa7920-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6e16656a6ee1de7232164767ccfa7920-Metadata.json},
 openalex = {W2145078771},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6e16656a6ee1de7232164767ccfa7920-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/6e16656a6ee1de7232164767ccfa7920-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_6faa8040,
 abstract = {We propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints. Our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model. The algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator. The framework yields several new models, including multi-task sparse additive models, multi-response sparse additive models, and sparse additive multi-category logistic regression. The methods are illustrated with experiments on synthetic data and gene microarray data.},
 author = {Liu, Han and Wasserman, Larry and Lafferty, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6faa8040da20ef399b63a72d0e4ab575-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6faa8040da20ef399b63a72d0e4ab575-Metadata.json},
 openalex = {W2107412135},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Nonparametric regression and classification with joint sparsity constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/6faa8040da20ef399b63a72d0e4ab575-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_703957b6,
 abstract = {Language and image understanding are two major goals of artificial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efficient polynomial-time parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efficient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful.

In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-fine representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efficiently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena.},
 author = {Zhu, Leo and Chen, Yuanhao and Lin, Yuan and Lin, Chenxi and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/703957b6dd9e3a7980e040bee50ded65-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/703957b6dd9e3a7980e040bee50ded65-Metadata.json},
 openalex = {W2121041256},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/703957b6dd9e3a7980e040bee50ded65-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Recursive Segmentation and Recognition Templates for 2D Parsing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/703957b6dd9e3a7980e040bee50ded65-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_704afe07,
 abstract = {MetalDetector identifies CYS and HIS involved in transition metal protein binding sites, starting from sequence alone.A major new feature of release 2.0 is the ability to predict which residues are jointly involved in the coordination of the same metal ion.The server is available at http://metaldetector.dsi .unifi.it/v2.0/.},
 author = {Frasconi, Paolo and Passerini, Andrea},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/704afe073992cbe4813cae2f7715336f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/704afe073992cbe4813cae2f7715336f-Metadata.json},
 openalex = {W1992176190},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/704afe073992cbe4813cae2f7715336f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {MetalDetector v2.0: predicting the geometry of metal binding sites from protein sequence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/704afe073992cbe4813cae2f7715336f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_70c639df,
 abstract = {Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with fixed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and firing state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between firing and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points.},
 author = {Ponzi, Adam and Wickens, Jeff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/70c639df5e30bdee440e4cdf599fec2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/70c639df5e30bdee440e4cdf599fec2b-Metadata.json},
 openalex = {W2099915273},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/70c639df5e30bdee440e4cdf599fec2b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/70c639df5e30bdee440e4cdf599fec2b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_72007983,
 abstract = {We study the behavior of block l1/l2 regulation for multivariate regression, where a K-dimensional response vector is regressed upon a fixed set of p covariates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Studying this problem under high-dimensional scaling (where the problem parameters as well as sample size n tend to infinity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θl1/l2(n, p, s) : = n/ [2Ѱ(B*) log(p - s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and Ѱ(B*) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefficient vectors that constitute the model. This sparsity-overlap function reveals that block l1/l2 regulation for multivariate regression never harms performance relative to a naive l1-approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal relative to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems.},
 author = {Obozinski, Guillaume R and Wainwright, Martin J and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/72007983849f4fcb0ad565439834756b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/72007983849f4fcb0ad565439834756b-Metadata.json},
 openalex = {W2158497838},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/72007983849f4fcb0ad565439834756b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {High-dimensional support union recovery in multivariate regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/72007983849f4fcb0ad565439834756b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_735143e9,
 abstract = {We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case.},
 author = {Smith, Andrew and Zha, Hongyuan and Wu, Xiao-ming},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/735143e9ff8c47def504f1ba0442df98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/735143e9ff8c47def504f1ba0442df98-Metadata.json},
 openalex = {W2156027586},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/735143e9ff8c47def504f1ba0442df98-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/735143e9ff8c47def504f1ba0442df98-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_73640de2,
 abstract = {We consider the problem of bounding from above the log-partition function corresponding to second-order Ising models for binary distributions. We introduce a new bound, the cardinality bound, which can be computed via convex optimization. The corresponding error on the logpartition function is bounded above by twice the distance, in model parameter space, to a class of “standard” Ising models, for which variable inter-dependence is described via a simple mean field term. In the context of maximum-likelihood, using the new bound instead of the exact log-partition function, while constraining the distance to the class of standard Ising models, leads not only to a good approximation to the log-partition function, but also to a model that is parsimonious, and easily interpretable. We compare our bound with the log-determinant bound introduced by Wainwright and Jordan (2006), and show that when the l1-norm of the model parameter vector is small enough, the latter is outperformed by the new bound.},
 author = {Ghaoui, Laurent and Gueye, Assane},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/73640de25b7d656733ce2f808a330f18-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/73640de25b7d656733ce2f808a330f18-Metadata.json},
 openalex = {W2132140282},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/73640de25b7d656733ce2f808a330f18-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Convex Upper Bound on the Log-Partition Function for Binary Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/73640de25b7d656733ce2f808a330f18-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7380ad8a,
 abstract = {Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefficients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneficial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory.},
 author = {Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7380ad8a673226ae47fce7bff88e9c33-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7380ad8a673226ae47fce7bff88e9c33-Metadata.json},
 openalex = {W2106294397},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7380ad8a673226ae47fce7bff88e9c33-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7380ad8a673226ae47fce7bff88e9c33-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_73c03186,
 abstract = {The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. A statistical model for ranking predicts how humans rank subsets V of some universe U . In this work we define a statistical model for ranking that satisfies certain desirable properties. The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. This offers a new generative approach to ranking which can be used for IR. There are two main contexts for this work. The first is the theory of econometrics and study of statistical models explaining human choice of alternatives. In this context, we will compare our model with other well known models. The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. Here, much work has been done in the discriminative setting, where different heuristics are used to define ranking risk functions. Our model is built rigorously and axiomatically based on very simple desirable properties defined locally for comparisons, and automatically implies the existence of a global score function serving as a natural model parameter which can be efficiently fitted to pairwise comparison judgment data by solving a convex optimization problem.},
 author = {Ailon, Nir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/73c03186765e199c116224b68adc5fa0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/73c03186765e199c116224b68adc5fa0-Metadata.json},
 openalex = {W2115119365},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/73c03186765e199c116224b68adc5fa0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/73c03186765e199c116224b68adc5fa0-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_75887499,
 abstract = {We analyse matching pursuit for kernel principal components analysis (KPCA) by proving that the sparse subspace it produces is a sample compression scheme. We show that this bound is tighter than the KPCA bound of Shawe-Taylor et al [7] and highly predictive of the size of the subspace needed to capture most of the variance in the data. We analyse a second matching pursuit algorithm called kernel matching pursuit (KMP) which does not correspond to a sample compression scheme. However, we give a novel bound that views the choice of subspace of the KMP algorithm as a compression scheme and hence provide a VC bound to upper bound its future loss. Finally we describe how the same bound can be applied to other matching pursuit related algorithms.},
 author = {Hussain, Zakria and Shawe-taylor, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/758874998f5bd0c393da094e1967a72b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/758874998f5bd0c393da094e1967a72b-Metadata.json},
 openalex = {W2116541867},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/758874998f5bd0c393da094e1967a72b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Theory of matching pursuit},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/758874998f5bd0c393da094e1967a72b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7647966b,
 abstract = {Many motor skills in humanoid robotics can be learned using parametrized motor primitives. While successful applications to date have been achieved with imitation learning, most of the interesting motor learning problems are high-dimensional reinforcement learning problems. These problems are often beyond the reach of current reinforcement learning methods. In this paper, we study parametrized policy search methods and apply these to benchmark problems of motor primitive learning in robotics. We show that many well-known parametrized policy search methods can be derived from a general, common framework. This framework yields both policy gradient methods and expectation-maximization (EM) inspired algorithms. We introduce a novel EM-inspired algorithm for policy learning that is particularly well-suited for dynamical system motor primitives. We compare this algorithm, both in simulation and on a real robot, to several well-known parametrized policy search methods such as episodic REINFORCE, ‘Vanilla’ Policy Gradients with optimal baselines, episodic Natural Actor Critic, and episodic Reward-Weighted Regression. We show that the proposed method out-performs them on an empirical benchmark of learning dynamical system motor primitives both in simulation and on a real robot. We apply it in the context of motor learning and show that it can learn a complex Ball-in-a-Cup task on a real Barrett WAM™ robot arm.},
 author = {Kober, Jens and Peters, Jan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7647966b7343c29048673252e490f736-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7647966b7343c29048673252e490f736-Metadata.json},
 openalex = {W2012392077},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7647966b7343c29048673252e490f736-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7647966b7343c29048673252e490f736-Supplemental.zip},
 title = {Policy search for motor primitives in robotics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7647966b7343c29048673252e490f736-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_788d9869,
 abstract = {We formulate and study a new variant of the k-armed bandit problem, motivated by e-commerce applications. In our model, arms have (stochastic) lifetime after which they expire. In this setting an algorithm needs to continuously explore new arms, in contrast to the standard k-armed bandit model in which arms are available indefinitely and exploration is reduced once an optimal arm is identified with near-certainty. The main motivation for our setting is online-advertising, where ads have limited lifetime due to, for example, the nature of their content and their campaign budgets. An algorithm needs to choose among a large collection of ads, more than can be fully explored within the typical ad lifetime.

We present an optimal algorithm for the state-aware (deterministic reward function) case, and build on this technique to obtain an algorithm for the state-oblivious (stochastic reward function) case. Empirical studies on various reward distributions, including one derived from a real-world ad serving application, show that the proposed algorithms significantly outperform the standard multi-armed bandit approaches applied to these settings.},
 author = {Chakrabarti, Deepayan and Kumar, Ravi and Radlinski, Filip and Upfal, Eli},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/788d986905533aba051261497ecffcbb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/788d986905533aba051261497ecffcbb-Metadata.json},
 openalex = {W2123016239},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/788d986905533aba051261497ecffcbb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Mortal Multi-Armed Bandits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/788d986905533aba051261497ecffcbb-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7a614fd0,
 abstract = {We study the profit-maximization problem of a monopolistic market-maker who sets two-sided prices in an asset market. The sequential decision problem is hard to solve because the state space is a function. We demonstrate that the belief state is well approximated by a Gaussian distribution. We prove a key monotonicity property of the Gaussian state update which makes the problem tractable, yielding the first optimal sequential market-making algorithm in an established model. The algorithm leads to a surprising insight: an optimal monopolist can provide more liquidity than perfectly competitive market-makers in periods of extreme uncertainty, because a monopolist is willing to absorb initial losses in order to learn a new valuation rapidly so she can extract higher profits later.},
 author = {Das, Sanmay and Magdon-Ismail, Malik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7a614fd06c325499f1680b9896beedeb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7a614fd06c325499f1680b9896beedeb-Metadata.json},
 openalex = {W2120824918},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7a614fd06c325499f1680b9896beedeb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7a614fd06c325499f1680b9896beedeb-Supplemental.zip},
 title = {Adapting to a Market Shock: Optimal Sequential Market-Making},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7a614fd06c325499f1680b9896beedeb-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7b13b220,
 abstract = {Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in finding a reduced dimensionality representation. Specifically, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classification task and show how our model can identify shared topics across classes as well as class-dependent topics.},
 author = {Lacoste-Julien, Simon and Sha, Fei and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7b13b2203029ed80337f27127a9f1d28-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7b13b2203029ed80337f27127a9f1d28-Metadata.json},
 openalex = {W2122683976},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7b13b2203029ed80337f27127a9f1d28-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7d04bbbe,
 abstract = {EEG connectivity measures could provide a new type of feature space for inferring a subject's intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the γ-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions.},
 author = {Grosse-wentrup, Moritz},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Metadata.json},
 openalex = {W2162494220},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7eacb532,
 abstract = {This paper presents the first Rademacher complexity-based error bounds for non-i.i.d. settings, a generalization of similar existing bounds derived for the i.i.d. case. Our bounds hold in the scenario of dependent samples generated by a stationary β-mixing process, which is commonly adopted in many previous studies of non-i.i.d. settings. They benefit from the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from such finite samples and lead to tighter generalization bounds. We also present the first margin bounds for kernel-based classification in this non-i.i.d. setting and briefly study their convergence.},
 author = {Mohri, Mehryar and Rostamizadeh, Afshin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7eacb532570ff6858afd2723755ff790-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7eacb532570ff6858afd2723755ff790-Metadata.json},
 openalex = {W2140956554},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7eacb532570ff6858afd2723755ff790-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Rademacher Complexity Bounds for Non-I.I.D. Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7eacb532570ff6858afd2723755ff790-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7f1171a7,
 abstract = {Young children demonstrate the ability to make inferences about the preferences of other agents based on their choices. However, there exists no overarching account of what children are doing when they learn about preferences or how they use that knowledge. We use a rational model of preference learning, drawing on ideas from economics and computer science, to explain the behavior of children in several recent experiments. Specifically, we show how a simple econometric model can be extended to capture two- to four-year-olds' use of statistical information in inferring preferences, and their generalization of these preferences.},
 author = {Lucas, Christopher and Griffiths, Thomas and Xu, Fei and Fawcett, Christine},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Metadata.json},
 openalex = {W2165364063},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A rational model of preference learning and choice prediction by children},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7f1171a78ce0780a2142a6eb7bc4f3c8-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7f1de29e,
 abstract = {Before the age of 4 months, infants make inductive inferences about the motions of physical objects. Developmental psychologists have provided verbal accounts of the knowledge that supports these inferences, but often these accounts focus on categorical rather than probabilistic principles. We propose that infant object perception is guided in part by probabilistic principles like persistence: things tend to remain the same, and when they change they do so gradually. To illustrate this idea we develop an ideal observer model that incorporates probabilistic principles of rigidity and inertia. Like previous researchers, we suggest that rigid motions are expected from an early age, but we challenge the previous claim that the inertia principle is relatively slow to develop [1]. We support these arguments by modeling several experiments from the developmental literature.},
 author = {Kemp, Charles and Xu, Fei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f1de29e6da19d22b51c68001e7e0e54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f1de29e6da19d22b51c68001e7e0e54-Metadata.json},
 openalex = {W2130660894},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f1de29e6da19d22b51c68001e7e0e54-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f1de29e6da19d22b51c68001e7e0e54-Supplemental.zip},
 title = {An ideal observer model of infant object perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7f1de29e6da19d22b51c68001e7e0e54-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7f24d240,
 abstract = {The ROC curve is known to be the golden standard for measuring performance of a test/scoring statistic regarding its capacity of discrimination between two populations in a wide variety of applications, ranging from anomaly detection in signal processing to information retrieval, through medical diagnosis. Most practical performance measures used in scoring applications such as the AUC, the local AUC, the p-norm push, the DCG and others, can be seen as summaries of the ROC curve. This paper highlights the fact that many of these empirical criteria can be expressed as (conditional) linear rank statistics. We investigate the properties of empirical maximizers of such performance criteria and provide preliminary results for the concentration properties of a novel class of random variables that we will call a linear rank process.},
 author = {Cl\'{e}men\c{c}con, St\'{e}phan and Vayatis, Nicolas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f24d240521d99071c93af3917215ef7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f24d240521d99071c93af3917215ef7-Metadata.json},
 openalex = {W2161781176},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f24d240521d99071c93af3917215ef7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Empirical performance maximization for linear rank statistics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7f24d240521d99071c93af3917215ef7-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_7f975a56,
 abstract = {This paper addresses the problem of sparsity pattern detection for unknown k-sparse n-dimensional signals observed through m noisy, random linear measurements. Sparsity pattern recovery arises in a number of settings including statistical model selection, pattern detection, and image acquisition. The main results in this paper are necessary and sufficient conditions for asymptotically-reliable sparsity pattern recovery in terms of the dimensions m, n and k as well as the signal-to-noise ratio (SNR) and the minimum-to-average ratio (MAR) of the nonzero entries of the signal. We show that m > 2k log(n - k)/(SNR · MAR) is necessary for any algorithm to succeed, regardless of complexity; this matches a previous sufficient condition for maximum likelihood estimation within a constant factor under certain scalings of k, SNR and MAR with n. We also show a sufficient condition for a computationally-trivial thresholding algorithm that is larger than the previous expression by only a factor of 4(1 + SNR) and larger than the requirement for lasso by only a factor of 4/MAR. This provides insight on the precise value and limitations of convex programming-based algorithms.},
 author = {Rangan, Sundeep and Goyal, Vivek and Fletcher, Alyson K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f975a56c761db6506eca0b37ce6ec87-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f975a56c761db6506eca0b37ce6ec87-Metadata.json},
 openalex = {W2163089989},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/7f975a56c761db6506eca0b37ce6ec87-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Resolution Limits of Sparse Coding in High Dimensions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/7f975a56c761db6506eca0b37ce6ec87-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8065d07d,
 abstract = {This paper addresses the important tradeoff between privacy and learnability, when designing algorithms for learning from private databases. We focus on privacy-preserving logistic regression. First we apply an idea of Dwork et al. [6] to design a privacy-preserving logistic regression algorithm. This involves bounding the sensitivity of regularized logistic regression, and perturbing the learned classifier with noise proportional to the sensitivity.

We then provide a privacy-preserving regularized logistic regression algorithm based on a new privacy-preserving technique: solving a perturbed optimization problem. We prove that our algorithm preserves privacy in the model due to [6]. We provide learning guarantees for both algorithms, which are tighter for our new algorithm, in cases in which one would typically apply logistic regression. Experiments demonstrate improved learning performance of our method, versus the sensitivity method. Our privacy-preserving technique does not depend on the sensitivity of the function, and extends easily to a class of convex loss functions. Our work also reveals an interesting connection between regularization and privacy.},
 author = {Chaudhuri, Kamalika and Monteleoni, Claire},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Metadata.json},
 openalex = {W2112380340},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Privacy-preserving logistic regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_816b112c,
 abstract = {We give polynomial-time algorithms for the exact computation of lowest-energy states, worst margin violators, partition functions, and marginals in certain binary undirected graphical models. Our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph. Maximum-margin parameter estimation for a boundary detection task shows our approach to be efficient and effective. A C++ implementation is available from http://nic.schraudolph.org/isinf/.},
 author = {Schraudolph, Nicol and Kamenetsky, Dmitry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/816b112c6105b3ebd537828a39af4818-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/816b112c6105b3ebd537828a39af4818-Metadata.json},
 openalex = {W2168840938},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/816b112c6105b3ebd537828a39af4818-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Exact Inference in Planar Ising Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/816b112c6105b3ebd537828a39af4818-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_85d8ce59,
 abstract = {In analogy to the PCA setting, the sparse PCA problem is often solved by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deflation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justification from the PCA context. In this work, we demonstrate that the standard PCA deflation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we first develop several deflation alternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCA optimization problem to explicitly reflect the maximum additional variance objective on each round. The result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.},
 author = {Mackey, Lester},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Metadata.json},
 openalex = {W2148980251},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Deflation Methods for Sparse PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8613985e,
 abstract = {Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social and protein interaction networks.},
 author = {Airoldi, Edo M and Blei, David and Fienberg, Stephen and Xing, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8613985ec49eb8f757ae6439e879bb2a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8613985ec49eb8f757ae6439e879bb2a-Metadata.json},
 openalex = {W2107107106},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8613985ec49eb8f757ae6439e879bb2a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Mixed Membership Stochastic Blockmodels.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8613985ec49eb8f757ae6439e879bb2a-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_883e881b,
 abstract = {We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman-Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state-of-the-art methods, while simultaneously discovering categories shared among natural scenes.},
 author = {Sudderth, Erik and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/883e881bb4d22a7add958f2d6b052c9f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/883e881bb4d22a7add958f2d6b052c9f-Metadata.json},
 openalex = {W2135226297},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/883e881bb4d22a7add958f2d6b052c9f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/883e881bb4d22a7add958f2d6b052c9f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8844c5f0,
 author = {Heitz, Geremy and Elidan, Gal and Packer, Benjamin and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8844c5f00372df2c3c4ee857c2451b45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8844c5f00372df2c3c4ee857c2451b45-Metadata.json},
 openalex = {W1978493028},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8844c5f00372df2c3c4ee857c2451b45-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Shape-Based Object Localization for Descriptive Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8844c5f00372df2c3c4ee857c2451b45-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8b16ebc0,
 abstract = {Covariance estimation for high dimensional vectors is a classically difficult problem in statistical analysis and machine learning. In this paper, we propose a maximum likelihood (ML) approach to covariance estimation, which employs a novel sparsity constraint. More specifically, the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform (SMT). The SMT is formed by a product of pairwise coordinate rotations known as Givens rotations. Using this framework, the covariance can be efficiently estimated using greedy minimization of the log likelihood function, and the number of Givens rotations can be efficiently computed using a cross-validation procedure. The resulting estimator is positive definite and well-conditioned even when the sample size is limited. Experiments on standard hyperspectral data sets show that the SMT covariance estimate is consistently more accurate than both traditional shrinkage estimates and recently proposed graphical lasso estimates for a variety of different classes and sample sizes.},
 author = {Cao, Guangzhi and Bouman, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8b16ebc056e613024c057be590b542eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8b16ebc056e613024c057be590b542eb-Metadata.json},
 openalex = {W2155569237},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8b16ebc056e613024c057be590b542eb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8b16ebc056e613024c057be590b542eb-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8c235f89,
 abstract = {The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefficients. We explore a variety of copula models for joint neural response distributions, and derive an efficient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. We find that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their firing rate distribution.},
 author = {Berkes, Pietro and Wood, Frank and Pillow, Jonathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8c235f89a8143a28a1d6067e959dd858-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8c235f89a8143a28a1d6067e959dd858-Metadata.json},
 openalex = {W2113547349},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Characterizing neural dependencies with copula models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8c235f89a8143a28a1d6067e959dd858-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8c7bbbba,
 abstract = {Regularized Least Squares (RLS) algorithms have the ability to avoid over-fitting problems and to express solutions as kernel expansions. However, we observe that the current RLS algorithms cannot provide a satisfactory interpretation even on the penalty of a constant function. Based on the intuition that a good kernel-based inductive function should be consistent with both the data and the kernel, a novel learning scheme is proposed. The advantages of this scheme lie in its corresponding Representer Theorem, its strong interpretation ability about what kind of functions should not be penalized, and its promising accuracy improvements shown in a number of experiments. Furthermore, we provide a detailed technical description about heat kernels, which serves as an example for the readers to apply similar techniques for other kernels. Our work provides a preliminary step in a new direction to explore the varying consistency between inductive functions and kernels under various distributions.},
 author = {Yang, Haixuan and King, Irwin and Lyu, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8c7bbbba95c1025975e548cee86dfadc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8c7bbbba95c1025975e548cee86dfadc-Metadata.json},
 openalex = {W2148213200},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8c7bbbba95c1025975e548cee86dfadc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning with Consistency between Inductive Functions and Kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8c7bbbba95c1025975e548cee86dfadc-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8d3bba74,
 abstract = {We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree-specific syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents.},
 author = {Boyd-graber, Jordan and Blei, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8d3bba7425e7c98c50f52ca1b52d3735-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8d3bba7425e7c98c50f52ca1b52d3735-Metadata.json},
 openalex = {W1712618182},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8d3bba7425e7c98c50f52ca1b52d3735-Supplemental.zip},
 title = {Syntactic Topic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8d3bba7425e7c98c50f52ca1b52d3735-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8dd48d6a,
 abstract = {How is information decoded in the brain? is one of the most difficult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simplified. First, we hierarchically construct simplified probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simplified models, i.e., We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information.},
 author = {Oizumi, Masafumi and Ishii, Toshiyuki and Ishibashi, Kazuya and Hosoya, Toshihiko and Okada, Masato},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8dd48d6a2e2cad213179a3992c0be53c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8dd48d6a2e2cad213179a3992c0be53c-Metadata.json},
 openalex = {W2126545975},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A general framework for investigating how far the decoding process in the brain can be simplified},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8df707a9,
 abstract = {Policy gradient (PG) reinforcement learning algorithms have strong (local) convergence guarantees, but their learning performance is typically limited by a large variance in the estimate of the gradient. In this paper, we formulate the variance reduction problem by describing a signal-to-noise ratio (SNR) for policy gradient algorithms, and evaluate this SNR carefully for the popular Weight Perturbation (WP) algorithm. We confirm that SNR is a good predictor of long-term learning performance, and that in our episodic formulation, the cost-to-go function is indeed the optimal baseline. We then propose two modifications to traditional model-free policy gradient algorithms in order to optimize the SNR. First, we examine WP using anisotropic sampling distributions, which introduces a bias into the update but increases the SNR; this bias can be interpreted as following the natural gradient of the cost function. Second, we show that non-Gaussian distributions can also increase the SNR, and argue that the optimal isotropic distribution is a 'shell' distribution with a constant magnitude and uniform distribution in direction. We demonstrate that both modifications produce substantial improvements in learning performance in challenging policy gradient experiments.},
 author = {Roberts, John and Tedrake, Russ},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8df707a948fac1b4a0f97aa554886ec8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8df707a948fac1b4a0f97aa554886ec8-Metadata.json},
 openalex = {W2098936573},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8df707a948fac1b4a0f97aa554886ec8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8df707a948fac1b4a0f97aa554886ec8-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8f468c87,
 abstract = {The odor transduction process has a large time constant and is susceptible to various types of noise. Therefore, the olfactory code at the sensor/receptor level is in general a slow and highly variable indicator of the input odor in both natural and artificial situations. Insects overcome this problem by using a neuronal device in their Antennal Lobe (AL), which transforms the identity code of olfactory receptors to a spatio-temporal code. This transformation improves the decision of the Mushroom Bodies (MBs), the subsequent classifier, in both speed and accuracy. Here we propose a rate model based on two intrinsic mechanisms in the insect AL, namely integration and inhibition. Then we present a MB classifier model that resembles the sparse and random structure of insect MB. A local Hebbian learning procedure governs the plasticity in the model. These formulations not only help to understand the signal conditioning and classification methods of insect olfactory systems, but also can be leveraged in synthetic problems. Among them, we consider here the discrimination of odor mixtures from pure odors. We show on a set of records from metal-oxide gas sensors that the cascade of these two new models facilitates fast and accurate discrimination of even highly imbalanced mixtures from pure odors.},
 author = {Muezzinoglu, Mehmet and Vergara, Alexander and Huerta, Ramon and Nowotny, Thomas and Rulkov, Nikolai and Abarbanel, Henry and Selverston, Allen and Rabinovich, Mikhail},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f468c873a32bb0619eaeb2050ba45d1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f468c873a32bb0619eaeb2050ba45d1-Metadata.json},
 openalex = {W2157170745},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f468c873a32bb0619eaeb2050ba45d1-Supplemental.zip},
 title = {Artificial Olfactory Brain for Mixture Identification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8f468c873a32bb0619eaeb2050ba45d1-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8f53295a,
 abstract = {This letter discusses the robustness issue of kernel principal component analysis. A class of new robust procedures is proposed based on eigenvalue decomposition of weighted covariance. The proposed procedures will place less weight on deviant patterns and thus be more resistant to data contamination and model deviation. Theoretical influence functions are derived, and numerical examples are presented as well. Both theoretical and numerical results indicate that the proposed robust method outperforms the conventional approach in the sense of being less sensitive to outliers. Our robust method and results also apply to functional principal component analysis.},
 author = {Nguyen, Minh and Torre, Fernando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f53295a73878494e9bc8dd6c3c7104f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f53295a73878494e9bc8dd6c3c7104f-Metadata.json},
 openalex = {W1972832829},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f53295a73878494e9bc8dd6c3c7104f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Robust Kernel Principal Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8f53295a73878494e9bc8dd6c3c7104f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8f7d807e,
 abstract = {In classification problems, Support Vector Machines maximize the margin of separation between two classes. While the paradigm has been successful, the solution obtained by SVMs is dominated by the directions with large data spread and biased to separate the classes by cutting along large spread directions. This article proposes a novel formulation to overcome such sensitivity and maximizes the margin relative to the spread of the data. The proposed formulation can be efficiently solved and experiments on digit datasets show drastic performance improvements over SVMs.},
 author = {Jebara, Tony and Shivaswamy, Pannagadatta},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f7d807e1f53eff5f9efbe5cb81090fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f7d807e1f53eff5f9efbe5cb81090fb-Metadata.json},
 openalex = {W2162828817},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f7d807e1f53eff5f9efbe5cb81090fb-Supplemental.zip},
 title = {Relative Margin Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8f7d807e1f53eff5f9efbe5cb81090fb-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_8f855179,
 abstract = {In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efficient, requires no sampling, automatically rejects outliers and has only one prior to be specified. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law.},
 author = {Ting, Jo-anne and Kalakrishnan, Mrinal and Vijayakumar, Sethu and Schaal, Stefan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f85517967795eeef66c225f7883bdcb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f85517967795eeef66c225f7883bdcb-Metadata.json},
 openalex = {W2142149019},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/8f85517967795eeef66c225f7883bdcb-Supplemental.zip},
 title = {Bayesian Kernel Shaping for Learning Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/8f85517967795eeef66c225f7883bdcb-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_903ce922,
 abstract = {We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efficient methods, i.e., Semi-Infinite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can significantly improve efficiency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method.},
 author = {Xu, Zenglin and Jin, Rong and King, Irwin and Lyu, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/903ce9225fca3e988c2af215d4e544d3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/903ce9225fca3e988c2af215d4e544d3-Metadata.json},
 openalex = {W2117866949},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/903ce9225fca3e988c2af215d4e544d3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {An Extended Level Method for Efficient Multiple Kernel Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/903ce9225fca3e988c2af215d4e544d3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_92c8c96e,
 abstract = {We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular $L_1$-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.},
 author = {Langford, John and Li, Lihong and Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/92c8c96e4c37100777c7190b76d28233-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/92c8c96e4c37100777c7190b76d28233-Metadata.json},
 openalex = {W2616657226},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/92c8c96e4c37100777c7190b76d28233-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/92c8c96e4c37100777c7190b76d28233-Supplemental.zip},
 title = {Sparse Online Learning via Truncated Gradient},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/92c8c96e4c37100777c7190b76d28233-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_92cc2275,
 abstract = {Classical game theoretic approaches that make strong rationality assumptions have difficulty modeling human behaviour in economic games. We investigate the role of finite levels of iterated reasoning and non-selfish utility functions in a Partially Observable Markov Decision Process model that incorporates game theoretic notions of interactivity. Our generative model captures a broad class of characteristic behaviours in a multi-round Investor-Trustee game. We invert the generative process for a recognition model that is used to classify 200 subjects playing this game against randomly matched opponents.},
 author = {Ray, Debajyoti and King-casas, Brooks and Montague, P. and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/92cc227532d17e56e07902b254dfad10-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/92cc227532d17e56e07902b254dfad10-Metadata.json},
 openalex = {W2096486583},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/92cc227532d17e56e07902b254dfad10-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Model of Behaviour in Economic Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/92cc227532d17e56e07902b254dfad10-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_934815ad,
 abstract = {We present a characterization of a useful class of skills based on a graphical representation of an agent's interaction with its environment. Our characterization uses betweenness, a measure of centrality on graphs. It captures and generalizes (at least intuitively) the bottleneck concept, which has inspired many of the existing skill-discovery algorithms. Our characterization may be used directly to form a set of skills suitable for a given task. More importantly, it serves as a useful guide for developing incremental skill-discovery algorithms that do not rely on knowing or representing the interaction graph in its entirety.},
 author = {\c{S}im\c{s}ek, \"{O}zg\"{u}r and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/934815ad542a4a7c5e8a2dfa04fea9f5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/934815ad542a4a7c5e8a2dfa04fea9f5-Metadata.json},
 openalex = {W2168640731},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Skill Characterization Based on Betweenness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/934815ad542a4a7c5e8a2dfa04fea9f5-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_941e1aaa,
 abstract = {We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random field characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of αβ-Swap and α-Expansion respectively that fully exploit the form of the pairwise potentials. Specifically, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efficient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems.},
 author = {Torr, Philip and Kumar, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/941e1aaaba585b952b62c14a3a175a61-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/941e1aaaba585b952b62c14a3a175a61-Metadata.json},
 openalex = {W2106809923},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/941e1aaaba585b952b62c14a3a175a61-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/941e1aaaba585b952b62c14a3a175a61-Supplemental.zip},
 title = {Improved Moves for Truncated Convex Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/941e1aaaba585b952b62c14a3a175a61-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_94c7bb58,
 abstract = {Object matching is a fundamental operation in data analysis. It typically requires the definition of a similarity measure between the classes of objects to be matched. Instead, we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes. This is achieved by maximizing the dependency between matched pairs of observations by means of the Hilbert-Schmidt Independence Criterion. This problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for finding a locally optimal solution.},
 author = {Quadrianto, Novi and Song, Le and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/94c7bb58efc3b337800875b5d382a072-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/94c7bb58efc3b337800875b5d382a072-Metadata.json},
 openalex = {W2293888570},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/94c7bb58efc3b337800875b5d382a072-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/94c7bb58efc3b337800875b5d382a072-Supplemental.zip},
 title = {Kernelized Sorting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/94c7bb58efc3b337800875b5d382a072-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_94f6d7e0,
 abstract = {Kernel supervised learning methods can be unified by utilizing the tools from regularization theory. The duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated Bayesian interpretations of kernel methods. In this paper we pursue a Bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as Silverman's g-prior. We provide a theoretical analysis of the posterior consistency of a Bayesian model choice procedure based on this prior. We also establish the asymptotic relationship between this procedure and the Bayesian information criterion.},
 author = {Zhang, Zhihua and Jordan, Michael and Yeung, Dit-Yan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/94f6d7e04a4d452035300f18b984988c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/94f6d7e04a4d452035300f18b984988c-Metadata.json},
 openalex = {W2142670722},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Posterior Consistency of the Silverman g-prior in Bayesian Model Choice},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/94f6d7e04a4d452035300f18b984988c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_950a4152,
 abstract = {Many nonlinear dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes. We consider two such models: the switching linear dynamical system (SLDS) and the switching vector autoregressive (VAR) process. Our nonparametric Bayesian approach utilizes a hierarchical Dirichlet process prior to learn an unknown number of persistent, smooth dynamical modes. We develop a sampling algorithm that combines a truncated approximation to the Dirichlet process with efficient joint sampling of the mode and state sequences. The utility and flexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, and the IBOVESPA stock index.},
 author = {Fox, Emily and Sudderth, Erik and Jordan, Michael and Willsky, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/950a4152c2b4aa3ad78bdd6b366cc179-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/950a4152c2b4aa3ad78bdd6b366cc179-Metadata.json},
 openalex = {W2139688922},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/950a4152c2b4aa3ad78bdd6b366cc179-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/950a4152c2b4aa3ad78bdd6b366cc179-Supplemental.zip},
 title = {Nonparametric Bayesian Learning of Switching Linear Dynamical Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/950a4152c2b4aa3ad78bdd6b366cc179-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_98dce83d,
 abstract = {Working memory is a central topic of cognitive neuroscience because it is critical for solving real-world problems in which information from multiple temporally distant sources must be combined to generate appropriate behavior. However, an often neglected fact is that learning to use working memory effectively is itself a difficult problem. The Gating framework [1-4] is a collection of psychological models that show how dopamine can train the basal ganglia and prefrontal cortex to form useful working memory representations in certain types of problems. We unite Gating with machine learning theory concerning the general problem of memory-based optimal control [5-6]. We present a normative model that learns, by online temporal difference methods, to use working memory to maximize discounted future reward in partially observable settings. The model successfully solves a benchmark working memory problem, and exhibits limitations similar to those observed in humans. Our purpose is to introduce a concise, normative definition of high level cognitive concepts such as working memory and cognitive control in terms of maximizing discounted future rewards.},
 author = {Todd, Michael and Niv, Yael and Cohen, Jonathan D},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/98dce83da57b0395e163467c9dae521b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/98dce83da57b0395e163467c9dae521b-Metadata.json},
 openalex = {W2139283936},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/98dce83da57b0395e163467c9dae521b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/98dce83da57b0395e163467c9dae521b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_9a96876e,
 abstract = {Many popular optimization algorithms, like the Levenberg-Marquardt algorithm (LMA), use heuristic-based that modulate the behavior of the optimizer during the optimization process. For example, in the LMA a damping parameter λ is dynamically modified based on a set of rules that were developed using heuristic arguments. Reinforcement learning (RL) is a machine learning approach to learn optimal controllers from examples and thus is an obvious candidate to improve the heuristic-based controllers implicit in the most popular and heavily used optimization algorithms.

Improving the performance of off-the-shelf optimizers is particularly important for time-constrained optimization problems. For example the LMA algorithm has become popular for many real-time computer vision problems, including object tracking from video, where only a small amount of time can be allocated to the optimizer on each incoming video frame.

Here we show that a popular modern reinforcement learning technique using a very simple state space can dramatically improve the performance of general purpose optimizers, like the LMA. Surprisingly the controllers learned for a particular domain also work well in very different optimization domains. For example we used RL methods to train a new controller for the damping parameter of the LMA. This controller was trained on a collection of classic, relatively small, non-linear regression problems. The modified LMA performed better than the standard LMA on these problems. This controller also dramatically outperformed the standard LMA on a difficult computer vision problem for which it had not been trained. Thus the controller appeared to have extracted control rules that were not just domain specific but generalized across a range of optimization domains.},
 author = {Ruvolo, Paul and Fasel, Ian and Movellan, Javier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Metadata.json},
 openalex = {W2132295085},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Optimization on a Budget: A Reinforcement Learning Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/9a96876e2f8f3dc4f3cf45f02c61c0c1-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_9ab0d884,
 abstract = {In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle appearance and disappearance of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of infinite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known.},
 author = {Gasthaus, Jan and Wood, Frank and Gorur, Dilan and Teh, Yee},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9ab0d88431732957a618d4a469a0d4c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9ab0d88431732957a618d4a469a0d4c3-Metadata.json},
 openalex = {W2159157127},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9ab0d88431732957a618d4a469a0d4c3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Dependent Dirichlet Process Spike Sorting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/9ab0d88431732957a618d4a469a0d4c3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_9ad6aaed,
 abstract = {The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.},
 author = {Sutskever, Ilya and Hinton, Geoffrey E and Taylor, Graham W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9ad6aaed513b73148b7d49f70afcfb32-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9ad6aaed513b73148b7d49f70afcfb32-Metadata.json},
 openalex = {W2135341757},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9ad6aaed513b73148b7d49f70afcfb32-Supplemental.zip},
 title = {The Recurrent Temporal Restricted Boltzmann Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_9be40cee,
 abstract = {We report a compact realization of short-term depression (STD) in a VLSI stochastic synapse. The behavior of the circuit is based on a subtractive single release model of STD. Experimental results agree well with simulation and exhibit expected STD behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems. The dynamic stochastic synapse could potentially be a powerful addition to existing deterministic VLSI spiking neural systems.},
 author = {Xu, Peng and Horiuchi, Timothy and Abshire, Pamela},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9be40cee5b0eee1462c82c6964087ff9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9be40cee5b0eee1462c82c6964087ff9-Metadata.json},
 openalex = {W2106143872},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9be40cee5b0eee1462c82c6964087ff9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Short-Term Depression in VLSI Stochastic Synapse},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/9be40cee5b0eee1462c82c6964087ff9-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_9cf81d80,
 abstract = {We apply robust Bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the true class proportions are unknown. For the generative case, we derive an entropy-based weighting that maximizes expected log likelihood under the worst-case true class proportions. For the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data, an extreme case of labeling bias since there is no absence data. On a benchmark dataset, we find that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data.},
 author = {Phillips, Steven and Dud\'{\i}k, Miroslav},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9cf81d8026a9018052c429cc4e56739b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9cf81d8026a9018052c429cc4e56739b-Metadata.json},
 openalex = {W2138115675},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9cf81d8026a9018052c429cc4e56739b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Generative and Discriminative Learning with Unknown Labeling Bias},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/9cf81d8026a9018052c429cc4e56739b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_9de6d14f,
 abstract = {Applications of multi-class classification, such as document categorization, often appear in cost-sensitive settings. Recent work has significantly improved the state of the art by moving beyond ``flat'' classification through incorporation of class hierarchies [Cai and Hoffman 04]. We present a novel algorithm that goes beyond hierarchical classification and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classification is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efficiently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization.},
 author = {Weinberger, Kilian Q and Chapelle, Olivier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9de6d14fff9806d4bcd1ef555be766cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9de6d14fff9806d4bcd1ef555be766cd-Metadata.json},
 openalex = {W2146753383},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9de6d14fff9806d4bcd1ef555be766cd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Large Margin Taxonomy Embedding for Document Categorization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/9de6d14fff9806d4bcd1ef555be766cd-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a02ffd91,
 abstract = {Language comprehension in humans is significantly constrained by memory, yet rapid, highly incremental, and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input. In contrast, most of the leading psycholinguistic models and fielded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events. We present a new limited-memory model of sentence comprehension which involves an adaptation of the particle filter, a sequential Monte Carlo method, to the problem of incremental parsing. We show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the first rational account of an outstanding problem in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information.},
 author = {Levy, Roger and Reali, Florencia and Griffiths, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a02ffd91ece5e7efeb46db8f10a74059-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a02ffd91ece5e7efeb46db8f10a74059-Metadata.json},
 openalex = {W2154424448},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Modeling the effects of memory on human online sentence processing with particle filters},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a02ffd91ece5e7efeb46db8f10a74059-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a1140a3d,
 abstract = {We propose a new class of consistency constraints for Linear Programming (LP) relaxations for finding the most probable (MAP) configuration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and significantly faster than a method that does not use partitioning.},
 author = {Sontag, David and Globerson, Amir and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a1140a3d0df1c81e24ae954d935e8926-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a1140a3d0df1c81e24ae954d935e8926-Metadata.json},
 openalex = {W2123534506},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Clusters and Coarse Partitions in LP Relaxations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a3d68b46,
 abstract = {We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (belief). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis.},
 author = {Mooij, Joris M and Kappen, Hilbert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Metadata.json},
 openalex = {W2123632224},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Supplemental.zip},
 title = {Bounds on marginal probability distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a3d68b461bd9d3533ee1dd3ce4628ed4-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a49e9411,
 abstract = {Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean field variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k - 1) + √log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation.},
 author = {Mukherjee, Indraneel and Blei, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a49e9411d64ff53eccfdd09ad10a15b3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a49e9411d64ff53eccfdd09ad10a15b3-Metadata.json},
 openalex = {W2148111240},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a49e9411d64ff53eccfdd09ad10a15b3-Supplemental.zip},
 title = {Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a4a042cf,
 abstract = {For supervised and unsupervised learning, positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the l1-norm or the block l1-norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.},
 author = {Bach, Francis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a4a042cf4fd6bfb47701cbc8a1653ada-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a4a042cf4fd6bfb47701cbc8a1653ada-Metadata.json},
 openalex = {W1646506067},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a4a042cf4fd6bfb47701cbc8a1653ada-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a4a042cf4fd6bfb47701cbc8a1653ada-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a532400e,
 abstract = {We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classifiers which compete for images by their expertise. Each boosting classifier is an aggregation of weak-learners, i.e. simple visual features. The obtained classifiers are useful for object detection tasks which exhibit multi-modalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classifiers in object detection tasks.},
 author = {Kim, Tae-kyun and Cipolla, Roberto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a532400ed62e772b9dc0b86f46e583ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a532400ed62e772b9dc0b86f46e583ff-Metadata.json},
 openalex = {W2151673653},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a532400ed62e772b9dc0b86f46e583ff-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a532400ed62e772b9dc0b86f46e583ff-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a5771bce,
 abstract = {Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. A critical idea is that of limited capacity, the allocation of which has produced continual conflict about such phenomena as early and late selection. An influential resolution of this debate is based on the notion of perceptual load (Lavie, 2005), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difficult tasks grab all resources for themselves, leaving distractors high and dry. We argue that this theory presents a challenge to Bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data.},
 author = {Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a5771bce93e200c36f7cd9dfd0e5deaa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a5771bce93e200c36f7cd9dfd0e5deaa-Metadata.json},
 openalex = {W2142948230},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a5771bce93e200c36f7cd9dfd0e5deaa-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Load and Attentional Bayes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a5771bce93e200c36f7cd9dfd0e5deaa-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a5e00132,
 abstract = {The computational role of the local recurrent network in primary visual cortex is still a matter of debate. To address this issue, we analyze intracellular recording data of cat V1, which combine measuring the tuning of a range of neuronal properties with a precise localization of the recording sites in the orientation preference map. For the analysis, we consider a network model of Hodgkin-Huxley type neurons arranged according to a biologically plausible two-dimensional topographic orientation preference map. We then systematically vary the strength of the recurrent excitation and inhibition relative to the strength of the afferent input. Each parametrization gives rise to a different model instance for which the tuning of model neurons at different locations of the orientation map is compared to the experimentally measured orientation tuning of membrane potential, spike output, excitatory, and inhibitory conductances. A quantitative analysis shows that the data provides strong evidence for a network model in which the afferent input is dominated by strong, balanced contributions of recurrent excitation and inhibition. This recurrent regime is close to a regime of instability, where strong, self-sustained activity of the network occurs. The firing rate of neurons in the best-fitting network is particularly sensitive to small modulations of model parameters, which could be one of the functional benefits of a network operating in this particular regime.},
 author = {Wimmer, Klaus and Stimberg, Marcel and Martin, Robert and Schwabe, Lars and Mari\~{n}o, Jorge and Schummers, James and Lyon, David and Sur, Mriganka and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a5e00132373a7031000fd987a3c9f87b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a5e00132373a7031000fd987a3c9f87b-Metadata.json},
 openalex = {W2153818238},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a87ff679,
 abstract = {The stochastic approximation method is behind the solution to many important, actively-studied problems in machine learning. Despite its far-reaching application, there is almost no work on applying stochastic approximation to learning problems with general constraints. The reason for this, we hypothesize, is that no robust, widely-applicable stochastic approximation method exists for handling such problems. We propose that interior-point methods are a natural solution. We establish the stability of a stochastic interior-point approximation method both analytically and empirically, and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via L1 regularization.},
 author = {Carbonetto, Peter and Schmidt, Mark and Freitas, Nando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a87ff679a2f3e71d9181a67b7542122c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a87ff679a2f3e71d9181a67b7542122c-Metadata.json},
 openalex = {W2170419304},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a87ff679a2f3e71d9181a67b7542122c-Supplemental.zip},
 title = {An interior-point stochastic approximation method and an L1-regularized delta rule},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a87ff679a2f3e71d9181a67b7542122c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_a8baa565,
 abstract = {A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return.},
 author = {Hou, Xiaodi and Zhang, Liqing},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a8baa56554f96369ab93e4f3bb068c22-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a8baa56554f96369ab93e4f3bb068c22-Metadata.json},
 openalex = {W2107055466},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Dynamic visual attention: searching for coding length increments},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_aa169b49,
 abstract = {We consider the following instance of transfer learning: given a pair of regression problems, suppose that the regression coefficients share a partially common support, parameterized by the overlap fraction $\overlap$ between the two supports. This set-up suggests the use of $1, \infty$-regularized linear regression for recovering the support sets of both regression vectors. Our main contribution is to provide a sharp characterization of the sample complexity of this $1,\infty$ relaxation, exactly pinning down the minimal sample size $n$ required for joint support recovery as a function of the model dimension $\pdim$, support size $\spindex$ and overlap $\overlap \in [0,1]$. For measurement matrices drawn from standard Gaussian ensembles, we prove that the joint $1,\infty$-regularized method undergoes a phase transition characterized by order parameter $\orpar(\numobs, \pdim, \spindex, \overlap) = \numobs{(4 - 3 \overlap) s \log(p-(2-\overlap)s)}$. More precisely, the probability of successfully recovering both supports converges to $1$ for scalings such that $\orpar > 1$, and converges to $0$ to scalings for which $\orpar 2/3$), but performs worse than a naive approach if $\overlap < 2/3$. We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations. Thus, our results illustrate both the benefits and dangers associated with block-$1,\infty$ regularization in high-dimensional inference.},
 author = {Negahban, Sahand and Wainwright, Martin J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/aa169b49b583a2b5af89203c2b78c67c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/aa169b49b583a2b5af89203c2b78c67c-Metadata.json},
 openalex = {W2167981729},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Phase transitions for high-dimensional joint support recovery},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/aa169b49b583a2b5af89203c2b78c67c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_aa68c75c,
 abstract = {Metric learning algorithms can provide useful distance functions for a variety of domains, and recent work has shown good accuracy for problems where the learner can access all distance constraints at once. However, in many real applications, constraints are only available incrementally, thus necessitating methods that can perform online updates to the learned metric. Existing online algorithms offer bounds on worst-case performance, but typically do not perform well in practice as compared to their offline counterparts. We present a new online metric learning algorithm that updates a learned Mahalanobis metric based on LogDet regularization and gradient descent. We prove theoretical worst-case performance bounds, and empirically compare the proposed method against existing online metric learning algorithms. To further boost the practicality of our approach, we develop an online locality-sensitive hashing scheme which leads to efficient updates to data structures used for fast approximate similarity search. We demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines.},
 author = {Jain, Prateek and Kulis, Brian and Dhillon, Inderjit and Grauman, Kristen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Metadata.json},
 openalex = {W2118979615},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Online Metric Learning and Fast Similarity Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/aa68c75c4a77c87f97fb686b2f068676-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_ab88b157,
 abstract = {Many interesting problems, including Bayesian network structure-search, can be cast in terms of finding the optimum value of a function over the space of graphs. However, this function is often expensive to compute exactly. We here present a method derived from the study of Reproducing Kernel Hilbert Spaces which takes advantage of the regular structure of the space of all graphs on a fixed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate, but that the BDe score itself varies quadratically over the space of all graphs.},
 author = {Yackley, Benjamin and Corona, Eduardo and Lane, Terran},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ab88b15733f543179858600245108dd8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ab88b15733f543179858600245108dd8-Metadata.json},
 openalex = {W2126867230},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ab88b15733f543179858600245108dd8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Network Score Approximation using a Metagraph Kernel},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/ab88b15733f543179858600245108dd8-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_abd81528,
 abstract = {We study convergence properties of empirical minimization of a stochastic strongly convex objective, where the stochastic component is linear. We show that the value attained by the empirical minimizer converges to the optimal value with rate 1/n. The result applies, in particular, to the SVM objective. Thus, we obtain a rate of 1/n on the convergence of the SVM objective (with fixed regularization parameter) to its infinite data limit. We demonstrate how this is essential for obtaining certain type of oracle inequalities for SVMs. The results extend also to approximate minimization as well as to strong convexity with respect to an arbitrary norm, and so also to objectives regularized using other lp norms.},
 author = {Sridharan, Karthik and Shalev-shwartz, Shai and Srebro, Nathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/abd815286ba1007abfbb8415b83ae2cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/abd815286ba1007abfbb8415b83ae2cf-Metadata.json},
 openalex = {W2114609025},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/abd815286ba1007abfbb8415b83ae2cf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fast Rates for Regularized Objectives},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/abd815286ba1007abfbb8415b83ae2cf-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_ac1dd209,
 abstract = {We present a novel method for inducing synchronous context free grammars (SCFGs) from a corpus of parallel string pairs. SCFGs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings. We develop a non-parametric Bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this field. Using a variational Bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over maximum likelihood models.},
 author = {Blunsom, Phil and Cohn, Trevor and Osborne, Miles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Metadata.json},
 openalex = {W2131932654},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Synchronous Grammar Induction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_ac627ab1,
 abstract = {Continuous attractor neural networks (CANNs) are emerging as promising models for describing the encoding of continuous stimuli in neural systems. Due to the translational invariance of their neuronal interactions, CANNs can hold a continuous family of neutrally stable states. In this study, we systematically explore how neutral stability of a CANN facilitates its tracking performance, a capacity believed to have wide applications in brain functions. We develop a perturbative approach that utilizes the dominant movement of the network stationary states in the state space. We quantify the distortions of the bump shape during tracking, and study their effects on the tracking performance. Results are obtained on the maximum speed for a moving stimulus to be trackable, and the reaction time to catch up an abrupt change in stimulus.},
 author = {Wong, K. and Wu, Si and Fung, Chi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ac627ab1ccbdb62ec96e702f07f6425b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ac627ab1ccbdb62ec96e702f07f6425b-Metadata.json},
 openalex = {W2159029046},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ac627ab1ccbdb62ec96e702f07f6425b-Supplemental.zip},
 title = {Tracking Changing Stimuli in Continuous Attractor Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/ac627ab1ccbdb62ec96e702f07f6425b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_ad972f10,
 abstract = {We consider the problem of extracting smooth, low-dimensional neural trajectories that summarize the activity recorded simultaneously from many neurons on individual experimental trials. Beyond the benefit of visualizing the high-dimensional, noisy spiking activity in a compact form, such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity. Current methods for extracting neural trajectories involve a two-stage process: the spike trains are first smoothed over time, then a static dimensionality-reduction technique is applied. We first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way and that account for spiking variability, which may vary both across neurons and across time. We then present a novel method for extracting neural trajectories-Gaussian-process factor analysis (GPFA)-which unifies the smoothing and dimensionality-reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that the proposed extensions improved the predictive ability of the two-stage methods. The predictive ability was further improved by going to GPFA. From the extracted trajectories, we directly observed a convergence in neural state during motor planning, an effect that was shown indirectly by previous studies. We then show how such methods can be a powerful tool for relating the spiking activity across a neural population to the subject's behavior on a single-trial basis. Finally, to assess how well the proposed methods characterize neural population activity when the underlying time course is known, we performed simulations that revealed that GPFA performed tens of percent better than the best two-stage method.},
 author = {Yu, Byron M and Cunningham, John P and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna V and Sahani, Maneesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Metadata.json},
 openalex = {W2157380881},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_addfa9b7,
 abstract = {Many machine learning algorithms can be formulated in the framework of statistical independence such as the Hilbert Schmidt Independence Criterion. In this paper, we extend this criterion to deal with structured and interdependent observations. This is achieved by modeling the structures using undirected graphical models and comparing the Hilbert space embeddings of distributions. We apply this new criterion to independent component analysis and sequence clustering.},
 author = {Zhang, Xinhua and Song, Le and Gretton, Arthur and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/addfa9b7e234254d26e9c7f2af1005cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/addfa9b7e234254d26e9c7f2af1005cb-Metadata.json},
 openalex = {W2165140712},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Kernel Measures of Independence for non-iid Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_aeb3135b,
 abstract = {In this paper we consider approximate policy-iteration-based reinforcement learning algorithms. In order to implement a flexible function approximation scheme we propose the use of non-parametric methods with regularization, providing a convenient way to control the complexity of the function approximator. We propose two novel regularized policy iteration algorithms by adding L2-regularization to two widely-used policy evaluation methods: Bellman residual minimization (BRM) and least-squares temporal difference learning (LSTD). We derive efficient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel Hilbert space. We also provide finite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions.},
 author = {Farahmand, Amir and Ghavamzadeh, Mohammad and Mannor, Shie and Szepesv\'{a}ri, Csaba},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/aeb3135b436aa55373822c010763dd54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/aeb3135b436aa55373822c010763dd54-Metadata.json},
 openalex = {W2158738729},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/aeb3135b436aa55373822c010763dd54-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Regularized Policy Iteration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/aeb3135b436aa55373822c010763dd54-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_b3e3e393,
 abstract = {In solving complex visual learning tasks, adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance. These representations are typically high dimensional and assume diverse forms. Thus finding a way to transform them into a unified space of lower dimension generally facilitates the underlying tasks, such as object recognition or clustering. We describe an approach that incorporates multiple kernel learning with dimensionality reduction (MKL-DR). While the proposed framework is flexible in simultaneously tackling data in various feature representations, the formulation itself is general in that it is established upon graph embedding. It follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations.},
 author = {Lin, Yen-yu and Liu, Tyng-luh and Fuh, Chiou-shann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Metadata.json},
 openalex = {W2095847652},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Dimensionality Reduction for Data in Multiple Feature Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/b3e3e393c77e35a4a3f3cbd1e429b5dc-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_b73ce398,
 abstract = {Continuously-Adaptive Discretization for Message-Passing (CAD-MP) is a new message-passing algorithm for approximate inference. Most message-passing algorithms approximate continuous probability distributions using either: a family of continuous distributions such as the exponential family; a particle-set of discrete samples; or a fixed, uniform discretization. In contrast, CAD-MP uses a discretization that is (i) non-uniform, and (ii) adaptive to the structure of the marginal distributions. Non-uniformity allows CAD-MP to localize interesting features (such as sharp peaks) in the marginal belief distributions with time complexity that scales logarithmically with precision, as opposed to uniform discretization which scales at best linearly. We give a principled method for altering the non-uniform discretization according to information-based measures. CAD-MP is shown in experiments to estimate marginal beliefs much more precisely than competing approaches for the same computational expense.},
 author = {Isard, Michael and MacCormick, John and Achan, Kannan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b73ce398c39f506af761d2277d853a92-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b73ce398c39f506af761d2277d853a92-Metadata.json},
 openalex = {W2108133817},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b73ce398c39f506af761d2277d853a92-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Continuously-adaptive discretization for message-passing algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/b73ce398c39f506af761d2277d853a92-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_b99d193b,
 abstract = {Randomly connected recurrent neural circuits have proven to be very powerful models for online computations when a trained memoryless readout function is appended. Such Reservoir Computing (RC) systems are commonly used in two flavors: with analog or binary (spiking) neurons in the recurrent circuits. Previous work showed a fundamental difference between these two incarnations of the RC idea. The performance of a RC system built from binary neurons seems to depend strongly on the network connectivity structure. In networks of analog neurons such dependency has not been observed. In this article we investigate this apparent dichotomy in terms of the in-degree of the circuit nodes. Our analyses based amongst others on the Lyapunov exponent reveal that the phase transition between ordered and chaotic network behavior of binary circuits qualitatively differs from the one in analog circuits. This explains the observed decreased computational performance of binary circuits of high node in-degree. Furthermore, a novel mean-field predictor for computational performance is introduced and shown to accurately predict the numerically obtained results.},
 author = {Schrauwen, Benjamin and Buesing, Lars and Legenstein, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b99d193b66a6542917d2b7bee52c2574-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b99d193b66a6542917d2b7bee52c2574-Metadata.json},
 openalex = {W2161355584},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b99d193b66a6542917d2b7bee52c2574-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/b99d193b66a6542917d2b7bee52c2574-Supplemental.zip},
 title = {On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/b99d193b66a6542917d2b7bee52c2574-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_bd686fd6,
 abstract = {We describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms. Our framework yields the tightest known logarithmic regret bounds for Follow-The-Leader and for the gradient descent algorithm proposed in Hazan et al. [2006]. We then show that one can interpolate between these two extreme cases. In particular, we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations. Finally, we further extend our framework for generalized strongly convex functions.},
 author = {Shalev-shwartz, Shai and Kakade, Sham M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/bd686fd640be98efaae0091fa301e613-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/bd686fd640be98efaae0091fa301e613-Metadata.json},
 openalex = {W2145630740},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/bd686fd640be98efaae0091fa301e613-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/bd686fd640be98efaae0091fa301e613-Supplemental.zip},
 title = {Mind the Duality Gap: Logarithmic regret algorithms for online optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/bd686fd640be98efaae0091fa301e613-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_be1bc799,
 abstract = {Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random field (HSCRF), a generalisation of embedded undirected Markov chains to model complex hierarchical, nested Markov processes. It is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference. Importantly, we develop efficient algorithms for learning and constrained inference in a partially-supervised setting, which is important issue in practice where labels can only be obtained sparsely. We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases.},
 author = {Truyen, Tran and Phung, Dinh and Bui, Hung and Venkatesh, Svetha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/be1bc7997695495f756312886f566110-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/be1bc7997695495f756312886f566110-Metadata.json},
 openalex = {W2168444100},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/be1bc7997695495f756312886f566110-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/be1bc7997695495f756312886f566110-Supplemental.zip},
 title = {Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/be1bc7997695495f756312886f566110-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_be3159ad,
 abstract = {Statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy. We present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level. It also maintains separate parameters for treatment and control groups, which allows us to estimate treatment effects explicitly. We use the model to investigate the sequence evolution of HIV populations exposed to a recently developed antisense gene therapy, as well as a more conventional drug therapy. The detection of biologically relevant and plausible signals in both therapy studies demonstrates the effectiveness of the method.},
 author = {Braunstein, Alexander and Wei, Zhi and Jensen, Shane and Mcauliffe, Jon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/be3159ad04564bfb90db9e32851ebf9c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/be3159ad04564bfb90db9e32851ebf9c-Metadata.json},
 openalex = {W2101705774},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A spatially varying two-sample recombinant coalescent, with applications to HIV escape response.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/be3159ad04564bfb90db9e32851ebf9c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_beed1360,
 abstract = {Aiming towards the development of a general clustering theory, we discuss abstract axiomatization for clustering. In this respect, we follow up on the work of Kleinberg, ([1]) that showed an impossibility result for such axiomatization. We argue that an impossibility result is not an inherent feature of clustering, but rather, to a large extent, it is an artifact of the specific formalism used in [1].

As opposed to previous work focusing on clustering functions, we propose to address clustering quality measures as the object to be axiomatized. We show that principles like those formulated in Kleinberg's axioms can be readily expressed in the latter framework without leading to inconsistency.

A clustering-quality measure (CQM) is a function that, given a data set and its partition into clusters, returns a non-negative real number representing how strong or conclusive the clustering is. We analyze what clustering-quality measures should look like and introduce a set of requirements (axioms) for such measures. Our axioms capture the principles expressed by Kleinberg's axioms while retaining consistency.

We propose several natural clustering quality measures, all satisfying the proposed axioms. In addition, we analyze the computational complexity of evaluating the quality of a given clustering and show that, for the proposed CQMs, it can be computed in polynomial time.},
 author = {Ben-David, Shai and Ackerman, Margareta},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/beed13602b9b0e6ecb5b568ff5058f07-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/beed13602b9b0e6ecb5b568ff5058f07-Metadata.json},
 openalex = {W2096879261},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Measures of Clustering Quality: A Working Set of Axioms for Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/beed13602b9b0e6ecb5b568ff5058f07-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_bf62768c,
 abstract = {Studies of sequential decision-making in humans frequently find suboptimal performance relative to an ideal actor that has perfect knowledge of the model of how rewards and events are generated in the environment. Rather than being suboptimal, we argue that the learning problem humans face is more complex, in that it also involves learning the structure of reward generation in the environment. We formulate the problem of structure learning in sequential decision tasks using Bayesian reinforcement learning, and show that learning the generative model for rewards qualitatively changes the behavior of an optimal learning agent. To test whether people exhibit structure learning, we performed experiments involving a mixture of one-armed and two-armed bandit reward models, where structure learning produces many of the qualitative behaviors deemed suboptimal in previous studies. Our results demonstrate humans can perform structure learning in a near-optimal manner.},
 author = {Acuna, Daniel and Schrater, Paul R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/bf62768ca46b6c3b5bea9515d1a1fc45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/bf62768ca46b6c3b5bea9515d1a1fc45-Metadata.json},
 openalex = {W1970966707},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Structure Learning in Human Sequential Decision-Making},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/bf62768ca46b6c3b5bea9515d1a1fc45-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_c058f544,
 abstract = {From an information-theoretic perspective, a noisy transmission system such as a visual Brain-Computer Interface (BCI) speller could benefit from the use of error-correcting codes. However, optimizing the code solely according to the maximal minimum-Hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli, and hence a significantly reduced average target-to-target interval (TTI), leading to difficulties in classifying the individual event-related potentials (ERPs) due to overlap and refractory effects. Clearly any change to the stimulus setup must also respect the possible psychophysiological consequences. Here we report new EEG data from experiments in which we explore stimulus types and codebooks in a within-subject design, finding an interaction between the two factors. Our data demonstrate that the traditional, row-column code has particular spatial properties that lead to better performance than one would expect from its TTIs and Hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used.},
 author = {Hill, Jeremy and Farquhar, Jason and Martens, Suzanna and Biessmann, Felix and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c058f544c737782deacefa532d9add4c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c058f544c737782deacefa532d9add4c-Metadata.json},
 openalex = {W2141067130},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c058f544c737782deacefa532d9add4c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c058f544c737782deacefa532d9add4c-Supplemental.zip},
 title = {Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/c058f544c737782deacefa532d9add4c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_c0f168ce,
 abstract = {It is now well established that sparse signal models are well suited to restoration tasks and can effectively be learned from audio, image, and video data. Recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones. This paper proposes a new step in that direction, with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and multiple class-decision functions. The linear variant of the proposed model admits a simple probabilistic interpretation, while its most general variant admits an interpretation in terms of kernels. An optimization framework for learning all the components of the proposed model is presented, along with experimental results on standard handwritten digit and texture classification tasks.},
 author = {Mairal, Julien and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew and Bach, Francis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c0f168ce8900fa56e57789e2a2f2c9d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c0f168ce8900fa56e57789e2a2f2c9d0-Metadata.json},
 openalex = {W2128638419},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Supervised Dictionary Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/c0f168ce8900fa56e57789e2a2f2c9d0-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_c16a5320,
 abstract = {We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models. We demonstrate this approach on the challenging problem of natural image denoising. Using a test set with a hundred natural images, we find that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and Markov random field (MRF) methods. Moreover, we find that a convolutional network offers similar performance in the blind de-noising setting as compared to other techniques in the non-blind setting. We also show how convolutional networks are mathematically related to MRF approaches by presenting a mean field theory for an MRF specially designed for image denoising. Although these approaches are related, convolutional networks avoid computational difficulties in MRF approaches that arise from probabilistic learning and inference. This makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is significantly less than that associated with inference in MRF approaches with even hundreds of parameters.},
 author = {Jain, Viren and Seung, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c16a5320fa475530d9583c34fd356ef5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c16a5320fa475530d9583c34fd356ef5-Metadata.json},
 openalex = {W2098477387},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c16a5320fa475530d9583c34fd356ef5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Natural Image Denoising with Convolutional Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/c16a5320fa475530d9583c34fd356ef5-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_c24cd76e,
 abstract = {In most cognitive and motor tasks, speed-accuracy tradeoffs are observed: Individuals can respond slowly and accurately, or quickly yet be prone to errors. Control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history. When stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials. We propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difficulty corresponds to the drift rate for the correct response. The model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence. We derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding. Trial-by-trial tracking of difficulty thus leads to sequential effects in speed and accuracy. Simulations show the model explains a variety of phenomena in human speeded decision making. We argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures.},
 author = {Jones, Matt and Kinoshita, Sachiko and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c24cd76e1ce41366a4bbe8a49b02a028-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c24cd76e1ce41366a4bbe8a49b02a028-Metadata.json},
 openalex = {W2106703741},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c24cd76e1ce41366a4bbe8a49b02a028-Supplemental.zip},
 title = {Optimal Response Initiation: Why Recent Experience Matters},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_c361bc7b,
 abstract = {We show how improved sequences for magnetic resonance imaging can be found through optimization of Bayesian design scores. Combining approximate Bayesian inference and natural image statistics with high-performance numerical computation, we propose the first Bayesian experimental design framework for this problem of high relevance to clinical and brain research. Our solution requires large-scale approximate inference for dense, non-Gaussian models. We propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modified to compute primitives in our framework. Our approach is evaluated on raw data from a 3T MR scanner.},
 author = {Nickisch, Hannes and Pohmann, Rolf and Sch\"{o}lkopf, Bernhard and Seeger, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c361bc7b2c033a83d663b8d9fb4be56e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c361bc7b2c033a83d663b8d9fb4be56e-Metadata.json},
 openalex = {W2124026599},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c361bc7b2c033a83d663b8d9fb4be56e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Experimental Design of Magnetic Resonance Imaging Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/c361bc7b2c033a83d663b8d9fb4be56e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_c52f1bd6,
 abstract = {Cognitive control refers to the flexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efficiently refined with subsequent task experience.},
 author = {Reynolds, Jeremy and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c52f1bd66cc19d05628bd8bf27af3ad6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c52f1bd66cc19d05628bd8bf27af3ad6-Metadata.json},
 openalex = {W2157012098},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Temporal Dynamics of Cognitive Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/c52f1bd66cc19d05628bd8bf27af3ad6-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_c6e19e83,
 abstract = {ROC curves are one of the most widely used displays to evaluate performance of scoring functions. In the paper, we propose a statistical method for directly optimizing the ROC curve. The target is known to be the regression function up to an increasing transformation and this boils down to recovering the level sets of the latter. We propose to use classifiers obtained by empirical risk minimization of a weighted classification error and then to construct a scoring rule by overlaying these classifiers. We show the consistency and rate of convergence to the optimal ROC curve of this procedure in terms of supremum norm and also, as a byproduct of the analysis, we derive an empirical estimate of the optimal ROC curve.},
 author = {Cl\'{e}men\c{c}con, St\'{e}phan and Vayatis, Nicolas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Metadata.json},
 openalex = {W2741688533},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Overlaying classifiers: a practical approach for optimal ranking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/c6e19e830859f2cb9f7c8f8cacb8d2a6-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_cc1aa436,
 abstract = {Psychophysical experiments show that humans are better at perceiving rotation and expansion than translation. These findings are inconsistent with standard models of motion integration which predict best performance for translation [6]. To explain this discrepancy, our theory formulates motion perception at two levels of inference: we first perform model selection between the competing models (e.g. translation, rotation, and expansion) and then estimate the velocity using the selected model. We define novel prior models for smooth rotation and expansion using techniques similar to those in the slow-and-smooth model [17] (e.g. Green functions of differential operators). The theory gives good agreement with the trends observed in human experiments.},
 author = {Wu, Shuang and Lu, Hongjing and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cc1aa436277138f61cda703991069eaf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cc1aa436277138f61cda703991069eaf-Metadata.json},
 openalex = {W2163259821},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cc1aa436277138f61cda703991069eaf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Model selection and velocity estimation using novel priors for motion patterns},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/cc1aa436277138f61cda703991069eaf-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_ccb09896,
 abstract = {We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or Kullback-Leibler divergence. The objective of this paper is two-fold. First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with fixed k converge almost surely, even though the k-nearest-neighbor density estimation with fixed k does not converge to its true measure. Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion.},
 author = {P\'{e}rez-Cruz, Fernando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ccb0989662211f61edae2e26d58ea92f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ccb0989662211f61edae2e26d58ea92f-Metadata.json},
 openalex = {W2157510479},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Estimation of Information Theoretic Measures for Continuous Random Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/ccb0989662211f61edae2e26d58ea92f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_cee63112,
 abstract = {Clustering stability is an increasingly popular family of methods for performing model selection in data clustering. The basic idea is that the chosen model should be stable under perturbation or resampling of the data. Despite being reasonably effective in practice, these methods are not well understood theoretically, and present some difficulties. In particular, when the data is assumed to be sampled from an underlying distribution, the solutions returned by the clustering algorithm will usually become more and more stable as the sample size increases. This raises a potentially serious practical difficulty with these methods, because it means there might be some hard-to-compute sample size, beyond which clustering stability estimators 'break down' and become unreliable in detecting the most stable model. Namely, all models will be relatively stable, with differences in their stability measures depending mostly on random and meaningless sampling artifacts. In this paper, we provide a set of general sufficient conditions, which ensure the reliability of clustering stability estimators in the large sample regime. In contrast to previous work, which concentrated on specific toy distributions or specific idealized clustering frameworks, here we make no such assumptions. We then exemplify how these conditions apply to several important families of clustering algorithms, such as maximum likelihood clustering, certain types of kernel clustering, and centroid-based clustering with any Bregman divergence. In addition, we explicitly derive the non-trivial asymptotic behavior of these estimators, for any framework satisfying our conditions. This can help us understand what is considered a 'stable' model by these estimators, at least for large enough samples.},
 author = {Shamir, Ohad and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cee631121c2ec9232f3a2f028ad5c89b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cee631121c2ec9232f3a2f028ad5c89b-Metadata.json},
 openalex = {W2158813839},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cee631121c2ec9232f3a2f028ad5c89b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cee631121c2ec9232f3a2f028ad5c89b-Supplemental.zip},
 title = {On the Reliability of Clustering Stability in the Large Sample Regime},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/cee631121c2ec9232f3a2f028ad5c89b-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_cfa0860e,
 abstract = {We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn first-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has_wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, -3) ∈ inverse or (has_husband, has_wife) ∈ higher_oppsex. We further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations +3 or has_wife even though it has not been trained on any first-order examples involving these relations.},
 author = {Sutskever, Ilya and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cfa0860e83a4c3a763a7e62d825349f7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cfa0860e83a4c3a763a7e62d825349f7-Metadata.json},
 openalex = {W2131854866},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/cfa0860e83a4c3a763a7e62d825349f7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Using matrices to model symbolic relationship},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/cfa0860e83a4c3a763a7e62d825349f7-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d04d42cd,
 abstract = {Decision making lies at the very heart of many psychiatric diseases. It is also a central theoretical concern in a wide variety of fields and has undergone detailed, in-depth, analyses. We take as an example Major Depressive Disorder (MDD), applying insights from a Bayesian reinforcement learning framework. We focus on anhedonia and helplessness. Helplessness—a core element in the conceptualizations of MDD that has lead to major advances in its treatment, pharmacological and neurobiological understanding—is formalized as a simple prior over the outcome entropy of actions in uncertain environments. Anhedonia, which is an equally fundamental aspect of the disease, is related to the effective reward size. These formulations allow for the design of specific tasks to measure anhedonia and helplessness behaviorally. We show that these behavioral measures capture explicit, questionnaire-based cognitions. We also provide evidence that these tasks may allow classification of subjects into healthy and MDD groups based purely on a behavioural measure and avoiding any verbal reports.},
 author = {Huys, Quentin and Vogelstein, Joshua and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d04d42cdf14579cd294e5079e0745411-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d04d42cdf14579cd294e5079e0745411-Metadata.json},
 openalex = {W2111575575},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d04d42cdf14579cd294e5079e0745411-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Psychiatry: Insights into depression through normative decision-making models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d04d42cdf14579cd294e5079e0745411-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d07e70ef,
 abstract = {Embeddings of random variables in reproducing kernel Hilbert spaces (RKHSs) may be used to conduct statistical inference based on higher order moments. For sufficiently rich (characteristic) RKHSs, each probability distribution has a unique embedding, allowing all statistical properties of the distribution to be taken into consideration. Necessary and sufficient conditions for an RKHS to be characteristic exist for ℝn. In the present work, conditions are established for an RKHS to be characteristic on groups and semigroups. Illustrative examples are provided, including characteristic kernels on periodic domains, rotation matrices, and ℝn+.},
 author = {Fukumizu, Kenji and Gretton, Arthur and Sch\"{o}lkopf, Bernhard and Sriperumbudur, Bharath K.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d07e70efcfab08731a97e7b91be644de-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d07e70efcfab08731a97e7b91be644de-Metadata.json},
 openalex = {W2147943944},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d07e70efcfab08731a97e7b91be644de-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d07e70efcfab08731a97e7b91be644de-Supplemental.zip},
 title = {Characteristic Kernels on Groups and Semigroups},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d07e70efcfab08731a97e7b91be644de-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d1f491a4,
 abstract = {We present a multi-label multiple kernel learning (MKL) formulation in which the data are embedded into a low-dimensional space directed by the instance-label correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, which can be cast into a semi-infinite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem. In addition, we show that the objective function of the approximate formulation is differentiable with Lipschitz continuous gradient, and hence existing methods can be employed to compute the optimal solution efficiently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.},
 author = {Ji, Shuiwang and Sun, Liang and Jin, Rong and Ye, Jieping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d1f491a404d6854880943e5c3cd9ca25-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d1f491a404d6854880943e5c3cd9ca25-Metadata.json},
 openalex = {W2096032893},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multi-label Multiple Kernel Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d56b9fc4,
 abstract = {The visual and auditory map alignment in the Superior Colliculus (SC) of barn owl is important for its accurate localization for prey behavior. Prism learning or Blindness may interfere this alignment and cause loss of the capability of accurate prey. However, juvenile barn owl could recover its sensory map alignment by shifting its auditory map. The adaptation of this map alignment is believed based on activity dependent axon developing in Inferior Colliculus (IC). A model is built to explore this mechanism. In this model, axon growing process is instructed by an inhibitory network in SC while the strength of the inhibition adjusted by Spike Timing Dependent Plasticity (STDP). We test and analyze this mechanism by application of the neural structures involved in spatial localization in a robotic system.},
 author = {Huo, Juan and Yang, Zhijun and Murray, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Metadata.json},
 openalex = {W2108936281},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d56b9fc4b0f1be8871f5e1c40c0067e7-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d58072be,
 abstract = {Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to efficiently calculate the code of a novel data-point. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art.},
 author = {Weiss, Yair and Torralba, Antonio and Fergus, Rob},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d58072be2820e8682c0a27c0518e805e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d58072be2820e8682c0a27c0518e805e-Metadata.json},
 openalex = {W2293597654},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d58072be2820e8682c0a27c0518e805e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Spectral Hashing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d58072be2820e8682c0a27c0518e805e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d5cfead9,
 abstract = {The essence of exploration is acting to try to decrease uncertainty. We propose a new methodology for representing uncertainty in continuous-state control problems. Our approach, multi-resolution exploration (MRE), uses a hierarchical mapping to identify regions of the state space that would benefit from additional samples. We demonstrate MRE's broad utility by using it to speed up learning in a prototypical model-based and value-based reinforcement-learning method. Empirical results show that MRE improves upon state-of-the-art exploration approaches.},
 author = {Nouri, Ali and Littman, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d5cfead94f5350c12c322b5b664544c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d5cfead94f5350c12c322b5b664544c1-Metadata.json},
 openalex = {W2106907982},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multi-resolution Exploration in Continuous Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d5cfead94f5350c12c322b5b664544c1-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d709f38e,
 abstract = {We introduce a new reinforcement-learning model for the role of the hippocampus in classical conditioning, focusing on the differences between trace and delay conditioning. In the model, all stimuli are represented both as unindividuated wholes and as a series of temporal elements with varying delays. These two stimulus representations interact, producing different patterns of learning in trace and delay conditioning. The model proposes that hippocampal lesions eliminate long-latency temporal elements, but preserve short-latency temporal elements. For trace conditioning, with no contiguity between cue and reward, these long-latency temporal elements are necessary for learning adaptively timed responses. For delay conditioning, the continued presence of the cue supports conditioned responding, and the short-latency elements suppress responding early in the cue. In accord with the empirical data, simulated hippocampal damage impairs trace conditioning, but not delay conditioning, at medium-length intervals. With longer intervals, learning is impaired in both procedures, and, with shorter intervals, in neither. In addition, the model makes novel predictions about the response topography with extended cues or post-training lesions. These results demonstrate how temporal contiguity, as in delay conditioning, changes the timing problem faced by animals, rendering it both easier and less susceptible to disruption by hippocampal lesions.},
 author = {Ludvig, Elliot and Sutton, Richard S and Verbeek, Eric and Kehoe, E.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d709f38ef758b5066ef31b18039b8ce5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d709f38ef758b5066ef31b18039b8ce5-Metadata.json},
 openalex = {W2142650989},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d709f38ef758b5066ef31b18039b8ce5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A computational model of hippocampal function in trace conditioning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d709f38ef758b5066ef31b18039b8ce5-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d736bb10,
 abstract = {We continue our study of online prediction of the labelling of a graph. We show a fundamental limitation of Laplacian-based algorithms: if the graph has a large diameter then the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices, even when tackling simple problems. We overcome this drawback by means of an efficient algorithm which achieves a logarithmic mistake bound. It is based on the notion of a spine, a path graph which provides a linear embedding of the original graph. In practice, graphs may exhibit cluster structure; thus in the last part, we present a modified algorithm which achieves the best of both worlds: it performs well locally in the presence of cluster structure, and globally on large diameter graphs.},
 author = {Herbster, Mark and Lever, Guy and Pontil, Massimiliano},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d736bb10d83a904aefc1d6ce93dc54b8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d736bb10d83a904aefc1d6ce93dc54b8-Metadata.json},
 openalex = {W2139989320},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d736bb10d83a904aefc1d6ce93dc54b8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Online Prediction on Large Diameter Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d736bb10d83a904aefc1d6ce93dc54b8-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d93ed5b6,
 abstract = {We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks.},
 author = {Archambeau, C\'{e}dric and Bach, Francis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Metadata.json},
 openalex = {W2126497681},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sparse probabilistic projections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d93ed5b6db83be78efb0d05ae420158e-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d947bf06,
 abstract = {In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the epsilon-insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we briefly discuss a trade-off in epsilon between sparsity and accuracy if the SVM is used to estimate the conditional median.},
 author = {Steinwart, Ingo and Christmann, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d947bf06a885db0d477d707121934ff8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d947bf06a885db0d477d707121934ff8-Metadata.json},
 openalex = {W2164707709},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d947bf06a885db0d477d707121934ff8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sparsity of SVMs that use the epsilon-insensitive loss},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d947bf06a885db0d477d707121934ff8-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_d96409bf,
 abstract = {Many machine learning algorithms require the summation of Gaussian kernel functions, an expensive operation if implemented straightforwardly. Several methods have been proposed to reduce the computational complexity of evaluating such sums, including tree and analysis based methods. These achieve varying speedups depending on the bandwidth, dimension, and prescribed error, making the choice between methods difficult for machine learning tasks. We provide an algorithm that combines tree methods with the Improved Fast Gauss Transform (IFGT). As originally proposed the IFGT suffers from two problems: (1) the Taylor series expansion does not perform well for very low bandwidths, and (2) parameter selection is not trivial and can drastically affect performance and ease of use. We address the first problem by employing a tree data structure, resulting in four evaluation methods whose performance varies based on the distribution of sources and targets and input parameters such as desired accuracy and bandwidth. To solve the second problem, we present an online tuning approach that results in a black box method that automatically chooses the evaluation method and its parameters to yield the best performance for the input data, desired accuracy, and bandwidth. In addition, the new IFGT parameter selection approach allows for tighter error bounds. Our approach chooses the fastest method at negligible additional cost, and has superior performance in comparisons with previous approaches.},
 author = {Morariu, Vlad and Srinivasan, Balaji and Raykar, Vikas C and Duraiswami, Ramani and Davis, Larry S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d96409bf894217686ba124d7356686c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d96409bf894217686ba124d7356686c9-Metadata.json},
 openalex = {W2099027195},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d96409bf894217686ba124d7356686c9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Automatic online tuning for fast Gaussian summation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/d96409bf894217686ba124d7356686c9-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_da4fb5c6,
 abstract = {We consider the problem of transforming a signal to a representation in which the components are statistically independent. When the signal is generated as a linear transformation of independent Gaussian or non-Gaussian sources, the solution may be computed using a linear transformation (PCA or ICA, respectively). Here, we consider a complementary case, in which the source is non-Gaussian but elliptically symmetric. Such a source cannot be decomposed into independent components using a linear transform, but we show that a simple nonlinear transformation, which we call radial Gaussianization (RG), is able to remove all dependencies. We apply this methodology to natural signals, demonstrating that the joint distributions of nearby bandpass filter responses, for both sounds and images, are closer to being elliptically symmetric than linearly transformed factorial sources. Consistent with this, we demonstrate that the reduction in dependency achieved by applying RG to either pairs or blocks of bandpass filter responses is significantly greater than that achieved by PCA or ICA.},
 author = {Lyu, Siwei and Simoncelli, Eero},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/da4fb5c6e93e74d3df8527599fa62642-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/da4fb5c6e93e74d3df8527599fa62642-Metadata.json},
 openalex = {W2151442982},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Reducing statistical dependencies in natural signals using radial Gaussianization.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/da4fb5c6e93e74d3df8527599fa62642-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_dc6a7071,
 abstract = {We propose two transductive bounds on the risk of majority votes that are estimated over partially labeled training sets. The first one involves the margin distribution of the classifier and a risk bound on its associate Gibbs classifier. The bound is tight when so is the Gibbs's bound and when the errors of the majority vote classifier is concentrated on a zone of low margin. In semi-supervised learning, considering the margin as an indicator of confidence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions. Following this assumption, we propose to bound the error probability of the voted classifier on the examples for whose margins are above a fixed threshold. As an application, we propose a self-learning algorithm which iteratively assigns pseudo-labels to the set of unlabeled training examples that have their margin above a threshold obtained from this bound. Empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the TSVM in which the threshold is fixed manually.},
 author = {Amini, Massih R. and Usunier, Nicolas and Laviolette, Fran\c{c}ois},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/dc6a70712a252123c40d2adba6a11d84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/dc6a70712a252123c40d2adba6a11d84-Metadata.json},
 openalex = {W2129911503},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/dc6a70712a252123c40d2adba6a11d84-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/dc6a70712a252123c40d2adba6a11d84-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_dc82d632,
 abstract = {Existing approaches to nonrigid structure from motion assume that the instantaneous 3D shape of a deforming object is a linear combination of basis shapes, which have to be estimated anew for each video sequence. In contrast, we propose that the evolving 3D structure be described by a linear combination of basis trajectories. The principal advantage of this approach is that we do not need to estimate any basis vectors during computation. We show that generic bases over trajectories, such as the Discrete Cosine Transform (DCT) basis, can be used to compactly describe most real motions. This results in a significant reduction in unknowns, and corresponding stability in estimation. We report empirical performance, quantitatively using motion capture data, and qualitatively on several video sequences exhibiting nonrigid motions including piece-wise rigid motion, partially nonrigid motion (such as a facial expression), and highly nonrigid motion (such as a person dancing).},
 author = {Akhter, Ijaz and Sheikh, Yaser and Khan, Sohaib and Kanade, Takeo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Metadata.json},
 openalex = {W2138816964},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Nonrigid Structure from Motion in Trajectory Space},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/dc82d632c9fcecb0778afbc7924494a6-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_df263d99,
 abstract = {We address the problem of learning classifiers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small - possibly even empty - labeled samples and large unlabeled samples are available. While the unlabeled samples reflect the target distribution, the labeled samples may be biased. This setting is motivated by the problem of predicting sociodemo-graphic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a portion of each portal's users produce biased samples. We derive a transfer learning procedure that produces resampling weights which match the pool of all examples to the target distribution of any given task. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy.},
 author = {Bickel, Steffen and Sawade, Christoph and Scheffer, Tobias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/df263d996281d984952c07998dc54358-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/df263d996281d984952c07998dc54358-Metadata.json},
 openalex = {W2152231303},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/df263d996281d984952c07998dc54358-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Transfer Learning by Distribution Matching for Targeted Advertising},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/df263d996281d984952c07998dc54358-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e0c64119,
 abstract = {We introduce the first temporal-difference learning algorithm that is stable with linear function approximation and off-policy training, for any finite Markov decision process, target policy, and exciting behavior policy, and whose complexity scales linearly in the number of parameters. We consider an i.i.d. policy-evaluation setting in which the data need not come from on-policy experience. The gradient temporal-difference (GTD) algorithm estimates the expected update vector of the TD(0) algorithm and performs stochastic gradient descent on its L_2 norm. Our analysis proves that its expected update is in the direction of the gradient, assuring convergence under the usual stochastic approximation conditions to the same least-squares solution as found by the LSTD, but without its quadratic computational complexity. GTD is online and incremental, and does not involve multiplying by products of likelihood ratios as in importance-sampling methods.},
 author = {Sutton, Richard S and Maei, Hamid and Szepesv\'{a}ri, Csaba},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e0c641195b27425bb056ac56f8953d24-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e0c641195b27425bb056ac56f8953d24-Metadata.json},
 openalex = {W2165905123},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e0cf1f47,
 abstract = {Query expansion is a long-studied approach for improving retrieval effectiveness by enhancing the user's original query with additional related words. Current algorithms for automatic query expansion can often improve retrieval accuracy on average, but are not robust: that is, they are highly unstable and have poor worst-case performance for individual queries. To address this problem, we introduce a novel formulation of query expansion as a convex optimization problem over a word graph. The model combines initial weights from a baseline feedback algorithm with edge weights based on word similarity, and integrates simple constraints to enforce set-based criteria such as aspect balance, aspect coverage, and term centrality. Results across multiple standard test collections show consistent and significant reductions in the number and magnitude of expansion failures, while retaining the strong positive gains of the baseline algorithm. Our approach does not assume a particular retrieval model, making it applicable to a broad class of existing expansion algorithms.},
 author = {Collins-thompson, Kevyn},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e0cf1f47118daebc5b16269099ad7347-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e0cf1f47118daebc5b16269099ad7347-Metadata.json},
 openalex = {W2108877333},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e0cf1f47118daebc5b16269099ad7347-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Estimating Robust Query Models with Convex Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e0cf1f47118daebc5b16269099ad7347-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e0ec453e,
 abstract = {We introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters. The algorithms work by maximizing the dependence between the taxonomy and the original data. The resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-of-the-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach). We demonstrate our algorithm on image and text data.},
 author = {Blaschko, Matthew and Gretton, Arthur},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e0ec453e28e061cc58ac43f91dc2f3f0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e0ec453e28e061cc58ac43f91dc2f3f0-Metadata.json},
 openalex = {W2140067190},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e0ec453e28e061cc58ac43f91dc2f3f0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning Taxonomies by Dependence Maximization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e0ec453e28e061cc58ac43f91dc2f3f0-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e2230b85,
 abstract = {Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditional-independence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable.},
 author = {Pellet, Jean-philippe and Elisseeff, Andr\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e2230b853516e7b05d79744fbd4c9c13-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e2230b853516e7b05d79744fbd4c9c13-Metadata.json},
 openalex = {W2145390121},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e2230b853516e7b05d79744fbd4c9c13-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e2230b853516e7b05d79744fbd4c9c13-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e369853d,
 abstract = {Research in animal learning and behavioral neuroscience has distinguished between two forms of action control: a habit-based form, which relies on stored actio n values, and a goal-dir ected form, which forecasts and compares action outcomes based on a model of the environment. While habit-based control has been the subject of extensive computational research, the computational principles underlying goal-directed control in animals have so far received less attention. In the present paper, we advance a computational framework for goal-directed control in animals and humans. We take three empirically motivated points as founding premises: (1) Neurons in dorsolateral prefrontal cortex represent action policies, (2) Neurons in orbitofrontal cortex represent rewards, and (3) Neural computation, across domains, can be appropriately understood as performing structured probabilistic inference. On a purely computational level, the resulting account relates closely to previous work using Bayesian inference to solve Markov decision problems, but extends this work by introducing a new algorithm, which provably converges on optimal plans. On a cognitive and neuroscientific level, the theory provides a unifying framework for several different forms of goal-directed action selection, placing emphasis on a novel form, within which orbitofrontal reward representations directly drive policy selection.},
 author = {Botvinick, Matthew and An, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e369853df766fa44e1ed0ff613f563bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e369853df766fa44e1ed0ff613f563bd-Metadata.json},
 openalex = {W2109925931},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e369853df766fa44e1ed0ff613f563bd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Goal-directed decision making in prefrontal cortex: A computational framework.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e369853df766fa44e1ed0ff613f563bd-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e3796ae8,
 abstract = {We develop a supervised dimension reduction method that integrates the idea of localization from manifold learning with the sliced inverse regression framework. We call our method localized sliced inverse regression (LSIR) since it takes into account the local structure of the explanatory variables. The resulting projection from LSIR is a linear subspace of the explanatory variables that captures the nonlinear structure relevant to predicting the response. LSIR applies to both classification and regression problems and can be easily extended to incorporate the ancillary unlabeled data in semi-supervised learning. We illustrate the utility of LSIR on real and simulated data. Computer codes and datasets from simulations are available online.},
 author = {Wu, Qiang and Mukherjee, Sayan and Liang, Feng},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e3796ae838835da0b6f6ea37bcf8bcb7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e3796ae838835da0b6f6ea37bcf8bcb7-Metadata.json},
 openalex = {W2072738586},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e3796ae838835da0b6f6ea37bcf8bcb7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Localized Sliced Inverse Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e3796ae838835da0b6f6ea37bcf8bcb7-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e4a6222c,
 abstract = {For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s,s' there is a policy which moves from s to s' in at most D steps (on average). We present a reinforcement learning algorithm with total regret O(DS√AT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of Ω(√DSAT) on the total regret of any learning algorithm is given as well.

These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T.

Finally, we also consider a setting where the MDP is allowed to change a fixed number of l times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound of O(l1/3T2/3DS√A).},
 author = {Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e4a6222cdb5b34375400904f03d8e6a5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e4a6222cdb5b34375400904f03d8e6a5-Metadata.json},
 openalex = {W1850488217},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e4a6222cdb5b34375400904f03d8e6a5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Near-optimal Regret Bounds for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e4a6222cdb5b34375400904f03d8e6a5-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e555ebe0,
 abstract = {This paper discusses non-parametric regression between Riemannian manifolds. This learning problem arises frequently in many application areas ranging from signal processing, computer vision, over robotics to computer graphics. We present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization. The regularization functional takes into account the geometry of input and output manifold, and we show that it implements a prior which is particularly natural. Moreover, we demonstrate that our algorithm performs well in a difficult surface registration problem.},
 author = {Steinke, Florian and Hein, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e555ebe0ce426f7f9b2bef0706315e0c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e555ebe0ce426f7f9b2bef0706315e0c-Metadata.json},
 openalex = {W2152790960},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e555ebe0ce426f7f9b2bef0706315e0c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e555ebe0ce426f7f9b2bef0706315e0c-Supplemental.zip},
 title = {Non-parametric Regression Between Manifolds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e555ebe0ce426f7f9b2bef0706315e0c-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e7061188,
 abstract = {Polysemy is a problem for methods that exploit image search engines to build object category models. Existing unsupervised approaches do not take word sense into consideration. We propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data. The use of LDA to discover a latent sense space makes the model robust despite the very limited nature of dictionary definitions. The definitions are used to learn a distribution in the latent space that best represents a sense. The algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense. An object classifier is trained on the resulting sense-specific images. We evaluate our method on a dataset obtained by searching the web for polysemous words. Category classification experiments show that our dictionary-based approach outperforms baseline methods.},
 author = {Saenko, Kate and Darrell, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e70611883d2760c8bbafb4acb29e3446-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e70611883d2760c8bbafb4acb29e3446-Metadata.json},
 openalex = {W2154744205},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e70611883d2760c8bbafb4acb29e3446-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Learning of Visual Sense Models for Polysemous Words},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e70611883d2760c8bbafb4acb29e3446-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e7f8a7fb,
 abstract = {Subspace-based learning problems involve data whose elements are linear sub-spaces of a vector space. To handle such data structures, Grassmann kernels have been proposed and used previously. In this paper, we analyze the relationship between Grassmann kernels and probabilistic similarity measures. Firstly, we show that the KL distance in the limit yields the Projection kernel on the Grassmann manifold, whereas the Bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems. Secondly, based on our analysis of the KL distance, we propose extensions of the Projection kernel which can be extended to the set of affine as well as scaled subspaces. We demonstrate the advantages of these extended kernels for classification and recognition tasks with Support Vector Machines and Kernel Discriminant Analysis using synthetic and real image databases.},
 author = {Hamm, Jihun and Lee, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e7f8a7fb0b77bcb3b283af5be021448f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e7f8a7fb0b77bcb3b283af5be021448f-Metadata.json},
 openalex = {W2107081403},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e7f8a7fb0b77bcb3b283af5be021448f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Extended Grassmann Kernels for Subspace-Based Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e7f8a7fb0b77bcb3b283af5be021448f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e820a45f,
 abstract = {We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures three-way interactions among visible units, hidden units, and a single hidden discrete variable that represents the cluster label. The distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized. Instead, they are defined implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reflect the class structure in the data.},
 author = {Nair, Vinod and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e820a45f1dfc7b95282d10b6087e11c0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e820a45f1dfc7b95282d10b6087e11c0-Metadata.json},
 openalex = {W2149845449},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Mixtures of Restricted Boltzmann Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e820a45f1dfc7b95282d10b6087e11c0-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_e8a642ed,
 abstract = {Large networks of spiking neurons show abrupt changes in their collective dynamics resembling phase transitions studied in statistical physics. An example of this phenomenon is the transition from irregular, noise-driven dynamics to regular, self-sustained behavior observed in networks of integrate-and-fire neurons as the interaction strength between the neurons increases. In this work we show how a network of spiking neurons is able to self-organize towards a critical state for which the range of possible inter-spike-intervals (dynamic range) is maximized. Self-organization occurs via synaptic dynamics that we analytically derive. The resulting plasticity rule is defined locally so that global homeostasis near the critical state is achieved by local regulation of individual synapses.},
 author = {G\'{o}mez, Vicen\c{c}c and Kaltenbrunner, Andreas and L\'{o}pez, Vicente and Kappen, Hilbert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e8a642ed6a9ad20fb159472950db3d65-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e8a642ed6a9ad20fb159472950db3d65-Metadata.json},
 openalex = {W2125660399},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/e8a642ed6a9ad20fb159472950db3d65-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Self-organization using synaptic plasticity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/e8a642ed6a9ad20fb159472950db3d65-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_eb160de1,
 abstract = {We propose a novel application of Formal Concept Analysis (FCA) to neural decoding: instead of just trying to figure out which stimulus was presented, we demonstrate how to explore the semantic relationships in the neural representation of large sets of stimuli. FCA provides a way of displaying and interpreting such relationships via concept lattices. We explore the effects of neural code sparsity on the lattice. We then analyze neurophysiological data from high-level visual cortical area STSa, using an exact Bayesian approach to construct the formal context needed by FCA. Prominent features of the resulting concept lattices are discussed, including hierarchical face representation and indications for a product-of-experts code in real neurons.},
 author = {Endres, Dominik and Foldiak, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eb160de1de89d9058fcb0b968dbbbd68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eb160de1de89d9058fcb0b968dbbbd68-Metadata.json},
 openalex = {W2132155633},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eb160de1de89d9058fcb0b968dbbbd68-Supplemental.zip},
 title = {Interpreting the neural code with Formal Concept Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/eb160de1de89d9058fcb0b968dbbbd68-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_ebd9629f,
 abstract = {This paper studies global ranking problem by learning to rank methods. Conventional learning to rank methods are usually designed for 'local ranking', in the sense that the ranking model is defined on a single object, for example, a document in information retrieval. For many applications, this is a very loose approximation. Relations always exist between objects and it is better to define the ranking model as a function on all the objects to be ranked (i.e., the relations are also included). This paper refers to the problem as global ranking and proposes employing a Continuous Conditional Random Fields (CRF) for conducting the learning task. The Continuous CRF model is defined as a conditional probability distribution over ranking scores of objects conditioned on the objects. It can naturally represent the content information of objects as well as the relation information between objects, necessary for global ranking. Taking two specific information retrieval tasks as examples, the paper shows how the Continuous CRF method can perform global ranking better than baselines.},
 author = {Qin, Tao and Liu, Tie-yan and Zhang, Xu-dong and Wang, De-sheng and Li, Hang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Metadata.json},
 openalex = {W2159973364},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Global Ranking Using Continuous Conditional Random Fields},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/ebd9629fc3ae5e9f6611e2ee05a31cef-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_ec5aa0b7,
 abstract = {A series of corrections is developed for the fixed points of Expectation Propagation (EP), which is one of the most popular methods for approximate probabilistic inference. These corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when EP yields unrealiable results.},
 author = {Opper, Manfred and Paquet, Ulrich and Winther, Ole},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ec5aa0b7846082a2415f0902f0da88f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ec5aa0b7846082a2415f0902f0da88f2-Metadata.json},
 openalex = {W2146803603},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Improving on Expectation Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_eddea82a,
 abstract = {We present a discriminative part-based approach for human action recognition from video sequences using motion features. Our model is based on the recently proposed hidden conditional random field (hCRF) for object recognition. Similar to hCRF for object recognition, we model a human action by a flexible constellation of parts conditioned on image observations. Different from object recognition, our model combines both large-scale global features and local patch features to distinguish various actions. Our experimental results show that our model is comparable to other state-of-the-art approaches in action recognition. In particular, our experimental results demonstrate that combining large-scale global features and local patch features performs significantly better than directly applying hCRF on local patches alone.},
 author = {Wang, Yang and Mori, Greg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eddea82ad2755b24c4e168c5fc2ebd40-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eddea82ad2755b24c4e168c5fc2ebd40-Metadata.json},
 openalex = {W2140978750},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning a discriminative hidden part model for human action recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_edfbe1af,
 abstract = {How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? One could think of using template matching or matching pursuit to find the arbitrarily shifted linear components. However, traditional matching approaches require that the templates be known a priori. To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semi-NMF) that we extend to allow for time shifts when matching the templates to the signal. The algorithm estimates templates directly from the data along with their non-negative amplitudes. The resulting method can be thought of as an adaptive template matching procedure. We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. On these data the algorithm essentially performs spike detection and unsupervised spike clustering. Results on simulated data and extracellular recordings indicate that the method performs well for signal-to-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are sufficiently different.},
 author = {Roux, Jonathan and Cheveign\'{e}, Alain and Parra, Lucas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/edfbe1afcf9246bb0d40eb4d8027d90f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/edfbe1afcf9246bb0d40eb4d8027d90f-Metadata.json},
 openalex = {W2117110810},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Template Matching with Shift-Invariant Semi-NMF},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/edfbe1afcf9246bb0d40eb4d8027d90f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_ee14c41e,
 abstract = {Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the firing rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efficient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird.},
 author = {Katahira, Kentaro and Nishikawa, Jun and Okanoya, Kazuo and Okada, Masato},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ee14c41e92ec5c97b54cf9b74e25bd99-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ee14c41e92ec5c97b54cf9b74e25bd99-Metadata.json},
 openalex = {W2163194183},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ee14c41e92ec5c97b54cf9b74e25bd99-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/ee14c41e92ec5c97b54cf9b74e25bd99-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_eecca5b6,
 abstract = {Learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive, sometime unattainable fully annotated training data. While likelihood-based methods have been extensively explored, to our knowledge, learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem. In this paper, we present a partially observed Maximum Entropy Discrimination Markov Network (PoMEN) model that attempts to combine the advantages of Bayesian and margin based paradigms for learning Markov networks from partially labeled data. PoMEN leads to an averaging prediction rule that resembles a Bayes predictor that is more robust to overfitting, but is also built on the desirable discriminative laws resemble those of the M3N. We develop an EM-style algorithm utilizing existing convex optimization algorithms for M3N as a subroutine. We demonstrate competent performance of PoMEN over existing methods on a real-world web data extraction task.},
 author = {Zhu, Jun and Xing, Eric and Zhang, Bo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eecca5b6365d9607ee5a9d336962c534-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eecca5b6365d9607ee5a9d336962c534-Metadata.json},
 openalex = {W2139653464},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Partially Observed Maximum Entropy Discrimination Markov Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/eecca5b6365d9607ee5a9d336962c534-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f0e52b27,
 abstract = {Compared to invasive Brain-Computer Interfaces (BCI), non-invasive BCI systems based on Electroencephalogram (EEG) signals have not been applied successfully for precisely timed control tasks. In the present study, however, we demonstrate and report on the interaction of subjects with a real device: a pinball machine. Results of this study clearly show that fast and well-timed control well beyond chance level is possible, even though the environment is extremely rich and requires precisely timed and complex predictive behavior. Using machine learning methods for mental state decoding, BCI-based pinball control is possible within the first session without the necessity to employ lengthy subject training. The current study shows clearly that very compelling control with excellent timing and dynamics is possible for a non-invasive BCI.},
 author = {Krauledat, Matthias and Grzeska, Konrad and Sagebaum, Max and Blankertz, Benjamin and Vidaurre, Carmen and M\"{u}ller, Klaus-Robert and Schr\"{o}der, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Metadata.json},
 openalex = {W2154716307},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Supplemental.zip},
 title = {Playing Pinball with non-invasive BCI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f0e52b27a7a5d6a1a87373dffa53dbe5-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f11bec14,
 abstract = {We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use different priors.},
 author = {Cohen, Shay and Gimpel, Kevin and Smith, Noah A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f11bec1411101c743f64df596773d0b2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f11bec1411101c743f64df596773d0b2-Metadata.json},
 openalex = {W2104846675},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f11bec1411101c743f64df596773d0b2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f11bec1411101c743f64df596773d0b2-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f1c15925,
 abstract = {We derive risk bounds for the randomized classifiers in Sample Compression setting where the classifier-specification utilizes two sources of information viz. the compression set and the message string. By extending the recently proposed Occam's Hammer principle to the data-dependent settings, we derive point-wise versions of the bounds on the stochastic sample compressed classifiers and also recover the corresponding classical PAC-Bayes bound. We further show how these compare favorably to the existing results.},
 author = {Shah, Mohak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f1c1592588411002af340cbaedd6fc33-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f1c1592588411002af340cbaedd6fc33-Metadata.json},
 openalex = {W2136945987},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Risk Bounds for Randomized Sample Compressed Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f2201f51,
 abstract = {In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any specific task in the same space through regularization. In an empirical study, we construct 190 different text classification tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classification tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the specific task to be enhanced, and the prediction model used.},
 author = {Zhang, Yi and Dubrawski, Artur and Schneider, Jeff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f2201f5191c4e92cc5af043eebfd0946-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f2201f5191c4e92cc5af043eebfd0946-Metadata.json},
 openalex = {W2151923808},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f2201f5191c4e92cc5af043eebfd0946-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f387624d,
 abstract = {We consider a generalization of stochastic bandit problems where the set of arms, X, is allowed to be a generic topological space. We constraint the mean-payoff function with a dissimilarity function over X in a way that is more general than Lipschitz. We construct an arm selection policy whose regret improves upon previous result for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally Holder with a known exponent, then the expected regret is bounded up to a logarithmic factor by $\sqrt{n}$, i.e., the rate of the growth of the regret is independent of the dimension of the space. Moreover, we prove the minimax optimality of our algorithm for the class of mean-payoff functions we consider.},
 author = {Bubeck, S\'{e}bastien and Stoltz, Gilles and Szepesv\'{a}ri, Csaba and Munos, R\'{e}mi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f387624df552cea2f369918c5e1e12bc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f387624df552cea2f369918c5e1e12bc-Metadata.json},
 openalex = {W2268509491},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f387624df552cea2f369918c5e1e12bc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f387624df552cea2f369918c5e1e12bc-Supplemental.zip},
 title = {Online Optimization in X-Armed Bandits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f387624df552cea2f369918c5e1e12bc-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f4b9ec30,
 abstract = {Mixture of Gaussian processes models extended a single Gaussian process with ability of modeling multi-modal data and reduction of training complexity. Previous inference algorithms for these models are mostly based on Gibbs sampling, which can be very slow, particularly for large-scale data sets. We present a new generative mixture of experts model. Each expert is still a Gaussian process but is reformulated by a linear model. This breaks the dependency among training outputs and enables us to use a much faster variational Bayesian algorithm for training. Our gating network is more flexible than previous generative approaches as inputs for each expert are modeled by a Gaussian mixture model. The number of experts and number of Gaussian components for an expert are inferred automatically. A variety of tests show the advantages of our method.},
 author = {Yuan, Chao and Neubauer, Claus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f4b9ec30ad9f68f89b29639786cb62ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f4b9ec30ad9f68f89b29639786cb62ef-Metadata.json},
 openalex = {W2095849032},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Variational Mixture of Gaussian Process Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f5deaeea,
 abstract = {The machine learning problem of classifier design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the specification of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the specification of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classification problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classifier design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations.},
 author = {Masnadi-shirazi, Hamed and Vasconcelos, Nuno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f5deaeeae1538fb6c45901d524ee2f98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f5deaeeae1538fb6c45901d524ee2f98-Metadata.json},
 openalex = {W2136688338},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f5deaeeae1538fb6c45901d524ee2f98-Supplemental.zip},
 title = {On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f5deaeeae1538fb6c45901d524ee2f98-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f5f8590c,
 abstract = {Learning dynamic Bayesian network structures provides a principled mechanism for identifying conditional dependencies in time-series data. An important assumption of traditional DBN structure learning is that the data are generated by a stationary process, an assumption that is not true in many important settings. In this paper, we introduce a new class of graphical model called a non-stationary dynamic Bayesian network, in which the conditional dependence structure of the underlying data-generation process is permitted to change over time. Non-stationary dynamic Bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time. Some examples of evolving networks are transcriptional regulatory networks during an organism's development, neural pathways during learning, and traffic patterns during the day. We define the non-stationary DBN model, present an MCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data.},
 author = {Robinson, Joshua and Hartemink, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f5f8590cd58a54e94377e6ae2eded4d9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f5f8590cd58a54e94377e6ae2eded4d9-Metadata.json},
 openalex = {W1593373597},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f5f8590cd58a54e94377e6ae2eded4d9-Supplemental.zip},
 title = {Learning Non-Stationary Dynamic Bayesian Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f5f8590cd58a54e94377e6ae2eded4d9-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f7664060,
 abstract = {The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to fit them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identified. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identification power provided by nonlinearities.},
 author = {Hoyer, Patrik and Janzing, Dominik and Mooij, Joris M and Peters, Jonas and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f7664060cc52bc6f3d620bcedc94a4b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f7664060cc52bc6f3d620bcedc94a4b6-Metadata.json},
 openalex = {W2165582599},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Nonlinear causal discovery with additive noise models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f76a89f0,
 abstract = {We present a novel mathematical formalism for the idea of a of an uncontrolled dynamical system, a model that makes only certain predictions in only certain situations. As a result of its restricted responsibilities, a local model may be far simpler than a complete model of the system. We then show how one might combine several local models to produce a more detailed model. We demonstrate our ability to learn a collection of local models on a large-scale example and do a preliminary empirical comparison of learning a collection of local models and some other model learning methods.},
 author = {Talvitie, Erik and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f76a89f0cb91bc419542ce9fa43902dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f76a89f0cb91bc419542ce9fa43902dc-Metadata.json},
 openalex = {W2161485371},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f76a89f0cb91bc419542ce9fa43902dc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Simple Local Models for Complex Dynamical Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f76a89f0cb91bc419542ce9fa43902dc-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f79921bb,
 abstract = {Recently, fitted Q-iteration (FQI) based methods have become more popular due to their increased sample efficiency, a more stable learning process and the higher quality of the resulting policy. However, these methods remain hard to use for continuous action spaces which frequently occur in real-world tasks, e.g., in robotics and other technical applications. The greedy action selection commonly used for the policy improvement step is particularly problematic as it is expensive for continuous actions, can cause an unstable learning process, introduces an optimization bias and results in highly non-smooth policies unsuitable for real-world systems. In this paper, we show that by using a soft-greedy action selection the policy improvement step used in FQI can be simplified to an inexpensive advantage-weighted regression. With this result, we are able to derive a new, computationally efficient FQI algorithm which can even deal with high dimensional action spaces.},
 author = {Neumann, Gerhard and Peters, Jan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f79921bbae40a577928b76d2fc3edc2a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f79921bbae40a577928b76d2fc3edc2a-Metadata.json},
 openalex = {W2126685977},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fitted Q-iteration by Advantage Weighted Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f79921bbae40a577928b76d2fc3edc2a-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f90f2aca,
 abstract = {This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of PEGASOS (with high probability), a recently proposed method for solving the SVM optimization problem.},
 author = {Kakade, Sham M and Tewari, Ambuj},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f90f2aca5c640289d0a29417bcb63a37-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f90f2aca5c640289d0a29417bcb63a37-Metadata.json},
 openalex = {W2138682935},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f90f2aca5c640289d0a29417bcb63a37-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On the Generalization Ability of Online Strongly Convex Programming Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f90f2aca5c640289d0a29417bcb63a37-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_f9b902fc,
 abstract = {We introduce a new interpretation of multiscale random fields (MSRFs) that admits efficient optimization in the framework of regular (single level) random fields (RFs). It is based on a new operator, called append, that combines sets of random variables (RVs) to single RVs. We assume that a MSRF can be decomposed into disjoint trees that link RVs at different pyramid levels. The append operator is then applied to map RVs in each tree structure to a single RV. We demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images. It provides a natural representation of multiscale contour models, which is needed in order to cope with unstable contour decompositions. The append operator allows us to find optimal image segment labels using the classical framework of relaxation labeling. Alternative methods like Markov Chain Monte Carlo (MCMC) could also be used.},
 author = {Latecki, Longin and Lu, Chengen and Sobel, Marc and Bai, Xiang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f9b902fc3289af4dd08de5d1de54f68f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f9b902fc3289af4dd08de5d1de54f68f-Metadata.json},
 openalex = {W2119873625},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/f9b902fc3289af4dd08de5d1de54f68f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multiscale Random Fields with Application to Contour Grouping},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/f9b902fc3289af4dd08de5d1de54f68f-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_fa7cdfad,
 abstract = {Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three findings: (1) copula-based distributions provide significantly better fits than discretized multivariate normal distributions; (2) negative binomial margins fit the data significantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses.},
 author = {Onken, Arno and Gr\"{u}new\"{a}lder, Steffen and Munk, Matthias and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Metadata.json},
 openalex = {W2111761474},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_fa83a11a,
 abstract = {We tackle the computational problem of query-conditioned search. Given a machine-learned scoring rule and a query distribution, we build a predictive index by precomputing lists of potential results sorted based on an expected score of the result over future queries. The predictive index datastructure supports an anytime algorithm for approximate retrieval of the top elements. The general approach is applicable to webpage ranking, internet advertisement, and approximate nearest neighbor search. It is particularly effective in settings where standard techniques (e.g., inverted indices) are intractable. We experimentally find substantial improvement over existing methods for internet advertisement and approximate nearest neighbors.},
 author = {Goel, Sharad and Langford, John and Strehl, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fa83a11a198d5a7f0bf77a1987bcd006-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fa83a11a198d5a7f0bf77a1987bcd006-Metadata.json},
 openalex = {W2120165387},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fa83a11a198d5a7f0bf77a1987bcd006-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fa83a11a198d5a7f0bf77a1987bcd006-Supplemental.zip},
 title = {Predictive Indexing for Fast Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/fa83a11a198d5a7f0bf77a1987bcd006-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_fc49306d,
 abstract = {We investigate a topic at the interface of machine learning and cognitive science. Human active learning, where learners can actively query the world for information, is contrasted with passive learning from random examples. Furthermore, we compare human active learning performance with predictions from statistical learning theory. We conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood, and dramatically distinct. Our results indicate that humans are capable of actively selecting informative queries, and in doing so learn better and faster than if they are given random training data, as predicted by learning theory. However, the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms. To the best of our knowledge, this is the first quantitative study comparing human category learning in active versus passive settings.},
 author = {Castro, Rui and Kalish, Charles and Nowak, Robert and Qian, Ruichen and Rogers, Tim and Zhu, Jerry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fc49306d97602c8ed1be1dfbf0835ead-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fc49306d97602c8ed1be1dfbf0835ead-Metadata.json},
 openalex = {W2131495543},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fc49306d97602c8ed1be1dfbf0835ead-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Human Active Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/fc49306d97602c8ed1be1dfbf0835ead-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_fccb3cdc,
 abstract = {In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the IEDB MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non convex methods dedicated to the same problem.},
 author = {Jacob, Laurent and Vert, Jean-philippe and Bach, Francis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fccb3cdc9acc14a6e70a12f74560c026-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fccb3cdc9acc14a6e70a12f74560c026-Metadata.json},
 openalex = {W2949664970},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Clustered Multi-Task Learning: A Convex Formulation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/fccb3cdc9acc14a6e70a12f74560c026-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_fccb60fb,
 abstract = {Actor-critic algorithms for reinforcement learning are achieving renewed popularity due to their good convergence properties in situations where other approaches often fail (e.g., when function approximation is involved). Interestingly, there is growing evidence that actor-critic approaches based on phasic dopamine signals play a key role in biological learning through cortical and basal ganglia loops. We derive a temporal difference based actor critic learning algorithm, for which convergence can be proved without assuming widely separated time scales for the actor and the critic. The approach is demonstrated by applying it to networks of spiking neurons. The established relation between phasic dopamine and the temporal difference signal lends support to the biological relevance of such algorithms.},
 author = {Castro, Dotan and Volkinshtein, Dmitry and Meir, Ron},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fccb60fb512d13df5083790d64c4d5dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fccb60fb512d13df5083790d64c4d5dd-Metadata.json},
 openalex = {W2167144895},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fccb60fb512d13df5083790d64c4d5dd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fccb60fb512d13df5083790d64c4d5dd-Supplemental.zip},
 title = {Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/fccb60fb512d13df5083790d64c4d5dd-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_fe7ee8fc,
 abstract = {Conditional Random Sampling (CRS) was originally proposed for efficiently computing pairwise (l2, l1) distances, in static, large-scale, and sparse data. This study modifies the original CRS and extends CRS to handle dynamic or streaming data, which much better reflect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a significant advantage in that it is one-sketch-for-all. In particular, we demonstrate the effectiveness of CRS in efficiently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances.

We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval.},
 author = {Li, Ping and Church, Kenneth and Hastie, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fe7ee8fc1959cc7214fa21c4840dff0a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fe7ee8fc1959cc7214fa21c4840dff0a-Metadata.json},
 openalex = {W2112036337},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fe7ee8fc1959cc7214fa21c4840dff0a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {One sketch for all: Theory and Application of Conditional Random Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/fe7ee8fc1959cc7214fa21c4840dff0a-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_fe8c15fe,
 abstract = {We describe a novel class of distributions, called Mondrian processes, which can be interpreted as probability distributions over kd-tree data structures. Mondrian processes are multidimensional generalizations of Poisson processes and this connection allows us to construct multidimensional generalizations of the stick-breaking process described by Sethuraman (1994), recovering the Dirichlet process in one dimension. After introducing the Aldous-Hoover representation for jointly and separately exchangeable arrays, we show how the process can be used as a nonparametric prior distribution in Bayesian models of relational data.},
 author = {Roy, Daniel M and Teh, Yee},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fe8c15fed5f808006ce95eddb7366e35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fe8c15fed5f808006ce95eddb7366e35-Metadata.json},
 openalex = {W2144207912},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Mondrian Process},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/fe8c15fed5f808006ce95eddb7366e35-Abstract.html},
 volume = {21},
 year = {2008}
}

@inproceedings{NIPS2008_ffd52f3c,
 abstract = {We present a new family of linear time algorithms for string comparison with mismatches under the string kernels framework. Based on sufficient statistics, our algorithms improve theoretical complexity bounds of existing approaches while scaling well in sequence alphabet size, the number of allowed mismatches and the size of the dataset. In particular, on large alphabets and under loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure. We evaluate our algorithms on synthetic data and real applications in music genre classification, protein remote homology detection and protein fold prediction. The scalability of the algorithms allows us to consider complex sequence transformations, modeled using longer string features and larger numbers of mismatches, leading to a state-of-the-art performance with significantly reduced running times.},
 author = {Kuksa, Pavel and Huang, Pai-hsi and Pavlovic, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ffd52f3c7e12435a724a8f30fddadd9c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ffd52f3c7e12435a724a8f30fddadd9c-Metadata.json},
 openalex = {W2108837639},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/ffd52f3c7e12435a724a8f30fddadd9c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Scalable Algorithms for String Kernels with Inexact Matching},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/hash/ffd52f3c7e12435a724a8f30fddadd9c-Abstract.html},
 volume = {21},
 year = {2008}
}
