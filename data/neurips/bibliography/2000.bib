@inproceedings{NIPS2000_04df4d43,
 abstract = {Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(0) and V(0); the latter algorithm was used in the well-known TD-Gammon program.},
 author = {Gordon, Geoffrey J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/04df4d434d481c5bb723be1b6df1ee65-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/04df4d434d481c5bb723be1b6df1ee65-Metadata.json},
 openalex = {W2137466452},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning with Function Approximation Converges to a Region},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_05233523,
 abstract = {Low-dimensional representations are key to solving problems in high-level vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in accounting for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is reduced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates.},
 author = {Penev, Penio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/052335232b11864986bb2fa20fa38748-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/052335232b11864986bb2fa20fa38748-Metadata.json},
 openalex = {W2114375410},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/052335232b11864986bb2fa20fa38748-Paper.pdf},
 publisher = {MIT Press},
 title = {Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/052335232b11864986bb2fa20fa38748-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_0533a888,
 abstract = {We introduce a novel algorithm, termed PPA (Performance Prediction Algorithm), that quantitatively measures the contributions of elements of a neural system to the tasks it performs. The algorithm identifies the neurons or areas which participate in a cognitive or behavioral task, given data about performance decrease in a small set of lesions. It also allows the accurate prediction of performances due to multi-element lesions. The effectiveness of the new algorithm is demonstrated in two models of recurrent neural networks with complex interactions among the elements. The algorithm is scalable and applicable to the analysis of large neural networks. Given the recent advances in reversible inactivation techniques, it has the potential to significantly contribute to the understanding of the organization of biological nervous systems, and to shed light on the long-lasting debate about local versus distributed computation in the brain.},
 author = {Aharonov-Barki, Ranit and Meilijson, Isaac and Ruppin, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0533a888904bd4867929dffd884d60b8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0533a888904bd4867929dffd884d60b8-Metadata.json},
 openalex = {W2109207068},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0533a888904bd4867929dffd884d60b8-Paper.pdf},
 publisher = {MIT Press},
 title = {Who Does What? A Novel Algorithm to Determine Function Localization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/0533a888904bd4867929dffd884d60b8-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_059fdcd9,
 abstract = {The conventional wisdom is that backprop nets with excess hidden units generalize poorly. We show that nets with excess capacity generalize well when trained with backprop and early stopping. Experiments suggest two reasons for this: 1) Overfitting can vary significantly in different regions of the model. Excess capacity allows better fit to regions of high non-linearity, and backprop often avoids overfitting the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents in similar sequence. Big nets pass through stages similar to those learned by smaller nets. Early stopping can stop training the large net when it generalizes comparably to a smaller net. We also show that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when learning to fit regions of high non-linearity.},
 author = {Caruana, Rich and Lawrence, Steve and Giles, C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/059fdcd96baeb75112f09fa1dcc740cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/059fdcd96baeb75112f09fa1dcc740cc-Metadata.json},
 openalex = {W2156876426},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf},
 publisher = {MIT Press},
 title = {Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/059fdcd96baeb75112f09fa1dcc740cc-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_0609154f,
 abstract = {The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redundancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blue-yellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient representation of colors. Our findings suggest that color opponency is a result of the properties of natural spectra and not solely a consequence of the overlapping cone spectral sensitivities.},
 author = {Lee, Te-Won and Wachtler, Thomas and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0609154fa35b3194026346c9cac2a248-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0609154fa35b3194026346c9cac2a248-Metadata.json},
 openalex = {W2132295526},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0609154fa35b3194026346c9cac2a248-Paper.pdf},
 publisher = {MIT Press},
 title = {Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/0609154fa35b3194026346c9cac2a248-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_069654d5,
 abstract = {We present a new view of image segmentation by pairwise similarities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This interpretation shows that spectral methods for clustering and segmentation have a probabilistic foundation. In particular, we prove that the Normalized Cut method arises naturally from our framework. Finally, the framework provides a principled method for learning the similarity function as a combination of features.},
 author = {Meila, Marina and Shi, Jianbo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/069654d5ce089c13f642d19f09a3d1c0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/069654d5ce089c13f642d19f09a3d1c0-Metadata.json},
 openalex = {W2171009857},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/069654d5ce089c13f642d19f09a3d1c0-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Segmentation by Random Walks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/069654d5ce089c13f642d19f09a3d1c0-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_06a15eb1,
 abstract = {We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC-Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets.},
 author = {Herbrich, Ralf and Graepel, Thore},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/06a15eb1c3836723b53e4abca8d9b879-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/06a15eb1c3836723b53e4abca8d9b879-Metadata.json},
 openalex = {W2158388185},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/06a15eb1c3836723b53e4abca8d9b879-Paper.pdf},
 publisher = {MIT Press},
 title = {A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/06a15eb1c3836723b53e4abca8d9b879-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_0731460a,
 abstract = {Bayesian networks are graphical representations of probability distributions. In virtually all of the work on learning these networks, the assumption is that we are presented with a data set consisting of randomly generated instances from the underlying distribution. In many situations, however, we also have the option of active learning, where we have the possibility of guiding the sampling process by querying for certain types of samples. This paper addresses the problem of estimating the parameters of Bayesian networks in an active learning setting. We provide a theoretical framework for this problem, and an algorithm that chooses which active learning queries to generate based on the model learned so far. We present experimental results showing that our active learning algorithm can significantly reduce the need for training data in many situations.},
 author = {Tong, Simon and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0731460a8a5ce1626210cbf4385ae0ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0731460a8a5ce1626210cbf4385ae0ef-Metadata.json},
 openalex = {W2121615264},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0731460a8a5ce1626210cbf4385ae0ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Learning for Parameter Estimation in Bayesian Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/0731460a8a5ce1626210cbf4385ae0ef-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_07a4e20a,
 abstract = {We present a method to bound the partition function of a Boltzmann machine neural network with any odd-order polynomial. This is a direct extension of the mean-field bound, which is first order. We show that the third-order bound is strictly better than mean field. Additionally, we derive a third-order bound for the likelihood of sigmoid belief networks. Numerical experiments indicate that an error reduction of a factor of two is easily reached in the region where expansion-based approximations are useful.},
 author = {Leisink, Martijn and Kappen, Hilbert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/07a4e20a7bbeeb7a736682b26b16ebe8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/07a4e20a7bbeeb7a736682b26b16ebe8-Metadata.json},
 openalex = {W2162995719},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/07a4e20a7bbeeb7a736682b26b16ebe8-Paper.pdf},
 publisher = {MIT Press},
 title = {A Tighter Bound for Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/07a4e20a7bbeeb7a736682b26b16ebe8-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_0950ca92,
 author = {Rasmussen, Carl and Ghahramani, Zoubin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0950ca92a4dcf426067cfd2246bb5ff3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0950ca92a4dcf426067cfd2246bb5ff3-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0950ca92a4dcf426067cfd2246bb5ff3-Paper.pdf},
 publisher = {MIT Press},
 title = {Occam\textquotesingle s Razor},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/0950ca92a4dcf426067cfd2246bb5ff3-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_09b15d48,
 abstract = {We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling.},
 author = {Boyan, Justin and Littman, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/09b15d48a1514d8209b192a8b8f34e48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/09b15d48a1514d8209b192a8b8f34e48-Metadata.json},
 openalex = {W2168945108},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/09b15d48a1514d8209b192a8b8f34e48-Paper.pdf},
 publisher = {MIT Press},
 title = {Exact Solutions to Time-Dependent MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/09b15d48a1514d8209b192a8b8f34e48-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_09fb05dd,
 abstract = {The principle of maximizing mutual information is applied to learning overcomplete and recurrent representations. The underlying model consists of a network of input units driving a larger number of output units with recurrent interactions. In the limit of zero noise, the network is deterministic and the mutual information can be related to the entropy of the output units. Maximizing this entropy with respect to both the feedforward connections as well as the recurrent interactions results in simple learning rules for both sets of parameters. The conventional independent components (ICA) learning algorithm can be recovered as a special case where there is an equal number of output units and no recurrent connections. The application of these new learning rules is illustrated on a simple two-dimensional input example.},
 author = {Shriki, Oren and Sompolinsky, Haim and Lee, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/09fb05dd477d4ae6479985ca56c5a12d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/09fb05dd477d4ae6479985ca56c5a12d-Metadata.json},
 openalex = {W2133087656},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/09fb05dd477d4ae6479985ca56c5a12d-Paper.pdf},
 publisher = {MIT Press},
 title = {An Information Maximization Approach to Overcomplete and Recurrent Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/09fb05dd477d4ae6479985ca56c5a12d-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_0e087ec5,
 abstract = {Novelty detection involves modeling the normal behaviour of a system hence enabling detection of any divergence from normality. It has potential applications in many areas such as detection of machine damage or highlighting abnormal features in medical data. One approach is to build a hypothesis estimating the support of the normal data i.e. constructing a function which is positive in the region where the data is located and negative elsewhere. Recently kernel methods have been proposed for estimating the support of a distribution and they have performed well in practice - training involves solution of a quadratic programming problem. In this paper we propose a simpler kernel method for estimating the support based on linear programming. The method is easy to implement and can learn large datasets rapidly. We demonstrate the method on medical and fault detection datasets.},
 author = {Campbell, Colin and Bennett, Kristin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0e087ec55dcbe7b2d7992d6b69b519fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0e087ec55dcbe7b2d7992d6b69b519fb-Metadata.json},
 openalex = {W2126652588},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/0e087ec55dcbe7b2d7992d6b69b519fb-Paper.pdf},
 publisher = {MIT Press},
 title = {A Linear Programming Approach to Novelty Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/0e087ec55dcbe7b2d7992d6b69b519fb-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_11108a3d,
 abstract = {This dissertation examines the use of partial programming as a means of designing agents for large Markov Decision Problems. In this approach, a programmer specifies only that which they know to be correct and the system then learns the rest from experience using reinforcement learning. 
In contrast to previous low-level languages for partial programming, this dissertation presents ALisp, a Lisp-based high-level partial programming language. ALisp allows the programmer to constrain the policies considered by a learning process and to express his or her prior knowledge in a concise manner. Optimally completing a partial ALisp program is shown to be equivalent to solving a Semi-Markov Decision Problem (SMDP). Under a finite memory-use condition, online learning algorithms for ALisp are proved to converge to an optimal solution of the SMDP and thus to an optimal completion of the partial program. 
This dissertation then presents methods for exploiting the modularity allows an agent to ignore aspects of its current state that are irrelevant to its current decision, and therefore speeds up reinforcement learning. By decomposing representations of the value of actions along subroutine boundaries, optimality, i.e., optimality among all policies consistent with the partial program. These methods are demonstrated on two simulated taxi tasks. 
Function approximation, a method for representing the value of actions, allows reinforcement learning to be applied to problems where exact methods are intractable. Soft shaping is a method for guiding an agent toward a solution without constraining the search space. Both can be integrated with ALisp. ALisp with function approximation and reward shaping is successfully applied on a difficult continuous variant of the simulated taxi task. 
Together, the methods presented in this work comprise a system for agent design that allows the programmer to specify what they know, hint at what they suspect using soft shaping, and leave unspecified that which they don't know; the system then optimally completes the program through experience and takes advantage of the hierarchical structure of the specified program to speed learning.},
 author = {Andre, David and Russell, Stuart},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/11108a3dbfe4636cb40b84b803b2fff6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/11108a3dbfe4636cb40b84b803b2fff6-Metadata.json},
 openalex = {W2145739724},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/11108a3dbfe4636cb40b84b803b2fff6-Paper.pdf},
 publisher = {MIT Press},
 title = {Programmable Reinforcement Learning Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/11108a3dbfe4636cb40b84b803b2fff6-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_11f524c3,
 abstract = {People can understand complex auditory and visual information, often using one to disambiguate the other. Automated analysis, even at a low-level, faces severe challenges, including the lack of accurate statistical models for the signals, and their high-dimensionality and varied sampling rates. Previous approaches [6] assumed simple parametric models for the joint distribution which, while tractable, cannot capture the complex signal relationships. We learn the joint distribution of the visual and auditory signals using a non-parametric approach. First, we project the data into a maximally informative, low-dimensional subspace, suitable for density estimation. We then model the complicated stochastic relationships between the signals using a nonparametric density estimator. These learned densities allow processing across signal modalities. We demonstrate, on synthetic and real signals, localization in video of the face that is speaking in audio, and, conversely, audio enhancement of a particular speaker selected from the video.},
 author = {Fisher III, John W and Darrell, Trevor and Freeman, William and Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/11f524c3fbfeeca4aa916edcb6b6392e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/11f524c3fbfeeca4aa916edcb6b6392e-Metadata.json},
 openalex = {W2106488367},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/11f524c3fbfeeca4aa916edcb6b6392e-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Joint Statistical Models for Audio-Visual Fusion and Segregation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/11f524c3fbfeeca4aa916edcb6b6392e-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_13168e6a,
 abstract = {Large margin linear classification methods have been successfully applied to many applications. For a linearly separable problem, it is known that under appropriate assumptions, the expected misclassification error of the computed optimal hyperplane approaches zero at a rate proportional to the inverse training sample size. This rate is usually characterized by the margin and the maximum norm of the input data. In this paper, we argue that another quantity, namely the robustness of the input data distribution, also plays an important role in characterizing the convergence behavior of expected misclassification error. Based on this concept of robustness, we show that for a large margin separable linear classification problem, the expected misclassification error may converge exponentially in the number of training sample size.},
 author = {Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/13168e6a2e6c84b4b7de9390c0ef5ec5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/13168e6a2e6c84b4b7de9390c0ef5ec5-Metadata.json},
 openalex = {W2122789612},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/13168e6a2e6c84b4b7de9390c0ef5ec5-Paper.pdf},
 publisher = {MIT Press},
 title = {Convergence of Large Margin Separable Linear Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/13168e6a2e6c84b4b7de9390c0ef5ec5-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_136f9513,
 author = {Bogacz, Rafal and Brown, Malcolm and Giraud-Carrier, Christophe},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/136f951362dab62e64eb8e841183c2a9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/136f951362dab62e64eb8e841183c2a9-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/136f951362dab62e64eb8e841183c2a9-Paper.pdf},
 publisher = {MIT Press},
 title = {Emergence of Movement Sensitive Neurons\textquotesingle Properties by Learning a Sparse Code for Natural Moving Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/136f951362dab62e64eb8e841183c2a9-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_139f0874,
 abstract = {Recent work has exploited boundedness of data in the unsupervised learning of new types of generative model. For nonnegative data it was recently shown that the maximum-entropy generative model is a Nonnegative Boltzmann Distribution not a Gaussian distribution, when the model is constrained to match the first and second order statistics of the data. Learning for practical sized problems is made difficult by the need to compute expectations under the model distribution. The computational cost of Markov chain Monte Carlo methods and low fidelity of naive mean field techniques has led to increasing interest in advanced mean field theories and variational methods. Here I present a second-order mean-field approximation for the Nonnegative Boltzmann Machine model, obtained using a high-temperature expansion. The theory is tested on learning a bimodal 2-dimensional model, a high-dimensional translationally invariant distribution, and a generative model for handwritten digits.},
 author = {Downs, Oliver},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/139f0874f2ded2e41b0393c4ac5644f7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/139f0874f2ded2e41b0393c4ac5644f7-Metadata.json},
 openalex = {W2133499455},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/139f0874f2ded2e41b0393c4ac5644f7-Paper.pdf},
 publisher = {MIT Press},
 title = {High-temperature Expansions for Learning Models of Nonnegative Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/139f0874f2ded2e41b0393c4ac5644f7-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_14cfdb59,
 abstract = {We present a novel method for clustering using the support vector machine approach. Data points are mapped to a high dimensional feature space, where support vectors are used to define a sphere enclosing them. The boundary of the sphere forms in data space a set of closed contours containing the data. Data points enclosed by each contour are defined as a cluster. As the width parameter of the Gaussian kernel is decreased, these contours fit the data more tightly and splitting of contours occurs. The algorithm works by separating clusters according to valleys in the underlying probability distribution, and thus clusters can take on arbitrary geometrical shapes. As in other SV algorithms, outliers can be dealt with by introducing a soft margin constant leading to smoother cluster boundaries. The structure of the data is explored by varying the two parameters. We investigate the dependence of our method on these parameters and apply it to several data sets.},
 author = {Ben-Hur, Asa and Horn, David and Siegelmann, Hava and Vapnik, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/14cfdb59b5bda1fc245aadae15b1984a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/14cfdb59b5bda1fc245aadae15b1984a-Metadata.json},
 openalex = {W2097755267},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/14cfdb59b5bda1fc245aadae15b1984a-Paper.pdf},
 publisher = {MIT Press},
 title = {A Support Vector Method for Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/14cfdb59b5bda1fc245aadae15b1984a-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_155fa095,
 abstract = {An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data.},
 author = {Cauwenberghs, Gert and Poggio, Tomaso},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/155fa09596c7e18e50b58eb7e0c6ccb4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/155fa09596c7e18e50b58eb7e0c6ccb4-Metadata.json},
 openalex = {W2108807072},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/155fa09596c7e18e50b58eb7e0c6ccb4-Paper.pdf},
 publisher = {MIT Press},
 title = {Incremental and Decremental Support Vector Machine Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/155fa09596c7e18e50b58eb7e0c6ccb4-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_15d185ea,
 abstract = {Active inference in concept learning Jonathan D. Nelson (jnelson@cogsci.ucsd.edu)* Joshua B. Tenenbaum (jbt@psych.stanford.edu)^ Javier R. Movellan (movellan@cogsci.ucsd.edu)* *Cognitive Science Department, UCSD La Jolla, CA 92093-0515 ^Psychology Department, Stanford University Stanford, CA 94305 Abstract People are active experimenters, constantly seeking new information relevant to their goals. A reasonable approach to active information gathering is to ask questions and conduct experiments that minimize the expected state of uncertainty, or maximize the expected information gain, given current beliefs (Fedorov, 1972; MacKay, 1992; Oaksford & Chater, 1994). In this paper we present results on an exploratory experiment designed to study people’s active information gathering behavior on a concept learning task. The results of the experiment suggest subjects’ behavior may be explained well from the point of view of Bayesian information maximization. Introduction In scientific inquiry and in everyday life, people seek out information relevant to perceptual and cognitive tasks. Whether performing experiments to uncover causal relationships, saccading to informative areas of visual scenes, or turning towards a surprising sound, people actively seek out information relative to their goals. Consider a person learning a foreign language, who notices a particular word, “tikos,” used to refer to a baby moose, a baby penguin, and a baby cheetah. Based on those examples, she may attempt to discover what tikos really means. Logically, there are an infinite number of possibilities. For instance, tikos could mean baby animals, or simply animals, or even baby animals and antique telephones. Yet a few examples are often enough for human learners to form strong intuitions about what meanings are most likely. Suppose the learner could point to a baby duck, an adult duck, or an antique telephone, to inquire whether that object is “tikos.” What question would she ask? Why do we think that pointing to the telephone is not a good idea, even though from a logical point of view, a phone could very well be tikos? In this paper we present a normative theoretical framework, to try to predict the questions people ask in concept learning tasks (Fedorov, 1972; MacKay, 1992; Oaksford & Chater, 1994). A Bayesian concept learning model In the approach presented here, we evaluate questions in terms of their information value. Formally, information is defined with respect to a probability model. Here we use a Bayesian framework in the sense that we model internal beliefs as probability distributions. In order to quantify the information value (in bits) of a person’s questions, we first need a model of her beliefs, and the way those beliefs are updated as new information is obtained. Tenenbaum (1999, 2000) provides such a model of people’s beliefs, for a number concept learning task. While Tenenbaum (1999, 2000); and the first and last authors of the present paper, in a pilot study, found that his model described subjects’ beliefs well, there were some deviations between model predictions and subjects’ beliefs. The concept learning model used in the present study, which we describe below, is based on Tenenbaum’s original model, but extended in ways that reduce previously observed deviations between model predictions and study participants’ beliefs. We formalize the concept learning situation described by the number concept model using standard probabilistic notation: random variables are represented with capital letters, and specific values taken by those variables are represented with small letters. The random variable C represents the correct hidden concept on a given trial. This concept is not directly observable by study participants; rather, they infer it on the basis of example numbers that are consistent with the true concept. Notation of the form “C=c” is shorthand for the event that the random variable C takes the specific value c, e.g. that the correct concept (or “hypothesis”) is prime numbers. We represent the examples given to the subjects by the random vector X. The subject’s beliefs about which concepts are probable prior to the presentation of any examples is represented by the probability function P(C=c). The subject’s updated belief about a concept’s probability, after she sees the},
 author = {Nelson, Jonathan and Movellan, Javier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/15d185eaa7c954e77f5343d941e25fbd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/15d185eaa7c954e77f5343d941e25fbd-Metadata.json},
 openalex = {W2625039335},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/15d185eaa7c954e77f5343d941e25fbd-Paper.pdf},
 publisher = {MIT Press},
 title = {Active inference in concept learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/15d185eaa7c954e77f5343d941e25fbd-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_17c3433f,
 abstract = {Although connectionist models have provided insights into the nature of perception and motor control, connectionist accounts of higher cognition seldom go beyond an implementation of traditional symbol-processing theories. We describe a connectionist constraint satisfaction model of how people solve anagram problems. The model exploits statistics of English orthography, but also addresses the interplay of sub symbolic and symbolic computation by a mechanism that extracts approximate symbolic representations (partial orderings of letters) from subsymbolic structures and injects the extracted representation back into the model to assist in the solution of the anagram. We show the computational benefit of this extraction-injection process and discuss its relationship to conscious mental processes and working memory. We also account for experimental data concerning the difficulty of anagram solution based on the orthographic structure of the anagram string and the target word.},
 author = {Grimes, David and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/17c3433fecc21b57000debdf7ad5c930-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/17c3433fecc21b57000debdf7ad5c930-Metadata.json},
 openalex = {W2138520983},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/17c3433fecc21b57000debdf7ad5c930-Paper.pdf},
 publisher = {MIT Press},
 title = {The Interplay of Symbolic and Subsymbolic Processes in Anagram Problem Solving},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/17c3433fecc21b57000debdf7ad5c930-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_19de10ad,
 author = {Williams, Christopher and Seeger, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/19de10adbaa1b2ee13f77f679fa1483a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/19de10adbaa1b2ee13f77f679fa1483a-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf},
 publisher = {MIT Press},
 title = {Using the Nystr\"{o}m Method to Speed Up Kernel Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/19de10adbaa1b2ee13f77f679fa1483a-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_1e913e1b,
 abstract = {Nonlinear Support Vector Machines (SVMs) are investigated for visual sex classification with low resolution thumbnail faces (21- by-12 pixels) processed from 1,755 images from the FERET face database. The performance of SVMs is shown to be superior to traditional pattern classifiers (Linear, Quadratic, Fisher Linear Discriminant, Nearest-Neighbor) as well as more modern techniques such as Radial Basis Function (RBF) classifiers and large ensemble-RBF networks. Furthermore, the SVM performance (3.4% error) is currently the best result reported in the open literature.},
 author = {Moghaddam, Baback and Yang, Ming-Hsuan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/1e913e1b06ead0b66e30b6867bf63549-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/1e913e1b06ead0b66e30b6867bf63549-Metadata.json},
 openalex = {W2168237270},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/1e913e1b06ead0b66e30b6867bf63549-Paper.pdf},
 publisher = {MIT Press},
 title = {Sex with Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/1e913e1b06ead0b66e30b6867bf63549-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_1f1baa5b,
 abstract = {The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - 1)th level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data.},
 author = {Mayraz, Guy and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Metadata.json},
 openalex = {W2097119693},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf},
 publisher = {MIT Press},
 title = {Recognizing Hand-written Digits Using Hierarchical Products of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_201d7288,
 abstract = {We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.},
 author = {St-Aubin, Robert and Hoey, Jesse and Boutilier, Craig},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/201d7288b4c18a679e48b31c72c30ded-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/201d7288b4c18a679e48b31c72c30ded-Metadata.json},
 openalex = {W2120406836},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/201d7288b4c18a679e48b31c72c30ded-Paper.pdf},
 publisher = {MIT Press},
 title = {APRICODD: Approximate Policy Construction Using Decision Diagrams},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/201d7288b4c18a679e48b31c72c30ded-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_2647c1db,
 abstract = {To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg movements by driving motors with time varying voltages which are the outputs of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The relationship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal relationship is only approximately true due to transistor mismatch and offsets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (unlabeled) examples how to set the parameters to the chip in order to obtain a desired motor behavior.},
 author = {Still, Susanne and Sch\"{o}lkopf, Bernhard and Hepp, Klaus and Douglas, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/2647c1dba23bc0e0f9cdf75339e120d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/2647c1dba23bc0e0f9cdf75339e120d2-Metadata.json},
 openalex = {W2101853778},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/2647c1dba23bc0e0f9cdf75339e120d2-Paper.pdf},
 publisher = {MIT Press},
 title = {Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/2647c1dba23bc0e0f9cdf75339e120d2-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_29530de2,
 abstract = {We investigate a new kernel-based classifier: the Kernel Fisher Discriminant (KFD). A mathematical programming formulation based on the observation that KFD maximizes the average margin permits an interesting modification of the original KFD algorithm yielding the sparse KFD. We find that both, KFD and the proposed sparse KFD, can be understood in an unifying probabilistic context. Furthermore, we show connections to Support Vector Machines and Relevance Vector Machines. From this understanding, we are able to outline an interesting kernel-regression technique based upon the KFD algorithm. Simulations support the usefulness of our approach.},
 author = {Mika, Sebastian and R\"{a}tsch, Gunnar and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/29530de21430b7540ec3f65135f7323c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/29530de21430b7540ec3f65135f7323c-Metadata.json},
 openalex = {W2132890143},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/29530de21430b7540ec3f65135f7323c-Paper.pdf},
 publisher = {MIT Press},
 title = {A Mathematical Programming Approach to the Kernel Fisher Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/29530de21430b7540ec3f65135f7323c-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_2d1b2a5f,
 abstract = {The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-learning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.},
 author = {Sallans, Brian and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/2d1b2a5ff364606ff041650887723470-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/2d1b2a5ff364606ff041650887723470-Metadata.json},
 openalex = {W2165235114},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/2d1b2a5ff364606ff041650887723470-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/2d1b2a5ff364606ff041650887723470-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_2f25f6e3,
 abstract = {Substantial data support a temporal difference (TO) model of dopamine (OA) neuron activity in which the cells provide a global error signal for reinforcement learning. However, in certain circumstances, DA activity seems anomalous under the TD model, responding to non-rewarding stimuli. We address these anomalies by suggesting that DA cells multiplex information about reward bonuses, including Sutton's exploration bonuses and Ng et al's non-distorting shaping bonuses. We interpret this additional role for DA in terms of the unconditional attentional and psychomotor effects of dopamine, having the computational role of guiding exploration.},
 author = {Kakade, Sham and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/2f25f6e326adb93c5787175dda209ab6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/2f25f6e326adb93c5787175dda209ab6-Metadata.json},
 openalex = {W2295762292},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/2f25f6e326adb93c5787175dda209ab6-Paper.pdf},
 publisher = {MIT Press},
 title = {Dopamine Bonuses},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/2f25f6e326adb93c5787175dda209ab6-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_3214a6d8,
 abstract = {We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n2m), storage is O(nm), the cost for prediction is O(n) and the cost to compute confidence bounds is O(nm), where n ≪ m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems.},
 author = {Smola, Alex and Bartlett, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3214a6d842cc69597f9edf26df552e43-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3214a6d842cc69597f9edf26df552e43-Metadata.json},
 openalex = {W2123687908},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3214a6d842cc69597f9edf26df552e43-Paper.pdf},
 publisher = {MIT Press},
 title = {Sparse Greedy Gaussian Process Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/3214a6d842cc69597f9edf26df552e43-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_33322217,
 abstract = {The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires O(m2) of memory and O(N ċ m2) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%.},
 author = {Herbrich, Ralf and Graepel, Thore},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/333222170ab9edca4785c39f55221fe7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/333222170ab9edca4785c39f55221fe7-Metadata.json},
 openalex = {W2101416458},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/333222170ab9edca4785c39f55221fe7-Paper.pdf},
 publisher = {MIT Press},
 title = {Large Scale Bayes Point Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/333222170ab9edca4785c39f55221fe7-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_39027dfa,
 abstract = {We consider the existence of efficient algorithms for learning the class of half-spaces in Rn in the agnostic learning model (i.e., making no prior assumptions on the example-generating distribution). The resulting combinatorial problem - finding the best agreement half-space over an input sample - is NP hard to approximate to within some constant factor. We suggest a way to circumvent this theoretical bound by introducing a new measure of success for such algorithms. An algorithm is µ-margin successful if the agreement ratio of the half-space it outputs is as good as that of any half-space once training points that are inside the µ-margins of its separating hyper-plane are disregarded. We prove crisp computational complexity results with respect to this success measure: On one hand, for every positive µ, there exist efficient (poly-time) µ-margin successful learning algorithms. On the other hand, we prove that unless P=NP, there is no algorithm that runs in time polynomial in the sample size and in 1/µ that is µ-margin successful for all µ > 0.},
 author = {Ben-David, Shai and Simon, Hans-Ulrich},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/39027dfad5138c9ca0c474d71db915c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/39027dfad5138c9ca0c474d71db915c3-Metadata.json},
 openalex = {W2106948187},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/39027dfad5138c9ca0c474d71db915c3-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient Learning of Linear Perceptrons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/39027dfad5138c9ca0c474d71db915c3-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_3c947bc2,
 abstract = {Iterative Gaussianization is a fixed-point iteration procedure that can transform any continuous random vector into a Gaussian one. Based on iterative Gaussianization, we propose a new type of normalizing flow model that enables both efficient computation of likelihoods and efficient inversion for sample generation. We demonstrate that these models, named Gaussianization flows, are universal approximators for continuous probability distributions under some regularity conditions. Because of this guaranteed expressivity, they can capture multimodal target distributions without compromising the efficiency of sample generation. Experimentally, we show that Gaussianization flows achieve better or comparable performance on several tabular datasets compared to other efficiently invertible flow models such as Real NVP, Glow and FFJORD. In particular, Gaussianization flows are easier to initialize, demonstrate better robustness with respect to different transformations of the training data, and generalize better on small training sets.},
 author = {Chen, Scott and Gopinath, Ramesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3c947bc2f7ff007b86a9428b74654de5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3c947bc2f7ff007b86a9428b74654de5-Metadata.json},
 openalex = {W3010559047},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3c947bc2f7ff007b86a9428b74654de5-Paper.pdf},
 publisher = {MIT Press},
 title = {Gaussianization Flows.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/3c947bc2f7ff007b86a9428b74654de5-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_3fab5890,
 abstract = {We analyze Gallager codes by employing a simple mean-field approximation that distorts the model geometry and preserves important interactions between sites. The method naturally recovers the probability propagation decoding algorithm as an extremization of a proper free-energy. We find a thermodynamic phase transition that coincides with information theoretical upper-bounds and explain the practical code performance in terms of the free-energy landscape.},
 author = {Vicente, Renato and Saad, David and Kabashima, Yoshiyuki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3fab5890d8113d0b5a4178201dc842ad-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3fab5890d8113d0b5a4178201dc842ad-Metadata.json},
 openalex = {W2149043643},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf},
 publisher = {MIT Press},
 title = {Error-correcting Codes on a Bethe-like Lattice},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/3fab5890d8113d0b5a4178201dc842ad-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_414e773d,
 abstract = {In this work, we explore homeostasis in a silicon integrate-and-fire neuron. The neuron adapts its firing rate over long time periods on the order of seconds or minutes so that it returns to its spontaneous firing rate after a lasting perturbation. Homeostasis is implemented via two schemes. One scheme looks at the presynaptic activity and adapts the synaptic weight depending on the presynaptic spiking rate. The second scheme adapts the synaptic depending on the neuron's activity. The threshold is lowered if the neuron's activity decreases over a long time and is increased for prolonged increase in postsynaptic activity. Both these mechanisms for adaptation use floating-gate technology. The results shown here are measured from a chip fabricated in a 2-µm CMOS process.},
 author = {Liu, Shih-Chii and Minch, Bradley},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/414e773d5b7e5c06d564f594bf6384d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/414e773d5b7e5c06d564f594bf6384d0-Metadata.json},
 openalex = {W2101387899},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/414e773d5b7e5c06d564f594bf6384d0-Paper.pdf},
 publisher = {MIT Press},
 title = {Homeostasis in a Silicon Integrate and Fire Neuron},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/414e773d5b7e5c06d564f594bf6384d0-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_44968aec,
 abstract = {Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.},
 author = {Dugas, Charles and Bengio, Yoshua and B\'{e}lisle, Fran\c{c}ois and Nadeau, Claude and Garcia, Ren\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/44968aece94f667e4095002d140b5896-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/44968aece94f667e4095002d140b5896-Metadata.json},
 openalex = {W2168148636},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/44968aece94f667e4095002d140b5896-Paper.pdf},
 publisher = {MIT Press},
 title = {Incorporating Second-Order Functional Knowledge for Better Option Pricing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/44968aece94f667e4095002d140b5896-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_4496bf24,
 abstract = {We apply to oscillatory networks a class of learning rules in which synaptic weights change proportional to pre- and post-synaptic activity, with a kernel A(τ) measuring the effect for a postsynaptic spike a time τ after the presynaptic one. The resulting synaptic matrices have an outer-product form in which the oscillating patterns are represented as complex vectors. In a simple model, the even part of A(τ) enhances the resonant response to learned stimulus by reducing the effective damping, while the odd part determines the frequency of oscillation. We relate our model to the olfactory cortex and hippocampus and their presumed roles in forming associative memories and input representations.},
 author = {Scarpetta, Silvia and Li, Zhaoping and Hertz, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4496bf24afe7fab6f046bf4923da8de6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4496bf24afe7fab6f046bf4923da8de6-Metadata.json},
 openalex = {W2139448833},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf},
 publisher = {MIT Press},
 title = {Spike-Timing-Dependent Learning for Oscillatory Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_44a2e080,
 author = {Jebara, Tony and Pentland, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/44a2e0804995faf8d2e3b084a1e2db1d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/44a2e0804995faf8d2e3b084a1e2db1d-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/44a2e0804995faf8d2e3b084a1e2db1d-Paper.pdf},
 publisher = {MIT Press},
 title = {On Reversing Jensen\textquotesingle s Inequality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/44a2e0804995faf8d2e3b084a1e2db1d-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_48882413,
 abstract = {We establish a principled framework for adaptive transform coding. Transform coders are often constructed by concatenating an ad hoc choice of transform with suboptimal bit allocation and quantizer design. Instead, we start from a probabilistic latent variable model in the form of a mixture of constrained Gaussian mixtures. From this model we derive a transform coding algorithm, which is a constrained version of the generalized Lloyd algorithm for vector quantizer design. A byproduct of our derivation is the introduction of a new transform basis, which unlike other transforms (PCA, DCT, etc.) is explicitly optimized for coding. Image compression experiments show adaptive transform coders designed with our algorithm improve compressed image signal-to-noise ratio up to 3 dB compared to global transform coding and 0.5 to 2 dB compared to other adaptive transform coders.},
 author = {Archer, Cynthia and Leen, Todd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4888241374e8c62ddd9b4c3cfd091f96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4888241374e8c62ddd9b4c3cfd091f96-Metadata.json},
 openalex = {W2137991339},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4888241374e8c62ddd9b4c3cfd091f96-Paper.pdf},
 publisher = {MIT Press},
 title = {From Mixtures of Mixtures to Adaptive Transform Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/4888241374e8c62ddd9b4c3cfd091f96-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_49ad23d1,
 abstract = {We present a novel way of obtaining PAC-style bounds on the generalization error of learning algorithms, explicitly using their stability properties. A stable learner is one for which the learned solution does not change much with small changes in the training set. The bounds we obtain do not depend on any measure of the complexity of the hypothesis space (e.g. VC dimension) but rather depend on how the learning algorithm searches this space, and can thus be applied even when the VC dimension is infinite. We demonstrate that regularization networks possess the required stability property and apply our method to obtain new bounds on their generalization performance.},
 author = {Bousquet, Olivier and Elisseeff, Andr\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Metadata.json},
 openalex = {W2110630246},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
 publisher = {MIT Press},
 title = {Algorithmic Stability and Generalization Performance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_4e87337f,
 abstract = {A method is described which, like the kernel trick in support vector machines (SVMs), lets us generalize distance-based algorithms to operate in feature spaces, usually nonlinearly related to the input space. This is done by identifying a class of kernels which can be represented as norm-based distances in Hilbert spaces. It turns out that common kernel algorithms, such as SVMs and kernel PCA, are actually really distance based algorithms and can be run with that class of kernels, too.

As well as providing a useful new insight into how these algorithms work, the present work can form the basis for conceiving new algorithms.},
 author = {Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4e87337f366f72daa424dae11df0538c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4e87337f366f72daa424dae11df0538c-Metadata.json},
 openalex = {W2133324003},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/4e87337f366f72daa424dae11df0538c-Paper.pdf},
 publisher = {MIT Press},
 title = {The Kernel Trick for Distances},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/4e87337f366f72daa424dae11df0538c-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_5103c358,
 abstract = {We present evidence that several higher-order statistical properties of natural images and signals can be explained by a stochastic model which simply varies scale of an otherwise stationary Gaussian process. We discuss two interesting consequences. The first is that a variety of natural signals can be related through a common model of spherically invariant random processes, which have the attractive property that the joint densities can be constructed from the one dimensional marginal. The second is that in some cases the non-stationarity assumption and only second order methods can be explicitly exploited to find a linear basis that is equivalent to independent components obtained with higher-order methods. This is demonstrated on spectro-temporal components of speech.},
 author = {Parra, Lucas and Spence, Clay and Sajda, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5103c3584b063c431bd1268e9b5e76fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5103c3584b063c431bd1268e9b5e76fb-Metadata.json},
 openalex = {W2150211397},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf},
 publisher = {MIT Press},
 title = {Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_52d2752b,
 abstract = {We propose a novel probabilistic framework for semantic video indexing. We define probabilistic multimedia objects (multijects) to map low-level media features to high-level semantic labels. A graphical network of such multijects (multinet) captures scene context by discovering intra-frame as well as inter-frame dependency relations between the concepts. The main contribution is a novel application of a factor graph framework to model this network. We model relations between semantic concepts in terms of their co-occurrence as well as the temporal dependencies between these concepts within video shots. Using the sum-product algorithm [1] for approximate or exact inference in these factor graph multinets, we attempt to correct errors made during isolated concept detection by forcing high-level constraints. This results in a significant improvement in the overall detection performance.},
 author = {Naphade, Milind and Kozintsev, Igor and Huang, Thomas S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/52d2752b150f9c35ccb6869cbf074e48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/52d2752b150f9c35ccb6869cbf074e48-Metadata.json},
 openalex = {W2147573454},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Semantic Video Indexing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/52d2752b150f9c35ccb6869cbf074e48-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_537de305,
 abstract = {One way to approximate inference in richly-connected graphical models is to apply the sum-product algorithm (a.k.a. probability propagation algorithm), while ignoring the fact that the graph has cycles. The sum-product algorithm can be directly applied in Gaussian and in graphs for coding, but for many conditional probability functions - including the sigmoid function - direct application of the sum-product algorithm is not possible. We introduce networks that have low local complexity (but exponential global complexity) so the sum-product algorithm can be directly applied. In an accumulator network, the probability of a child given its parents is computed by accumulating the inputs from the parents in a Markov chain or more generally a tree. After giving expressions for inference and learning in accumulator networks, we give results on the bars and on the problem of extracting translated, overlapping faces from an image.},
 author = {Frey, Brendan J and Kannan, Anitha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/537de305e941fccdbba5627e3eefbb24-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/537de305e941fccdbba5627e3eefbb24-Metadata.json},
 openalex = {W2141144997},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/537de305e941fccdbba5627e3eefbb24-Paper.pdf},
 publisher = {MIT Press},
 title = {Accumulator Networks: Suitors of Local Probability Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/537de305e941fccdbba5627e3eefbb24-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_56f9f889,
 abstract = {We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world data sets indicate the efficiency of the approach.},
 author = {Csat\'{o}, Lehel and Opper, Manfred},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/56f9f88906aebf4ad985aaec7fa01313-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/56f9f88906aebf4ad985aaec7fa01313-Metadata.json},
 openalex = {W2149842772},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/56f9f88906aebf4ad985aaec7fa01313-Paper.pdf},
 publisher = {MIT Press},
 title = {Sparse Representation for Gaussian Process Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/56f9f88906aebf4ad985aaec7fa01313-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_57c0531e,
 abstract = {In memory consolidation, declarative memories which initially require the hippocampus for their recall, ultimately become independent of it. Consolidation has been the focus of numerous experimental and qualitative modeling studies, but only little quantitative exploration. We present a consolidation model in which hierarchical connections in the cortex, that initially instantiate purely semantic information acquired through probabilistic unsupervised learning, come to instantiate episodic information as well. The hippocampus is responsible for helping complete partial input patterns before consolidation is complete, while also training the cortex to perform appropriate completion by itself.},
 author = {K\'{a}li, Szabolcs and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/57c0531e13f40b91b3b0f1a30b529a1d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/57c0531e13f40b91b3b0f1a30b529a1d-Metadata.json},
 openalex = {W2146094102},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/57c0531e13f40b91b3b0f1a30b529a1d-Paper.pdf},
 publisher = {MIT Press},
 title = {Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/57c0531e13f40b91b3b0f1a30b529a1d-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_596dedf4,
 abstract = {Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be automated. We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illustrate the resulting algorithm using a simple hierarchical task. Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective.},
 author = {Jonsson, Anders and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/596dedf4498e258e4bdc9fd70df9a859-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/596dedf4498e258e4bdc9fd70df9a859-Metadata.json},
 openalex = {W2107628283},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/596dedf4498e258e4bdc9fd70df9a859-Paper.pdf},
 publisher = {MIT Press},
 title = {Automated State Abstraction for Options using the U-Tree Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/596dedf4498e258e4bdc9fd70df9a859-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_59bcda7c,
 abstract = {We use graphical models to explore the question of how people learn simple causal relationships from data. The two leading psychological theories can both be seen as estimating the parameters of a fixed graph. We argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure, and we propose to model this inductive process as a Bayesian inference. Our argument is supported through the discussion of three data sets.},
 author = {Tenenbaum, Joshua and Griffiths, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/59bcda7c438bad7d2afffe9e2fed00be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/59bcda7c438bad7d2afffe9e2fed00be-Metadata.json},
 openalex = {W2154521990},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/59bcda7c438bad7d2afffe9e2fed00be-Paper.pdf},
 publisher = {MIT Press},
 title = {Structure Learning in Human Causal Induction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/59bcda7c438bad7d2afffe9e2fed00be-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_59e0b265,
 abstract = {We consider the problem of designing a linear transformation θ ∈ Rp × n, of rank p ≤ n, which projects the features of a classifier x ∈ Rn onto y = θx ∈ Rp such as to achieve minimum Bayes error (or probability of misclassification). Two avenues will be explored: the first is to maximize the θ-average divergence between the class densities and the second is to minimize the union Bhattacharyya bound in the range of θ. While both approaches yield similar performance in practice, they outperform standard LDA features and show a 10% relative improvement in the word error rate over state-of-the-art cepstral features on a large vocabulary telephony speech recognition task.},
 author = {Saon, George and Padmanabhan, Mukund},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/59e0b2658e9f2e77f8d4d83f8d07ca84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/59e0b2658e9f2e77f8d4d83f8d07ca84-Metadata.json},
 openalex = {W2163055872},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/59e0b2658e9f2e77f8d4d83f8d07ca84-Paper.pdf},
 publisher = {MIT Press},
 title = {Minimum Bayes Error Feature Selection for Continuous Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/59e0b2658e9f2e77f8d4d83f8d07ca84-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_5b6ba13f,
 abstract = {Prior knowledge about video structure can be used both as a means to improve the performance of content analysis and to extract features that allow semantic classification. We introduce statistical models for two important components of this structure, shot duration and activity, and demonstrate the usefulness of these models by introducing a Bayesian formulation for the shot segmentation problem. The new formulations is shown to extend standard thresholding methods in an adaptive and intuitive way, leading to improved segmentation accuracy.},
 author = {Vasconcelos, Nuno and Lippman, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5b6ba13f79129a74a3e819b78e36b922-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5b6ba13f79129a74a3e819b78e36b922-Metadata.json},
 openalex = {W2100128635},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5b6ba13f79129a74a3e819b78e36b922-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Video Shot Segmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/5b6ba13f79129a74a3e819b78e36b922-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_5bce843d,
 abstract = {Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy.},
 author = {Szummer, Martin and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5bce843dd76db8c939d5323dd3e54ec9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5bce843dd76db8c939d5323dd3e54ec9-Metadata.json},
 openalex = {W2120720283},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5bce843dd76db8c939d5323dd3e54ec9-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernel Expansions with Unlabeled Examples},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/5bce843dd76db8c939d5323dd3e54ec9-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_5fa9e41b,
 abstract = {The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions. One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system. We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate. On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. Thus the neural code has a quantifiable mixture of individuality and universality.},
 author = {Schneidman, Elad and Brenner, Naama and Tishby, Naftali and van Steveninck, Robert and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5fa9e41bfec0725742cc9d15ef594120-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5fa9e41bfec0725742cc9d15ef594120-Metadata.json},
 openalex = {W2155760164},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/5fa9e41bfec0725742cc9d15ef594120-Paper.pdf},
 publisher = {MIT Press},
 title = {Universality and Individuality in a Neural Code},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/5fa9e41bfec0725742cc9d15ef594120-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_61b1fb3f,
 abstract = {Belief propagation (BP) was only supposed to work for treelike networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs.

We show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference.

More importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more accurate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we derive generalized belief propagation (GBP) versions of these Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable increase in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP.},
 author = {Yedidia, Jonathan S and Freeman, William and Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/61b1fb3f59e28c67f3925f3c79be81a1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/61b1fb3f59e28c67f3925f3c79be81a1-Metadata.json},
 openalex = {W2098678088},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalized Belief Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/61b1fb3f59e28c67f3925f3c79be81a1-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_645098b0,
 abstract = {Many neural systems extend their dynamic range by adaptation. We examine the timescales of adaptation in the context of dynamically modulated rapidly-varying stimuli, and demonstrate in the fly visual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time-dependent stimulus. Further, while the rate response has long transients, the adaptation takes place on timescales consistent with optimal variance estimation.},
 author = {Fairhall, Adrienne and Lewen, Geoffrey and Bialek, William and van Steveninck, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/645098b086d2f9e1e0e939c27f9f2d6f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/645098b086d2f9e1e0e939c27f9f2d6f-Metadata.json},
 openalex = {W2121654015},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/645098b086d2f9e1e0e939c27f9f2d6f-Paper.pdf},
 publisher = {MIT Press},
 title = {Multiple Timescales of Adaptation in a Neural Code},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/645098b086d2f9e1e0e939c27f9f2d6f-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_65699726,
 abstract = {This paper presents a unified probabilistic framework for denoising and dereverberation of speech signals. The framework transforms the denoising and dereverberation problems into Bayes-optimal signal estimation. The key idea is to use a strong speech model that is pre-trained on a large data set of clean speech. Computational efficiency is achieved by using variational EM, working in the frequency domain, and employing conjugate priors. The framework covers both single and multiple microphones. We apply this approach to noisy reverberant speech signals and get results substantially better than standard methods.},
 author = {Attias, Hagai and Platt, John and Acero, Alex and Deng, Li},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/65699726a3c601b9f31bf04019c8593c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/65699726a3c601b9f31bf04019c8593c-Metadata.json},
 openalex = {W2133908567},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/65699726a3c601b9f31bf04019c8593c-Paper.pdf},
 publisher = {MIT Press},
 title = {Speech Denoising and Dereverberation Using Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/65699726a3c601b9f31bf04019c8593c-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_65fc52ed,
 abstract = {We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.},
 author = {Becker, Suzanna and Burgess, Neil},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/65fc52ed8f88c81323a418ca94cec2ed-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/65fc52ed8f88c81323a418ca94cec2ed-Metadata.json},
 openalex = {W2142905457},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/65fc52ed8f88c81323a418ca94cec2ed-Paper.pdf},
 publisher = {MIT Press},
 title = {Modelling Spatial Recall, Mental Imagery and Neglect},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/65fc52ed8f88c81323a418ca94cec2ed-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_68148596,
 abstract = {Theories of object recognition often assume that only one representation scheme is used within one visual-processing pathway. Versatility of the visual system comes from having multiple visual-processing pathways, each specialized in a different category of objects. We propose a theoretically simpler alternative, capable of explaining the same set of data and more. A single primary visual-processing pathway, loosely modular, is assumed. Memory modules are attached to sites along this pathway. Object-identity decision is made independently at each site. A site's response time is a monotonic-decreasing function of its confidence regarding its decision. An observer's response is the first-arriving response from any site. The effective representation(s) of such a system, determined empirically, can appear to be specialized for different tasks and stimuli, consistent with recent clinical and functional-imaging findings. This, however, merely reflects a decision being made at its appropriate level of abstraction. The system itself is intrinsically flexible and adaptive.},
 author = {Tjan, Bosco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/68148596109e38cf9367d27875e185be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/68148596109e38cf9367d27875e185be-Metadata.json},
 openalex = {W2110541775},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/68148596109e38cf9367d27875e185be-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive Object Representation with Hierarchically-Distributed Memory Sites},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/68148596109e38cf9367d27875e185be-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_68c694de,
 abstract = {We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a standard word feature space kernel [6] is made showing encouraging results.},
 author = {Lodhi, Huma and Shawe-Taylor, John and Cristianini, Nello and Watkins, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/68c694de94e6c110f42e587e8e48d852-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/68c694de94e6c110f42e587e8e48d852-Metadata.json},
 openalex = {W2107425660},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/68c694de94e6c110f42e587e8e48d852-Paper.pdf},
 publisher = {MIT Press},
 title = {Text Classification using String Kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/68c694de94e6c110f42e587e8e48d852-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_69d1fc78,
 abstract = {We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within first-order as a function of model complexity. This general property of diminishing returns is illustrated on a number of real data sets and learning problems, including finite mixture modeling and multivariate linear regression.},
 author = {Cadez, Igor and Smyth, Padhraic},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/69d1fc78dbda242c43ad6590368912d4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/69d1fc78dbda242c43ad6590368912d4-Metadata.json},
 openalex = {W2130080687},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/69d1fc78dbda242c43ad6590368912d4-Paper.pdf},
 publisher = {MIT Press},
 title = {Model Complexity, Goodness of Fit and Diminishing Returns},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/69d1fc78dbda242c43ad6590368912d4-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_6ae07dcb,
 abstract = {In this communication we present a new algorithm for solving Support Vector Classifiers (SVC) with large training data sets. The new algorithm is based on an Iterative Re-Weighted Least Squares procedure which is used to optimize the SVC. Moreover, a novel sample selection strategy for the working set is presented, which randomly chooses the working set among the training samples that do not fulfill the stopping criteria. The validity of both proposals, the optimization procedure and sample selection strategy, is shown by means of computer experiments using well-known data sets.},
 author = {P\'{e}rez-Cruz, Fernando and Alarc\'{o}n-Diana, Pedro and Navia-V\'{a}zquez, Angel and Art\'{e}s-Rodr\'{\i}guez, Antonio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/6ae07dcb33ec3b7c814df797cbda0f87-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/6ae07dcb33ec3b7c814df797cbda0f87-Metadata.json},
 openalex = {W2100366146},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/6ae07dcb33ec3b7c814df797cbda0f87-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Training of Support Vector Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/6ae07dcb33ec3b7c814df797cbda0f87-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_6be5336d,
 abstract = {In this paper we develop the method of bounding the generalization error of a classifier in terms of its margin distribution which was introduced in the recent papers of Bartlett and Schapire, Freund, Bartlett and Lee. The theory of Gaussian and empirical processes allow us to prove the margin type inequalities for the most general functional classes, the complexity of the class being measured via the so called Gaussian complexity functions. As a simple application of our results, we obtain the bounds of Schapire, Freund, Bartlett and Lee for the generalization error of boosting. We also substantially improve the results of Bartlett on bounding the generalization error of neural networks in terms of l1-norms of the weights of neurons. Furthermore, under additional assumptions on the complexity of the class of hypotheses we provide some tighter bounds, which in the case of boosting improve the results of Schapire, Freund, Bartlett and Lee.},
 author = {Koltchinskii, Vladimir and Panchenko, Dmitriy and Lozano, Fernando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/6be5336db2c119736cf48f475e051bfe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/6be5336db2c119736cf48f475e051bfe-Metadata.json},
 openalex = {W2147433012},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/6be5336db2c119736cf48f475e051bfe-Paper.pdf},
 publisher = {MIT Press},
 title = {Some New Bounds on the Generalization Error of Combined Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/6be5336db2c119736cf48f475e051bfe-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_6e79ed05,
 abstract = {The goal of many unsupervised learning procedures is to bring two probability distributions into alignment. Generative models such as Gaussian mixtures and Boltzmann machines can be cast in this light, as can recoding models such as ICA and projection pursuit. We propose a novel sample-based error measure for these classes of models, which applies even in situations where maximum likelihood (ML) and probability density estimation-based formulations cannot be applied, e.g., models that are nonlinear or have intractable posteriors. Furthermore, our sample-based error measure avoids the difficulties of approximating a density function. We prove that with an unconstrained model, (1) our approach converges on the correct solution as the number of samples goes to infinity, and (2) the expected solution of our approach in the generative framework is the ML solution. Finally, we evaluate our approach via simulations of linear and nonlinear models on mixture of Gaussians and ICA problems. The experiments show the broad applicability and generality of our approach.},
 author = {Hochreiter, Sepp and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/6e79ed05baec2754e25b4eac73a332d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/6e79ed05baec2754e25b4eac73a332d2-Metadata.json},
 openalex = {W2143552366},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/6e79ed05baec2754e25b4eac73a332d2-Paper.pdf},
 publisher = {MIT Press},
 title = {Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/6e79ed05baec2754e25b4eac73a332d2-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_728f206c,
 abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a wor...},
 author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Metadata.json},
 openalex = {W2998704965},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf},
 publisher = {MIT Press},
 title = {A neural probabilistic language model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_7302e3f5,
 abstract = {A system has been developed to extract diagnostic information from jet engine carcass vibration data. Support Vector Machines applied to novelty detection provide a measure of how unusual the shape of a vibration signature is, by learning a representation of normality. We describe a novel method for Support Vector Machines of including information from a second class for novelty detection and give results from the application to Jet Engine vibration analysis.},
 author = {Hayton, Paul and Sch\"{o}lkopf, Bernhard and Tarassenko, Lionel and Anuzis, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7302e3f5e7c072aea8801faf8a492be0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7302e3f5e7c072aea8801faf8a492be0-Metadata.json},
 openalex = {W2125951032},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7302e3f5e7c072aea8801faf8a492be0-Paper.pdf},
 publisher = {MIT Press},
 title = {Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/7302e3f5e7c072aea8801faf8a492be0-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_7385db9a,
 author = {Gray, Alexander and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7385db9a3f11415bc0e9e2625fae3734-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7385db9a3f11415bc0e9e2625fae3734-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7385db9a3f11415bc0e9e2625fae3734-Paper.pdf},
 publisher = {MIT Press},
 title = {\textasciigrave N-Body\textquotesingle Problems in Statistical Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/7385db9a3f11415bc0e9e2625fae3734-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_73e0f748,
 abstract = {Competitive learning is a technique for training classification and clustering networks. We have designed and fabricated an 11- transistor primitive, that we term an automaximizing bump circuit, that implements competitive learning dynamics. The circuit performs a similarity computation, affords nonvolatile storage, and implements simultaneous local adaptation and computation. We show that our primitive is suitable for implementing competitive learning in VLSI, and demonstrate its effectiveness in a standard clustering task.},
 author = {Hsu, David and Figueroa, Miguel and Diorio, Chris},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/73e0f7487b8e5297182c5a711d20bf26-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/73e0f7487b8e5297182c5a711d20bf26-Metadata.json},
 openalex = {W2104579226},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/73e0f7487b8e5297182c5a711d20bf26-Paper.pdf},
 publisher = {MIT Press},
 title = {A Silicon Primitive for Competitive Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/73e0f7487b8e5297182c5a711d20bf26-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_7503cfac,
 abstract = {A central issue in principal component analysis (PCA) is choosing the number of principal components to be retained. By interpreting PCA as density estimation, we show how to use Bayesian model selection to estimate the true dimensionality of the data. The resulting estimate is simple to compute yet guaranteed to pick the correct dimensionality, given enough data. The estimate involves an integral over the Steifel manifold of k-frames, which is difficult to compute exactly. But after choosing an appropriate parameterization and applying Laplace's method, an accurate and practical estimator is obtained. In simulations, it is convincingly better than cross-validation and other proposed algorithms, plus it runs much faster.},
 author = {Minka, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7503cfacd12053d309b6bed5c89de212-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7503cfacd12053d309b6bed5c89de212-Metadata.json},
 openalex = {W2133028937},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf},
 publisher = {MIT Press},
 title = {Automatic Choice of Dimensionality for PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/7503cfacd12053d309b6bed5c89de212-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_77369e37,
 abstract = {Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set.},
 author = {Ghahramani, Zoubin and Beal, Matthew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/77369e37b2aa1404f416275183ab055f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/77369e37b2aa1404f416275183ab055f-Metadata.json},
 openalex = {W2098084154},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/77369e37b2aa1404f416275183ab055f-Paper.pdf},
 publisher = {MIT Press},
 title = {Propagation Algorithms for Variational Bayesian Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/77369e37b2aa1404f416275183ab055f-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_7e9e346d,
 abstract = {Nearest neighbor classification assumes locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with finite samples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. We propose a locally adaptive nearest neighbor classification method to try to minimize bias. We use a Chi-squared distance analysis to compute a flexible metric for producing neighborhoods that are elongated along less relevant feature dimensions and constricted along most influential ones. As a result, the class conditional probabilities tend to be smoother in the modified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other techniques using a variety of real world data.},
 author = {Domeniconi, Carlotta and Peng, Jing and Gunopulos, Dimitrios},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7e9e346dc5fd268b49bf418523af8679-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7e9e346dc5fd268b49bf418523af8679-Metadata.json},
 openalex = {W2125834616},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7e9e346dc5fd268b49bf418523af8679-Paper.pdf},
 publisher = {MIT Press},
 title = {An Adaptive Metric Machine for Pattern Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/7e9e346dc5fd268b49bf418523af8679-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_7ffd85d9,
 abstract = {This paper describes a method of dogleg trust-region steps, or restricted Levenberg-Marquardt steps, based on a projection process onto the Krylov subspaces for neural networks nonlinear least squares problems. In particular, the linear conjugate gradient (CG) method works as the inner iterative algorithm for solving the linearized Gauss-Newton normal equation, whereas the outer nonlinear algorithm repeatedly takes so-called Krylov-dogleg steps, relying only on matrix-vector multiplication without explicitly forming the Jacobian matrix or the Gauss-Newton model Hessian. That is, our iterative dogleg algorithm can reduce both operational counts and memory space by a factor of O(n) (the number of parameters) in comparison with a direct linear-equation solver. This memory-less property is useful for large-scale problems.},
 author = {Mizutani, Eiji and Demmel, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7ffd85d93a3e4de5c490d304ccd9f864-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7ffd85d93a3e4de5c490d304ccd9f864-Metadata.json},
 openalex = {W2112135927},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/7ffd85d93a3e4de5c490d304ccd9f864-Paper.pdf},
 publisher = {MIT Press},
 title = {On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/7ffd85d93a3e4de5c490d304ccd9f864-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_82cadb06,
 abstract = {Many processes in biology, from the regulation of gene expression in bacteria to memory in the brain, involve switches constructed from networks of biochemical reactions. Crucial molecules are present in small numbers, raising questions about noise and stability. Analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules. Prospects for direct tests of this prediction, as well as implications, are discussed.},
 author = {Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/82cadb0649a3af4968404c9f6031b233-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/82cadb0649a3af4968404c9f6031b233-Metadata.json},
 openalex = {W3016097997},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/82cadb0649a3af4968404c9f6031b233-Paper.pdf},
 publisher = {MIT Press},
 title = {Stability and noise in biochemical switches},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/82cadb0649a3af4968404c9f6031b233-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_8562ae5e,
 abstract = {Using statistical mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying the results to learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires O(ds)-training examples to learn input features of order s(d is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP can be considered as an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm.},
 author = {Winther, Ole},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8562ae5e286544710b2e7ebe9858833b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8562ae5e286544710b2e7ebe9858833b-Metadata.json},
 openalex = {W2134214176},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8562ae5e286544710b2e7ebe9858833b-Paper.pdf},
 publisher = {MIT Press},
 title = {Computing with Finite and Infinite Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/8562ae5e286544710b2e7ebe9858833b-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_85f007f8,
 abstract = {A key challenge for reinforcement learning is scaling up to large partially observable domains. In this paper, we show how a hierarchy of behaviors can be used to create and select among variable length short-term memories appropriate for a task. At higher levels in the hierarchy, the agent abstracts over lower-level details and looks back over a variable number of high-level decisions in time. We formalize this idea in a framework called Hierarchical Suffix Memory (HSM). HSM uses a memory-based SMDP learning method to rapidly propagate delayed reward across long decision sequences. We describe a detailed experimental study comparing memory vs. hierarchy using the HSM framework on a realistic corridor navigation task.},
 author = {Hernandez-Gardiol, Natalia and Mahadevan, Sridhar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/85f007f8c50dd25f5a45fca73cad64bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/85f007f8c50dd25f5a45fca73cad64bd-Metadata.json},
 openalex = {W2153175335},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/85f007f8c50dd25f5a45fca73cad64bd-Paper.pdf},
 publisher = {MIT Press},
 title = {Hierarchical Memory-Based Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/85f007f8c50dd25f5a45fca73cad64bd-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_865dfbde,
 abstract = {In this paper, we derive a second order mean field theory for directed graphical probability models. By using an information theoretic argument it is shown how this can be done in the absense of a partition function. This method is a direct generalisation of the well-known TAP approximation for Boltzmann Machines. In a numerical example, it is shown that the method greatly improves the first order mean field approximation. For a restricted class of graphical models, so-called single overlap graphs, the second order method has comparable complexity to the first order method. For sigmoid belief networks, the method is shown to be particularly fast and effective.},
 author = {Kappen, Hilbert and Wiegerinck, Wim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/865dfbde8a344b44095495f3591f7407-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/865dfbde8a344b44095495f3591f7407-Metadata.json},
 openalex = {W2139275499},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/865dfbde8a344b44095495f3591f7407-Paper.pdf},
 publisher = {MIT Press},
 title = {Second Order Approximations for Probability Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/865dfbde8a344b44095495f3591f7407-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_8c3039bd,
 abstract = {We introduce a method of feature selection for Support Vector Machines. The method is based upon finding those features which minimize bounds on the leave-one-out error. This search can be efficiently performed via gradient descent. The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA microarray data.},
 author = {Weston, Jason and Mukherjee, Sayan and Chapelle, Olivier and Pontil, Massimiliano and Poggio, Tomaso and Vapnik, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8c3039bd5842dca3d944faab91447818-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8c3039bd5842dca3d944faab91447818-Metadata.json},
 openalex = {W2097839764},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8c3039bd5842dca3d944faab91447818-Paper.pdf},
 publisher = {MIT Press},
 title = {Feature Selection for SVMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/8c3039bd5842dca3d944faab91447818-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_8c8a58fa,
 author = {Brown, Timothy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8c8a58fa97c205ff222de3685497742c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8c8a58fa97c205ff222de3685497742c-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8c8a58fa97c205ff222de3685497742c-Paper.pdf},
 publisher = {MIT Press},
 title = {Direct Classification with Indirect Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/8c8a58fa97c205ff222de3685497742c-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_8d55a249,
 abstract = {Constrained independent component analysis (cICA) is a general framework to incorporate a priori information from problem into the negentropy contrast function as constrained terms to form an augmented Lagrangian function. In this letter, a new improved algorithm for cICA is presented through the investigation of the inequality constraints, in which different closeness measurements are compared. The utility of our proposed algorithm is demonstrated by the experiments with synthetic data and electroencephalogram (EEG) data.},
 author = {Lu, Wei and Rajapakse, Jagath},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8d55a249e6baa5c06772297520da2051-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8d55a249e6baa5c06772297520da2051-Metadata.json},
 openalex = {W2165605966},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8d55a249e6baa5c06772297520da2051-Paper.pdf},
 publisher = {MIT Press},
 title = {A New Constrained Independent Component Analysis Method},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_8d9fc230,
 abstract = {In adaptive boosting, several weak learners trained sequentially are combined to boost the overall algorithm performance. Recently adaptive boosting methods for classification problems have been derived as gradient descent algorithms. This formulation justifies key elements and parameters in the methods, all chosen to optimize a single common objective function. We propose an analogous formulation for adaptive boosting of regression problems, utilizing a novel objective function that leads to a simple boosting algorithm. We prove that this method reduces training error, and compare its performance to other regression methods.},
 author = {Zemel, Richard and Pitassi, Toniann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8d9fc2308c8f28d2a7d2f6f48801c705-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8d9fc2308c8f28d2a7d2f6f48801c705-Metadata.json},
 openalex = {W2100559472},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/8d9fc2308c8f28d2a7d2f6f48801c705-Paper.pdf},
 publisher = {MIT Press},
 title = {A Gradient-Based Boosting Algorithm for Regression Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/8d9fc2308c8f28d2a7d2f6f48801c705-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_90599c8f,
 abstract = {The strong correlation between the frequency of words and their naming latency has been well documented. However, as early as 1973, the Age of Acquisition (AoA) of a word was alleged to be the actual variable of interest, but these studies seem to have been ignored in most of the literature. Recently, there has been a resurgence of interest in AoA. While some studies have shown that frequency has no effect when AoA is controlled for, more recent studies have found independent contributions of frequency and AoA. Connectionist models have repeatedly shown strong effects of frequency, but little attention has been paid to whether they can also show AoA effects. Indeed, several researchers have explicitly claimed that they cannot show AoA effects. In this work, we explore these claims using a simple feed forward neural network. We find a significant contribution of AoA to naming latency, as well as conditions under which frequency provides an independent contribution.},
 author = {Smith, Mark and Cottrell, Garrison and Anderson, Karen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/90599c8fdd2f6e7a03ad173e2f535751-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/90599c8fdd2f6e7a03ad173e2f535751-Metadata.json},
 openalex = {W2157833316},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/90599c8fdd2f6e7a03ad173e2f535751-Paper.pdf},
 publisher = {MIT Press},
 title = {The Early Word Catches the Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/90599c8fdd2f6e7a03ad173e2f535751-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_90e13578,
 abstract = {Preliminary work by the authors made use of the so-called Manhattan world assumption about the scene statistics of city and indoor scenes. This assumption stated that such scenes were built on a cartesian grid which led to regularities in the image edge gradient statistics. In this paper we explore the general applicability of this assumption and show that, surprisingly, it holds in a large variety of less structured environments including rural scenes. This enables us, from a single image, to determine the orientation of the viewer relative to the scene structure and also to detect target objects which are not aligned with the grid. These inferences are performed using a Bayesian model with probability distributions (e.g. on the image gradient statistics) learnt from real data.},
 author = {Coughlan, James and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/90e1357833654983612fb05e3ec9148c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/90e1357833654983612fb05e3ec9148c-Metadata.json},
 openalex = {W2152380633},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/90e1357833654983612fb05e3ec9148c-Paper.pdf},
 publisher = {MIT Press},
 title = {The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/90e1357833654983612fb05e3ec9148c-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_944626ad,
 abstract = {Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their weight changes on a short time scale by several hundred percent in dependence of the past input to the synapse. In this article we explore the consequences that these synaptic dynamics entail for the computational power of feedforward neural networks. We show that gradient descent suffices to approximate a given (quadratic) filter by a rather small neural system with dynamic synapses. We also compare our network model to artificial neural networks designed for time series processing. Our numerical results are complemented by theoretical analysis which show that even with just a single hidden layer such networks can approximate a surprisingly large large class of nonlinear filters: all filters that can be characterized by Volterra series. This result is robust with regard to various changes in the model for synaptic dynamics.},
 author = {Natschl\"{a}ger, Thomas and Maass, Wolfgang and Sontag, Eduardo and Zador, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/944626adf9e3b76a3919b50dc0b080a4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/944626adf9e3b76a3919b50dc0b080a4-Metadata.json},
 openalex = {W2161048147},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/944626adf9e3b76a3919b50dc0b080a4-Paper.pdf},
 publisher = {MIT Press},
 title = {Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/944626adf9e3b76a3919b50dc0b080a4-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_958adb57,
 abstract = {We present techniques for rendering and animation of realistic scenes by analyzing and training on short video sequences. This work extends the new paradigm for computer animation, video textures, which uses recorded video to generate novel animations by replaying the video samples in a new order. Here we concentrate on video sprites, which are a special type of video texture. In video sprites, instead of storing whole images, the object of interest is separated from the background and the video samples are stored as a sequence of alpha-matted sprites with associated velocity information. They can be rendered anywhere on the screen to create a novel animation of the object. We present methods to create such animations by finding a sequence of sprite samples that is both visually smooth and follows a desired path. To estimate visual smoothness, we train a linear classifier to estimate visual similarity between video samples. If the motion path is known in advance, we use beam search to find a good sample sequence. We can specify the motion interactively by precomputing the sequence cost function using Q-learning.},
 author = {Sch\"{o}dl, Arno and Essa, Irfan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/958adb57686c2fdec5796398de5f317a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/958adb57686c2fdec5796398de5f317a-Metadata.json},
 openalex = {W2128929330},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/958adb57686c2fdec5796398de5f317a-Paper.pdf},
 publisher = {MIT Press},
 title = {Machine Learning for Video-Based Rendering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/958adb57686c2fdec5796398de5f317a-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_95e6834d,
 abstract = {A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for structural signatures of hidden variables - substructures in the learned network that tend to suggest the presence of a hidden variable. We make this basic idea concrete, and show how to integrate it with structure-search algorithms. We evaluate this method on several synthetic and real-life datasets, and show that it performs surprisingly well.},
 author = {Elidan, Gal and Lotner, Noam and Friedman, Nir and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/95e6834d0a3d99e9ea8811855ae9229d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/95e6834d0a3d99e9ea8811855ae9229d-Metadata.json},
 openalex = {W2103160678},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/95e6834d0a3d99e9ea8811855ae9229d-Paper.pdf},
 publisher = {MIT Press},
 title = {Discovering Hidden Variables: A Structure-Based Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/95e6834d0a3d99e9ea8811855ae9229d-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_96c5c28b,
 abstract = {We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities.},
 author = {Schwartz, Odelia and Simoncelli, Eero},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/96c5c28becf18e71190460a9955aa4d8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/96c5c28becf18e71190460a9955aa4d8-Metadata.json},
 openalex = {W2134367463},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/96c5c28becf18e71190460a9955aa4d8-Paper.pdf},
 publisher = {MIT Press},
 title = {Natural Sound Statistics and Divisive Normalization in the Auditory System},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/96c5c28becf18e71190460a9955aa4d8-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_9a1de01f,
 abstract = {In theory, the Winnow multiplicative update has certain advantages over the Perceptron additive update when there are many irrelevant attributes. Recently, there has been much effort on enhancing the Perceptron algorithm by using regularization, leading to a class of linear classification methods called support vector machines. Similarly, it is also possible to apply the regularization idea to the Winnow algorithm, which gives methods we call regularized Winnows. We show that the resulting methods compare with the basic Winnows in a similar way that a support vector machine compares with the Perceptron. We investigate algorithmic issues and learning properties of the derived methods. Some experimental results will also be provided to illustrate different methods.},
 author = {Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9a1de01f893e0d2551ecbb7ce4dc963e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9a1de01f893e0d2551ecbb7ce4dc963e-Metadata.json},
 openalex = {W2150067374},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9a1de01f893e0d2551ecbb7ce4dc963e-Paper.pdf},
 publisher = {MIT Press},
 title = {Regularized Winnow Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/9a1de01f893e0d2551ecbb7ce4dc963e-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_9f699296,
 abstract = {FaceSync is an optimal linear algorithm that finds the degree of synchronization between the audio and image recordings of a human speaker. Using canonical correlation, it finds the best direction to combine all the audio and image data, projecting them onto a single axis. FaceSync uses Pearson's correlation to measure the degree of synchronization between the audio and image data. We derive the optimal linear transform to combine the audio and visual information and describe an implementation that avoids the numerical problems caused by computing the correlation matrices.},
 author = {Slaney, Malcolm and Covell, Michele},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9f6992966d4c363ea0162a056cb45fe5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9f6992966d4c363ea0162a056cb45fe5-Metadata.json},
 openalex = {W2100561338},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9f6992966d4c363ea0162a056cb45fe5-Paper.pdf},
 publisher = {MIT Press},
 title = {FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/9f6992966d4c363ea0162a056cb45fe5-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_9fdb62f9,
 abstract = {We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an infinite number of Experts. Inference in this model may be done efficiently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.},
 author = {Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9fdb62f932adf55af2c0e09e55861964-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9fdb62f932adf55af2c0e09e55861964-Metadata.json},
 openalex = {W2098949458},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/9fdb62f932adf55af2c0e09e55861964-Paper.pdf},
 publisher = {MIT Press},
 title = {Infinite Mixtures of Gaussian Process Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/9fdb62f932adf55af2c0e09e55861964-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_a19acd7d,
 abstract = {In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the Hodgkin-Huxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrate-and-fire or perceptron model.},
 author = {Ag\"{u}era y Arcas, Blaise and Fairhall, Adrienne and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/a19acd7d2689207f9047f8cb01357370-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/a19acd7d2689207f9047f8cb01357370-Metadata.json},
 openalex = {W2108674742},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/a19acd7d2689207f9047f8cb01357370-Paper.pdf},
 publisher = {MIT Press},
 title = {What Can a Single Neuron Compute},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/a19acd7d2689207f9047f8cb01357370-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_a3545bd7,
 abstract = {We present the embedded trees algorithm, an iterative technique for estimation of Gaussian processes defined on arbitrary graphs. By exactly solving a series of modified problems on embedded spanning trees, it computes the conditional means with an efficiency comparable to or better than other techniques. Unlike other methods, the embedded trees algorithm also computes exact error covariances. The error covariance computation is most efficient for graphs in which removing a small number of edges reveals an embedded tree. In this context, we demonstrate that sparse loopy graphs can provide a significant increase in modeling power relative to trees, with only a minor increase in estimation complexity.},
 author = {Wainwright, Martin J and Sudderth, Erik and Willsky, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/a3545bd79d31f9a72d3a78690adf73fc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/a3545bd79d31f9a72d3a78690adf73fc-Metadata.json},
 openalex = {W2161023163},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/a3545bd79d31f9a72d3a78690adf73fc-Paper.pdf},
 publisher = {MIT Press},
 title = {Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/a3545bd79d31f9a72d3a78690adf73fc-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_aa2a7737,
 abstract = {It has long been known that lateral inhibition in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. Here we show how to organize lateral inhibition so that groups of neurons compete to be active. Given a collection of potentially overlapping groups, the inhibitory connectivity is set by a formula that can be interpreted as arising from a simple learning rule. Our analysis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some degenerate cases. In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons.},
 author = {Xie, Xiaohui and Hahnloser, Richard and Seung, H. Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/aa2a77371374094fe9e0bc1de3f94ed9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/aa2a77371374094fe9e0bc1de3f94ed9-Metadata.json},
 openalex = {W2098860182},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/aa2a77371374094fe9e0bc1de3f94ed9-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/aa2a77371374094fe9e0bc1de3f94ed9-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_ab731488,
 abstract = {We introduce total wire length as salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation- and scale-invariant sensory processing. We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length.},
 author = {Legenstein, Robert and Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ab7314887865c4265e896c6e209d1cd6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ab7314887865c4265e896c6e209d1cd6-Metadata.json},
 openalex = {W2146516813},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ab7314887865c4265e896c6e209d1cd6-Paper.pdf},
 publisher = {MIT Press},
 title = {Foundations for a Circuit Complexity Theory of Sensory Processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/ab7314887865c4265e896c6e209d1cd6-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_ac5dab2e,
 abstract = {A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad.},
 author = {Pedersen, Liam and Apostolopoulos, Dimitrios and Whittaker, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ac5dab2e99eee9cf9ec672e383691302-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ac5dab2e99eee9cf9ec672e383691302-Metadata.json},
 openalex = {W2146997108},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ac5dab2e99eee9cf9ec672e383691302-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayes Networks on Ice: Robotic Search for Antarctic Meteorites},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/ac5dab2e99eee9cf9ec672e383691302-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_ad4cc1fb,
 abstract = {We present methods for learning and tracking human motion in video. We estimate a statistical model of typical activities from a large set of 3D periodic human motion data by segmenting these data automatically into Then the mean and the principal components of the cycles are computed using a new algorithm that accounts for missing information and enforces smooth transitions between cycles. The learned temporal model provides a prior probability distribution over human motions that can be used in a Bayesian framework for tracking human subjects in complex monocular video sequences and recovering their 3D motion.},
 author = {Ormoneit, Dirk and Sidenbladh, Hedvig and Black, Michael and Hastie, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ad4cc1fb9b068faecfb70914acc63395-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ad4cc1fb9b068faecfb70914acc63395-Metadata.json},
 openalex = {W2171165942},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ad4cc1fb9b068faecfb70914acc63395-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning and Tracking Cyclic Human Motion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/ad4cc1fb9b068faecfb70914acc63395-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_af3303f8,
 abstract = {How should we decide among competing explanations of a cognitive process given limited observations? The problem of model selection is at the heart of progress in cognitive science. In this paper, Minimum Description Length (MDL) is introduced as a method for selecting among computational models of cognition. We also show that differential geometry provides an intuitive understanding of what drives model selection in MDL. Finally, adequacy of MDL is demonstrated in two areas of cognitive modeling.},
 author = {Myung, In and Pitt, Mark and Zhang, Shaobo and Balasubramanian, Vijay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/af3303f852abeccd793068486a391626-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/af3303f852abeccd793068486a391626-Metadata.json},
 openalex = {W2095636500},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/af3303f852abeccd793068486a391626-Paper.pdf},
 publisher = {MIT Press},
 title = {The Use of MDL to Select among Computational Models of Cognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/af3303f852abeccd793068486a391626-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_b139e104,
 abstract = {An active set strategy is applied to the dual of a simple reformulation of the standard quadratic program of a linear support vector machine. This application generates a fast new dual algorithm that consists of solving a finite number of linear equations, with a typically large dimensionality equal to the number of points to be classified. However, by making novel use of the Sherman-Morrison-Woodbury formula, a much smaller matrix of the order of the original input space is inverted at each step. Thus, a problem with a 32-dimensional input space and 7 million points required inverting positive definite symmetric matrices of size 33 × 33 with a total running time of 96 minutes on a 400 MHz Pentium II. The algorithm requires no specialized quadratic or linear programming code, but merely a linear equation solver which is publicly available.},
 author = {Mangasarian, Olvi and Musicant, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/b139e104214a08ae3f2ebcce149cdf6e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/b139e104214a08ae3f2ebcce149cdf6e-Metadata.json},
 openalex = {W2170720453},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/b139e104214a08ae3f2ebcce149cdf6e-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Support Vector Machine Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/b139e104214a08ae3f2ebcce149cdf6e-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_b432f34c,
 abstract = {This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem.},
 author = {Carlstr\"{o}m, Jakob},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/b432f34c5a997c8e7c806a895ecc5e25-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/b432f34c5a997c8e7c806a895ecc5e25-Metadata.json},
 openalex = {W2142613925},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/b432f34c5a997c8e7c806a895ecc5e25-Paper.pdf},
 publisher = {MIT Press},
 title = {Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/b432f34c5a997c8e7c806a895ecc5e25-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_b4568df2,
 abstract = {We demonstrate that statistical analysis of ill-posed data sets is subject to a bias, which can be observed when projecting independent test set examples onto a basis defined by the training examples. Because the training examples in an ill-posed data set do not fully span the signal space the observed training set variances in each basis vector will be too high compared to the average variance of the test set projections onto the same basis vectors. On basis of this understanding we introduce the Generalizable Singular Value Decomposition (GenSVD) as a means to reduce this bias by re-estimation of the singular values obtained in a conventional Singular Value Decomposition, allowing for a generalization performance increase of a subsequent statistical model. We demonstrate that the algorithm succesfully corrects bias in a data set from a functional PET activation study of the human brain.},
 author = {Kjems, Ulrik and Hansen, Lars and Strother, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/b4568df26077653eeadf29596708c94b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/b4568df26077653eeadf29596708c94b-Metadata.json},
 openalex = {W2114198958},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/b4568df26077653eeadf29596708c94b-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalizable Singular Value Decomposition for Ill-posed Datasets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/b4568df26077653eeadf29596708c94b-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_ba9a56ce,
 abstract = {The Vicinal Risk Minimization principle establishes a bridge between generative models and methods derived from the Structural Risk Minimization Principle such as Support Vector Machines or Statistical Regularization. We explain how VRM provides a framework which integrates a number of existing algorithms, such as Parzen windows, Support Vector Machines, Ridge Regression, Constrained Logistic Classifiers and Tangent-Prop. We then show how the approach implies new algorithms for solving problems usually associated with generative models. New algorithms are described for dealing with pattern recognition problems with very different pattern distributions and dealing with unlabeled data. Preliminary empirical results are presented.},
 author = {Chapelle, Olivier and Weston, Jason and Bottou, L\'{e}on and Vapnik, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Metadata.json},
 openalex = {W2151239833},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Paper.pdf},
 publisher = {MIT Press},
 title = {Vicinal Risk Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_bf201d54,
 abstract = {'Kernel' principal component analysis (PCA) is an elegant nonlinear generalisation of the popular linear data analysis method, where a kernel function implicitly defines a nonlinear transformation into a feature space wherein standard PCA is performed. Unfortunately, the technique is not 'sparse', since the components thus obtained are expressed in terms of kernels associated with every training vector. This paper shows that by approximating the covariance matrix in feature space by a reduced number of example vectors, using a maximum-likelihood approach, we may obtain a highly sparse form of kernel PCA without loss of effectiveness.},
 author = {Tipping, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/bf201d5407a6509fa536afc4b380577e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/bf201d5407a6509fa536afc4b380577e-Metadata.json},
 openalex = {W2119388857},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf},
 publisher = {MIT Press},
 title = {Sparse Kernel Principal Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/bf201d5407a6509fa536afc4b380577e-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_c164bbc9,
 abstract = {Vapnik's result that expectation of generalisation of optimal hyperplane is bounded by expectation of ratio of number of support vectors to number of training examples is extended to a broad class of kernel machines. The class includes Support Vector Machines for soft margin classification and regression, and Regularization Networks with a variety of kernels and cost functions. We show that key inequalities in Vapnik's result become equalities once the classification error is replaced by the margin error, with latter defined as an instance with positive cost. In particular we show that expectations of true margin and empirical margin are equal, and that sparse solutions for kernel machines are possible only if cost function is partially insensitive.},
 author = {Kowalczyk, Adam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c164bbc9d6c72a52c599bbb43d8db8e1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c164bbc9d6c72a52c599bbb43d8db8e1-Metadata.json},
 openalex = {W2118966834},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c164bbc9d6c72a52c599bbb43d8db8e1-Paper.pdf},
 publisher = {MIT Press},
 title = {Sparsity of Data Representation of Optimal Kernel Machine and Leave-one-out Estimator},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/c164bbc9d6c72a52c599bbb43d8db8e1-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_c366c2c9,
 abstract = {We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.},
 author = {Teh, Yee Whye and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c366c2c97d47b02b24c3ecade4c40a01-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c366c2c97d47b02b24c3ecade4c40a01-Metadata.json},
 openalex = {W2114153178},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c366c2c97d47b02b24c3ecade4c40a01-Paper.pdf},
 publisher = {MIT Press},
 title = {Rate-coded Restricted Boltzmann Machines for Face Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/c366c2c97d47b02b24c3ecade4c40a01-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_c44799b0,
 abstract = {We develop an approach to object recognition based on matching shapes and using a resulting measure of similarity in a nearest neighbor classifier. The key algorithmic problem here is that of finding pointwise correspondences between an image shape and a stored prototype shape. We introduce a new shape descriptor, the shape context, which makes this possible, using a simple and robust algorithm. The shape context at a point captures the distribution over relative positions of other shape points and thus summarizes global shape in a rich, local descriptor. We demonstrate that shape contexts greatly simplify recovery of correspondences between points of two given shapes. Once shapes are aligned, shape contexts are used to define a robust score for measuring shape similarity. We have used this score in a nearest-neighbor classifier for recognition of hand written digits as well as 3D objects, using exactly the same distance function. On the benchmark MNIST dataset of handwritten digits, this yields an error rate of 0.63%, outperforming other published techniques.},
 author = {Belongie, Serge and Malik, Jitendra and Puzicha, Jan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c44799b04a1c72e3c8593a53e8000c78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c44799b04a1c72e3c8593a53e8000c78-Metadata.json},
 openalex = {W2108444897},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c44799b04a1c72e3c8593a53e8000c78-Paper.pdf},
 publisher = {MIT Press},
 title = {Shape Context: A New Descriptor for Shape Matching and Object Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/c44799b04a1c72e3c8593a53e8000c78-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_c4500821,
 abstract = {Stimulus arrays are inevitably presented at different positions on the retina in visual tasks, even those that nominally require fixation. In particular, this applies to many perceptual learning tasks. We show that perceptual inference or discrimination in the face of positional variance has a structurally different quality from inference about fixed position stimuli, involving a particular, quadratic, non-linearity rather than a purely linear discrimination. We show the advantage taking this non-linearity into account has for discrimination, and suggest it as a role for recurrent connections in area VI, by demonstrating the superior discrimination performance of a recurrent network. We propose that learning the feedforward and recurrent neural connections for these tasks corresponds to the fast and slow components of learning observed in perceptual learning tasks.},
 author = {Li, Zhaoping and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c45008212f7bdf6eab6050c2a564435a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c45008212f7bdf6eab6050c2a564435a-Metadata.json},
 openalex = {W2105746095},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c45008212f7bdf6eab6050c2a564435a-Paper.pdf},
 publisher = {MIT Press},
 title = {Position Variance, Recurrence and Perceptual Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/c45008212f7bdf6eab6050c2a564435a-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_c8cbd669,
 abstract = {The richness and complexity of recurrent cortical circuits is an inexhaustible source of inspiration for thinking about high-level biological computation. In past theoretical studies, constraints on the synaptic connection patterns of threshold-linear networks were found that guaranteed bounded network dynamics, convergence to attractive fixed points, and multistability, all fundamental aspects of cortical information processing. However, these conditions were only sufficient, and it remained unclear which were the minimal (necessary) conditions for convergence and multistability. We show that symmetric threshold-linear networks converge to a set of attractive fixed points if and only if the network matrix is copositive. Furthermore, the set of attractive fixed points is nonconnected (the network is multiattractive) if and only if the network matrix is not positive semidefinite. There are permitted sets of neurons that can be coactive at a stable steady state and forbidden sets that cannot. Permitted sets are clustered in the sense that subsets of permitted sets are permitted and supersets of forbidden sets are forbidden. By viewing permitted sets as memories stored in the synaptic connections, we provide a formulation of long-term memory that is more general than the traditional perspective of fixed-point attractor networks. There is a close correspondence between threshold-linear networks and networks defined by the generalized Lotka-Volterra equations.},
 author = {Hahnloser, Richard and Seung, H. Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c8cbd669cfb2f016574e9d147092b5bb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c8cbd669cfb2f016574e9d147092b5bb-Metadata.json},
 openalex = {W2115761837},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c8cbd669cfb2f016574e9d147092b5bb-Paper.pdf},
 publisher = {MIT Press},
 title = {Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/c8cbd669cfb2f016574e9d147092b5bb-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_c91591a8,
 abstract = {Hebbian and competitive Hebbian algorithms are almost ubiquitous in modeling pattern formation in cortical development. We analyse in theoretical detail a particular model (adapted from Piepenbrock & Obermayer, 1999) for the development of Id stripe-like patterns, which places competitive and interactive cortical influences, and free and restricted initial arborisation onto a common footing.},
 author = {Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c91591a8d461c2869b9f535ded3e213e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c91591a8d461c2869b9f535ded3e213e-Metadata.json},
 openalex = {W2121347745},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c91591a8d461c2869b9f535ded3e213e-Paper.pdf},
 publisher = {MIT Press},
 title = {Competition and Arbors in Ocular Dominance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/c91591a8d461c2869b9f535ded3e213e-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_ca460332,
 abstract = {The human figure exhibits complex and rich dynamic behavior that is both nonlinear and time-varying. Effective models of human dynamics can be learned from motion capture data using switching linear dynamic system (SLDS) models. We present results for human motion synthesis, classification, and visual tracking using learned SLDS models. Since exact inference in SLDS is intractable, we present three approximate inference algorithms and compare their performance. In particular, a new variational inference algorithm is obtained by casting the SLDS model as a Dynamic Bayesian Network. Classification experiments show the superiority of SLDS over conventional HMM's for our problem domain.},
 author = {Pavlovic, Vladimir and Rehg, James M and MacCormick, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ca460332316d6da84b08b9bcf39b687b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ca460332316d6da84b08b9bcf39b687b-Metadata.json},
 openalex = {W2110935301},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ca460332316d6da84b08b9bcf39b687b-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Switching Linear Models of Human Motion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/ca460332316d6da84b08b9bcf39b687b-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_cb79f8fa,
 abstract = {In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent) experts, each expert focusing on a different characteristic of the signal, and that the different stream likelihoods (or posteriors) are combined at some (temporal) stage to yield a global recognition output. As a further extension to multi-stream ASR, we will finally introduce a new approach, referred to as HMM2, where the HMM emission probabilities are estimated via state specific feature based HMMs responsible for merging the stream information and modeling their possible correlation.},
 author = {Bourlard, Herv\'{e} and Bengio, Samy and Weber, Katrin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/cb79f8fa58b91d3af6c9c991f63962d3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/cb79f8fa58b91d3af6c9c991f63962d3-Metadata.json},
 openalex = {W2101732903},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf},
 publisher = {MIT Press},
 title = {New Approaches Towards Robust and Adaptive Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_cd14821d,
 abstract = {We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.},
 author = {Arleo, Angelo and Smeraldi, Fabrizio and Hug, St\'{e}phane and Gerstner, Wulfram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/cd14821dab219ea06e2fd1a2df2e3582-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/cd14821dab219ea06e2fd1a2df2e3582-Metadata.json},
 openalex = {W2155286882},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/cd14821dab219ea06e2fd1a2df2e3582-Paper.pdf},
 publisher = {MIT Press},
 title = {Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/cd14821dab219ea06e2fd1a2df2e3582-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_cd63a3ee,
 abstract = {Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem.},
 author = {Ormoneit, Dirk and Glynn, Peter W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/cd63a3eec3319fd9c84c942a08316e00-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/cd63a3eec3319fd9c84c942a08316e00-Metadata.json},
 openalex = {W2101307636},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/cd63a3eec3319fd9c84c942a08316e00-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/cd63a3eec3319fd9c84c942a08316e00-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d072677d,
 abstract = {A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p 2 for a set of linearly separable data. Our algorithm, called ALMA_p (Approximate La...},
 author = {Gentile, Claudio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d072677d210ac4c03ba046120f0802ec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d072677d210ac4c03ba046120f0802ec-Metadata.json},
 openalex = {W3003652643},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf},
 publisher = {MIT Press},
 title = {A new approximate maximal margin classification algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d072677d210ac4c03ba046120f0802ec-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d1e946f4,
 abstract = {We propose a general Bayesian framework for performing independent component analysis (ICA) which relies on ensemble learning and linear response theory known from statistical physics. We apply it to both discrete and continuous sources. For the continuous source the underdetermined (overcomplete) case is studied. The naive mean-field approach fails in this case whereas linear response theory-which gives an improved estimate of covariances-is very efficient. The examples given are for sources without temporal correlations. However, this derivation can easily be extended to treat temporal correlations. Finally, the framework offers a simple way of generating new ICA algorithms without needing to define the prior distribution of the sources explicitly.},
 author = {H\o jen-S\o rensen, Pedro and Winther, Ole and Hansen, Lars},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d1e946f4e67db4b362ad23818a6fb78a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d1e946f4e67db4b362ad23818a6fb78a-Metadata.json},
 openalex = {W2134650532},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d1e946f4e67db4b362ad23818a6fb78a-Paper.pdf},
 publisher = {MIT Press},
 title = {Ensemble Learning and Linear Response Theory for ICA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d1e946f4e67db4b362ad23818a6fb78a-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d2541440,
 abstract = {In this paper we give necessary and sufficient conditions under which kernels of dot product type k(x, y) = k(x ċ y) satisfy Mercer's condition and thus may be used in Support Vector Machines (SVM), Regularization Networks (RN) or Gaussian Processes (GP). In particular, we show that if the kernel is analytic (i.e. can be expanded in a Taylor series), all expansion coefficients have to be nonnegative. We give an explicit functional form for the feature map by calculating its eigenfunctions and eigenvalues.},
 author = {Smola, Alex and \'{O}v\'{a}ri, Zolt\'{a}n and Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d25414405eb37dae1c14b18d6a2cac34-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d25414405eb37dae1c14b18d6a2cac34-Metadata.json},
 openalex = {W2139999468},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d25414405eb37dae1c14b18d6a2cac34-Paper.pdf},
 publisher = {MIT Press},
 title = {Regularization with Dot-Product Kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d25414405eb37dae1c14b18d6a2cac34-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d305281f,
 abstract = {We present an improvement of Novikoff's perceptron convergence theorem. Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC-style generalisation error bound for the classifier learned by the perceptron learning algorithm. The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel. Ironically, the bound yields better guarantees than are currently available for the support vector solution itself.},
 author = {Graepel, Thore and Herbrich, Ralf and Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d305281faf947ca7acade9ad5c8c818c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d305281faf947ca7acade9ad5c8c818c-Metadata.json},
 openalex = {W2098641446},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d305281faf947ca7acade9ad5c8c818c-Paper.pdf},
 publisher = {MIT Press},
 title = {From Margin to Sparsity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d305281faf947ca7acade9ad5c8c818c-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d4b2aeb2,
 author = {Williams, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d4b2aeb2453bdadaa45cbe9882ffefcf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d4b2aeb2453bdadaa45cbe9882ffefcf-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d4b2aeb2453bdadaa45cbe9882ffefcf-Paper.pdf},
 publisher = {MIT Press},
 title = {On a Connection between Kernel PCA and Metric Multidimensional Scaling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d4b2aeb2453bdadaa45cbe9882ffefcf-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d523773c,
 abstract = {Source separation, or computational auditory scene analysis, attempts to extract individual acoustic objects from input which contains a mixture of sounds from different sources, altered by the acoustic environment. Unmixing algorithms such as ICA and its extensions recover sources by reweighting multiple observation sequences, and thus cannot operate when only a single observation signal is available. I present a technique called refiltering which recovers sources by a nonstationary reweighting (masking) of frequency sub-bands from a single recording, and argue for the application of statistical algorithms to learning this masking function. I present results of a simple factorial HMM system which learns on recordings of single speakers and can then separate mixtures using only one observation signal by computing the masking function and then refiltering.},
 author = {Roweis, Sam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d523773c6b194f37b938d340d5d02232-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d523773c6b194f37b938d340d5d02232-Metadata.json},
 openalex = {W2168793898},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d523773c6b194f37b938d340d5d02232-Paper.pdf},
 publisher = {MIT Press},
 title = {One Microphone Source Separation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d523773c6b194f37b938d340d5d02232-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d54e99a6,
 abstract = {In this work, we introduce an Interactive Parts (IP) model as an alternative to Hidden Markov Models (HMMs). We tested both models on a database of on-line cursive script. We show that implementations of HMMs and the IP model, in which all letters are assumed to have the same average width, give comparable results. However, in contrast to HMMs, the IP model can handle duration modeling without an increase in computational complexity.},
 author = {Neskovic, Predrag and Davis, Philip and Cooper, Leon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d54e99a6c03704e95e6965532dec148b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d54e99a6c03704e95e6965532dec148b-Metadata.json},
 openalex = {W2139729036},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf},
 publisher = {MIT Press},
 title = {Interactive Parts Model: An Application to Recognition of On-line Cursive Script},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d54e99a6c03704e95e6965532dec148b-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d757719e,
 abstract = {Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data.},
 author = {Den\`{e}ve, Sophie and Duhamel, Jean-Ren\'{e} and Pouget, Alexandre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d757719ed7c2b66dd17dcee2a3cb29f4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d757719ed7c2b66dd17dcee2a3cb29f4-Metadata.json},
 openalex = {W2114715177},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d757719ed7c2b66dd17dcee2a3cb29f4-Paper.pdf},
 publisher = {MIT Press},
 title = {A New Model of Spatial Representation in Multimodal Brain Areas},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d757719ed7c2b66dd17dcee2a3cb29f4-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d7657583,
 abstract = {When trying to recover 3D structure from a set of images, the most difficult problem is establishing the correspondence between the measurements. Most existing approaches assume that features can be tracked across frames, whereas methods that exploit rigidity constraints to facilitate matching do so only under restricted camera motion. In this paper we propose a Bayesian approach that avoids the brittleness associated with singling out one best correspondence, and instead consider the distribution over all possible correspondences. We treat both a fully Bayesian approach that yields a posterior distribution, and a MAP approach that makes use of EM to maximize this posterior. We show how Markov chain Monte Carlo methods can be used to implement these techniques in practice, and present experimental results on real data.},
 author = {Dellaert, Frank and Seitz, Steven and Thrun, Sebastian and Thorpe, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d7657583058394c828ee150fada65345-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d7657583058394c828ee150fada65345-Metadata.json},
 openalex = {W2157635638},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d7657583058394c828ee150fada65345-Paper.pdf},
 publisher = {MIT Press},
 title = {Feature Correspondence: A Markov Chain Monte Carlo Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d7657583058394c828ee150fada65345-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_d7fd118e,
 abstract = {We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. Both mean-field analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation.},
 author = {Wong, K. Y. Michael and Nishimori, Hidetoshi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d7fd118e6f226a71b5f1ffe10efd0a78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d7fd118e6f226a71b5f1ffe10efd0a78-Metadata.json},
 openalex = {W2112902274},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/d7fd118e6f226a71b5f1ffe10efd0a78-Paper.pdf},
 publisher = {MIT Press},
 title = {Stagewise Processing in Error-correcting Codes and Image Restoration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/d7fd118e6f226a71b5f1ffe10efd0a78-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_dc40b712,
 abstract = {We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise.},
 author = {Graepel, Thore and Herbrich, Ralf},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dc40b7120e77741d191c0d2b82cea7be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dc40b7120e77741d191c0d2b82cea7be-Metadata.json},
 openalex = {W2156622608},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dc40b7120e77741d191c0d2b82cea7be-Paper.pdf},
 publisher = {MIT Press},
 title = {The Kernel Gibbs Sampler},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/dc40b7120e77741d191c0d2b82cea7be-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_dc5d637e,
 abstract = {We show how a wavelet basis may be adapted to best represent natural images in terms of sparse coefficients. The wavelet basis, which may be either complete or overcomplete, is specified by a small number of spatial functions which are repeated across space and combined in a recursive fashion so as to be self-similar across scale. These functions are adapted to minimize the estimated code length under a model that assumes images are composed of a linear superposition of sparse, independent components. When adapted to natural images, the wavelet bases take on different orientations and they evenly tile the orientation domain, in stark contrast to the standard, non-oriented wavelet bases used in image compression. When the basis set is allowed to be overcomplete, it also yields higher coding efficiency than standard wavelet bases.},
 author = {Olshausen, Bruno and Sallee, Phil and Lewicki, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dc5d637ed5e62c36ecb73b654b05ba2a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dc5d637ed5e62c36ecb73b654b05ba2a-Metadata.json},
 openalex = {W2102024628},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dc5d637ed5e62c36ecb73b654b05ba2a-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Sparse Image Codes using a Wavelet Pyramid Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/dc5d637ed5e62c36ecb73b654b05ba2a-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_dea9ddb2,
 abstract = {The problem of constructing weak classifiers for boosting algorithms is studied. We present an algorithm that produces a linear classifier that is guaranteed to achieve an error better than random guessing for any distribution on the data. While this weak learner is not useful for learning in general, we show that under reasonable conditions on the distribution it yields an effective weak learner for one-dimensional problems. Preliminary simulations suggest that similar behavior can be expected in higher dimensions, a result which is corroborated by some recent theoretical bounds. Additionally, we provide improved convergence rate bounds for the generalization error in situations where the empirical error can be made small, which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established.},
 author = {Mannor, Shie and Meir, Ron},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dea9ddb25cbf2352cf4dec30222a02a5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dea9ddb25cbf2352cf4dec30222a02a5-Metadata.json},
 openalex = {W2164303082},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf},
 publisher = {MIT Press},
 title = {Weak Learners and Improved Rates of Convergence in Boosting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/dea9ddb25cbf2352cf4dec30222a02a5-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_dfce0680,
 abstract = {Condensation, a form of likelihood-weighted particle filtering, has been successfully used to infer the shapes of highly constrained contours in video sequences. However, when the contours are highly flexible (e.g. for tracking fingers of a hand), a computationally burdensome number of particles is needed to successfully approximate the distribution. We show how the Metropolis algorithm can be used to update a particle set representing a distribution over contours at each frame in a video sequence. We compare this method to condensation using a video sequence that requires highly flexible contours, and show that the new algorithm performs dramatically better that the condensation algorithm. We discuss the incorporation of this method into the active contour framework where a shape-subspace is used constrain shape variation.},
 author = {Kristjansson, Trausti and Frey, Brendan J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dfce06801e1a85d6d06f1fdd4475dacd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dfce06801e1a85d6d06f1fdd4475dacd-Metadata.json},
 openalex = {W2123081750},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/dfce06801e1a85d6d06f1fdd4475dacd-Paper.pdf},
 publisher = {MIT Press},
 title = {Keeping Flexible Active Contours on Track using Metropolis Updates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/dfce06801e1a85d6d06f1fdd4475dacd-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_e06f967f,
 abstract = {We introduce a new, non-parametric and principled, distance based clustering method. This method combines a pairwise based approach with a vector-quantization method which provide a meaningful interpretation to the resulting clusters. The idea is based on turning the distance matrix into a Markov process and then examine the decay of mutual-information during the relaxation of this process. The clusters emerge as quasi-stable structures during this relaxation, and then are extracted using the information bottleneck method. These clusters capture the information about the initial point of the relaxation in the most effective way. The method can cluster data with no geometric or other bias and makes no assumption about the underlying distribution.},
 author = {Tishby, Naftali and Slonim, Noam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e06f967fb0d355592be4e7674fa31d26-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e06f967fb0d355592be4e7674fa31d26-Metadata.json},
 openalex = {W2127086485},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e06f967fb0d355592be4e7674fa31d26-Paper.pdf},
 publisher = {MIT Press},
 title = {Data Clustering by Markovian Relaxation and the Information Bottleneck Method},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/e06f967fb0d355592be4e7674fa31d26-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_e0ab531e,
 abstract = {For many problems which would be natural for reinforcement learning, the reward signal is not a single scalar value but has multiple scalar components. Examples of such problems include agents with multiple goals and agents with multiple users. Creating a single reward value by combining the multiple components can throw away vital information and can lead to incorrect solutions. We describe the multiple reward source problem and discuss the problems with applying traditional reinforcement learning. We then present an new algorithm for finding a solution and results on simulated environments.},
 author = {Shelton, Christian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e0ab531ec312161511493b002f9be2ee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e0ab531ec312161511493b002f9be2ee-Metadata.json},
 openalex = {W2151726636},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf},
 publisher = {MIT Press},
 title = {Balancing Multiple Sources of Reward in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/e0ab531ec312161511493b002f9be2ee-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_e1314fc0,
 abstract = {The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters.},
 author = {Chechik, Gal and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e1314fc026da60d837353d20aefaf054-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e1314fc026da60d837353d20aefaf054-Metadata.json},
 openalex = {W2127480198},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e1314fc026da60d837353d20aefaf054-Paper.pdf},
 publisher = {MIT Press},
 title = {Temporally Dependent Plasticity: An Information Theoretic Account},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/e1314fc026da60d837353d20aefaf054-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_e5b294b7,
 abstract = {We analyze the bit error probability of multiuser demodulators for direct-sequence binary phase-shift-keying (DS/BPSK) CDMA channel with additive gaussian noise. The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is applied to evaluate the performance of the resulting MPM (Marginal Posterior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. An approximate implementation of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. Results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, especially in the cases of small information bit rate and low noise level.},
 author = {Tanaka, Toshiyuki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e5b294b70c9647dcf804d7baa1903918-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e5b294b70c9647dcf804d7baa1903918-Metadata.json},
 openalex = {W2153135682},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e5b294b70c9647dcf804d7baa1903918-Paper.pdf},
 publisher = {MIT Press},
 title = {Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/e5b294b70c9647dcf804d7baa1903918-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_e74c0d42,
 abstract = {A variational derivation of Plefka's mean-field theory is presented. This theory is then applied to sigmoidal belief networks with the aid of further approximations. Empirical evaluation on small scale networks show that the proposed approximations are quite competitive.},
 author = {Bhattacharyya, Chiranjib and Keerthi, S.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e74c0d42b4433905293aab661fcf8ddb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e74c0d42b4433905293aab661fcf8ddb-Metadata.json},
 openalex = {W2101058190},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e74c0d42b4433905293aab661fcf8ddb-Paper.pdf},
 publisher = {MIT Press},
 title = {A Variational Mean-Field Theory for Sigmoidal Belief Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/e74c0d42b4433905293aab661fcf8ddb-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_e8dfff46,
 abstract = {This letter proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as modeling errors. The use of environmental models in RL is quite popular for both offline learning using simulations and for online action planning. However, the difference between the model and the real environment can lead to unpredictable, and often unwanted, results. Based on the theory of H(infinity) control, we consider a differential game in which a "disturbing" agent tries to make the worst possible disturbance while a "control" agent tries to make the best control input. The problem is formulated as finding a min-max solution of a value function that takes into account the amount of the reward and the norm of the disturbance. We derive online learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in reference to the value function. We tested the paradigm, which we call robust reinforcement learning (RRL), on the control task of an inverted pendulum. In the linear domain, the policy and the value function learned by online algorithms coincided with those derived analytically by the linear H(infinity) control theory. For a fully nonlinear swing-up task, RRL achieved robust performance with changes in the pendulum weight and friction, while a standard reinforcement learning algorithm could not deal with these changes. We also applied RRL to the cart-pole swing-up task, and a robust swing-up policy was acquired.},
 author = {Morimoto, Jun and Doya, Kenji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e8dfff4676a47048d6f0c4ef899593dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e8dfff4676a47048d6f0c4ef899593dd-Metadata.json},
 openalex = {W2105078254},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/e8dfff4676a47048d6f0c4ef899593dd-Paper.pdf},
 publisher = {MIT Press},
 title = {Robust Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/e8dfff4676a47048d6f0c4ef899593dd-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_eb1e7832,
 abstract = {Explaining away has mostly been considered in terms of inference of states in belief networks. We show how it can also arise in a Bayesian context in inference about the weights governing relationships such as those between stimuli and reinforcers in conditioning experiments such as backward blocking. We show how explaining away in weight space can be accounted for using an extension of a Kalman filter model; provide a new approximate way of looking at the Kalman gain matrix as a whitener for the correlation matrix of the observation process; suggest a network implementation of this whitener using an architecture due to Goodall; and show that the resulting model exhibits backward blocking.},
 author = {Dayan, Peter and Kakade, Sham},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/eb1e78328c46506b46a4ac4a1e378b91-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/eb1e78328c46506b46a4ac4a1e378b91-Metadata.json},
 openalex = {W2122925528},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf},
 publisher = {MIT Press},
 title = {Explaining Away in Weight Space},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/eb1e78328c46506b46a4ac4a1e378b91-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_ecd62de2,
 abstract = {Output coding is a general method for solving multiclass problems by reducing them to multiple binary classification problems. Previous research on output coding has employed, almost solely, predefined discrete codes. We describe an algorithm that improves the performance of output codes by relaxing them to continuous codes. The relaxation procedure is cast as an optimization problem and is reminiscent of the quadratic program for support vector machines. We describe experiments with the proposed algorithm, comparing it to standard discrete output codes. The experimental results indicate that continuous relaxations of output codes often improve the generalization performance, especially for short codes.},
 author = {Crammer, Koby and Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ecd62de20ea67e1c2d933d311b08178a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ecd62de20ea67e1c2d933d311b08178a-Metadata.json},
 openalex = {W2121420451},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ecd62de20ea67e1c2d933d311b08178a-Paper.pdf},
 publisher = {MIT Press},
 title = {Improved Output Coding for Classification Using Continuous Relaxation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/ecd62de20ea67e1c2d933d311b08178a-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_ed519dac,
 abstract = {Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaussian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models.},
 author = {Malzahn, D\"{o}rthe and Opper, Manfred},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ed519dacc89b2bead3f453b0b05a4a8b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ed519dacc89b2bead3f453b0b05a4a8b-Metadata.json},
 openalex = {W2100538233},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ed519dacc89b2bead3f453b0b05a4a8b-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/ed519dacc89b2bead3f453b0b05a4a8b-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f0bbac6f,
 author = {Frey, Brendan J and Patrascu, Relu and Jaakkola, Tommi and Moran, Jodi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f0bbac6fa079f1e00b2c14c1d3c6ccf0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f0bbac6fa079f1e00b2c14c1d3c6ccf0-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f0bbac6fa079f1e00b2c14c1d3c6ccf0-Paper.pdf},
 publisher = {MIT Press},
 title = {Sequentially Fitting \textasciigrave \textasciigrave Inclusive\textquotesingle \textquotesingle Trees for Inference in Noisy-OR Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f0bbac6fa079f1e00b2c14c1d3c6ccf0-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f0fcf351,
 abstract = {It has been shown that the receptive fields of simple cells in VI can be explained by assuming optimal encoding, provided that an extra constraint of sparseness is added. This finding suggests that there is a reason, independent of optimal representation, for sparseness. However this work used an ad hoc model for the noise. Here I show that, if a biologically more plausible noise model, describing neurons as Poisson processes, is used sparseness does not have to be added as a constraint. Thus I conclude that sparseness is not a feature that evolution has striven for, but is simply the result of the evolutionary pressure towards an optimal representation.},
 author = {van Vreeswijk, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f0fcf351df4eb6786e9bb6fc4e2dee02-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f0fcf351df4eb6786e9bb6fc4e2dee02-Metadata.json},
 openalex = {W2294853626},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f0fcf351df4eb6786e9bb6fc4e2dee02-Paper.pdf},
 publisher = {MIT Press},
 title = {Whence Sparseness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f0fcf351df4eb6786e9bb6fc4e2dee02-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f22e4747,
 abstract = {An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA). Our method--called periodic component analysis (πCA)--uses constructive interference to enhance periodic components of the frequency spectrum and destructive interference to cancel noise. The front end emulates important aspects of auditory processing, such as cochlear filtering, nonlinear compression, and insensitivity to phase, with the aim of approaching the robustness of human listeners. The method avoids the inefficiencies of autocorrelation at the pitch period: it does not require long delay lines, and it correlates signals at a clock rate on the order of the actual pitch, as opposed to the original sampling rate. We derive its cost function and present some experimental results.},
 author = {Saul, Lawrence and Allen, Jont},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f22e4747da1aa27e363d86d40ff442fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f22e4747da1aa27e363d86d40ff442fe-Metadata.json},
 openalex = {W2123911240},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf},
 publisher = {MIT Press},
 title = {Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f26dab9b,
 abstract = {This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models.},
 author = {Movellan, Javier and Mineiro, Paul and Williams, Ruth},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f26dab9bf6a137c3b6782e562794c2f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f26dab9bf6a137c3b6782e562794c2f2-Metadata.json},
 openalex = {W2102432043},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f26dab9bf6a137c3b6782e562794c2f2-Paper.pdf},
 publisher = {MIT Press},
 title = {Partially Observable SDE Models for Image Sequence Recognition Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f26dab9bf6a137c3b6782e562794c2f2-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f2d887e0,
 abstract = {We present an algorithm which compensates for the mismatches between characteristics of real-world problems and assumptions of independent component analysis algorithm. To provide additional information to the ICA network, we incorporate top-down selective attention. An MLP classifier is added to the separated signal channel and the error of the classifier is backpropagated to the ICA network. This backpropagation process results in estimation of expected ICA output signal for the top-down attention. Then, the unmixing matrix is retrained according to a new cost function representing the backpropagated error as well as independence. It modifies the density of recovered signals to the density appropriate for classification. For noisy speech signal recorded in real environments, the algorithm improved the recognition performance and showed robustness against parametric changes.},
 author = {Bae, Un-Min and Lee, Soo-Young},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f2d887e01a80e813d9080038decbbabb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f2d887e01a80e813d9080038decbbabb-Metadata.json},
 openalex = {W2141637884},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f2d887e01a80e813d9080038decbbabb-Paper.pdf},
 publisher = {MIT Press},
 title = {Combining ICA and Top-Down Attention for Robust Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f2d887e01a80e813d9080038decbbabb-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f31b2046,
 abstract = {We examine eight different techniques for developing visual representations in machine vision tasks. In particular we compare different versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global methods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance.},
 author = {Gray, Michael and Sejnowski, Terrence J and Movellan, Javier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f31b20466ae89669f9741e047487eb37-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f31b20466ae89669f9741e047487eb37-Metadata.json},
 openalex = {W2151462654},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f31b20466ae89669f9741e047487eb37-Paper.pdf},
 publisher = {MIT Press},
 title = {A Comparison of Image Processing Techniques for Visual Speech Recognition Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f31b20466ae89669f9741e047487eb37-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f39ae9ff,
 abstract = {Learning of a smooth but nonparametric probability density can be regularized using methods of Quantum Field Theory. We implement a field theoretic prior numerically, test its efficacy, and show that the data and the phase space factors arising from the integration over the model space determine the free parameter of the theory (smoothness scale) self-consistently. This persists even for distributions that are atypical in the prior and is a step towards a model-independent theory for learning continuous distributions. Finally, we point out that a wrong parameterization of a model family may sometimes be advantageous for small data sets.},
 author = {Nemenman, Ilya and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f39ae9ff3a81f499230c4126e01f421b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f39ae9ff3a81f499230c4126e01f421b-Metadata.json},
 openalex = {W2962776596},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f39ae9ff3a81f499230c4126e01f421b-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning continuous distributions: Simulations with field theoretic priors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f39ae9ff3a81f499230c4126e01f421b-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f442d33f,
 abstract = {Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold, since Fisher information matrices are singular. In this paper, the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIC, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIC. It is useful for model selection, but not for generalization.},
 author = {Watanabe, Sumio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f442d33fa06832082290ad8544a8da27-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f442d33fa06832082290ad8544a8da27-Metadata.json},
 openalex = {W2133370198},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f442d33fa06832082290ad8544a8da27-Paper.pdf},
 publisher = {MIT Press},
 title = {Algebraic Information Geometry for Learning Machines with Singularities},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f442d33fa06832082290ad8544a8da27-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f45a1078,
 abstract = {We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis.},
 author = {Cohn, David and Hofmann, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f45a1078feb35de77d26b3f7a52ef502-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f45a1078feb35de77d26b3f7a52ef502-Metadata.json},
 openalex = {W2108346334},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f45a1078feb35de77d26b3f7a52ef502-Paper.pdf},
 publisher = {MIT Press},
 title = {The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f45a1078feb35de77d26b3f7a52ef502-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f4a4da9a,
 abstract = {We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.},
 author = {Punyakanok, Vasin and Roth, Dan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f4a4da9aa7eadfd23c7bdb7cf57b3112-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f4a4da9aa7eadfd23c7bdb7cf57b3112-Metadata.json},
 openalex = {W2161290181},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f4a4da9aa7eadfd23c7bdb7cf57b3112-Paper.pdf},
 publisher = {MIT Press},
 title = {The Use of Classifiers in Sequential Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f4a4da9aa7eadfd23c7bdb7cf57b3112-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f542eae1,
 abstract = {Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.},
 author = {Natschl\"{a}ger, Thomas and Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f542eae1949358e25d8bfeefe5b199f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f542eae1949358e25d8bfeefe5b199f1-Metadata.json},
 openalex = {W2115469135},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f542eae1949358e25d8bfeefe5b199f1-Paper.pdf},
 publisher = {MIT Press},
 title = {Finding the Key to a Synapse},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f542eae1949358e25d8bfeefe5b199f1-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f5c3dd75,
 abstract = {In this paper, we propose a new particle filter based on sequential importance sampling. The algorithm uses a bank of unscented filters to obtain the importance proposal distribution. This proposal has two very nice properties. Firstly, it makes efficient use of the latest available information and, secondly, it can have heavy tails. As a result, we find that the algorithm outperforms standard particle filtering and other nonlinear filtering methods very substantially. This experimental finding is in agreement with the theoretical convergence proof for the algorithm. The algorithm also includes resampling and (possibly) Markov chain Monte Carlo (MCMC) steps.},
 author = {van der Merwe, Rudolph and Doucet, Arnaud and de Freitas, Nando and Wan, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f5c3dd7514bf620a1b85450d2ae374b1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f5c3dd7514bf620a1b85450d2ae374b1-Metadata.json},
 openalex = {W2124156864},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f5c3dd7514bf620a1b85450d2ae374b1-Paper.pdf},
 publisher = {MIT Press},
 title = {The Unscented Particle Filter},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f5c3dd7514bf620a1b85450d2ae374b1-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_f9d11525,
 abstract = {Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the Expectation-Maximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.},
 author = {Lee, Daniel and Seung, H. Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Metadata.json},
 openalex = {W2135029798},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf},
 publisher = {MIT Press},
 title = {Algorithms for Non-negative Matrix Factorization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f9d1152547c0bde01830b7e8bd60024c-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_faacbcd5,
 abstract = {We describe an analogy between psychophysically measured effects in contrast masking, and the behavior of a simple integrate-and-fire neuron that receives time-modulated inhibition. In the psychophysical experiments, we tested observers ability to discriminate contrasts of peripheral Gabor patches in the presence of collinear Gabor flankers. The data reveal a complex interaction pattern that we account for by assuming that flankers provide divisive inhibition to the target unit for low target contrasts, but provide subtractive inhibition to the target unit for higher target contrasts. A similar switch from divisive to subtractive inhibition is observed in an integrate-and-fire unit that receives inhibition modulated in time such that the cell spends part of the time in a high-inhibition state and part of the time in a low-inhibition state. The similarity between the effects suggests that one may cause the other. The biophysical model makes testable predictions for physiological single-cell recordings.},
 author = {Zenger, Barbara and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/faacbcd5bf1d018912c116bf2783e9a1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/faacbcd5bf1d018912c116bf2783e9a1-Metadata.json},
 openalex = {W2151518221},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/faacbcd5bf1d018912c116bf2783e9a1-Paper.pdf},
 publisher = {MIT Press},
 title = {Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/faacbcd5bf1d018912c116bf2783e9a1-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_fb8feff2,
 abstract = {A new form of covariance modelling for Gaussian mixture models and hidden Markov models is presented. This is an extension to an efficient form of covariance modelling used in speech recognition, semi-tied covariance matrices. In the standard form of semi-tied covariance matrices the covariance matrix is decomposed into a highly shared decorrelating transform and a component-specific diagonal covariance matrix. The use of a factored decorrelating transform is presented in this paper. This factoring effectively increases the number of possible transforms without increasing the number of free parameters. Maximum likelihood estimation schemes for all the model parameters are presented including the component/ transform assignment, transform and component parameters. This new model form is evaluated on a large vocabulary speech recognition task. It is shown that using this factored form of covariance modelling reduces the word error rate.},
 author = {Gales, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/fb8feff253bb6c834deb61ec76baa893-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/fb8feff253bb6c834deb61ec76baa893-Metadata.json},
 openalex = {W2168958822},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/fb8feff253bb6c834deb61ec76baa893-Paper.pdf},
 publisher = {MIT Press},
 title = {Factored Semi-Tied Covariance Matrices},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/fb8feff253bb6c834deb61ec76baa893-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_fc4ddc15,
 abstract = {A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency channels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which represent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppression is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speaker-independent digit recognition experiments and compared to noise suppression by Spectral Subtraction.},
 author = {Tchorz, J\"{u}rgen and Kleinschmidt, Michael and Kollmeier, Birger},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/fc4ddc15f9f4b4b06ef7844d6bb53abf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/fc4ddc15f9f4b4b06ef7844d6bb53abf-Metadata.json},
 openalex = {W2167423898},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/fc4ddc15f9f4b4b06ef7844d6bb53abf-Paper.pdf},
 publisher = {MIT Press},
 title = {Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/fc4ddc15f9f4b4b06ef7844d6bb53abf-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_ff1418e8,
 abstract = {Neurons in area V4 have relatively large receptive fields (RFs), so multiple visual features are simultaneously seen by these cells. Recordings from single V4 neurons suggest that simultaneously presented stimuli compete to set the output firing rate, and that attention acts to isolate individual features by biasing the competition in favor of the attended object. We propose that both stimulus competition and attentional biasing arise from the spatial segregation of afferent synapses onto different regions of the excitable dendritic tree of V4 neurons. The pattern of feedforward, stimulus-driven inputs follows from a Hebbian rule: excitatory afferents with similar RFs tend to group together on the dendritic tree, avoiding randomly located inhibitory inputs with similar RFs. The same principle guides the formation of inputs that mediate attentional modulation. Using both biophysically detailed compartmental models and simplified models of computation in single neurons, we demonstrate that such an architecture could account for the response properties and attentional modulation of V4 neurons. Our results suggest an important role for nonlinear dendritic conductances in extrastriate cortical processing.},
 author = {Archie, Kevin and Mel, Bartlett},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Metadata.json},
 openalex = {W2147002520},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Paper.pdf},
 publisher = {MIT Press},
 title = {Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/ff1418e8cc993fe8abcfe3ce2003e5c5-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_ff7d0f52,
 abstract = {The smart vision chip has a large potential for application in general purpose high speed image processing systems. In order to fabricate smart vision chips including photo detector compactly, we have proposed the application of three dimensional LSI technology for smart vision chips. Three dimensional technology has great potential to realize new neuromorphic systems inspired by not only the biological function but also the biological structure. In this paper, we describe our three dimensional LSI technology for neuromorphic circuits and the design of smart vision chips.},
 author = {Kurino, Hiroyuki and Nakagawa, M. and Lee, Kang and Nakamura, Tomonori and Yamada, Yuusuke and Park, Ki and Koyanagi, Mitsumasa},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ff7d0f525b3be596a51fb919492c099c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ff7d0f525b3be596a51fb919492c099c-Metadata.json},
 openalex = {W2111067929},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/ff7d0f525b3be596a51fb919492c099c-Paper.pdf},
 publisher = {MIT Press},
 title = {Smart Vision Chip Fabricated Using Three Dimensional Integration Technology},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/ff7d0f525b3be596a51fb919492c099c-Abstract.html},
 volume = {13},
 year = {2000}
}

@inproceedings{NIPS2000_fface838,
 abstract = {We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common middle-scale parts, represented as image fragments. The model addresses the same concerns as previous work on compositional representation through the use of what+where receptive fields and attentional gain modulation. It does not require prior exposure to the individual parts, and avoids the need for abstract symbolic binding.},
 author = {Edelman, Shimon and Intrator, Nathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2000/file/fface8385abbf94b4593a0ed53a0c70f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2000/file/fface8385abbf94b4593a0ed53a0c70f-Metadata.json},
 openalex = {W2144712082},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2000/file/fface8385abbf94b4593a0ed53a0c70f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Productive, Systematic Framework for the Representation of Visual Structure},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/fface8385abbf94b4593a0ed53a0c70f-Abstract.html},
 volume = {13},
 year = {2000}
}
