@inproceedings{NIPS1991_01f78be6,
 author = {Fanty, Mark and Cole, Ronald and Roginski, Krist},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/01f78be6f7cad02658508fe4616098a9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/01f78be6f7cad02658508fe4616098a9-Metadata.json},
 openalex = {W2407582742},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/01f78be6f7cad02658508fe4616098a9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {English alphabet recognition with telephone speech},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/01f78be6f7cad02658508fe4616098a9-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_0353ab4c,
 abstract = {This paper describes an approach, called centered object integrated segmentation and recognition (COISR), for integrating object segmentation and recognition within a single neural network. The application is hand-printed character recognition. Two versions of the system are described. One uses a backpropagation network that scans exhaustively over a field of characters and is trained to recognize whether it is centered over a single character or between characters. When it is centered over a character, the net classifies the character. The approach is tested on a dataset of hand-printed digits. Very low error rates are reported. The second version, COISR-SACCADE, avoids the need for exhaustive scans. The net is trained as before, but also is trained to compute ballistic 'eye' movements that enable the input window to jump from one character to the next.},
 author = {Martin, Gale L. and Rashid, Mosfeq},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/0353ab4cbed5beae847a7ff6e220b5cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/0353ab4cbed5beae847a7ff6e220b5cf-Metadata.json},
 openalex = {W2155153031},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/0353ab4cbed5beae847a7ff6e220b5cf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Recognizing Overlapping Hand-Printed Characters by Centered-Object Integrated Segmentation and Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/0353ab4cbed5beae847a7ff6e220b5cf-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_05f971b5,
 author = {Nowlan, Steven and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/05f971b5ec196b8c65b75d2ef8267331-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/05f971b5ec196b8c65b75d2ef8267331-Metadata.json},
 openalex = {W2171170455},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/05f971b5ec196b8c65b75d2ef8267331-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive Soft Weight Tying using Gaussian Mixtures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/05f971b5ec196b8c65b75d2ef8267331-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_069d3bb0,
 abstract = {We present the Multi-State Time Delay Neural Network (MS-TDNN) as an extension of the TDNN to robust word recognition. Unlike most other hybrid methods, the MS-TDNN embeds an alignment search procedure into the connectionist architecture, and allows for word level supervision. The resulting system has the ability to manage the sequential order of subword units, while optimizing for the recognizer performance. In this paper we present extensive new evaluations of this approach over speaker-dependent and speaker-independent connected alphabet.},
 author = {Haffner, Patrick and Waibel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/069d3bb002acd8d7dd095917f9efe4cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/069d3bb002acd8d7dd095917f9efe4cb-Metadata.json},
 openalex = {W2149814369},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/069d3bb002acd8d7dd095917f9efe4cb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Multi-State Time Delay Networks for Continuous Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/069d3bb002acd8d7dd095917f9efe4cb-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_07563a3f,
 abstract = {The subject of this paper is the integration of multi-layered Artificial Neural Networks (ANN) with probability density functions such as Gaussian mixtures found in continuous density Hidden Markov Models (HMM). In the first part of this paper we present an ANN/HMM hybrid in which all the parameters of the system are simultaneously optimized with respect to a single criterion. In the second part of this paper, we study the relationship between the density of the inputs of the network and the density of the outputs of the networks. A few experiments are presented to explore how to perform density estimation with ANNs.},
 author = {Bengio, Yoshua and De Mori, Renato and Flammia, Giovanni and Kompe, Ralf},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/07563a3fe3bbe7e3ba84431ad9d055af-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/07563a3fe3bbe7e3ba84431ad9d055af-Metadata.json},
 openalex = {W2126514931},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network - Gaussian Mixture Hybrid for Speech Recognition or Density Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_08740852,
 abstract = {This paper applies the theory of Probably Approximately Correct (PAC) learning to multiple output feedforward threshold networks in which the weights conform to certain equivalences. It is shown that the sample size for reliable learning can be bounded above by a formula similar to that required for single output networks with no equivalences. The best previously obtained bounds are improved for all cases.},
 author = {Shawe-Taylor, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/087408522c31eeb1f982bc0eaf81d35f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/087408522c31eeb1f982bc0eaf81d35f-Metadata.json},
 openalex = {W2160999252},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/087408522c31eeb1f982bc0eaf81d35f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Threshold Network Learning in the Presence of Equivalences},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/087408522c31eeb1f982bc0eaf81d35f-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_0fcbc61a,
 author = {Wynne-Jones, Mike},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/0fcbc61acd0479dc77e3cccc0f5ffca7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/0fcbc61acd0479dc77e3cccc0f5ffca7-Metadata.json},
 openalex = {W2114747792},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/0fcbc61acd0479dc77e3cccc0f5ffca7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Node splitting: A constructive algorithm for feed-forward neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/0fcbc61acd0479dc77e3cccc0f5ffca7-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_10a7cdd9,
 abstract = {The method of Structural Risk Minimization refers to tuning the capacity of the classifier to the available amount of training data. This capacity is influenced by several factors, including: (1) properties of the input space, (2) nature and structure of the classifier, and (3) learning algorithm. Actions based on these three factors are combined here to control the capacity of linear classifiers and improve generalization on the problem of handwritten digit recognition.},
 author = {Guyon, I. and Vapnik, V. and Boser, B. and Bottou, L. and Solla, S. A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Metadata.json},
 openalex = {W2165217207},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Structural Risk Minimization for Character Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/10a7cdd970fe135cf4f7bb55c0e3b59f-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_11b921ef,
 abstract = {Five experiments were performed using several neural network architectures to identify the location of a wave in the time ordered graphical results from a medical test. Baseline results from the first experiment found correct identification of the target wave in 85% of cases (n=20). Other experiments investigated the effect of different architectures and preprocessing the raw data on the results. The methods used seem most appropriate for time oriented graphical data which has a clear starting point such as electrophoresis Or spectrometry rather than continuous tests such as ECGs and EEGs.},
 author = {Freeman, Donald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/11b921ef080f7736089c757404650e40-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/11b921ef080f7736089c757404650e40-Metadata.json},
 openalex = {W2165596939},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/11b921ef080f7736089c757404650e40-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Computer Recognition of Wave Location in Graphical Data by a Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/11b921ef080f7736089c757404650e40-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_13f320e7,
 abstract = {A high performance speaker-independent isolated-word hybrid speech recognizer was developed which combines Hidden Markov Models (HMMs) and Radial Basis Function (RBF) neural networks. In recognition experiments using a speaker-independent E-set database, the hybrid recognizer had an error rate of 11.5% compared to 15.7% for the robust unimodal Gaussian HMM recognizer upon which the hybrid system was based. These results and additional experiments demonstrate that RBF networks can be successfully incorporated in hybrid recognizers and suggest that they may be capable of good performance with fewer parameters than required by Gaussian mixture classifiers. A global parameter optimization method designed to minimize the overall word error rather than the frame recognition error failed to reduce the error rate.},
 author = {Singer, Elliot and Lippmann, Richard P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/13f320e7b5ead1024ac95c3b208610db-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/13f320e7b5ead1024ac95c3b208610db-Metadata.json},
 openalex = {W2118861666},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/13f320e7b5ead1024ac95c3b208610db-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Improved Hidden Markov Model Speech Recognition Using Radial Basis Function Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/13f320e7b5ead1024ac95c3b208610db-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_13f3cf8c,
 abstract = {Issues relating to the estimation of hidden Markov model (HMM) local probabilities are discussed. In particular we note the isomorphism of radial basis functions (RBF) networks to tied mixture density modelling; additionally we highlight the differences between these methods arising from the different training criteria employed. We present a method in which connectionist training can be modified to resolve these differences and discuss some preliminary experiments. Finally, we discuss some outstanding problems with discriminative training.},
 author = {Renals, Steve and Morgan, Nelson and Bourlard, Herv\'{e} and Franco, Horacio and Cohen, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/13f3cf8c531952d72e5847c4183e6910-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/13f3cf8c531952d72e5847c4183e6910-Metadata.json},
 openalex = {W2103434757},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connectionist Optimisation of Tied Mixture Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/13f3cf8c531952d72e5847c4183e6910-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_15d4e891,
 abstract = {The notion of generalization ability can be defined precisely as the prediction risk, the expected performance of an estimator in predicting new observations. In this paper, we propose the prediction risk as a measure of the generalization ability of multi-layer perceptron networks and use it to select an optimal network architecture from a set of possible architectures. We also propose a heuristic search strategy to explore the space of possible architectures. The prediction risk is estimated from the available data; here we estimate the prediction risk by v-fold cross-validation and by asymptotic approximations of generalized cross-validation or Akaike's final prediction error. We apply the technique to the problem of predicting corporate bond ratings. This problem is very attractive as a case study, since it is characterized by the limited availability of the data and by the lack of a complete a priori model which could be used to impose a structure to the network architecture.},
 author = {Moody, John and Utans, Joachim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/15d4e891d784977cacbfcbb00c48f133-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/15d4e891d784977cacbfcbb00c48f133-Metadata.json},
 openalex = {W2113668251},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Principled Architecture Selection for Neural Networks: Application to Corporate Bond Rating Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/15d4e891d784977cacbfcbb00c48f133-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_15de21c6,
 abstract = {Simple second-order recurrent networks are shown to readily learn small known regular grammars when trained with positive and negative strings examples. We show that similar methods are appropriate for learning unknown grammars from examples of their strings. The training algorithm is an incremental real-time, recurrent learning (RTRL) method that computes the complete gradient and updates the weights at the end of each string. After or during training, a dynamic clustering algorithm extracts the production rules that the neural network has learned. The methods are illustrated by extracting rules from unknown deterministic regular grammars. For many cases the extracted grammar outperforms the neural net from which it was extracted in correctly classifying unseen strings.},
 author = {Giles, C. L. and Miller, C. B. and Chen, D. and Sun, G. Z. and Chen, H. H. and Lee, Y. C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/15de21c670ae7c3f6f3f1f37029303c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/15de21c670ae7c3f6f3f1f37029303c9-Metadata.json},
 openalex = {W2167615167},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/15de21c670ae7c3f6f3f1f37029303c9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Extracting and Learning an Unknown Grammar with Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/15de21c670ae7c3f6f3f1f37029303c9-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_16c222aa,
 abstract = {Single nerve cells with static properties have traditionally been viewed as the building blocks for networks that show emergent phenomena. In contrast to this approach, we study here how the overall network activity can control single cell parameters such as input resistance, as well as time and space constants, parameters that are crucial for excitability and spatio-temporal integration. Using detailed computer simulations of neocortical pyramidal cells, we show that the spontaneous background firing of the network provides a means for setting these parameters. The mechanism for this control is through the large conductance change of the membrane that is induced by both non-NMDA and NMDA excitatory and inhibitory synapses activated by the spontaneous background activity.},
 author = {Bernander, \"{O}jvind and Koch, Christof and Douglas, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/16c222aa19898e5058938167c8ab6c57-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/16c222aa19898e5058938167c8ab6c57-Metadata.json},
 openalex = {W2161642883},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/16c222aa19898e5058938167c8ab6c57-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Network activity determines spatio-temporal integration in single cells},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/16c222aa19898e5058938167c8ab6c57-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_1728efbd,
 abstract = {We developed a neural net architecture for segmenting complex images, i.e., to localize two-dimensional geometrical shapes in a scene, without prior knowledge of the objects' positions and sizes. A scale variation is built into the network to deal with varying sizes. This algorithm has been applied to video images of railroad cars, to find their identification numbers. Over 95% of the characters were located correctly in a data base of 300 images, despite a large variation in lighting conditions and often a poor quality of the characters. A part of the network is executed on a processor board containing an analog neural net chip (Graf et al. 1991), while the rest is implemented as a software model on a workstation or a digital signal processor.},
 author = {Graf, Hans and Nohl, Craig and Ben, Jan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1728efbda81692282ba642aafd57be3a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1728efbda81692282ba642aafd57be3a-Metadata.json},
 openalex = {W2172252700},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1728efbda81692282ba642aafd57be3a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Image Segmentation with Networks of Variable Scales},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/1728efbda81692282ba642aafd57be3a-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_1a5b1e4d,
 abstract = {We have investigated the properties of neurons in inferior temporal (IT) cortex in monkeys performing a pattern matching task. Simple back-propagation networks were trained to discriminate the various stimulus conditions on the basis of the measured neuronal signal. We also trained networks to predict the neuronal response waveforms from the spatial patterns of the stimuli. The results indicate that IT neurons convey temporally encoded information about both current and remembered patterns, as well as about their behavioral context.},
 author = {Eskandar, Emad and Richmond, Barry and Hertz, John and Optican, Lance and Kj\ae r, Troels},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1a5b1e4daae265b790965a275b53ae50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1a5b1e4daae265b790965a275b53ae50-Metadata.json},
 openalex = {W2148496688},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1a5b1e4daae265b790965a275b53ae50-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Decoding of Neuronal Signals in Visual Pattern Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/1a5b1e4daae265b790965a275b53ae50-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_1bb91f73,
 abstract = {We present experimental data from an analog CMOS LSI chip that implements the Herault-Jutten adaptive neural network. Testing procedures and results in time and frequency-domain are described. These include weight convergence trajectories, extraction of a signal in noise, and separation of statistically complex signals such as speech.},
 author = {Cohen, Marc and Pouliquen, Philippe and Andreou, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1bb91f73e9d31ea2830a5e73ce3ed328-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1bb91f73e9d31ea2830a5e73ce3ed328-Metadata.json},
 openalex = {W2107641916},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1bb91f73e9d31ea2830a5e73ce3ed328-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analog LSI Implementation of an Auto-Adaptive Network for Real-Time Separation of Independent Signals},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/1bb91f73e9d31ea2830a5e73ce3ed328-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_1be3bc32,
 abstract = {This paper is concerned with the problem of learning in networks where some or all of the functions involved are not smooth. Examples of such networks are those whose neural transfer functions are piecewise-linear and those whose error function is defined in terms of the l∞ norm.

Up to now, networks whose neural transfer functions are piecewise-linear have received very little consideration in the literature, but the possibility of using an error function defined in terms of the l∞ norm has received some attention. In this latter work, however, the problems that can occur when gradient methods are used for non smooth error functions have not been addressed.

In this paper we draw upon some recent results from the field of nonsmooth optimization (NSO) to present an algorithm for the non smooth case. Our motivation for this work arose out of the fact that we have been able to show that, in backpropagation, an error function based upon the l∞ norm overcomes the difficulties which can occur when using the l2 norm.},
 author = {Redding, Nicholas and Downs, T.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1be3bc32e6564055d5ca3e5a354acbef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1be3bc32e6564055d5ca3e5a354acbef-Metadata.json},
 openalex = {W2104774461},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning in Feedforward Networks with Nonsmooth Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/1be3bc32e6564055d5ca3e5a354acbef-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_2050e03c,
 abstract = {This paper considers the problem of expressing predicate calculus in connectionist networks that are based on energy minimization. Given a first-order-logic knowledge base and a bound k, a symmetric network is constructed (like a Boltzman machine or a Hopfield network) that searches for a proof for a given query. If a resolution-based proof of length no longer than k exists, then the global minima of the energy function that is associated with the network represent such proofs. The network that is generated is of size cubic in the bound k and linear in the knowledge size. There are no restrictions on the type of logic formulas that can be represented. The network is inherently fault tolerant and can cope with inconsistency and nonmonotonicity.},
 author = {Pinkus, Gadi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2050e03ca119580f74cca14cc6e97462-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2050e03ca119580f74cca14cc6e97462-Metadata.json},
 openalex = {W2101159598},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2050e03ca119580f74cca14cc6e97462-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Constructing Proofs in Symmetric Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/2050e03ca119580f74cca14cc6e97462-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_218a0aef,
 abstract = {In this work, the modification of the probabilistic neural network (PNN) is proposed. The traditional network is adjusted by introducing the weight coefficients between pattern and summation layer. The weights are derived using the sensitivity analysis (SA) procedure. The performance of the weighted PNN (WPNN) is examined in data classification problems on benchmark data sets. The obtained WPNN's efficiency results are compared with these achieved by a modified PNN model put forward in literature, the original PNN and selected state-of-the-art classification algorithms: support vector machine, multilayer perceptron, radial basis function neural network, k-nearest neighbor method and gene expression programming algorithm. All classifiers are collated by computing the prediction accuracy obtained with the use of a k-fold cross validation procedure. It is shown that in seven out of ten classification cases, WPNN outperforms both the weighted PNN classifier introduced in literature and the original model. Furthermore, according to the ranking statistics, the proposed WPNN takes the first place among all tested algorithms.},
 author = {Montana, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/218a0aefd1d1a4be65601cc6ddc1520e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/218a0aefd1d1a4be65601cc6ddc1520e-Metadata.json},
 openalex = {W2770357778},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Weighted probabilistic neural network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/218a0aefd1d1a4be65601cc6ddc1520e-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_250cf8b5,
 abstract = {We compare two strategies for training connectionist (as well as non-connectionist) models for statistical pattern recognition. The probabilistic strategy is based on the notion that Bayesian discrimination (i.e., optimal classification) is achieved when the classifier learns the a posteriori class distributions of the random feature vector. The differential strategy is based on the notion that the identity of the largest class a posteriori probability of the feature vector is all that is needed to achieve Bayesian discrimination. Each strategy is directly linked to a family of objective functions that can be used in the supervised training procedure. We prove that the probabilistic strategy - linked with error measure objective functions such as mean-squared-error and cross-entropy - typically used to train classifiers necessarily requires larger training sets and more complex classifier architectures than those needed to approximate the Bayesian discriminant function. In contrast. we prove that the differential strategy - linked with classificationfigure-of-merit objective functions (CFMmono) [3] - requires the minimum classifier functional complexity and the fewest training examples necessary to approximate the Bayesian discriminant function with specified precision (measured in probability of error). We present our proofs in the context of a game of chance in which an unfair C-sided die is tossed repeatedly. We show that this rigged game of dice is a paradigm at the root of all statistical pattern recognition tasks, and demonstrate how a simple extension of the concept leads us to a general information-theoretic model of sample complexity for statistical pattern recognition.},
 author = {Hampshire II, J. B. and Kumar, B.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/250cf8b51c773f3f8dc8b4be867a9a02-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/250cf8b51c773f3f8dc8b4be867a9a02-Metadata.json},
 openalex = {W2165831782},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Shooting Craps in Search of an Optimal Strategy for Training Connectionist Pattern Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_258be18e,
 abstract = {This paper deals with an application of Neural Networks to satellite remote sensing observations. Because of the complexity of the application and the large amount of data, the problem cannot be solved by using a single method. The solution we propose is to build multimodules NN architectures where several NN cooperate together. Such system suffer from generic problem for whom we propose solutions. They allow to reach accurate performances for multi-valued function approximations and probability estimations. The results are compared with six other methods which have been used for this problem. We show that the methodology we have developed is general and can be used for a large variety of applications.},
 author = {Thiria, Sylvie and Mejia, Carlos and Badran, Fouad and Cr\'{e}pon, Michel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/258be18e31c8188555c2ff05b4d542c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/258be18e31c8188555c2ff05b4d542c3-Metadata.json},
 openalex = {W2113872785},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/258be18e31c8188555c2ff05b4d542c3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Multimodular Architecture for Remote Sensing Operations.},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/258be18e31c8188555c2ff05b4d542c3-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_25ddc0f8,
 abstract = {Vestibular compensation is the process whereby normal functioning is regained following destruction of one member of the pair of peripheral vestibular receptors. Compensation was simulated by lesioning a dynamic neural network model of the vestibulo-ocular reflex (VOR) and retraining it using recurrent back-propagation. The model reproduced the pattern of VOR neuron activity experimentally observed in compensated animals, but only if connections heretofore considered uninvolved were allowed to be plastic. Because the model incorporated nonlinear units, it was able to reconcile previously conflicting, linear analyses of experimental results on the dynamic properties of VOR neurons in normal and compensated animals.},
 author = {Anastasio, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Metadata.json},
 openalex = {W2152613550},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning in the Vestibular System: Simulations of Vestibular Compensation Using Recurrent Back-Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/25ddc0f8c9d3e22e03d3076f98d83cb2-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_26337353,
 abstract = {An alternative to the typical technique of selecting training examples independently from a fixed distribution is formulated and analyzed, in which the current example is presented repeatedly until the error for that item is reduced to some criterion value, β; then, another item is randomly selected. The convergence time can be dramatically increased or decreased by this heuristic, depending on the task, and is very sensitive to the value of β.},
 author = {Munro, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/26337353b7962f533d78c762373b3318-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/26337353b7962f533d78c762373b3318-Metadata.json},
 openalex = {W2136725480},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/26337353b7962f533d78c762373b3318-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Repeat Until Bored: A Pattern Selection Strategy},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/26337353b7962f533d78c762373b3318-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_285e19f2,
 abstract = {The KBANN (Knowledge-Based Artificial Neural Networks) approach uses neural networks to refine knowledge that can be written in the form of simple propositional rules. We extend this idea further by presenting the MANNCON (Multivariable Artificial Neural Network Control) algorithm by which the mathematical equations governing a PID (Proportional-Integral-Derivative) controller determine the topology and initial weights of a network, which is further trained using backpropagation. We apply this method to the task of controlling the outflow and temperature of a water tank, producing statistically significant gains in accuracy over both a standard neural network approach and a nonlearning PID controller. Furthermore, using the PID knowledge to initialize the weights of the network produces statistically less variation in test set accuracy when compared to networks initialized with small random numbers.},
 author = {Scott, Gary and Shavlik, Jude and Ray, W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/285e19f20beded7d215102b49d5c09a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/285e19f20beded7d215102b49d5c09a0-Metadata.json},
 openalex = {W2050414686},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/285e19f20beded7d215102b49d5c09a0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Refining PID Controllers Using Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/285e19f20beded7d215102b49d5c09a0-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_298f95e1,
 abstract = {A neurophysiologically-based model is presented that controls a simulated kinematic arm during goal-directed reaches. The network generates a quasi-feedforward motor command that is learned using training signals generated by corrective movements. For each target, the network selects and sets the output of a subset of pattern generators. During the movement, feedback from proprioceptors turns off the pattern generators. The task facing individual pattern generators is to recognize when the arm reaches the target and to turn off. A distributed representation of the motor command that resembles population vectors seen in vivo was produced naturally by these simulations.},
 author = {Berthier, N. and Singh, S. P. and Barto, A. G. and Houk, J. C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/298f95e1bf9136124592c8d4825a06fc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/298f95e1bf9136124592c8d4825a06fc-Metadata.json},
 openalex = {W2122608718},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/298f95e1bf9136124592c8d4825a06fc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Cortico-Cerebellar Model that Learns to Generate Distributed Motor Commands to Control a Kinematic Arm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/298f95e1bf9136124592c8d4825a06fc-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_2b8a6159,
 abstract = {Visual object recognition involves the identification of images of 3-D objects seen from arbitrary viewpoints. We suggest an approach to object recognition in which a view is represented as a collection of points given by their location in the image. An object is modeled by a set of 2-D views together with the correspondence between the views. We show that any novel view of the object can be expressed as a linear combination of the stored views. Consequently, we build a linear operator that distinguishes between views of a specific object and views of other objects. This operator can be implemented using neural network architectures with relatively simple structures.},
 author = {Basri, Ronen and Ullman, Shimon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2b8a61594b1f4c4db0902a8a395ced93-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2b8a61594b1f4c4db0902a8a395ced93-Metadata.json},
 openalex = {W2124712368},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Linear Operator for Object Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/2b8a61594b1f4c4db0902a8a395ced93-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_2bb232c0,
 abstract = {Do you want your neural net algorithm to learn sequences? Do not limit yourself to conventional gradient descent (or approximations thereof). Instead, use your sequence learning algorithm (any will do) to implement the following method for history compression. No matter what your final goals are, train a network to predict its next input from the previous ones. Since only unpredictable inputs convey new information, ignore all predictable inputs but let all unexpected inputs (plus information about the time step at which they occurred) become inputs to a higher-level network of the same kind (working on a slower, self-adjusting time scale). Go on building a hierarchy of such networks. This principle reduces the descriptions of event sequences without loss of information, thus easing supervised or reinforcement learning tasks. Alternatively, you may use two recurrent networks to collapse a multi-level predictor hierarchy into a single recurrent net. Experiments show that systems based on these principles can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets. Finally you can modify the above method such that predictability is not defined in a yes-or-no fashion but in a continuous fashion.},
 author = {Schmidhuber, J\"{u}rgen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2bb232c0b13c774965ef8558f0fbd615-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2bb232c0b13c774965ef8558f0fbd615-Metadata.json},
 openalex = {W2118127772},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2bb232c0b13c774965ef8558f0fbd615-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Unambiguous Reduced Sequence Descriptions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/2bb232c0b13c774965ef8558f0fbd615-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_2d6cc4b2,
 abstract = {Winner-Take-All (WTA) networks, in which inhibitory interconnections are used to determine the most highly-activated of a pool of units, are an important part of many neural network models. Unfortunately, convergence of normal WTA networks is extremely sensitive to the magnitudes of their weights, which must be hand-tuned and which generally only provide the right amount of inhibition across a relatively small range of initial conditions. This paper presents Dynamically Adaptive Winner-Teke-All (DAWTA) networks, which use a regulatory unit to provide the competitive inhibition to the units in the network. The DAWTA regulatory unit dynamically adjusts its level of activation during competition to provide the right amount of inhibition to differentiate between competitors and drive a single winner. This dynamic adaptation allows DAWTA networks to perform the winner-take-all function for nearly any network size or initial condition. using O(N) connections. In addition, the DAWTA regulatory unit can be biased to find the level of inhibition necessary to settle upon the K most highly-activated units, and therefore serve as a K-Winners-Take-All network.},
 author = {Lange, Trent},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Metadata.json},
 openalex = {W2168328168},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dynamically-Adaptive Winner-Take-All Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/2d6cc4b2d139a53512fb8cbb3086ae2e-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_2f55707d,
 abstract = {This paper will address an important question in machine learning: What kind of network architectures work better on what kind of problems? A projection pursuit learning network has a very similar structure to a one hidden layer sigmoidal neural network. A general method based on a continuous version of projection pursuit regression is developed to show that projection pursuit regression works better on angular smooth functions than on Laplacian smooth functions. There exists a ridge function approximation scheme to avoid the curse of dimensionality for approximating functions in L2(θd).},
 author = {Zhao, Ying and Atkeson, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2f55707d4193dc27118a0f19a1985716-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2f55707d4193dc27118a0f19a1985716-Metadata.json},
 openalex = {W2134550823},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/2f55707d4193dc27118a0f19a1985716-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Some Approximation Properties of Projection Pursuit Learning Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/2f55707d4193dc27118a0f19a1985716-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_33e8075e,
 abstract = {We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model can be represented by a particular type of Boltzmann machine with a bipartite graph structure that we call the combination machine. This machine is closely related to the Harmonium model defined by Smolensky. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters of the model. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images.},
 author = {Freund, Yoav and Haussler, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/33e8075e9970de0cfea955afd4644bb2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/33e8075e9970de0cfea955afd4644bb2-Metadata.json},
 openalex = {W2165225968},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Unsupervised learning of distributions on binary vectors using two layer networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_35051070,
 abstract = {Best-first model merging is a general technique for dynamically choosing the structure of a neural or related architecture while avoiding overfitting. It is applicable to both learning and recognition tasks and often generalizes significantly better than fixed structures. We demonstrate the approach applied to the tasks of choosing radial basis functions for function learning, choosing local affine models for curve and constraint surface modelling, and choosing the structure of a balltree or bumptree to maximize efficiency of access.},
 author = {Omohundro, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/35051070e572e47d2c26c241ab88307f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/35051070e572e47d2c26c241ab88307f-Metadata.json},
 openalex = {W2096498841},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/35051070e572e47d2c26c241ab88307f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Best-First Model Merging for Dynamic Learning and Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/35051070e572e47d2c26c241ab88307f-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_37f0e884,
 abstract = {An important issue in neural computation is the dynamic range of weights in the neural networks. Many experimental results on learning indicate that the weights in the networks can grow prohibitively large with the size of the inputs. Here we address this issue by studying the tradeoffs between the depth and the size of weights in polynomial-size networks of linear threshold elements (LTEs). We show that there is an efficient way of simulating a network of LTEs with large weights by a network of LTEs with small weights. In particular, we prove that every depth-d, polynomial-size network of LTEs with exponentially large integer weights can be simulated by a depth-(2d + 1), polynomial-size network of LTEs with polynomially bounded integer weights. To prove these results, we use tools from harmonic analysis of Boolean functions. Our technique is quite general, it provides insights to some other problems. For example, we are able to improve the best known results on the depth of a network of linear threshold elements that computes the COMPARISON, SUM and PRODUCT of two n-bits numbers, and the MAXIMUM and the SORTING of n n-bit numbers.},
 author = {Siu, Kai-Yeung and Bruck, Jehoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/37f0e884fbad9667e38940169d0a3c95-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/37f0e884fbad9667e38940169d0a3c95-Metadata.json},
 openalex = {W2095969291},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/37f0e884fbad9667e38940169d0a3c95-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Computing with Small Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/37f0e884fbad9667e38940169d0a3c95-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_38913e1d,
 abstract = {We have developed a four-language automatic language identification system for high-quality speech. The system uses a neural network-based segmentation algorithm to segment speech into seven broad phonetic categories. Phonetic and prosodic features computed on these categories are then input to a second network that performs the language classification. The system was trained and tested on separate sets of speakers of American English, Japanese, Mandarin Chinese and Tamil. It currently performs with an accuracy of 89.5% on the utterances of the test set.},
 author = {Muthusamy, Yeshwant and Cole, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/38913e1d6a7b94cb0f55994f679f5956-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/38913e1d6a7b94cb0f55994f679f5956-Metadata.json},
 openalex = {W2164787385},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/38913e1d6a7b94cb0f55994f679f5956-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Segment-Based Automatic Language Identification System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/38913e1d6a7b94cb0f55994f679f5956-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_389bc7bb,
 abstract = {Optimizing the performance of self-organizing feature maps like the Kohonen map involves the choice of the output space topology. We present a topographic product which measures the preservation of neighborhood relations as a criterion to optimize the output space topology of the map with regard to the global dimensionality DA as well as to the dimensions in the individual directions. We test the topographic product method not only on synthetic mapping examples, but also on speech data. In the latter application our method suggests an output space dimensionality of DA = 3, in coincidence with recent recognition results on the same data set.},
 author = {Bauer, Hans-Ulrich and Pawelzik, Klaus and Geisel, Theo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/389bc7bb1e1c2a5e7e147703232a88f6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/389bc7bb1e1c2a5e7e147703232a88f6-Metadata.json},
 openalex = {W2104037735},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Topographic Product for the Optimization of Self-Organizing Feature Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/389bc7bb1e1c2a5e7e147703232a88f6-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_3a077244,
 abstract = {Learning a map from an input set to an output set is similar to the problem of reconstructing hypersurfaces from sparse data (Poggio and Girosi, 1990). In this framework, we discuss the problem of automatically selecting minimal surface data. The objective is to be able to approximately reconstruct the surface from the selected sparse data. We show that this problem is equivalent to the one of compressing information by data removal and the one of learning how to teach. Our key step is to introduce a process that statistically selects the data according to the model. During the process of data selection (learning how to teach) our system (teacher) is capable of predicting the new surface, the approximated one provided by the selected data. We concentrate on piecewise smooth surfaces, e.g. images, and use mean field techniques to obtain a deterministic network that is shown to compress image data.},
 author = {Geiger, Davi and Pereira, Ricardo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/3a0772443a0739141292a5429b952fe6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/3a0772443a0739141292a5429b952fe6-Metadata.json},
 openalex = {W2153521522},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/3a0772443a0739141292a5429b952fe6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning How to Teach or Selecting Minimal Surface Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/3a0772443a0739141292a5429b952fe6-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_3cf166c6,
 abstract = {I exhibit a systematic way to derive neural nets for vision problems. It involves formulating a vision problem as Bayesian inference or decision on a comprehensive model of the visual domain given by a probabilistic grammar.},
 author = {Mjolsness, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/3cf166c6b73f030b4f67eeaeba301103-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/3cf166c6b73f030b4f67eeaeba301103-Metadata.json},
 openalex = {W2126674203},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/3cf166c6b73f030b4f67eeaeba301103-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Visual Grammars and Their Neural Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/3cf166c6b73f030b4f67eeaeba301103-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_3dc4876f,
 abstract = {Neurons encoding simple visual features in area V1 such as orientation, direction of motion and color are organized in retinotopic maps. However, recent physiological experiments have shown that the responses of many neurons in V1 and other cortical areas are modulated by the direction of gaze. We have developed a neural network model of the visual cortex to explore the hypothesis that visual features are encoded in head-centered coordinates at early stages of visual processing. New experiments are suggested for testing this hypothesis using electrical stimulations and psychophysical observations.},
 author = {Pouget, Alexandre and Fisher, Stephen and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/3dc4876f3f08201c7c76cb71fa1da439-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/3dc4876f3f08201c7c76cb71fa1da439-Metadata.json},
 openalex = {W2104724025},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/3dc4876f3f08201c7c76cb71fa1da439-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Hierarchical Transformation of Space in the Visual System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/3dc4876f3f08201c7c76cb71fa1da439-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_428fca9b,
 abstract = {It is shown that both changes in viewing position and illumination conditions can be compensated for, prior to recognition, using combinations of images taken from different viewing positions and different illumination conditions. It is also shown that, in agreement with psychophysical findings, the computation requires at least a sign-bit image as input -- contours alone are not sufficient.},
 author = {Shashua, Amnon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/428fca9bc1921c25c5121f9da7815cde-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/428fca9bc1921c25c5121f9da7815cde-Metadata.json},
 openalex = {W2120893006},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/428fca9bc1921c25c5121f9da7815cde-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Illumination and View Position in 3D Visual Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/428fca9bc1921c25c5121f9da7815cde-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_42998cf3,
 abstract = {The complexity of learning in shallow 1-Dimensional neural networks has been shown elsewhere to be linear in the size of the network. However, when the network has a huge number of units (as cortex has) even linear time might be unacceptable. Furthermore, the algorithm that was given to achieve this time was based on a single serial processor and was biologically implausible.

In this work we consider the more natural parallel model of processing and demonstrate an expected-time complexity that is constant (i.e. independent of the size of the network). This holds even when internode communication channels are short and local, thus adhering to more biological and VLSI constraints.},
 author = {Judd, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/42998cf32d552343bc8e460416382dca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/42998cf32d552343bc8e460416382dca-Metadata.json},
 openalex = {W2131379160},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/42998cf32d552343bc8e460416382dca-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Constant-Time Loading of Shallow 1-Dimensional Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/42998cf32d552343bc8e460416382dca-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_46922a08,
 abstract = {A combined neural network and rule-based approach is suggested as a general framework for pattern recognition. This approach enables unsupervised and supervised learning, respectively, while providing probability estimates for the output classes. The probability maps are utilized for higher level analysis such as a feedback for smoothing over the output label maps and the identification of unknown patterns (pattern discovery). The suggested approach is presented and demonstrated in the texture - analysis task. A correct classification rate in the 90 percentile is achieved for both unstructured and structured natural texture mosaics. The advantages of the probabilistic approach to pattern analysis are demonstrated.},
 author = {Greenspan, Hayit K. and Goodman, Rodney and Chellappa, Rama},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/46922a0880a8f11f8f69cbb52b1396be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/46922a0880a8f11f8f69cbb52b1396be-Metadata.json},
 openalex = {W2164848902},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Combined Neural Network and Rule-Based Framework for Probabilistic Pattern Recognition and Discovery},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/46922a0880a8f11f8f69cbb52b1396be-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_49ae49a2,
 abstract = {We report learning measurements from a system composed of a cascadable learning chip, data generators and analyzers for training pattern presentation, and an X-windows based software interface. The 32 neuron learning chip has 496 adaptive synapses and can perform Boltzmann and mean-field learning using separate noise and gain controls. We have used this system to do learning experiments on the parity and replication problem. The system settling time limits the learning speed to about 100,000 patterns per second roughly independent of system size.},
 author = {Alspector, Joshua and Jayakumar, Anthony and Luna, Stephan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/49ae49a23f67c759bf4fc791ba842aa2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/49ae49a23f67c759bf4fc791ba842aa2-Metadata.json},
 openalex = {W2108424167},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/49ae49a23f67c759bf4fc791ba842aa2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Experimental Evaluation of Learning in a Neural Microsystem},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/49ae49a23f67c759bf4fc791ba842aa2-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_4e4b5fbb,
 abstract = {We have constructed a recurrent network that stabilizes images of a moving object on the retina of a simulated eye. The structure of the network was motivated by the organization of the primate visual target tracking system. The basic components of a complete target tracking system were simulated, including visual processing, sensory-motor interface, and motor control. Our model is simpler in structure, function and performance than the primate system, but many of the complexities inherent in a complete system are present.},
 author = {Viola, Paul and Lisberger, Stephen and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Metadata.json},
 openalex = {W2150726599},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Recurrent Eye Tracking Network Using a Distributed Representation of Image Motion},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/4e4b5fbbbb602b6d35bea8460aa8f8e5-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_51d92be1,
 abstract = {Networks for reconstructing a sparse or noisy function often use an edge field to segment the function into homogeneous regions, This approach assumes that these regions do not overlap or have disjoint parts, which is often false. For example, images which contain regions split by an occluding object can't be properly reconstructed using this type of network. We have developed a network that overcomes these limitations, using support maps to represent the segmentation of a signal. In our approach, the support of each region in the signal is explicitly represented. Results from an initial implementation demonstrate that this method can reconstruct images and motion sequences which contain complicated occlusion.},
 author = {Darrell, Trevor and Pentland, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/51d92be1c60d1db1d2e5e7a07da55b26-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/51d92be1c60d1db1d2e5e7a07da55b26-Metadata.json},
 openalex = {W2152582447},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Against Edges: Function Approximation with Multiple Support Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/51d92be1c60d1db1d2e5e7a07da55b26-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_53fde96f,
 abstract = {Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation.},
 author = {Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/53fde96fcc4b4ce72d7739202324cd49-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/53fde96fcc4b4ce72d7739202324cd49-Metadata.json},
 openalex = {W2128499899},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/53fde96fcc4b4ce72d7739202324cd49-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Induction of Multiscale Temporal Structure},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/53fde96fcc4b4ce72d7739202324cd49-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_550a141f,
 abstract = {To test whether the known connectivies of neurons in the lamprey spinal cord are sufficient to account for locomotor rhythmogenesis, a connectionist neural network simulation was done using identical cells connected according to experimentally established patterns. It was demonstrated that the network oscillates in a stable manner with the same phase relationships among the neurons as observed in the lamprey. The model was then used to explore coupling between identical oscillators. It was concluded that the neurons can have a dual role as rhythm generators and as coordinators between oscillators to produce the phase relations observed among segmental oscillators during swimming.},
 author = {Buchanan, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/550a141f12de6341fba65b0ad0433500-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/550a141f12de6341fba65b0ad0433500-Metadata.json},
 openalex = {W2148348511},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/550a141f12de6341fba65b0ad0433500-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Locomotion in a Lower Vertebrate: Studies of the Cellular Basis of Rhythmogenesis and Oscillator Coupling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/550a141f12de6341fba65b0ad0433500-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_559cb990,
 abstract = {We consider a noisy bistable single neuron model driven by a periodic external modulation. The modulation introduces a correlated switching between states driven by the noise. The information flow through the system from the modulation to the output switching events, leads to a succession of strong peaks in the power spectrum. The signal-to-noise ratio (SNR) obtained from this power spectrum is a measure of the information content in the neuron response. With increasing noise intensity, the SNR passes through a maximum, an effect which has been called stochastic resonance. We treat the problem within the framework of a recently developed approximate theory, valid in the limits of weak noise intensity, weak periodic forcing and low forcing frequency. A comparison of the results of this theory with those obtained from a linear system FFT is also presented.},
 author = {Bulsara, A. R. and Jacobs, W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/559cb990c9dffd8675f6bc2186971dc2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/559cb990c9dffd8675f6bc2186971dc2-Metadata.json},
 openalex = {W2137487739},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/559cb990c9dffd8675f6bc2186971dc2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Single Neuron Model: Response to Weak Modulation in the Presence of Noise},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/559cb990c9dffd8675f6bc2186971dc2-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_55a7cf9c,
 abstract = {A method for transforming performance evaluation signals distal both in space and time into proximal signals usable by supervised learning algorithms, presented in [Jordan & Jacobs 90], is examined. A simple observation concerning differentiation through models trained with redundant inputs (as one of their networks is) explains a weakness in the original architecture and suggests a modification: an internal world model that encodes action-space exploration and, crucially, cancels input redundancy to the forward model is added. Learning time on an example task, cartpole balancing, is thereby reduced about 50 to 100 times.},
 author = {Brody, Carlos},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/55a7cf9c71f1c9c495413f934dd1a158-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/55a7cf9c71f1c9c495413f934dd1a158-Metadata.json},
 openalex = {W2165313311},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Fast Learning with Predictive Forward Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/55a7cf9c71f1c9c495413f934dd1a158-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_57370345,
 abstract = {The dendritic trees of cortical pyramidal neurons seem ideally suited to perform local processing on inputs. To explore some of the implications of this complexity for the computational power of neurons, we simulated a realistic biophysical model of a hippocampal pyramidal cell in which a spot--a high density patch of inhibitory Ca-dependent K channels and a colocalized patch of Ca channels--was present at a dendritic branch point. The cold spot induced a non monotonic relationship between the strength of the synaptic input and the probability of neuronal firing. This effect could also be interpreted as an analog stochastic XOR.},
 author = {Zador, Anthony and Claiborne, Brenda and Brown, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5737034557ef5b8c02c0e46513b98f90-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5737034557ef5b8c02c0e46513b98f90-Metadata.json},
 openalex = {W2133256886},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5737034557ef5b8c02c0e46513b98f90-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Nonlinear Pattern Separation in Single Hippocampal Neurons with Active Dendritic Membrane},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/5737034557ef5b8c02c0e46513b98f90-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_58ae749f,
 abstract = {This paper briefly describes an artificial neural network for preattentive visual processing. The network is capable of determining image motion in a type of stimulus which defeats most popular methods of motion detection - a subset of second-order visual motion stimuli known as drift-balanced stimuli(DBS). The processing stages of the network described in this paper are integratable into a model capable of simultaneous motion extraction, edge detection, and the determination of occlusion.},
 author = {Tunley, Hilary},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/58ae749f25eded36f486bc85feb3f0ab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/58ae749f25eded36f486bc85feb3f0ab-Metadata.json},
 openalex = {W2150461992},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/58ae749f25eded36f486bc85feb3f0ab-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Neural Network for Motion Detection of Drift-Balanced Stimuli},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/58ae749f25eded36f486bc85feb3f0ab-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_598b3e71,
 abstract = {This paper presents PARSEC--a system for generating connectionist parsing networks from example parses. PARSEC is not based on formal grammar systems and is geared toward spoken language tasks. PARSEC networks exhibit three strengths important for application to speech processing: 1) they learn to parse, and generalize well compared to hand-coded grammars; 2) they tolerate several types of noise; 3) they can learn to use multi-modal input. Presented are the PARSEC architecture and performance analyses along several dimensions that demonstrate PARSEC's features. PARSEC's performance is compared to that of traditional grammar-based parsing systems.},
 author = {Jain, Ajay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/598b3e71ec378bd83e0a727608b5db01-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/598b3e71ec378bd83e0a727608b5db01-Metadata.json},
 openalex = {W2161327273},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/598b3e71ec378bd83e0a727608b5db01-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generalization Performance in PARSEC - A Structured Connectionist Parsing Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/598b3e71ec378bd83e0a727608b5db01-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_59b90e10,
 abstract = {In this paper we present a neural network architecture that discovers a recursive decomposition of its input space. Based on a generalization of the modular architecture of Jacobs, Jordan, Nowlan, and Hinton (1991), the architecture uses competition among networks to recursively split the input space into nested regions and to learn separate associative mappings within each region. The learning algorithm is shown to perform gradient ascent in a log likelihood function that captures the architecture's hierarchical structure.},
 author = {Jordan, Michael and Jacobs, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/59b90e1005a220e2ebc542eb9d950b1e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/59b90e1005a220e2ebc542eb9d950b1e-Metadata.json},
 openalex = {W2114083375},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Hierarchies of adaptive experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/59b90e1005a220e2ebc542eb9d950b1e-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_5b69b9cb,
 abstract = {We demonstrate a self-organizing system based on photorefractive ring oscillators. We employ the system in two ways that can both be thought of as feature extractors; one acts on a set of images exposed repeatedly to the system strictly as a linear feature extractor, and the other serves as a signal demultiplexer for fiber optic communications. Both systems implement unsupervised competitive learning embedded within the mode interaction dynamics between the modes of a set of ring oscillators. After a training period, the modes of the rings become associated with the different image features or carrier frequencies within the incoming data stream.},
 author = {Anderson, Dana and Benkert, Claus and Hebler, Verena and Jang, Ju-Seog and Montgomery, Don and Saffman, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5b69b9cb83065d403869739ae7f0995e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5b69b9cb83065d403869739ae7f0995e-Metadata.json},
 openalex = {W2146289280},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optical Implementation of a Self-Organizing Feature Extractor},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/5b69b9cb83065d403869739ae7f0995e-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_5e388103,
 abstract = {A board is described that contains the ANNA neural-network chip, and a DSP32C digital signal processor. The ANNA (Analog Neural Network Arithmetic unit) chip performs mixed analog/digital processing. The combination of ANNA with the DSP allows high-speed, end-to-end execution of numerous signal-processing applications, including the preprocessing, the neural-net calculations, and the postprocessing steps. The ANNA board evaluates neural networks 10 to 100 times faster than the DSP alone. The board is suitable for implementing large (million connections) networks with sparse weight matrices. Three applications have been implemented on the board: a convolver network for slant detection of text blocks, a handwritten digit recognizer, and a neural network for recognition-based segmentation.},
 author = {S\"{a}ckinger, Eduard and Boser, Bernhard and Jackel, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5e388103a391daabe3de1d76a6739ccd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5e388103a391daabe3de1d76a6739ccd-Metadata.json},
 openalex = {W2128394885},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5e388103a391daabe3de1d76a6739ccd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Neurocomputer Board Based on the ANNA Neural Network Chip},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/5e388103a391daabe3de1d76a6739ccd-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_5ea1649a,
 abstract = {Animal locomotion patterns are controlled by recurrent neural networks called central pattern generators (CPGs). Although a CPG can oscillate autonomously, its rhythm and phase must be well coordinated with the state of the physical system using sensory inputs. In this paper we propose a learning algorithm for synchronizing neural and physical oscillators with specific phase relationships. Sensory input connections are modified by the correlation between cellular activities and input signals. Simulations show that the learning rule can be used for setting sensory feedback connections to a CPG as well as coupling connections between CPGs.},
 author = {Doya, Kenji and Yoshizawa, Shuji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5ea1649a31336092c05438df996a3e59-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5ea1649a31336092c05438df996a3e59-Metadata.json},
 openalex = {W2108597081},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5ea1649a31336092c05438df996a3e59-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive Synchronization of Neural and Physical Oscillators},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/5ea1649a31336092c05438df996a3e59-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_5ef0b4eb,
 abstract = {There exist large classes of time series, such as those with nonlinear moving average components, that are not well modeled by feedforward networks or linear models, but can be modeled by recurrent networks. We show that recurrent neural networks are a type of nonlinear autoregressive-moving average (NARMA) model. Practical ability will be shown in the results of a competition sponsored by the Puget Sound Power and Light Company, where the recurrent networks gave the best performance on electric load forecasting.},
 author = {Connor, Jerome and Atlas, Les and Martin, Douglas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Metadata.json},
 openalex = {W2168431040},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Recurrent Networks and NARMA Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/5ef0b4eba35ab2d6180b0bca7e46b6f9-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_605ff764,
 abstract = {Current Intra-Cardia defibrillators make use of simple classification algorithms to determine patient conditions and subsequently to enable proper therapy. The simplicity is primarily due to the constraints on power dissipation and area available for implementation. Sub-threshold implementation of artificial neural networks offer potential classifiers with higher performance than commercially available defibrillators. In this paper we explore several classifier architectures and discuss micro-electronic implementation issues.},
 author = {Jabri, M. and Pickard, S. and Leong, P. and Chi, Z. and Flower, B. and Xie, Y.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/605ff764c617d3cd28dbbdd72be8f9a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/605ff764c617d3cd28dbbdd72be8f9a2-Metadata.json},
 openalex = {W2155949228},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/605ff764c617d3cd28dbbdd72be8f9a2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {ANN Based Classification for Heart Defibrillators},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/605ff764c617d3cd28dbbdd72be8f9a2-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_63538fe6,
 abstract = {We present an approach for development of a decoder for any complex binary error-correcting code (ECC) via training from examples of decoded received words. Our decoder is a connectionist architecture. We describe two separate solutions: A system-level solution (the Cascaded Networks Decoder); and the ECC-Enhanced Decoder, a solution which simplifies the mapping problem which must be solved for decoding. Although both solutions meet our basic approach constraint for simplicity and compactness, only the ECC-Enhanced Decoder meets our second basic constraint of being a generic solution.},
 author = {Gish, Sheri and Blaum, Mario},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/63538fe6ef330c13a05a3ed7e599d5f7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/63538fe6ef330c13a05a3ed7e599d5f7-Metadata.json},
 openalex = {W2141785936},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/63538fe6ef330c13a05a3ed7e599d5f7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive Development of Connectionist Decoders for Complex Error-Correcting Codes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/63538fe6ef330c13a05a3ed7e599d5f7-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_647bba34,
 author = {Bell, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/647bba344396e7c8170902bcf2e15551-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/647bba344396e7c8170902bcf2e15551-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/647bba344396e7c8170902bcf2e15551-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Self-organization in real neurons: Anti-Hebb in \textquotesingle Channel Space\textquotesingle ?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/647bba344396e7c8170902bcf2e15551-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_65658fde,
 abstract = {In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera).

We have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform.},
 author = {Simard, Patrice and Victorri, Bernard and LeCun, Yann and Denker, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/65658fde58ab3c2b6e5132a39fae7cb9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/65658fde58ab3c2b6e5132a39fae7cb9-Metadata.json},
 openalex = {W2111494971},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Tangent Prop - A formalism for specifying selected invariances in an adaptive network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/65658fde58ab3c2b6e5132a39fae7cb9-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_67f7fb87,
 author = {Bernasconi, Jakob and Gustafson, Karl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/67f7fb873eaf29526a11a9b7ac33bfac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/67f7fb873eaf29526a11a9b7ac33bfac-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Human and Machine \textquotesingle Quick Modeling\textquotesingle},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_68ce199e,
 abstract = {This paper examines whether temporal difference methods for training connectionist networks, such as Sutton's TD(λ) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical perspective. These practical issues are then examined in the context of a case study in which TD(λ) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex non-trivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. This indicates that TD learning may work better in practice than one would expect based on current theory, and it suggests that further analysis of TD methods, as well as applications in other complex domains, may be worth investigating.},
 author = {Tesauro, Gerald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Metadata.json},
 openalex = {W2103626435},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Practical issues in temporal difference learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/68ce199ec2c5517597ce0a4d89620f55-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_69421f03,
 abstract = {The localized linear discriminant network (LLDN) has been designed to address classification problems containing relatively closely spaced data from different classes (encounter zones [1], the accuracy problem [2]). Locally trained hyperplane segments are an effective way to define the decision boundaries for these regions [3]. The LLD uses a modified perceptron training algorithm for effective discovery of separating hyperplane/sigmoid units within narrow boundaries. The basic unit of the network is the discriminant receptive field (DRF) which combines the LLD function with Gaussians representing the dispersion of the local training data with respect to the hyperplane. The DRF implements a local distance measure [4], and obtains the benefits of networks oflocalized units [5]. A constructive algorithm for the two-class case is described which incorporates DRF's into the hidden layer to solve local discrimination problems. The output unit produces a smoothed, piecewise linear decision boundary. Preliminary results indicate the ability of the LLDN to efficiently achieve separation when boundaries are narrow and complex, in cases where both the standard multilayer perceptron (MLP) and k-nearest neighbor (KNN) yield high error rates on training data. 1 The LLD Training Algorithm and DRF Generation The LLD is defined by the hyperplane normal vector V and its midpoint M (a translated origin [1] near the center of gravity of the training data in feature space). Incremental corrections to V and M accrue for each training token feature vector Y j in the training set, as iIlustrated in figure 1 (exaggerated magnitudes). The surface of the hyperplane is appropriately moved either towards or away from Yj by rotating V, and shifting M along},
 author = {Glassman, Martin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/69421f032498c97020180038fddb8e24-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/69421f032498c97020180038fddb8e24-Metadata.json},
 openalex = {W2531975909},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/69421f032498c97020180038fddb8e24-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Network Of Localized Linear Discriminants},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/69421f032498c97020180038fddb8e24-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_6e2713a6,
 abstract = {We present a feed-forward network architecture for recognizing an unconstrained handwritten multi-digit string. This is an extension of previous work on recognizing isolated digits. In this architecture a single digit recognizer is replicated over the input. The output layer of the network is coupled to a Viterbi alignment module that chooses the best interpretation of the input. Training errors are propagated through the Viterbi module. The novelty in this procedure is that segmentation is done on the feature maps developed in the Space Displacement Neural Network (SDNN) rather than the input (pixel) space.},
 author = {Matan, Ofer and Burges, Christopher J. C. and LeCun, Yann and Denker, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/6e2713a6efee97bacb63e52c54f0ada0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/6e2713a6efee97bacb63e52c54f0ada0-Metadata.json},
 openalex = {W2100921332},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Multi-Digit Recognition Using a Space Displacement Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_6ea2ef73,
 abstract = {We present JANUS, a speech-to-speech translation system that utilizes diverse processing strategies, including connectionist learning, traditional AI knowledge representation approaches, dynamic programming, and stochastic techniques. JANUS translates continuously spoken English and German into German, English, and Japanese. JANUS currently achieves 87% translation fidelity from English speech and 97% from German speech. We present the JANUS system along with comparative evaluations of its interchangeable processing components, with special emphasis on the connectionist modules.},
 author = {Waibel, Alex and Jain, Ajay and McNair, Arthur and Tebelskis, Joe and Osterholtz, Louise and Saito, Hiroaki and Schmidbauer, Otto and Sloboda, Tilo and Woszczyna, Monika},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Metadata.json},
 openalex = {W2113941657},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {JANUS: Speech-to-Speech Translation Using Connectionist and Non-Connectionist Techniques},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/6ea2ef7311b482724a9b7b0bc0dd85c6-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_74071a67,
 abstract = {A CCD based signal processing IC that computes a fully parallel single quadrant vector-matrix multiplication has been designed and fabricated with a 2µm CCD/CMOS process. The device incorporates an array of Charge Coupled Devices (CCD) which hold an analog matrix of charge encoding the matrix elements. Input vectors are digital with 1 - 8 bit accuracy.},
 author = {Neugebauer, Charles and Yariv, Amnon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/74071a673307ca7459bcf75fbd024e09-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/74071a673307ca7459bcf75fbd024e09-Metadata.json},
 openalex = {W2111959810},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Parallel Analog CCD/CMOS Signal Processor},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/74071a673307ca7459bcf75fbd024e09-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_7bcdf75a,
 abstract = {We present an iterative algorithm for nonlinear regression based on construction of sparse polynomials. Polynomials are built sequentially from lower to higher order. Selection of new terms is accomplished using a novel look-ahead approach that predicts whether a variable contributes to the remaining error. The algorithm is based on the tree-growing heuristic in LMS Trees which we have extended to approximation of arbitrary polynomials of the input features. In addition, we provide a new theoretical justification for this heuristic approach. The algorithm is shown to discover a known polynomial from samples, and to make accurate estimates of pixel values in an image-processing task.},
 author = {Sanger, Terence and Sutton, Richard S and Matheus, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7bcdf75ad237b8e02e301f4091fb6bc8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7bcdf75ad237b8e02e301f4091fb6bc8-Metadata.json},
 openalex = {W2153167349},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Iterative Construction of Sparse Polynomial Approximations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/7bcdf75ad237b8e02e301f4091fb6bc8-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_7d04bbbe,
 abstract = {Using the double-step target displacement paradigm the mechanisms underlying arm trajectory modification were investigated. Using short (10- 110 msec) inter-stimulus intervals the resulting hand motions were initially directed in between the first and second target locations. The kinematic features of the modified motions were accounted for by the superposition scheme, which involves the vectorial addition of two independent point-to-point motion units: one for moving the hand toward an internally specified location and a second one for moving between that location and the final target location. The similarity between the inferred internally specified locations and previously reported measured end-points of the first saccades in double-step eye-movement studies may suggest similarities between perceived target locations in eye and hand motor control.},
 author = {Henis, Ealan and Flash, Tamar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Metadata.json},
 openalex = {W2156796329},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Computational Mechanism to Account for Averaged Modified Hand Trajectories},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_7dcd340d,
 abstract = {We introduce and demonstrate a bootstrap method for construction of an inverse function for the robot kinematic mapping using only sample configuration--space/ workspace data. Unsupervised learning (clustering) techniques are used on pre-image neighborhoods in order to learn to partition the configuration space into subsets over which the kinematic mapping is invertible. Supervised learning is then used separately on each of the partitions to approximate the inverse function. The ill-posed inverse kinematics function is thereby regularized, and a global inverse kinematics solution for the wristless Puma manipulator is developed.},
 author = {DeMers, David and Kreutz-Delgado, Kenneth},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7dcd340d84f762eba80aa538b0c527f7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7dcd340d84f762eba80aa538b0c527f7-Metadata.json},
 openalex = {W2117280091},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7dcd340d84f762eba80aa538b0c527f7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Global Direct Inverse Kinematics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/7dcd340d84f762eba80aa538b0c527f7-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_7f24d240,
 abstract = {Visual attention is the ability to dynamically restrict processing to a subset of the visual field. Researchers have long argued that such a mechanism is necessary to efficiently perform many intermediate level visual tasks. This paper describes VISIT, a novel neural network model of visual attention. The current system models the search for target objects in scenes containing multiple distractors. This is a natural task for people, it is studied extensively by psychologists, and it requires attention. The network's behavior closely matches the known psychophysical data on visual search and visual attention. VISIT also matches much of the physiological data on attention and provides a novel view of the functionality of a number of visual areas. This paper concentrates on the biological plausibility of the model and its relationship to the primary visual cortex, pulvinar, superior colliculus and posterior parietal areas.},
 author = {Ahmad, Subutai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7f24d240521d99071c93af3917215ef7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7f24d240521d99071c93af3917215ef7-Metadata.json},
 openalex = {W2120078010},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7f24d240521d99071c93af3917215ef7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {VISIT: A Neural Model of Covert Visual Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/7f24d240521d99071c93af3917215ef7-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_7fe1f8ab,
 abstract = {One method proposed for improving the generalization capability of a feedforward network trained with the backpropagation algorithm is to use artificial training vectors which are obtained by adding noise to the original training vectors. The authors discuss the connection of such backpropagation training with noise to kernel density and kernel regression estimation. They compare by simulated examples backpropagation, backpropagation with noise, and kernel regression in mapping estimation and pattern classification contexts. It is concluded that additive noise can improve the generalization capability of a feedforward network trained with the backpropagation approach. The magnitude of the noise cannot be selected blindly, though. Cross-validation-type procedures seem to be well suited for the selection of noise magnitude. Kernel regression, however, seems to perform well whenever backpropagation with noise performs well.< <ETX xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">&gt;</ETX>},
 author = {Koistinen, Petri and Holmstr\"{o}m, Lasse},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7fe1f8abaad094e0b5cb1b01d712f708-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7fe1f8abaad094e0b5cb1b01d712f708-Metadata.json},
 openalex = {W1548957680},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Kernel regression and backpropagation training with noise},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/7fe1f8abaad094e0b5cb1b01d712f708-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_81448138,
 abstract = {We present two neural network controller learning schemes based on feedback-error-learning and modular architecture for recognition and control of multiple manipulated objects. In the first scheme, a Gating Network is trained to acquire object-specific representations for recognition of a number of objects (or sets of objects). In the second scheme, an Estimation Network is trained to acquire function-specific, rather than object-specific, representations which directly estimate physical parameters. Both recognition networks are trained to identify manipulated objects using somatic and/or visual information. After learning, appropriate motor commands for manipulation of each object are issued by the control networks.},
 author = {Gomi, Hiroaki and Kawato, Mitsuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/81448138f5f163ccdba4acc69819f280-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/81448138f5f163ccdba4acc69819f280-Metadata.json},
 openalex = {W2118845783},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/81448138f5f163ccdba4acc69819f280-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Recognition of Manipulated Objects by Motor Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/81448138f5f163ccdba4acc69819f280-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_821fa74b,
 abstract = {Many auditory theorists consider the temporal adaptation of the auditory nerve a key aspect of speech coding in the auditory periphery. Experiments with models of auditory localization and pitch perception also suggest temporal adaptation is an important element of practical auditory processing. I have designed, fabricated, and successfully tested an analog integrated circuit that models many aspects of auditory nerve response, including temporal adaptation.},
 author = {Lazzaro, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/821fa74b50ba3f7cba1e6c53e8fa6845-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/821fa74b50ba3f7cba1e6c53e8fa6845-Metadata.json},
 openalex = {W2103130226},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Temporal Adaptation in a Silicon Auditory Nerve},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/821fa74b50ba3f7cba1e6c53e8fa6845-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_85422afb,
 abstract = {We investigate a model in which excitatory neurons have dynamical thresholds which display both fatigue and potentiation. The fatigue property leads to oscillatory behavior. It is responsible for the ability of the model to perform segmentation, i.e., decompose a mixed input into staggered oscillations of the activities of the cell-assemblies (memories) affected by it. Potentiation is responsible for sustaining these staggered oscillations after the input is turned off, i.e. the system serves as a model for short term memory. It has a limited STM capacity, reminiscent of the magical number 7±2.},
 author = {Horn, David and Usher, Marius},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/85422afb467e9456013a2a51d4dff702-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/85422afb467e9456013a2a51d4dff702-Metadata.json},
 openalex = {W2112669236},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/85422afb467e9456013a2a51d4dff702-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Oscillatory Model of Short Term Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/85422afb467e9456013a2a51d4dff702-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_854d9fca,
 abstract = {In this paper we investigate an average-case model of concept learning, and give results that place the popular statistical physics and VC dimension theories of learning curve behavior in a common framework.},
 author = {Haussler, David and Kearns, Michael and Opper, Manfred and Schapire, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/854d9fca60b4bd07f9bb215d59ef5561-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/854d9fca60b4bd07f9bb215d59ef5561-Metadata.json},
 openalex = {W2146017151},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/854d9fca60b4bd07f9bb215d59ef5561-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Estimating Average-Case Learning Curves Using Bayesian, Statistical Physics and VC Dimension Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_877a9ba7,
 abstract = {We present a framework for programming tbe bidden unit representations of simple recurrent networks based on the use of hint units (additional targets at the output layer). We present two ways of analysing a network trained within this framework: Input patterns act as operators on the information encoded by the context units; symmetrically, patterns of activation over tbe context units act as curried functions of the input sequences. Simulations demonstrate that a network can learn to represent three different functions simultaneously and canonical discriminant analysis is used to investigate how operators and curried functions are represented in the space of hidden unit activations.},
 author = {Wiles, Janet and Bloesch, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/877a9ba7a98f75b90a9d49f53f15a858-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/877a9ba7a98f75b90a9d49f53f15a858-Metadata.json},
 openalex = {W2110085047},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/877a9ba7a98f75b90a9d49f53f15a858-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Operators and curried functions: Training and analysis of simple recurrent networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/877a9ba7a98f75b90a9d49f53f15a858-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_8b16ebc0,
 abstract = {I present a modular network architecture and a learning algorithm based on incremental dynamic programming that allows a single learning agent to learn to solve multiple Markovian decision tasks (MDTs) with significant transfer of learning across the tasks. I consider a class of MDTs, called composite tasks, formed by temporally concatenating a number of simpler, elemental MDTs. The architecture is trained on a set of composite and elemental MDTs. The temporal structure of a composite task is assumed to be unknown and the architecture learns to produce a temporal decomposition. It is shown that under certain conditions the solution of a composite MDT can be constructed by computationally inexpensive modifications of the solutions of its constituent elemental MDTs.},
 author = {Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8b16ebc056e613024c057be590b542eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8b16ebc056e613024c057be590b542eb-Metadata.json},
 openalex = {W2146738023},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8b16ebc056e613024c057be590b542eb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Efficient Learning of Multiple Task Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/8b16ebc056e613024c057be590b542eb-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_8d34201a,
 abstract = {Existing metrics for the learning performance of feed-forward neural networks do not provide a satisfactory basis for comparison because the choice of the training epoch limit can determine the results of the comparison. I propose new metrics which have the desirable property of being independent of the training epoch limit. The efficiency measures the yield of correct networks in proportion to the training effort expended. The optimal epoch limit provides the greatest efficiency. The learning performance is modelled statistically, and asymptotic performance is estimated. Implementation details may be found in (Hamey, 1992).},
 author = {Hamey, Leonard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8d34201a5b85900908db6cae92723617-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8d34201a5b85900908db6cae92723617-Metadata.json},
 openalex = {W2102949100},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8d34201a5b85900908db6cae92723617-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Benchmarking Feed-Forward Neural Networks: Models and Measures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/8d34201a5b85900908db6cae92723617-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_8e6b42f1,
 abstract = {A CCD-based processor that we call the NNC2 is presented. The NNC2 implements a fully connected 192-input, 32-output two-layer network and can be cascaded to form multilayer networks or used in parallel for additional input or output nodes. The device computes 1.92 × 109 connections/ sec when clocked at 10 MHz. Network weights can be specified to six bits of accuracy and are stored on-chip in programmable digital memories. A neural network pattern recognition system using NNC2 and CCD image feature extractor (IFE) devices is described. Additionally, we report a CCD output circuit that exploits inherent nonlinearities in the charge injection process to realize an adjustable-threshold sigmoid in a chip area of 40 × 80 µm2.},
 author = {Chiang, Alice and Chuang, Michael and LaFranchise, Jeffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8e6b42f1644ecb1327dc03ab345e618b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8e6b42f1644ecb1327dc03ab345e618b-Metadata.json},
 openalex = {W2123805759},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {CCD Neural Network Processors for Pattern Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/8e6b42f1644ecb1327dc03ab345e618b-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_8eefcfdf,
 abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
 author = {Krogh, Anders and Hertz, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Metadata.json},
 openalex = {W2144513243},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Simple Weight Decay Can Improve Generalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/8eefcfdf5990e441f0fb6f3fad709e21-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_941e1aaa,
 abstract = {We have explored the use of iutificial neural networks to diagnose avascular necrosis (AVN) of the femoral head from magnetic resonance images. We developed mu1 ti-layer perceptron networks, trained with conjugate gradient optimization, which diagnose AVN from single sagittal images of the femoral head with 100% accuracy on the training data and 97% accuracy on test data. These networks use only the raw image as input (with minimal preprocessing to average the images down to 32x32 size and to scale the input data values) and leam to extract their own features for the diagnosis decision. Various experiments with these networks are described.},
 author = {Manduca, Armando and Christy, Paul and Ehman, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/941e1aaaba585b952b62c14a3a175a61-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/941e1aaaba585b952b62c14a3a175a61-Metadata.json},
 openalex = {W2534426600},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/941e1aaaba585b952b62c14a3a175a61-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Diagnosis Of Avascular Necrosis From Magnetic Resonance Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/941e1aaaba585b952b62c14a3a175a61-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_9431c87f,
 abstract = {A method is described for generating plan-like, reflexive, obstacle avoidance behaviour in a mobile robot. The experiments reported here use a simulated vehicle with a primitive range sensor. Avoidance behaviour is encoded as a set of continuous functions of the perceptual input space. These functions are stored using CMACs and trained by a variant of Barto and Sutton's adaptive critic algorithm. As the vehicle explores its surroundings it adapts its responses to sensory stimuli so as to minimise the negative reinforcement arising from collisions. Strategies for local navigation are therefore acquired in an explicitly goal-driven fashion. The resulting trajectories form elegant collision-free paths through the environment.},
 author = {Prescott, Tony and Mayhew, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9431c87f273e507e6040fcb07dcb4509-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9431c87f273e507e6040fcb07dcb4509-Metadata.json},
 openalex = {W2102947728},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9431c87f273e507e6040fcb07dcb4509-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Obstacle Avoidance through Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/9431c87f273e507e6040fcb07dcb4509-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_9461cce2,
 abstract = {A general relationship is developed between the VC-dimension and the statistical lower epsilon-capacity which shows that the VC-dimension can be lower bounded (in order) by the statistical lower epsilon-capacity of a network trained with random samples. This relationship explains quantitatively how generalization takes place after memorization, and relates the concept of generalization (consistency) with the capacity of the optimal classifier over a class of classifiers with the same structure and the capacity of the Bayesian classifier. Furthermore, it provides a general methodology to evaluate a lower bound for the VC-dimension of feedforward multilayer neural networks.

This general methodology is applied to two types of networks which are important for hardware implementations: two layer (N - 2L - 1) networks with binary weights, integer thresholds for the hidden units and zero threshold for the output unit, and a single neuron ((N - 1) networks) with binary weigths and a zero threshold. Specifically, we obtain O(W/lnL) ≤ d2 ≤ O(W), and d1 - O(N). Here W is the total number of weights of the (N - 2L - 1) networks. d1 and d2 represent the VC-dimensions for the (N - 1) and (N - 2L - 1) networks respectively.},
 author = {Ji, Chuanyi and Psaltis, Demetri},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9461cce28ebe3e76fb4b931c35a169b0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9461cce28ebe3e76fb4b931c35a169b0-Metadata.json},
 openalex = {W2099303499},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9461cce28ebe3e76fb4b931c35a169b0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The VC-Dimension versus the Statistical Capacity of Multilayer Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/9461cce28ebe3e76fb4b931c35a169b0-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_97e8527f,
 abstract = {Three methods for improving the performance of (gaussian) radial basis function (RBF) networks were tested on the NETtalk task. In RBF, a new example is classified by computing its Euclidean distance to a set of centers chosen by unsupervised methods. The application of supervised learning to learn a non-Euclidean distance metric was found to reduce the error rate of RBF networks, while supervised learning of each center's variance resulted in inferior performance. The best improvement in accuracy was achieved by networks called generalized radial basis function (GRBF) networks. In GRBF, the center locations are determined by supervised learning. After training on 1000 words, RBF classifies 56.5% of letters correct, while GRBF scores 73.4% letters correct (on a separate test set). From these and other experiments, we conclude that supervised learning of center locations can be very important for radial basis function learning.},
 author = {Wettschereck, Dietrich and Dietterich, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/97e8527feaf77a97fc38f34216141515-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/97e8527feaf77a97fc38f34216141515-Metadata.json},
 openalex = {W2110916332},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/97e8527feaf77a97fc38f34216141515-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Improving the Performance of Radial Basis Function Networks by Learning Center Locations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/97e8527feaf77a97fc38f34216141515-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_98b29795,
 abstract = {During waking and sleep, the brain and mind undergo a tightly linked and precisely specified set of changes in state. At the level of neurons, this process has been modeled by variations of Volterra-Lotka equations for cyclic fluctuations of brainstem cell populations. However, neural network models based upon rapidly developing knowledge of the specific population connectivities and their differential responses to drugs have not yet been developed. Furthermore, only the most preliminary attempts have been made to model across states. Some of our own attempts to link rapid eye movement (REM) sleep neurophysiology and dream cognition using neural network approaches are summarized in this paper.},
 author = {Hobson, J. and Mamelak, Adam and Sutton, Jeffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/98b297950041a42470269d56260243a1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/98b297950041a42470269d56260243a1-Metadata.json},
 openalex = {W2159512711},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/98b297950041a42470269d56260243a1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Models Wanted: Must Fit Dimensions of Sleep and Dreaming},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/98b297950041a42470269d56260243a1-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_99c5e07b,
 abstract = {Several parallel analogue algorithms, based upon mean field theory (MFT) approximations to an underlying statistical mechanics formulation, and requiring an externally prescribed annealing schedule, now exist for finding approximate solutions to difficult combinatorial optimisation problems. They have been applied to the Travelling Salesman Problem (TSP), as well as to various issues in computational vision and cluster analysis. I show here that any given MFT algorithm can be combined in a natural way with notions from the areas of constrained optimisation and adaptive simulated annealing to yield a single homogenous and efficient parallel relaxation technique, for which an externally prescribed annealing schedule is no longer required. The results of numerical simulations on 50-city and 100-city TSP problems are presented, which show that the ensuing algorithms are typically an order of magnitude faster than the MFT algorithms alone, and which also show, on occasion, superior solutions as well.},
 author = {Stolorz, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Metadata.json},
 openalex = {W2105822056},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/99c5e07b4d5de9d18c350cdf64c5aa3d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Merging Constrained Optimisation with Deterministic Annealing to "Solve" Combinatorially Hard Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/99c5e07b4d5de9d18c350cdf64c5aa3d-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_9a96876e,
 abstract = {In a Bayesian framework, we give a principled account of how domain-specific prior knowledge such as imperfect analytic domain theories can be optimally incorporated into networks of locally-tuned units: by choosing a specific architecture and by applying a specific training regimen. Our method proved successful in overcoming the data deficiency problem in a large-scale application to devise a neural control for a hot line rolling mill. It achieves in this application significantly higher accuracy than optimally-tuned standard algorithms such as sigmoidal backpropagation, and outperforms the state-of-the-art solution.},
 author = {R\"{o}scheisen, Martin and Hofmann, Reimar and Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Metadata.json},
 openalex = {W2156255805},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Control for Rolling Mills: Incorporating Domain Theories to Overcome Data Deficiency},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/9a96876e2f8f3dc4f3cf45f02c61c0c1-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_9ad6aaed,
 abstract = {We define the concept of polynomial uniform convergence of relative frequencies to probabilities in the distribution-dependent context. Let Xn = {0,1}n, let Pn be a probability distribution on Xn and let Fn ⊂ 2Xn be a family of events. The family {(Xn, Pn, Fn)}n≥1 has the property of polynomial uniform convergence if the probability that the maximum difference (over Fn) between the relative frequency and the probability of an event exceed a given positive e be at most δ (0 0 such that M(n, t) = O(n/tβ). Applications to distribution-dependent PAC learning are discussed.},
 author = {Bertoni, Alberto and Campadelli, Paola and Morpurgo, Anna and Panizza, Sandra},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9ad6aaed513b73148b7d49f70afcfb32-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9ad6aaed513b73148b7d49f70afcfb32-Metadata.json},
 openalex = {W2098728981},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Polynomial Uniform Convergence of Relative Frequencies to Probabilities},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_9b70e8fe,
 abstract = {We propose a paradigm for modeling speech production based on neural networks. We focus on characteristics of the musculoskeletal system. Using real physiological data - articulator movements and EMG from muscle activity - a neural network learns the forward dynamics relating motor commands to muscles and the ensuing articulator behavior. After learning, simulated perturbations, were used to asses properties of the acquired model, such as natural frequency, damping, and interarticulator couplings. Finally, a cascade neural network is used to generate continuous motor commands from a sequence of discrete articulatory targets.},
 author = {Hirayama, Makoto and Vatikiotis-Bateson, Eric and Kawato, Mitsuo and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9b70e8fe62e40c570a322f1b0b659098-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9b70e8fe62e40c570a322f1b0b659098-Metadata.json},
 openalex = {W2102084103},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9b70e8fe62e40c570a322f1b0b659098-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Forward Dynamics Modeling of Speech Motor Control Using Physiological Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/9b70e8fe62e40c570a322f1b0b659098-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_9b72e31d,
 abstract = {Despite the fact that complex visual scenes contain multiple, overlapping objects, people perform object recognition with ease and accuracy. One operation that facilitates recognition is an early segmentation process in which features of objects are grouped and labeled according to which object they belong. Current computational systems that perform this operation are based on predefined grouping heuristics. We describe a system called MAGIC that learns how to group features based on a set of presegmented examples. In many cases, MAGIC discovers grouping heuristics similar to those previously proposed, but it also has the capability of finding nonintuitive structural regularities in images. Grouping is performed by a relaxation network that attempts to dynamically bind related features. Features transmit a complex-valued signal (amplitude and phase) to one another; binding can thus be represented by phase locking related features. MAGIC's training procedure is a generalization of recurrent backpropagation to complex-valued units.},
 author = {Mozer, Michael C and Zemel, Richard and Behrmann, Marlene},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9b72e31dac81715466cd580a448cf823-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9b72e31dac81715466cd580a448cf823-Metadata.json},
 openalex = {W1970363173},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/9b72e31dac81715466cd580a448cf823-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning to Segment Images Using Dynamic Feature Binding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/9b72e31dac81715466cd580a448cf823-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_a516a87c,
 abstract = {This work discusses various optimization techniques which were proposed in models for controlling arm movements. In particular, the minimum-muscle-tension-change model is investigated. A dynamic simulator of the monkey's arm, including seventeen single and double joint muscles, is utilized to generate horizontal hand movements. The hand trajectories produced by this algorithm are discussed.},
 author = {Dornay, Menashe and Uno, Yoji and Kawato, Mitsuo and Suzuki, Ryoji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a516a87cfcaef229b342c437fe2b95f7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a516a87cfcaef229b342c437fe2b95f7-Metadata.json},
 openalex = {W2103639258},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Simulation of Optimal Movements Using the Minimum-Muscle-Tension-Change Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/a516a87cfcaef229b342c437fe2b95f7-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_a7608800,
 abstract = {Automated monitoring of vigilance in attention intensive tasks such as air traffic control or sonar operation is highly desirable. As the operator monitors the instrument, the instrument would monitor the operator, insuring against lapses. We have taken a first step toward this goal by using feedforward neural networks trained with backpropagation to interpret event related potentials (ERPs) and electroencephalogram (EEG) associated with periods of high and low vigilance. The accuracy of our system on an ERP data set averaged over 28 minutes was 96%, better than the 83% accuracy obtained using linear discriminant analysis. Practical vigilance monitoring will require prediction over shorter time periods. We were able to average the ERP over as little as 2 minutes and still get 90% correct prediction of a vigilance measure. Additionally, we achieved similarly good performance using segments of EEG power spectrum as short as 56 sec.},
 author = {Venturini, Rita and Lytton, William and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a760880003e7ddedfef56acb3b09697f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a760880003e7ddedfef56acb3b09697f-Metadata.json},
 openalex = {W2158771940},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a760880003e7ddedfef56acb3b09697f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Analysis of Event Related Potentials and Electroencephalogram Predicts Vigilance},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/a760880003e7ddedfef56acb3b09697f-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_a7aeed74,
 abstract = {HARMONET, a system employing connectionist networks for music processing, is presented. After being trained on some dozen Bach chorales using error backpropagation, the system is capable of producing four-part chorales in the style of J.S. Bach, given a one-part melody. Our system solves a musical real-world problem on a performance level appropriate for musical practice. HARMONET's power is based on (a) a new coding scheme capturing musically relevant information and (b) the integration of backpropagation and symbolic algorithms in a hierarchical system, combining the advantages of both.},
 author = {Hild, Hermann and Feulner, Johannes and Menzel, Wolfram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a7aeed74714116f3b292a982238f83d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a7aeed74714116f3b292a982238f83d2-Metadata.json},
 openalex = {W2118730391},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a7aeed74714116f3b292a982238f83d2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {HARMONET: A Neural Net for Harmonizing Chorales in the Style of J. S. Bach},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/a7aeed74714116f3b292a982238f83d2-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_a86c450b,
 abstract = {We present a neural network algorithm that simultaneously performs segmentation and recognition of input patterns that self-organizes to detect input pattern locations and pattern boundaries. We demonstrate this neural network architecture on character recognition using the NIST database and report on results herein. The resulting system simultaneously segments and recognizes touching or overlapping characters, broken characters, and noisy images with high accuracy.},
 author = {Keeler, Jim and Rumelhart, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a86c450b76fb8c371afead6410d55534-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a86c450b76fb8c371afead6410d55534-Metadata.json},
 openalex = {W2115641419},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a86c450b76fb8c371afead6410d55534-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Self-Organizing Integrated Segmentation and Recognition Neural Net},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/a86c450b76fb8c371afead6410d55534-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_a8849b05,
 abstract = {Two projection based feedforward network learning methods for modelfree regression problems are studied and compared in this paper: one is the popular back-propagation learning (BPL); the other is the projection pursuit learning (PPL). Unlike the totally parametric BPL method, the PPL non-parametrically estimates unknown nonlinear functions sequentially (neuron-by-neuron and layer-by-Iayer) at each iteration while jointly estimating the interconnection weights. In terms of learning efficiency, both methods have comparable training speed when based on a GaussNewton optimization algorithm while the PPL is more parsimonious. In terms of learning robustness toward noise outliers, the BPL is more sensitive to the outliers.},
 author = {Huang, Jenq-Neng and Li, Hang and Maechler, Martin and Martin, R. and Schimert, Jim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a8849b052492b5106526b2331e526138-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a8849b052492b5106526b2331e526138-Metadata.json},
 openalex = {W3149848624},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a8849b052492b5106526b2331e526138-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Comparison of Projection Pursuit and Neural Network Regression Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/a8849b052492b5106526b2331e526138-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_a8abb4bb,
 author = {Bridle, John and Heading, Anthony and MacKay, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a8abb4bb284b5b27aa7cb790dc20f80b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a8abb4bb284b5b27aa7cb790dc20f80b-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a8abb4bb284b5b27aa7cb790dc20f80b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Unsupervised Classifiers, Mutual Information and \textquotesingle Phantom Targets},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/a8abb4bb284b5b27aa7cb790dc20f80b-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_a9a1d531,
 abstract = {A large class of motor control tasks requires that on each cycle the controller is told its current state and must choose an action to achieve a specified, state-dependent, goal behaviour. This paper argues that the optimization of learning rate, the number of experimental control decisions before adequate performance is obtained, and robustness is of prime importance--if necessary at the expense of computation per control cycle and memory requirement. This is motivated by the observation that a robot which requires two thousand learning steps to achieve adequate performance, or a robot which occasionally gets stuck while learning, will always be undesirable, whereas moderate computational expense can be accommodated by increasingly powerful computer hardware. It is not unreasonable to assume the existence of inexpensive 100 Mflop controllers within a few years and so even processes with control cycles in the low tens of milliseconds will have millions of machine instructions in which to make their decisions. This paper outlines a learning control scheme which aims to make effective use of such computational power.},
 author = {Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a9a1d5317a33ae8cef33961c34144f84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a9a1d5317a33ae8cef33961c34144f84-Metadata.json},
 openalex = {W2119659301},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Fast, Robust Adaptive Control by Learning only Forward Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_a9a6653e,
 abstract = {Second-order recurrent networks that recognize simple finite state languages over {0,1}* are induced from positive and negative examples. Using the complete gradient of the recurrent network and sufficient training examples to constrain the definition of the language to be induced, solutions are obtained that correctly recognize strings of arbitrary length. A method for extracting a finite state automaton corresponding to an optimized network is demonstrated.},
 author = {Watrous, Raymond and Kuhn, Gary},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a9a6653e48976138166de32772b1bf40-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a9a6653e48976138166de32772b1bf40-Metadata.json},
 openalex = {W2108789285},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/a9a6653e48976138166de32772b1bf40-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Induction of Finite-State Automata Using Second-Order Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/a9a6653e48976138166de32772b1bf40-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_ab817c93,
 abstract = {We designed and trained a connectionist network to generate letterforms in a new given just a few exemplars from that font. During learning, our network constructed a distributed internal representation of fonts as well as letters, despite the fact that each training instance exemplified both a and a letter. It was necessary to have separate but interconnected hidden units for letter and font representations - several alternative architectures were not successful.},
 author = {Grebert, Igor and Stork, David and Keesing, Ron and Mims, Steve},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ab817c9349cf9c4f6877e1894a1faa00-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ab817c9349cf9c4f6877e1894a1faa00-Metadata.json},
 openalex = {W2097901663},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ab817c9349cf9c4f6877e1894a1faa00-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Network generalization for production: Learning and producing styled letterforms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/ab817c9349cf9c4f6877e1894a1faa00-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_b337e84d,
 author = {Sun, Guo-Zheng and Chen, Hsing-Hen and Lee, Yee-Chun},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/b337e84de8752b27eda3a12363109e80-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/b337e84de8752b27eda3a12363109e80-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/b337e84de8752b27eda3a12363109e80-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Green\textquotesingle s Function Method for Fast On-Line Learning Algorithm of Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/b337e84de8752b27eda3a12363109e80-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_b534ba68,
 abstract = {We use constrained optimization to select operating parameters for two circuits: a simple 3-transistor square root circuit, and an analog VLSI artificial cochlea. This automated method uses computer controlled measurement and test equipment to choose chip parameters which minimize the difference between the actual circuit's behavior and a specified goal behavior. Choosing the proper circuit parameters is important to compensate for manufacturing deviations or adjust circuit performance within a certain range. As biologically-motivated analog VLSI circuits become increasingly complex, implying more parameters, setting these parameters by hand will become more cumbersome. Thus an automated parameter setting method can be of great value [Fleischer 90]. Automated parameter setting is an integral part of a goal-based engineering design methodology in which circuits are constructed with parameters enabling a wide range of behaviors, and are then tuned to the desired behaviors automatically.},
 author = {Kirk, David and Fleischer, Kurt and Watts, Lloyd and Barr, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/b534ba68236ba543ae44b22bd110a1d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/b534ba68236ba543ae44b22bd110a1d6-Metadata.json},
 openalex = {W2161232375},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Constrained Optimization Applied to the Parameter Setting Problem for Analog Circuits},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/b534ba68236ba543ae44b22bd110a1d6-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_b5b41fac,
 abstract = {We describe in this paper a novel application of neural networks to system health monitoring of a large antenna for deep space communications. The paper outlines our approach to building a monitoring system using hybrid signal processing and neural network techniques, including autoregressive modelling, pattern recognition, and Hidden Markov models. We discuss several problems which are somewhat generic in applications of this kind - in particular we address the problem of detecting classes which were not present in the training data. Experimental results indicate that the proposed system is sufficiently reliable for practical implementation.},
 author = {Smyth, Padhraic and Mellstrom, Jeff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/b5b41fac0361d157d9673ecb926af5ae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/b5b41fac0361d157d9673ecb926af5ae-Metadata.json},
 openalex = {W2166053099},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/b5b41fac0361d157d9673ecb926af5ae-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Fault Diagnosis of Antenna Pointing Systems using Hybrid Neural Network and Signal Processing Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/b5b41fac0361d157d9673ecb926af5ae-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_ba2fd310,
 abstract = {A network model with temporal sequencing and state-dependent modulatory features is described. The model is motivated by neurocognitive data characterizing different states of waking and sleeping. Computer studies demonstrate how unique states of sequencing can exist within the same network under different aminergic and cholinergic modulatory influences. Relationships between state-dependent modulation, memory, sequencing and learning are discussed.},
 author = {Sutton, Jeffrey and Mamelak, Adam and Hobson, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ba2fd310dcaa8781a9a652a31baf3c68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ba2fd310dcaa8781a9a652a31baf3c68-Metadata.json},
 openalex = {W2139907769},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Network Model of State-Dependent Sequencing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/ba2fd310dcaa8781a9a652a31baf3c68-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_c203d8a1,
 abstract = {Connections between spline approximation, approximation with rational functions, and feedforward neural networks are studied. The potential improvement in the degree of approximation in going from single to two hidden layer networks is examined. Some results of Birman and Solomjak regarding the degree of approximation achievable when knot positions are chosen on the basis of the probability distribution of examples rather than the function values are extended.},
 author = {Williamson, Robert C and Bartlett, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c203d8a151612acf12457e4d67635a95-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c203d8a151612acf12457e4d67635a95-Metadata.json},
 openalex = {W2157901778},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c203d8a151612acf12457e4d67635a95-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Splines, Rational Functions and Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/c203d8a151612acf12457e4d67635a95-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_c399862d,
 abstract = {We have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces, this procedure learns to extract surface depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hinton, 1992). In this paper, we propose two new models which handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized, asymmetric interpolators that do not cross the discontinuities.},
 author = {Becker, Suzanna and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c399862d3b9d6b76c8436e924a68c45b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c399862d3b9d6b76c8436e924a68c45b-Metadata.json},
 openalex = {W2129675393},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning to Make Coherent Predictions in Domains with Discontinuities},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_c3c59e5f,
 abstract = {The Bayesian model comparison framework is reviewed, and the Bayesian Occam's razor is explained. This framework can be applied to feedforward networks, making possible (1) objective comparisons between solutions using alternative network architectures; (2) objective choice of magnitude and type of weight decay terms; (3) quantified estimates of the error bars on network parameters and on network output. The framework also generates a measure of the effective number of parameters determined by the data.

The relationship of Bayesian model comparison to recent work on prediction of generalisation ability (Guyon et al., 1992, Moody, 1992) is discussed.},
 author = {MacKay, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c3c59e5f8b3e9753913f4d435b53c308-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c3c59e5f8b3e9753913f4d435b53c308-Metadata.json},
 openalex = {W2140838683},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bayesian Model Comparison and Backprop Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/c3c59e5f8b3e9753913f4d435b53c308-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_c410003e,
 abstract = {We have created new networks to unmix signals which have been mixed either with time delays or via filtering. We first show that a subset of the Herault-Jutten learning rules fulfills a principle of minimum output power. We then apply this principle to extensions of the Herault-Jutten network which have delays in the feedback path. Our networks perform well on real speech and music signals that have been mixed using time delays or filtering.},
 author = {Platt, John and Faggin, Federico},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c410003ef13d451727aeff9082c29a5c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c410003ef13d451727aeff9082c29a5c-Metadata.json},
 openalex = {W2100804100},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c410003ef13d451727aeff9082c29a5c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Networks for the Separation of Sources that are Superimposed and Delayed},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/c410003ef13d451727aeff9082c29a5c-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_c6e19e83,
 abstract = {The dynamic behavior of a network model consisting of all-to-all excitatory coupled binary neurons with global inhibition is studied analytically and numerically. We prove that for random input signals, the output of the network consists of synchronized bursts with apparently random intermissions of noisy activity. Our results suggest that synchronous bursts can be generated by a simple neuronal architecture which amplifies incoming coincident signals. This synchronization process is accompanied by dampened oscillations which, by themselves, however, do not play any constructive role in this and can therefore be considered to be an epiphenomenon.},
 author = {Schuster, Heinz and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Metadata.json},
 openalex = {W2130976507},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Burst Synchronization without Frequency Locking in a Completely Solvable Neural Network Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/c6e19e830859f2cb9f7c8f8cacb8d2a6-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_c75b6f11,
 abstract = {We present a distribution-free model for incremental learning when concepts vary with time. Concepts are caused to change by an adversary while an incremental learning algorithm attempts to track the changing concepts by minimizing the error between the current target concept and the hypothesis. For a single half-plane and the intersection of two half-planes, we show that the average mistake rate depends on the maximum rate at which an adversary can modify the concept. These theoretical predictions are verified with simulations of several learning algorithms including back propagation.},
 author = {Kuh, Anthony and Petsche, Thomas and Rivest, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c75b6f114c23a4d7ea11331e7c00e73c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c75b6f114c23a4d7ea11331e7c00e73c-Metadata.json},
 openalex = {W2139002523},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c75b6f114c23a4d7ea11331e7c00e73c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Incrementally Learning Time-varying Half-planes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/c75b6f114c23a4d7ea11331e7c00e73c-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_c9892a98,
 abstract = {A neural network solution is proposed for solving path planning problems faced by mobile robots. The proposed network is a two-dimensional sheet of neurons forming a distributed representation of the robot's workspace. Lateral interconnections between neurons are cooperative, so that the network exhibits oscillatory behaviour. These oscillations are used to generate solutions of Bellman's dynamic programming equation in the context of path planning. Simulation experiments imply that these networks locate global optimal paths even in the presence of substantial levels of circuit nOlse.},
 author = {Lemmon, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c9892a989183de32e976c6f04e700201-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c9892a989183de32e976c6f04e700201-Metadata.json},
 openalex = {W2138536041},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/c9892a989183de32e976c6f04e700201-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Oscillatory Neural Fields for Globally Optimal Path Planning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/c9892a989183de32e976c6f04e700201-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_cbcb58ac,
 abstract = {During visual development, projections from retinal ganglion cells (RGCs) to the lateral geniculate nucleus (LGN) in cat are refined to produce ocular dominance layering and precise topographic mapping. Normal development depends upon activity in RGCs, suggesting a key role for activity-dependent synaptic plasticity. Recent experiments on prenatal retina show that during early development, waves of activity pass across RGCs (Meister, et al., 1991). We provide the first simulations to demonstrate that such retinal waves, in conjunction with Hebbian synaptic competition and early arrival of contralateral axons, can account for observed patterns of retinogeniculate projections in normal and experimentally-treated animals.},
 author = {Keesing, Ron and Stork, David and Shatz, Carla},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cbcb58ac2e496207586df2854b17995f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cbcb58ac2e496207586df2854b17995f-Metadata.json},
 openalex = {W2104669654},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cbcb58ac2e496207586df2854b17995f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Retinogeniculate Development: The Role of Competition and Correlated Retinal Activity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/cbcb58ac2e496207586df2854b17995f-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_ccb1d45f,
 abstract = {Accurate saccades require interaction between brainstem circuitry and the cerebellum. A model of this interaction is described, based on Kawato's principle of feedback-error-learning. In the model a part of the brainstem (the superior colliculus) acts as a simple feedback controller with no knowledge of initial eye position, and provides an error signal for the cerebellum to correct for eye-muscle nonlinearities. This teaches the cerebellum, modelled as a CMAC, to adjust appropriately the gain on the brainstem burst-generator's internal feedback loop and so alter the size of burst sent to the motoneurons. With direction-only errors the system rapidly learns to make accurate horizontal eye movements from any starting position, and adapts realistically to subsequent simulated eye-muscle weakening or displacement of the saccadic target.},
 author = {Dean, Paul and Mayhew, John and Langdon, Pat},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Metadata.json},
 openalex = {W2115991890},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Neural Net Model for Adaptive Control of Saccadic Accuracy by Primate Cerebellum and Brainstem},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/ccb1d45fb76f7c5a0bf619f979c6cf36-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_cee63112,
 abstract = {A novel segmentation algorithm has been developed utilizing an absolute-value smoothness penalty instead of the more common quadratic regularizer. This functional imposes a piece-wise constant constraint on the segmented data. Since the minimized energy is guaranteed to be convex, there are no problems with local minima and no complex continuation methods are necessary to find the unique global minimum. By interpreting the minimized energy as the generalized power of a nonlinear resistive network, a continuous-time analog segmentation circuit was constructed.},
 author = {Harris, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cee631121c2ec9232f3a2f028ad5c89b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cee631121c2ec9232f3a2f028ad5c89b-Metadata.json},
 openalex = {W2141314077},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cee631121c2ec9232f3a2f028ad5c89b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Segmentation Circuits Using Constrained Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/cee631121c2ec9232f3a2f028ad5c89b-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_cf67355a,
 abstract = {We describe a neural network, called RuleNet, that learns explicit, symbolic condition-action rules in a formal string manipulation domain. RuleNet discovers functional categories over elements of the domain, and, at various points during learning, extracts rules that operate on these categories. The rules are then injected back into RuleNet and training continues, in a process called iterative projection. By incorporating rules in this way, RuleNet exhibits enhanced learning and generalization performance over alternative neural net approaches. By integrating symbolic rule learning and subsymbolic category learning, RuleNet has capabilities that go beyond a purely symbolic system. We show how this architecture can be applied to the problem of case-role assignment in natural language processing, yielding a novel rule-based solution.},
 author = {McMillan, Clayton and Mozer, Michael C and Smolensky, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cf67355a3333e6e143439161adc2d82e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cf67355a3333e6e143439161adc2d82e-Metadata.json},
 openalex = {W2170702935},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cf67355a3333e6e143439161adc2d82e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Rule Induction through Integrated Symbolic and Subsymbolic Processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/cf67355a3333e6e143439161adc2d82e-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_cfee3986,
 abstract = {Recently, high resolution images of the simultaneous representation of orientation preference, orientation selectivity and ocular dominance have been obtained for large areas in monkey striate cortex by optical imaging [1-3]. These data allow for the first time a local as well as global description of the spatial patterns and provide strong evidence for correlations between orientation selectivity and ocular dominance.

A quantitative analysis reveals that these correlations arise when a five-dimensional feature space (two dimensions for retinotopic space, one each for orientation preference, orientation specificity, and ocular dominance) is mapped into the two available dimensions of cortex while locally preserving topology. These results provide strong evidence for the concept of topology preserving maps which have been suggested as a basic design principle of striate cortex [4-7].},
 author = {Obermayer, K. and Schulten, K. and Blasdel, G. G.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cfee398643cbc3dc5eefc89334cacdc1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cfee398643cbc3dc5eefc89334cacdc1-Metadata.json},
 openalex = {W2158792143},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/cfee398643cbc3dc5eefc89334cacdc1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A comparison between a neural network model for the formation of brain maps and experimental data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/cfee398643cbc3dc5eefc89334cacdc1-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_d07e70ef,
 abstract = {We use connectionist modeling to develop an analysis of stress systems in terms of ease of learnability. In traditional linguistic analyses, learnability arguments determine default parameter settings based on the feasibilty of logically deducing correct settings from an initial state. Our approach provides an empirical alternative to such arguments. Based on perceptron learning experiments using data from nineteen human languages, we develop a novel characterization of stress patterns in terms of six parameters. These provide both a partial description of the stress pattern itself and a prediction of its learnability, without invoking abstract theoretical constructs such as metrical feet. This work demonstrates that machine learning methods can provide a fresh approach to understanding linguistic phenomena.},
 author = {Gupta, Prahlad and Touretzky, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d07e70efcfab08731a97e7b91be644de-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d07e70efcfab08731a97e7b91be644de-Metadata.json},
 openalex = {W2145123224},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d07e70efcfab08731a97e7b91be644de-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Connectionist Learning Approach to Analyzing Linguistic Stress},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/d07e70efcfab08731a97e7b91be644de-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_d18f655c,
 abstract = {Based on a general non-stationary point process model, we computed estimates of the synaptic coupling strength (efficacy) as a function of time after stimulus onset between an inhibitory interneuron and its target postsynaptic cell in the feline dorsal cochlear nucleus. The data consist of spike trains from pairs of neurons responding to brief tone bursts recorded in vivo. Our results suggest that the synaptic efficacy is non-stationary. Further. synaptic efficacy is shown to be inversely and approximately linearly related to average presynaptic spike rate. A second-order analysis suggests that the latter result is not due to non-linear interactions. Synaptic efficacy is less strongly correlated with postsynaptic rate and the correlation is not consistent across neural pairs.},
 author = {Sydorenko, Mark R. and Young, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d18f655c3fce66ca401d5f38b48c89af-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d18f655c3fce66ca401d5f38b48c89af-Metadata.json},
 openalex = {W2147229057},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d18f655c3fce66ca401d5f38b48c89af-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Stationarity of Synaptic Coupling Strength Between Neurons with Nonstationary Discharge Properties},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/d18f655c3fce66ca401d5f38b48c89af-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_d61e4bbd,
 abstract = {Recently, much interest has been generated regarding speech recognition systems based on Hidden Markov Models (HMMs) and neural network (NN) hybrids. Such systems attempt to combine the best features of both models: the temporal structure of HMMs and the discriminative power of neural networks. In this work we define a time-warping (TW) neuron that extends the operation of the formal neuron of a back-propagation network by warping the input pattern to match it optimally to its weights. We show that a single-layer network of TW neurons is equivalent to a Gaussian density HMM-based recognition system, and we propose to improve the discriminative power of this system by using back-propagation discriminative training, and/or by generalizing the structure of the recognizer to a multi-layered net. The performance of the proposed network was evaluated on a highly confusable, isolated word, multi speaker recognition task. The results indicate that not only does the recognition performance improve, but the separation between classes is enhanced also, allowing us to set up a rejection criterion to improve the confidence of the system.},
 author = {Levin, Esther and Pieraccini, Roberto and Bocchieri, Enrico},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d61e4bbd6393c9111e6526ea173a7c8b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d61e4bbd6393c9111e6526ea173a7c8b-Metadata.json},
 openalex = {W2110026519},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d61e4bbd6393c9111e6526ea173a7c8b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Time-Warping Network: A Hybrid Framework for Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/d61e4bbd6393c9111e6526ea173a7c8b-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_d64a340b,
 abstract = {We present an analysis of how the generalization performance (expected test set error) relates to the expected training set error for nonlinear learning systems, such as multilayer perceptrons and radial basis functions. The principal result is the following relationship (computed to second order) between the expected test set and training set errors: 〈etest(λ)〉ξξ′ ≈ 〈etrain(λ)〉ξ + 2σeff2 peff(λ)/n (1) Here, n is the size of the training sample ξ, σeff2 is the effective noise variance in the response variable(s), λ, is a regularization or weight decay parameter, and Peff(λ) is the effective number of parameters in the nonlinear model. The expectations 〈 〉 of training set and test set errors are taken over possible training sets ξ and training and test sets ξ′ respectively. The effective number of parameters peff(λ) usually differs from the true number of model parameters p for nonlinear or regularized models; this theoretical conclusion is supported by Monte Carlo experiments. In addition to the surprising result that peff(λ) ≠ p, we propose an estimate of (1) called the generalized prediction error (GPE) which generalizes well established estimates of prediction risk such as Akaike's F P E and AIC, Mallows Cp, and Barron's P S E to the nonlinear setting.},
 author = {Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d64a340bcb633f536d56e51874281454-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d64a340bcb633f536d56e51874281454-Metadata.json},
 openalex = {W2147299889},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/d64a340bcb633f536d56e51874281454-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Effective Number of Parameters: An Analysis of Generalization and Regularization in Nonlinear Learning Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/d64a340bcb633f536d56e51874281454-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_db85e259,
 abstract = {In single cells of the cat striate cortex, lateral inhibition across orientation and/or spatial frequency is found to enhance pre-existing biases. A contrast-dependent but spatially non-selective inhibitory component is also found. Stimulation with ascending and descending contrasts reveals the latter as a response hysteresis that is sensitive, powerful and rapid, suggesting that it is active in day-to-day vision. Both forms of inhibition are not recurrent but are rather network properties. These findings suggest two fundamental inhibitory mechanisms: a global mechanism that limits dynamic range and creates spatial selectivity through thresholding and a local mechanism that specifically refines spatial filter properties. Analysis of burst patterns in spike trains demonstrates that these two mechanisms have unique physiological origins.},
 author = {Bonds, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/db85e2590b6109813dafa101ceb2faeb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/db85e2590b6109813dafa101ceb2faeb-Metadata.json},
 openalex = {W2127685653},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/db85e2590b6109813dafa101ceb2faeb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dual Inhibitory Mechanisms for Definition of Receptive Field Characteristics in a Cat Striate Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/db85e2590b6109813dafa101ceb2faeb-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_dc6a6489,
 abstract = {We develop a model-independent method for characterizing the reliability of neural responses to brief stimuli. This approach allows us to measure the discriminability of similar stimuli, based on the real-time response of a single neuron. Neurophysiological data were obtained from a movement-sensitive neuron (H1) in the visual system of the blowfly Calliphora erythrocephala. Furthermore, recordings were made from blowfly photoreceptor cells to quantify the signal to noise ratios in the peripheral visual system. As photoreceptors form the input to the visual system, the reliability of their signals ultimately determines the reliability of any visual discrimination task. For the case of movement detection, this limit can be computed, and compared to the H1 neuron's reliability. Under favorable conditions, the performance of the H1 neuron closely approaches the theoretical limit, which means that under these conditions the nervous system adds little noise in the process of computing movement from the correlations of signals in the photoreceptor array.},
 author = {de Ruyter van Steveninck, Rob and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/dc6a6489640ca02b0d42dabeb8e46bb7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/dc6a6489640ca02b0d42dabeb8e46bb7-Metadata.json},
 openalex = {W2144721502},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Statistical Reliability of a Blowfly Movement-Sensitive Neuron},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/dc6a6489640ca02b0d42dabeb8e46bb7-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_dd458505,
 abstract = {Feedforward networks composed of units which compute a sigmoidal function of a weighted sum of their inputs have been much investigated. We tested the approximation and estimation capabilities of networks using functions more complex than sigmoids. Three classes of functions were tested: polynomials, rational functions, and flexible Fourier series. Unlike sigmoids, these classes can fit non-monotonic functions. They were compared on three problems: prediction of Boston housing prices, the sunspot count, and robot arm inverse dynamics. The complex units attained clearly superior performance on the robot arm problem, which is a highly non-monotonic, pure approximation problem. On the noisy and only mildly nonlinear Boston housing and sunspot problems, differences among the complex units were revealed; polynomials did poorly, whereas rationals and flexible Fourier series were comparable to sigmoids.},
 author = {Moody, John and Yarvin, Norman},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/dd458505749b2941217ddd59394240e8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/dd458505749b2941217ddd59394240e8-Metadata.json},
 openalex = {W2135814312},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/dd458505749b2941217ddd59394240e8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Networks with Learned Unit Response Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/dd458505749b2941217ddd59394240e8-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_df877f38,
 abstract = {Hand-printed digits can be modeled as splines that are governed by about 8 control points. For each known digit, the control points have preferred locations, and deformations of the digit are generated by moving the control points away from their home locations. Images of digits can be produced by placing Gaussian ink generators uniformly along the spline. Real images can be recognized by finding the digit model most likely to have generated the data. For each digit model we use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The model with the lowest total energy wins. If a uniform noise process is included in the model of image generation, some of the inked pixels can be rejected as noise as a digit model is fitting a poorly segmented image. The digit models learn by modifying the home locations of the control points.},
 author = {Hinton, Geoffrey E and Williams, Christopher and Revow, Michael D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/df877f3865752637daa540ea9cbc474f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/df877f3865752637daa540ea9cbc474f-Metadata.json},
 openalex = {W2162383514},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/df877f3865752637daa540ea9cbc474f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive Elastic Models for Hand-Printed Character Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/df877f3865752637daa540ea9cbc474f-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_e1e32e23,
 abstract = {The focused gamma network is proposed as one of the possible implementations of the gamma neural model. The focused gamma network is compared with the focused backpropagation network and TDNN for a time series prediction problem, and with ADALINE in a system identification problem.},
 author = {Pr\'{\i}ncipe, Jos\'{e} and de Vries, Bert and Kuo, Jyh-Ming and de Oliveira, Pedro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e1e32e235eee1f970470a3a6658dfdd5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e1e32e235eee1f970470a3a6658dfdd5-Metadata.json},
 openalex = {W2145765229},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Modeling Applications with the Focused Gamma Net},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_e2230b85,
 abstract = {Stochastic gradient descent is a general algorithm which includes LMS, on-line backpropagation, and adaptive k-means clustering as special cases. The standard choices of the learning rate η (both adaptive and fixed functions of time) often perform quite poorly. In contrast, our recently proposed class of search then converge learning rate schedules (Darken and Moody, 1990) display the theoretically optimal asymptotic convergence rate and a superior ability to escape from poor local minima. However, the user is responsible for setting a key parameter. We propose here a new methodology for creating the first completely automatic adaptive learning rates which achieve the optimal rate of convergence.},
 author = {Darken, Christian and Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e2230b853516e7b05d79744fbd4c9c13-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e2230b853516e7b05d79744fbd4c9c13-Metadata.json},
 openalex = {W2148141518},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e2230b853516e7b05d79744fbd4c9c13-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Towards Faster Stochastic Gradient Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/e2230b853516e7b05d79744fbd4c9c13-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_e44fea3b,
 abstract = {Batch gradient descent, Δw(t) = -νdE/dw(t), converges to a minimum of quadratic form with a time constant no better than 1/4λmax/λmin where λmin and λmax are the minimum and maximum eigenvalues of the Hessian matrix of E with respect to w. It was recently shown that adding a momentum term Δw(t) = -νdE/dw(t) + αΔw(t - 1) improves this to 1/4√λmax/λmin, although only in the batch case. Here we show that second-order momentum, Δw(t) = -νdE/dw(t) + αΔw(t -1) + βΔw(t - 2), can lower this no further. We then regard gradient descent with momentum as a dynamic system and explore a non quadratic error surface, showing that saturation of the error accounts for a variety of effects observed in simulations and justifies some popular heuristics.},
 author = {Pearlmutter, Barak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e44fea3bec53bcea3b7513ccef5857ac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e44fea3bec53bcea3b7513ccef5857ac-Metadata.json},
 openalex = {W2137477616},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e44fea3bec53bcea3b7513ccef5857ac-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Gradient Descent: Second Order Momentum and Saturating Error},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/e44fea3bec53bcea3b7513ccef5857ac-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_e5f6ad6c,
 abstract = {Whenever an agent learns to control an unknown environment, two opposing principles have to be combined, namely: exploration (long-term optimization) and exploitation (short-term optimization). Many real-valued connectionist approaches to learning control realize exploration by randomness in action selection. This might be disadvantageous when costs are assigned to negative experiences. The basic idea presented in this paper is to make an agent explore unknown regions in a more directed manner. This is achieved by a so-called competence map, which is trained to predict the controller's accuracy, and is used for guiding exploration. Based on this, a bistable system enables smoothly switching attention between two behaviors - exploration and exploitation - depending on expected costs and knowledge gain.

The appropriateness of this method is demonstrated by a simple robot navigation task.},
 author = {Thrun, Sebastian B. and M\"{o}ller, Knut},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e5f6ad6ce374177eef023bf5d0c018b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e5f6ad6ce374177eef023bf5d0c018b6-Metadata.json},
 openalex = {W2106639887},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Active Exploration in Dynamic Environments},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/e5f6ad6ce374177eef023bf5d0c018b6-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_e6b4b2a7,
 abstract = {G/SPLINES is an algorithm for building functional models of data. It uses genetic search to discover combinations of basis functions which are then used to build a least-squares regression model. Because it produces a population of models which evolve over time rather than a single model, it allows analysis not possible with other regression-based approaches.},
 author = {Rogers, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e6b4b2a746ed40e1af829d1fa82daa10-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e6b4b2a746ed40e1af829d1fa82daa10-Metadata.json},
 openalex = {W2133238962},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Data Analysis using G/SPLINES},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/e6b4b2a746ed40e1af829d1fa82daa10-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_e836d813,
 abstract = {The goal of perception is to extract invariant properties of the underlying world. By computing contrast at edges, the retina reduces incident light intensities spanning twelve decades to a twentyfold variation. In one stroke, it solves the dynamic range problem and extracts relative reflectivity, bringing us a step closer to the goal. We have built a contrast-sensitive silicon retina that models all major synaptic interactions in the outer-plexiform layer of the vertebrate retina using current-mode CMOS circuits: namely, reciprocal synapses between cones and horizontal cells, which produce the antagonistic center/surround receptive field, and cone and horizontal cell gap junctions, which determine its size. The chip has 90 × 92 pixels on a 6.8 × 6.9mm die in 2µm n-well technology and is fully functional.},
 author = {Boahen, Kwabena A and Andreou, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e836d813fd184325132fca8edcdfb40e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e836d813fd184325132fca8edcdfb40e-Metadata.json},
 openalex = {W2137609251},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e836d813fd184325132fca8edcdfb40e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Contrast Sensitive Silicon Retina with Reciprocal Synapses},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/e836d813fd184325132fca8edcdfb40e-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_e8c0653f,
 abstract = {Network vision systems must make inferences from evidential information across levels of representational abstraction, from low level invariants, through intermediate scene segments, to high level behaviorally relevant object descriptions. This paper shows that such networks can be realized as Markov Random Fields (MRFs). We show first how to construct an MRF functionally equivalent to a Hough transform parameter network, thus establishing a principled probabilistic basis for visual networks. Second, we show that these MRF parameter networks are more capable and flexible than traditional methods. In particular, they have a well-defined probabilistic interpretation, intrinsically incorporate feedback, and offer richer representations and decision capabilities.},
 author = {Cooper, Paul and Prokopowicz, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e8c0653fea13f91bf3c48159f7c24f78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e8c0653fea13f91bf3c48159f7c24f78-Metadata.json},
 openalex = {W2116521537},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/e8c0653fea13f91bf3c48159f7c24f78-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Markov Random Fields Can Bridge Levels of Abstraction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/e8c0653fea13f91bf3c48159f7c24f78-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_eba0dc30,
 abstract = {Automatic determination of proper neural network topology by trimming over-sized networks is an important area of study, which has previously been addressed using a variety of techniques. In this paper, we present Information Measure Based Skeletonisation (IMBS), a new approach to this problem where superfluous hidden units are removed based on their information measure (IM). This measure, borrowed from decision tree induction techniques, reflects the degree to which the hyperplane formed by a hidden unit discriminates between training data classes. We show the results of applying IMBS to three classification tasks and demonstrate that it removes a substantial number of hidden units without significantly affecting network performance.},
 author = {Ramachandran, Sowmya and Pratt, Lorien},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/eba0dc302bcd9a273f8bbb72be3a687b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/eba0dc302bcd9a273f8bbb72be3a687b-Metadata.json},
 openalex = {W2106267433},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/eba0dc302bcd9a273f8bbb72be3a687b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Information Measure Based Skeletonisation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/eba0dc302bcd9a273f8bbb72be3a687b-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_ed265bc9,
 abstract = {We propose and empirically evaluate a method for the extraction of expert-comprehensible rules from trained neural networks. Our method operates in the context of a three-step process for learning that uses rule-based domain knowledge in combination with neural networks. Empirical tests using real-worlds problems from molecular biology show that the rules our method extracts from trained neural networks: closely reproduce the accuracy of the network from which they came, are superior to the rules derived by a learning system that directly refines symbolic rules, and are expert-comprehensible.},
 author = {Towell, Geoffrey and Shavlik, Jude},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ed265bc903a5a097f61d3ec064d96d2e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ed265bc903a5a097f61d3ec064d96d2e-Metadata.json},
 openalex = {W2169273134},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Interpretation of Artificial Neural Networks: Mapping Knowledge-Based Neural Networks into Rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_ef575e88,
 abstract = {Although the detection of invariant structure in a given set of input patterns is vital to many recognition tasks, connectionist learning rules tend to focus on directions of high variance (principal components). The prediction paradigm is often used to reconcile this dichotomy; here we suggest a more direct approach to invariant learning based on an anti-Hebbian learning rule. An unsupervised two-layer network implementing this method in a competitive setting learns to extract coherent depth information from random-dot stereograms.},
 author = {Schraudolph, Nicol and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ef575e8837d065a1683c022d2077d342-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ef575e8837d065a1683c022d2077d342-Metadata.json},
 openalex = {W2141247461},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ef575e8837d065a1683c022d2077d342-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Competitive Anti-Hebbian Learning of Invariants},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/ef575e8837d065a1683c022d2077d342-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_f0e52b27,
 abstract = {Intrator (1990) proposed a feature extraction method that is related to recent statistical theory (Huber, 1985; Friedman, 1987), and is based on a biologically motivated model of neuronal plasticity (Bienenstock et al., 1982). This method has been recently applied to feature extraction in the context of recognizing 3D objects from single 2D views (Intrator and Gold, 1991). Here we describe experiments designed to analyze the nature of the extracted features, and their relevance to the theory and psychophysics of object recognition.},
 author = {Intrator, Nathan and Gold, Joshua and B\"{u}lthoff, Heinrich and Edelman, Shimon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Metadata.json},
 openalex = {W2134162430},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {3D Object Recognition Using Unsupervised Feature Extraction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/f0e52b27a7a5d6a1a87373dffa53dbe5-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_f1b6f285,
 abstract = {Because eye muscles never cocontract and do not deal with external loads, one can write an equation that relates motoneuron firing rate to eye position and velocity - a very uncommon situation in the CNS. The semicircular canals transduce head velocity in a linear manner by using a high background discharge rate, imparting linearity to the premotor circuits that generate eye movements. This has allowed deducing some of the signal processing involved, including a neural network that integrates. These ideas are often summarized by block diagrams. Unfortunately, they are of little value in describing the behavior of single neurons - a finding supported by neural network models.},
 author = {Robinson, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Metadata.json},
 openalex = {W2097088576},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Information Processing to Create Eye Movements},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/f1b6f2857fb6d44dd73c7041e0aa0f19-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_f387624d,
 abstract = {We present a Parallel Distributed Semantic (PDS) Network architecture that addresses the problems of sequencing and ambiguity resolution in natural language understanding. A PDS Network stores phrases and their meanings using multiple PDP networks, structured in the form of a semantic net. A mechanism called Propagation Filters is employed: (1) to control communication between networks, (2) to properly sequence the components of a phrase, and (3) to resolve ambiguities. Simulation results indicate that PDS Networks and Propagation Filters can successfully represent high-level knowledge, can be trained relatively quickly, and provide for parallel inferencing at the knowledge level.},
 author = {Sumida, Ronald and Dyer, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f387624df552cea2f369918c5e1e12bc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f387624df552cea2f369918c5e1e12bc-Metadata.json},
 openalex = {W2143342721},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f387624df552cea2f369918c5e1e12bc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Propagation Filters in PDS Networks for Sequencing and Ambiguity Resolution},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/f387624df552cea2f369918c5e1e12bc-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_f3f27a32,
 abstract = {A routing scheme that uses a neural network has been developed that can aid in establishing point-to-point communication routes through multistage interconnection networks (MINs). The neural network is a network of the type that was examined by Hopfield (Hopfield, 1984 and 1985). In this work, the problem of establishing routes through random MINs (RMINs) in a shared-memory, distributed computing system is addressed. The performance of the neural network routing scheme is compared to two more traditional approaches - exhaustive search routing and greedy routing. The results suggest that a neural network router may be competitive for certain RMINs.},
 author = {Goudreau, Mark and Giles, C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f3f27a324736617f20abbf2ffd806f6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f3f27a324736617f20abbf2ffd806f6d-Metadata.json},
 openalex = {W2169893641},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f3f27a324736617f20abbf2ffd806f6d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Routing for Random Multistage Interconnection Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/f3f27a324736617f20abbf2ffd806f6d-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_f4be0027,
 abstract = {Biological retinas extract spatial and temporal features in an attempt to reduce the complexity of performing visual tasks. We have built and tested a silicon retina which encodes several useful temporal features found in vertebrate retinas. The cells in our silicon retina are selective to direction, highly sensitive to positive contrast changes around an ambient light level, and tuned to a particular velocity. Inhibitory connections in the null direction perform the direction selectivity we desire. This silicon retina is on a 4.6 × 6.8mm die and consists of a 47 × 41 array of photoreceptors.},
 author = {Benson, Ronald and Delbr\"{u}ck, Tobi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f4be00279ee2e0a53eafdaa94a151e2c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f4be00279ee2e0a53eafdaa94a151e2c-Metadata.json},
 openalex = {W2114324411},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f4be00279ee2e0a53eafdaa94a151e2c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Direction Selective Silicon Retina that uses Null Inhibition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/f4be00279ee2e0a53eafdaa94a151e2c-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_f5deaeea,
 abstract = {Experimental research on Artificial Neural Network (ANN) algorithms requires either writing variations on the same program or making one monolithic program with many parameters and options. By using an object-oriented library, the size of these experimental programs is reduced while making them easier to read, write and modify. An efficient and flexible realization of this idea is Connectionist Layered Object-oriented Network Simulator (CLONES). CLONES runs on UNIX workstations and on the 100-1000 MFLOP Ring Array Processor (RAP) that we built with ANN algorithms in mind. In this report we describe CLONES and show how it is implemented on the RAP.},
 author = {Kohn, Phil and Bilmes, Jeff and Morgan, Nelson and Beck, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f5deaeeae1538fb6c45901d524ee2f98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f5deaeeae1538fb6c45901d524ee2f98-Metadata.json},
 openalex = {W2130665813},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Software for ANN training on a Ring Array Processor},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/f5deaeeae1538fb6c45901d524ee2f98-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_f5f8590c,
 abstract = {Are single neocortical neurons as powerful as multi-layered networks? A recent compartmental modeling study has shown that voltage-dependent membrane nonlinearities present in a complex dendritic tree can provide a virtual layer of local nonlinear processing elements between synaptic inputs and the final output at the cell body, analogous to a hidden layer in a multi-layer network. In this paper, an abstract model neuron is introduced, called a clusteron, which incorporates aspects of the dendritic cluster-sensitivity phenomenon seen in these detailed biophysical modeling studies. It is shown, using a clusteron, that a Hebb-type learning rule can be used to extract higher-order statistics from a set of training patterns, by manipulating the spatial ordering of synaptic connections onto the dendritic tree. The potential neurobiological relevance of these higher-order statistics for nonlinear pattern discrimination is then studied within a full compartmental model of a neocortical pyramidal cell, using a training set of 1000 high-dimensional sparse random patterns.},
 author = {Mel, Bartlett},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f5f8590cd58a54e94377e6ae2eded4d9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f5f8590cd58a54e94377e6ae2eded4d9-Metadata.json},
 openalex = {W2144994141},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Clusteron: Toward a Simple Abstraction for a Complex Neuron},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/f5f8590cd58a54e94377e6ae2eded4d9-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_f770b62b,
 abstract = {In this paper, a tree based neural network viz. MARS (Friedman, 1991) for the modelling of the yield strength of a steel rolling plate mill is described. The inputs to the time series model are temperature, strain, strain rate, and interpass time and the output is the corresponding yield stress. It is found that the MARS-based model reveals which variable's functional dependence is nonlinear, and significant. The results are compared with those obtained by using a Kalman filter based online tuning method and other classification methods, e.g. CART, C4.5, Bayesian classification. It is found that the MARS-based method consistently outperforms the other methods.},
 author = {Tsoi, Ah},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f770b62bc8f42a0b66751fe636fc6eb0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f770b62bc8f42a0b66751fe636fc6eb0-Metadata.json},
 openalex = {W2143999858},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Application of Neural Network Methodology to the Modelling of the Yield Strength in a Steel Rolling Plate Mill},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/f770b62bc8f42a0b66751fe636fc6eb0-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_fde9264c,
 abstract = {The backpropagation algorithm can be used for both recognition and generation of time trajectories. When used as a recognizer, it has been shown that the performance of a network can be greatly improved by adding structure to the architecture. The same is true in trajectory generation. In particular a new architecture corresponding to a TDNN is proposed. Results show dramatic improvement of performance in the generation of hand-written characters. A combination of TDNN and reversed TDNN for compact encoding is also suggested.},
 author = {Simard, Patrice and Le Cun, Yann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/fde9264cf376fffe2ee4ddf4a988880d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/fde9264cf376fffe2ee4ddf4a988880d-Metadata.json},
 openalex = {W2115046692},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/fde9264cf376fffe2ee4ddf4a988880d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Reverse TDNN: An Architecture For Trajectory Generation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/fde9264cf376fffe2ee4ddf4a988880d-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_ff4d5fbb,
 abstract = {Learning is posed as a problem of function estimation, for which two principles of solution are considered: empirical risk minimization and structural risk minimization. These two principles are applied to two different statements of the function estimation problem: global and local. Systematic improvements in prediction power are illustrated in application to zip-code recognition.},
 author = {Vapnik, V.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ff4d5fbbafdf976cfdc032e3bde78de5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ff4d5fbbafdf976cfdc032e3bde78de5-Metadata.json},
 openalex = {W2155195660},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ff4d5fbbafdf976cfdc032e3bde78de5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Principles of Risk Minimization for Learning Theory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/ff4d5fbbafdf976cfdc032e3bde78de5-Abstract.html},
 volume = {4},
 year = {1991}
}

@inproceedings{NIPS1991_ffeabd22,
 abstract = {Recently Linsker [2] and MacKay and Miller [3,4] have analysed Hebbian correlational rules for synaptic development in the visual system, and Miller [5,8] has studied such rules in the case of two populations of fibres (particularly two eyes). Miller's analysis has so far assumed that each of the two populations has exactly the same correlational structure. Relaxing this constraint by considering the effects of small perturbative correlations within and between eyes permits study of the stability of the solutions. We predict circumstances in which qualitative changes are seen, including the production of binocularly rather than monocularly driven units.},
 author = {Dayan, Peter and Goodhill, Geoffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ffeabd223de0d4eacb9a3e6e53e5448d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ffeabd223de0d4eacb9a3e6e53e5448d-Metadata.json},
 openalex = {W2139828410},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Perturbing Hebbian Rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/1991/hash/ffeabd223de0d4eacb9a3e6e53e5448d-Abstract.html},
 volume = {4},
 year = {1991}
}
