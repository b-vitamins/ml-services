@inproceedings{NIPS1992_00ac8ed3,
 abstract = {This paper examines and extends the work of Linsker (1986) on self organising feature detectors. Linsker concentrates on the visual processing system, but infers that the weak assumptions made will allow the model to be used in the processing of other sensory information. This claim is examined here, with special attention paid to the auditory system, where there is much lower connectivity and therefore more statistical variability. On-line training is utilised, to obtain an idea of training times. These are then compared to the time available to pre-natal mammals for the formation of feature sensitive cells.},
 author = {Walton, Lance and Bisset, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/00ac8ed3b4327bdd4ebbebcb2ba10a00-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/00ac8ed3b4327bdd4ebbebcb2ba10a00-Metadata.json},
 openalex = {W2132174985},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/00ac8ed3b4327bdd4ebbebcb2ba10a00-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Parameterising Feature Sensitive Cell Formation in Linsker Networks in the Auditory System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/00ac8ed3b4327bdd4ebbebcb2ba10a00-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_04ecb1fa,
 abstract = {Within a simple test-bed, application of feed-forward neurocontrol for short-term planning of robot trajectories in a dynamic environment is studied. The action network is embedded in a sensory-motoric system architecture that contains a separate world model. It is continuously fed with short-term predicted spatio-temporal obstacle trajectories, and receives robot state feedback. The action net allows for external switching between alternative planning tasks. It generates goal-directed motor actions - subject to the robot's kinematic and dynamic constraints - such that collisions with moving obstacles are avoided. Using supervised learning, we distribute examples of the optimal planner mapping over a structure-level adapted parsimonious higher order network. The training database is generated by a Dynamic Programming algorithm. Extensive simulations reveal, that the local planner mapping is highly nonlinear, but can be effectively and sparsely represented by the chosen powerful net model. Excellent generalization occurs for unseen obstacle configurations. We also discuss the limitations of feed-forward neurocontrol for growing planning horizons.},
 author = {Fahner, Gerald and Eckmiller, Rolf},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/04ecb1fa28506ccb6f72b12c0245ddbc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/04ecb1fa28506ccb6f72b12c0245ddbc-Metadata.json},
 openalex = {W2134859459},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/04ecb1fa28506ccb6f72b12c0245ddbc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Spatio-Temporal Planning from a Dynamic Programming Teacher: Feed-Forward Neurocontrol for Moving Obstacle Avoidance},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/04ecb1fa28506ccb6f72b12c0245ddbc-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_051e4e12,
 abstract = {Hidden Markov Models (HMMs) can be applied to several important problems in molecular biology. We introduce a new convergent learning algorithm for HMMs that, unlike the classical Baum-Welch algorithm is smooth and can be applied on-line or in batch mode, with or without the usual Viterbi most likely path approximation. Left-right HMMs with insertion and deletion states are then trained to represent several protein families including immunoglobulins and kinases. In all cases, the models derived capture all the important statistical properties of the families and can be used efficiently in a number of important tasks such as multiple alignment, motif detection, and classification.},
 author = {Baldi, Pierre and Chauvin, Yves and Hunkapiller, Tim and McClure, Marcella},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/051e4e127b92f5d98d3c79b195f2b291-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/051e4e127b92f5d98d3c79b195f2b291-Metadata.json},
 openalex = {W2128818550},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/051e4e127b92f5d98d3c79b195f2b291-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Hidden Markov Models in Molecular Biology: New Algorithms and Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/051e4e127b92f5d98d3c79b195f2b291-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_076a0c97,
 abstract = {We interpret the time interval data obtained from periodically stimulated sensory neurons in terms of two simple dynamical systems driven by noise with an embedded weak periodic function called the signal: 1) a bistable system defined by two potential wells separated by a barrier, and 2) a FitzHugh-Nagumo system. The implementation is by analog simulation: electronic circuits which mimic the dynamics. For a given signal frequency, our simulators have only two adjustable parameters, the signal and noise intensities. We show that experimental data obtained from the periodically stimulated mechanoreceptor in the crayfish tailfan can be accurately approximated by these simulations. Finally, we discuss stochastic resonance in the two models.},
 author = {Douglass, John K. and Moss, Frank and Longtin, Andr\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Metadata.json},
 openalex = {W2168513534},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Statistical and Dynamical Interpretation of ISIH Data from Periodically Stimulated Sensory Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/076a0c97d09cf1a0ec3e19c7f2529f2b-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_07a96b1f,
 abstract = {The formation of propagating spiral waves is studied in a randomly connected neural network composed of integrate-and-fire neurons with recovery period and excitatory connections using computer simulations. Network activity is initiated by periodic stimulation at a single point. The results suggest that spiral waves can arise in such a network via a sub-critical Hopf bifurcation.},
 author = {Milton, John and Chu, Po and Cowan, Jack},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/07a96b1f61097ccb54be14d6a47439b0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/07a96b1f61097ccb54be14d6a47439b0-Metadata.json},
 openalex = {W2167885267},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/07a96b1f61097ccb54be14d6a47439b0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Spiral Waves in Integrate-and-Fire Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/07a96b1f61097ccb54be14d6a47439b0-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_07c5807d,
 abstract = {We present a theory of cortico-hippocampal interaction in discrimination learning. The hippocampal region is presumed to form new stimulus representations which facilitate learning by enhancing the discriminability of predictive stimuli and compressing stimulus-stimulus redundancies. The cortical and cerebellar regions, which are the sites of long-term memory. may acquire these new representations but are not assumed to be capable of forming new representations themselves. Instantiated as a connectionist model. this theory accounts for a wide range of trial-level classical conditioning phenomena in normal (intact) and hippocampal-lesioned animals. It also makes several novel predictions which remain to be investigated empirically. The theory implies that the hippocampal region is involved in even the simplest learning tasks; although hippocampal-lesioned animals may be able to use other strategies to learn these tasks. the theory predicts that they will show consistently different patterns of transfer and generalization when the task demands change.},
 author = {Gluck, Mark and Myers, Catherine E.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/07c5807d0d927dcd0980f86024e5208b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/07c5807d0d927dcd0980f86024e5208b-Metadata.json},
 openalex = {W2170926942},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/07c5807d0d927dcd0980f86024e5208b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive Stimulus Representations: A Computational Theory of Hippocampal-Region Function},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/07c5807d0d927dcd0980f86024e5208b-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_08b255a5,
 abstract = {Artificial neural networks are comprised of an interconnected collection of certain nonlinear devices; examples of commonly used devices include linear threshold elements, sigmoidal elements and radial-basis elements. We employ results from harmonic analysis and the theory of rational approximation to obtain almost tight lower bounds on the size (i.e. number of elements) of neural networks. The class of neural networks to which our techniques can be applied is quite general; it includes any feedforward network in which each element can be piecewise approximated by a low degree rational function. For example, we prove that any depth-(d + 1) network of sigmoidal units or linear threshold elements computing the parity function of n variables must have Ω(dn1/d-≈) size, for any fixed ≈ > 0. In addition, we prove that this lower bound is almost tight by showing that the parity function can be computed with O(dn1/d) sigmoidal units or linear threshold elements in a depth-(d + 1) network. These almost tight bounds are the first known complexity results on the size of neural networks with depth more than two. Our lower bound techniques yield a unified approach to the complexity analysis of various models of neural networks with feedforward structures. Moreover, our results indicate that in the context of computing highly oscillating symmetric Boolean functions, networks of continuous-output units such as sigmoidal elements do not offer significant reduction in size compared with networks of linear threshold elements of binary outputs.},
 author = {Siu, Kai-Yeung and Roychowdhury, Vwani and Kailath, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/08b255a5d42b89b0585260b6f2360bdd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/08b255a5d42b89b0585260b6f2360bdd-Metadata.json},
 openalex = {W2158368251},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Computing with Almost Optimal Size Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/08b255a5d42b89b0585260b6f2360bdd-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_08c5433a,
 abstract = {Platt's resource-allocation network (RAN) (Platt, 1991a, 1991b) is modified for a reinforcement-learning paradigm and to existing hidden units rather than adding new units. After restarting, units continue to learn via back-propagation. The resulting restart algorithm is tested in a Q-learning network that learns to solve an inverted pendulum problem. Solutions are found faster on average with the restart algorithm than without it.},
 author = {Anderson, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/08c5433a60135c32e34f46a71175850c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/08c5433a60135c32e34f46a71175850c-Metadata.json},
 openalex = {W2165028756},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/08c5433a60135c32e34f46a71175850c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Q-Learning with Hidden-Unit Restarting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/08c5433a60135c32e34f46a71175850c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_08d98638,
 abstract = {We analyse the effects of analog noise on the synaptic arithmetic during MultiLayer Perceptron training, by expanding the cost function to include noise-mediated penalty terms. Predictions are made in the light of these calculations which suggest that fault tolerance, generalisation ability and learning trajectory should be improved by such noise-injection. Extensive simulation experiments on two distinct classification problems substantiate the claims. The results appear to be perfectly general for all training schemes where weights are adjusted incrementally, and have wide-ranging implications for all applications, particularly those involving inaccurate analog neural VLSI.},
 author = {Murray, Alan and Edwards, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/08d98638c6fcd194a4b1e6992063e944-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/08d98638c6fcd194a4b1e6992063e944-Metadata.json},
 openalex = {W2118784266},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Synaptic Weight Noise During MLP Learning Enhances Fault-Tolerance, Generalization and Learning Trajectory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/08d98638c6fcd194a4b1e6992063e944-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_0cb929ea,
 abstract = {A connection is drawn between rational functions, the realization theory of dynamical systems, and feedforward neural networks. This allows us to parametrize single hidden layer scalar neural networks with (almost) arbitrary analytic activation functions in terms of strictly proper rational functions. Hence, we can solve the uniqueness of parametrization problem for such networks.},
 author = {Helmke, Uwe and Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0cb929eae7a499e50248a3a78f7acfc7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0cb929eae7a499e50248a3a78f7acfc7-Metadata.json},
 openalex = {W2129965880},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0cb929eae7a499e50248a3a78f7acfc7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Rational Parametrizations of Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/0cb929eae7a499e50248a3a78f7acfc7-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_0d7de1ac,
 abstract = {We present a general formulation for a network of stochastic directional units. This formulation is an extension of the Boltzmann machine in which the units are not binary, but take on values in a cyclic range, between 0 and 2π radians. The state of each unit in a Directional-Unit Boltzmann Machine (DUBM) is described by a complex variable, where the phase component specifies a direction; the weights are also complex variables. We associate a quadratic energy function, and corresponding probability, with each DUBM configuration. The conditional distribution of a unit's stochastic state is a circular version of the Gaussian probability distribution, known as the von Mises distribution. In a mean-field approximation to a stochastic DUBM, the phase component of a unit's state represents its mean direction, and the magnitude component specifies the degree of certainty associated with this direction. This combination of a value and a certainty provides additional representational power in a unit. We describe a learning algorithm and simulations that demonstrate a mean-field DUBM'S ability to learn interesting mappings.},
 author = {Zemel, Richard and Williams, Christopher and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0d7de1aca9299fe63f3e0041f02638a3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0d7de1aca9299fe63f3e0041f02638a3-Metadata.json},
 openalex = {W2095801082},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0d7de1aca9299fe63f3e0041f02638a3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Directional-Unit Boltzmann Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/0d7de1aca9299fe63f3e0041f02638a3-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_0f966132,
 abstract = {Multiple single neuron responses were recorded from a single electrode in V1 of alert, behaving monkeys. Drifting sinusoidal gratings were presented in the cells' overlapping receptive fields, and the stimulus was varied along several visual dimensions. The degree of dimensional separability was calculated for a large population of neurons, and found to be a continuum. Several cells showed different temporal response dependencies to variation of different stimulus dimensions, i.e. the tuning of the modulated firing was not necessarily the same as that of the mean firing rate. We describe a multidimensional receptive field, and use simultaneously recorded responses to compute a multi-neuron receptive field, describing the information processing capabilities of a group of cells. Using dynamic correlation analysis, we propose several computational schemes for multidimensional spatiotemporal tuning for groups of cells. The implications for neuronal coding of stimuli are discussed.},
 author = {Stern, Edward and Aertsen, Ad and Vaadia, Eilon and Hochstein, Shaul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0f96613235062963ccde717b18f97592-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0f96613235062963ccde717b18f97592-Metadata.json},
 openalex = {W2128566102},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0f96613235062963ccde717b18f97592-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Stimulus Encoding by Multidimensional Receptive Fields in Single Cells and Cell Populations in V1 of Awake Monkey},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/0f96613235062963ccde717b18f97592-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_0ff39bbb,
 abstract = {We present a novel classification and regression method that combines exploratory projection pursuit (unsupervised training) with projection pursuit regression (supervised training), to yield a new family of cost/complexity penalty terms. Some improved generalization properties are demonstrated on real world problems.},
 author = {Intrator, Nathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0ff39bbbf981ac0151d340c9aa40e63e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0ff39bbbf981ac0151d340c9aa40e63e-Metadata.json},
 openalex = {W2159119809},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {On the Use of Projection Pursuit Constraints for Training Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/0ff39bbbf981ac0151d340c9aa40e63e-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_0ff8033c,
 abstract = {The bootstrap algorithm is a computational intensive procedure to derive nonparametric confidence intervals of statistical estimators in situations where an analytic solution is intractable. It is applied to neural networks to estimate the predictive distribution for unseen inputs. The consistency of different bootstrap procedures and their convergence speed is discussed. A small scale simulation experiment shows the applicability of the bootstrap to practical problems and its potential use.},
 author = {Paass, Gerhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0ff8033cf9437c213ee13937b1c4c455-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0ff8033cf9437c213ee13937b1c4c455-Metadata.json},
 openalex = {W2160920198},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Assessing and Improving Neural Network Predictions by the Bootstrap Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/0ff8033cf9437c213ee13937b1c4c455-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_109a0ca3,
 abstract = {In the presence of outliers, the existing self-organizing rules for Principal Component Analysis (PCA) perform poorly. Using statistical physics techniques including the Gibbs distribution, binary decision fields and effective energies, we propose self-organizing PCA rules which are capable of resisting outliers while fulfilling various PCA-related tasks such as obtaining the first principal component vector, the first k principal component vectors, and directly finding the subspace spanned by the first k vector principal component vectors without solving for each vector individually. Comparative experiments have shown that the proposed robust rules improve the performances of the existing PCA algorithms significantly when outliers are present.},
 author = {Xu, Lei and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/109a0ca3bc27f3e96597370d5c8cf03d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/109a0ca3bc27f3e96597370d5c8cf03d-Metadata.json},
 openalex = {W2124460284},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/109a0ca3bc27f3e96597370d5c8cf03d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Self-Organizing Rules for Robust Principal Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/109a0ca3bc27f3e96597370d5c8cf03d-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_10a5ab2d,
 abstract = {Hidden units in multi-layer networks form a representation space in which each region can be identified with a class of equivalent outputs (Elman, 1989) or a logical state in a finite state machine (Cleeremans, Servan-Schreiber & McClelland, 1989; Giles, Sun, Chen, Lee, & Chen, 1990). We extend the analysis of the spatial structure of hidden unit space to a combinatorial task, based on binding features together in a visual scene. The logical structure requires a combinatorial number of states to represent all valid scenes. On analysing our networks, we find that the high dimensionality of hidden unit space is exploited by using the intersection of neighboring regions to represent conjunctions of features. These results show how combinatorial structure can be based on the spatial nature of networks, and not just on their emulation of logical structure.},
 author = {Wiles, Janet and Ollila, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/10a5ab2db37feedfdeaab192ead4ac0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/10a5ab2db37feedfdeaab192ead4ac0e-Metadata.json},
 openalex = {W2106522089},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Intersecting regions: The Key to combinatorial structure in hidden unit space},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/10a5ab2db37feedfdeaab192ead4ac0e-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_1595af64,
 abstract = {Typical methods for gradient descent in neural network learning involve calculation of derivatives based on a detailed knowledge of the network model. This requires extensive, time consuming calculations for each pattern presentation and high precision that makes it difficult to implement in VLSI. We present here a perturbation technique that measures, not calculates, the gradient. Since the technique uses the actual network as a measuring device, errors in modeling neuron activation and synaptic weights do not cause errors in gradient descent. The method is parallel in nature and easy to implement in VLSI. We describe the theory of such an algorithm, an analysis of its domain of applicability, some simulations using it and an outline of a hardware implementation.},
 author = {Alspector, J. and Meir, R. and Yuhas, B. and Jayakumar, A. and Lippe, D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/1595af6435015c77a7149e92a551338e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/1595af6435015c77a7149e92a551338e-Metadata.json},
 openalex = {W2162811812},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/1595af6435015c77a7149e92a551338e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Parallel Gradient Descent Method for Learning in Analog VLSI Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/1595af6435015c77a7149e92a551338e-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_17c276c8,
 abstract = {Simplified models of the lateral geniculate nucles (LGN) and striate cortex illustrate the possibility that feedback to the LGN may be used for robust, low-level pattern analysis. The information fed back to the LGN is rebroadcast to cortex using the LGN's full fan-out, so the cortex→LGN→cortex pathway mediates extensive cortico-cortical communication while keeping the number of necessary connections small.},
 author = {Brody, Carlos},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/17c276c8e723eb46aef576537e9d56d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/17c276c8e723eb46aef576537e9d56d0-Metadata.json},
 openalex = {W2150747610},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Model of Feedback to the Lateral Geniculate Nucleus},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/17c276c8e723eb46aef576537e9d56d0-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_185c29dc,
 abstract = {AbstractWe present a methodological framework enabling a detailed description of the performance of Hopfield-like attractor neural networks (ANN) in the first two iterations. Using the Bayesian approach, we find that performance is improved when a history-based term is included in the neuron's dynamics. A further enhancement of the network's performance is achieved by judiciously choosing the censored neurons (those which become active in a given iteration) on the basis of the magnitude of their post-synaptic potentials. The contribution of biologically plausible, censored, history-dependent dynamics is especially marked in conditions of low firing activity and sparse connectivity, two important characteristics of the mammalian cortex. In such networks, the performance attained is higher than the performance of two ‘independent’ iterations, which represents an upper bound on the performance of history-independent networks.},
 author = {Meilijson, Isaac and Ruppin, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/185c29dc24325934ee377cfda20e414c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/185c29dc24325934ee377cfda20e414c-Metadata.json},
 openalex = {W2149508600},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/185c29dc24325934ee377cfda20e414c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {History-dependent attractor neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/185c29dc24325934ee377cfda20e414c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_192fc044,
 abstract = {We analyze in detail the performance of a Hamming network classifying inputs that are distorted versions of one of its m stored memory patterns, each being a binary vector of length n. It is shown that the activation function of the memory neurons in the original Hamming network may be replaced by a simple threshold function. By judiciously determining the threshold value, the "winner-take-all" subnet of the Hamming network (known to be the essential factor determining the time complexity of the network's computation) may be altogether discarded. For m growing exponentially in n, the resulting threshold Hamming network correctly classifies the input pattern in a single iteration, with probability approaching 1.},
 author = {Meilijson, Isaac and Ruppin, Eytan and Sipper, Moshe},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/192fc044e74dffea144f9ac5dc9f3395-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/192fc044e74dffea144f9ac5dc9f3395-Metadata.json},
 openalex = {W2156968760},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A single-iteration threshold Hamming network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/192fc044e74dffea144f9ac5dc9f3395-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_19bc9161,
 abstract = {Recent research on reinforcement learning has focused on algorithms based on the principles of Dynamic Programming (DP). One of the most promising areas of application for these algorithms is the control of dynamical systems, and some impressive results have been achieved. However, there are significant gaps between practice and theory. In particular, there are no convergence proofs for problems with continuous state and action spaces, or for systems involving non-linear function approximators (such as multilayer perceptrons). This paper presents research applying DP-based reinforcement learning theory to Linear Quadratic Regulation (LQR), an important class of control problems involving continuous state and action spaces and requiring a simple type of non-linear function approximator. We describe an algorithm based on Q-learning that is proven to converge to the optimal controller for a large class of LQR problems. We also describe a slightly different algorithm that is only locally convergent to the optimal Q-function, demonstrating one of the possible pitfalls of using a non-linear function approximator with DP-based learning.},
 author = {Bradtke, Steven},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/19bc916108fc6938f52cb96f7e087941-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/19bc916108fc6938f52cb96f7e087941-Metadata.json},
 openalex = {W2121832485},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/19bc916108fc6938f52cb96f7e087941-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Reinforcement Learning Applied to Linear Quadratic Regulation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/19bc916108fc6938f52cb96f7e087941-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_1ecfb463,
 abstract = {The classical computational model for stereo vision incorporates a uniqueness inhibition constraint to enforce a one-to-one feature match, thereby sacrificing the ability to handle transparency. Critics of the model disregard the uniqueness constraint and argue that the smoothness constraint can provide the excitation support required for transparency computation. However, this modification fails in neighborhoods with sparse features. We propose a Bayesian approach to stereo vision with priors favoring cohesive over transparent surfaces. The disparity and its segmentation into a multi-layer depth planes representation are simultaneously computed. The smoothness constraint propagates support within each layer, providing mutual excitation for non-neighboring transparent or partially occluded regions. Test results for various random-dot and other stereograms are presented.},
 author = {Madarasmi, Suthep and Kersten, Daniel and Pong, Ting-Chuen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/1ecfb463472ec9115b10c292ef8bc986-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/1ecfb463472ec9115b10c292ef8bc986-Metadata.json},
 openalex = {W2155559900},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/1ecfb463472ec9115b10c292ef8bc986-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Computation of Stereo Disparity for Transparent and for Opaque Surfaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/1ecfb463472ec9115b10c292ef8bc986-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_2291d2ec,
 abstract = {We are developing a forecaster for daily extremes of demand for electric power encountered in the service area of a large midwestern utility and using this application as a testbed for approaches to input dimension reduction and decomposition of network training. Projection pursuit regression representations and the ability of algorithms like SIR to quickly find reasonable weighting vectors enable us to confront the vexing architecture selection problem by reducing high-dimensional gradient searchs to fitting single-input single-output (SISO) subnets. We introduce dimension reduction algorithms, to select features or relevant subsets of a set of many variables, based on minimizing an index of level-set dispersions (closely related to a projection index and to SIR), and combine them with backfitting to implement a neural network version of projection pursuit. The performance achieved by our approach, when trained on 1989, 1990 data and tested on 1991 data, is comparable to that achieved in our earlier study of backpropagation trained networks.},
 author = {Yuan, Jen-Lun and Fine, Terrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Metadata.json},
 openalex = {W2132635375},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Forecasting Demand for Electric Power},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_23350907,
 abstract = {The real time computation of motion from real images using a single chip with integrated sensors is a hard problem. We present two analog VLSI schemes that use pulse domain neuromorphic circuits to compute motion. Pulses of variable width, rather than graded potentials, represent a natural medium for evaluating temporal relationships. Both algorithms measure speed by timing a moving edge in the image. Our first model is inspired by Reichardt's algorithm in the fly and yields a non-monotonic response vs. velocity curve. We present data from a chip that implements this model. Our second algorithm yields a monotonic response vs. velocity curve and is currently being translated into silicon.},
 author = {Sarpeshkar, Rahul and Bair, Wyeth and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/233509073ed3432027d48b1a83f5fbd2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/233509073ed3432027d48b1a83f5fbd2-Metadata.json},
 openalex = {W2100690131},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/233509073ed3432027d48b1a83f5fbd2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Visual Motion Computation in Analog VLSI Using Pulses},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/233509073ed3432027d48b1a83f5fbd2-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_24681928,
 abstract = {A peg-in-hole insertion task is used as an example to illustrate the utility of direct associative reinforcement learning methods for learning control under real-world conditions of uncertainty and noise. Task complexity due to the use of an unchamfered hole and a clearance of less than 0.2mm is compounded by the presence of positional uncertainty of magnitude exceeding 10 to 50 times the clearance. Despite this extreme degree of uncertainty, our results indicate that direct reinforcement learning can be used to learn a robust reactive control strategy that results in skillful peg-in-hole insertions.},
 author = {Gullapalli, Vijaykumar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/24681928425f5a9133504de568f5f6df-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/24681928425f5a9133504de568f5f6df-Metadata.json},
 openalex = {W2157580235},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/24681928425f5a9133504de568f5f6df-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Control Under Extreme Uncertainty},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/24681928425f5a9133504de568f5f6df-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_26408ffa,
 abstract = {Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases.},
 author = {Simard, Patrice and LeCun, Yann and Denker, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/26408ffa703a72e8ac0117e74ad46f33-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/26408ffa703a72e8ac0117e74ad46f33-Metadata.json},
 openalex = {W2137291015},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/26408ffa703a72e8ac0117e74ad46f33-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Efficient Pattern Recognition Using a New Transformation Distance},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/26408ffa703a72e8ac0117e74ad46f33-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_26dd0dbc,
 abstract = {We propose in this paper a statistical model (planar hidden Markov model - PHMM) describing statistical properties of images. The model generalizes the single-dimensional HMM, used for speech processing, to the planar case. For this model to be useful an efficient segmentation algorithm, similar to the Viterbi algorithm for HMM, must exist. We present conditions in terms of the PHMM parameters that are sufficient to guarantee that the planar segmentation problem can be solved in polynomial time, and describe an algorithm for that. This algorithm aligns optimally the image with the model, and therefore is insensitive to elastic distortions of images. Using this algorithm a joint optimal segmentation and recognition of the image can be performed, thus overcoming the weakness of traditional OCR systems where segmentation is performed independently before the recognition leading to unrecoverable recognition errors.

The PHMM approach was evaluated using a set of isolated handwritten digits. An overall digit recognition accuracy of 95% was achieved. An analysis of the results showed that even in the simple case of recognition of isolated characters, the elimination of elastic distortions enhances the performance Significantly. We expect that the advantage of this approach will be even more significant for tasks such as connected writing recognition/spotting, for which there is no known high accuracy method of recognition.},
 author = {Levin, Esther and Pieraccini, Roberto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/26dd0dbc6e3f4c8043749885523d6a25-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/26dd0dbc6e3f4c8043749885523d6a25-Metadata.json},
 openalex = {W2113029692},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/26dd0dbc6e3f4c8043749885523d6a25-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Planar Hidden Markov Modeling: From Speech to Optical Character Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/26dd0dbc6e3f4c8043749885523d6a25-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_291597a1,
 abstract = {The inverse kinematics problem for redundant manipulators is ill-posed and nonlinear. There are two fundamentally different issues which result in the need for some form of regularization; the existence of multiple solution branches (global ill-posedness) and the existence of excess degrees of freedom (local ill-posedness). For certain classes of manipulators, learning methods applied to input-output data generated from the forward function can be used to globally regularize the problem by partitioning the domain of the forward mapping into a finite set of regions over which the inverse problem is well-posed. Local regularization can be accomplished by an appropriate parameterization of the redundancy consistently over each region. As a result, the ill-posed problem can be transformed into a finite set of well-posed problems. Each can then be solved separately to construct approximate direct inverse functions.},
 author = {DeMers, David and Kreutz-Delgado, Kenneth},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/291597a100aadd814d197af4f4bab3a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/291597a100aadd814d197af4f4bab3a7-Metadata.json},
 openalex = {W2131531084},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Global Regularization of Inverse Kinematics for Redundant Manipulators},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/291597a100aadd814d197af4f4bab3a7-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_2dea61ee,
 abstract = {We describe two successfully working, analog VLSI vision circuits that move beyond pixel-based early vision algorithms. One circuit, implementing the dynamic wires model, provides for dedicated lines of communication among groups of pixels that share a common property. The chip uses the dynamic wires model to compute the arclength of visual contours. Another circuit labels all points inside a given contour with one voltage and all other with another voltage. Its behavior is very robust, since small breaks in contours are automatically sealed, providing for Figure-Ground segregation in a noisy environment. Both chips are implemented using networks of resistors and switches and represent a step towards object level processing since a single voltage value encodes the property of an ensemble of pixels.},
 author = {Koch, Christof and Mathur, Binnal and Liu, Shih-Chii and Harris, John and Luo, Jin and Sivilotti, Massimo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/2dea61eed4bceec564a00115c4d21334-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/2dea61eed4bceec564a00115c4d21334-Metadata.json},
 openalex = {W2129834190},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/2dea61eed4bceec564a00115c4d21334-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Object-Based Analog VLSI Vision Circuits},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/2dea61eed4bceec564a00115c4d21334-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_2f37d101,
 abstract = {Which processes underly our ability to quickly recognize familiar objects within a complex visual input scene? In this paper an implemented neural network model is described that attempts to specify how selective visual attention, perceptual organisation, and invariance transformations might work together in order to segment, select, and recognize objects out of complex input scenes containing multiple, possibly overlapping objects. Retinotopically organized feature maps serve as input for two main processing routes: the 'where-pathway' dealing with location information and the 'what-pathway' computing the shape and attributes of objects. A location-based attention mechanism operates on an early stage of visual processing selecting a contigous region of the visual field for preferential processing. Additionally, location-based attention plays an important role for invariant object recognition controling appropriate normalization processes within the what-pathway. Object recognition is supported through the segmentation of the visual field into distinct entities. In order to represent different segmented entities at the same time, the model uses an oscillatory binding mechanism. Connections between the where-pathway and the what-pathway lead to a flexible cooperation between different functional subsystems producing an overall behavior which is consistent with a variety of psychophysical data.},
 author = {Goebel, Rainer},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/2f37d10131f2a483a8dd005b3d14b0d9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/2f37d10131f2a483a8dd005b3d14b0d9-Metadata.json},
 openalex = {W2143558878},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Perceiving Complex Visual Scenes: An Oscillator Neural Network Model that Integrates Selective Attention, Perceptual Organisation, and Invariant Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/2f37d10131f2a483a8dd005b3d14b0d9-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_303ed4c6,
 abstract = {We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Solla, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-1 from training data and structural information of the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weight decay on three benchmark MONK's problems [Thrun et al., 1991]. Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987] used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1560 weights, yielding better generalization.},
 author = {Hassibi, Babak and Stork, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Metadata.json},
 openalex = {W2125389748},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Second order derivatives for network pruning: Optimal Brain Surgeon},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_30bb3825,
 author = {LeCun, Yann and Simard, Patrice and Pearlmutter, Barak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/30bb3825e8f631cc6075c0f87bb4978c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/30bb3825e8f631cc6075c0f87bb4978c-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/30bb3825e8f631cc6075c0f87bb4978c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Automatic Learning Rate Maximization by On-Line Estimation of the Hessian\textquotesingle s Eigenvectors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/30bb3825e8f631cc6075c0f87bb4978c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_30ef30b6,
 abstract = {We have designed a neural network which detects the direction of egomotion from optic flow in the presence of eye movements (Lappe and Rauschecker, 1993). The performance of the network is consistent with human psychophysical data, and its output neurons show great similarity to triple component cells in area MSTd of monkey visual cortex. We now show that by using assumptions about the kind of eye movements that the observer is likely to perform, our model can generate various other cell types found in MSTd as well.},
 author = {Lappe, Markus and Rauschecker, Josef},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/30ef30b64204a3088a26bc2e6ecf7602-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/30ef30b64204a3088a26bc2e6ecf7602-Metadata.json},
 openalex = {W2126223776},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/30ef30b64204a3088a26bc2e6ecf7602-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Computation of Heading Direction from Optic Flow in Visual Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/30ef30b64204a3088a26bc2e6ecf7602-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_3328bdf9,
 abstract = {So far there has been no general method for relating extracellular electrophysiological measured activity of neurons in the associative cortex to underlying network or cognitive states. We propose to model such data using a multivariate Poisson Hidden Markov Model. We demonstrate the application of this approach for temporal segmentation of the firing patterns, and for characterization of the cortical responses to external stimuli. Using such a statistical model we can significantly discriminate two behavioral modes of the monkey, and characterize them by the different firing patterns, as well as by the level of coherency of their multi-unit firing activity.

Our study utilized measurements carried out on behaving Rhesus monkeys by M. Abeles, E. Vaadia, and H. Bergman, of the Hadassa Medical School of the Hebrew University.},
 author = {Gat, Itay and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3328bdf9a4b9504b9398284244fe97c2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3328bdf9a4b9504b9398284244fe97c2-Metadata.json},
 openalex = {W2126170855},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3328bdf9a4b9504b9398284244fe97c2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Statistical Modeling of Cell Assemblies Activities in Associative Cortex of Behaving Monkeys},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/3328bdf9a4b9504b9398284244fe97c2-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_3435c378,
 abstract = {Basic connectionist principles imply that grammars should take the form of systems of parallel soft constraints defining an optimization problem the solutions to which are the well-formed structures in the language. Such Harmonic Grammars have been successfully applied to a number of problems in the theory of natural languages. Here it is shown that formal languages too can be specified by Harmonic Grammars, rather than by conventional serial rewrite rule systems.},
 author = {Smolensky, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3435c378bb76d4357324dd7e69f3cd18-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3435c378bb76d4357324dd7e69f3cd18-Metadata.json},
 openalex = {W2166930810},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Harmonic Grammars for Formal Languages},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/3435c378bb76d4357324dd7e69f3cd18-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_3493894f,
 abstract = {Several research groups are implementing analog integrated circuit models of biological auditory processing. The outputs of these circuit models have taken several forms, including video format for monitor display, simple scanned output for oscilloscope display, and parallel analog outputs suitable for data-acquisition systems. Here, an alternative output method for silicon auditory models, suitable for direct interface to digital computers, is described. As a prototype of this method, an integrated circuit model of temporal adaptation in the auditory nerve that functions as a peripheral to a workstation running Unix is described. Data from a working hybrid system that includes the auditory model, a digital interface, and asynchronous software are given. This system produces a real-time X-window display of the response of the auditory nerve model.},
 author = {Lazzaro, John and Wawrzynek, John and Mahowald, M. and Sivilotti, Massimo and Gillespie, Dave},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3493894fa4ea036cfc6433c3e2ee63b0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3493894fa4ea036cfc6433c3e2ee63b0-Metadata.json},
 openalex = {W2129066464},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3493894fa4ea036cfc6433c3e2ee63b0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Silicon auditory processors as computer peripherals},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/3493894fa4ea036cfc6433c3e2ee63b0-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_3871bd64,
 abstract = {We analyze the by algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of thresholded smooth functions.},
 author = {Freund, Yoav and Seung, H. Sebastian and Shamir, Eli and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3871bd64012152bfb53fdf04b401193f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3871bd64012152bfb53fdf04b401193f-Metadata.json},
 openalex = {W2110327402},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Information, Prediction, and Query by Committee},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/3871bd64012152bfb53fdf04b401193f-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_3a066bda,
 abstract = {We are interested in the use of analog neural networks for recognizing visual objects. Objects are described by the set of parts they are composed of and their structural relationship. Structural models are stored in a database and the recognition problem reduces to matching data to models in a structurally consistent way. The object recognition problem is in general very difficult in that it involves coupled problems of grouping, segmentation and matching. We limit the problem here to the simultaneous labelling of the parts of a single object and the determination of analog parameters. This coupled problem reduces to a weighted match problem in which an optimizing neural network must minimize E(M, p) = Σαi Mαi Wαi(p), where the {Mαi} are binary match variables for data parts i to model parts α and {Wαi(P)} are weights dependent on parameters p. In this work we show that by first solving for estimates p without solving for Mαi, we may obtain good initial parameter estimates that yield better solutions for M and p.},
 author = {Utans, Joachim and Gindi, Gene},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3a066bda8c96b9478bb0512f0a43028c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3a066bda8c96b9478bb0512f0a43028c-Metadata.json},
 openalex = {W2162044963},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3a066bda8c96b9478bb0512f0a43028c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Improving Convergence in Hierarchical Matching Networks for Object Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/3a066bda8c96b9478bb0512f0a43028c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_3d2d8ccb,
 abstract = {We describe a model of visual word recognition that accounts for several aspects of the temporal processing of sequences of briefly presented words. The model utilizes a new representation for written words, based on dynamic time warping and multidimensional scaling. The visual input passes through cascaded perceptual, comparison, and detection stages. We describe how these dynamical processes can account for several aspects of word recognition, including repetition priming and repetition blindness.},
 author = {Bavelier, Daphne and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3d2d8ccb37df977cb6d9da15b76c3f3a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3d2d8ccb37df977cb6d9da15b76c3f3a-Metadata.json},
 openalex = {W2119656315},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/3d2d8ccb37df977cb6d9da15b76c3f3a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A dynamical model of priming and repetition blindness},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/3d2d8ccb37df977cb6d9da15b76c3f3a-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_42e77b63,
 abstract = {We present the information-theoretic derivation of a learning algorithm that clusters unlabelled data with linear discriminants. In contrast to methods that try to preserve information about the input patterns, we maximize the information gained from observing the output of robust binary discriminators implemented with sigmoid nodes. We derive a local weight adaptation rule via gradient ascent in this objective, demonstrate its dynamics on some simple data sets, relate our approach to previous work and suggest directions in which it may be extended.},
 author = {Schraudolph, Nicol and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/42e77b63637ab381e8be5f8318cc28a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/42e77b63637ab381e8be5f8318cc28a2-Metadata.json},
 openalex = {W2124817648},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/42e77b63637ab381e8be5f8318cc28a2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Unsupervised Discrimination of Clustered Data via Optimization of Binary Information Gain},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/42e77b63637ab381e8be5f8318cc28a2-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_443cb001,
 abstract = {In this paper, we discuss on-line estimation strategies that model the optimal value function of a typical optimal control problem. We present a general strategy that uses local corridor solutions obtained via dynamic programming to provide local optimal control sequence training data for a neural architecture model of the optimal value function.},
 author = {Peterson, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/443cb001c138b2561a0d90720d6ce111-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/443cb001c138b2561a0d90720d6ce111-Metadata.json},
 openalex = {W2106849842},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/443cb001c138b2561a0d90720d6ce111-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {On-Line Estimation of the Optimal Value Function: HJB- Estimators},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/443cb001c138b2561a0d90720d6ce111-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_44c4c173,
 abstract = {We present a local learning rule in which Hebbian learning is conditional on an incorrect prediction of a reinforcement signal. We propose a biological interpretation of such a framework and display its utility through examples in which the reinforcement signal is cast as the delivery of a neuromodulator to its target. Three examples are presented which illustrate how this framework can be applied to the development of the oculomotor system.},
 author = {Montague, P. and Dayan, P. and Nowlan, S.J. and Pouget, A and Sejnowski, T.J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/44c4c17332cace2124a1a836d9fc4b6f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/44c4c17332cace2124a1a836d9fc4b6f-Metadata.json},
 openalex = {W2137441310},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/44c4c17332cace2124a1a836d9fc4b6f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Using Aperiodic Reinforcement for Directed Self-Organization During Development},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/44c4c17332cace2124a1a836d9fc4b6f-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_45645a27,
 abstract = {We present a neural net architecture that can discover hierarchical and recursive structure in symbol strings. To detect structure at multiple levels, the architecture has the capability of reducing symbols substrings to single symbols, and makes use of an external stack memory. In terms of formal languages, the architecture can learn to parse strings in an LR(O) context-free grammar. Given training sets of positive and negative exemplars, the architecture has been trained to recognize many different grammars. The architecture has only one layer of modifiable weights, allowing for a straightforward interpretation of its behavior.},
 author = {Mozer, Michael C and Das, Sreerupa},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/45645a27c4f1adc8a7a835976064a86d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/45645a27c4f1adc8a7a835976064a86d-Metadata.json},
 openalex = {W2144782183},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/45645a27c4f1adc8a7a835976064a86d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Connectionist Symbol Manipulator That Discovers the Structure of Context-Free Languages},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/45645a27c4f1adc8a7a835976064a86d-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_48ab2f9b,
 abstract = {Neural network models have been criticized for their inability to make use of compositional representations. In this paper, we describe a series of psychological phenomena that demonstrate the role of structured representations in cognition. These findings suggest that people compare relational representations via a process of structural alignment. This process will have to be captured by any model of cognition, symbolic or subsymbolic.},
 author = {Gentner, Dedre and Markman, Arthur},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/48ab2f9b45957ab574cf005eb8a76760-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/48ab2f9b45957ab574cf005eb8a76760-Metadata.json},
 openalex = {W2129882871},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analogy-- Watershed or Waterloo? Structural alignment and the development of connectionist models of analogy},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/48ab2f9b45957ab574cf005eb8a76760-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_4a47d298,
 abstract = {This paper presents a neural network able to control saccadic movements. The input to the network is a specification of a stimulation site on the collicular motor map. The output is the time course of the eye position in the orbit (horizontal and vertical angles). The units in the network exhibit a one-to-one correspondance with neurons in the intermediate layer of the superior colliculus (collicular motor map), in the brainstem and with oculomotor neurons. Simulations carried out with this network demonstrate its ability to reproduce in a straightforward fashion many experimental observations.},
 author = {Massone, Lina L.E.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/4a47d2983c8bd392b120b627e0e1cab4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/4a47d2983c8bd392b120b627e0e1cab4-Metadata.json},
 openalex = {W2123470896},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/4a47d2983c8bd392b120b627e0e1cab4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Recurrent Neural Network for Generation of Occular Saccades},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/4a47d2983c8bd392b120b627e0e1cab4-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_4c27cea8,
 abstract = {We demonstrate in this paper how certain forms of rule-based knowledge can be used to prestructure a neural network of normalized basis functions and give a probabilistic interpretation of the network architecture. We describe several ways to assure that rule-based knowledge is preserved during training and present a method for complexity reduction that tries to minimize the number of rules and the number of conjuncts. After training the refined rules are extracted and analyzed.},
 author = {Tresp, Volker and Hollatz, J\"{u}rgen and Ahmad, Subutai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/4c27cea8526af8cfee3be5e183ac9605-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/4c27cea8526af8cfee3be5e183ac9605-Metadata.json},
 openalex = {W2147684693},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Network Structuring and Training Using Rule-based Knowledge},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/4c27cea8526af8cfee3be5e183ac9605-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_4ffce04d,
 abstract = {We demonstrate the use of a digital signal processing board to construct hybrid networks consisting of computer model neurons connected to a biological neural network. This system operates in real time, and the synaptic connections are realistic effective conductances. Therefore, the synapses made from the computer model neuron are integrated correctly by the postsynaptic biological neuron. This method provides us with the ability to add additional, completely known elements to a biological network and study their effect on network activity. Moreover, by changing the parameters of the model neuron, it is possible to assess the role of individual conductances in the activity of the neuron, and in the network in which it participates.},
 author = {Masson, Sylvie and Le Masson, Gwendal and Marder, Eve and Abbott, L.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/4ffce04d92a4d6cb21c1494cdfcd6dc1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/4ffce04d92a4d6cb21c1494cdfcd6dc1-Metadata.json},
 openalex = {W2121512596},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/4ffce04d92a4d6cb21c1494cdfcd6dc1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Hybrid Circuits of Interacting Computer Model and Biological Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/4ffce04d92a4d6cb21c1494cdfcd6dc1-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_500e75a0,
 abstract = {We propose a model of the development of geometric reasoning in children that explicitly involves learning. The model uses a neural network that is initialized with an understanding of geometry similar to that of second-grade children. Through the presentation of a series of examples, the model is shown to develop an understanding of geometry similar to that of fifth-grade children who were trained using similar materials.},
 author = {Towell, Geoffrey and Lehrer, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/500e75a036dc2d7d2fec5da1b71d36cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/500e75a036dc2d7d2fec5da1b71d36cc-Metadata.json},
 openalex = {W2143889994},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Knowledge-Based Model of Geometry Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/500e75a036dc2d7d2fec5da1b71d36cc-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_53e3a716,
 abstract = {The primate brain must solve two important problems in grasping movements. The first problem concerns the recognition of grasped objects: specifically, how does the brain integrate visual and motor information on a grasped object? The second problem concerns hand shape planning: specifically, how does the brain design the hand configuration suited to the shape of the object and the manipulation task? A neural network model that solves these problems has been developed. The operations of the network are divided into a learning phase and an optimization phase. In the learning phase, internal representations, which depend on the grasped objects and the task, are acquired by integrating visual and somatosensory information. In the optimization phase, the most suitable hand shape for grasping an object is determined by using a relaxation computation of the network.},
 author = {Uno, Yoji and Fukumura, Naohiro and Suzuki, Ryoji and Kawato, Mitsuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/53e3a7161e428b65688f14b84d61c610-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/53e3a7161e428b65688f14b84d61c610-Metadata.json},
 openalex = {W2148400830},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/53e3a7161e428b65688f14b84d61c610-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Integration of Visual and Somatosensory Information for Preshaping Hand in Grasping Movements},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/53e3a7161e428b65688f14b84d61c610-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_5487315b,
 abstract = {A performance comparison of two self-organizing networks, the Kohonen Feature Map and the recently proposed Growing Cell Structures is made. For this purpose several performance criteria for self-organizing networks are proposed and motivated. The models are tested with three example problems of increasing difficulty. The Kohonen Feature Map demonstrates slightly superior results only for the simplest problem. For the other more difficult and also more realistic problems the Growing Cell Structures exhibit significantly better performance by every criterion. Additional advantages of the new model are that all parameters are constant over time and that size as well as structure of the network are determined automatically.},
 author = {Fritzke, Bernd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5487315b1286f907165907aa8fc96619-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5487315b1286f907165907aa8fc96619-Metadata.json},
 openalex = {W2139171208},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5487315b1286f907165907aa8fc96619-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Kohonen Feature Maps and Growing Cell Structures - a Performance Comparison},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/5487315b1286f907165907aa8fc96619-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_556f3919,
 abstract = {In the electrosensory system of weakly electric fish, descending pathways to a first-order sensory nucleus have been shown to influence the gain of its output neurons. The underlying neural mechanisms that subserve this descending gain control capability are not yet fully understood. We suggest that one possible gain control mechanism could involve the regulation of total membrane conductance of the output neurons. In this paper, a neural model based on this idea is used to demonstrate how activity levels on descending pathways could control both the gain and baseline excitation of a target neuron.},
 author = {Nelson, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/556f391937dfd4398cbac35e050a2177-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/556f391937dfd4398cbac35e050a2177-Metadata.json},
 openalex = {W2151011296},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/556f391937dfd4398cbac35e050a2177-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Neural Model of Descending Gain Control in the Electrosensory System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/556f391937dfd4398cbac35e050a2177-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_55743cc0,
 abstract = {We present a new algorithm, Prioritized Sweeping, for efficient prediction and control of stochastic Markov systems. Incremental learning methods such as Temporal Differencing and Q-learning have fast real time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized Sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare Prioritized Sweeping with other reinforcement learning schemes for a number of different stochastic optimal control problems. It successfully solves large state-space real time problems with which other methods have difficulty.},
 author = {Moore, Andrew and Atkeson, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/55743cc0393b1cb4b8b37d09ae48d097-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/55743cc0393b1cb4b8b37d09ae48d097-Metadata.json},
 openalex = {W2100684997},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/55743cc0393b1cb4b8b37d09ae48d097-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Memory-Based Reinforcement Learning: Efficient Computation with Prioritized Sweeping},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/55743cc0393b1cb4b8b37d09ae48d097-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_55b37c5c,
 abstract = {A three-step method for function approximation with a fuzzy system is proposed. First, the membership functions and an initial rule representation are learned; second, the rules are compressed as much as possible using information theory; and finally, a computational network is constructed to compute the function value. This system is applied to two control examples: learning the truck and trailer backer-upper control system, and learning a cruise control system for a radio-controlled model car.},
 author = {Higgins, Charles and Goodman, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/55b37c5c270e5d84c793e486d798c01d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/55b37c5c270e5d84c793e486d798c01d-Metadata.json},
 openalex = {W2131527651},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/55b37c5c270e5d84c793e486d798c01d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Fuzzy Rule-Based Neural Networks for Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/55b37c5c270e5d84c793e486d798c01d-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_58d4d1e7,
 abstract = {The overall goal is to reduce spacecraft weight, volume, and cost by online adaptive non-linear control of flexible structural components. The objective of this effort is to develop an adaptive Neural Network (NN) controller for the Ball C-Side 1m × 3m antenna with embedded actuators and the RAMS sensor system. A traditional optimal controller for the major modes is provided perturbations by the NN to compensate for unknown residual modes. On-line training of recurrent and feed-forward NN architectures have achieved adaptive vibration control with unknown modal variations and noisy measurements. On-line training feedback to each actuator NN output is computed via Newton's method to reduce the difference between desired and achieved antenna positions.},
 author = {Bowman, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/58d4d1e7b1e97b258c9ed0b37e02d087-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/58d4d1e7b1e97b258c9ed0b37e02d087-Metadata.json},
 openalex = {W2147232561},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/58d4d1e7b1e97b258c9ed0b37e02d087-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network On-Line Learning Control of Spacecraft Smart Structures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/58d4d1e7b1e97b258c9ed0b37e02d087-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_5c049256,
 abstract = {This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge.},
 author = {Stolcke, Andreas and Omohundro, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5c04925674920eb58467fb52ce4ef728-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5c04925674920eb58467fb52ce4ef728-Metadata.json},
 openalex = {W2162995740},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Hidden Markov Model Induction by Bayesian Model Merging},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_5d44ee6f,
 abstract = {We use statistical mechanics to study generalization in large committee machines. For an architecture with nonoverlapping receptive fields a replica calculation yields the generalization error in the limit of a large number of hidden units. For continuous weights the generalization error falls off asymptotically inversely proportional to α, the number of training examples per weight. For binary weights we find a discontinuous transition from poor to perfect generalization followed by a wide region of metastability. Broken replica symmetry is found within this region at low temperatures. For a fully connected architecture the generalization error is calculated within the annealed approximation. For both binary and continuous weights we find transitions from a symmetric state to one with specialized hidden units, accompanied by discontinuous drops in the generalization error.},
 author = {Schwarze, Holm and Hertz, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5d44ee6f2c3f71b73125876103c8f6c4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5d44ee6f2c3f71b73125876103c8f6c4-Metadata.json},
 openalex = {W2142433639},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Statistical Mechanics of Learning in a Large Committee Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_5dd9db5e,
 abstract = {Information theory is used to derive a simple formula for the amount of information conveyed by the firing rate of a neuron about any experimentally measured variable or combination of variables (e.g. running speed, head direction, location of the animal, etc.). The derivation treats the cell as a communication channel whose input is the measured variable and whose output is the cell's spike train. Applying the formula, we find systematic differences in the information content of hippocampal place cells in different experimental conditions.},
 author = {Skaggs, William and McNaughton, Bruce and Gothard, Katalin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Metadata.json},
 openalex = {W2126079698},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Information-Theoretic Approach to Deciphering the Hippocampal Code},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_5e9f92a0,
 author = {Burgess, Neil and O\textquotesingle Keefe, John and Recce, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5e9f92a01c986bafcabbafd145520b13-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5e9f92a01c986bafcabbafd145520b13-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/5e9f92a01c986bafcabbafd145520b13-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Using hippocampal \textquotesingle place cells\textquotesingle for navigation, exploiting phase coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/5e9f92a01c986bafcabbafd145520b13-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_6766aa27,
 abstract = {The ensemble dynamics of stochastic learning algorithms can be studied using theoretical techniques from statistical physics. We develop the equations of motion for the weight space probability densities for stochastic learning algorithms. We discuss equilibria in the diffusion approximation and provide expressions for special cases of the LMS algorithm. The equilibrium densities are not in general thermal (Gibbs) distributions in the objective function being minimized, but rather depend upon an effective potential that includes diffusion effects. Finally we present an exact analytical expression for the time evolution of the density for a learning algorithm with weight updates proportional to the sign of the gradient.},
 author = {Leen, Todd and Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/6766aa2750c19aad2fa1b32f36ed4aee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/6766aa2750c19aad2fa1b32f36ed4aee-Metadata.json},
 openalex = {W2119731487},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Weight Space Probability Densities in Stochastic Learning: I. Dynamics and Equilibria},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/6766aa2750c19aad2fa1b32f36ed4aee-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_67e103b0,
 abstract = {Previously, we have introduced the idea of neural network transfer, where learning on a target problem is sped up by using the weights obtained from a network trained for a related source task. Here, we present a new algorithm, called Discriminability-Based Transfer (DBT), which uses an information measure to estimate the utility of hyperplanes defined by source weights in the target network, and rescales transferred weight magnitudes accordingly. Several experiments demonstrate that target networks initialized via DBT learn significantly faster than networks initialized randomly.},
 author = {Pratt, L. Y.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/67e103b0761e60683e83c559be18d40c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/67e103b0761e60683e83c559be18d40c-Metadata.json},
 openalex = {W2106837051},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/67e103b0761e60683e83c559be18d40c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Discriminability-Based Transfer between Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/67e103b0761e60683e83c559be18d40c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_68264bdb,
 abstract = {An incremental, higher-order, non-recurrent network combines two properties found to be useful for learning sequential tasks: higher-order connections and incremental introduction of new units. The network adds higher orders when needed by adding new units that dynamically modify connection weights. Since the new units modify the weights at the next time-step with information from the previous step, temporal tasks can be learned without the use of feedback, thereby greatly simplifying training. Furthermore, a theoretically unlimited number of units can be added to reach into the arbitrarily distant past. Experiments with the Reber grammar have demonstrated speedups of two orders of magnitude over recurrent networks.},
 author = {Ring, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/68264bdb65b97eeae6788aa3348e553c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/68264bdb65b97eeae6788aa3348e553c-Metadata.json},
 openalex = {W2111871983},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/68264bdb65b97eeae6788aa3348e553c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Sequential Tasks by Incrementally Adding Higher Orders},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/68264bdb65b97eeae6788aa3348e553c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_6a10bbd4,
 author = {Goodhill, Geoffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/6a10bbd480e4c5573d8f3af73ae0454b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/6a10bbd480e4c5573d8f3af73ae0454b-Metadata.json},
 openalex = {W2141669308},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/6a10bbd480e4c5573d8f3af73ae0454b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Topography and ocular dominance: a model exploring positive correlations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/6a10bbd480e4c5573d8f3af73ae0454b-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_6aca9700,
 abstract = {Untill recently, state-of-the-art, large-vocabulary, continuous speech recognition (CSR) has employed Hidden Markov Modeling (HMM) to model speech sounds. In an attempt to improve over HMM we developed a hybrid system that integrates HMM technology with neural networks. We present the concept of a Segmental Neural Net (SNN) for phonetic modeling in CSR. By taking into account all the frames of a phonetic segment simultaneously, the SNN overcomes the well-known conditional-independence limitation of HMMs. In several speaker-independent experiments with the DARPA Resource Management corpus, the hybrid system showed a consistent improvement in performance over the baseline HMM system.},
 author = {Zavaliagkos, G. and Zhao, Y. and Schwartz, R. and Makhoul, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/6aca97005c68f1206823815f66102863-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/6aca97005c68f1206823815f66102863-Metadata.json},
 openalex = {W2140121194},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/6aca97005c68f1206823815f66102863-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Hybrid Neural Net System for State-of-the-Art Continuous Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/6aca97005c68f1206823815f66102863-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_71a3cb15,
 abstract = {It is known from biological data that the response patterns of interneurons in the olfactory macroglomerulus (MGC) of insects are of central importance for the coding of the olfactory signal. We propose an analytically tractable model of the MGC which allows us to relate the distribution of response patterns to the architecture of the network.},
 author = {Linster, Christiane and Marsan, David and Masson, Claudine and Kerszberg, Michel and Dreyfus, G\'{e}rard and Personnaz, L\'{e}on},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/71a3cb155f8dc89bf3d0365288219936-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/71a3cb155f8dc89bf3d0365288219936-Metadata.json},
 openalex = {W2102461049},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/71a3cb155f8dc89bf3d0365288219936-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Formal Model of the Insect Olfactory Macroglomerulus: Simulations and Analytic Results},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/71a3cb155f8dc89bf3d0365288219936-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_766ebcd5,
 abstract = {Although considerable interest has been shown in language inference and automata induction using recurrent neural networks, success of these models has mostly been limited to regular languages. We have previously demonstrated that Neural Network Pushdown Automaton (NNPDA) model is capable of learning deterministic context-free languages (e.g., anbn and parenthesis languages) from examples. However, the learning task is computationally intensive. In this paper we discus some ways in which a priori knowledge about the task and data could be used for efficient learning. We also observe that such knowledge is often an experimental prerequisite for learning nontrivial languages (eg. anbncbmam).},
 author = {Das, Sreerupa and Giles, C. and Sun, Guo-Zheng},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/766ebcd59621e305170616ba3d3dac32-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/766ebcd59621e305170616ba3d3dac32-Metadata.json},
 openalex = {W2099683551},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Using Prior Knowledge in a NNPDA to Learn Context-Free Languages},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/766ebcd59621e305170616ba3d3dac32-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_7750ca35,
 abstract = {We address the problem of learning an unknown function by putting together several pieces of information (hints) that we know about the function. We introduce a method that generalizes learning from examples to learning from hints. A canonical representation of hints is defined and illustrated for new types of hints. All the hints are represented to the learning process by examples, and examples of the function are treated on equal footing with the rest of the hints. During learning, examples from different hints are selected for processing according to a given schedule. We present two types of schedules; fixed schedules that specify the relative emphasis of each hint, and adaptive schedules that are based on how well each hint has been learned so far. Our learning method is compatible with any descent technique that we may choose to use.},
 author = {Abu-Mostafa, Yaser},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/7750ca3559e5b8e1f44210283368fc16-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/7750ca3559e5b8e1f44210283368fc16-Metadata.json},
 openalex = {W2149200300},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/7750ca3559e5b8e1f44210283368fc16-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Method for Learning From Hints},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/7750ca3559e5b8e1f44210283368fc16-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_7f5d04d1,
 abstract = {Holographic Recurrent Networks (HRNs) are recurrent networks which incorporate associative memory techniques for storing sequential structure. HRNs can be easily and quickly trained using gradient descent techniques to generate sequences of discrete outputs and trajectories through continuous space. The performance of HRNs is found to be superior to that of ordinary recurrent networks on these sequence generation tasks.},
 author = {Plate, Tony A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/7f5d04d189dfb634e6a85bb9d9adf21e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/7f5d04d189dfb634e6a85bb9d9adf21e-Metadata.json},
 openalex = {W2119216348},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Holographic Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/7f5d04d189dfb634e6a85bb9d9adf21e-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_84117275,
 abstract = {In [5], a new incremental cascade network architecture has been presented. This paper discusses the properties of such cascade networks and investigates their generalization abilities under the particular constraint of small data sets. The evaluation is done for cascade networks consisting of local linear maps using the Mackey-Glass time series prediction task as a benchmark. Our results indicate that to bring the potential of large networks to bear on the problem of extracting information from small data sets without running the risk of overfitting, deeply cascaded network architectures are more favorable than shallow broad architectures that contain the same number of nodes.},
 author = {Littmann, E. and Ritter, H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/84117275be999ff55a987b9381e01f96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/84117275be999ff55a987b9381e01f96-Metadata.json},
 openalex = {W2117799453},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/84117275be999ff55a987b9381e01f96-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generalization Abilities of Cascade Network Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/84117275be999ff55a987b9381e01f96-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_851ddf50,
 abstract = {How can artificial neural nets generalize better from fewer examples? In order to generalize successfully, neural network learning methods typically require large training data sets. We introduce a neural network learning method that generalizes rationally from many fewer data points, relying instead on prior knowledge encoded in previously learned neural networks. For example, in robot control learning tasks reported here, previously learned networks that model the effects of robot actions are used to guide subsequent learning of robot control functions. For each observed training example of the target function (e.g. the robot control policy), the learner explains the observed example in terms of its prior knowledge, then analyzes this explanation to infer additional information about the shape, or slope, of the target function. This shape knowledge is used to bias generalization when learning the target function. Results are presented applying this approach to a simulated robot task based on reinforcement learning.},
 author = {Mitchell, Tom M and Thrun, Sebastian B.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/851ddf5058cf22df63d3344ad89919cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/851ddf5058cf22df63d3344ad89919cf-Metadata.json},
 openalex = {W2140256637},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/851ddf5058cf22df63d3344ad89919cf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Explanation-Based Neural Network Learning for Robot Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/851ddf5058cf22df63d3344ad89919cf-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_85fc37b1,
 abstract = {In visual processing the ability to deal with missing and noisy information is crucial. Occlusions and unreliable feature detectors often lead to situations where little or no direct information about features is available. However the available information is usually sufficient to highly constrain the outputs. We discuss Bayesian techniques for extracting class probabilities given partial data. The optimal solution involves integrating over the missing dimensions weighted by the local probability densities. We show how to obtain closed-form approximations to the Bayesian solution using Gaussian basis function networks. The framework extends naturally to the case of noisy features. Simulations on a complex task (3D hand gesture recognition) validate the theory. When both integration and weighting by input densities are used, performance decreases gracefully with the number of missing or noisy features. Performance is substantially degraded if either step is omitted.},
 author = {Ahmad, Subutai and Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/85fc37b18c57097425b52fc7afbb6969-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/85fc37b18c57097425b52fc7afbb6969-Metadata.json},
 openalex = {W2144847675},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Some Solutions to the Missing Feature Problem in Vision},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_8757150d,
 abstract = {Vector Quantization is useful for data compression. Competitive Learning which minimizes reconstruction error is an appropriate algorithm for vector quantization of unlabelled data. Vector quantization of labelled data for classification has a different objective, to minimize the number of misclassifications, and a different algorithm is appropriate. We show that a variant of Kohonen's LVQ2.1 algorithm can be seen as a multiclass extension of an algorithm which in a restricted 2 class case can be proven to converge to the Bayes optimal classification boundary. We compare the performance of the LVQ2.1 algorithm to that of a modified version having a decreasing window and normalized step size, on a ten class vowel classification problem.},
 author = {de Sa, Virginia and Ballard, Dana},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8757150decbd89b0f5442ca3db4d0e0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8757150decbd89b0f5442ca3db4d0e0e-Metadata.json},
 openalex = {W2115020216},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8757150decbd89b0f5442ca3db4d0e0e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Note on Learning Vector Quantization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/8757150decbd89b0f5442ca3db4d0e0e-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_884d247c,
 author = {Finnoff, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/884d247c6f65a96a7da4d1105d584ddd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/884d247c6f65a96a7da4d1105d584ddd-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Diffusion Approximations for the Constant Learning Rate Backpropagation Algorithm and Resistence to Local Minima},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/884d247c6f65a96a7da4d1105d584ddd-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_8a0e1141,
 abstract = {This paper discusses the parameterization of speech by an analog cochlear model. The tradeoff between time and frequency resolution is viewed as the fundamental difference between conventional spectrographic analysis and cochlear signal processing for broadband, rapid-changing signals. The model's response exhibits a wavelet-like analysis in the scale domain that preserves good temporal resolution; the frequency of each spectral component in a broadband signal can be accurately determined from the interpeak intervals in the instantaneous firing rates of auditory fibers. Such properties of the cochlear model are demonstrated with natural speech and synthetic complex signals.},
 author = {Liu, Weimin and Andreou, Andreas and Goldstein, Moise},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Metadata.json},
 openalex = {W2139694799},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analog Cochlear Model for Multiresolution Speech Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/8a0e1141fd37fa5b98d5bb769ba1a7cc-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_8c7bbbba,
 author = {Golea, Mostefa and Marchand, Mario and Hancock, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8c7bbbba95c1025975e548cee86dfadc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8c7bbbba95c1025975e548cee86dfadc-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8c7bbbba95c1025975e548cee86dfadc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {On Learning \mathrm{\mu}-Perceptron Networks with Binary Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/8c7bbbba95c1025975e548cee86dfadc-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_8df707a9,
 abstract = {A new boundary hunting radial basis function (BH-RBF) classifier which allocates RBF centers constructively near class boundaries is described. This classifier creates complex decision boundaries only in regions where confusions occur and corresponding RBF outputs are similar. A predicted square error measure is used to determine how many centers to add and to determine when to stop adding centers. Two experiments are presented which demonstrate the advantages of the BH-RBF classifier. One uses artificial data with two classes and two input features where each class contains four clusters but only one cluster is near a decision region boundary. The other uses a large seismic database with seven classes and 14 input features. In both experiments the BH-RBF classifier provides a lower error rate with fewer centers than are required by more conventional RBF, Gaussian mixture, or MLP classifiers.},
 author = {Chang, Eric and Lippmann, Richard P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8df707a948fac1b4a0f97aa554886ec8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8df707a948fac1b4a0f97aa554886ec8-Metadata.json},
 openalex = {W2112659388},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8df707a948fac1b4a0f97aa554886ec8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Boundary Hunting Radial Basis Function Classifier which Allocates Centers Constructively},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/8df707a948fac1b4a0f97aa554886ec8-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_8ebda540,
 abstract = {This paper describes an approach to integrated segmentation and recognition of hand-printed characters. The approach, called Saccade, integrates ballistic and corrective saccades (eye movements) with character recognition. A single backpropagation net is trained to make a classification decision on a character centered in its input window, as well as to estimate the distance of the current and next character from the center of the input window. The net learns to accurately estimate these distances regardless of variations in character width, spacing between characters, writing style and other factors. During testing, the system uses the net-extracted classification and distance information, along with a set of jumping rules, to jump from character to character.},
 author = {Martin, Gale and Rashid, Mosfeq and Chapman, David and Pittman, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8ebda540cbcc4d7336496819a46a1b68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8ebda540cbcc4d7336496819a46a1b68-Metadata.json},
 openalex = {W2172192122},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8ebda540cbcc4d7336496819a46a1b68-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning to See Where and What: Training a Net to Make Saccades and Recognize Handwritten Characters},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/8ebda540cbcc4d7336496819a46a1b68-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_8fecb208,
 abstract = {A switching between apparently coherent (oscillatory) and stochastic episodes of activity has been observed in responses from cat and monkey visual cortex. We describe the dynamics of these phenomena in two parallel approaches, a phenomenological and a rather microscopic one. On the one hand we analyze neuronal responses in terms of a hidden state model (HSM). The parameters of this model are extracted directly from experimental spike trains. They characterize the underlying dynamics as well as the coupling of individual neurons to the network. This phenomenological model thus provides a new framework for the experimental analysis of network dynamics. The application of this method to multi unit activities from the visual cortex of the cat substantiates the existence of oscillatory and stochastic states and quantifies the switching behaviour in the assembly dynamics. On the other hand we start from the single spiking neuron and derive a master equation for the time evolution of the assembly state which we represent by a phase density. This phase density dynamics (PDD) exhibits costability of two attractors, a limit cycle, and a fixed point when synaptic interaction is nonlinear. External fluctuations can switch the bistable system from one state to the other. Finally we show, that the two approaches are mutually consistent and therefore both explain the detailed time structure in the data.},
 author = {Pawelzik, K. and Bauer, H.-U. and Deppisch, J. and Geisel, T.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8fecb20817b3847419bb3de39a609afe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8fecb20817b3847419bb3de39a609afe-Metadata.json},
 openalex = {W2109612445},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/8fecb20817b3847419bb3de39a609afe-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {How Oscillatory Neuronal Responses Reflect Bistability and Switching of the Hidden Assembly Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/8fecb20817b3847419bb3de39a609afe-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_996a7fa0,
 abstract = {The algorithm presented performs gradient descent on the weight space of an Artificial Neural Network (ANN), using a finite difference to approximate the gradient. The method is novel in that it achieves a computational complexity similar to that of Node Perturbation, O(N3), but does not require access to the activity of hidden or internal neurons. This is possible due to a stochastic relation between perturbations at the weights and the neurons of an ANN. The algorithm is also similar to Weight Perturbation in that it is optimal in terms of hardware requirements when used for the training of VLSI implementations of ANN's.},
 author = {Flower, Barry and Jabri, Marwan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/996a7fa078cc36c46d02f9af3bef918b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/996a7fa078cc36c46d02f9af3bef918b-Metadata.json},
 openalex = {W2102489567},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Summed Weight Neuron Perturbation: An O(N) Improvement Over Weight Perturbation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/996a7fa078cc36c46d02f9af3bef918b-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_9b698eb3,
 abstract = {Many techniques for model selection in the field of neural networks correspond to well established statistical methods. The method of 'stopped training', on the other hand, in which an oversized network is trained until the error on a further validation set of examples deteriorates, then training is stopped, is a true innovation, since model selection doesn't require convergence of the training process.

In this paper we show that this performance can be significantly enhanced by extending the 'non convergent model selection method' of stopped training to include dynamic topology modifications (dynamic weight pruning) and modified complexity penalty term methods in which the weighting of the penalty term is adjusted during the training process.},
 author = {Finnoff, W. and Hergert, F. and Zimmermann, H. G.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9b698eb3105bd82528f23d0c92dedfc0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9b698eb3105bd82528f23d0c92dedfc0-Metadata.json},
 openalex = {W2170496524},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9b698eb3105bd82528f23d0c92dedfc0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Extended Regularization Methods for Nonconvergent Model Selection},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/9b698eb3105bd82528f23d0c92dedfc0-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_9c82c714,
 abstract = {We have designed an architecture to span the gap between biophysics and cognitive science to address and explore issues of how a discrete symbol processing system can arise from the continuum, and how complex dynamics like oscillation and synchronization can then be employed in its operation and affect its learning. We show how a discrete-time recurrent Elman network architecture can be constructed from recurrently connected oscillatory associative memory modules described by continuous nonlinear ordinary differential equations. The modules can learn connection weights between themselves which will cause the system to evolve under a clocked machine by a sequence of transitions of within the modules, much as a digital computer evolves by transitions of its binary flip-flop attractors. The architecture thus employs the principle of computing with attractors used by macroscopic systems for reliable computation in the presence of noise. We have specifically constructed a system which functions as a finite state automaton that recognizes or generates the infinite set of six symbol strings that are defined by a Reber grammar. It is a symbol processing system, but with analog input and oscillatory subsymbolic representations. The time steps (machine cycles) of the system are implemented by rhythmic variation (clocking) of a bifurcation parameter. This holds input and modules clamped at their while 'hidden and output modules change state, then clamps hidden and output states while context modules are released to load those states as the new context for the next cycle of input. Superior noise immunity has been demonstrated for systems with dynamic over systems with static attractors, and synchronization (binding) between coupled oscillatory in different modules has been shown to be important for effecting reliable transitions.},
 author = {Baird, Bill and Troyer, Todd and Eeckman, Frank},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9c82c7143c102b71c593d98d96093fde-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9c82c7143c102b71c593d98d96093fde-Metadata.json},
 openalex = {W2104185260},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9c82c7143c102b71c593d98d96093fde-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Synchronization and Grammatical Inference in an Oscillating Elman Net},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/9c82c7143c102b71c593d98d96093fde-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_9cc138f8,
 abstract = {Networks with local inhibition are shown to have enhanced computational performance with respect to the classical Hopfield-like networks. In particular the critical capacity of the network is increased as well as its capability to store correlated patterns. Chaotic dynamic behaviour (exponentially long transients) of the devices indicates the overloading of the associative memory. An implementation based on a programmable logic device is here presented. A 16 neurons circuit is implemented whit a XILINK 4020 device. The peculiarity of this solution is the possibility to change parts of the project (weights, transfer function or the whole architecture) with a simple software download of the configuration into the XILINK chip.},
 author = {Pasero, E. and Zecchina, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9cc138f8dc04cbf16240daa92d8d50e2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9cc138f8dc04cbf16240daa92d8d50e2-Metadata.json},
 openalex = {W2109044140},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9cc138f8dc04cbf16240daa92d8d50e2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Attractor Neural Networks with Local Inhibition: from Statistical Physics to a Digitial Programmable Integrated Circuit},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/9cc138f8dc04cbf16240daa92d8d50e2-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_9cf81d80,
 abstract = {Given a set of training examples, determining the appropriate number of free parameters is a challenging problem. Constructive learning algorithms attempt to solve this problem automatically by adding hidden units, and therefore free parameters, during learning. We explore an alternative class of algorithms--called metamorphosis algorithms--in which the number of units is fixed, but the number of free parameters gradually increases during learning. The architecture we investigate is composed of RBF units on a lattice, which imposes flexible constraints on the parameters of the network. Virtues of this approach include variable subset selection, robust parameter selection, multiresolution processing, and interpolation of sparse training data.},
 author = {Bonnlander, Brian and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9cf81d8026a9018052c429cc4e56739b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9cf81d8026a9018052c429cc4e56739b-Metadata.json},
 openalex = {W2167997423},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9cf81d8026a9018052c429cc4e56739b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Metamorphosis Networks: An Alternative to Constructive Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/9cf81d8026a9018052c429cc4e56739b-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_9f396fe4,
 abstract = {Channel equalization problem is an important problem in high-speed communications. The sequences of symbols transmitted are distorted by neighboring symbols. Traditionally, the channel equalization problem is considered as a channel-inversion operation. One problem of this approach is that there is no direct correspondence between error probability and residual error produced by the channel inversion operation. In this paper, the optimal equalizer design is formulated as a classification problem. The optimal classifier can be constructed by Bayes decision rule. In general it is nonlinear. An efficient hybrid linear/nonlinear equalizer approach has been proposed to train the equalizer. The error probability of new linear/nonlinear equalizer has been shown to be better than a linear equalizer in an experimental channel.},
 author = {Lee, Wei-Tsih and Pearson, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9f396fe44e7c05c16873b05ec425cbad-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9f396fe44e7c05c16873b05ec425cbad-Metadata.json},
 openalex = {W2172049308},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Hybrid Linear/Nonlinear Approach to Channel Equalization Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/9f396fe44e7c05c16873b05ec425cbad-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_9fe8593a,
 abstract = {This study demonstrates a paradigm for modeling speech production based on neural networks. Using physiological data from speech utterances, a neural network learns the forward dynamics relating motor commands to muscles and the ensuing articulator behavior that allows articulator trajectories to be generated from motor commands constrained by phoneme input strings and global performance parameters. From these movement trajectories, a second neural network generates PARCOR parameters that are then used to synthesize the speech acoustics.},
 author = {Hirayama, Makoto and Vatikiotis-Bateson, Eric and Honda, Kiyoshi and Koike, Yasuharu and Kawato, Mitsuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9fe8593a8a330607d76796b35c64c600-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9fe8593a8a330607d76796b35c64c600-Metadata.json},
 openalex = {W2154495707},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/9fe8593a8a330607d76796b35c64c600-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Physiologically Based Speech Synthesis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/9fe8593a8a330607d76796b35c64c600-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_a532400e,
 abstract = {In stochastic learning, weights are random variables whose time evolution is governed by a Markov process. At each time-step, n, the weights can be described by a probability density function P(w, n). We summarize the theory of the time evolution of P, and give graphical examples of the time evolution that contrast the behavior of stochastic learning with true gradient descent (batch learning). Finally, we use the formalism to obtain predictions of the time required for noise-induced hopping between basins of different optima. We compare the theoretical predictions with simulations of large ensembles of networks for simple problems in supervised and unsupervised learning.},
 author = {Orr, Genevieve and Leen, Todd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/a532400ed62e772b9dc0b86f46e583ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/a532400ed62e772b9dc0b86f46e583ff-Metadata.json},
 openalex = {W2165137214},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/a532400ed62e772b9dc0b86f46e583ff-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Weight Space Probability Densities in Stochastic Learning: II. Transients and Basin Hopping Times},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/a532400ed62e772b9dc0b86f46e583ff-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_a733fa9b,
 author = {Tebelskis, Joe and Waibel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/a733fa9b25f33689e2adbe72199f0e62-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/a733fa9b25f33689e2adbe72199f0e62-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/a733fa9b25f33689e2adbe72199f0e62-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Performance Through Consistency: MS-TDNN\textquotesingle s for Large Vocabulary Continuous Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/a733fa9b25f33689e2adbe72199f0e62-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_ab233b68,
 abstract = {An analog CMOS VLSI neural processing chip has been designed and fabricated. The device employs pulse-stream neural state signalling, and is capable of computing some 360 million synaptic connections per second. In addition to basic characterisation results, the performance of the chip in solving real-world problems is also demonstrated.},
 author = {Churcher, Stephen and Baxter, Donald and Hamilton, Alister and Murray, Alan and Reekie, H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/ab233b682ec355648e7891e66c54191b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/ab233b682ec355648e7891e66c54191b-Metadata.json},
 openalex = {W2117317202},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/ab233b682ec355648e7891e66c54191b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generic Analog Neural Computation - The EPSILON Chip},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/ab233b682ec355648e7891e66c54191b-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_abd81528,
 abstract = {We describe an analog VLSI implementation of a multi-dimensional gradient estimation and descent technique for minimizing an onchip scalar function f(). The implementation uses noise injection and multiplicative correlation to estimate derivatives, as in [Anderson, Kerns 92]. One intended application of this technique is setting circuit parameters on-chip automatically, rather than manually [Kirk 91]. Gradient descent optimization may be used to adjust synapse weights for a backpropagation or other on-chip learning implementation. The approach combines the features of continuous multi-dimensional gradient descent and the potential for an annealing style of optimization. We present data measured from our analog VLSI implementation.},
 author = {Kirk, David B. and Kerns, Douglas and Fleischer, Kurt and Barr, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/abd815286ba1007abfbb8415b83ae2cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/abd815286ba1007abfbb8415b83ae2cf-Metadata.json},
 openalex = {W2163068376},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/abd815286ba1007abfbb8415b83ae2cf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analog VLSI Implementation of Multi-dimensional Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/abd815286ba1007abfbb8415b83ae2cf-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_acc3e040,
 abstract = {A boosting algorithm converts a learning machine with error rate less than 50% to one with an arbitrarily low error rate. However, the algorithm discussed here depends on having a large supply of independent training samples. We show how to circumvent this problem and generate an ensemble of learning machines whose performance in optical character recognition problems is dramatically improved over that of a single network. We report the effect of boosting on four databases (all handwritten) consisting of 12,000 digits from segmented ZIP codes from the United State Postal Service (USPS) and the following from the National Institute of Standards and Testing (NIST): 220,000 digits, 45,000 upper case alphas, and 45,000 lower case alphas. We use two performance measures: the raw error rate (no rejects) and the reject rate required to achieve a 1% error rate on the patterns not rejected. Boosting improved performance in some cases by a factor of three.},
 author = {Drucker, Harris and Schapire, Robert and Simard, Patrice},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/acc3e0404646c57502b480dc052c4fe1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/acc3e0404646c57502b480dc052c4fe1-Metadata.json},
 openalex = {W2119113516},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/acc3e0404646c57502b480dc052c4fe1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Improving Performance in Neural Networks Using a Boosting Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/acc3e0404646c57502b480dc052c4fe1-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_ae0eb3ee,
 abstract = {The field of software simulators for neural networks has been expanding very rapidly in the last years but their importance is still being underestimated. They must provide increasing levels of assistance for the design, simulation and analysis of neural networks. With our object-oriented framework (SESAME) we intend to show that very high degrees of transparency, manageability and flexibility for complex experiments can be obtained. SESAME's basic design philosophy is inspired by the natural way in which researchers explain their computational models. Experiments are performed with networks of building blocks, which can be extended very easily. Mechanisms have been integrated to facilitate the construction and analysis of very complex architectures. Among these mechanisms are the automatic configuration of building blocks for an experiment and multiple inheritance at run-time.},
 author = {Linden, A. and Sudbrak, Th. and Tietz, Ch. and Weber, F.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/ae0eb3eed39d2bcef4622b2499a05fe6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/ae0eb3eed39d2bcef4622b2499a05fe6-Metadata.json},
 openalex = {W2168199886},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Object-Oriented Framework for the Simulation of Neural Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_afd48367,
 abstract = {Trajectory Extension is a new technique for Learning Control in Robots which assumes that there exists some parameter of the desired trajectory that can be smoothly varied from a region of easy solvability of the dynamics to a region of desired behavior which may have more difficult dynamics. By gradually varying the parameter, practice movements remain near the desired path while a Neural Network learns to approximate the inverse dynamics. For example, the average speed of motion might be varied, and the inverse dynamics can be bootstrapped from slow movements with simpler dynamics to fast movements. This provides an example of the more general concept of a Practice Strategy in which a sequence of intermediate tasks is used to simplify learning a complex task. I show an example of the application of this idea to a real 2-joint direct drive robot arm.},
 author = {Sanger, Terence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/afd4836712c5e77550897e25711e1d96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/afd4836712c5e77550897e25711e1d96-Metadata.json},
 openalex = {W2138924197},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/afd4836712c5e77550897e25711e1d96-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Practice Strategy for Robot Learning Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/afd4836712c5e77550897e25711e1d96-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_b1eec33c,
 abstract = {We would like to incorporate speaker-dependent consistencies, such as gender, in an otherwise speaker-independent speech recognition system. In this paper we discuss a Gender Dependent Neural Network (GDNN) which can be tuned for each gender, while sharing most of the speaker independent parameters. We use a classification network to help generate gender-dependent phonetic probabilities for a statistical (HMM) recognition system. The gender classification net predicts the gender with high accuracy, 98.3% on a Resource Management test set. However, the integration of the GDNN into our hybrid HMM-neural network recognizer provided an improvement in the recognition score that is not statistically significant on a Resource Management test set.},
 author = {Konig, Yochai and Morgan, Nelson and Wooters, Chuck and Abrash, Victor and Cohen, Michael and Franco, Horacio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b1eec33c726a60554bc78518d5f9b32c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b1eec33c726a60554bc78518d5f9b32c-Metadata.json},
 openalex = {W2120981055},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b1eec33c726a60554bc78518d5f9b32c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Modeling Consistency in a Speaker Independent Continuous Speech Recognition System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/b1eec33c726a60554bc78518d5f9b32c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_b2eeb736,
 abstract = {The invariance of an objects' identity as it transformed over time provides a powerful cue for perceptual learning. We present an unsupervised learning procedure which maximizes the mutual information between the representations adopted by a feed-forward network at consecutive time steps. We demonstrate that the network can learn, entirely unsupervised, to classify an ensemble of several patterns by observing pattern trajectories, even though there are abrupt transitions from one object to another between trajectories. The same learning procedure should be widely applicable to a variety of perceptual learning tasks.},
 author = {Becker, Suzanna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b2eeb7362ef83deff5c7813a67e14f0a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b2eeb7362ef83deff5c7813a67e14f0a-Metadata.json},
 openalex = {W2153944708},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning to categorize objects using temporal coherence},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/b2eeb7362ef83deff5c7813a67e14f0a-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_b2f627ff,
 abstract = {Learning curves show how a neural network is improved as the number of training examples increases and how it is related to the network complexity. The present paper clarifies asymptotic properties and their relation of two learning curves, one concerning the predictive loss or generalization loss and the other the training loss. The result gives a natural definition of the complexity of a neural network. Moreover, it provides a new criterion of model selection.},
 author = {Murata, Noboru and Yoshizawa, Shuji and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b2f627fff19fda463cb386442eac2b3d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b2f627fff19fda463cb386442eac2b3d-Metadata.json},
 openalex = {W2116191868},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b2f627fff19fda463cb386442eac2b3d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Curves, Model Selection and Complexity of Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/b2f627fff19fda463cb386442eac2b3d-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_b4288d9c,
 abstract = {An artificial neural network (ANN) is commonly modeled by a threshold circuit, a network of interconnected processing units called linear threshold gates. The depth of a network represents the number of unit delays or the time for parallel computation. The Size of a circuit is the number of gates and measures the amount of hardware. It was known that traditional logic circuits consisting of only unbounded fan-in AND, OR, NOT gates would require at least Ω(log n/log log n) depth to compute common arithmetic functions such as the product or the quotient of two n-bit numbers, unless we allow the size (and fan-in) to increase exponentially (in n). We show in this paper that ANNs can be much more powerful than traditional logic circuits. In particular, we prove that that iterated addition can be computed by depth-2 ANN, and multiplication and division can be computed by depth-3 ANNs with polynomial size and polynomially bounded integer weights, respectively. Moreover, it follows from known lower bound results that these ANNs are optimal in depth. We also indicate that these techniques can be applied to construct polynomial-size depth-3 ANN for powering, and depth-4 ANN for multiple product.},
 author = {Siu, Kai-Yeung and Roychowdhury, Vwani},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b4288d9c0ec0a1841b3b3728321e7088-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b4288d9c0ec0a1841b3b3728321e7088-Metadata.json},
 openalex = {W2141469882},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b4288d9c0ec0a1841b3b3728321e7088-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Depth Neural Networks for Multiplication and Related Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/b4288d9c0ec0a1841b3b3728321e7088-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_b4a52895,
 abstract = {We proposed a model of Time Warping Invariant Neural Networks (TWINN) to handle the time warped continuous signals. Although TWINN is a simple modification of well known recurrent neural network, analysis has shown that TWINN completely removes time warping and is able to handle difficult classification problem. It is also shown that TWINN has certain advantages over the current available sequential processing schemes: Dynamic Programming(DP)[1], Hidden Markov Model(HMM)[2], Time Delayed Neural Networks(TDNN) [3] and Neural Network Finite Automata(NNFA)[4].

We also analyzed the time continuity employed in TWINN and pointed out that this kind of structure can memorize longer input history compared with Neural Network Finite Automata (NNFA). This may help to understand the well accepted fact that for learning grammatical reference with NNFA one had to start with very short strings in training set.

The numerical example we used is a trajectory classification problem. This problem, making a feature of variable sampling rates, having internal states, continuous dynamics, heavily time-warped data and deformed phase space trajectories, is shown to be difficult to other schemes. With TWINN this problem has been learned in 100 iterations. For benchmark we also trained the exact same problem with TDNN and completely failed as expected.},
 author = {Sun, Guo-Zheng and Chen, Hsing-Hen and Lee, Yee-Chun},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b4a528955b84f584974e92d025a75d1f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b4a528955b84f584974e92d025a75d1f-Metadata.json},
 openalex = {W2162704388},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b4a528955b84f584974e92d025a75d1f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Time Warping Invariant Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/b4a528955b84f584974e92d025a75d1f-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_b5dc4e5d,
 abstract = {An information-theoretic optimization principle ('infomax') has previously been used for unsupervised learning of statistical regularities in an input ensemble. The principle states that the input-output mapping implemented by a processing stage should be chosen so as to maximize the average mutual information between input and output patterns, subject to constraints and in the presence of processing noise. In the present work I show how infomax, when applied to a class of nonlinear input-output mappings, can under certain conditions generate optimal filters that have additional useful properties: (1) Output activity (for each input pattern) tends to be concentrated among a relatively small number of nodes. (2) The filters are sensitive to higher-order statistical structure (beyond pairwise correlations). If the input features are localized, the filters' receptive fields tend to be localized as well. (3) Multiresolution sets of filters with subsampling at low spatial frequencies - related to pyramid coding and wavelet representations - emerge as favored solutions for certain types of input ensembles.},
 author = {Linsker, Ralph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b5dc4e5d9b495d0196f61d45b26ef33e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b5dc4e5d9b495d0196f61d45b26ef33e-Metadata.json},
 openalex = {W2105206569},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Deriving Receptive Fields Using an Optimal Encoding Criterion},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/b5dc4e5d9b495d0196f61d45b26ef33e-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_b73dfe25,
 abstract = {The Multi-State Time Delay Neural Network (MS-TDNN) integrates a nonlinear time alignment procedure (DTW) and the high-accuracy phoneme spotting capabilities of a TDNN into a connectionist speech recognition system with word-level classification and error backpropagation. We present an MS-TDNN for recognizing continuously spelled letters, a task characterized by a small but highly confusable vocabulary. Our MS-TDNN achieves 98.5/92.0% word accuracy on speaker dependent/independent tasks, outperforming previously reported results on the same databases. We propose training techniques aimed at improving sentence level performance, including free alignment across word boundaries, word duration modeling and error backpropagation on the sentence rather than the word level. Architectures integrating submodules specialized on a subset of speakers achieved further improvements.},
 author = {Hild, Hermann and Waibel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b73dfe25b4b8714c029b37a6ad3006fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b73dfe25b4b8714c029b37a6ad3006fa-Metadata.json},
 openalex = {W2139935378},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b73dfe25b4b8714c029b37a6ad3006fa-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connected Letter Recognition with a Multi-State Time Delay Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/b73dfe25b4b8714c029b37a6ad3006fa-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_b7bb35b9,
 author = {Pomerleau, Dean A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Metadata.json},
 openalex = {W2165901038},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Input Reconstruction Reliability Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/b7bb35b9c6ca2aee2df08cf09d7016c2-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_be3159ad,
 abstract = {A computer model of the musculoskeletal system of the lobster gastric mill was constructed in order to provide a behavioral interpretation of the rhythmic patterns obtained from isolated stomatogastric ganglion. The model was based on Hill's muscle model and quasi-static approximation of the skeletal dynamics and could simulate the change of chewing patterns by the effect of neuromodulators.},
 author = {Doya, Kenji and Boyle, Mary and Selverston, Allen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/be3159ad04564bfb90db9e32851ebf9c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/be3159ad04564bfb90db9e32851ebf9c-Metadata.json},
 openalex = {W2159818709},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Mapping Between Neural and Physical Activities of the Lobster Gastric Mill},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/be3159ad04564bfb90db9e32851ebf9c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_c06d06da,
 abstract = {A parallel stochastic algorithm is investigated for error-descent learning and optimization in deterministic networks of arbitrary topology. No explicit information about internal network structure is needed. The method is based on the model-free distributed learning mechanism of Dembo and Kailath. A modified parameter update rule is proposed by which each individual parameter vector perturbation contributes a decrease in error. A substantially faster learning speed is hence allowed. Furthermore, the modified algorithm supports learning time-varying features in dynamical networks. We analyze the convergence and scaling properties of the algorithm, and present simulation results for dynamic trajectory learning in recurrent networks.},
 author = {Cauwenberghs, Gert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c06d06da9666a219db15cf575aff2824-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c06d06da9666a219db15cf575aff2824-Metadata.json},
 openalex = {W2127621215},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c06d06da9666a219db15cf575aff2824-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Fast Stochastic Error-Descent Algorithm for Supervised Learning and Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/c06d06da9666a219db15cf575aff2824-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_c361bc7b,
 abstract = {In a multi-layered neural network, any one of the hidden layers can be viewed as computing a distributed representation of the input. Several encoder experiments have shown that when the representation space is small it can be fully used. But computing with such a representation requires completely dependable nodes. In the case where the hidden nodes are noisy and unreliable, we find that error correcting schemes emerge simply by using noisy units during training; random errors injected during backpropagation result in spreading representations apart. Average and minimum distances increase with misfire probability, as predicted by coding-theoretic considerations. Furthermore, the effect of this noise is to protect the machine against permanent node failure, thereby potentially extending the useful lifetime of the machine.},
 author = {Judd, Stephen and Munro, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c361bc7b2c033a83d663b8d9fb4be56e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c361bc7b2c033a83d663b8d9fb4be56e-Metadata.json},
 openalex = {W2155189051},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c361bc7b2c033a83d663b8d9fb4be56e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Nets with Unreliable Hidden Nodes Learn Error-Correcting Codes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/c361bc7b2c033a83d663b8d9fb4be56e-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_c3992e9a,
 abstract = {Human vision systems integrate information nonlocally, across long spatial ranges. For example, a moving stimulus appears smeared when viewed briefly (30 ms), yet sharp when viewed for a longer exposure (100 ms) (Burr, 1980). This suggests that visual systems combine information along a trajectory that matches the motion of the stimulus. Our self-organizing neural network model shows how developmental exposure to moving stimuli can direct the formation of horizontal trajectory-specific motion integration pathways that unsmear representations of moving stimuli. These results account for Burr's data and can potentially also model other phenomena, such as visual inertia.},
 author = {Martin, Kevin and Marshall, Jonathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c3992e9a68c5ae12bd18488bc579b30d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c3992e9a68c5ae12bd18488bc579b30d-Metadata.json},
 openalex = {W2164446118},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Unsmearing Visual Motion: Development of Long-Range Horizontal Intrinsic Connections},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_c5ab0bc6,
 abstract = {In this work we apply a texture classification network to remote sensing image analysis. The goal is to extract the characteristics of the area depicted in the input image, thus achieving a segmented map of the region. We have recently proposed a combined neural network and rule-based framework for texture recognition. The framework uses unsupervised and supervised learning, and provides probability estimates for the output classes. We describe the texture classification network and extend it to demonstrate its application to the Landsat and Aerial image analysis domain.},
 author = {Greenspan, Hayit K. and Goodman, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c5ab0bc60ac7929182aadd08703f1ec6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c5ab0bc60ac7929182aadd08703f1ec6-Metadata.json},
 openalex = {W2134401318},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/c5ab0bc60ac7929182aadd08703f1ec6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Remote Sensing Image Analysis via a Texture Classification Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/c5ab0bc60ac7929182aadd08703f1ec6-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_ca9c267d,
 abstract = {Neurons in area MT of primate visual cortex encode the velocity of moving objects. We present a model of how MT cells aggregate responses from VI to form such a velocity representation. Two different sets of units, with local receptive fields, receive inputs from motion energy filters. One set of units forms estimates of local motion, while the second set computes the utility of these estimates. Outputs from this second set of units gate the outputs from the first set through a gain control mechanism. This active process for selecting only a subset of local motion responses to integrate into more global responses distinguishes our model from previous models of velocity estimation. The model yields accurate velocity estimates in synthetic images containing multiple moving targets of varying size, luminance, and spatial frequency profile and deals well with a number of transparency phenomena.},
 author = {Nowlan, Steven and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/ca9c267dad0305d1a6308d2a0cf1c39c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/ca9c267dad0305d1a6308d2a0cf1c39c-Metadata.json},
 openalex = {W2169145282},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/ca9c267dad0305d1a6308d2a0cf1c39c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Filter Selection Model for Generating Visual Motion Signals},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/ca9c267dad0305d1a6308d2a0cf1c39c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_cdc0d6e6,
 abstract = {A method for creating a non-linear encoder-decoder for multidimensional data with compact representations is presented. The commonly used technique of autoassociation is extended to allow non-linear representations, and an objective function which penalizes activations of individual hidden units is shown to result in minimum dimensional encodings with respect to allowable error in reconstruction.},
 author = {DeMers, David and Cottrell, Garrison},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/cdc0d6e63aa8e41c89689f54970bb35f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/cdc0d6e63aa8e41c89689f54970bb35f-Metadata.json},
 openalex = {W2123421115},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/cdc0d6e63aa8e41c89689f54970bb35f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Non-Linear Dimensionality Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/cdc0d6e63aa8e41c89689f54970bb35f-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_d14220ee,
 abstract = {One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who, in turn, learn how to satisfy them. Submanagers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command.

We illustrate the system using a simple maze task. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map.},
 author = {Dayan, Peter and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d14220ee66aeec73c49038385428ec4c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d14220ee66aeec73c49038385428ec4c-Metadata.json},
 openalex = {W2160371091},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d14220ee66aeec73c49038385428ec4c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Feudal Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/d14220ee66aeec73c49038385428ec4c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_d490d7b4,
 abstract = {The planar thallium-201 ( 201 Tl) myocardial perfusion scintigram is a widely used diagnostic technique for detecting and estimating the risk of coronary artery disease. Interpretation is currently based on visual scoring of myocardial defects combined with image quantitation and is known to have a significant subjective component. Neural networks learned to interpret thallium scintigrams as determined by both individual and multiple (consensus) expert ratings. Four different types of networks were explored: single-layer, two-layer backpropagation (BP), BP with weight smoothing, and two-layer radial basis function (RBF). The RBF network was found to yield the best performance (94.8% generalization by region) and compares favorably with human experts. We conclude that this network is a valuable clinical tool that can be used as a reference "diagnostic support system" to help reduce inter- and intraobserver variability. This system is now being further developed to include other variables that are expected to improve the final clinical diagnosis.},
 author = {Rosenberg, Charles and Erel, Jacob and Atlan, Henri},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d490d7b4576290fa60eb31b5fc917ad1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d490d7b4576290fa60eb31b5fc917ad1-Metadata.json},
 openalex = {W1991513497},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Neural Network That Learns to Interpret Myocardial Planar Thallium Scintigrams},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/d490d7b4576290fa60eb31b5fc917ad1-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_d6c651dd,
 abstract = {We have trained networks of Σ - II units with short-range connections to simulate simple cellular automata that exhibit complex or chaotic behaviour. Three levels of learning are possible (in decreasing order of difficulty): learning the underlying automaton rule, learning asymptotic dynamical behaviour, and learning to extrapolate the training history. The levels of learning achieved with and without weight sharing for different automata provide new insight into their dynamics.},
 author = {Wulff, N. and Hertz, J A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d6c651ddcd97183b2e40bc464231c962-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d6c651ddcd97183b2e40bc464231c962-Metadata.json},
 openalex = {W2111902629},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d6c651ddcd97183b2e40bc464231c962-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Cellular Automaton Dynamics with Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/d6c651ddcd97183b2e40bc464231c962-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_d7a728a6,
 abstract = {We present an algorithm for creating a neural network which produces accurate probability estimates as outputs. The network implements a Gibbs probability distribution model of the training database. This model is created by a new transformation relating the joint probabilities of attributes in the database to the weights (Gibbs potentials) of the distributed network model. The theory of this transformation is presented together with experimental results. One advantage of this approach is the network weights are prescribed without iterative gradient descent. Used as a classifier the network tied or outperformed published results on a variety of databases.},
 author = {Miller, John and Goodman, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d7a728a67d909e714c0774e22cb806f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d7a728a67d909e714c0774e22cb806f2-Metadata.json},
 openalex = {W2110159558},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d7a728a67d909e714c0774e22cb806f2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Probability Estimation from a Database Using a Gibbs Energy Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/d7a728a67d909e714c0774e22cb806f2-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_d86ea612,
 abstract = {Representations for semantic information about words are necessary for many applications of neural networks in natural language processing. This paper describes an efficient, corpus-based method for inducing distributed semantic representations for a large number of words (50,000) from lexical coccurrence statistics by means of a large-scale linear regression. The representations are successfully applied to word sense disambiguation using a nearest neighbor method.},
 author = {Sch\"{u}tze, Hinrich},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d86ea612dec96096c5e0fcc8dd42ab6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d86ea612dec96096c5e0fcc8dd42ab6d-Metadata.json},
 openalex = {W2295097532},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Word Space},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_daca4121,
 abstract = {We have designed, fabricated, and tested an analog VLSI chip which computes radial basis functions in parallel. We have developed a synapse circuit that approximates a quadratic function. We aggregate these circuits to form radial basis functions. These radial basis functions are then averaged together using a follower aggregator.},
 author = {Anderson, Janeen and Platt, John and Kirk, David B.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/daca41214b39c5dc66674d09081940f0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/daca41214b39c5dc66674d09081940f0-Metadata.json},
 openalex = {W2115205246},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/daca41214b39c5dc66674d09081940f0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Analog VLSI Chip for Radial Basis Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/daca41214b39c5dc66674d09081940f0-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_dbe272ba,
 abstract = {Recurrent networks of threshold elements have been studied intensively as associative memories and pattern-recognition devices. While most research has concentrated on fully-connected symmetric networks, which relax to stable fixed points, asymmetric networks show richer dynamical behavior, and can be used as sequence generators or flexible pattern-recognition devices. In this paper, we approach the problem of predicting the complex global behavior of a class of random asymmetric networks in terms of network parameters. These networks can show fixed-point, cyclical or effectively aperiodic behavior, depending on parameter values, and our approach can be used to set parameters, as necessary, to obtain a desired complexity of dynamics. The approach also provides qualitative insight into why the system behaves as it does and suggests possible applications.},
 author = {Minai, Ali and Levy, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/dbe272bab69f8e13f14b405e038deb64-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/dbe272bab69f8e13f14b405e038deb64-Metadata.json},
 openalex = {W2115234748},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Predicting Complex Behavior in Sparse Asymmetric Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/dbe272bab69f8e13f14b405e038deb64-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_dc6a7071,
 abstract = {The occurence of chaos in recurrent neural networks is supposed to depend on the architecture and on the synaptic coupling strength. It is studied here for a randomly diluted architecture. By normalizing the variance of synaptic weights, we produce a bifurcation parameter, dependent on this variance and on the slope of the transfer function but independent of the connectivity, that allows a sustained activity and the occurence of chaos when reaching a critical value. Even for weak connectivity and small size, we find numerical results in accordance with the theoretical ones previously established for fully connected infinite sized networks. Moreover the route towards chaos is numerically checked to be a quasi-periodic one, whatever the type of the first bifurcation is (Hopf bifurcation, pitchfork or flip).},
 author = {Doyon, Bernard and Cessac, Bruno and Quoy, Mathias and Samuelides, Manuel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/dc6a70712a252123c40d2adba6a11d84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/dc6a70712a252123c40d2adba6a11d84-Metadata.json},
 openalex = {W2110804722},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/dc6a70712a252123c40d2adba6a11d84-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Destabilization and Route to Chaos in Neural Networks with Random Connectivity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/dc6a70712a252123c40d2adba6a11d84-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_dc82d632,
 abstract = {This paper reports on the performance of two methods for recognition-based segmentation of strings of on-line handprinted capital Latin characters. The input strings consist of a time-ordered sequence of X-Y coordinates, punctuated by pen-lifts. The methods were designed to work in run-on mode where there is no constraint on the spacing between characters. While both methods use a neural network recognition engine and a graph-algorithmic post-processor, their approaches to segmentation are quite different. The first method, which we call INSEG (for input segmentation), uses a combination of heuristics to identify particular penlifts as tentative segmentation points. The second method, which we call OUTSEG (for output segmentation), relies on the empirically trained recognition engine for both recognizing characters and identifying relevant segmentation points.},
 author = {Schenkel, M. and Weissman, H. and Guyon, I. and Nohl, C. and Henderson, D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/dc82d632c9fcecb0778afbc7924494a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/dc82d632c9fcecb0778afbc7924494a6-Metadata.json},
 openalex = {W2141207507},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Recognition-based Segmentation of On-Line Hand-printed Words},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/dc82d632c9fcecb0778afbc7924494a6-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_e4bb4c51,
 abstract = {The feed-forward networks with fixed hidden units (FHU-networks) are compared against the category of remaining feed-forward networks with variable hidden units (VHU-networks). Two broad classes of tasks on a finite domain X ⊂ Rn are considered: approximation of every function from an open subset of functions on X and representation of every dichotomy of X. For the first task it is found that both network categories require the same minimal number of synaptic weights. For the second task and X in general position it is shown that VHU-networks with threshold logic hidden units can have approximately 1/n times fewer hidden units than any FHU-network must have.},
 author = {Kowalczyk, Adam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e4bb4c5173c2ce17fd8fcd40041c068f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e4bb4c5173c2ce17fd8fcd40041c068f-Metadata.json},
 openalex = {W2161355392},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Some Estimates of Necessary Number of Connections and Hidden Units for Feed-Forward Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/e4bb4c5173c2ce17fd8fcd40041c068f-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_e555ebe0,
 abstract = {We compare activation functions in terms of the approximation power of their feedforward nets. We consider the case of analog as well as boolean input.},
 author = {DasGupta, Bhaskar and Schnitger, Georg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e555ebe0ce426f7f9b2bef0706315e0c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e555ebe0ce426f7f9b2bef0706315e0c-Metadata.json},
 openalex = {W2160397751},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e555ebe0ce426f7f9b2bef0706315e0c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Power of Approximating: a Comparison of Activation Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/e555ebe0ce426f7f9b2bef0706315e0c-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_e5841df2,
 abstract = {Two theorems and a lemma are presented about the use of jackknife estimator and the cross-validation method for model selection. Theorem 1 gives the asymptotic form for the jackknife estimator. Combined with the model selection criterion, this asymptotic form can be used to obtain the fit of a model. The model selection criterion we used is the negative of the average predictive likehood, the choice of which is based on the idea of the cross-validation method. Lemma 1 provides a formula for further exploration of the asymptotics of the model selection criterion. Theorem 2 gives an asymptotic form of the model selection criterion for the regression case, when the parameters optimization criterion has a penalty term. Theorem 2 also proves the asymptotic equivalence of Moody's model selection criterion (Moody, 1992) and the cross-validation method, when the distance measure between response y and regression function takes the form of a squared difference.},
 author = {Liu, Yong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e5841df2166dd424a57127423d276bbe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e5841df2166dd424a57127423d276bbe-Metadata.json},
 openalex = {W2146767532},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e5841df2166dd424a57127423d276bbe-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Model Selection Using Asymptotic Jackknife Estimator and Cross-Validation Method},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/e5841df2166dd424a57127423d276bbe-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_e7061188,
 abstract = {A number of hybrid multilayer perceptron (MLP)/hidden Markov model (HMM) speech recognition systems have been developed in recent years (Morgan and Bourlard, 1990). In this paper, we present a new MLP architecture and training algorithm which allows the modeling of context-dependent phonetic classes in a hybrid MLP/HMM framework. The new training procedure smooths MLPs trained at different degrees of context dependence in order to obtain a robust estimate of the context-dependent probabilities. Tests with the DARPA Resource Management database have shown substantial advantages of the context-dependent MLPs over earlier context-independent MLPs, and have shown substantial advantages of this hybrid approach over a pure HMM approach.},
 author = {Cohen, Michael and Franco, Horacio and Morgan, Nelson and Rumelhart, David and Abrash, Victor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e70611883d2760c8bbafb4acb29e3446-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e70611883d2760c8bbafb4acb29e3446-Metadata.json},
 openalex = {W2144578449},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e70611883d2760c8bbafb4acb29e3446-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Context-Dependent Multiple Distribution Phonetic Modeling with MLPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/e70611883d2760c8bbafb4acb29e3446-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_e7f8a7fb,
 abstract = {The Bayesian ``evidence'' approximation, which is closely related to generalized maximum likelihood, has recently been employed to determine the noise and weight-penalty terms for training neural nets. This paper shows that it is far simpler to perform the exact calculation than it is to set up the evidence approximation. Moreover, unlike that approximation, the exact result does not have to be re-calculated for every new data set. Nor does it require the running of complex numerical computer code (the exact result is closed form). In addition, it turns out that for neural nets, the evidence procedure's MAP estimate is {\it in toto} approximation error. Another advantage of the exact analysis is that it does not lead to incorrect intuition, like the claim that one can ``evaluate different priors in light of the data.'' This paper ends by discussing sufficiency conditions for the evidence approximation to hold, along with the implications of those conditions. Although couched in terms of neural nets, the anlaysis of this paper holds for any Bayesian interpolation problem.},
 author = {Wolpert, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e7f8a7fb0b77bcb3b283af5be021448f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e7f8a7fb0b77bcb3b283af5be021448f-Metadata.json},
 openalex = {W3122150125},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/e7f8a7fb0b77bcb3b283af5be021448f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {On the Use of Evidence in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/e7f8a7fb0b77bcb3b283af5be021448f-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_eaae339c,
 abstract = {Large VC-dimension classifiers can learn difficult tasks, but are usually impractical because they generalize well only if they are trained with huge quantities of data. In this paper we show that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension. This is achieved with a maximum margin algorithm (the Generalized Portrait). The technique is applicable to a wide variety of classifiers, including Perceptrons, polynomial classifiers (sigma-pi unit networks) and Radial Basis Functions. The effective number of parameters is adjusted automatically by the training algorithm to match the complexity of the problem. It is shown to equal the number of those training patterns which are closest patterns to the decision boundary (supporting patterns). Bounds on the generalization error and the speed of convergence of the algorithm are given. Experimental results on handwritten digit recognition demonstrate good generalization compared to other algorithms.},
 author = {Guyon, I. and Boser, B. and Vapnik, V.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/eaae339c4d89fc102edd9dbdb6a28915-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/eaae339c4d89fc102edd9dbdb6a28915-Metadata.json},
 openalex = {W2164096571},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/eaae339c4d89fc102edd9dbdb6a28915-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Automatic Capacity Tuning of Very Large VC-Dimension Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/eaae339c4d89fc102edd9dbdb6a28915-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_eb6fdc36,
 abstract = {The relationships between learning, development and evolution in Nature is taken seriously, to suggest a model of the developmental process whereby the genotypes manipulated by the Genetic Algorithm (GA) might be expressed to form phenotypic neural networks (NNet) that then go on to learn. ONTOL is a grammar for generating polynomial NNets for time-series prediction. Genomes correspond to an ordered sequence of ONTOL productions and define a grammar that is expressed to generate a NNet. The NNet's weights are then modified by learning, and the individual's prediction error is used to determine GA fitness. A new gene doubling operator appears critical to the formation of new genetic alternatives in the preliminary but encouraging results presented.},
 author = {Belew, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/eb6fdc36b281b7d5eabf33396c2683a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/eb6fdc36b281b7d5eabf33396c2683a2-Metadata.json},
 openalex = {W2136200203},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/eb6fdc36b281b7d5eabf33396c2683a2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Interposing an ontogenetic model between Genetic Algorithms and Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/eb6fdc36b281b7d5eabf33396c2683a2-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_f29c21d4,
 abstract = {The attempt to find a single optimal weight vector in conventional network training can lead to overfitting and poor generalization. Bayesian methods avoid this, without the need for a validation set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution.},
 author = {Neal, Radford},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/f29c21d4897f78948b91f03172341b7b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/f29c21d4897f78948b91f03172341b7b-Metadata.json},
 openalex = {W2149991584},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bayesian Learning via Stochastic Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/f29c21d4897f78948b91f03172341b7b-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_f64eac11,
 abstract = {The vestibulo-ocular reflex (VOR) is a compensatory eye movement that stabilizes images on the retina during head turns. Its magnitude, or gain, can be modified by visual experience during head movements. Possible learning mechanisms for this adaptation have been explored in a model of the oculomotor system based on anatomical and physiological constraints. The local correlational learning rules in our model reproduce the adaptation and behavior of the VOR under certain parameter conditions. From these conditions, predictions for the time course of adaptation at the learning sites are made.},
 author = {Coenen, Olivier and Sejnowski, Terrence J and Lisberger, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/f64eac11f2cd8f0efa196f8ad173178e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/f64eac11f2cd8f0efa196f8ad173178e-Metadata.json},
 openalex = {W2166330381},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/f64eac11f2cd8f0efa196f8ad173178e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Biologically Plausible Local Learning Rules for the Adaptation of the Vestibulo-Ocular Reflex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/f64eac11f2cd8f0efa196f8ad173178e-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_f76a89f0,
 abstract = {This paper describes RAPTURE - a system for revising probabilistic knowledge bases that combines neural and symbolic learning methods. RAPTURE uses a modified version of backpropagation to refine the certainty factors of a MYCIN-style rule base and uses ID3's information gain heuristic to add new rules. Results on refining two actual expert knowledge bases demonstrate that this combined approach performs better than previous methods.},
 author = {Mahoney, J. and Mooney, Raymond},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/f76a89f0cb91bc419542ce9fa43902dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/f76a89f0cb91bc419542ce9fa43902dc-Metadata.json},
 openalex = {W2168019098},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/f76a89f0cb91bc419542ce9fa43902dc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Combining Neural and Symbolic Learning to Revise Probabilistic Rule Bases},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/f76a89f0cb91bc419542ce9fa43902dc-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_fae0b27c,
 abstract = {Matched filtering has been one of the most powerful techniques employed for transient detection. Here we will show that a dynamic neural network outperforms the conventional approach. When the artificial neural network (ANN) is trained with supervised learning schemes there is a need to supply the desired signal for all time, although we are only interested in detecting the transient. In this paper we also show the effects on the detection agreement of different strategies to construct the desired signal. The extension of the Bayes decision rule (0/1 desired signal), optimal in static classification, performs worse than desired signals constructed by random noise or prediction during the background.},
 author = {Pr\'{\i}ncipe, Jos\'{e} and Zahalka, Abir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/fae0b27c451c728867a567e8c1bb4e53-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/fae0b27c451c728867a567e8c1bb4e53-Metadata.json},
 openalex = {W2118000011},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/fae0b27c451c728867a567e8c1bb4e53-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Transient Signal Detection with Neural Networks: The Search for the Desired Signal},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/fae0b27c451c728867a567e8c1bb4e53-Abstract.html},
 volume = {5},
 year = {1992}
}

@inproceedings{NIPS1992_fccb3cdc,
 abstract = {We have attempted to use information theoretic quantities for analyzing neuronal connection structure from spike trains. Two point mutual information and its maximum value, channel capacity, between a pair of neurons were found to be useful for sensitive detection of crosscorrelation and for estimation of synaptic strength, respectively. Three point mutual information among three neurons could give their interconnection structure. Therefore, our information theoretic analysis was shown to be a very powerful technique for deducing neuronal connection structure. Some concrete examples of its application to simulated spike trains are presented.},
 author = {Shiono, Satoru and Yamada, Satoshi and Nakashima, Michio and Matsumoto, Kenji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1992/file/fccb3cdc9acc14a6e70a12f74560c026-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1992/file/fccb3cdc9acc14a6e70a12f74560c026-Metadata.json},
 openalex = {W2126501523},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1992/file/fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Information Theoretic Analysis of Connection Structure from Spike Trains},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/hash/fccb3cdc9acc14a6e70a12f74560c026-Abstract.html},
 volume = {5},
 year = {1992}
}
