@inproceedings{NIPS1994_01882513,
 abstract = {We present a new algorithm for finding low complexity networks with high generalization capability. The algorithm searches for large connected regions of so-called minima of the error function. In the weight-space environment of a minimum, the error remains approximately constant. Using an MDL-based argument, flat minima can be shown to correspond to low expected overfitting. Although our algorithm requires the computation of second order derivatives, it has backprop's order of complexity. Experiments with feedforward and recurrent nets are described. In an application to stock market prediction, the method outperforms conventional backprop, weight decay, and optimal brain surgeon.},
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/01882513d5fa7c329e940dda99b12147-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/01882513d5fa7c329e940dda99b12147-Metadata.json},
 openalex = {W2143163787},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/01882513d5fa7c329e940dda99b12147-Paper.pdf},
 publisher = {MIT Press},
 title = {SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/01882513d5fa7c329e940dda99b12147-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_024d7f84,
 author = {Skaggs, William and Knierim, James and Kudrimoti, Hemant and McNaughton, Bruce},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf},
 publisher = {MIT Press},
 title = {A Model of the Neural Basis of the Rat\textquotesingle s Sense of Direction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/024d7f84fff11dd7e8d9c510137a2381-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_03e0704b,
 abstract = {Diagnosis of human disease or machine fault is a missing data problem since many variables are initially unknown. Additional information needs to be obtained. The joint probability distribution of the data can be used to solve this problem. We model this with mixture models whose parameters are estimated by the EM algorithm. This gives the benefit that missing data in the database itself can also be handled correctly. The request for new information to refine the diagnosis is performed using the maximum utility principle. Since the system is based on learning it is domain independent and less labor intensive than expert systems or probabilistic networks. An example using a heart disease database is presented.},
 author = {Stensmo, Magnus and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/03e0704b5690a2dee1861dc3ad3316c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/03e0704b5690a2dee1861dc3ad3316c9-Metadata.json},
 openalex = {W2123355486},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf},
 publisher = {MIT Press},
 title = {A Mixture Model System for Medical and Machine Diagnosis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/03e0704b5690a2dee1861dc3ad3316c9-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_043c3d7e,
 abstract = {Prior knowledge constraints are imposed upon a learning problem in the form of distance measures. Prototypical 2D point sets and graphs are learned by clustering with point-matching and graph-matching distance measures. The point-matching distance measure is approximately invariant under affine transformations—translation, rotation, scale, and shear—and permutations. It operates between noisy images with missing and spurious points. The graph-matching distance measure operates on weighted graphs and is invariant under permutations. Learning is formulated as an optimization problem. Large objectives so formulated (∼ million variables) are efficiently minimized using a combination of optimization techniques—softassign, algebraic transformations, clocked objectives, and deterministic annealing.},
 author = {Gold, Steven and Rangarajan, Anand and Mjolsness, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/043c3d7e489c69b48737cc0c92d0f3a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/043c3d7e489c69b48737cc0c92d0f3a2-Metadata.json},
 openalex = {W2133277031},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning with Preknowledge: Clustering with Point and Graph Matching Distance Measures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/043c3d7e489c69b48737cc0c92d0f3a2-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_061412e4,
 abstract = {We present a new method for obtaining local error bars for nonlinear regression, i.e., estimates of the confidence in predicted values that depend on the input. We approach this problem by applying a maximum-likelihood framework to an assumed distribution of errors. We demonstrate our method first on computer-generated data with locally varying, normally distributed target noise. We then apply it to laser data from the Santa Fe Time Series Competition where the underlying system noise is known quantization error and the error bars give local estimates of model misspecification. In both cases, the method also provides a weighted regression effect that improves generalization performance.},
 author = {Nix, David and Weigend, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/061412e4a03c02f9902576ec55ebbe77-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/061412e4a03c02f9902576ec55ebbe77-Metadata.json},
 openalex = {W2106644119},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Local Error Bars for Nonlinear Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/061412e4a03c02f9902576ec55ebbe77-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_07871915,
 abstract = {Semi-Markov Decision Problems are continuous time generalizations of discrete time Markov Decision Problems. A number of reinforcement learning algorithms have been developed recently for the solution of Markov Decision Problems, based on the ideas of asynchronous dynamic programming and stochastic approximation. Among these are TD(λ), Q-learning, and Real-time Dynamic Programming. After reviewing semi-Markov Decision Problems and Bellman's optimality equation in that context, we propose algorithms similar to those named above, adapted to the solution of semi-Markov Decision Problems. We demonstrate these algorithms by applying them to the problem of determining the optimal control for a simple queueing system. We conclude with a discussion of circumstances under which these algorithms may be usefully applied.},
 author = {Bradtke, Steven and Duff, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Metadata.json},
 openalex = {W2153947321},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning Methods for Continuous-Time Markov Decision Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/07871915a8107172b3b5dc15a6574ad3-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_08fe2621,
 abstract = {The paper presents a rapid speaker-normalization technique based on neural network spectral mapping. The neural network is used as a front-end of a continuous speech recognition system (speaker-dependent, HMM-based) to normalize the input acoustic data from a new speaker. The spectral difference between speakers can be reduced using a limited amount of new acoustic data (40 phonetically rich sentences). Recognition error of phone units from the acoustic-phonetic continuous speech corpus APASCI is decreased with an adaptability ratio of 25%. We used local basis networks of elliptical Gaussian kernels, with recursive allocation of units and on-line optimization of parameters (GRAN model). For this application, the model included a linear term. The results compare favorably with multivariate linear mapping based on constrained orthonormal transformations.},
 author = {Furlanello, Cesare and Giuliani, Diego and Trentin, Edmondo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/08fe2621d8e716b02ec0da35256a998d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/08fe2621d8e716b02ec0da35256a998d-Metadata.json},
 openalex = {W2163242682},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/08fe2621d8e716b02ec0da35256a998d-Paper.pdf},
 publisher = {MIT Press},
 title = {Connectionist Speaker Normalization with Generalized Resource Allocating Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/08fe2621d8e716b02ec0da35256a998d-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_0a113ef6,
 abstract = {Songbirds learn to imitate a tutor song through auditory and motor learning. We have developed a theoretical framework for song learning that accounts for response properties of neurons that have been observed in many of the nuclei that are involved in song learning. Specifically, we suggest that the anterior forebrain pathway, which is not needed for song production in the adult but is essential for song acquisition, provides synaptic perturbations and adaptive evaluations for syllable vocalization learning. A computer model based on reinforcement learning was constructed that could replicate a real zebra finch song with 90% accuracy based on a spectrographic measure. The second generation of the birdsong model replicated the tutor song with 96% accuracy.},
 author = {Doya, Kenji and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0a113ef6b61820daa5611c870ed8d5ee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0a113ef6b61820daa5611c870ed8d5ee-Metadata.json},
 openalex = {W2145700039},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf},
 publisher = {MIT Press},
 title = {A Novel Reinforcement Model of Birdsong Vocalization Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_0b8aff04,
 abstract = {We consider the effect of combining several least squares estimators on the expected performance of a regression problem. Computing the exact bias and variance curves as a function of the sample size we are able to quantitatively compare the effect of the combination on the bias and variance separately, and thus on the expected error which is the sum of the two. Our exact calculations, demonstrate that the combination of estimators is particularly useful in the case where the data set is small and noisy and the function to be learned is unrealizable. For large data sets the single estimator produces superior results. Finally, we show that by splitting the data set into several independent parts and training each estimator on a different subset, the performance can in some cases be significantly improved.},
 author = {Meir, Ronny},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0b8aff0438617c055eb55f0ba5d226fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0b8aff0438617c055eb55f0ba5d226fa-Metadata.json},
 openalex = {W2167829771},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0b8aff0438617c055eb55f0ba5d226fa-Paper.pdf},
 publisher = {MIT Press},
 title = {Bias, Variance and the Combination of Least Squares Estimators},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/0b8aff0438617c055eb55f0ba5d226fa-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_0d0871f0,
 abstract = {We incorporate the hierarchical mixtures of experts (HME) method of probability estimation, developed by Jordan (see Neural Computation, 1994), into an HMM-based continuous speech recognition system. The resulting system can be thought of as a continuous-density HMM system, but instead of using Gaussian mixtures, the HME system employs a large set of hierarchically organized but relatively small neural networks to perform the probability density estimation. The hierarchical structure is reminiscent of a decision tree except for two important differences: each "expert" or neural net performs a "soft" decision rather than a hard decision, and, unlike ordinary decision trees, the parameters of all the neural nets in the HME are automatically trainable using the EM algorithm. We report results on the ARPA 5,000-word and 40,000-word Wall Street Journal corpus using HME models.},
 author = {Zhao, Ying and Schwartz, Richard and Sroka, Jason and Makhoul, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0d0871f0806eae32d30983b62252da50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0d0871f0806eae32d30983b62252da50-Metadata.json},
 openalex = {W1808825387},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0d0871f0806eae32d30983b62252da50-Paper.pdf},
 publisher = {MIT Press},
 title = {Hierarchical mixtures of experts methodology applied to continuous speech recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/0d0871f0806eae32d30983b62252da50-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_0efe3284,
 abstract = {We present a unifying view of discrete-time operator models used in the context of finite word length linear signal processing. Comparisons are made between the recently presented gamma operator model, and the delta and rho operator models for performing nonlinear system identification and prediction using neural networks. A new model based on an adaptive bilinear transformation which generalizes all of the above models is presented.},
 author = {Back, Andrew and Tsoi, Ah},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0efe32849d230d7f53049ddc4a4b0c60-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0efe32849d230d7f53049ddc4a4b0c60-Metadata.json},
 openalex = {W2100341880},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf},
 publisher = {MIT Press},
 title = {A Comparison of Discrete-Time Operator Models for Nonlinear System Identification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_0f840be9,
 abstract = {Hinton [6] proposed that generalization in artificial neural nets should improve if nets learn to represent the domain's underlying regularities. Abu-Mustafa's hints work [1] shows that the outputs of a backprop net can be used as inputs through which domain-specific information can be given to the net. We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify five mechanisms by which multitask backprop improves generalization and give empirical evidence that multitask backprop generalizes better in real domains.},
 author = {Caruana, Rich},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Metadata.json},
 openalex = {W2168939893},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Many Related Tasks at the Same Time with Backpropagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/0f840be9b8db4d3fbd5ba2ce59211f55-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_1587965f,
 abstract = {Visualizing and structuring pairwise dissimilarity data are difficult combinatorial optimization problems known as multidimensional scaling or pairwise data clustering. Algorithms for embedding dissimilarity data set in a Euclidian space, for clustering these data and for actively selecting data to support the clustering process are discussed in the maximum entropy framework. Active data selection provides a strategy to discover structure in a data set efficiently with partially unknown data.},
 author = {Hofmann, Thomas and Buhmann, Joachim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1587965fb4d4b5afe8428a4a024feb0d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1587965fb4d4b5afe8428a4a024feb0d-Metadata.json},
 openalex = {W2161638904},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1587965fb4d4b5afe8428a4a024feb0d-Paper.pdf},
 publisher = {MIT Press},
 title = {Multidimensional Scaling and Data Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/1587965fb4d4b5afe8428a4a024feb0d-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_168908dd,
 abstract = {Experiments demonstrated that sigmoid multilayer perceptron (MLP) networks provide slightly better risk prediction than conventional logistic regression when used to predict the risk of death, stroke, and renal failure on 1257 patients who underwent coronary artery bypass operations at the Lahey Clinic. networks with no hidden layer and networks with one hidden layer were trained using stochastic gradient descent with early stopping. networks and logistic regression used the same input features and were evaluated using bootstrap sampling with 50 replications. ROC areas for predicting mortality using preoperative input features were 70.5% for logistic regression and 76.0% for networks. Regularization provided by early stopping was an important component of improved performance. A simplified approach to generating intervals for risk predictions using an auxiliary confidence MLP was developed. The is trained to reproduce intervals that were generated during training using the outputs of 50 networks trained with different bootstrap samples.},
 author = {Lippmann, Richard P and Kukolich, Linda and Shahian, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/168908dd3227b8358eababa07fcaf091-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/168908dd3227b8358eababa07fcaf091-Metadata.json},
 openalex = {W2144633776},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/168908dd3227b8358eababa07fcaf091-Paper.pdf},
 publisher = {MIT Press},
 title = {Predicting the Risk of Complications in Coronary Artery Bypass Operations using Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/168908dd3227b8358eababa07fcaf091-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_170c9449,
 abstract = {The parietal cortex is thought to represent the egocentric positions of objects in particular coordinate systems. We propose an alternative approach to spatial perception of objects in the parietal cortex from the perspective of sensorimotor transformations. The responses of single parietal neurons can be modeled as a gaussian function of retinal position multiplied by a sigmoid function of eye position, which form a set of basis functions. We show here how these basis functions can be used to generate receptive fields in either retinotopic or head-centered coordinates by simple linear transformations. This raises the possibility that the parietal cortex does not attempt to compute the positions of objects in a particular frame of reference but instead computes a general purpose representation of the retinal location and eye position from which any transformation can be synthesized by direct projection. This representation predicts that hemineglect, a neurological syndrome produced by parietal lesions, should not be confined to egocentric coordinates, but should be observed in multiple frames of reference in single patients, a prediction supported by several experiments.},
 author = {Pouget, Alexandre and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/170c944978496731ba71f34c25826a34-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/170c944978496731ba71f34c25826a34-Metadata.json},
 openalex = {W2109035914},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/170c944978496731ba71f34c25826a34-Paper.pdf},
 publisher = {MIT Press},
 title = {Spatial Representations in the Parietal Cortex May Use Basis Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/170c944978496731ba71f34c25826a34-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_1c1d4df5,
 abstract = {Increasing attention has been paid to reinforcement learning algorithms in recent years, partly due to successes in the theoretical analysis of their behavior in Markov environments. If the Markov assumption is removed, however, neither generally the algorithms nor the analyses continue to be usable. We propose and analyze a new learning algorithm to solve a certain class of non-Markov decision problems. Our algorithm applies to problems in which the environment is Markov, but the learner has restricted access to state information. The algorithm involves a Monte-Carlo policy evaluation combined with a policy improvement method that is similar to that of Markov decision problems and is guaranteed to converge to a local maximum. The algorithm operates in the space of stochastic policies, a space which can yield a policy that performs considerably better than any deterministic policy. Although the space of stochastic policies is continuous--even for a discrete action space--our algorithm is computationally tractable.},
 author = {Jaakkola, Tommi and Singh, Satinder and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Metadata.json},
 openalex = {W2160067530},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/1c1d4df596d01da60385f0bb17a4a9e0-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_1cc3633c,
 abstract = {The basic paradigm for learning in neural networks is 'learning from examples' where a training set of input-output examples is used to teach the network the target function. Learning from hints is a generalization of learning from examples where additional information about the target function can be incorporated in the same learning process. Such information can come from common sense rules or special expertise. In financial market applications where the training data is very noisy, the use of such hints can have a decisive advantage. We demonstrate the use of hints in foreign-exchange trading of the U.S. Dollar versus the British Pound, the German Mark, the Japanese Yen, and the Swiss Franc, over a period of 32 months. We explain the general method of learning from hints and how it can be applied to other markets. The learning model for this method is not restricted to neural networks.},
 author = {Abu-Mostafa, Yaser},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1cc3633c579a90cfdd895e64021e2163-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1cc3633c579a90cfdd895e64021e2163-Metadata.json},
 openalex = {W2163311648},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1cc3633c579a90cfdd895e64021e2163-Paper.pdf},
 publisher = {MIT Press},
 title = {FINANCIAL APPLICATIONS OF LEARNING FROM HINTS},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/1cc3633c579a90cfdd895e64021e2163-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_1ce927f8,
 abstract = {The localization and orientation to various novel or interesting events in the environment is a critical sensorimotor ability in all animals, predator or prey. In mammals, the superior colliculus (SC) plays a major role in this behavior, the deeper layers exhibiting topographically mapped responses to visual, auditory, and somatosensory stimuli. Sensory information arriving from different modalities should then be represented in the same coordinate frame. Auditory cues, in particular, are thought to be computed in head-based coordinates which must then be transformed to retinal coordinates. In this paper, an analog VLSI implementation for auditory localization in the azimuthal plane is described which extends the architecture proposed for the barn owl to a primate eye movement system where further transformation is required. This transformation is intended to model the projection in primates from auditory cortical areas to the deeper layers of the primate superior colliculus. This system is interfaced with an analog VLSI-based saccadic eye movement system also being constructed in our laboratory.},
 author = {Horiuchi, Timothy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1ce927f875864094e3906a4a0b5ece68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1ce927f875864094e3906a4a0b5ece68-Metadata.json},
 openalex = {W2155814377},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf},
 publisher = {MIT Press},
 title = {An Auditory Localization and Coordinate Transform Chip},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/1ce927f875864094e3906a4a0b5ece68-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_1e056d2b,
 abstract = {Random errors and insufficiencies in databases limit the performance of any classifier trained from and applied to the database. In this paper we propose a method to estimate the limiting performance of classifiers imposed by the database. We demonstrate this technique on the task of predicting failure in telecommunication paths.},
 author = {Cortes, Corinna and Jackel, L. D. and Chiang, Wan-Ping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1e056d2b0ebd5c878c550da6ac5d3724-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1e056d2b0ebd5c878c550da6ac5d3724-Metadata.json},
 openalex = {W1513440331},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf},
 publisher = {MIT Press},
 title = {Limits on learning machine accuracy imposed by data quality},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_1e48c442,
 abstract = {Experiments were performed to reveal some of the computational properties of the human motor memory system. We show that as humans practice reaching movements while interacting with a novel mechanical environment, they learn an internal model of the inverse dynamics of that environment. Subjects show recall of this model at testing sessions 24 hours after the initial practice. The representation of the internal model in memory is such that there is interference when there is an attempt to learn a new inverse dynamics map immediately after an anticorrelated mapping was learned. We suggest that this interference is an indication that the same computational elements used to encode the first inverse dynamics map are being used to learn the second mapping. We predict that this leads to a forgetting of the initially learned skill.},
 author = {Shadmehr, Reza and Brashers-Krug, Tom and Mussa-Ivaldi, Ferdinando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Metadata.json},
 openalex = {W2129039201},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf},
 publisher = {MIT Press},
 title = {Interference in Learning Internal Models of Inverse Dynamics in Humans},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/1e48c4420b7073bc11916c6c1de226bb-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_1f4477ba,
 abstract = {The theory of Optimal Unsupervised Motor Learning shows how a network can discover a reduced-order controller for an unknown nonlinear system by representing only the most significant modes. Here, I extend the theory to apply to command sequences, so that the most significant components discovered by the network correspond to motion primitives. Combinations of these primitives can be used to produce a wide variety of different movements. I demonstrate applications to human handwriting decomposition and synthesis, as well as to the analysis of electrophysiological experiments on movements resulting from stimulation of the frog spinal cord.},
 author = {Sanger, Terence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1f4477bad7af3616c1f933a02bfabe4e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1f4477bad7af3616c1f933a02bfabe4e-Metadata.json},
 openalex = {W2129071973},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimal Movement Primitives},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_1f50893f,
 author = {Baluja, Shumeet and Pomerleau, Dean A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1f50893f80d6830d62765ffad7721742-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1f50893f80d6830d62765ffad7721742-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1f50893f80d6830d62765ffad7721742-Paper.pdf},
 publisher = {MIT Press},
 title = {Using a Saliency Map for Active Spatial Selective Attention: Implementation \&amp; Initial Results},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/1f50893f80d6830d62765ffad7721742-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_20aee3a5,
 abstract = {Many real world learning problems are best characterized by an interaction of multiple independent causes or factors. Discovering such causal structure from the data is the focus of this paper. Based on Zemel and Hinton's cooperative vector quantizer (CVQ) architecture, an unsupervised learning algorithm is derived from the Expectation-Maximization (EM) framework. Due to the combinatorial nature of the data generation process, the exact E-step is computationally intractable. Two alternative methods for computing the E-step are proposed: Gibbs sampling and mean-field approximation, and some promising empirical results are presented.},
 author = {Ghahramani, Zoubin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/20aee3a5f4643755a79ee5f6a73050ac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/20aee3a5f4643755a79ee5f6a73050ac-Metadata.json},
 openalex = {W2161335636},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf},
 publisher = {MIT Press},
 title = {Factorial Learning and the EM Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/20aee3a5f4643755a79ee5f6a73050ac-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_210f760a,
 abstract = {Multi-class classification problems can be efficiently solved by partitioning the original problem into sub-problems involving only two classes: for each pair of classes, a (potentially small) neural network is trained using only the data of these two classes. We show how to combine the outputs of the two-class neural networks in order to obtain posterior probabilities for the class decisions. The resulting probabilistic pairwise classifier is part of a handwriting recognition system which is currently applied to check reading. We present results on real world data bases and show that, from a practical point of view, these results compare favorably to other neural network approaches.},
 author = {Price, David and Knerr, Stefan and Personnaz, L\'{e}on and Dreyfus, G\'{e}rard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Metadata.json},
 openalex = {W2167477483},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf},
 publisher = {MIT Press},
 title = {Pairwise Neural Network Classifiers with Probabilistic Outputs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/210f760a89db30aa72ca258a3483cc7f-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_2387337b,
 abstract = {In this paper we present results from the first use of neural networks for real-time control of the high-temperature plasma in a tokamak fusion experiment. The tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion. In an effort to improve the energy confinement properties of the high-temperature plasma inside tokamaks, recent experiments have focused on the use of noncircular cross-sectional plasma shapes. However, the accurate generation of such plasmas represents a demanding problem involving simultaneous control of several parameters on a time scale as short as a few tens of microseconds. Application of neural networks to this problem requires fast hardware, for which we have developed a fully parallel custom implementation of a multilayer perceptron, based on a hybrid of digital and analogue techniques.},
 author = {Bishop, Chris M. and Haynes, Paul S. and Smith, Mike E U and Todd, Tom N. and Trotman, David L. and Windsor, Colin G.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2387337ba1e0b0249ba90f55b2ba2521-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2387337ba1e0b0249ba90f55b2ba2521-Metadata.json},
 openalex = {W2151292090},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf},
 publisher = {MIT Press},
 title = {Real-Time Control of a Tokamak Plasma Using Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/2387337ba1e0b0249ba90f55b2ba2521-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_23ce1851,
 abstract = {We prove the convergence of an actor/critic algorithm that is equivalent to Q-learning by construction. Its equivalence is achieved by encoding Q-values within the policy and value function of the actor and critic. The resultant actor/critic algorithm is novel in two ways: it updates the critic only when the most probable action is executed from any given state, and it rewards the actor using criteria that depend on the relative probability of the action that was executed.},
 author = {Crites, Robert and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/23ce1851341ec1fa9e0c259de10bf87c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/23ce1851341ec1fa9e0c259de10bf87c-Metadata.json},
 openalex = {W2108772579},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf},
 publisher = {MIT Press},
 title = {An Actor/Critic Algorithm that is Equivalent to Q-Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/23ce1851341ec1fa9e0c259de10bf87c-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_24896ee4,
 abstract = {Casting neural network weights in symbolic terms is crucial for interpreting and explaining the behavior of a network. Additionally, in some domains, a symbolic description may lead to more robust generalization. We present a principled approach to symbolic rule extraction based on the notion of weight templates, parameterized regions of weight space corresponding to specific symbolic expressions. With an appropriate choice of representation, we show how template parameters may be efficiently identified and instantiated to yield the optimal match to a unit's actual weights. Depending on the requirements of the application domain, our method can accommodate arbitrary disjunctions and conjunctions with O(k) complexity, simple n-of-m expressions with O(k2) complexity, or a more general class of recursive n-of-m expressions with O(k3) complexity, where k is the number of inputs to a unit. Our method of rule extraction offers several benefits over alternative approaches in the literature, and simulation results on a variety of problems demonstrate its effectiveness.},
 author = {Alexander, Jay and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/24896ee4c6526356cc127852413ea3b4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/24896ee4c6526356cc127852413ea3b4-Metadata.json},
 openalex = {W2115027220},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/24896ee4c6526356cc127852413ea3b4-Paper.pdf},
 publisher = {MIT Press},
 title = {Template-Based Algorithms for Connectionist Rule Extraction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/24896ee4c6526356cc127852413ea3b4-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_287e03db,
 abstract = {It is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning (RL) algorithms to real-world problems. Unfortunately almost all of the theory of reinforcement learning assumes lookup table representations. In this paper we address the pressing issue of combining function approximation and RL, and present 1) a function approximator based on a simple extension to state aggregation (a commonly used form of compact representation), namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state aggregation on online RL, and 4) a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non-discrete nature of soft state aggregation. Preliminary empirical results are also presented.},
 author = {Singh, Satinder and Jaakkola, Tommi and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Metadata.json},
 openalex = {W2125510930},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning with Soft State Aggregation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/287e03db1d99e0ec2edb90d079e142f3-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_298923c8,
 abstract = {Each year people spend a huge amount of time typing. The text people type typically contains a tremendous amount of redundancy due to predictable word usage patterns and the text's structure. This paper describes a neural network system call AutoTypist that monitors a person's typing and predicts what will be entered next. AutoTypist displays the most likely subsequent word to the typist, who can accept it with a single keystroke, instead of typing it in its entirety. The multi-layer perceptron at the heart of AutoTypist adapts its predictions of likely subsequent text to the user's word usage pattern, and to the characteristics of the text currently being typed. Increases in typing speed of 2-3% when typing English prose and 10-20% when typing C code have been demonstrated using the system, suggesting a potential time savings of more than 20 hours per user per year. In addition to increasing typing speed, AutoTypist reduces the number of keystrokes a user must type by a similar amount (2-3% for English, 10- 20% for computer programs). This keystroke savings has the potential to significantly reduce the frequency and severity of repeated stress injuries caused by typing, which are the most common injury suffered in today's office environment.},
 author = {Pomerleau, Dean},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/298923c8190045e91288b430794814c4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/298923c8190045e91288b430794814c4-Metadata.json},
 openalex = {W2096938337},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/298923c8190045e91288b430794814c4-Paper.pdf},
 publisher = {MIT Press},
 title = {A Connectionist Technique for Accelerated Textual Input: Letting a Network Do the Typing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/298923c8190045e91288b430794814c4-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_2a9d121c,
 abstract = {An application of reinforcement learning to a linear-quadratic, differential game is presented. The reinforcement learning system uses a recently developed algorithm, the residual gradient form of advantage updating. The game is a Markov Decision Process (MDP) with continuous time, states, and actions, linear dynamics, and a quadratic cost function. The game consists of two players, a missile and a plane; the missile pursues the plane and the plane evades the missile. The reinforcement learning algorithm for optimal control is modified for differential games in order to find the minimax point, rather than the maximum. Simulation results are compared to the optimal solution, demonstrating that the simulated reinforcement learning system converges to the optimal answer. The performance of both the residual gradient and non-residual gradient forms of advantage updating and Q-learning are compared. The results show that advantage updating converges faster than Q-learning in all simulations. The results also show advantage updating converges regardless of the time step duration; Q-learning is unable to converge as the time step duration grows small.},
 author = {Harmon, Mance E. and Baird, Leemon and Klopf, A. Harry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Metadata.json},
 openalex = {W2124215603},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Paper.pdf},
 publisher = {MIT Press},
 title = {Advantage Updating Applied to a Differential Game},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_2ab56412,
 abstract = {Deciding the appropriate representation to use for modeling human auditory processing is a critical issue in auditory science. While engineers have successfully performed many single-speaker tasks with LPC and spectrogram methods, more difficult problems will need a richer representation. This paper describes a powerful auditory representation known as the correlogram and shows how this non-linear representation can be converted back into sound, with no loss of perceptually important information. The correlogram is interesting because it is a neurophysiologically plausible representation of sound. This paper shows improved methods for spectrogram inversion (conventional pattern playback), inversion of a cochlear model, and inversion of the correlogram representation.},
 author = {Slaney, Malcolm},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2ab56412b1163ee131e1246da0955bd1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2ab56412b1163ee131e1246da0955bd1-Metadata.json},
 openalex = {W2115956902},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf},
 publisher = {MIT Press},
 title = {Pattern Playback in the 90s},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/2ab56412b1163ee131e1246da0955bd1-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_2ba59664,
 abstract = {We consider the problem of decoding block coded data, using a physical dynamical system. We sketch out a decompression algorithm for fractal block codes and then show how to implement a recurrent neural network using physically simple but highly-nonlinear, analog circuit models of neurons and synapses. The nonlinear system has many fixed points, but we have at our disposal a procedure to choose the parameters in such a way that only one solution, the desired solution, is stable. As a partial proof of the concept, we present experimental data from a small system a 16-neuron analog CMOS chip fabricated in a 2m analog p-well process. This chip operates in the subthreshold regime and, for each choice of parameters, converges to a unique stable state. Each state exhibits a qualitatively fractal shape.},
 author = {Pineda, Fernando and Andreou, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2ba596643cbbbc20318224181fa46b28-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2ba596643cbbbc20318224181fa46b28-Metadata.json},
 openalex = {W2130225117},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2ba596643cbbbc20318224181fa46b28-Paper.pdf},
 publisher = {MIT Press},
 title = {An Analog Neural Network Inspired by Fractal Block Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/2ba596643cbbbc20318224181fa46b28-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_2f885d0f,
 abstract = {Existing recurrent net learning algorithms are inadequate. We introduce the conceptual framework of viewing recurrent training as matching vector fields of dynamical systems in phase space. Phase-space reconstruction techniques make the hidden states explicit, reducing temporal learning to a feed-forward problem. In short, we propose viewing iterated prediction [LF88] as the best way of training recurrent networks on deterministic signals. Using this framework, we can train multiple trajectories, insure their stability, and design arbitrary dynamical systems.},
 author = {Tsung, Fu-Sheng and Cottrell, Garrison},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2f885d0fbe2e131bfc9d98363e55d1d4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2f885d0fbe2e131bfc9d98363e55d1d4-Metadata.json},
 openalex = {W2102058908},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2f885d0fbe2e131bfc9d98363e55d1d4-Paper.pdf},
 publisher = {MIT Press},
 title = {Phase-Space Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/2f885d0fbe2e131bfc9d98363e55d1d4-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_31b3b31a,
 abstract = {Many different discrete-time recurrent neural network architectures have been proposed. However, there has been virtually no effort to compare these architectures experimentally. In this paper we review and categorize many of these architectures and compare how they perform on various classes of simple problems including grammatical inference and nonlinear system identification.},
 author = {Horne, Bill and Giles, C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/31b3b31a1c2f8a370206f111127c0dbd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/31b3b31a1c2f8a370206f111127c0dbd-Metadata.json},
 openalex = {W2163509534},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf},
 publisher = {MIT Press},
 title = {An experimental comparison of recurrent neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/31b3b31a1c2f8a370206f111127c0dbd-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_3cef96dc,
 abstract = {In remote sensing applications ground-truth data is often used as the basis for training pattern recognition algorithms to generate thematic maps or to detect objects of interest. In practical situations, experts may visually examine the images and provide a subjective noisy estimate of the truth. Calibrating the reliability and bias of expert labellers is a non-trivial problem. In this paper we discuss some of our recent work on this topic in the context of detecting small volcanoes in Magellan SAR images of Venus. Empirical results (using the Expectation-Maximization procedure) suggest that accounting for subjective noise can be quite significant in terms of quantifying both human and algorithm detection performance.},
 author = {Smyth, Padhraic and Fayyad, Usama and Burl, Michael and Perona, Pietro and Baldi, Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3cef96dcc9b8035d23f69e30bb19218a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3cef96dcc9b8035d23f69e30bb19218a-Metadata.json},
 openalex = {W2144660879},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3cef96dcc9b8035d23f69e30bb19218a-Paper.pdf},
 publisher = {MIT Press},
 title = {Inferring Ground Truth from Subjective Labelling of Venus Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/3cef96dcc9b8035d23f69e30bb19218a-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_3df1d4b9,
 abstract = {Simard, LeCun & Denker (1993) showed that the performance of nearest-neighbor classification schemes for handwritten character recognition can be improved by incorporating invariance to specific transformations in the underlying distance metric - the so called tangent distance. The resulting classifier, however, can be prohibitively slow and memory intensive due to the large amount of prototypes that need to be stored and used in the distance comparisons. In this paper we develop rich models for representing large subsets of the prototypes. These models are either used singly per class, or as basic building blocks in conjunction with the K-means clustering algorithm.},
 author = {Hastie, Trevor and Simard, Patrice},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3df1d4b96d8976ff5986393e8767f5b2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3df1d4b96d8976ff5986393e8767f5b2-Metadata.json},
 openalex = {W2100439325},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Prototype Models for Tangent Distance},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/3df1d4b96d8976ff5986393e8767f5b2-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_3e89ebdb,
 abstract = {This paper studies the problem of diffusion in Markovian models, such as hidden Markov models (HMMs) and how it makes very difficult the task of learning of long-term dependencies in sequences. Using results from Markov chain theory, we show that the problem of diffusion is reduced if the transition probabilities approach 0 or 1. Under this condition, standard HMMs have very limited modeling capabilities, but input/output HMMs can still perform interesting computations.},
 author = {Bengio, Yoshua and Frasconi, Paolo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3e89ebdb49f712c7d90d1b39e348bbbf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3e89ebdb49f712c7d90d1b39e348bbbf-Metadata.json},
 openalex = {W2095684453},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3e89ebdb49f712c7d90d1b39e348bbbf-Paper.pdf},
 publisher = {MIT Press},
 title = {Diffusion of Credit in Markovian Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/3e89ebdb49f712c7d90d1b39e348bbbf-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_430c3626,
 abstract = {In this paper we present a new version of the standard multilayer perceptron (MLP) algorithm for the state-of-the-art in neural network VLSI implementations: the Intel Ni1000. This new version of the MLP uses a fundamental property of high dimensional spaces which allows the l2-norm to be accurately approximated by the l1-norm. This approach enables the standard MLP to utilize the parallel architecture of the Ni1000 to achieve on the order of 40000, 256-dimensional classifications per second.},
 author = {Perrone, Michael and Cooper, Leon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/430c3626b879b4005d41b8a46172e0c0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/430c3626b879b4005d41b8a46172e0c0-Metadata.json},
 openalex = {W2106737455},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf},
 publisher = {MIT Press},
 title = {The Ni1000: High Speed Parallel VLSI for Implementing Multilayer Perceptrons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/430c3626b879b4005d41b8a46172e0c0-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_4311359e,
 author = {Murphy, Sean D. and Kairiss, Edward W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4311359ed4969e8401880e3c1836fbe1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4311359ed4969e8401880e3c1836fbe1-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4311359ed4969e8401880e3c1836fbe1-Paper.pdf},
 publisher = {MIT Press},
 title = {Model of a Biological Neuron as a Temporal Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/4311359ed4969e8401880e3c1836fbe1-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_437d7d1d,
 abstract = {In this paper we consider speech coding as a problem of speech modelling. In particular, prediction of parameterised speech over short time segments is performed using the Hierarchical Mixture of Experts (HME) (Jordan & Jacobs 1994). The HME gives two advantages over traditional non-linear function approximators such as the Multi-Layer Perceptron (MLP); a statistical understanding of the operation of the predictor and provision of information about the performance of the predictor in the form of likelihood information and local error bars. These two issues are examined on both toy and real world problems of regression and time series prediction. In the speech coding context, we extend the principle of combining local predictions via the HME to a Vector Quantization scheme in which fixed local codebooks are combined on-line for each observation.},
 author = {Waterhouse, Steve and Robinson, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/437d7d1d97917cd627a34a6a0fb41136-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/437d7d1d97917cd627a34a6a0fb41136-Metadata.json},
 openalex = {W2126943512},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/437d7d1d97917cd627a34a6a0fb41136-Paper.pdf},
 publisher = {MIT Press},
 title = {Non-linear Prediction of Acoustic Vectors Using Hierarchical Mixtures of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/437d7d1d97917cd627a34a6a0fb41136-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_4b025079,
 abstract = {Unsupervised learning procedures have been successful at low-level feature extraction and preprocessing of raw sensor data. So far, however, they have had limited success in learning higher-order representations, e.g., of objects in visual images. A promising approach is to maximize some measure of agreement between the outputs of two groups of units which receive inputs physically separated in space, time or modality, as in (Becker and Hinton, 1992; Becker, 1993; de Sa, 1993). Using the same approach, a much simpler learning procedure is proposed here which discovers features in a single-layer network consisting of several populations of units, and can be applied to multi-layer networks trained one layer at a time. When trained with this algorithm on image sequences of moving geometric objects a two-layer network can learn to perform accurate position-invariant object classification.},
 author = {Becker, Suzanna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4b0250793549726d5c1ea3906726ebfe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4b0250793549726d5c1ea3906726ebfe-Metadata.json},
 openalex = {W2115657909},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf},
 publisher = {MIT Press},
 title = {JPMAX: Learning to Recognize Moving Objects as a Model-fitting Problem},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_4b6538a4,
 abstract = {The spatial distribution and time course of electrical signals in neurons have important theoretical and practical consequences. Because it is difficult to infer how neuronal form affects electrical signaling, we have developed a quantitative yet intuitive approach to the analysis of electrotonus. This approach transforms the architecture of the cell from anatomical to electrotonic space, using the logarithm of voltage attenuation as the distance metric. We describe the theory behind this approach and illustrate its use.},
 author = {Carnevale, Nicholas T and Tsai, Kenneth Y. and Claiborne, Brenda and Brown, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4b6538a44a1dfdc2b83477cd76dee98e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4b6538a44a1dfdc2b83477cd76dee98e-Metadata.json},
 openalex = {W2103296959},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf},
 publisher = {MIT Press},
 title = {The Electrotonic Transformation: a Tool for Relating Neuronal Form to Function},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/4b6538a44a1dfdc2b83477cd76dee98e-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_4daa3db3,
 author = {Principe, Jose C. and Kuo, Jyh-Ming},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4daa3db355ef2b0e64b472968cb70f0d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4daa3db355ef2b0e64b472968cb70f0d-Metadata.json},
 openalex = {W2306589129},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamic Modeling of Chaotic Time Series by Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/4daa3db355ef2b0e64b472968cb70f0d-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_4e0cb6fb,
 abstract = {We propose a statistical mechanical framework for the modeling of discrete time series. Maximum likelihood estimation is done via Boltzmann learning in one-dimensional networks with tied weights. We call these networks Boltzmann chains and show that they contain hidden Markov models (HMMs) as a special case. Our framework also motivates new architectures that address particular shortcomings of HMMs. We look at two such architectures: parallel chains that model feature sets with disparate time scales, and looped networks that model long-term dependencies between hidden states. For these networks, we show how to implement the Boltzmann learning rule exactly, in polynomial time, without resort to simulated or mean-field annealing. The necessary computations are done by exact decimation procedures from statistical mechanics.},
 author = {Saul, Lawrence and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4e0cb6fb5fb446d1c92ede2ed8780188-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4e0cb6fb5fb446d1c92ede2ed8780188-Metadata.json},
 openalex = {W2151457493},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4e0cb6fb5fb446d1c92ede2ed8780188-Paper.pdf},
 publisher = {MIT Press},
 title = {Boltzmann Chains and Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/4e0cb6fb5fb446d1c92ede2ed8780188-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_4fac9ba1,
 abstract = {Product units provide a method of automatically learning the higher-order input combinations required for efficient learning in neural networks. However, we show that problems are encountered when using backpropagation to train networks containing these units. This paper examines these problems, and proposes some atypical heuristics to improve learning. Using these heuristics a constructive method is introduced which solves well-researched problems with significantly less neurons than previously reported. Secondly, product units are implemented as candidate units in the Cascade Correlation (Fahlman & Lebiere, 1990) system. This resulted in smaller networks which trained faster than when using sigmoidal or Gaussian units.},
 author = {Leerink, Laurens and Giles, C. and Horne, Bill and Jabri, Marwan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4fac9ba115140ac4f1c22da82aa0bc7f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4fac9ba115140ac4f1c22da82aa0bc7f-Metadata.json},
 openalex = {W2119139220},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning with Product Units},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/4fac9ba115140ac4f1c22da82aa0bc7f-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_54a367d6,
 abstract = {We present efficient algorithms for dealing with the problem of missing inputs (incomplete feature vectors) during training and recall. Our approach is based on the approximation of the input data distribution using Parzen windows. For recall, we obtain closed form solutions for arbitrary feedforward networks. For training, we show how the backpropagation step for an incomplete pattern can be approximated by a weighted averaged backpropagation step. The complexity of the solutions for training and recall is independent of the number of missing features. We verify our theoretical results using one classification and one regression problem.},
 author = {Tresp, Volker and Neuneier, Ralph and Ahmad, Subutai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/54a367d629152b720749e187b3eaa11b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/54a367d629152b720749e187b3eaa11b-Metadata.json},
 openalex = {W2151779605},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/54a367d629152b720749e187b3eaa11b-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient Methods for Dealing with Missing Data in Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/54a367d629152b720749e187b3eaa11b-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_5705e116,
 abstract = {To text files, a neural predictor network P is used to approximate the conditional probability distribution of possible next characters, given n previous characters. P's outputs are fed into standard coding algorithms that generate short codes for characters with high predicted probability and long codes for highly unpredictable characters. Tested on short German newspaper articles, our method outperforms widely used Lempel-Ziv algorithms (used in UNIX functions such as compress and gzip).},
 author = {Schmidhuber, J\"{u}rgen and Heil, Stefan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5705e1164a8394aace6018e27d20d237-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5705e1164a8394aace6018e27d20d237-Metadata.json},
 openalex = {W2110984569},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5705e1164a8394aace6018e27d20d237-Paper.pdf},
 publisher = {MIT Press},
 title = {Predictive Coding with Neural Nets: Application to Text Compression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/5705e1164a8394aace6018e27d20d237-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_58e4d44e,
 abstract = {One of the fundamental properties that both neural networks and the central nervous system share is the ability to learn and generalize from examples. While this property has been studied extensively in the neural network literature it has not been thoroughly explored in human perceptual and motor learning. We have chosen a coordinate transformation system-the visuomotor map which transforms visual coordinates into motor coordinates--to study the generalization effects of learning new input-output pairs. Using a paradigm of computer controlled altered visual feedback, we have studied the generalization of the visuomotor map subsequent to both local and context-dependent remappings. A local remapping of one or two input-output pairs induced a significant global, yet decaying, change in the visuomotor map, suggesting a representation for the map composed of units with large functional receptive fields. Our study of context-dependent remappings indicated that a single point in visual space can be mapped to two different finger locations depending on a context variable-the starting point of the movement. Furthermore, as the context is varied there is a gradual shift between the two remappings, consistent with two visuomotor modules being learned and gated smoothly with the context.},
 author = {Ghahramani, Zoubin and Wolpert, Daniel M and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Metadata.json},
 openalex = {W2135872653},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf},
 publisher = {MIT Press},
 title = {Computational Structure of coordinate transformations: A generalization study},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/58e4d44e550d0f7ee0a23d6b02d9b0db-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_5c936263,
 abstract = {We construct a mixture of locally linear generative models of a collection of pixel-based images of digits, and use them for recognition. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance.},
 author = {Hinton, Geoffrey E and Revow, Michael and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5c936263f3428a40227908d5a3847c0b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5c936263f3428a40227908d5a3847c0b-Metadata.json},
 openalex = {W2114309103},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf},
 publisher = {MIT Press},
 title = {Recognizing Handwritten Digits Using Mixtures of Linear Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/5c936263f3428a40227908d5a3847c0b-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_5d616dd3,
 abstract = {More than ten of the most prominent for the structure and for the activity dependent formation of orientation and ocular dominance columns in the striate cortex have been evaluated. We implemented those on parallel machines, we extensively explored parameter space, and we quantitatively compared model predictions with experimental data which were recorded optically from macaque striate cortex.

In our contribution we present a summary of our results to date. Briefly, we find that (i) despite apparent differences, many are based on similar principles and, consequently, make similar predictions, (ii) certain pattern models as well as the developmental correlation-based learning disagree with the experimental data, and (iii) of the we have investigated, competitive Hebbian and the recent model of Swindale provide the best match with experimental data.},
 author = {Erwin, E. and Obermayer, K. and Schulten, K.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5d616dd38211ebb5d6ec52986674b6e4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5d616dd38211ebb5d6ec52986674b6e4-Metadata.json},
 openalex = {W2113020402},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5d616dd38211ebb5d6ec52986674b6e4-Paper.pdf},
 publisher = {MIT Press},
 title = {A Critical Comparison of Models for Orientation and Ocular Dominance Columns in the Striate Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/5d616dd38211ebb5d6ec52986674b6e4-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_621461af,
 abstract = {In this paper, we derive classifiers which are winner-take-all (WTA) approximations to a Bayes classifier with Gaussian mixtures for class conditional densities. The derived classifiers include clustering based algorithms like LVQ and k-Means. We propose a constrained rank Gaussian mixtures model and derive a WTA algorithm for it. Our experiments with two speech classification tasks indicate that the constrained rank model and the WTA approximations improve the performance over the unconstrained models.},
 author = {Kambhatla, Nanda and Leen, Todd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/621461af90cadfdaf0e8d4cc25129f91-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/621461af90cadfdaf0e8d4cc25129f91-Metadata.json},
 openalex = {W2147041973},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf},
 publisher = {MIT Press},
 title = {Classifying with Gaussian Mixtures and Clusters},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/621461af90cadfdaf0e8d4cc25129f91-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_6395ebd0,
 abstract = {This chapter discusses anatomical origin and computational role of diversity in the response properties of cortical neurons. A fundamental feature of cortical architecture is its columnar organization, manifested in the tendency of neurons with similar properties to be organized in columns that run perpendicular to the cortical surface. To test the effect of the ratio between axonal patch and dendritic arbor size on the diversity of the neuronal population, computer simulations based on anatomical data concerning patchy projections were conducted. The patches were modeled by disks, placed at regular intervals of twice the patch diameter, as revealed by anatomical labeling. Maximal diversity is attained when the ratio of dendritic and axonal arbor sizes is equal to one, as it is found in many cortical areas and across species. Maximization of diversity leads to better performance in systems of receptive fields implementing oriented steerable/shiftable filters, and in matching spatially distributed signals, a problem that arises in visual tasks such as stereopsis, motion processing, and recognition.},
 author = {Spector, Kalanit and Edelman, Shimon and Malach, Rafael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6395ebd0f4b478145ecfbaf939454fa4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6395ebd0f4b478145ecfbaf939454fa4-Metadata.json},
 openalex = {W2133136389},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf},
 publisher = {MIT Press},
 title = {Anatomical origin and computational role of diversity in the response properties of cortical neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/6395ebd0f4b478145ecfbaf939454fa4-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_64223ccf,
 abstract = {An novel class of locally excitatory, globally inhibitory oscillator networks is proposed. The model of each oscillator corresponds to a standard relaxation oscillator with two time scales. The network exhibits a mechanism of selective gating, whereby an oscillator jumping up to its active phase rapidly recruits the oscillators stimulated by the same pattern, while preventing others from jumping up. We show analytically that with the selective gating mechanism the network rapidly achieves both synchronization within blocks of oscillators that are stimulated by connected regions and desynchronization between different blocks. Computer simulations demonstrate the network's promising ability for segmenting multiple input patterns in real time. This model lays a physical foundation for the oscillatory correlation theory of feature binding, and may provide an effective computational framework for scene segmentation and figure/ground segregation.},
 author = {Wang, Deliang and Terman, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/64223ccf70bbb65a3a4aceac37e21016-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/64223ccf70bbb65a3a4aceac37e21016-Metadata.json},
 openalex = {W2135664279},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/64223ccf70bbb65a3a4aceac37e21016-Paper.pdf},
 publisher = {MIT Press},
 title = {Synchrony and Desynchrony in Neural Oscillator Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/64223ccf70bbb65a3a4aceac37e21016-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_65cc2c82,
 abstract = {Accumulating data from neurophysiology and neuropsychology have suggested two information processing roles for prefrontal cortex (PFC): 1) short-term active memory; and 2) inhibition. We present a new behavioral task and a computational model which were developed in parallel. The task was developed to probe both of these prefrontal functions simultaneously, and produces a rich set of behavioral data that act as constraints on the model. The model is implemented in continuous-time, thus providing a natural framework in which to study the temporal dynamics of processing in the task. We show how the model can be used to examine the behavioral consequences of neuromodulation in PFC. Specifically, we use the model to make novel and testable predictions regarding the behavioral performance of schizophrenics, who are hypothesized to suffer from reduced dopaminergic tone in this brain area.},
 author = {Braver, Todd and Cohen, Jonathan D and Servan-Schreiber, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/65cc2c8205a05d7379fa3a6386f710e1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/65cc2c8205a05d7379fa3a6386f710e1-Metadata.json},
 openalex = {W2124809797},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf},
 publisher = {MIT Press},
 title = {A Computational Model of Prefrontal Cortex Function},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/65cc2c8205a05d7379fa3a6386f710e1-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_6602294b,
 abstract = {This paper discusses the linearly weighted combination of estimators in which the weighting functions are dependent on the input. We show that the weighting functions can be derived either by evaluating the input dependent variance of each estimator or by estimating how likely it is that a given estimator has seen data in the region of the input space close to the input pattern. The latter solution is closely related to the mixture of experts approach and we show how learning rules for the mixture of experts can be derived from the theory about learning with missing features. The presented approaches are modular since the weighting functions can easily be modified (no retraining) if more estimators are added. Furthermore, it is easy to incorporate estimators which were not derived from data such as expert systems or algorithms.},
 author = {Tresp, Volker and Taniguchi, Michiaki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6602294be910b1e3c4571bd98c4d5484-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6602294be910b1e3c4571bd98c4d5484-Metadata.json},
 openalex = {W2120170037},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6602294be910b1e3c4571bd98c4d5484-Paper.pdf},
 publisher = {MIT Press},
 title = {Combining Estimators Using Non-Constant Weighting Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/6602294be910b1e3c4571bd98c4d5484-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_692f93be,
 abstract = {We present here an analysis of the stochastic neurodynamics of a neural network composed of three-state neurons described by a master equation. An outer-product representation of the master equation is employed. In this representation, an extension of the analysis from two to three-state neurons is easily performed. We apply this formalism with approximation schemes to a simple three-state network and compare the results with Monte Carlo simulations.},
 author = {Ohira, Toru and Cowan, Jack},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/692f93be8c7a41525c0baf2076aecfb4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/692f93be8c7a41525c0baf2076aecfb4-Metadata.json},
 openalex = {W2139237084},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf},
 publisher = {MIT Press},
 title = {Stochastic Dynamics of Three-State Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/692f93be8c7a41525c0baf2076aecfb4-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_6aab1270,
 abstract = {We propose a computational framework for understanding and modeling human consciousness. This framework integrates many existing theoretical perspectives, yet is sufficiently concrete to allow simulation experiments. We do not attempt to explain qualia (subjective experience), but instead ask what differences exist within the cognitive information processing system when a person is conscious of mentally-represented information versus when that information is unconscious. The central idea we explore is that the contents of consciousness correspond to temporally persistent states in a network of computational modules. Three simulations are described illustrating that the behavior of persistent states in the models corresponds roughly to the behavior of conscious states people experience when performing similar tasks. Our simulations show that periodic settling to persistent (i.e., conscious) states improves performance by cleaning up inaccuracies and noise, forcing decisions, and helping keep the system on track toward a solution.},
 author = {Mathis, Donald and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6aab1270668d8cac7cef2566a1c5f569-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6aab1270668d8cac7cef2566a1c5f569-Metadata.json},
 openalex = {W2113249096},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Computational Utility of Consciousness},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/6aab1270668d8cac7cef2566a1c5f569-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_6b180037,
 abstract = {A neural network model for the self-organization of ocular dominance and lateral connections from binocular input is presented. The self-organizing process results in a network where (1) afferent weights of each neuron organize into smooth hill-shaped receptive fields primarily on one of the retinas, (2) neurons with common eye preference form connected, intertwined patches, and (3) lateral connections primarily link regions of the same eye preference. Similar self-organization of cortical structures has been observed experimentally in strabismic kittens. The model shows how patterned lateral connections in the cortex may develop based on correlated activity and explains why lateral connection patterns follow receptive field properties such as ocular dominance.},
 author = {Sirosh, Joseph and Miikkulainen, Risto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6b180037abbebea991d8b1232f8a8ca9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6b180037abbebea991d8b1232f8a8ca9-Metadata.json},
 openalex = {W2134104854},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf},
 publisher = {MIT Press},
 title = {Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_6c3cf77d,
 abstract = {We introduce and study methods of inserting synaptic noise into dynamically-driven recurrent neural networks and show that applying a controlled amount of noise during training may improve convergence and generalization. In addition, we analyze the effects of each noise parameter (additive vs. multiplicative, cumulative vs. non-cumulative, per time step vs. per string) and predict that best overall performance can be achieved by injecting additive noise at each time step. Extensive simulations on learning the dual parity grammar from temporal strings substantiate these predictions.},
 author = {Jim, Kam and Horne, Bill and Giles, C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6c3cf77d52820cd0fe646d38bc2145ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6c3cf77d52820cd0fe646d38bc2145ca-Metadata.json},
 openalex = {W2127342049},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf},
 publisher = {MIT Press},
 title = {Effects of Noise on Convergence and Generalization in Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/6c3cf77d52820cd0fe646d38bc2145ca-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_6cfe0e61,
 abstract = {In this study, an integrated neural network control architecture for nonlinear dynamic systems is presented. Most of the recent emphasis in the neural network control field has no error feedback as the control input, which rises the lack of adaptation problem. The integrated architecture in this paper combines feed forward control and error feedback adaptive control using neural networks. The paper reveals the different internal functionality of these two kinds of neural network controllers for certain input styles, e.g., state feedback and error feedback. With error feedback, neural network controllers learn the slopes or the gains with respect to the error feedback, producing an error driven adaptive control systems. The results demonstrate that the two kinds of control scheme can be combined to realize their individual advantages. Testing with disturbances added to the plant shows good tracking and adaptation with the integrated neural control architecture.},
 author = {Liu, Ke and Tokar, Robert and McVey, Brain},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6cfe0e6127fa25df2a0ef2ae1067d915-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6cfe0e6127fa25df2a0ef2ae1067d915-Metadata.json},
 openalex = {W2126083474},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf},
 publisher = {MIT Press},
 title = {An Integrated Architecture of Adaptive Neural Network Control for Dynamic Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_704afe07,
 abstract = {This paper describes a way of neural hardware implementation with the analog-digital mixed mode neural chip. The full custom neural VLSI of Universally Reconstructible Artificial Neural network (URAN) is used to implement Korean speech recognition system. A multi-layer perceptron with linear neurons is trained successfully under the limited accuracy in computations. The network with a large frame input layer is tested to recognize spoken korean words at a forward retrieval. Multichip hardware module is suggested with eight chips or more for the extended performance and capacity.},
 author = {Han, Il and Kim, Ki-Chul and Lee, Hwang-Soo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/704afe073992cbe4813cae2f7715336f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/704afe073992cbe4813cae2f7715336f-Metadata.json},
 openalex = {W2155683073},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/704afe073992cbe4813cae2f7715336f-Paper.pdf},
 publisher = {MIT Press},
 title = {Implementation of Neural Hardware with the Neural VLSI of URAN in Applications with Reduced Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/704afe073992cbe4813cae2f7715336f-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_74bba227,
 abstract = {Most of the common techniques for estimating conditional probability densities are inappropriate for applications involving periodic variables. In this paper we introduce three novel techniques for tackling such problems, and investigate their performance using synthetic data. We then apply these techniques to the problem of extracting the distribution of wind vector directions from radar scatterometer data gathered by a remote-sensing satellite.},
 author = {Bishop, Chris M. and Legleye, Claire},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/74bba22728b6185eec06286af6bec36d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/74bba22728b6185eec06286af6bec36d-Metadata.json},
 openalex = {W2120947313},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/74bba22728b6185eec06286af6bec36d-Paper.pdf},
 publisher = {MIT Press},
 title = {Estimating Conditional Probability Densities for Periodic Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/74bba22728b6185eec06286af6bec36d-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_7504adad,
 abstract = {Understanding knowledge representations in neural nets has been a difficult problem. Principal components analysis (PCA) of contributions (products of sending activations and connection weights) has yielded valuable insights into knowledge representations, but much of this work has focused on the correlation matrix of contributions. The present work shows that analyzing the variance-covariance matrix of contributions yields more valid insights by taking account of weights.},
 author = {Shultz, Thomas and Oshima-Takane, Yuriko and Takane, Yoshio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7504adad8bb96320eb3afdd4df6e1f60-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7504adad8bb96320eb3afdd4df6e1f60-Metadata.json},
 openalex = {W2143457277},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf},
 publisher = {MIT Press},
 title = {Analysis of Unstandardized Contributions in Cross Connected Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/7504adad8bb96320eb3afdd4df6e1f60-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_7634ea65,
 abstract = {We propose a novel rigorous approach for the analysis of Linsker's unsupervised Hebbian learning network. The behavior of this model is determined by the underlying nonlinear dynamics which are parameterized by a set of parameters originating from the Hebbian rule and the arbor density of the synapses. These parameters determine the presence or absence of a specific receptive field (also referred to as a 'connection pattern') as a saturated fixed point attractor of the model. In this paper, we perform a qualitative analysis of the underlying nonlinear dynamics over the parameter space, determine the effects of the system parameters on the emergence of various receptive fields, and predict precisely within which parameter regime the network will have the potential to develop a specially designated connection pattern. In particular, this approach exposes, for the first time, the crucial role played by the synaptic density functions, and provides a complete precise picture of the parameter space that defines the relationships among the different receptive fields. Our theoretical predictions are confirmed by numerical simulations.},
 author = {Feng, J. and Pan, H. and Roychowdhury, V. P.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7634ea65a4e6d9041cfd3f7de18e334a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7634ea65a4e6d9041cfd3f7de18e334a-Metadata.json},
 openalex = {W2106882993},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},
 publisher = {MIT Press},
 title = {A Rigorous Analysis of Linsker-type Hebbian Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_766d856e,
 abstract = {This paper outlines a dynamic theory of development and adaptation in neural networks with feedback connections. Given input ensemble, the connections change in strength according to an associative learning rule and approach a stable state where the neuronal outputs are decorrelated. We apply this theory to primary visual cortex and examine the implications of the dynamical decorrelation of the activities of orientation selective cells by the intracortical connections. The theory gives a unified and quantitative explanation of the psychophysical experiments on orientation contrast and orientation adaptation. Using only one parameter, we achieve good agreements between the theoretical predictions and the experimental data.},
 author = {Dong, Dawei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/766d856ef1a6b02f93d894415e6bfa0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/766d856ef1a6b02f93d894415e6bfa0e-Metadata.json},
 openalex = {W2166829413},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf},
 publisher = {MIT Press},
 title = {Associative Decorrelation Dynamics: A Theory of Self-Organization and Optimization in Feedback Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/766d856ef1a6b02f93d894415e6bfa0e-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_7b13b220,
 abstract = {This paper presents ongoing work on a speaker independent visual speech recognition system. The work presented here builds on previous research efforts in this area and explores the potential use of simple hidden Markov models for limited vocabulary, speaker independent visual speech recognition. The task at hand is recognition of the first four English digits, a task with possible applications in car-phone dialing. The images were modeled as mixtures of independent Gaussian distributions, and the temporal dependencies were captured with standard left-to-right hidden Markov models. The results indicate that simple hidden Markov models may be used to successfully recognize relatively unprocessed image sequences. The system achieved performance levels equivalent to untrained humans when asked to recognize the first four English digits.},
 author = {Movellan, Javier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7b13b2203029ed80337f27127a9f1d28-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7b13b2203029ed80337f27127a9f1d28-Metadata.json},
 openalex = {W2142518644},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf},
 publisher = {MIT Press},
 title = {Visual Speech Recognition with Stochastic Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/7b13b2203029ed80337f27127a9f1d28-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_7ce3284b,
 abstract = {Reinforcement learning addresses the problem of learning to select actions in order to maximize one's performance in unknown environments. To scale reinforcement learning to complex real-world tasks, such as typically studied in AI, one must ultimately be able to discover the structure in the world, in order to abstract away the myriad of details and to operate in more tractable problem spaces.

This paper presents the SKILLS algorithm. SKILLS discovers skills, which are partially defined action policies that arise in the context of multiple, related tasks. Skills collapse whole action sequences into single operators. They are learned by minimizing the compactness of action policies, using a description length argument on their representation. Empirical results in simple grid navigation tasks illustrate the successful discovery of structure in reinforcement learning.},
 author = {Thrun, Sebastian and Schwartz, Anton},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7ce3284b743aefde80ffd9aec500e085-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7ce3284b743aefde80ffd9aec500e085-Metadata.json},
 openalex = {W2114451917},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7ce3284b743aefde80ffd9aec500e085-Paper.pdf},
 publisher = {MIT Press},
 title = {Finding Structure in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/7ce3284b743aefde80ffd9aec500e085-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_7f975a56,
 abstract = {For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.},
 author = {Cohn, David and Ghahramani, Zoubin and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Metadata.json},
 openalex = {W2949071206},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Learning with Statistical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/7f975a56c761db6506eca0b37ce6ec87-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_7fa732b5,
 abstract = {Ideally pattern recognition machines provide constant output when the inputs are transformed under a group G of desired invariances. These invariances can be achieved by enhancing the training data to include examples of inputs transformed by elements of G, while leaving the corresponding targets unchanged. Alternatively the cost function for training can include a regularization term that penalizes changes in the output when the input is transformed under the group. This paper relates the two approaches, showing precisely the sense in which the regularized cost function approximates the result of adding transformed examples to the training data. We introduce the notion of a probability distribution over the group transformations, and use this to rewrite the cost function for the enhanced training data. Under certain conditions, the new cost function is equivalent to the sum of the original cost function plus a regularizer. For unbiased models, the regularizer reduces to the intuitively obvious choice—a term that penalizes changes in the output when the inputs are transformed under the group. For infinitesimal transformations, the coefficient of the regularization term reduces to the variance of the distortions introduced into the training data. This correspondence provides a simple bridge between the two approaches.},
 author = {Leen, Todd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7fa732b517cbed14a48843d74526c11a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7fa732b517cbed14a48843d74526c11a-Metadata.json},
 openalex = {W1994335526},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7fa732b517cbed14a48843d74526c11a-Paper.pdf},
 publisher = {MIT Press},
 title = {From Data Distributions to Regularization in Invariant Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/7fa732b517cbed14a48843d74526c11a-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_8065d07d,
 abstract = {We introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the EM algorithm. The resulting model has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation.},
 author = {Bengio, Yoshua and Frasconi, Paolo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Metadata.json},
 openalex = {W2161523118},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},
 publisher = {MIT Press},
 title = {An Input Output HMM Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_860320be,
 abstract = {Many cells in the dorsal part of the medial superior temporal (MST) area of visual cortex respond selectively to spiral flow patterns-specific combinations of expansion/contraction and rotation motions. Previous investigators have suggested that these cells may represent self-motion. Spiral patterns can also be generated by the relative motion of the observer and a particular object. An MST cell may then account for some portion of the complex flow field, and the set of active cells could encode the entire flow; in this manner, MST effectively segments moving objects. Such a grouping operation is essential in interpreting scenes containing several independent moving objects and observer motion. We describe a model based on the hypothesis that the selective tuning of MST cells reflects the grouping of object components undergoing coherent motion. Inputs to the model were generated from sequences of ray-traced images that simulated realistic motion situations, combining observer motion, eye movements, and independent object motion. The input representation was modeled after response properties of neurons in area MT, which provides the primary input to area MST. After applying an unsupervised learning algorithm, the units became tuned to patterns signaling coherent motion. The results match many of the known properties of MST cells and are consistent with recent studies indicating that these cells process 3-D object motion information.},
 author = {Zemel, Richard and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/860320be12a1c050cd7731794e231bd3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/860320be12a1c050cd7731794e231bd3-Metadata.json},
 openalex = {W2154283681},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/860320be12a1c050cd7731794e231bd3-Paper.pdf},
 publisher = {MIT Press},
 title = {Grouping Components of Three-Dimensional Moving Objects in Area MST of Visual Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/860320be12a1c050cd7731794e231bd3-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_892c91e0,
 abstract = {A neural network learning paradigm based on information theory is proposed as a way to perform in an unsupervised fashion, redundancy reduction among the elements of the output layer without loss of information from the sensory input. The model developed performs nonlinear decorrelation up to higher orders of the cumulant tensors and results in probabilistically independent components of the output layer. This means that we don't need to assume Gaussian distribution neither at the input nor at the output. The theory presented is related to the unsupervised-learning theory of Barlow, which proposes redundancy reduction as the goal of cognition. When nonlinear units are used nonlinear principal component analysis is obtained. In this case nonlinear manifolds can be reduced to minimum dimension manifolds. If such units are used the network performs a generalized principal component analysis in the sense that non-Gaussian distributions can be linearly decorrelated and higher orders of the correlation tensors are also taken into account. The basic structure of the architecture involves a general transformation that is volume conserving and therefore the entropy, yielding a map without loss of information. Minimization of the mutual information among the output neurons eliminates the redundancy between the outputs and results in statistical decorrelation of the extracted features. This is known as factorial learning.},
 author = {Deco, Gustavo and Brauer, Wilfried},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/892c91e0a653ba19df81a90f89d99bcd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/892c91e0a653ba19df81a90f89d99bcd-Metadata.json},
 openalex = {W2101793402},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf},
 publisher = {MIT Press},
 title = {Higher Order Statistical Decorrelation without Information Loss},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/892c91e0a653ba19df81a90f89d99bcd-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_89fcd07f,
 abstract = {We estimate the number of training samples required to ensure that the performance of a neural network on its training data matches that obtained when fresh data is applied to the network. Existing estimates are higher by orders of magnitude than practice indicates. This work seeks to narrow the gap between theory and practice by transforming the problem into determining the distribution of the supremum of a random field in the space of weight vectors, which in turn is attacked by application of a recent technique called the Poisson clumping heuristic.},
 author = {Turmon, Michael and Fine, Terrence L.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/89fcd07f20b6785b92134bd6c1d0fa42-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/89fcd07f20b6785b92134bd6c1d0fa42-Metadata.json},
 openalex = {W2159487471},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf},
 publisher = {MIT Press},
 title = {Sample Size Requirements for Feedforward Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_8b5040a8,
 abstract = {We discuss a model of consistent learning with an additional restriction on the probability distribution of training samples, the target concept and hypothesis class. We show that the model provides a significant improvement on the upper bounds of sample complexity, i.e. the minimal number of random training samples allowing a selection of the hypothesis with a predefined accuracy and confidence. Further, we show that the model has the potential for providing a finite sample complexity even in the case of infinite VC-dimension as well as for a sample complexity below VC-dimension. This is achieved by linking sample complexity to an average number of implement able dichotomies of a training sample rather than the maximal size of a shattered sample, i.e. VC-dimension.},
 author = {Kowalczyk, Adam and Ferr\'{a}, Herman},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8b5040a8a5baf3e0e67386c2e3a9b903-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8b5040a8a5baf3e0e67386c2e3a9b903-Metadata.json},
 openalex = {W2110001975},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalisation in Feedforward Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_8b6dd7db,
 abstract = {In this paper we present NPen++, a connectionist system for writer independent, large vocabulary on-line cursive handwriting recognition. This system combines a robust input representation, which preserves the dynamic writing information, with a neural network architecture, a so called Multi-State Time Delay Neural Network (MS-TDNN), which integrates recognition and segmentation in a single framework. Our preprocessing transforms the original coordinate sequence into a (still temporal) sequence of feature vectors, which combine strictly local features, like curvature or writing direction, with a bitmap-like representation of the coordinate's proximity. The MS-TDNN architecture is well suited for handling temporal sequences as provided by this input representation. Our system is tested both on writer dependent and writer independent tasks with vocabulary sizes ranging from 400 up to 20,000 words. For example, on a 20,000 word vocabulary we achieve word recognition rates up to 88.9% (writer dependent) and 84.1% (writer independent) without using any language models.},
 author = {Manke, Stefan and Finke, Michael and Waibel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8b6dd7db9af49e67306feb59a8bdc52c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8b6dd7db9af49e67306feb59a8bdc52c-Metadata.json},
 openalex = {W2105959208},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf},
 publisher = {MIT Press},
 title = {The Use of Dynamic Writing Information in a Connectionist On-Line Cursive Handwriting Recognition System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/8b6dd7db9af49e67306feb59a8bdc52c-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_8d6dc35e,
 author = {Kazlas, Peter T. and Weigend, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8d6dc35e506fc23349dd10ee68dabb64-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8d6dc35e506fc23349dd10ee68dabb64-Metadata.json},
 openalex = {W45851935},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf},
 publisher = {MIT Press},
 title = {Direct Multi-Step Time Series Prediction Using TD-lambda.},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/8d6dc35e506fc23349dd10ee68dabb64-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_8f468c87,
 abstract = {We have determined the capacity and information efficiency of an associative net configured in a brain-like way with partial connectivity and noisy input cues. Recall theory was used to calculate the capacity when pattern recall is achieved using a winners-take-all strategy. Transforming the dendritic sum according to input activity and unit usage can greatly increase the capacity of the associative net under these conditions. For moderately sparse patterns, maximum information efficiency is achieved with very low connectivity levels (≤ 10%). This corresponds to the level of connectivity commonly seen in the brain and invites speculation that the brain is connected in the most information efficient way.},
 author = {Graham, Bruce and Willshaw, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8f468c873a32bb0619eaeb2050ba45d1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8f468c873a32bb0619eaeb2050ba45d1-Metadata.json},
 openalex = {W2146465360},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf},
 publisher = {MIT Press},
 title = {Capacity and Information Efficiency of a Brain-like Associative Net},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/8f468c873a32bb0619eaeb2050ba45d1-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_90794e3b,
 abstract = {A self-organizing neural network for sequence classification called SARDNET is described and analyzed experimentally. SARDNET extends the Kohonen Feature Map architecture with activation retention and decay in order to create unique distributed response patterns for different sequences. SARDNET yields extremely dense yet descriptive representations of sequential input in very few training iterations. The network has proven successful on mapping arbitrary sequences of binary and real numbers, as well as phonemic representations of English words. Potential applications include isolated spoken word recognition and cognitive science models of sequence processing.},
 author = {James, Daniel L. and Miikkulainen, Risto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/90794e3b050f815354e3e29e977a88ab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/90794e3b050f815354e3e29e977a88ab-Metadata.json},
 openalex = {W2125848004},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/90794e3b050f815354e3e29e977a88ab-Paper.pdf},
 publisher = {MIT Press},
 title = {SARDNET: A Self-Organizing Feature Map for Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/90794e3b050f815354e3e29e977a88ab-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_92262bf9,
 abstract = {We present a deterministic annealing variant of the EM algorithm for maximum likelihood parameter estimation problems. In our approach, the EM process is reformulated as the problem of minimizing the thermodynamic free energy by using the principle of maximum entropy and statistical mechanics analogy. Unlike simulated annealing approaches, this minimization is deterministically performed. Moreover, the derived algorithm, unlike the conventional EM algorithm, can obtain better estimates free of the initial parameter values.},
 author = {Ueda, Naonori and Nakano, Ryohei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/92262bf907af914b95a0fc33c3f33bf6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/92262bf907af914b95a0fc33c3f33bf6-Metadata.json},
 openalex = {W2107042471},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/92262bf907af914b95a0fc33c3f33bf6-Paper.pdf},
 publisher = {MIT Press},
 title = {Deterministic Annealing Variant of the EM Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/92262bf907af914b95a0fc33c3f33bf6-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_9232fe81,
 abstract = {A new learning algorithm is derived which performs online stochastic gradient ascent in the mutual information between outputs and inputs of a network. In the absence of a priori knowledge about the 'signal' and 'noise' components of the input, propagation of information depends on calibrating network non-linearities to the detailed higher-order moments of the input density functions. By incidentally minimising mutual information between outputs, as well as maximising their individual entropies, the network 'factorises' the input into independent components. As an example application, we have achieved near-perfect separation of ten digitally mixed speech signals. Our simulations lead us to believe that our network performs better at blind separation than the Herault-Jutten network, reflecting the fact that it is derived rigorously from the mutual information objective.},
 author = {Bell, Anthony and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9232fe81225bcaef853ae32870a2b0fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9232fe81225bcaef853ae32870a2b0fe-Metadata.json},
 openalex = {W2130683060},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf},
 publisher = {MIT Press},
 title = {A Non-linear Information Maximisation Algorithm that Performs Blind Separation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/9232fe81225bcaef853ae32870a2b0fe-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_9246444d,
 abstract = {A novel two-terminal device, consisting of a thin 1000A layer of p+ a-Si:H sandwiched between Vanadium and Chromium electrodes, exhibits a non-volatile, analogue memory action. This device stores synaptic weights in an ANN chip, replacing the capacitor previously used for dynamic weight storage. Two different synapse designs are discussed and results are presented.},
 author = {Holmes, A. and Murray, Alan and Churcher, Stephen and Hajto, J. and Rose, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9246444d94f081e3549803b928260f56-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9246444d94f081e3549803b928260f56-Metadata.json},
 openalex = {W2114554370},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9246444d94f081e3549803b928260f56-Paper.pdf},
 publisher = {MIT Press},
 title = {Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/9246444d94f081e3549803b928260f56-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_92977ae4,
 abstract = {Dynamic Cell Structures (DCS) represent a family of artificial neural architectures suited both for unsupervised and supervised learning. They belong to the recently [Martinetz94] introduced class of Topology Representing Networks (TRN) which build perfectly topology preserving feature maps. DCS employ a modified Kohonen learning rule in conjunction with competitive Hebbian learning. The Kohonen type learning rule serves to adjust the synaptic weight vectors while Hebbian learning establishes a dynamic lateral connection structure between the units reflecting the topology of the feature manifold. In case of supervised learning, i.e. function approximation, each neural unit implements a Radial Basis Function, and an additional layer of linear output units adjusts according to a delta-rule. DCS is the first RBF-based approximation scheme attempting to concurrently learn and utilize a perfectly topology preserving map for improved performance.

Simulations on a selection of CMU-Benchmarks indicate that the DCS idea applied to the Growing Cell Structure algorithm [Fritzke93] leads to an efficient and elegant algorithm that can beat conventional models on similar tasks.},
 author = {Bruske, J\"{o}rg and Sommer, Gerald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/92977ae4d2ba21425a59afb269c2a14e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/92977ae4d2ba21425a59afb269c2a14e-Metadata.json},
 openalex = {W2144850958},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamic Cell Structures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/92977ae4d2ba21425a59afb269c2a14e-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_934815ad,
 abstract = {We describe single-transistor silicon synapses that compute, learn, and provide non-volatile memory retention. The single transistor synapses simultaneously perform long term weight storage, compute the product of the input and the weight value, and update the weight value according to a Hebbian or a backpropagation learning rule. Memory is accomplished via charge storage on polysilicon floating gates, providing long-term retention without refresh. The synapses efficiently use the physics of silicon to perform weight updates; the weight value is increased using tunneling and the weight value decreases using hot electron injection. The small size and low power operation of single transistor synapses allows the development of dense synaptic arrays. We describe the design, fabrication, characterization, and modeling of an array of single transistor synapses. When the steady state source current is used as the representation of the weight value, both the incrementing and decrementing functions are proportional to a power of the source current. The synaptic array was fabricated in the standard 2µm double - poly, analog process available from MOSIS.},
 author = {Hasler, Paul and Diorio, Chris and Minch, Bradley and Mead, Carver},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/934815ad542a4a7c5e8a2dfa04fea9f5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/934815ad542a4a7c5e8a2dfa04fea9f5-Metadata.json},
 openalex = {W2134682148},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf},
 publisher = {MIT Press},
 title = {Single Transistor Learning Synapses},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/934815ad542a4a7c5e8a2dfa04fea9f5-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_9908279e,
 abstract = {The TNM staging system has been used since the early 1960's to predict breast cancer patient outcome. In an attempt to increase prognostic accuracy, many putative prognostic factors have been identified. Because the TNM stage model can not accommodate these new factors, the proliferation of factors in breast cancer has lead to clinical confusion. What is required is a new computerized prognostic system that can test putative prognostic factors and integrate the predictive factors with the TNM variables in order to increase prognostic accuracy. Using the area under the curve of the receiver operating characteristic, we compare the accuracy of the following predictive models in terms of five year breast cancer-specific survival: pTNM staging system, principal component analysis, classification and regression trees, logistic regression, cascade correlation neural network, conjugate gradient descent neural, probabilistic neural network, and backpropagation neural network. Several statistical models are significantly more accurate than the TNM staging system. Logistic regression and the backpropagation neural network are the most accurate prediction models for predicting five year breast cancer-specific survival.},
 author = {Burke, Harry B. and Rosen, David B. and Goodman, Philip H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9908279ebbf1f9b250ba689db6a0222b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9908279ebbf1f9b250ba689db6a0222b-Metadata.json},
 openalex = {W2170406036},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf},
 publisher = {MIT Press},
 title = {Comparing the prediction accuracy of artificial neural networks and other statistical models for breast cancer survival},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/9908279ebbf1f9b250ba689db6a0222b-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_9ab0d884,
 abstract = {Perceptual learning is defined as fast improvement in performance and retention of the learned ability over a period of time. In a set of psychophysical experiments we demonstrated that perceptual learning occurs for the discrimination of direction in stochastic motion stimuli. Here we model this learning using two approaches: a clustering model that learns to accommodate the motion noise, and an averaging model that learns to ignore the noise. Simulations of the models show performance similar to the psychophysical results.},
 author = {Sundareswaran, V. and Vaina, Lucia},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9ab0d88431732957a618d4a469a0d4c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9ab0d88431732957a618d4a469a0d4c3-Metadata.json},
 openalex = {W2123911898},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9ab0d88431732957a618d4a469a0d4c3-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning direction in global motion: two classes of psychophysically-motivated models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/9ab0d88431732957a618d4a469a0d4c3-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_9c01802d,
 abstract = {The performance of on-line algorithms for learning dichotomies is studied. In on-line learning, the number of examples P is equivalent to the learning time, since each example is presented only once. The learning curve, or generalization error as a function of P, depends on the schedule at which the learning rate is lowered. For a target that is a perceptron rule, the learning curve of the perceptron algorithm can decrease as fast as p-1, if the schedule is optimized. If the target is not realizable by a perceptron, the perceptron algorithm does not generally converge to the solution with lowest generalization error. For the case of unrealizability due to a simple output noise, we propose a new on-line algorithm for a perceptron yielding a learning curve that can approach the optimal generalization error as fast as P-1/2. We then generalize the perceptron algorithm to any class of thresholded smooth functions learning a target from that class. For well-behaved input distributions, if this algorithm converges to the optimal solution, its learning curve can decrease as fast as P-1.},
 author = {Barkai, N. and Seung, H. and Sompolinsky, H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9c01802ddb981e6bcfbec0f0516b8e35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9c01802ddb981e6bcfbec0f0516b8e35-Metadata.json},
 openalex = {W2128266854},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf},
 publisher = {MIT Press},
 title = {On-line Learning of Dichotomies},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/9c01802ddb981e6bcfbec0f0516b8e35-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_9f53d83e,
 abstract = {We study the asymptotic properties of the sequence of iterates of weight-vector estimates obtained by training a multilayer feed forward neural network with a basic gradient-descent method using a fixed learning constant and no batch-processing. In the one-dimensional case, an exact analysis establishes the existence of a limiting distribution that is not Gaussian in general. For the general case and small learning constant, a linearization approximation permits the application of results from the theory of random matrices to again establish the existence of a limiting distribution. We study the first few moments of this distribution to compare and contrast the results of our analysis with those of techniques of stochastic approximation.},
 author = {Mukherjee, Sayandev and Fine, Terrence L.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9f53d83ec0691550f7d2507d57f4f5a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9f53d83ec0691550f7d2507d57f4f5a2-Metadata.json},
 openalex = {W2118407430},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf},
 publisher = {MIT Press},
 title = {Asymptotics of Gradient-based Neural Network Training Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/9f53d83ec0691550f7d2507d57f4f5a2-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_a1140a3d,
 abstract = {This paper studies the convergence properties of the well known K-Means clustering algorithm. The K-Means algorithm can be described either as a gradient descent algorithm or by slightly extending the mathematics of the EM algorithm to this hard threshold case. We show that the K-Means algorithm actually minimizes the quantization error using the very fast Newton algorithm.},
 author = {Bottou, L\'{e}on and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Metadata.json},
 openalex = {W2115665694},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf},
 publisher = {MIT Press},
 title = {Convergence Properties of the K-Means Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_a3d68b46,
 abstract = {Speech recognizers provide good performance for most users but the error rate often increases dramatically for a small percentage of talkers who are different from those talkers used for training. One expensive solution to this problem is to gather more training data in an attempt to sample these outlier users. A second solution, explored in this paper, is to artificially enlarge the number of training talkers by transforming the speech of existing training talkers. This approach is similar to enlarging the training set for OCR digit recognition by warping the training digit images, but is more difficult because continuous speech has a much larger number of dimensions (e.g. linguistic, phonetic, style, temporal, spectral) that differ across talkers. We explored the use of simple linear spectral warping to enlarge a 48-talker training data base used for word spotting. The average detection rate overall was increased by 2.9 percentage points (from 68.3% to 71.2%) for male speakers and 2.5 percentage points (from 64.8% to 67.3%) for female speakers. This increase is small but similar to that obtained by doubling the amount of training data.},
 author = {Chang, Eric and Lippmann, Richard P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Metadata.json},
 openalex = {W2124735141},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Voice Transformations to Create Additional Training Talkers for Word Spotting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/a3d68b461bd9d3533ee1dd3ce4628ed4-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_a4300b00,
 abstract = {Based on computational principles, with as yet no direct experimental validation, it has been proposed that the central nervous system (CNS) uses an internal model to simulate the dynamic behavior of the motor system in planning, control and learning (Sutton and Barto, 1981; Ito, 1984; Kawato et al., 1987; Jordan and Rumelhart, 1992; Miall et al., 1993). We present experimental results and simulations based on a novel approach that investigates the temporal propagation of errors in the sensorimotor integration process. Our results provide direct support for the existence of an internal model.},
 author = {Wolpert, Daniel M and Ghahramani, Zoubin and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a4300b002bcfb71f291dac175d52df94-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a4300b002bcfb71f291dac175d52df94-Metadata.json},
 openalex = {W2144436116},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a4300b002bcfb71f291dac175d52df94-Paper.pdf},
 publisher = {MIT Press},
 title = {Forward dynamic models in human motor control: Psychophysical evidence},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/a4300b002bcfb71f291dac175d52df94-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_a64c94ba,
 abstract = {Almost all models of orientation and direction selectivity in visual cortex are based on feedforward connection schemes, where geniculate input provides all excitation to both pyramidal and inhibitory neurons. The latter neurons then suppress the response of the former for non-optimal stimuli. However, anatomical studies show that up to 90 % of the excitatory synaptic input onto any cortical cell is provided by other cortical cells. The massive excitatory feedback nature of cortical circuits is embedded in the canonical microcircuit of Douglas & Martin (1991). We here investigate analytically and through biologically realistic simulations the functioning of a detailed model of this circuitry, operating in a hysteretic mode. In the model, weak geniculate input is dramatically amplified by intracortical excitation, while inhibition has a dual role: (i) to prevent the early geniculate-induced excitation in the null direction and (ii) to restrain excitation and ensure that the neurons fire only when the stimulus is in their receptive-field. Among the insights gained are the possibility that hysteresis underlies visual cortical function, paralleling proposals for short-term memory, and strong limitations on linearity tests that use gratings. Properties of visual cortical neurons are compared in detail to this model and to a classical model of direction selectivity that does not include excitatory cortico-cortical connections. The model explain a number of puzzling features of direction-selective simple cells, including the small somatic input conductance changes that have been measured experimentally during stimulation in the null direction. The model also allows us to understand why the velocity-response curve of area 17 neurons is different from that of their LGN afferents, and the origin of expansive and compressive nonlinearities in the contrast-response curve of striate cortical neurons.},
 author = {Suarez, Humbert and Koch, Christof and Douglas, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a64c94baaf368e1840a1324e839230de-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a64c94baaf368e1840a1324e839230de-Metadata.json},
 openalex = {W2104639290},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a64c94baaf368e1840a1324e839230de-Paper.pdf},
 publisher = {MIT Press},
 title = {Direction Selectivity In Primary Visual Cortex Using Massive Intracortical Connections},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/a64c94baaf368e1840a1324e839230de-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_a9b7ba70,
 abstract = {If data collection is costly, there is much to be gained by actively selecting particularly informative data points in a sequential way. In a Bayesian decision-theoretic framework we develop a query selection criterion which explicitly takes into account the intended use of the model predictions. By Markov Chain Monte Carlo methods the necessary quantities can be approximated to a desired precision. As the number of data points grows, the model complexity is modified by a Bayesian model selection strategy. The properties of two versions of the criterion ate demonstrated in numerical experiments.},
 author = {Paass, Gerhard and Kindermann, J\"{o}rg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a9b7ba70783b617e9998dc4dd82eb3c5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a9b7ba70783b617e9998dc4dd82eb3c5-Metadata.json},
 openalex = {W2116733494},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Query Construction for Neural Network Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/a9b7ba70783b617e9998dc4dd82eb3c5-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_aa169b49,
 abstract = {We present a silicon model of an axon which shows promise as a building block for pulse-based neural computations involving correlations of pulses across both space and time. The circuit shares a number of features with its biological counterpart including an excitation threshold, a brief refractory period after pulse completion, pulse amplitude restoration, and pulse width restoration. We provide a simple explanation of circuit operation and present data from a chip fabricated in a standard 2µm CMOS process through the MOS Implementation Service (MOSIS). We emphasize the necessity of the restoration of the width of the pulse in time for stable propagation in axons.},
 author = {Minch, Bradley and Hasler, Paul and Diorio, Chris and Mead, Carver},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/aa169b49b583a2b5af89203c2b78c67c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/aa169b49b583a2b5af89203c2b78c67c-Metadata.json},
 openalex = {W2099579346},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf},
 publisher = {MIT Press},
 title = {A Silicon Axon},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/aa169b49b583a2b5af89203c2b78c67c-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_aa68c75c,
 abstract = {Differentiation between the nodes of a competitive learning network is conventionally achieved through competition on the basis of neural activity. Simple inhibitory mechanisms are limited to sparse representations, while decorrelation and factorization schemes that support distributed representations are computationally unattractive. By letting neural plasticity mediate the competitive interaction instead, we obtain diffuse, nonadaptive alternatives for fully distributed representations. We use this technique to Simplify and improve our binary information gain optimization algorithm for feature extraction (Schraudolph and Sejnowski, 1993); the same approach could be used to improve other learning algorithms.},
 author = {Schraudolph, Nicol and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/aa68c75c4a77c87f97fb686b2f068676-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/aa68c75c4a77c87f97fb686b2f068676-Metadata.json},
 openalex = {W2153777646},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf},
 publisher = {MIT Press},
 title = {Plasticity-Mediated Competitive Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/aa68c75c4a77c87f97fb686b2f068676-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_acf4b89d,
 abstract = {We develop a principled strategy to sample a function optimally for function approximation tasks within a Bayesian framework. Using ideas from optimal experiment design, we introduce an objective function (incorporating both bias and variance) to measure the degree of approximation, and the potential utility of the data points towards optimizing this objective. We show how the general strategy can be used to derive precise algorithms to select data for two cases: learning unit step functions and polynomial functions. In particular, we investigate whether such active algorithms can learn the target with fewer examples. We obtain theoretical and empirical results to suggest that this is the case.},
 author = {Sung, Kah and Niyogi, Partha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Metadata.json},
 openalex = {W2164353652},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Learning for Function Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/acf4b89d3d503d8252c9c4ba75ddbf6d-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_b55ec28c,
 abstract = {Current understanding of the effects of damage on neural networks is rudimentary, even though such understanding could lead to important insights concerning neurological and psychiatric disorders. Motivated by this consideration, we present a simple analytical framework for estimating the functional damage resulting from focal structural lesions to a neural network. The effects of focal lesions of varying area, shape and number on the retrieval capacities of a spatially-organized associative memory. Although our analytical results are based on some approximations, they correspond well with simulation results. This study sheds light on some important features characterizing the clinical manifestations of multiinfarct dementia, including the strong association between the number of infarcts and the prevalence of dementia after stroke, and the 'multiplicative' interaction that has been postulated to occur between Alzheimer's disease and multi-infarct dementia.},
 author = {Ruppin, Eytan and Reggia, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b55ec28c52d5f6205684a473a2193564-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b55ec28c52d5f6205684a473a2193564-Metadata.json},
 openalex = {W2113797412},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf},
 publisher = {MIT Press},
 title = {Patterns of damage in neural networks: The effects of lesion area, shape and number},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/b55ec28c52d5f6205684a473a2193564-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_b56a18e0,
 abstract = {We have continued our study of a parallel perturbative learning method [Alspector et al., 1993] and implications for its implementation in analog VLSI. Our new results indicate that, in most cases, a single parallel perturbation (per pattern presentation) of the function parameters (weights in a neural network) is theoretically the best course. This is not true, however, for certain problems and may not generally be true when faced with issues of implementation such as limited precision. In these cases, multiple parallel perturbations may be best as indicated in our previous results.},
 author = {Lippe, D. and Alspector, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Metadata.json},
 openalex = {W2144102635},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf},
 publisher = {MIT Press},
 title = {A Study of Parallel Perturbative Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/b56a18e0eacdf51aa2a5306b0f533204-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_b6a1085a,
 abstract = {We implement and study a computational model of Stevens' [1992] theory of the pathogenesis of schizophrenia. This theory hypothesizes that the onset of schizophrenia is associated with reactive synaptic regeneration occurring in brain regions receiving degenerating temporal lobe projections. Concentrating on one such area, the frontal cortex, we model a frontal module as an associative memory neural network whose input synapses represent incoming temporal projections. We analyze how, in the face of weakened external input projections, compensatory strengthening of internal synaptic connections and increased noise levels can maintain memory capacities (which are generally preserved in schizophrenia). However, These compensatory changes adversely lead to spontaneous, biased retrieval of stored memories, which corresponds to the occurrence of schizophrenic delusions and hallucinations without any apparent external trigger, and for their tendency to concentrate on just few central themes. Our results explain why these symptoms tend to wane as schizophrenia progresses, and why delayed therapeutical intervention leads to a much slower response.},
 author = {Ruppin, Eytan and Reggia, James and Horn, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b6a1085a27ab7bff7550f8a3bd017df8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b6a1085a27ab7bff7550f8a3bd017df8-Metadata.json},
 openalex = {W2102525857},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neural Model of Delusions and Hallucinations in Schizophrenia},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/b6a1085a27ab7bff7550f8a3bd017df8-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_b706835d,
 abstract = {We describe a framework for real-time tracking of facial expressions that uses neurally-inspired correlation and interpolation methods. A distributed view-based representation is used to characterize facial state, and is computed using a replicated correlation network. The ensemble response of the set of view correlation scores is input to a network based interpolation method, which maps perceptual state to motor control states for a simulated 3-D face model. Activation levels of the motor state correspond to muscle activations in an anatomically derived model. By integrating fast and robust 2-D processing with 3-D models, we obtain a system that is able to quickly track and interpret complex facial motions in real-time.},
 author = {Darrell, Trevor and Essa, Irfan and Pentland, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b706835de79a2b4e80506f582af3676a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b706835de79a2b4e80506f582af3676a-Metadata.json},
 openalex = {W2117732173},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b706835de79a2b4e80506f582af3676a-Paper.pdf},
 publisher = {MIT Press},
 title = {Correlation and Interpolation Networks for Real-time Expression Analysis/Synthesis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/b706835de79a2b4e80506f582af3676a-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_b8c37e33,
 abstract = {Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme.},
 author = {Krogh, Anders and Vedelsby, Jesper},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Metadata.json},
 openalex = {W2128073546},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Network Ensembles, Cross Validation, and Active Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/b8c37e33defde51cf91e1e03e51657da-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_bea5955b,
 abstract = {Although artificial neural networks have been applied in a variety of real-world scenarios with remarkable success, they have often been criticized for exhibiting a low degree of human comprehensibility. Techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations.

This paper presents an approach to the extraction of if-then rules from artificial neural networks. Its key mechanism is validity interval analysis, which is a generic tool for extracting symbolic knowledge by propagating rule-like knowledge through Backpropagation-style neural networks. Empirical studies in a robot arm domain illustrate the appropriateness of the proposed method for extracting rules from networks with real-valued and distributed representations.},
 author = {Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/bea5955b308361a1b07bc55042e25e54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/bea5955b308361a1b07bc55042e25e54-Metadata.json},
 openalex = {W2163532598},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf},
 publisher = {MIT Press},
 title = {Extracting Rules from Artificial Neural Networks with Distributed Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/bea5955b308361a1b07bc55042e25e54-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_c22abfa3,
 abstract = {A model of the hippocampus is presented which forms rapid self-organized representations of input arriving via the perforant path, performs recall of previous associations in region CA3, and performs comparison of this recall with afferent input in region CA1. This comparison drives feedback regulation of cholinergic modulation to set appropriate dynamics for learning of new representations in region CA3 and CA1. The network responds to novel patterns with increased cholinergic modulation, allowing storage of new self-organized representations, but responds to familiar patterns with a decrease in acetylcholine, allowing recall based on previous representations. This requires selectivity of the cholinergic suppression of synaptic transmission in stratum radiatum of regions CA3 and CA1, which has been demonstrated experimentally.},
 author = {Hasselmo, Michael and Schnell, Eric and Berke, Joshua and Barkai, Edi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c22abfa379f38b5b0411bc11fa9bf92f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c22abfa379f38b5b0411bc11fa9bf92f-Metadata.json},
 openalex = {W2101008211},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf},
 publisher = {MIT Press},
 title = {A model of the hippocampus combining self-organization and associative memory function},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/c22abfa379f38b5b0411bc11fa9bf92f-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_c2aee861,
 abstract = {Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a CyberGlove, a ContactGlove, a 3- space tracker, and a foot-pedal), a parallel formant speech synthesizer and 3 neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed, user-defined relationship between hand-position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly with speech quality similar to a text-to-speech synthesizer but with far more natural-sounding pitch variations.},
 author = {Fels, Sidney and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c2aee86157b4a40b78132f1e71a9e6f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c2aee86157b4a40b78132f1e71a9e6f1-Metadata.json},
 openalex = {W2149464648},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf},
 publisher = {MIT Press},
 title = {Glove-TalkII: Mapping Hand Gestures to Speech Using Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/c2aee86157b4a40b78132f1e71a9e6f1-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_c32d9bf2,
 abstract = {We present a new method for obtaining the response function G and its average G from which most of the properties of learning and generalization in linear perceptrons can be derived. We first rederive the known results for the 'thermodynamic limit' of infinite perceptron size N and show explicitly that G is self-averaging in this limit. We then discuss extensions of our method to more general learning scenarios with anisotropic teacher space priors, input distributions, and weight decay terms. Finally, we use our method to calculate the finite N corrections of order 1/N to G and discuss the corresponding finite size effects on generalization and learning dynamics. An important spin-off is the observation that results obtained in the thermodynamic limit are often directly relevant to systems of fairly modest, 'real-world' sizes.},
 author = {Sollich, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c32d9bf27a3da7ec8163957080c8628e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c32d9bf27a3da7ec8163957080c8628e-Metadata.json},
 openalex = {W2115922809},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning in large linear perceptrons and why the thermodynamic limit is relevant to the real world},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/c32d9bf27a3da7ec8163957080c8628e-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_c4015b7f,
 abstract = {We describe a framework for learning saccadic eye movements using a photometric representation of target points in natural scenes. The representation takes the form of a high-dimensional vector comprised of the responses of spatial filters at different orientations and scales. We first demonstrate the use of this response vector in the task of locating previously foveated points in a scene and subsequently use this property in a multisaccade strategy to derive an adaptive motor map for delivering accurate saccades.},
 author = {Rao, Rajesh and Ballard, Dana},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c4015b7f368e6b4871809f49debe0579-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c4015b7f368e6b4871809f49debe0579-Metadata.json},
 openalex = {W2135125960},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c4015b7f368e6b4871809f49debe0579-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Saccadic Eye Movements Using Multiscale Spatial Filters},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/c4015b7f368e6b4871809f49debe0579-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_c4b31ce7,
 abstract = {We present an analog VLSI chip for parallel analog vector quantization. The MOSIS 2.0 µm double-poly CMOS Tiny chip contains an array of 16 × 16 charge-based distance estimation cells, implementing a mean absolute difference (MAD) metric operating on a 16-input analog vector field and 16 analog template vectors. The distance cell including dynamic template storage measures 60 × 78 µm2. Additionally, the chip features a winner-take-all (WTA) output circuit of linear complexity, with global positive feedback for fast and decisive settling of a single winner output. Experimental results on the complete 16 × 16 VQ system demonstrate correct operation with 34 dB analog input dynamic range and 3 µsec cycle time at 0.7 mW power dissipation.},
 author = {Cauwenberghs, Gert and Pedroni, Volnei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c4b31ce7d95c75ca70d50c19aef08bf1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c4b31ce7d95c75ca70d50c19aef08bf1-Metadata.json},
 openalex = {W2096067080},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c4b31ce7d95c75ca70d50c19aef08bf1-Paper.pdf},
 publisher = {MIT Press},
 title = {A Charge-Based CMOS Parallel Analog Vector Quantizer},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/c4b31ce7d95c75ca70d50c19aef08bf1-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_c8c41c4a,
 abstract = {Radial Basis Function (RBF) Networks, also known as networks of locally-tuned processing units (see [6]) are well known for their ease of use. Most algorithms used to train these types of networks, however, require a fixed architecture, in which the number of units in the hidden layer must be determined before training starts. The RCE training algorithm, introduced by Reilly, Cooper and Elbaum (see [8]), and its probabilistic extension, the P-RCE algorithm, take advantage of a growing structure in which hidden units are only introduced when necessary. The nature of these algorithms allows training to reach stability much faster than is the case for gradient-descent based methods. Unfortunately P-RCE networks do not adjust the standard deviation of their prototypes individually, using only one global value for this parameter.

This paper introduces the Dynamic Decay Adjustment (DDA) algorithm which utilizes the constructive nature of the P-RCE algorithm together with independent adaptation of each prototype's decay factor. In addition, this radial adjustment is class dependent and distinguishes between different neighbours. It is shown that networks trained with the presented algorithm perform substantially better than common RBF networks.},
 author = {Berthold, Michael and Diamond, Jay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c8c41c4a18675a74e01c8a20e8a0f662-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c8c41c4a18675a74e01c8a20e8a0f662-Metadata.json},
 openalex = {W2102073628},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf},
 publisher = {MIT Press},
 title = {Boosting the Performance of RBF Networks with Dynamic Decay Adjustment},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/c8c41c4a18675a74e01c8a20e8a0f662-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_c8fbbc86,
 abstract = {We propose an alternative model for mixtures of experts which uses a different parametric form for the gating network. The modified model is trained by the EM algorithm. In comparison with earlier models--trained by either EM or gradient ascent--there is no need to select a learning stepsize. We report simulation experiments which show that the new architecture yields faster convergence. We also apply the new model to two problem domains: piecewise nonlinear function approximation and the combination of multiple previously trained classifiers.},
 author = {Xu, Lei and Jordan, Michael and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Metadata.json},
 openalex = {W2109703216},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Paper.pdf},
 publisher = {MIT Press},
 title = {An Alternative Model for Mixtures of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/c8fbbc86abe8bd6a5eb6a3b4d0411301-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_ca759101,
 abstract = {Biological sensorimotor systems are not static maps that transform input (sensory information) into output (motor behavior). Evidence from many lines of research suggests that their representations are plastic, experience-dependent entities. While this plasticity is essential for flexible behavior, it presents the nervous system with difficult organizational challenges. If the sensorimotor system adapts itself to perform well under one set of circumstances, will it then perform poorly when placed in an environment with different demands (negative transfer)? Will a later experience-dependent change undo the benefits of previous learning (catastrophic interference)? We explore the first question in a separate paper in this volume (Shadmehr et al. 1995). Here we present psychophysical and computational results that explore the question of catastrophic interference in the context of a dynamic motor learning task. Under some conditions, subjects show evidence of catastrophic interference. Under other conditions, however, subjects appear to be immune to its effects. These results suggest that motor learning can undergo a process of consolidation. Modular neural networks are well suited for the demands of learning multiple input/output mappings. By incorporating the notion of fast- and slow-changing connections into a modular architecture, we were able to account for the psychophysical results.},
 author = {Brashers-Krug, Tom and Shadmehr, Reza and Todorov, Emanuel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ca75910166da03ff9d4655a0338e6b09-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ca75910166da03ff9d4655a0338e6b09-Metadata.json},
 openalex = {W2167456076},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ca75910166da03ff9d4655a0338e6b09-Paper.pdf},
 publisher = {MIT Press},
 title = {Catastrophic Interference in Human Motor Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/ca75910166da03ff9d4655a0338e6b09-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_cbb6a3b8,
 abstract = {We investigate the computational power of a formal model for networks of spiking neurons, both for the assumption of an unlimited timing precision, and for the case of a limited timing precision. We also prove upper and lower bounds for the number of examples that are needed to train such networks.},
 author = {Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Metadata.json},
 openalex = {W2153669956},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Computational Complexity of Networks of Spiking Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/cbb6a3b884f4f88b3a8e3d44c636cbd8-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_cc1aa436,
 abstract = {A fundamental open problem in computer vision-determining pose and correspondence between two sets of points in space--is solved with a novel, robust and easily implementable algorithm. The technique works on noisy point sets that may be of unequal sizes and may differ by non-rigid transformations. A 2D variation calculates the pose between point sets related by an affine transformation--translation, rotation, scale and shear. A 3D to 3D variation calculates translation and rotation. An objective describing the problem is derived from Mean field theory. The objective is minimized with clocked (EM-like) dynamics. Experiments with both handwritten and synthetic data provide empirical evidence for the method.},
 author = {Gold, Steven and Lu, Chien-Ping and Rangarajan, Anand and Pappu, Suguna and Mjolsness, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cc1aa436277138f61cda703991069eaf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cc1aa436277138f61cda703991069eaf-Metadata.json},
 openalex = {W2116793329},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cc1aa436277138f61cda703991069eaf-Paper.pdf},
 publisher = {MIT Press},
 title = {New Algorithms for 2D and 3D Point Matching: Pose Estimation and Correspondence},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/cc1aa436277138f61cda703991069eaf-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_ccc0aa1b,
 abstract = {This paper presents a new method for image compression by neural networks. First, we show that we can use neural networks in a pyramidal framework, yielding the so-called PCA pyramids. Then we present an image compression method based on the PCA pyramid, which is similar to the Laplace pyramid and wavelet transform. Some experimental results with real images are reported. Finally, we present a method to combine the quantization step with the learning of the PCA pyramid.},
 author = {Bischof, Horst and Hornik, Kurt},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ccc0aa1b81bf81e16c676ddb977c5881-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ccc0aa1b81bf81e16c676ddb977c5881-Metadata.json},
 openalex = {W2117100281},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf},
 publisher = {MIT Press},
 title = {PCA-Pyramids for Image Compression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/ccc0aa1b81bf81e16c676ddb977c5881-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_cfbce4c1,
 abstract = {The macaque lateral geniculate nucleus (LGN) exhibits an intricate lamination pattern, which changes midway through the nucleus at a point coincident with small gaps due to the blind spot in the retina. We present a three-dimensional model of morphogenesis in which local cell interactions cause a wave of development of neuronal receptive fields to propagate through the nucleus and establish two distinct lamination patterns. We examine the interactions between the wave and the localized singularities due to the gaps, and find that the gaps induce the change in lamination pattern. We explore critical factors which determine general LGN organization.},
 author = {Tzonev, Svilen and Schulten, Klaus and Malpeli, Joseph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cfbce4c1d7c425baf21d6b6f2babe6be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cfbce4c1d7c425baf21d6b6f2babe6be-Metadata.json},
 openalex = {W2124081736},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf},
 publisher = {MIT Press},
 title = {Morphogenesis of the Lateral Geniculate Nucleus: How Singularities Affect Global Structure},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/cfbce4c1d7c425baf21d6b6f2babe6be-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_d045c59a,
 abstract = {The auditory system of the barn owl contains several spatial maps. In young barn owls raised with optical prisms over their eyes, these auditory maps are shifted to stay in register with the visual map, suggesting that the visual input imposes a frame of reference on the auditory maps. However, the optic tectum, the first site of convergence of visual with auditory information, is not the site of plasticity for the shift of the auditory maps; the plasticity occurs instead in the inferior colliculus, which contains an auditory map and projects into the optic tectum. We explored a model of the owl remapping in which a global reinforcement signal whose delivery is controlled by visual foveation. A hebb learning rule gated by reinforcement learned to appropriately adjust auditory maps. In addition, reinforcement learning preferentially adjusted the weights in the inferior colliculus, as in the owl brain, even though the weights were allowed to change throughout the auditory system. This observation raises the possibility that the site of learning does not have to be genetically specified, but could be determined by how the learning procedure interacts with the network architecture.},
 author = {Pouget, Alexandre and Deffayet, Cedric and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d045c59a90d7587d8d671b5f5aec4e7c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d045c59a90d7587d8d671b5f5aec4e7c-Metadata.json},
 openalex = {W2152537806},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning Predicts the Site of Plasticity for Auditory Remapping in the Barn Owl},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/d045c59a90d7587d8d671b5f5aec4e7c-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_d240e3d3,
 abstract = {A training method based on a form of continuous spatially distributed optical error back-propagation is presented for an all optical network composed of nondiscrete neurons and weighted interconnections. The all optical network is feed-forward and is composed of thin layers of a Kerrtype self focusing/defocusing nonlinear optical material. The training method is derived from a Lagrangian formulation of the constrained minimization of the network error at the output. This leads to a formulation that describes training as a calculation of the distributed error of the optical signal at the output which is then reflected back through the device to assign a spatially distributed error to the internal layers. This error is then used to modify the internal weighting values. Results from several computer simulations of the training are presented, and a simple optical table demonstration of the network is discussed.},
 author = {Steck, James and Skinner, Steven and Cruz-Cabrara, Alvaro and Behrman, Elizabeth},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d240e3d38a8882ecad8633c8f9c78c9b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d240e3d38a8882ecad8633c8f9c78c9b-Metadata.json},
 openalex = {W2155308653},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d240e3d38a8882ecad8633c8f9c78c9b-Paper.pdf},
 publisher = {MIT Press},
 title = {A Lagrangian Formulation For Optical Backpropagation Training In Kerr-Type Optical Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/d240e3d38a8882ecad8633c8f9c78c9b-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_d2ed45a5,
 abstract = {This paper presents instance-based state identification, an approach to reinforcement learning and hidden state that builds disambiguating amounts of short-term memory on-line, and also learns with an order of magnitude fewer training steps than several previous approaches. Inspired by a key similarity between learning with hidden state and learning in continuous geometrical spaces, this approach uses instance-based (or memory-based) learning, a method that has worked well in continuous spaces.},
 author = {McCallum, R. Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d2ed45a52bc0edfa11c2064e9edee8bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d2ed45a52bc0edfa11c2064e9edee8bf-Metadata.json},
 openalex = {W2172246523},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf},
 publisher = {MIT Press},
 title = {Instance-Based State Identification for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/d2ed45a52bc0edfa11c2064e9edee8bf-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_d516b136,
 abstract = {The problem of interpolating between specified images in an image sequence is a simple, but important task in model-based vision. We describe an approach based on the abstract task of manifold learning and present results on both synthetic and real image sequences. This problem arose in the development of a combined lip-reading and speech recognition system.},
 author = {Bregler, Christoph and Omohundro, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d516b13671a4179d9b7b458a6ebdeb92-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d516b13671a4179d9b7b458a6ebdeb92-Metadata.json},
 openalex = {W2151391352},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf},
 publisher = {MIT Press},
 title = {Nonlinear Image Interpolation using Manifold Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_d56b9fc4,
 abstract = {An incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple Hebb-like learning rule. In contrast to previous approaches like the neural gas method of Martinetz and Schulten (1991, 1994), this model has no parameters which change over time and is able to continue learning, adding units and connections, until a performance criterion has been met. Applications of the model include vector quantization, clustering, and interpolation.},
 author = {Fritzke, Bernd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Metadata.json},
 openalex = {W2138754805},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},
 publisher = {MIT Press},
 title = {A Growing Neural Gas Network Learns Topologies},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/d56b9fc4b0f1be8871f5e1c40c0067e7-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_d707329b,
 abstract = {When training neural networks by the classical backpropagation algorithm the whole problem to learn must be expressed by a set of inputs and desired outputs. However, we often have high-level knowledge about the learning problem. In optical character recognition (OCR), for instance, we know that the classification should be invariant under a set of transformations like rotation or translation. We propose a new modular classification system based on several autoassociative multilayer perceptrons which allows the efficient incorporation of such knowledge. Results are reported on the NIST database of upper case handwritten letters and compared to other approaches to the invariance problem.},
 author = {Schwenk, Holger and Milgram, Maurice},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d707329bece455a462b58ce00d1194c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d707329bece455a462b58ce00d1194c9-Metadata.json},
 openalex = {W2120486861},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d707329bece455a462b58ce00d1194c9-Paper.pdf},
 publisher = {MIT Press},
 title = {Transformation Invariant Autoassociation with Application to Handwritten Character Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/d707329bece455a462b58ce00d1194c9-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_d7322ed7,
 abstract = {This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach.},
 author = {Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Metadata.json},
 openalex = {W2149043954},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning to Play the Game of Chess},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/d7322ed717dedf1eb4e6e52a37ea7bcd-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_d79aac07,
 abstract = {This paper presents an alternating minimization (AM) algorithm used in the training of radial basis function and linear regressor networks. The algorithm is a modification of a small-step interior point method used in solving primal linear programs. The algorithm has a convergence rate of O(√n L) iterations where n is a measure of the network size and L is a measure of the resulting solution's accuracy. Two results are presented that specify how aggressively the two steps of the AM may be pursued to ensure convergence of each step of the alternating minimization.},
 author = {Lemmon, Michael and Szymanski, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d79aac075930c83c2f1e369a511148fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d79aac075930c83c2f1e369a511148fe-Metadata.json},
 openalex = {W2152098286},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf},
 publisher = {MIT Press},
 title = {Interior Point Implementations of Alternating Minimization Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/d79aac075930c83c2f1e369a511148fe-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_d93ed5b6,
 abstract = {We describe a system that can track a hand in a sequence of video frames and recognize hand gestures in a user-independent manner. The system locates the hand in each video frame and determines if the hand is open or closed. The tracking system is able to track the hand to within ±10 pixels of its correct location in 99.7% of the frames from a test set containing video sequences from 18 different individuals captured in 18 different room environments. The gesture recognition network correctly determines if the hand being tracked is open or closed in 99.1% of the frames in this test set. The system has been designed to operate in real time with existing hardware.},
 author = {Nowlan, Steven and Platt, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d93ed5b6db83be78efb0d05ae420158e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d93ed5b6db83be78efb0d05ae420158e-Metadata.json},
 openalex = {W2170837066},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf},
 publisher = {MIT Press},
 title = {A Convolutional Neural Network Hand Tracker},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/d93ed5b6db83be78efb0d05ae420158e-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_da0d1111,
 abstract = {This paper presents a rigorous characterization of how a general nonlinear learning machine generalizes during the training process when it is trained on a random sample using a gradient descent algorithm based on reduction of training error. It is shown, in particular, that best generalization performance occurs, in general, before the global minimum of the training error is achieved. The different roles played by the complexity of the machine class and the complexity of the specific machine in the class during learning are also precisely demarcated.},
 author = {Wang, Changfeng and Venkatesh, Santosh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/da0d1111d2dc5d489242e60ebcbaf988-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/da0d1111d2dc5d489242e60ebcbaf988-Metadata.json},
 openalex = {W2153814050},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf},
 publisher = {MIT Press},
 title = {Temporal Dynamics of Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/da0d1111d2dc5d489242e60ebcbaf988-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_dd8eb9f2,
 abstract = {We present a statistical method that PAC learns the class of stochastic perceptrons with arbitrary monotonic activation function and weights Wi ∈ {-1, 0, + 1} when the probability distribution that generates the input examples is member of a family that we call k-blocking distributions. Such distributions represent an important step beyond the case where each input variable is statistically independent since the 2k-blocking family contains all the Markov distributions of order k. By stochastic percept ron we mean a perceptron which, upon presentation of input vector x, outputs 1 with probability f(Σi wixi - θ). Because the same algorithm works for any monotonic (nondecreasing or nonincreasing) activation function f on Boolean domain, it handles the well studied cases of sigmoids and the usual radial basis functions.},
 author = {Marchand, Mario and Hadjifaradji, Saeed},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/dd8eb9f23fbd362da0e3f4e70b878c16-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/dd8eb9f23fbd362da0e3f4e70b878c16-Metadata.json},
 openalex = {W2122737207},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Stochastic Perceptrons Under k-Blocking Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/dd8eb9f23fbd362da0e3f4e70b878c16-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_df6d2338,
 abstract = {Second order properties of cost functions for recurrent networks are investigated. We analyze a layered fully recurrent architecture, the virtue of this architecture is that it features the conventional feedforward architecture as a special case. A detailed description of recursive computation of the full Hessian of the network cost function is provided. We discuss the possibility of invoking simplifying approximations of the Hessian and show how weight decays iron the cost function and thereby greatly assist training. We present tentative pruning results, using Hassibi et al.'s Optimal Brain Surgeon, demonstrating that recurrent networks can construct an efficient internal memory.},
 author = {Pedersen, Morten and Hansen, Lars},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Metadata.json},
 openalex = {W2159371173},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Paper.pdf},
 publisher = {MIT Press},
 title = {Recurrent Networks: Second Order Properties and Pruning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/df6d2338b2b8fce1ec2f6dda0a630eb0-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_e205ee2a,
 abstract = {This paper presents an unsupervised learning scheme for categorizing 3D objects from their 2D projected images. The scheme exploits an auto-associative network's ability to encode each view of a single object into a representation that indicates its view direction. We propose two models that employ different classification mechanisms; the first model selects an auto-associative network whose recovered view best matches the input view, and the second model is based on a modular architecture whose additional network classifies the views by splitting the input space nonlinearly. We demonstrate the effectiveness of the proposed classification models through simulations using 3D wire-frame objects.},
 author = {Suzuki, Satoshi and Ando, Hiroshi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e205ee2a5de471a70c1fd1b46033a75f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e205ee2a5de471a70c1fd1b46033a75f-Metadata.json},
 openalex = {W2163612564},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf},
 publisher = {MIT Press},
 title = {Unsupervised Classification of 3D Objects from 2D Views},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/e205ee2a5de471a70c1fd1b46033a75f-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_e6cb2a3c,
 abstract = {Using a statistical mechanical formalism we calculate the evidence, generalisation error and consistency measure for a linear perceptron trained and tested on a set of examples generated by a non linear teacher. The teacher is said to be unrealisable because the student can never model it without error. Our model allows us to interpolate between the known case of a linear teacher, and an unrealisable, nonlinear teacher. A comparison of the hyperparameters which maximise the evidence with those that optimise the performance measures reveals that, in the non-linear case, the evidence procedure is a misleading guide to optimising performance. Finally, we explore the extent to which the evidence procedure is unreliable and find that, despite being sub-optimal, in some circumstances it might be a useful method for fixing the hyperparameters.},
 author = {Marion, Glenn and Saad, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Metadata.json},
 openalex = {W2170603741},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Paper.pdf},
 publisher = {MIT Press},
 title = {Hyperparameters Evidence and Generalisation for an Unrealisable Rule},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/e6cb2a3c14431b55aa50c06529eaa21b-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_e744f91c,
 abstract = {For machines to perform classification tasks, such as speech and character recognition, appropriately handling deformed patterns is a key to achieving high performance. The authors presents a new type of classification system, an Adaptive Input Field Neural Network (AIFNN), which includes a simple pre-trained neural network and an elastic input field attached to an input layer. By using an iterative method, AIFNN can determine an optimal affine translation for an elastic input field to compensate for the original deformations. The convergence of the AIFNN algorithm is shown. AIFNN is applied for handwritten numerals recognition. Consequently, 10.83% of originally misclassified patterns are correctly categorized and total performance is improved, without modifying the neural network.},
 author = {Asogawa, Minoru},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e744f91c29ec99f0e662c9177946c627-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e744f91c29ec99f0e662c9177946c627-Metadata.json},
 openalex = {W2118176730},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e744f91c29ec99f0e662c9177946c627-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive Elastic Input Field for Recognition Improvement},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/e744f91c29ec99f0e662c9177946c627-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_e820a45f,
 abstract = {A new model for chemosensory reception is presented. It models reactions between odor molecules and receptor proteins and the activation of second messenger by receptor proteins. The mathematical formulation of the reaction kinetics is transformed into an artificial neural network (ANN). The resulting feed-forward network provides a powerful means for parameter fitting by applying learning algorithms. The weights of the network corresponding to chemical parameters can be trained by presenting experimental data. We demonstrate the simulation capabilities of the model with experimental data from honey bee chemosensory neurons. It can be shown that our model is sufficient to rebuild the observed data and that simpler models are not able to do this task.},
 author = {Malaka, Rainer and Ragg, Thomas and Hammer, Martin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e820a45f1dfc7b95282d10b6087e11c0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e820a45f1dfc7b95282d10b6087e11c0-Metadata.json},
 openalex = {W2117369166},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf},
 publisher = {MIT Press},
 title = {A Model for Chemosensory Reception},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/e820a45f1dfc7b95282d10b6087e11c0-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_ec5aa0b7,
 abstract = {Summary form only given, as follows. We describe an analog VLSI implementation of the ART1 algorithm (Carpenter, 1987). A prototype chip has been fabricated in a standard low cost 1.5 /spl mu/m double-mental single-poly CMOS process. It has a die area of 1 cm/sup 2/ and is mounted in a 120 pin PGA package. The chip realizes a modified version of the original ART1 architecture. Such modification has been shown to preserve all computational properties of the original algorithm (Serrano, 1994), while being more appropriate for VLSI realizations. The chip implements on ART1 network with 100 F1 nodes and 18 F2 nodes. It can, therefore, cluster 100 binary pixels input patterns into up to 18 different categories. Modular expansibility of the system is possible by assembling an N/spl times/M array of chips without any extra interfacing circuitry, resulting in an F1 layer with 100/spl times/N nodes, and an F2 layer with 18/spl times/M nodes. Pattern classification is performed in less than 1.8 /spl mu/s, which means an equivalent computing power of 2.2/spl times/10/sup 9/ connections and connection-updates per second. Although internally the chip is analog in nature, it interfaces to the outside world through digital signals, thus having a true asynchronous digital behavior. Experimental chip test results are available, which have been obtained through test equipment for digital chips.},
 author = {Serrano-Gotarredona, Teresa and Linares-Barranco, Bernab\'{e} and Huertas, Jos\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ec5aa0b7846082a2415f0902f0da88f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ec5aa0b7846082a2415f0902f0da88f2-Metadata.json},
 openalex = {W2130915421},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf},
 publisher = {MIT Press},
 title = {A real time clustering CMOS neural engine},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_eeb69a3c,
 abstract = {In supervised learning, learning from queries rather than from random examples can improve generalization performance significantly. We study the performance of query learning for problems where the student cannot learn the teacher perfectly, which occur frequently in practice. As a prototypical scenario of this kind, we consider a linear perceptron student learning a binary perceptron teacher. Two kinds of queries for maximum information gain, i.e., minimum entropy, are investigated: Minimum student space entropy (MSSE) queries, which are appropriate if the teacher space is unknown, and minimum teacher space entropy (MTSE) queries, which can be used if the teacher space is assumed to be known, but a student of a simpler form has deliberately been chosen. We find that for MSSE queries, the structure of the student space determines the efficacy of query learning, whereas MTSE queries lead to a higher generalization error than random examples, due to a lack of feedback about the progress of the student in the way queries are selected.},
 author = {Sollich, Peter and Saad, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/eeb69a3cb92300456b6a5f4162093851-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/eeb69a3cb92300456b6a5f4162093851-Metadata.json},
 openalex = {W2170705499},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/eeb69a3cb92300456b6a5f4162093851-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning from queries for maximum information gain in imperfectly learnable problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/eeb69a3cb92300456b6a5f4162093851-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_ef4e3b77,
 abstract = {We introduce a novel algorithm for factorial learning, motivated by segmentation problems in computational vision, in which the underlying factors correspond to clusters of highly correlated input features. The algorithm derives from a new kind of competitive clustering model, in which the cluster generators compete to explain each feature of the data set and cooperate to explain each input example, rather than competing for examples and cooperating on features, as in traditional clustering algorithms. A natural extension of the algorithm recovers hierarchical models of data generated from multiple unknown categories, each with a different, multiple causal structure. Several simulations demonstrate the power of this approach.},
 author = {Tenenbaum, Joshua and Todorov, Emanuel V.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ef4e3b775c934dada217712d76f3d51f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ef4e3b775c934dada217712d76f3d51f-Metadata.json},
 openalex = {W2135506505},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ef4e3b775c934dada217712d76f3d51f-Paper.pdf},
 publisher = {MIT Press},
 title = {Factorial Learning by Clustering Features},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/ef4e3b775c934dada217712d76f3d51f-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_ef50c335,
 abstract = {A straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neural net. Although this has been successful in the domain of backgammon, there is no guarantee of convergence. In this paper, we show that the combination of dynamic programming and function approximation is not robust, and in even very benign cases, may produce an entirely wrong policy. We then introduce Grow-Support, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization.},
 author = {Boyan, Justin and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ef50c335cca9f340bde656363ebd02fd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ef50c335cca9f340bde656363ebd02fd-Metadata.json},
 openalex = {W2125074935},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalization in Reinforcement Learning: Safely Approximating the Value Function},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/ef50c335cca9f340bde656363ebd02fd-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_f33ba15e,
 abstract = {We present a graph-based method for rapid, accurate search through prototypes for transformation-invariant pattern classification. Our method has in theory the same recognition accuracy as other recent methods based on distance [Simard et al., 1994], since it uses the same categorization rule. Nevertheless ours is significantly faster during classification because far fewer tangent distances need be computed. Crucial to the success of our system are 1) a novel graph architecture in which transformation constraints and geometric relationships among prototypes are encoded during learning, and 2) an improved graph search criterion, used during classification. These architectural insights are applicable to a wide range of problem domains. Here we demonstrate that on a handwriting recognition task, a basic implementation of our system requires less than half the computation of the Euclidean sorting method.},
 author = {Sperduti, Alessandro and Stork, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f33ba15effa5c10e873bf3842afb46a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f33ba15effa5c10e873bf3842afb46a6-Metadata.json},
 openalex = {W2171364801},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf},
 publisher = {MIT Press},
 title = {A Rapid Graph-based Method for Arbitrary Transformation-Invariant Pattern Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_f47d0ad3,
 abstract = {A model of short-term memory for serially ordered lists of verbal stimuli is proposed as an implementation of the 'articulatory loop' thought to mediate this type of memory (Baddeley, 1986). The model predicts the presence of a repeatable time-varying 'context' signal coding the timing of items' presentation in addition to a store of phonological information and a process of serial rehearsal. Items are associated with context nodes and phonemes by Hebbian connections showing both short and long term plasticity. Items are activated by phonemic input during presentation and reactivated by context and phonemic feedback during output. Serial selection of items occurs via a winner-take-all interaction amongst items, with the winner subsequently receiving decaying inhibition. An approximate analysis of error probabilities due to Gaussian noise during output is presented. The model provides an explanatory account of the probability of error as a function of serial position, list length, word length, phonemic similarity, temporal grouping, item and list familiarity, and is proposed as the starting point for a model of rehearsal and vocabulary acquisition.},
 author = {Burgess, Neil},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f47d0ad31c4c49061b9e505593e3db98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f47d0ad31c4c49061b9e505593e3db98-Metadata.json},
 openalex = {W2164290610},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f47d0ad31c4c49061b9e505593e3db98-Paper.pdf},
 publisher = {MIT Press},
 title = {A solvable connectionist model of immediate recall of ordered lists},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/f47d0ad31c4c49061b9e505593e3db98-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_f57a2f55,
 author = {Hassibi, Babak and Kailath, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f57a2f557b098c43f11ab969efe1504b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f57a2f557b098c43f11ab969efe1504b-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f57a2f557b098c43f11ab969efe1504b-Paper.pdf},
 publisher = {MIT Press},
 title = {H\infty Optimal Training Algorithms and their Relation to Backpropagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/f57a2f557b098c43f11ab969efe1504b-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_fba9d881,
 abstract = {Deformable models are an attractive approach to recognizing nonrigid objects which have considerable within class variability. However, there are severe search problems associated with fitting the models to data. We show that by using neural networks to provide better starting points, the search time can be significantly reduced. The method is demonstrated on a character recognition task.},
 author = {Williams, Christopher and Revow, Michael and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fba9d88164f3e2d9109ee770223212a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fba9d88164f3e2d9109ee770223212a0-Metadata.json},
 openalex = {W2147645535},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf},
 publisher = {MIT Press},
 title = {Using a neural net to instantiate a deformable model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/fba9d88164f3e2d9109ee770223212a0-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_fe7ee8fc,
 abstract = {This paper presents the design and simulation results of a self-organizing neural network which induces a grammar from example sentences. Input sentences are generated from a simple phrase structure grammar including number agreement, verb transitivity, and recursive noun phrase construction rules. The network induces a grammar explicitly in the form of symbol categorization rules and phrase structure rules.},
 author = {Negishi, Michiro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fe7ee8fc1959cc7214fa21c4840dff0a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fe7ee8fc1959cc7214fa21c4840dff0a-Metadata.json},
 openalex = {W2142628931},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fe7ee8fc1959cc7214fa21c4840dff0a-Paper.pdf},
 publisher = {MIT Press},
 title = {Grammar Learning by a Self-Organizing Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/fe7ee8fc1959cc7214fa21c4840dff0a-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_fec8d47d,
 abstract = {The efficiency of image search can be greatly improved by using a coarse-to-fine search strategy with a multi-resolution image representation. However, if the resolution is so low that the objects have few distinguishing features, search becomes difficult. We show that the performance of search at such low resolutions can be improved by using context information, i.e., objects visible at low-resolution which are not the objects of interest but are associated with them. The networks can be given explicit context information as inputs, or they can learn to detect the context objects, in which case the user does not have to be aware of their existence. We also use Integrated Feature Pyramids, which represent high-frequency information at low resolutions. The use of multiresolution search techniques allows us to combine information about the appearance of the objects on many scales in an efficient way. A natural form of exemplar selection also arises from these techniques. We illustrate these ideas by training hierarchical systems of neural networks to find clusters of buildings in aerial photographs of farmland.},
 author = {Spence, Clay and Pearson, John and Bergen, Jim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fec8d47d412bcbeece3d9128ae855a7a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fec8d47d412bcbeece3d9128ae855a7a-Metadata.json},
 openalex = {W2153580556},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf},
 publisher = {MIT Press},
 title = {Coarse-to-Fine Image Search Using Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/fec8d47d412bcbeece3d9128ae855a7a-Abstract.html},
 volume = {7},
 year = {1994}
}

@inproceedings{NIPS1994_fed33392,
 abstract = {An analogue VLSI neural network has been designed and tested to perform cardiac morphology classification tasks. Analogue techniques were chosen to meet the strict power and area requirements of an Implantable Cardioverter Defibrillator (ICD) system. The robustness of the neural network architecture reduces the impact of noise, drift and offsets inherent in analogue approaches. The network is a 10:6:3 multi-layer perceptron with on chip digital weight storage, a bucket brigade input to feed the Intracardiac Electrogram (ICEG) to the network and has a winner take all circuit at the output. The network was trained in loop and included a commercial ICD in the signal processing path. The system has successfully distinguished arrhythmia for different patients with better than 90% true positive and true negative detections for dangerous rhythms which cannot be detected by present ICDs. The chip was implemented in 1.2µm CMOS and consumes less than 200nW maximum average power in an area of 2.2 × 2.2mm2.},
 author = {Coggins, Richard and Jabri, Marwan and Flower, Barry and Pickard, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fed33392d3a48aa149a87a38b875ba4a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fed33392d3a48aa149a87a38b875ba4a-Metadata.json},
 openalex = {W2114592597},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fed33392d3a48aa149a87a38b875ba4a-Paper.pdf},
 publisher = {MIT Press},
 title = {ICEG Morphology Classification using an Analogue VLSI Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/hash/fed33392d3a48aa149a87a38b875ba4a-Abstract.html},
 volume = {7},
 year = {1994}
}
