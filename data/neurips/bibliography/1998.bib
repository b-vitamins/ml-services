@inproceedings{NIPS1998_020c8bfa,
 abstract = {Symmetrically connected recurrent networks have recently been used as models of a host of neural computations. However, biological neural networks have asymmetrical connections, at the very least because of the separation between excitatory and inhibitory neurons in the brain. We study characteristic differences between asymmetrical networks and their symmetrical counterparts in cases for which they act as selective amplifiers for particular classes of input patterns. We show that the dramatically different dynamical behaviours to which they have access, often make the asymmetrical networks computationally superior. We illustrate our results in networks that selectively amplify oriented bars and smooth contours in visual inputs.},
 author = {Li, Zhaoping and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/020c8bfac8de160d4c5543b96d1fdede-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/020c8bfac8de160d4c5543b96d1fdede-Metadata.json},
 openalex = {W4247335487},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/020c8bfac8de160d4c5543b96d1fdede-Paper.pdf},
 publisher = {MIT Press},
 title = {Computational differences between asymmetrical and symmetrical networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/020c8bfac8de160d4c5543b96d1fdede-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_06a81a4f,
 abstract = {We present a stochastic clustering algorithm based on pairwise similarity of datapoints. Our method extends existing deterministic methods, including agglomerative algorithms, min-cut graph algorithms, and connected components. Thus it provides a common framework for all these methods. Our graph-based method differs from existing stochastic methods which are based on analogy to physical systems. The stochastic nature of our method makes it more robust against noise, including accidental edges and small spurious clusters. We demonstrate the superiority of our algorithm using an example with 3 spiraling bands and a lot of noise.},
 author = {Gdalyahu, Yoram and Weinshall, Daphna and Werman, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/06a81a4fb98d149f2d31c68828fa6eb2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/06a81a4fb98d149f2d31c68828fa6eb2-Metadata.json},
 openalex = {W2152527808},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/06a81a4fb98d149f2d31c68828fa6eb2-Paper.pdf},
 publisher = {MIT Press},
 title = {A Randomized Algorithm for Pairwise Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/06a81a4fb98d149f2d31c68828fa6eb2-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_076023ed,
 abstract = {We derive a family of risk-sensitive reinforcement learning methods for agents, who face sequential decision-making tasks in uncertain environments. By applying a utility function to the temporal difference (TD) error, nonlinear transformations are effectively applied not only to the received rewards but also to the true transition probabilities of the underlying Markov decision process. When appropriate utility functions are chosen, the agents' behaviors express key features of human behavior as predicted by prospect theory (Kahneman and Tversky, 1979), for example different risk-preferences for gains and losses as well as the shape of subjective probability curves. We derive a risk-sensitive Q-learning algorithm, which is necessary for modeling human behavior when transition probabilities are unknown, and prove its convergence. As a proof of principle for the applicability of the new framework we apply it to quantify human behavior in a sequential investment task. We find, that the risk-sensitive variant provides a significantly better fit to the behavioral data and that it leads to an interpretation of the subject's responses which is indeed consistent with prospect theory. The analysis of simultaneously measured fMRI signals show a significant correlation of the risk-sensitive TD error with BOLD signal change in the ventral striatum. In addition we find a significant correlation of the risk-sensitive Q-values with neural activity in the striatum, cingulate cortex and insula, which is not present if standard Q-values are used.},
 author = {Neuneier, Ralph and Mihatsch, Oliver},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/076023edc9187cf1ac1f1163470e479a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/076023edc9187cf1ac1f1163470e479a-Metadata.json},
 openalex = {W3125893104},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/076023edc9187cf1ac1f1163470e479a-Paper.pdf},
 publisher = {MIT Press},
 title = {Risk-Sensitive Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/076023edc9187cf1ac1f1163470e479a-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_0771fc6f,
 abstract = {The W-S (Wake-Sleep) algorithm is a simple learning rule for the models with hidden variables. It is shown that this algorithm can be applied to a factor analysis model which is a linear version of the Helmholtz machine. But even for a factor analysis model, the general convergence is not proved theoretically. In this article, we describe the geometrical understanding of the W-S algorithm in contrast with the EM (Expectation-Maximization) algorithm and the em algorithm. As the result, we prove the convergence of the W-S algorithm for the factor analysis model. We also show the condition for the convergence in general models.},
 author = {Ikeda, Shiro and Amari, Shun-ichi and Nakahara, Hiroyuki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0771fc6f0f4b1d7d1bb73bbbe14e0e31-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0771fc6f0f4b1d7d1bb73bbbe14e0e31-Metadata.json},
 openalex = {W2139948828},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0771fc6f0f4b1d7d1bb73bbbe14e0e31-Paper.pdf},
 publisher = {MIT Press},
 title = {Convergence of the Wake-Sleep Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/0771fc6f0f4b1d7d1bb73bbbe14e0e31-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_08040837,
 abstract = {We discuss the application of TAP mean field methods known from the Statistical Mechanics of disordered systems to Bayesian classification models with Gaussian processes. In contrast to previous approaches, no knowledge about the distribution of inputs is needed. Simulation results for the Sonar data set are given.},
 author = {Opper, Manfred and Winther, Ole},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/08040837089cdf46631a10aca5258e16-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/08040837089cdf46631a10aca5258e16-Metadata.json},
 openalex = {W2161325980},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/08040837089cdf46631a10aca5258e16-Paper.pdf},
 publisher = {MIT Press},
 title = {Mean Field Methods for Classification with Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/08040837089cdf46631a10aca5258e16-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_0c8ce551,
 abstract = {Dyadzc data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This type of data arises naturally in many application ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework of learning from dyadic data by statistical mixture models. Our approach covers different models with fiat and hierarchical latent class structures. We propose an annealed version of the standard EM algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains.},
 author = {Hofmann, Thomas and Puzicha, Jan and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0c8ce55163055c4da50a81e0a273468c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0c8ce55163055c4da50a81e0a273468c-Metadata.json},
 openalex = {W2143144851},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0c8ce55163055c4da50a81e0a273468c-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning from Dyadic Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/0c8ce55163055c4da50a81e0a273468c-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_0c9ebb2d,
 abstract = {Fraud causes substantial losses to telecommunication carriers. Detection systems which automatically detect illegal use of the network can be used to alleviate the problem. Previous approaches worked on features derived from the call patterns of individual users. In this paper we present a call-based detection system based on a hierarchical regime-switching model. The detection problem is formulated as an inference problem on the regime probabilities. Inference is implemented by applying the junction tree algorithm to the underlying graphical model. The dynamics are learned from data using the EM algorithm and subsequent discriminative training. The methods are assessed using fraud data from a real mobile communication network.},
 author = {Hollm\'{e}n, Jaakko and Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0c9ebb2ded806d7ffda75cd0b95eb70c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0c9ebb2ded806d7ffda75cd0b95eb70c-Metadata.json},
 openalex = {W2160726788},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0c9ebb2ded806d7ffda75cd0b95eb70c-Paper.pdf},
 publisher = {MIT Press},
 title = {Call-Based Fraud Detection in Mobile Communication Networks Using a Hierarchical Regime-Switching Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/0c9ebb2ded806d7ffda75cd0b95eb70c-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_0d4f4805,
 abstract = {Event-related potentials (ERPs), are portions of electroencephalographic (EEG) recordings that are both time- and phase-locked to experimental events. ERPs are usually averaged to increase their signal/noise ratio relative to non-phase locked EEG activity, regardless of the fact that response activity in single epochs may vary widely in time course and scalp distribution. This study applies a linear decomposition tool, Independent Component Analysis (ICA) [1], to multichannel single-trial EEG records to derive spatial filters that decompose single-trial EEG epochs into a sum of temporally independent and spatially fixed components arising from distinct or overlapping brain or extra-brain networks. Our results on normal and autistic subjects show that ICA can separate artifactual, stimulus-locked, response-locked, and. nonevent related background EEG activities into separate components, allowing (1) removal of pervasive artifacts of all types from single-trial EEG records, and (2) identification of both stimulus- and response-locked EEG components. Second, this study proposes a new visualization tool, the 'ERP image', for investigating variability in latencies and amplitudes of event-evoked responses in spontaneous EEG or MEG records. We show that sorting single-trial ERP epochs in order of reaction time and plotting the potentials in 2-D clearly reveals underlying patterns of response variability linked to performance. These analysis and visualization tools appear broadly applicable to electrophyiological research on both normal and clinical populations.},
 author = {Jung, Tzyy-Ping and Makeig, Scott and Westerfield, Marissa and Townsend, Jeanne and Courchesne, Eric and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0d4f4805c36dc6853edfa4c7e1638b48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0d4f4805c36dc6853edfa4c7e1638b48-Metadata.json},
 openalex = {W2147593475},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0d4f4805c36dc6853edfa4c7e1638b48-Paper.pdf},
 publisher = {MIT Press},
 title = {Analyzing and Visualizing Single-Trial Event-Related Potentials},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/0d4f4805c36dc6853edfa4c7e1638b48-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_0ebcc77d,
 abstract = {The Expectation-Maximization (EM) algorithm is an iterative procedure for maximum likelihood parameter estimation from data sets with missing or hidden variables [2]. It has been applied to system identification in linear stochastic state-space models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously [9]. We present a generalization of the EM algorithm for parameter estimation in nonlinear dynamical systems. The expectation step makes use of Extended Kalman Smoothing to estimate the state, while the step re-estimates the parameters using these uncertain state estimates. In general, the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states. However, if Gaussian radial basis function (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems of linear equations.},
 author = {Ghahramani, Zoubin and Roweis, Sam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0ebcc77dc72360d0eb8e9504c78d38bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0ebcc77dc72360d0eb8e9504c78d38bd-Metadata.json},
 openalex = {W2118760499},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Nonlinear Dynamical Systems Using an EM Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/0ebcc77dc72360d0eb8e9504c78d38bd-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_0f3d014e,
 abstract = {We present a new energy-minimization framework for the graph isomorphism problem that is based on an equivalent maximum clique formulation. The approach is centered around a fundamental result proved by Motzkin and Straus in the mid-1960s, and recently expanded in various ways, which allows us to formulate the maximum clique problem in terms of a standard quadratic program. The attractive feature of this formulation is that a clear one-to-one correspondence exists between the solutions of the quadratic program and those in the original, combinatorial problem. To solve the program we use the so-called replicator equations--a class of straightforward continuous- and discrete-time dynamical systems developed in various branches of theoretical biology. We show how, despite their inherent inability to escape from local solutions, they nevertheless provide experimental results that are competitive with those obtained using more elaborate mean-field annealing heuristics.},
 author = {Pelillo, Marcello},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0f3d014eead934bbdbacb62a01dc4831-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0f3d014eead934bbdbacb62a01dc4831-Metadata.json},
 openalex = {W2147975091},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf},
 publisher = {MIT Press},
 title = {Replicator Equations, Maximal Cliques, and Graph Isomorphism},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/0f3d014eead934bbdbacb62a01dc4831-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_109d2dd3,
 abstract = {A recent neural model of illusory contour formation is based on a distribution of natural shapes traced by particles moving with constant speed in directions given by Brownian motions. The input to that model consists of pairs of position and direction constraints, and the output consists of the distribution of contours joining all such pairs. In general, these contours will not be closed, and their distribution will not be scale-invariant. In this article, we show how to compute a scale-invariant distribution of closed contours given position constraints alone and use this result to explain a well-known illusory contour effect.},
 author = {Thornber, Karvel and Williams, Lance},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/109d2dd3608f669ca17920c511c2a41e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/109d2dd3608f669ca17920c511c2a41e-Metadata.json},
 openalex = {W2140645652},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/109d2dd3608f669ca17920c511c2a41e-Paper.pdf},
 publisher = {MIT Press},
 title = {Orientation, Scale, and Discontinuity as Emergent Properties of Illusory Contour Shape},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/109d2dd3608f669ca17920c511c2a41e-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_1373b284,
 abstract = {A modular analogue neuro-chip set with on-chip learning capability is developed for active noise canceling. The analogue neuro-chip set incorporates the error backpropagation learning rule for practical applications, and allows pin-to-pin interconnections for multi-chip boards. The developed neuro-board demonstrated active noise canceling without any digital signal processor. Multi-path fading of acoustic channels, random noise, and nonlinear distortion of the loud speaker are compensated by the adaptive learning circuits of the neuro-chips. Experimental results are reported for cancellation of car noise in real time.},
 author = {Cho, Jung-Wook and Lee, Soo-Young},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1373b284bc381890049e92d324f56de0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1373b284bc381890049e92d324f56de0-Metadata.json},
 openalex = {W2113997972},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1373b284bc381890049e92d324f56de0-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Noise Canceling Using Analog Neuro-Chip with On-Chip Learning Capability},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/1373b284bc381890049e92d324f56de0-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_1415db70,
 abstract = {In order to grasp an object, we need to solve the inverse kinematics problem, i.e., the coordinate transformation from the visual coordinates to the joint angle vector coordinates of the arm. Although several models of coordinate transformation learning have been proposed, they suffer from a number of drawbacks. In human motion control, the learning of the hand position error feedback controller in the inverse kinematics solver is important. This paper proposes a novel model of the coordinate transformation learning of the human visual feedback controller that uses the change of the joint angle vector and the corresponding change of the square of the hand position error norm. The feasibility of the proposed model is illustrated using numerical simulations.},
 author = {Oyama, Eimei and Tachi, Susumu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1415db70fe9ddb119e23e9b2808cde38-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1415db70fe9ddb119e23e9b2808cde38-Metadata.json},
 openalex = {W2149395037},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1415db70fe9ddb119e23e9b2808cde38-Paper.pdf},
 publisher = {MIT Press},
 title = {Coordinate Transformation Learning of Hand Position Feedback Controller by Using Change of Position Error Norm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/1415db70fe9ddb119e23e9b2808cde38-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_1714726c,
 abstract = {A new paradigm is proposed for sorting spikes in multielectrode data using ratios of transfer functions between cells and electrodes. It is assumed that for every cell and electrode there is a stable linear relation. These are dictated by the properties of the tissue, the electrodes and their relative geometries. The main advantage of the method is that it is insensitive to variations in the shape and amplitude of a spike. Spike sorting is carried out in two separate steps. First, templates describing the statistics of each spike type are generated by clustering transfer function ratios then spikes are detected in the data using the spike statistics. These techniques were applied to data generated in the escape response system of the cockroach.},
 author = {Rinberg, Dmitry and Davidowitz, Hanan and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1714726c817af50457d810aae9d27a2e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1714726c817af50457d810aae9d27a2e-Metadata.json},
 openalex = {W2168690946},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1714726c817af50457d810aae9d27a2e-Paper.pdf},
 publisher = {MIT Press},
 title = {Multi-Electrode Spike Sorting by Clustering Transfer Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/1714726c817af50457d810aae9d27a2e-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_17e23e50,
 abstract = {We present Monte-Carlo generalized EM equations for learning in nonlinear state space models. The difficulties lie in the Monte-Carlo E-step which consists of sampling from the posterior distribution of the hidden variables given the observations. The new idea presented in this paper is to generate samples from a Gaussian approximation to the true posterior from which it is easy to obtain independent samples. The parameters of the Gaussian approximation are either derived from the extended Kalman filter or the Fisher scoring algorithm. In case the posterior density is multimodal we propose to approximate the posterior by a sum of Gaussians (mixture of modes approach). We show that sampling from the approximate posterior densities obtained by the above algorithms leads to better models than using point estimates for the hidden states. In our experiment, the Fisher scoring algorithm obtained a better approximation of the posterior mode than the EKF. For a multimodal distribution, the mixture of modes approach gave superior results.},
 author = {Briegel, Thomas and Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/17e23e50bedc63b4095e3d8204ce063b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/17e23e50bedc63b4095e3d8204ce063b-Metadata.json},
 openalex = {W2168685319},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/17e23e50bedc63b4095e3d8204ce063b-Paper.pdf},
 publisher = {MIT Press},
 title = {Fisher Scoring and a Mixture of Modes Approach for Approximate Inference and Learning in Nonlinear State Space Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_18d10dc6,
 abstract = {This paper presents probabilistic modeling methods to solve the problem of discriminating between five facial orientations with very little labeled data. Three models are explored. The first model maintains no inter-pixel dependencies, the second model is capable of modeling a set of arbitrary pair-wise dependencies, and the last model allows dependencies only between neighboring pixels. We show that for all three of these models, the accuracy of the learned models can be greatly improved by augmenting a small number of labeled training images with a large set of unlabeled images using Expectation-Maximization. This is important because it is often difficult to obtain image labels, while many unlabeled images are readily available. Through a large set of empirical tests, we examine the benefits of unlabeled data for each of the models. By using only two randomly selected labeled examples per class, we can discriminate between the five facial orientations with an accuracy of 94%; with six labeled examples, we achieve an accuracy of 98%.},
 author = {Baluja, Shumeet},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/18d10dc6e666eab6de9215ae5b3d54df-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/18d10dc6e666eab6de9215ae5b3d54df-Metadata.json},
 openalex = {W2158485347},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/18d10dc6e666eab6de9215ae5b3d54df-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Modeling for Face Orientation Discrimination: Learning from Labeled and Unlabeled Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/18d10dc6e666eab6de9215ae5b3d54df-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_18ead4c7,
 abstract = {Cumulative training margin distributions for AdaBoost versus our Direct Optimization Of Margins (DOOM) algorithm. The dark curve is AdaBoost, the light curve is DOOM. DOOM sacrifices significant training error for improved test error (horizontal marks on margin = 0 line).},
 author = {Mason, Llew and Bartlett, Peter and Baxter, Jonathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/18ead4c77c3f40dabf9735432ac9d97a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/18ead4c77c3f40dabf9735432ac9d97a-Metadata.json},
 openalex = {W2106332767},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/18ead4c77c3f40dabf9735432ac9d97a-Paper.pdf},
 publisher = {MIT Press},
 title = {Direct Optimization of Margins Improves Generalization in Combined Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/18ead4c77c3f40dabf9735432ac9d97a-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_1baff70e,
 abstract = {We describe the first single microphone sound localization system and its inspiration from theories of human monaural sound localization. Reflections and diffractions caused by the external ear (pinna) allow humans to estimate sound source elevations using only one ear. Our single microphone localization model relies on a specially shaped reflecting structure that serves the role of the pinna. Specially designed analog VLSI circuitry uses echo-time processing to localize the sound. A CMOS integrated circuit has been designed, fabricated, and successfully demonstrated on actual sounds.},
 author = {Harris, John and Pu, Chiang-Jung and Pr\'{\i}ncipe, Jos\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1baff70e2669e8376347efd3a874a341-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1baff70e2669e8376347efd3a874a341-Metadata.json},
 openalex = {W2137714933},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1baff70e2669e8376347efd3a874a341-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neuromorphic Monaural Sound Localizer},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/1baff70e2669e8376347efd3a874a341-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_1bc0249a,
 abstract = {Signal processing and pattern recognition algorithms make extensive use of convolution. In many cases, computational accuracy is not as important as computational speed. In feature extraction, for instance, the features of interest in a signal are usually quite distorted. This form of noise justifies some level of quantization in order to achieve faster feature extraction. Our approach consists of approximating regions of the signal with low degree polynomials, and then differentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions). With this representation, convolution becomes extremely simple and can be implemented quite effectively. The true convolution can be recovered by integrating the result of the convolution. This method yields substantial speed up in feature extraction and is applicable to convolutional neural networks.},
 author = {Simard, Patrice and Bottou, L\'{e}on and Haffner, Patrick and LeCun, Yann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1bc0249a6412ef49b07fe6f62e6dc8de-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1bc0249a6412ef49b07fe6f62e6dc8de-Metadata.json},
 openalex = {W2125199814},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1bc0249a6412ef49b07fe6f62e6dc8de-Paper.pdf},
 publisher = {MIT Press},
 title = {Boxlets: A Fast Convolution Algorithm for Signal Processing and Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/1bc0249a6412ef49b07fe6f62e6dc8de-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_1cd38823,
 abstract = {Partially Observable Markov Decision Processes (POMOPs) constitute an important class of reinforcement learning problems which present unique theoretical and computational difficulties. In the absence of the Markov property, popular reinforcement learning algorithms such as Q-Iearning may no longer be effective, and memory-based methods which remove partial observability via state-estimation are notoriously expensive. An alternative approach is to seek a stochastic memoryless policy which for each observation of the environment prescribes a probability distribution over available actions that maximizes the average reward per timestep. A reinforcement learning algorithm which learns a locally optimal stochastic memoryless policy has been proposed by Jaakkola, Singh and Jordan, but not empirically verified. We present a variation of this algorithm, discuss its implementation, and demonstrate its viability using four test problems.},
 author = {Williams, John and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1cd3882394520876dc88d1472aa2a93f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1cd3882394520876dc88d1472aa2a93f-Metadata.json},
 openalex = {W2149126181},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/1cd3882394520876dc88d1472aa2a93f-Paper.pdf},
 publisher = {MIT Press},
 title = {Experimental Results on Learning Stochastic Memoryless Policies for Partially Observable Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/1cd3882394520876dc88d1472aa2a93f-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_204da255,
 abstract = {We previously proposed a quantitative model of early visual processing in primates, based on non-linearly interacting visual filters and statistically efficient decision. We now use this model to interpret the observed modulation of a range of human psychophysical thresholds with and without focal visual attention. Our model - calibrated by an automatic fitting procedure - simultaneously reproduces thresholds for four classical pattern discrimination tasks, performed while attention was engaged by another concurrent task. Our model then predicts that the seemingly complex improvements of certain thresholds, which we observed when attention was fully available for the discrimination tasks, can best be explained by a strengthening of competition among early visual filters.},
 author = {Itti, Laurent and Braun, Jochen and Lee, Dale and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/204da255aea2cd4a75ace6018fad6b4d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/204da255aea2cd4a75ace6018fad6b4d-Metadata.json},
 openalex = {W2097258863},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/204da255aea2cd4a75ace6018fad6b4d-Paper.pdf},
 publisher = {MIT Press},
 title = {Attentional Modulation of Human Pattern Discrimination Psychophysics Reproduced by a Quantitative Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/204da255aea2cd4a75ace6018fad6b4d-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_21fe5b8b,
 abstract = {Here we derive measures quantifying the information loss of a synaptic signal due to the presence of neuronal noise sources, as it electrotonically propagates along a weakly-active dendrite. We model the dendrite as an infinite linear cable, with noise sources distributed along its length. The noise sources we consider are thermal noise, channel noise arising from the stochastic nature of voltage-dependent ionic channels (K+ and Na+) and synaptic noise due to spontaneous background activity. We assess the efficacy of information transfer using a signal detection paradigm where the objective is to detect the presence/absence of a presynaptic spike from the post-synaptic membrane voltage. This allows us to analytically assess the role of each of these noise sources in information transfer. For our choice of parameters, we find that the synaptic noise is the dominant noise source which limits the maximum length over which information be reliably transmitted.},
 author = {Manwani, Amit and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/21fe5b8ba755eeaece7a450849876228-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/21fe5b8ba755eeaece7a450849876228-Metadata.json},
 openalex = {W2171497772},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/21fe5b8ba755eeaece7a450849876228-Paper.pdf},
 publisher = {MIT Press},
 title = {Signal Detection in Noisy Weakly-Active Dendrites},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/21fe5b8ba755eeaece7a450849876228-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_226d1f15,
 abstract = {Kernel PCA as a nonlinear feature extractor has proven powerful as a preprocessing step for classification algorithms. But it can also be considered as a natural generalization of linear principal component analysis. This gives rise to the question how to use nonlinear features for data compression, reconstruction, and de-noising, applications common in linear PCA. This is a nontrivial task, as the results provided by kernel PCA live in some high dimensional feature space and need not have pre-images in input space. This work presents ideas for finding approximate pre-images, focusing on Gaussian kernels, and shows experimental results using these pre-images in data reconstruction and de-noising on toy examples as well as on real world data.},
 author = {Mika, Sebastian and Sch\"{o}lkopf, Bernhard and Smola, Alex and M\"{u}ller, Klaus-Robert and Scholz, Matthias and R\"{a}tsch, Gunnar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Metadata.json},
 openalex = {W2105732805},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernel PCA and De-Noising in Feature Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/226d1f15ecd35f784d2a20c3ecf56d7f-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_228499b5,
 abstract = {Humans demonstrate a remarkable ability to generate accurate and appropriate motor behavior under many different and oftpn uncprtain environmental conditions. This paper describes a new modular approach to human motor learning and control, based on multiple pairs of inverse (controller) and forward (prpdictor) models. This architecture simultaneously learns the multiple inverse models necessary for control as well as how to select the inverse models appropriate for a given environment. Simulations of object manipulation demonstrates the ability to learn mUltiple objects, appropriate generalization to novel objects and the inappropriate activation of motor programs based on visual cues, followed by on-line correction, seen in the size-weight illusion.},
 author = {Haruno, Masahiko and Wolpert, Daniel M and Kawato, Mitsuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/228499b55310264a8ea0e27b6e7c6ab6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/228499b55310264a8ea0e27b6e7c6ab6-Metadata.json},
 openalex = {W2122775046},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf},
 publisher = {MIT Press},
 title = {Multiple Paired Forward-Inverse Models for Human Motor Learning and Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/228499b55310264a8ea0e27b6e7c6ab6-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_253f7b5d,
 abstract = {We present a split-and-merge expectation-maximization (SMEM) algorithm to overcome the local maxima problem in parameter estimation of finite mixture models. In the case of mixture models, local maxima often involve having too many components of a mixture model in one part of the space and too few in another, widely separated part of the space. To escape from such configurations, we repeatedly perform simultaneous split-and-merge operations using a new criterion for efficiently selecting the split-and-merge candidates. We apply the proposed algorithm to the training of gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split-and-merge operations to improve the likelihood of both the training data and of held-out test data. We also show the practical usefulness of the proposed algorithm by applying it to image compression and pattern recognition problems.},
 author = {Ueda, Naonori and Nakano, Ryohei and Ghahramani, Zoubin and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/253f7b5d921338af34da817c00f42753-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/253f7b5d921338af34da817c00f42753-Metadata.json},
 openalex = {W2114759290},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/253f7b5d921338af34da817c00f42753-Paper.pdf},
 publisher = {MIT Press},
 title = {SMEM Algorithm for Mixture Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/253f7b5d921338af34da817c00f42753-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_277281aa,
 abstract = {One of the most important problems in visual perception is that of visual invariance: how are objects perceived to be the same despite undergoing transformations such as translations, rotations or scaling? In this paper, we describe a Bayesian method for learning invariances based on Lie group theory. We show that previous approaches based on first-order Taylor series expansions of inputs can be regarded as special cases of the Lie group approach, the latter being capable of handling in principle arbitrarily large transfonnations. Using a matrix-exponential based generative model of images, we derive an unsupervised algorithm for learning Lie group operators from input data containing infinitesimal transfonnations. The on-line unsupervised learning algorithm maximizes the posterior probability of generating the training data. We provide experimental results suggesting that the proposed method can learn Lie group operators for handling reasonably large 1-D translations and 2-D rotations.},
 author = {Rao, Rajesh and Ruderman, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/277281aada22045c03945dcb2ca6f2ec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/277281aada22045c03945dcb2ca6f2ec-Metadata.json},
 openalex = {W2169019105},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/277281aada22045c03945dcb2ca6f2ec-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Lie Groups for Invariant Visual Perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/277281aada22045c03945dcb2ca6f2ec-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_297fa777,
 abstract = {Graphical models provide a broad probabilistic framework with applications in speech recognition (Hidden Markov Models), medical diagnosis (Belief networks) and artificial intelligence (Boltzmann Machines). However, the computing time is typically exponential in the number of nodes in the graph. Within the variational framework for approximating these models, we present two classes of distributions, decimatable Boltzmann Machines and Tractable Belief Networks that go beyond the standard factorized approach. We give generalised mean-field equations for both these directed and undirected approximations. Simulation results on a small benchmark problem suggest using these richer approximations compares favorably against others previously reported in the literature.},
 author = {Barber, David and Wiegerinck, Wim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/297fa7777981f402dbba17e9f29e292d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/297fa7777981f402dbba17e9f29e292d-Metadata.json},
 openalex = {W2152895903},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/297fa7777981f402dbba17e9f29e292d-Paper.pdf},
 publisher = {MIT Press},
 title = {Tractable Variational Structures for Approximating Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/297fa7777981f402dbba17e9f29e292d-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_29921001,
 abstract = {In this paper we describe the architecture, implementation and experimental results for an Intracardiac Electrogram (ICEG) classification and compression chip. The chip processes and vector-quantises 30 dimensional analogue vectors while consuming a maximum of 2.5 µW power for a heart rate of 60 beats per minute (1 vector per second) from a 3.3 V supply. This represents a significant advance on previous work which achieved ultra low power supervised morphology classification since the template matching scheme used in this chip enables unsupervised blind classification of abnonnal rhythms and the computational support for low bit rate data compression. The adaptive template matching scheme used is tolerant to amplitude variations, and inter- and intra-sample time shifts.},
 author = {Coggins, Richard and Wang, Raymond and Jabri, Marwan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/29921001f2f04bd3baee84a12e98098f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/29921001f2f04bd3baee84a12e98098f-Metadata.json},
 openalex = {W2168997178},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Micropower CMOS Adaptive Amplitude and Shift Invariant Vector Quantiser},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/29921001f2f04bd3baee84a12e98098f-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_2b3bf3ee,
 abstract = {This paper formulates the problem of visual search as Bayesian inference and defines a Bayesian ensemble of problem instances. In particular, we address the problem of the detection of visual contours in noise/clutter by optimizing a global criterion which combines local intensity and geometry information. We analyze the convergence rates of A* search algorithms using results from information theory to bound the probability of rare events within the Bayesian ensemble. This analysis determines characteristics of the domain, which we call order parameters, that determine the convergence rates. In particular, we present a specific admissible A* algorithm with pruning which converges, with high probability, with expected time O(N) in the size of the problem. In addition, we briefly summarize extensions of this work which address fundamental limits of target contour detectability (i.e. algorithm independent results) and the use of non-admissible heuristics.},
 author = {Yuille, Alan L and Coughlan, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/2b3bf3eee2475e03885a110e9acaab61-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/2b3bf3eee2475e03885a110e9acaab61-Metadata.json},
 openalex = {W2156741519},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/2b3bf3eee2475e03885a110e9acaab61-Paper.pdf},
 publisher = {MIT Press},
 title = {Convergence Rates of Algorithms for Visual Search: Detecting Visual Contours},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/2b3bf3eee2475e03885a110e9acaab61-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_309fee4e,
 abstract = {Finding articulated objects, like people, in pictures presents a particularly difficult object recognition problem. We show how to find people by finding putative body segments, and then constructing assemblies of those segments that are consistent with the constraints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at least nine segments, it is not possible to present every group to a classifier. Instead, the search can be pruned by using projected versions of a classifier that accepts groups corresponding to people. We describe an efficient projection algorithm for one popular classifier, and demonstrate that our approach can be used to determine whether images of real scenes contain people.},
 author = {Ioffe, Sergey and Forsyth, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/309fee4e541e51de2e41f21bebb342aa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/309fee4e541e51de2e41f21bebb342aa-Metadata.json},
 openalex = {W2095767300},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning to Find Pictures of People},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/309fee4e541e51de2e41f21bebb342aa-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_351b3358,
 abstract = {Cluster analysis is a fundamental principle in exploratory data analysis, providing the user with a description of the group structure of given data. A key problem in this context is the interpretation and visualization of clustering solutions in high-dimensional or abstract data spaces. In particular, probabilistic descriptions of the group structure, essential to capture inter-cluster relationships, are hardly assessable by simple inspection ofthe probabilistic assignment variables. We present a novel approach to the visualization of group structure. It is based on a statistical model of the object assignments which have been observed or estimated by a probabilistic clustering procedure. The objects or data points are embedded in a low dimensional Euclidean space by approximating the observed data statistics with a Gaussian mixture model. The algorithm provides a new approach to the visualization of the inherent structure for a broad variety of data types, e.g. histogram data, proximity data and co-occurrence data. To demonstrate the power of the approach, histograms of textured images are visualized as an example of a large-scale data mining application.},
 author = {Held, Marcus and Puzicha, Jan and Buhmann, Joachim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/351b33587c5fdd93bd42ef7ac9995a28-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/351b33587c5fdd93bd42ef7ac9995a28-Metadata.json},
 openalex = {W2168257133},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/351b33587c5fdd93bd42ef7ac9995a28-Paper.pdf},
 publisher = {MIT Press},
 title = {Visualizing Group Structure},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/351b33587c5fdd93bd42ef7ac9995a28-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_35309226,
 abstract = {We examine the statistics of natural monochromatic images decomposed using a multi-scale wavelet basis. Although the coefficients of this representation are nearly decorrelated, they exhibit important higher-order statistical dependencies that cannot be eliminated with purely linear procssing. In particular, rectified coefficients corresponding to basis functions at neighboring spatial positions, orientations and scales are highly correlated. A method of removing these dependencies is to divide each coefficient by a weighted combination of its rectified neighbors. Several successful models of the steady -state behavior of neurons in primary visual cortex are based on such divisive computations, and thus our analysis provides a theoretical justification for these models. Perhaps more importantly, the statistical measurements explicitly specify the weights that should be used in computing the normalization signal. We demonstrate that this weighting is qualitatively consistent with recent physiological experiments that characterize the suppressive effect of stimuli presented outside of the classical receptive field. Our observations thus provide evidence for the hypothesis that early visual neural processing is well matched to these statistical properties of images.},
 author = {Simoncelli, Eero and Schwartz, Odelia},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/35309226eb45ec366ca86a4329a2b7c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/35309226eb45ec366ca86a4329a2b7c3-Metadata.json},
 openalex = {W2104244367},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/35309226eb45ec366ca86a4329a2b7c3-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling Surround Suppression in V1 Neurons with a Statistically Derived Normalization Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/35309226eb45ec366ca86a4329a2b7c3-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_35464c84,
 abstract = {The ARTMAP-FD neural network performs both identification (placing test patterns in classes encountered during training) and familiarity discrimination (judging whether a test pattern belongs to any of the classes encountered during training). The performance of ARTMAP-FD is tested on radar pulse data obtained in the field, and compared to that of the nearest-neighbor-based NEN algorithm and to a k > 1 extension of NEN.},
 author = {Granger, Eric and Grossberg, Stephen and Rubin, Mark and Streilein, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/35464c848f410e55a13bb9d78e7fddd0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/35464c848f410e55a13bb9d78e7fddd0-Metadata.json},
 openalex = {W2167518683},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/35464c848f410e55a13bb9d78e7fddd0-Paper.pdf},
 publisher = {MIT Press},
 title = {Familiarity Discrimination of Radar Pulses},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/35464c848f410e55a13bb9d78e7fddd0-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_3a20f62a,
 abstract = {We describe a real-time computer vision and machine learning system for modeling and recognizing human actions and interactions. Two different domains are explored: recognition of two-handed motions in the martial art 'Tai Chi', and multiple-person interactions in a visual surveillance task. Our system combines top-down with bottom-up information using a feedback loop, and is formulated with a Bayesian framework. Two different graphical models (HMMs and Coupled HMMs) are used for modeling both individual actions and multiple-agent interactions, and CHMMs are shown to work more efficiently and accurately for a given amount of training. Finally, to overcome the limited amounts of training data, we demonstrate that 'synthetic agents' (Alife-style agents) can be used to develop flexible prior models of the person-to-person interactions.},
 author = {Oliver, Nuria and Rosario, Barbara and Pentland, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/3a20f62a0af1aa152670bab3c602feed-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/3a20f62a0af1aa152670bab3c602feed-Metadata.json},
 openalex = {W2167023596},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf},
 publisher = {MIT Press},
 title = {Graphical Models for Recognizing Human Interactions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_3fb451ca,
 abstract = {We introduce a novel framework for simultaneous structure and parameter learning in hidden-variable conditional probability models, based on an entropic prior and a solution for its maximum a posteriori (MAP) estimator. The MAP estimate minimizes uncertainty in all respects: cross-entropy between model and data; entropy of the model; entropy of the data's descriptive statistics. Iterative estimation extinguishes weakly supported parameters, compressing and sparsifying the model. Trimming operators accelerate this process by removing excess parameters and, unlike most pruning schemes, guarantee an increase in posterior probability. Entropic estimation takes a overcomplete random model and simplifies it, inducing the structure of relations between hidden and observed variables. Applied to hidden Markov models (HMMs), it finds a concise finite-state machine representing the hidden structure of a signal. We entropically model music, handwriting, and video time-series, and show that the resulting models are highly concise, structured, predictive, and interpretable: Surviving states tend to be highly correlated with meaningful partitions of the data, while surviving transitions provide a low-perplexity model of the signal dynamics.},
 author = {Brand, Matthew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/3fb451ca2e89b3a13095b059d8705b15-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/3fb451ca2e89b3a13095b059d8705b15-Metadata.json},
 openalex = {W2135624911},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/3fb451ca2e89b3a13095b059d8705b15-Paper.pdf},
 publisher = {MIT Press},
 title = {An Entropic Estimator for Structure Discovery},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/3fb451ca2e89b3a13095b059d8705b15-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_41a60377,
 abstract = {We study the effect of correlated noise on the accuracy of population coding using a model of a population of neurons that are broadly tuned to an angle in two-dimension. The fluctuations in the neuronal activity is modeled as a Gaussian noise with pairwise correlations which decays exponentially with the difference between the preferred orientations of the pair. By calculating the Fisher information of the system, we show that in the biologically relevant regime of parameters positive correlations decrease the estimation capability of the network relative to the uncorrelated population. Moreover strong positive correlations result in information capacity which saturates to a finite value as the number of cells in the population grows. In contrast, negative correlations substantially increase the information capacity of the neuronal population.},
 author = {Yoon, Hyoungsoo and Sompolinsky, Haim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/41a60377ba920919939d83326ebee5a1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/41a60377ba920919939d83326ebee5a1-Metadata.json},
 openalex = {W2158571236},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/41a60377ba920919939d83326ebee5a1-Paper.pdf},
 publisher = {MIT Press},
 title = {The Effect of Correlations on the Fisher Information of Population Codes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/41a60377ba920919939d83326ebee5a1-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_42a39645,
 abstract = {A correlation-based learning rule at the spike level is formulated, mathematically analyzed, and compared to learning in a firing-rate description. A differential equation for the learning dynamics is derived under the assumption that the time scales of learning and spiking can be separated. For a linear Poissonian neuron model which receives time-dependent stochastic input we show that spike correlations on a millisecond time scale play indeed a role. Correlations between input and output spikes tend to stabilize structure formation, provided that the form of the learning window is in accordance with Hebb's principle. Conditions for an intrinsic normalization of the average synaptic weight are discussed.},
 author = {Kempter, Richard and Gerstner, Wulfram and van Hemmen, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/42a3964579017f3cb42b26605b9ae8ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/42a3964579017f3cb42b26605b9ae8ef-Metadata.json},
 openalex = {W2114555994},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/42a3964579017f3cb42b26605b9ae8ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Spike-Based Compared to Rate-Based Hebbian Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/42a3964579017f3cb42b26605b9ae8ef-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_42d6c7d6,
 abstract = {The contrast response function (CRF) of many neurons in the primary visual cortex saturates and shifts towards higher contrast values following prolonged presentation of high contrast visual stimuli. Using a recurrent neural network of excitatory spiking neurons with adapting synapses we show that both effects could be explained by a fast and a slow component in the synaptic adaptation. (i) Fast synaptic depression leads to saturation of the CRF and phase advance in the cortical response to high contrast stimuli. (ii) Slow adaptation of the synaptic transmitter release probability is derived such that the mutual information between the input and the output of a cortical neuron is maximal. This component--given by infomax learning rule--explains contrast adaptation of the averaged membrane potential (DC component) as well as the surprising experimental result, that the stimulus modulated component (F1 component) of a cortical cell's membrane potential adapts only weakly. Based on our results, we propose a new experiment to estimate the strength of the effective excitatory feedback to a cortical neuron, and we also suggest a relatively simple experimental test to justify our hypothesized synaptic mechanism for contrast adaptation.},
 author = {Adorj\'{a}n, P\'{e}ter and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/42d6c7d61481d1c21bd1635f59edae05-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/42d6c7d61481d1c21bd1635f59edae05-Metadata.json},
 openalex = {W2147217471},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/42d6c7d61481d1c21bd1635f59edae05-Paper.pdf},
 publisher = {MIT Press},
 title = {Contrast Adaptation in Simple Cells by Changing the Transmitter Release Probability},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/42d6c7d61481d1c21bd1635f59edae05-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_4462bf0d,
 abstract = {The perfonnance of dedicated VLSI neural processing hardware depends critically on the design of the implemented algorithms. We have previously proposed an algorithm for acoustic transient classification [1]. Having implemented and demonstrated this algorithm in a mixed-mode architecture, we now investigate variants on the algorithm, using time and frequency channel differencing, input and output nonnalization, and schemes to binarize and train the template values, with the goal of achieving optimal classification perfonnance for the chosen hardware.},
 author = {Edwards, R. and Cauwenberghs, Gert and Pineda, Fernando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4462bf0ddbe0d0da40e1e828ebebeb11-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4462bf0ddbe0d0da40e1e828ebebeb11-Metadata.json},
 openalex = {W2124972637},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4462bf0ddbe0d0da40e1e828ebebeb11-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimizing Correlation Algorithms for Hardware-Based Transient Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/4462bf0ddbe0d0da40e1e828ebebeb11-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_452bf208,
 abstract = {Information from the senses must be compressed into the limited range of firing rates generated by spiking nerve cells. Optimal compression uses all firing rates equally often, implying that the nerve cell's response matches the statistics of naturally occurring stimuli. Since changing the voltage-dependent ionic conductances in the cell membrane alters the flow of information, an unsupervised, non-Hebbian, developmental learning rule is derived to adapt the conductances in Hodgkin-Huxley model neurons. By maximizing the rate of information transmission, each firing rate within the model neuron's limited dynamic range is used equally often.},
 author = {Stemmler, Martin and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/452bf208bf901322968557227b8f6efe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/452bf208bf901322968557227b8f6efe-Metadata.json},
 openalex = {W2152454151},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/452bf208bf901322968557227b8f6efe-Paper.pdf},
 publisher = {MIT Press},
 title = {Information Maximization in Single Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/452bf208bf901322968557227b8f6efe-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_46031b3d,
 abstract = {Learning in many visual perceptual tasks has been shown to be specific to practiced stimuli, while new stimuli have to be learned from scratch. Here we demonstrate generalization using a novel paradigm in motion discrimination where learning has been previously shown to be specific. We trained subjects to discriminate directions of moving dots, and verified the previous results that learning does not transfer from a trained direction to a new one. However, by tracking the subjects' performance across time in the new direction, we found that their speed of learning doubled. Therefore, we found generalization in a task previously considered too difficult to generalize. We also replicated, in a second experiment, transfer following training with 'easy' stimuli, when the difference between motion directions is enlarged. In a third experiment we found a new mode of generalization: after mastering the task with an easy stimulus, subjects who have practiced briefly to discriminate the easy stimulus in a new direction generalize to a difficult stimulus in that direction. This generalization depends on both the mastering and the brief practice. The specificity of perceptual learning and the dichotomy between learning of 'easy' versus 'difficult' tasks have been assumed to involve different learning processes at different cortical areas. Here we show how to interpret these results in terms of signal detection theory. With the assumption of limited computational capacity, we obtain the observed phenomena--direct transfer and acceleration of learning--for increasing levels of task difficulty. Human perceptual learning and generalization, therefore, concur with a generic discrimination system.},
 author = {Liu, Zili and Weinshall, Daphna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/46031b3d04dc90994ca317a7c55c4289-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/46031b3d04dc90994ca317a7c55c4289-Metadata.json},
 openalex = {W2055463424},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/46031b3d04dc90994ca317a7c55c4289-Paper.pdf},
 publisher = {MIT Press},
 title = {Mechanisms of generalization in perceptual learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/46031b3d04dc90994ca317a7c55c4289-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_490640b4,
 abstract = {We present a probabilistic method for fusion of images produced by multiple sensors. The approach is based on an image formation model in which the sensor images are noisy, locally linear functions of an underlying, true scene. A Bayesian framework then provides for maximum likelihood or maximum a posteriori estimates of the true scene from the sensor images. Maximum likelihood estimates of the parameters of the image formation model involve (local) second order image statistics, and thus are related to local principal component analysis. We demonstrate the efficacy of the method on images from visible-band and infrared sensors.},
 author = {Sharma, Ravi and Leen, Todd and Pavel, Misha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/490640b43519c77281cb2f8471e61a71-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/490640b43519c77281cb2f8471e61a71-Metadata.json},
 openalex = {W2132760566},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/490640b43519c77281cb2f8471e61a71-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Image Sensor Fusion},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/490640b43519c77281cb2f8471e61a71-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_495dabfd,
 abstract = {We solve the dynamics of on-line Hebbian learning in perceptrons exactly, for the regime where the size of the training set scales linearly with the number of inputs. We consider both noiseless and noisy teachers. Our calculation cannot be extended to non-Hebbian rules, but the solution provides a nice benchmark to test more general and advanced theories for solving the dynamics of learning with restricted training sets.},
 author = {Rae, H. and Sollich, Peter and Coolen, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/495dabfd0ca768a3c3abd672079f48b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/495dabfd0ca768a3c3abd672079f48b6-Metadata.json},
 openalex = {W2148447114},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/495dabfd0ca768a3c3abd672079f48b6-Paper.pdf},
 publisher = {MIT Press},
 title = {On-Line Learning with Restricted Training Sets: Exact Solution as Benchmark for General Theories},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/495dabfd0ca768a3c3abd672079f48b6-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_49af6c4e,
 abstract = {We solve the dynamics of Hopfield-type neural networks which store sequences of patterns, close to saturation. The asymmetry of the interaction matrix in such models leads to violation of detailed balance, ruling out an equilibrium statistical mechanical analysis. Using generating functional methods we derive exact closed equations for dynamical order parameters, namely the sequence overlap and correlation and response functions, in the thermodynamic limit. We calculate the time translation invariant solutions of these equations, describing stationary limit cycles, which leads to a phase diagram. The effective retarded self-interaction usually appearing in symmetric models is here found to vanish, which causes a significantly enlarged storage capacity of , compared with for Hopfield networks storing static patterns. Our results are tested against extensive computer simulations and excellent agreement is found.},
 author = {D\"{u}ring, A. and Coolen, Anthony and Sherrington, D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/49af6c4e558a7569d80eee2e035e2bd7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/49af6c4e558a7569d80eee2e035e2bd7-Metadata.json},
 openalex = {W2022008179},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/49af6c4e558a7569d80eee2e035e2bd7-Paper.pdf},
 publisher = {MIT Press},
 title = {Phase diagram and storage capacity of sequence processing neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/49af6c4e558a7569d80eee2e035e2bd7-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_49b8b4f9,
 abstract = {We present a method for learning complex appearance mappings. such as occur with images of articulated objects. Traditional interpolation networks fail on this case since appearance is not necessarily a smooth function nor a linear manifold for articulated objects. We define an appearance mapping from examples by constructing a set of independently smooth interpolation networks; these networks can cover overlapping regions of parameter space. A set growing procedure is used to find example clusters which are well-approximated within their convex hull; interpolation then proceeds only within these sets of examples. With this method physically valid images are produced even in regions of parameter space where nearby examples have different appearances. We show results generating both simulated and real arm images.},
 author = {Darrell, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/49b8b4f95f02e055801da3b4f58e28b7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/49b8b4f95f02e055801da3b4f58e28b7-Metadata.json},
 openalex = {W2143862247},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf},
 publisher = {MIT Press},
 title = {Example-Based Image Synthesis of Articulated Figures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/49b8b4f95f02e055801da3b4f58e28b7-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_4d6e4749,
 abstract = {A new algorithm for Support Vector regression is described. For a priori chosen ν, it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction ν of the data points lie outside. Moreover, it is shown how to use parametric tube shapes with non-constant radius. The algorithm is analysed theoretically and experimentally.},
 author = {Sch\"{o}lkopf, Bernhard and Bartlett, Peter and Smola, Alex and Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4d6e4749289c4ec58c0063a90deb3964-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4d6e4749289c4ec58c0063a90deb3964-Metadata.json},
 openalex = {W2142767931},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4d6e4749289c4ec58c0063a90deb3964-Paper.pdf},
 publisher = {MIT Press},
 title = {Shrinking the Tube: A New Support Vector Regression Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/4d6e4749289c4ec58c0063a90deb3964-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_4dcf4354,
 abstract = {Lazy learning is a memory-based technique that, once a query is received, extracts a prediction interpolating locally the neighboring examples of the query which are considered relevant according to a distance measure. In this paper we propose a data-driven method to select on a query-by-query basis the optimal number of neighbors to be considered for each prediction. As an efficient way to identify and validate local models, the recursive least squares algorithm is introduced in the context of local approximation and lazy learning. Furthermore, beside the winner-takes-all strategy for model selection, a local combination of the most promising models is explored. The method proposed is tested on six different datasets and compared with a state-of-the-art approach.},
 author = {Birattari, Mauro and Bontempi, Gianluca and Bersini, Hugues},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4dcf435435894a4d0972046fc566af76-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4dcf435435894a4d0972046fc566af76-Metadata.json},
 openalex = {W2105738427},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4dcf435435894a4d0972046fc566af76-Paper.pdf},
 publisher = {MIT Press},
 title = {Lazy Learning Meets the Recursive Least Squares Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/4dcf435435894a4d0972046fc566af76-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_4e6cd952,
 abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
 author = {Moody, John and Saffell, Matthew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4e6cd95227cb0c280e99a195be5f6615-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4e6cd95227cb0c280e99a195be5f6615-Metadata.json},
 openalex = {W2107726111},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4e6cd95227cb0c280e99a195be5f6615-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning: A Survey},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/4e6cd95227cb0c280e99a195be5f6615-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_4e9cec1f,
 abstract = {Most theoretical and empirical studies of population codes make the assumption that underlying neuronal activities is a unique and unambiguous value of an encoded quantity. However, population activities can contain additional information about such things as multiple values of or uncertainty about the quantity. We have previously suggested a method to recover extra information by treating the activities of the population of cells as coding for a complete distribution over the coded quantity rather than just a single value. We now show how this approach bears on psychophysical and neurophysiological studies of population codes for motion direction in tasks involving transparent motion stimuli. We show that, unlike standard approaches, it is able to recover multiple motions from population responses, and also that its output is consistent with both correct and erroneous human performance on psychophysical tasks.},
 author = {Zemel, Richard and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4e9cec1f583056459111d63e24f3b8ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4e9cec1f583056459111d63e24f3b8ef-Metadata.json},
 openalex = {W2137463787},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/4e9cec1f583056459111d63e24f3b8ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Distributional Population Codes and Multiple Motion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/4e9cec1f583056459111d63e24f3b8ef-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_5129a5dd,
 abstract = {A COllective INtelligence (COIN) is a set of interacting reinforcement learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments using that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms.},
 author = {Wolpert, David and Tumer, Kagan and Frank, Jeremy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5129a5ddcd0dcd755232baa04c231698-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5129a5ddcd0dcd755232baa04c231698-Metadata.json},
 openalex = {W2126306851},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5129a5ddcd0dcd755232baa04c231698-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Collective Intelligence to Route Internet Traffic},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/5129a5ddcd0dcd755232baa04c231698-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_52947e0a,
 abstract = {Sparse coding is a method for finding a representation of data in which each of the components of the representation is only rarely significantly active. Such a representation is closely related to redundancy reduction and independent component analysis, and has some neurophysiological plausibility. In this paper, we show how sparse coding can be used for denoising. Using maximum likelihood estimation of nongaussian variables corrupted by gaussian noise, we show how to apply a shrinkage nonlinearity on the components of sparse coding so as to reduce noise. Furthermore, we show how to choose the optimal sparse coding basis for denoising. Our method is closely related to the method of wavelet shrinkage, but has the important benefit over wavelet methods that both the features and the shrinkage parameters are estimated directly from the data.},
 author = {Hyv\"{a}rinen, Aapo and Hoyer, Patrik and Oja, Erkki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/52947e0ade57a09e4a1386d08f17b656-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/52947e0ade57a09e4a1386d08f17b656-Metadata.json},
 openalex = {W2150956999},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/52947e0ade57a09e4a1386d08f17b656-Paper.pdf},
 publisher = {MIT Press},
 title = {Sparse Code Shrinkage: Denoising by Nonlinear Maximum Likelihood Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/52947e0ade57a09e4a1386d08f17b656-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_55c567fd,
 abstract = {Gaussian process (GP) prediction suffers from O(n3) scaling with the data set size n. By using a finite-dimensional basis to approximate the GP predictor, the computational complexity can be reduced. We derive optimal finite-dimensional predictors under a number of assumptions, and show the superiority of these predictors over the Projected Bayes Regression method (which is asymptotically optimal). We also show how to calculate the minimal model size for a given n. The calculations are backed up by numerical experiments.},
 author = {Ferrari-Trecate, Giancarlo and Williams, Christopher and Opper, Manfred},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/55c567fd4395ecef6d936cf77b8d5b2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/55c567fd4395ecef6d936cf77b8d5b2b-Metadata.json},
 openalex = {W2139479120},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/55c567fd4395ecef6d936cf77b8d5b2b-Paper.pdf},
 publisher = {MIT Press},
 title = {Finite-Dimensional Approximation of Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/55c567fd4395ecef6d936cf77b8d5b2b-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_5607fe88,
 abstract = {This paper introduces a method for regularization of HMM systems that avoids parameter overfitting caused by insufficient training data. Regularization is done by augmenting the EM training method by a penalty term that favors simple and smooth HMM systems. The penalty term is constructed as a mixture model of negative exponential distributions that is assumed to generate the state dependent emission probabilities of the HMMs. This new method is the successful transfer of a well known regularization approach in neural networks to the HMM domain and can be interpreted as a generalization of traditional state-tying for HMM systems. The effect of regularization is demonstrated for continuous speech recognition tasks by improving overfitted triphone models and by speaker adaptation with limited training data.},
 author = {Neukirchen, Christoph and Rigoll, Gerhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5607fe8879e4fd269e88387e8cb30b7e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5607fe8879e4fd269e88387e8cb30b7e-Metadata.json},
 openalex = {W2129495413},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5607fe8879e4fd269e88387e8cb30b7e-Paper.pdf},
 publisher = {MIT Press},
 title = {Controlling the Complexity of HMM Systems by Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/5607fe8879e4fd269e88387e8cb30b7e-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_596f713f,
 abstract = {The execution order of a block of computer instructions can make a difference in its running time by a factor of two or more. In order to achieve the best possible speed, compilers use heuristic schedulers appropriate to each specific architecture implementation. However, these heuristic schedulers are time-consuming and expensive to build. In this paper, we present results using both rollouts and reinforcement learning to construct heuristics for scheduling basic blocks. The rollout scheduler outperformed a commercial scheduler, and the reinforcement learning scheduler performed almost as well as the commercial scheduler.},
 author = {McGovern, Amy and Moss, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/596f713f9a7376fe90a62abaaedecc2d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/596f713f9a7376fe90a62abaaedecc2d-Metadata.json},
 openalex = {W2171960381},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/596f713f9a7376fe90a62abaaedecc2d-Paper.pdf},
 publisher = {MIT Press},
 title = {Scheduling Straight-Line Code Using Reinforcement Learning and Rollouts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/596f713f9a7376fe90a62abaaedecc2d-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_5c50b4df,
 abstract = {We seek the scene interpretation that best explains image data. For example, we may want to infer the projected velocities (scene) which best explain two consecutive image frames (image). From synthetic data, we model the relationship between image and scene patches, and between a scene patch and neighboring scene patches. Given a new image, we propagate likelihoods in a Markov network (ignoring the effect of loops) to infer the underlying scene. This yields an efficient method to form low-level scene interpretations. We demonstrate the technique for motion analysis and estimating high resolution images from low-resolution ones.},
 author = {Freeman, William and Pasztor, Egon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5c50b4df4b176845cd235b6a510c6903-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5c50b4df4b176845cd235b6a510c6903-Metadata.json},
 openalex = {W2153758647},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5c50b4df4b176845cd235b6a510c6903-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning to Estimate Scenes from Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/5c50b4df4b176845cd235b6a510c6903-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_5cbdfd0d,
 abstract = {I consider the problem of calculating learning curves (i.e., average generalization performance) of Gaussian processes used for regression. A simple expression for the generalization error in terms of the eigenvalue decomposition of the covariance function is derived, and used as the starting point for several approximation schemes. I identify where these become exact, and compare with existing bounds on learning curves; the new approximations, which can be used for any input space dimension, generally get substantially closer to the truth.},
 author = {Sollich, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5cbdfd0dfa22a3fca7266376887f549b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5cbdfd0dfa22a3fca7266376887f549b-Metadata.json},
 openalex = {W2118195892},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5cbdfd0dfa22a3fca7266376887f549b-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Curves for Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/5cbdfd0dfa22a3fca7266376887f549b-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_5cce8ded,
 abstract = {We compare the ability of three exemplar-based memory models, each using three different stimulus representations, to account for the probability a human subject responded old in an old/new facial memory experiment. The models are 1) the Generalized Context Model, 2) SimSample, a probabilistic sampling model, and 3) MMOM, a novel model related to kernel density estimation that explicitly encodes stimulus distinctiveness. The representations are 1) positions of stimuli in MDS face space, 2) projections of test faces onto the eigenfaces of the study set, and 3) a representation based on response to a grid of Gabor filter jets. Of the 9 model/representation combinations, only the distinctiveness model in MDS space predicts the observed morph familiarity inversion effect, in which the subjects' false alarm rate for morphs between similar faces is higher than their hit rate for many of the studied faces. This evidence is consistent with the hypothesis that human memory for faces is a kernel density estimation task, with the caveat that distinctive faces require larger kernels than do typical faces.},
 author = {Dailey, Matthew and Cottrell, Garrison and Busey, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5cce8dede893813f879b873962fb669f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5cce8dede893813f879b873962fb669f-Metadata.json},
 openalex = {W2169602138},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/5cce8dede893813f879b873962fb669f-Paper.pdf},
 publisher = {MIT Press},
 title = {Facial Memory Is Kernel Density Estimation (Almost)},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/5cce8dede893813f879b873962fb669f-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_6490791e,
 abstract = {A circuit for fast, compact and low-power focal-plane motion centroid localization is presented. This chip, which uses mixed signal CMOS components to implement photodetection, edge detection, ON-set detection and centroid localization, models the retina and superior colliculus. The centroid localization circuit uses time-windowed asynchronously triggered row and column address events and two linear resistive grids to provide the analog coordinates of the motion centroid. This VLSI chip is used to realize fast lightweight autonavigating vehicles. The obstacle avoiding line-following algorithm is discussed.},
 author = {Etienne-Cummings, Ralph and Gruev, Viktor and Ghani, Mohammed},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6490791e7abf6b29a381288cc23a8223-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6490791e7abf6b29a381288cc23a8223-Metadata.json},
 openalex = {W2167071023},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6490791e7abf6b29a381288cc23a8223-Paper.pdf},
 publisher = {MIT Press},
 title = {VLSI Implementation of Motion Centroid Localization for Autonomous Navigation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/6490791e7abf6b29a381288cc23a8223-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_655ea4bd,
 abstract = {We present exact analytical equilibrium solutions for a class of recurrent neural network models, with both sequential and parallel neuronal dynamics, in which there is a tunable competition between nearest-neighbour and long-range synaptic interactions. This competition is found to induce novel coexistence phenomena as well as discontinuous transitions between pattern recall states, 2-cycles and non-recall states.},
 author = {Skantzos, N. and Beckmann, C. and Coolen, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/655ea4bd3b5736d88afc30c9212ccddf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/655ea4bd3b5736d88afc30c9212ccddf-Metadata.json},
 openalex = {W2144566490},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/655ea4bd3b5736d88afc30c9212ccddf-Paper.pdf},
 publisher = {MIT Press},
 title = {Discontinuous Recall Transitions Induced by Competition Between Short- and Long-Range Interactions in Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/655ea4bd3b5736d88afc30c9212ccddf-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_69d658d0,
 abstract = {A robust, integrative algorithm is presented for computing the position of the focus of expansion or axis of rotation (the singular point) in optical flow fields such as those generated by self-motion. Measurements are shown of a fully parallel CMOS analog VLSI motion sensor array which computes the direction of local motion (sign of optical flow) at each pixel and can directly implement this algorithm. The flow field singular point is computed in real time with a power consumption of less than 2 mW. Computation of the singular point for more general flow fields requires measures of field expansion and rotation, which it is shown can also be computed in real-time hardware, again using only the sign of the optical flow field. These measures, along with the location of the singular point, provide robust real-time self-motion information for the visual guidance of a moving platform such as a robot.},
 author = {Higgins, Charles and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/69d658d0b2859e32cd4dc3b970c8496c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/69d658d0b2859e32cd4dc3b970c8496c-Metadata.json},
 openalex = {W2126892996},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/69d658d0b2859e32cd4dc3b970c8496c-Paper.pdf},
 publisher = {MIT Press},
 title = {An Integrated Vision Sensor for the Computation of Optical Flow Singular Points},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/69d658d0b2859e32cd4dc3b970c8496c-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_6b8eba43,
 abstract = {Parti-game (Moore 1994a; Moore 1994b; Moore and Atkeson 1995) is a reinforcement learning (RL) algorithm that has a lot of promise in overcoming the curse of dimensionality that can plague RL algorithms when applied to high-dimensional problems. In this paper we introduce modifications to the algorithm that further improve its performance and robustness. In addition, while parti-game solutions can be improved locally by standard local path-improvement techniques, we introduce an add-on algorithm in the same spirit as parti-game that instead tries to improve solutions in a non-local manner.},
 author = {Al-Ansari, Mohammad and Williams, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6b8eba43551742214453411664a0dcc8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6b8eba43551742214453411664a0dcc8-Metadata.json},
 openalex = {W2114054213},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6b8eba43551742214453411664a0dcc8-Paper.pdf},
 publisher = {MIT Press},
 title = {Robust, Efficient, Globally-Optimized Reinforcement Learning with the Parti-Game Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/6b8eba43551742214453411664a0dcc8-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_6ba3af5d,
 abstract = {Many belief networks have been proposed that are composed of binary units. However, for tasks such as object and speech recognition which produce real-valued data, binary network models are usually inadequate. Independent component analysis (ICA) learns a model from real data, but the descriptive power of this model is severly limited. We begin by describing the independent factor analysis (IFA) technique, which overcomes some of the limitations of ICA. We then create a multilayer network by cascading singlelayer IFA models. At each level, the IFA network extracts realvalued latent variables that are non-linear functions of the input data with a highly adaptive functional form, resulting in a hierarchical distributed representation of these data. Whereas exact maximum-likelihood learning of the network is intractable, we derive an algorithm that maximizes a lower bound on the likelihood, based on a variational approach.},
 author = {Attias, Hagai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6ba3af5d7b2790e73f0de32e5c8c1798-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6ba3af5d7b2790e73f0de32e5c8c1798-Metadata.json},
 openalex = {W2161289811},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6ba3af5d7b2790e73f0de32e5c8c1798-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning a Hierarchical Belief Network of Independent Factor Analyzers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/6ba3af5d7b2790e73f0de32e5c8c1798-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_6d3a1e06,
 abstract = {We describe a Reinforcement Learning algorithm for partially observable environments using short-term memory, which we call BLHT. Since BLHT learns a stochastic model based on Bayesian Learning, the over-fitting problem is reasonably solved. Moreover, BLHT has an efficient implementation. This paper shows that the model learned by BLHT converges to one which provides the most accurate predictions of percepts and rewards, given short-term memory.},
 author = {Suematsu, Nobuo and Hayashi, Akira},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6d3a1e06d6a06349436bc054313b648c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6d3a1e06d6a06349436bc054313b648c-Metadata.json},
 openalex = {W2113013121},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6d3a1e06d6a06349436bc054313b648c-Paper.pdf},
 publisher = {MIT Press},
 title = {A Reinforcement Learning Algorithm in Partially Observable Environments Using Short-Term Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/6d3a1e06d6a06349436bc054313b648c-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_6dd4e10e,
 abstract = {We describe Maximum-Likelihood Continuity Mapping (MALCOM), an alternative to Markov models (HMMs) for processing sequence data such as speech. While HMMs have a discrete hidden space constrained by a fixed finite-automaton architecture, MALCOM has a continuous space--a continuity map--that is constrained only by a smoothness requirement on paths through the space. MALCOM fits into the same probabilistic framework for speech recognition as HMMs, but it represents a more realistic model of the speech production process. To evaluate the extent to which MALCOM captures speech production information, we generated continuous speech continuity maps for three speakers and used the paths through them to predict measured speech articulator data. The median correlation between the MALCOM paths obtained from only the speech acoustics and articulator measurements was 0.77 on an independent test set not used to train MALCOM or the predictor. This unsupervised model achieved correlations over speakers and articulators only 0.02 to 0.15 lower than those obtained using an analogous supervised method which used articulatory measurements as well as acoustics.},
 author = {Nix, David and Hogden, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6dd4e10e3296fa63738371ec0d5df818-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6dd4e10e3296fa63738371ec0d5df818-Metadata.json},
 openalex = {W2100315011},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/6dd4e10e3296fa63738371ec0d5df818-Paper.pdf},
 publisher = {MIT Press},
 title = {Maximum-Likelihood Continuity Mapping (MALCOM): An Alternative to HMMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/6dd4e10e3296fa63738371ec0d5df818-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_70efba66,
 abstract = {Semiparametric models are useful tools in the case where domain knowledge exists about the function to be estimated or emphasis is put onto understandability of the model. We extend two learning algorithms - Support Vector machines and Linear Programming machines to this case and give experimental results for SV machines.},
 author = {Smola, Alex and Frie\ss , Thilo-Thomas and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/70efba66d3d8d53194fb1a8446ae07fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/70efba66d3d8d53194fb1a8446ae07fa-Metadata.json},
 openalex = {W2135845071},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/70efba66d3d8d53194fb1a8446ae07fa-Paper.pdf},
 publisher = {MIT Press},
 title = {Semiparametric Support Vector and Linear Programming Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/70efba66d3d8d53194fb1a8446ae07fa-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_71a58e8c,
 abstract = {Boosting methods maximize a hard classification margin and are known as powerful techniques that do not exhibit overfitting for low noise cases. Also for noisy data boosting will try to enforce a hard margin and thereby give too much weight to outliers, which then leads to the dilemma of non-smooth fits and overfitting. Therefore we propose three algorithms to allow for soft margin classification by introducing regularization with slack variables into the boosting concept: (1) AdaBoostreg and regularized versions of (2) linear and (3) quadratic programming AdaBoost. Experiments show the usefulness of the proposed algorithms in comparison to another soft margin classifier: the support vector machine.},
 author = {R\"{a}tsch, Gunnar and Onoda, Takashi and M\"{u}ller, Klaus R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/71a58e8cb75904f24cde464161c3e766-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/71a58e8cb75904f24cde464161c3e766-Metadata.json},
 openalex = {W2293879901},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/71a58e8cb75904f24cde464161c3e766-Paper.pdf},
 publisher = {MIT Press},
 title = {Regularizing AdaBoost},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/71a58e8cb75904f24cde464161c3e766-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_729c6888,
 abstract = {This paper describes a Bayesian graph matching algorithm for data-mining from large structural data-bases. The matching algorithm uses edge-consistency and node attribute similarity to determine the a posteriori probability of a query graph for each of the candidate matches in the data-base. The node feature-vectors are constructed by computing normalised histograms of pairwise geometric attributes. Attribute similarity is assessed by computing the Bhattacharyya distance between the histograms. Recognition is realised by selecting the candidate from the data-base which has the largest a posteriori probability. We illustrate the recognition technique on a data-base containing 2500 line patterns extracted from real-world imagery. Here the recognition technique is shown to significantly outperform a number of algorithm alternatives.},
 author = {Huet, Benoit and Cross, Andrew and Hancock, Edwin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/729c68884bd359ade15d5f163166738a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/729c68884bd359ade15d5f163166738a-Metadata.json},
 openalex = {W2115588952},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/729c68884bd359ade15d5f163166738a-Paper.pdf},
 publisher = {MIT Press},
 title = {Graph Matching for Shape Retrieval},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/729c68884bd359ade15d5f163166738a-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_748ba69d,
 abstract = {In this paper we introduce a new class of image models, which we call dynamic trees or DTs. A dynamic tree model specifies a prior over a large number of trees, each one of which is a tree-structured belief net (TSBN). Experiments show that DTs are capable of generating images that are less blocky, and the models have better translation invariance properties than a fixed, balanced TSBN. We also show that Simulated Annealing is effective at finding trees which have high posterior probability.},
 author = {Williams, Christopher and Adams, Nicholas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/748ba69d3e8d1af87f84fee909eef339-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/748ba69d3e8d1af87f84fee909eef339-Metadata.json},
 openalex = {W2108344269},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/748ba69d3e8d1af87f84fee909eef339-Paper.pdf},
 publisher = {MIT Press},
 title = {DTs: Dynamic Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/748ba69d3e8d1af87f84fee909eef339-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_77f959f1,
 abstract = {Gaussian Processes provide good prior models for spatial data, but can be too smooth. In many physical situations there are discontinuities along bounding surfaces, for example fronts in near-surface wind fields. We describe a modelling method for such a constrained discontinuity and demonstrate how to infer the model parameters in wind fields with MCMC sampling.},
 author = {Cornford, Dan and Nabney, Ian and Williams, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/77f959f119f4fb2321e9ce801e2f5163-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/77f959f119f4fb2321e9ce801e2f5163-Metadata.json},
 openalex = {W2124455576},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/77f959f119f4fb2321e9ce801e2f5163-Paper.pdf},
 publisher = {MIT Press},
 title = {Adding Constrained Discontinuities to Gaussian Process Models of Wind Fields},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/77f959f119f4fb2321e9ce801e2f5163-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_7949e456,
 abstract = {We show the similarity between belief propagation and TAP, for decoding corrupted messages encoded by Sourlas's method. The latter is a special case of the Gallager error-correcting code, where the code word comprises products of K bits selected randomly from the original message. We examine the efficacy of solutions obtained by the two methods for various values of K and show that solutions for K ≥ 3 may be sensitive to the choice of initial conditions in the case of unbiased patterns. Good approximations are obtained generally for K = 2 and for biased patterns in the case of K ≥ 3, especially when Nishimori's temperature is being used.},
 author = {Kabashima, Yoshiyuki and Saad, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7949e456002b28988d38185bd30e77fd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7949e456002b28988d38185bd30e77fd-Metadata.json},
 openalex = {W2106743539},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7949e456002b28988d38185bd30e77fd-Paper.pdf},
 publisher = {MIT Press},
 title = {The Belief in TAP},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/7949e456002b28988d38185bd30e77fd-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_7a6a74cb,
 abstract = {Determining the relationship between the activity of a single nerve cell to that of an entire population is a fundamental question that bears on the basic neural computation paradigms. In this paper we apply an information theoretic approach to quantify the level of cooperative activity among cells in a behavioral context. It is possible to discriminate between synergetic activity of the cells vs. redundant activity, depending on the difference between the information they provide when measured jointly and the information they provide independently. We define a synergy value that is positive in the first case and negative in the second and show that the synergy value can be measured by detecting the behavioral mode of the animal from simultaneously recorded activity of the cells. We observe that among cortical cells positive synergy can be found, while cells from the basal ganglia, active during the same task, do not exhibit similar synergetic activity.},
 author = {Gat, Itay and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7a6a74cbe87bc60030a4bd041dd47b78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7a6a74cbe87bc60030a4bd041dd47b78-Metadata.json},
 openalex = {W2123992910},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7a6a74cbe87bc60030a4bd041dd47b78-Paper.pdf},
 publisher = {MIT Press},
 title = {Synergy and Redundancy among Brain Cells of Behaving Monkeys},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/7a6a74cbe87bc60030a4bd041dd47b78-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_7bd28f15,
 abstract = {We investigate the problem of learning a classification task on data represented in terms of their pairwise proximities. This representation does not refer to an explicit feature representation of the data items and is thus more general than the standard approach of using Euclidean feature vectors, from which pairwise proximities can always be calculated. Our first approach is based on a combined linear embedding and classification procedure resulting in an extension of the Optimal Hyperplane algorithm to pseudo-Euclidean data. As an alternative we present another approach based on a linear threshold model in the proximity values themselves, which is optimized using Structural Risk Minimization. We show that prior knowledge about the problem can be incorporated by the choice of distance measures and examine different metrics W.r.t. their generalization. Finally, the algorithms are successfully applied to protein structure data and to data from the cat's cerebral cortex. They show better performance than K-nearest-neighbor classification.},
 author = {Graepel, Thore and Herbrich, Ralf and Bollmann-Sdorra, Peter and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7bd28f15a49d5e5848d6ec70e584e625-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7bd28f15a49d5e5848d6ec70e584e625-Metadata.json},
 openalex = {W2118216287},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7bd28f15a49d5e5848d6ec70e584e625-Paper.pdf},
 publisher = {MIT Press},
 title = {Classification on Pairwise Proximity Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/7bd28f15a49d5e5848d6ec70e584e625-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_7e1d842d,
 abstract = {Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) problem. This paper proposes an algorithm for training SVMs: Sequential Minimal Optimization, or SMO. SMO breaks the large QP problem into a series of smallest possible QP problems which are analytically solvable. Thus, SMO does not require a numerical QP library. SMO's computation time is dominated by evaluation of the kernel, hence kernel optimizations substantially quicken SMO. For the MNIST database, SMO is 1.7 times as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be 1500 times faster than the PCG chunking algorithm.},
 author = {Platt, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7e1d842d0f7ee600116ffc6b2d87d83f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7e1d842d0f7ee600116ffc6b2d87d83f-Metadata.json},
 openalex = {W2105038653},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7e1d842d0f7ee600116ffc6b2d87d83f-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Analytic QP and Sparseness to Speed Training of Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/7e1d842d0f7ee600116ffc6b2d87d83f-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_7fb8ceb3,
 abstract = {The kernel-parameter is one of the few tunable parameters in Support Vector machines, controlling the complexity of the resulting hypothesis. Its choice amounts to model selection and its value is usually found by means of a validation set. We present an algorithm which can automatically perform model selection with little additional computational cost and with no need of a validation set. In this procedure model selection and learning are not separate, but kernels are dynamically adjusted during the learning process to find the kernel parameter which provides the best possible upper bound on the generalisation error. Theoretical results motivating the approach and experimental results confirming its validity are presented.},
 author = {Cristianini, Nello and Campbell, Colin and Shawe-Taylor, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7fb8ceb3bd59c7956b1df66729296a4c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7fb8ceb3bd59c7956b1df66729296a4c-Metadata.json},
 openalex = {W2157083019},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamically Adapting Kernels in Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/7fb8ceb3bd59c7956b1df66729296a4c-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_806beafe,
 abstract = {Recent experimental data indicate that the strengthening or weakening of synaptic connections between neurons depends on the relative timing of pre- and postsynaptic action potentials. A Hebbian synaptic modification rule based on these data leads to a stable state in which the excitatory and inhibitory inputs to a neuron are balanced, producing an irregular pattern of firing. It has been proposed that neurons in vivo operate in such a mode.},
 author = {Abbott, L. and Song, Sen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/806beafe154032a5b818e97b4420ad98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/806beafe154032a5b818e97b4420ad98-Metadata.json},
 openalex = {W2162652458},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/806beafe154032a5b818e97b4420ad98-Paper.pdf},
 publisher = {MIT Press},
 title = {Temporally Asymmetric Hebbian Learning, Spike liming and Neural Response Variability},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/806beafe154032a5b818e97b4420ad98-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_819c9fbf,
 abstract = {The hierarchical representation of data has various applications in domains such as data mining, machine vision, or information retrieval. In this paper we introduce an extension of the Expectation-Maximization (EM) algorithm that learns mixture hierarchies in a computationally efficient manner. Efficiency is achieved by progressing in a bottom-up fashion, i.e. by clustering the mixture components of a given level in the hierarchy to obtain those of the level above. This cl ustering requires only knowledge of the mixture parameters, there being no need to resort to intermediate samples. In addition to practical applications, the algorithm allows a new interpretation of EM that makes clear the relationship with non-parametric kernel-based estimation methods, provides explicit control over the trade-off between the bias and variance of EM estimates, and offers new insights about the behavior of deterministic annealing methods commonly used with EM to escape local minima of the likelihood.},
 author = {Vasconcelos, Nuno and Lippman, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/819c9fbfb075d62a16393b9fe4fcbaa5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/819c9fbfb075d62a16393b9fe4fcbaa5-Metadata.json},
 openalex = {W2160560153},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/819c9fbfb075d62a16393b9fe4fcbaa5-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Mixture Hierarchies},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/819c9fbfb075d62a16393b9fe4fcbaa5-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_81c8727c,
 abstract = {We study the approximation of functions by two-layer feedforward neural networks, focusing on incremental algorithms which greedily add units, estimating single unit parameters at each stage. As opposed to standard algorithms for fixed architectures, the optimization at each stage is performed over a small number of parameters, mitigating many of the difficult numerical problems inherent in high-dimensional non-linear optimization. We establish upper bounds on the error incurred by the algorithm, when approximating functions from the Sobolev class, thereby extending previous results which only provided rates of convergence for functions in certain convex hulls of functional spaces. By comparing our results to recently derived lower bounds, we show that the greedy algorithms are nearly optimal. Combined with estimation error results for greedy algorithms, a strong case can be made for this type of approach.},
 author = {Meir, Ron and Maiorov, Vitaly},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/81c8727c62e800be708dbf37c4695dff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/81c8727c62e800be708dbf37c4695dff-Metadata.json},
 openalex = {W2099479398},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/81c8727c62e800be708dbf37c4695dff-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Optimality of Incremental Neural Network Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/81c8727c62e800be708dbf37c4695dff-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_83e8ef51,
 abstract = {This paper examines the application of reinforcement learning to a telecommunications networking problem. The problem requires that revenue be maximized while simultaneously meeting a quality of service constraint that forbids entry into certain states. We present a general solution to this multi-criteria problem that is able to earn significantly higher revenues than alternatives.},
 author = {Brown, Timothy and Tong, Hui and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/83e8ef518174e1eb6be4a0778d050c9d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/83e8ef518174e1eb6be4a0778d050c9d-Metadata.json},
 openalex = {W2167790374},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/83e8ef518174e1eb6be4a0778d050c9d-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimizing Admission Control while Ensuring Quality of Service in Multimedia Networks via Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/83e8ef518174e1eb6be4a0778d050c9d-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_83f25503,
 abstract = {A common way to represent a time series is to divide it into short-duration blocks, each of which is then represented by a set of basis functions. A limitation of this approach, however, is that the temporal alignment of the basis functions with the underlying structure in the time series is arbitrary. We present an algorithm for encoding a time series that does not require blocking the data. The algorithm finds an efficient representation by inferring the best temporal positions for functions in a kernel basis. These can have arbitrary temporal extent and are not constrained to be orthogonal. This allows the model to capture structure in the signal that may occur at arbitrary temporal positions and preserves the relative temporal structure of underlying events. The model is shown to be equivalent to a very sparse and highly overcomplete basis. Under this model, the mapping from the data to the representation is nonlinear, but can be computed efficiently. This form also allows the use of existing methods for adapting the basis itself to data. This approach is applied to speech data and results in a shift invariant, spikelike representation that resembles coding in the cochlear nerve.},
 author = {Lewicki, Michael and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/83f2550373f2f19492aa30fbd5b57512-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/83f2550373f2f19492aa30fbd5b57512-Metadata.json},
 openalex = {W2117289172},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/83f2550373f2f19492aa30fbd5b57512-Paper.pdf},
 publisher = {MIT Press},
 title = {Coding Time-Varying Signals Using Sparse, Shift-Invariant Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/83f2550373f2f19492aa30fbd5b57512-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_86df7dcf,
 abstract = {Based on computational principles, the concept of an internal model for adaptive control has been divided into a forward and an inverse model. However, there is as yet little evidence that learning control by the CNS is through adaptation of one or the other. Here we examine two adaptive control architectures, one based only on the inverse model and other based on a combination of forward and inverse models. We then show that for reaching movements of the hand in novel force fields, only the learning of the forward model results in key characteristics of performance that match the kinematics of human subjects. In contrast, the adaptive control system that relies only on the inverse model fails to produce the kinematic patterns observed in the subjects, despite the fact that it is more stable. Our results provide evidence that learning control of novel dynamics is via formation of a forward model.},
 author = {Bhushan, Nikhil and Shadmehr, Reza},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/86df7dcfd896fcaf2674f757a2463eba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/86df7dcfd896fcaf2674f757a2463eba-Metadata.json},
 openalex = {W2150705094},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf},
 publisher = {MIT Press},
 title = {Evidence for a Forward Dynamics Model in Human Adaptive Motor Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_87ec2f45,
 abstract = {The task in text retrieval is to find the subset of a collection of documents relevant to a user's information request, usually expressed as a set of words. Classically, documents and queries are represented as vectors of word counts. In its simplest form, relevance is defined to be the dot product between a document and a query vector-a measure of the number of common terms. A central difficulty in text retrieval is that the presence or absence of a word is not sufficient to determine relevance to a query. Linear dimensionality reduction has been proposed as a technique for extracting underlying structure from the document collection. In some domains (such as vision) dimensionality reduction reduces computational complexity. In text retrieval it is more often used to improve retrieval performance. We propose an alternative and novel technique that produces sparse representations constructed from sets of highly-related words. Documents and queries are represented by their distance to these sets, and relevance is measured by the number of common clusters. This technique significantly improves retrieval performance, is efficient to compute and shares properties with the optimal linear projection operator and the independent components of documents.},
 author = {Isbell, Charles and Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/87ec2f451208df97228105657edb717f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/87ec2f451208df97228105657edb717f-Metadata.json},
 openalex = {W2155602043},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/87ec2f451208df97228105657edb717f-Paper.pdf},
 publisher = {MIT Press},
 title = {Restructuring Sparse High Dimensional Data for Effective Retrieval},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/87ec2f451208df97228105657edb717f-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_88a19961,
 abstract = {A key question in vision is how to represent our knowledge of previously encountered objects to classify new ones. The answer depends on how we determine the similarity of two objects. Similarity tells us how relevant each previously seen object is in determining the category to which a new object belongs. Here a dichotomy emerges. Complex notions of similarity appear necessary for cognitive models and applications, while simple notions of similarity form a tractable basis for current computational approaches to classification. We explore the nature of this dichotomy and why it calls for new approaches to well-studied problems in learning. We begin this process by demonstrating new computational methods for supervised learning that can handle complex notions of similarity. (1) We discuss how to implement parametric methods that represent a class by its mean when using non-metric similarity functions; and (2) We review non-parametric methods that we have developed using nearest neighbor classification in non-metric spaces. Point (2), and some of the background of our work have been described in more detail in [8].},
 author = {Weinshall, Daphna and Jacobs, David and Gdalyahu, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/88a199611ac2b85bd3f76e8ee7e55650-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/88a199611ac2b85bd3f76e8ee7e55650-Metadata.json},
 openalex = {W2131473842},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/88a199611ac2b85bd3f76e8ee7e55650-Paper.pdf},
 publisher = {MIT Press},
 title = {Classification in Non-Metric Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/88a199611ac2b85bd3f76e8ee7e55650-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_894b77f8,
 abstract = {Inference is a key component in learning probabilistic models from partially observable data. When learning temporal models, each of the many inference phases requires a traversal over an entire long data sequence; furthermore, the data structures manipulated are exponentially large, making this process computationally expensive. In [2], we describe an approximate inference algorithm for monitoring stochastic processes, and prove bounds on its approximation error. In this paper, we apply this algorithm as an approximate forward propagation step in an EM algorithm for learning temporal Bayesian networks. We provide a related approximation for the backward step, and prove error bounds for the combined algorithm. We show empirically that, for a real-life domain, EM using our inference algorithm is much faster than EM using exact inference, with almost no degradation in quality of the learned model. We extend our analysis to the online learning task, showing a bound on the error resulting from restricting attention to a small window of observations. We present an online EM learning algorithm for dynamic systems, and show that it learns much faster than standard offline EM.},
 author = {Boyen, Xavier and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/894b77f805bd94d292574c38c5d628d5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/894b77f805bd94d292574c38c5d628d5-Metadata.json},
 openalex = {W2132588110},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/894b77f805bd94d292574c38c5d628d5-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximate Learning of Dynamic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/894b77f805bd94d292574c38c5d628d5-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_89ae0fe2,
 abstract = {Calcium (Ca2+)is an ubiquitous intracellular messenger which regulates cellular processes, such as secretion, contraction, and cell proliferation. A number of different cell types respond to hormonal stimuli with periodic oscillations of the intracellular free calcium concentration ([Ca2+]i). These Ca2+ signals are often organized in complex temporal and spatial patterns even under conditions of sustained stimulation. Here we study the spatio-temporal aspects of intracellular calcium ([Ca2+]i) oscillations in clonal β-cells (hamster insulin secreting cells, HIT) under pharmacological stimulation (Schofl et al., 1996). We use a novel fast fixed-point algorithm (Hyvarinen and Oja, 1997) for Independent Component Analysis (ICA) to blind source separation of the spatio-temporal dynamics of [Ca2+]i in a HIT-cell. Using this approach we find two significant independent components out of five differently mixed input signals: one [Ca2+]i signal with a mean oscillatory period of 68s and a high frequency signal with a broadband power spectrum with considerable spectral density. This results is in good agreement with a study on high-frequency [Ca2+]i oscillations (Palus et al., 1998) Further theoretical and experimental studies have to be performed to resolve the question on the functional impact of intracellular signaling of these independent [Ca2+]i signals.},
 author = {Prank, Klaus and B\"{o}rger, Julia and von zur M\"{u}hlen, Alexander and Brabant, Georg and Sch\"{o}fl, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/89ae0fe22c47d374bc9350ef99e01685-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/89ae0fe22c47d374bc9350ef99e01685-Metadata.json},
 openalex = {W2142400845},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/89ae0fe22c47d374bc9350ef99e01685-Paper.pdf},
 publisher = {MIT Press},
 title = {Independent Component Analysis of Intracellular Calcium Spike Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/89ae0fe22c47d374bc9350ef99e01685-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_8a146f1a,
 author = {Christianson, G. and Becker, Suzanna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/8a146f1a3da4700cbf03cdc55e2daae6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/8a146f1a3da4700cbf03cdc55e2daae6-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/8a146f1a3da4700cbf03cdc55e2daae6-Paper.pdf},
 publisher = {MIT Press},
 title = {A Model for Associative Multiplication},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/8a146f1a3da4700cbf03cdc55e2daae6-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_8c00dee2,
 abstract = {We describe a new iterative method for parameter estimation of Gaussian mixtures. The new method is based on a framework developed by Kivinen and Warmuth for supervised on-line learning. In contrast to gradient descent and EM, which estimate the mixture's covariance matrices, the proposed method estimates the inverses of the covariance matrices. Furthennore, the new parameter estimation procedure can be applied in both on-line and batch settings. We show experimentally that it is typically faster than EM, and usually requires about half as many iterations as EM.},
 author = {Singer, Yoram and Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/8c00dee24c9878fea090ed070b44f1ab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/8c00dee24c9878fea090ed070b44f1ab-Metadata.json},
 openalex = {W2125145210},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/8c00dee24c9878fea090ed070b44f1ab-Paper.pdf},
 publisher = {MIT Press},
 title = {Batch and On-Line Parameter Estimation of Gaussian Mixtures Based on the Joint Entropy},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/8c00dee24c9878fea090ed070b44f1ab-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_8f19793b,
 abstract = {We present a method for automatically constructing macro-actions from scratch from primitive actions during the reinforcement learning process. The overall idea is to reinforce the tendency to perform action b after action a if such a pattern of actions has been rewarded. We test the method on a bicycle task, the car-on-the-hill task, the race-track task and some grid-world tasks. For the bicycle and race-track tasks the use of macro-actions approximately halves the learning time, while for one of the grid-world tasks the learning time is reduced by a factor of 5. The method did not work for the car-on-the-hill task for reasons we discuss in the conclusion.},
 author = {Randlov, Jette},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/8f19793b2671094e63a15ab883d50137-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/8f19793b2671094e63a15ab883d50137-Metadata.json},
 openalex = {W2106688224},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/8f19793b2671094e63a15ab883d50137-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Macro-Actions in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/8f19793b2671094e63a15ab883d50137-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_92af93f7,
 abstract = {Computer animation through the numerical simulation of physics-based graphics models offers unsurpassed realism, but it can be computationally demanding. This paper demonstrates the possibility of replacing the numerical simulation of nontrivial dynamic models with a dramatically more efficient NeuroAnimator that exploits neural networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physics-based models in action. Depending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. We demonstrate NeuroAnimators for a variety of physics-based models.},
 author = {Grzeszczuk, Radek and Terzopoulos, Demetri and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/92af93f73faf3cefc129b6bc55a748a9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/92af93f73faf3cefc129b6bc55a748a9-Metadata.json},
 openalex = {W2166885679},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/92af93f73faf3cefc129b6bc55a748a9-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Neural Network Emulation of Dynamical Systems for Computer Animation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/92af93f73faf3cefc129b6bc55a748a9-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_93279690,
 abstract = {We introduce two new techniques for density estimation. Our approach poses the problem as a supervised learning task which can be performed using Neural Networks. We introduce a stochastic method for learning the cumulative distribution and an analogous deterministic technique. We demonstrate convergence of our methods both theoretically and experimentally, and provide comparisons with the Parzen estimate. Our theoretical results demonstrate better convergence properties than the Parzen estimate.},
 author = {Magdon-Ismail, Malik and Atiya, Amir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9327969053c0068dd9e07c529866b94d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9327969053c0068dd9e07c529866b94d-Metadata.json},
 openalex = {W2114858870},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9327969053c0068dd9e07c529866b94d-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Networks for Density Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/9327969053c0068dd9e07c529866b94d-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_9597353e,
 abstract = {In robotics and other control applications it is commonplace to have a preexisting set of controllers for solving subtasks, perhaps hand-crafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible. In this paper we present a framework based on Markov decision processes and semi-Markov decision processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem. In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers, with negligible additional cost and no re-planning. In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pairs.},
 author = {Sutton, Richard S and Singh, Satinder and Precup, Doina and Ravindran, Balaraman},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9597353e41e6957b5e7aa79214fcb256-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9597353e41e6957b5e7aa79214fcb256-Metadata.json},
 openalex = {W2123956999},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9597353e41e6957b5e7aa79214fcb256-Paper.pdf},
 publisher = {MIT Press},
 title = {Improved Switching among Temporally Abstract Actions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/9597353e41e6957b5e7aa79214fcb256-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_95d309f0,
 abstract = {We study the dynamics of supervised learning in layered neural networks, in the regime where the size $p$ of the training set is proportional to the number $N$ of inputs. Here the local fields are no longer described by Gaussian probability distributions. We show how dynamical replica theory can be used to predict the evolution of macroscopic observables, including the relevant performance measures, incorporating the old formalism in the limit $\alpha=p/N\to\infty$ as a special case. For simplicity we restrict ourselves to single-layer networks and realizable tasks.},
 author = {Coolen, Anthony and Saad, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/95d309f0b035d97f69902e7972c2b2e6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/95d309f0b035d97f69902e7972c2b2e6-Metadata.json},
 openalex = {W2950895772},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/95d309f0b035d97f69902e7972c2b2e6-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamics of Supervised Learning with Restricted Training Sets},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/95d309f0b035d97f69902e7972c2b2e6-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_962e56a8,
 abstract = {We examine the problem of estimating the parameters of a multinomial distribution over a large number of discrete outcomes, most of which do not appear in the training data. We analyze this problem from a Bayesian perspective and develop a hierarchical prior that incorporates the assumption that the observed outcomes constitute only a small subset of the possible outcomes. We show how to efficiently perform exact inference with this form of hierarchical prior and compare it to standard approaches.},
 author = {Friedman, Nir and Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/962e56a8a0b0420d87272a682bfd1e53-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/962e56a8a0b0420d87272a682bfd1e53-Metadata.json},
 openalex = {W2113958614},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/962e56a8a0b0420d87272a682bfd1e53-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient Bayesian Parameter Estimation in Large Discrete Domains},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/962e56a8a0b0420d87272a682bfd1e53-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_97d01458,
 abstract = {Principal curves have been defined as self consistent smooth curves which pass through the middle of a d-dimensional probability distribution or data cloud. Recently, we [1] have offered a new approach by defining principal curves as continuous curves of a given length which minimize the expected squared distance between the curve and points of the space randomly chosen according to a given distribution. The new definition made it possible to carry out a theoretical analysis of learning principal curves from training data. In this paper we propose a practical construction based on the new definition. Simulation results demonstrate that the new algorithm compares favorably with previous methods both in terms of performance and computational complexity.},
 author = {K\'{e}gl, Bal\'{a}zs and Krzyzak, Adam and Linder, Tam\'{a}s and Zeger, Kenneth},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/97d0145823aeb8ed80617be62e08bdcc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/97d0145823aeb8ed80617be62e08bdcc-Metadata.json},
 openalex = {W2134781447},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/97d0145823aeb8ed80617be62e08bdcc-Paper.pdf},
 publisher = {MIT Press},
 title = {A Polygonal Line Algorithm for Constructing Principal Curves},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/97d0145823aeb8ed80617be62e08bdcc-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_98986c00,
 abstract = {Human and animal studies show that mammalian brain undergoes massive synaptic pruning during childhood, removing about half of the synapses until puberty. We have previously shown that maintaining network memory performance while synapses are deleted, requires that synapses are properly modified and pruned, removing the weaker synapses. We now show that neuronal regulation, a mechanism recently observed to maintain the average neuronal input field, results in weight-dependent synaptic modification. Under the correct range of the degradation dimension and synaptic upper bound, neuronal regulation removes the weaker synapses and judiciously modifies the remaining synapses. It implements near optimal synaptic modification, and maintains the memory performance of a network undergoing massive synaptic pruning. Thus, this paper shows that in addition to the known effects of Hebbian changes, neuronal regulation may play an important role in the self-organization of brain networks during development.},
 author = {Chechik, Gal and Meilijson, Isaac and Ruppin, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/98986c005e5def2da341b4e0627d4712-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/98986c005e5def2da341b4e0627d4712-Metadata.json},
 openalex = {W2117281029},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/98986c005e5def2da341b4e0627d4712-Paper.pdf},
 publisher = {MIT Press},
 title = {Neuronal Regulation Implements Efficient Synaptic Pruning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/98986c005e5def2da341b4e0627d4712-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_99adff45,
 abstract = {In this paper, we address two issues of long-standing interest in the reinforcement learning literature. First, what kinds of performance guarantees can be made for Q-learning after only a finite number of actions? Second, what quantitative comparisons can be made between Q-learning and model-based (indirect) approaches, which use experience to estimate next-state distributions for off-line value iteration?

We first show that both Q-learning and the indirect approach enjoy rather rapid convergence to the optimal policy as a function of the number of state transitions observed. In particular, on the order of only (N log(1/e)/e2)(log(N) + loglog(l/e)) transitions are sufficient for both algorithms to come within e of the optimal policy, in an idealized model that assumes the observed transitions are well-mixed throughout an N-state MDP. Thus, the two approaches have roughly the same sample complexity. Perhaps surprisingly, this sample complexity is far less than what is required for the model-based approach to actually construct a good approximation to the next-state distribution. The result also shows that the amount of memory required by the model-based approach is closer to N than to N2.

For either approach, to remove the assumption that the observed transitions are well-mixed, we consider a model in which the transitions are determined by a fixed, arbitrary exploration policy. Bounds on the number of transitions required in order to achieve a desired level of performance are then related to the stationary distribution and mixing time of this policy.},
 author = {Kearns, Michael and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/99adff456950dd9629a5260c4de21858-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/99adff456950dd9629a5260c4de21858-Metadata.json},
 openalex = {W2122701159},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/99adff456950dd9629a5260c4de21858-Paper.pdf},
 publisher = {MIT Press},
 title = {Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/99adff456950dd9629a5260c4de21858-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_9c19a2aa,
 abstract = {We propose a novel strategy for training neural networks using sequential Monte Carlo algorithms. This global optimisation strategy allows us to learn the probability distribution of the network weights in a sequential framework. It is well suited to applications involving on-line, nonlinear or non-stationary signal processing. We show how the new algorithms can outperform extended Kalman filter (EKF) training.},
 author = {de Freitas, Jo\~{a}o and Niranjan, Mahesan and Doucet, Arnaud and Gee, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9c19a2aa1d84e04b0bd4bc888792bd1e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9c19a2aa1d84e04b0bd4bc888792bd1e-Metadata.json},
 openalex = {W1529632851},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9c19a2aa1d84e04b0bd4bc888792bd1e-Paper.pdf},
 publisher = {MIT Press},
 title = {Global optimisation of neural network models via sequential sampling-importance resampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/9c19a2aa1d84e04b0bd4bc888792bd1e-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_9e984c10,
 abstract = {A non-linear modification to PI control has been developed which is appropriate for plants requiring exact set-point matching and disturbance attenuation in the presence of infrequent step changes in load disturbances or set-point. This novel control algorithm, labeled PII (proportional with intermittent integral), is motivated by a model of a signal transduction pathway active in mammalian blood pressure regulation. It is possible that set-point regulation and disturbance rejection are achieved through independent control loops in mammals. This control paradigm implements, somewhat independently, two control loops to achieve the goals of disturbance attenuation and set-point matching. By developing a disturbance rejection controller with no consideration of set-point matching, it is possible to design for greater disturbance attenuation. As a first step, the disturbance attenuator is a simple proportional controller, and set-point matching will be achieved by intermittently invoking an integral controller. The mechanisms observed in a neuronal signal transduction model are used to tie these controllers together. Superior performance of this approach as compared to traditional PI control has been shown on models of level control loops and cyclopentenol production in a CSTR reactor. The cyclopentenol process contains a maximum steady-state production level. PI control cannot be used at this peak operating point as the sign change in plant gain results in an unstable closed-loop system. Application of this new approach to this problem results in stable exact setpoint matching for achievable set-points.},
 author = {Brown, Lyndon and Gonye, Gregory and Schwaber, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9e984c108157cea74c894b5cf34efc44-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9e984c108157cea74c894b5cf34efc44-Metadata.json},
 openalex = {W1663438825},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/9e984c108157cea74c894b5cf34efc44-Paper.pdf},
 publisher = {MIT Press},
 title = {Non-linear PI control inspired by biological control systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/9e984c108157cea74c894b5cf34efc44-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_a14ac55a,
 abstract = {We describe a unifying method for proving relative loss bounds for online threshold classification algorithms, such as the Perceptron and the Winnow algorithms. For classification problems the discrete loss is used, i.e., the total number of prediction mistakes. We introduce a continuous loss function, called the linear hinge loss, that can be employed to derive the updates of the algorithms. We first prove bounds w.r.t. the hinge loss and then convert them to the discrete loss. We introduce a notion of margin of a set of examples. We show how relative loss bounds based on the hinge loss can be converted to relative loss bounds i.t.o. the discrete loss using the average margin.},
 author = {Gentile, Claudio and Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a14ac55a4f27472c5d894ec1c3c743d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a14ac55a4f27472c5d894ec1c3c743d2-Metadata.json},
 openalex = {W2163046677},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 publisher = {MIT Press},
 title = {Linear Hinge Loss and Average Margin},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_a1afc58c,
 abstract = {Reinforcement learning methods can be used to improve the performance of local search algorithms for combinatorial optimization by learning an evaluation function that predicts the outcome of search. The evaluation function is therefore able to guide search to low-cost solutions better than can the original cost function. We describe a reinforcement learning method for enhancing local search that combines aspects of previous work by Zhang and Dietterich (1995) and Boyan and Moore (1997, Boyan 1998). In an off-line learning phase, a value function is learned that is useful for guiding search for multiple problem sizes and instances. We illustrate our technique by developing several such functions for the Dial-A-Ride Problem. Our learning-enhanced local search algorithm exhibits an improvement of more then 30% over a standard local search algorithm.},
 author = {Moll, Robert and Barto, Andrew and Perkins, Theodore and Sutton, Richard S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a1afc58c6ca9540d057299ec3016d726-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a1afc58c6ca9540d057299ec3016d726-Metadata.json},
 openalex = {W2150795121},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Instance-Independent Value Functions to Enhance Local Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/a1afc58c6ca9540d057299ec3016d726-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_a2cc63e0,
 abstract = {Face recognition is a K class problem, where K is the number of known individuals; and support vector machines (SVMs) are a binary classification method.By reformulating the face recognition problem and reinterpreting the output of the SVM classifier, we developed a SVM-based face recognition algorithm.The face recognition problem is formulated as a problem in difference space, which models dissimilarities between two facial images.In difference space we formulate face recognition as a two class problem.The classes are; dissimilarities between faces of the same person, and dissimilarities between faces of different people.By modifying the interpretation of the decision surface generated by SVM, we generated a similarity metric between faces that is learned from examples of differences between faces.The SVM-based algorithm is compared with a principal component analysis (PCA) based algorithm on a difficult set of images from the FERET database.Performance was measured for both verification and identification scenarios.The identification performance for SVM is 77-78% versus 54% for PCA.For verification, the equal error rate is 7% for SVM and 13% for PCA. 1},
 author = {Phillips, P.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a2cc63e065705fe938a4dda49092966f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a2cc63e065705fe938a4dda49092966f-Metadata.json},
 openalex = {W2103971661},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a2cc63e065705fe938a4dda49092966f-Paper.pdf},
 publisher = {MIT Press},
 title = {Support vector machines applied to face recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/a2cc63e065705fe938a4dda49092966f-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_a368b0de,
 abstract = {I present a theory of mean field approximation based on information geometry. This theory includes in a consistent way the naive mean field approximation, as well as the TAP approach and the linear response theorem in statistical physics, giving clear information-theoretic interpretations to them.},
 author = {Tanaka, Toshiyuki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a368b0de8b91cfb3f91892fbf1ebd4b2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a368b0de8b91cfb3f91892fbf1ebd4b2-Metadata.json},
 openalex = {W2172158897},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a368b0de8b91cfb3f91892fbf1ebd4b2-Paper.pdf},
 publisher = {MIT Press},
 title = {A Theory of Mean Field Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/a368b0de8b91cfb3f91892fbf1ebd4b2-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_a981f2b7,
 abstract = {Recent works in parameter estimation and neural coding have demonstrated that optimal performance are related to the mutual information between parameters and data. We consider the mutual information in the case where the dependency in the parameter (a vector θ) of the conditional p.d.f. of each observation (a vector ξ, is through the scalar product θξ only. We derive bounds and asymptotic behaviour for the mutual information and compare with results obtained on the same model with the replica technique.},
 author = {Herschkowitz, Didier and Nadal, Jean-Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a981f2b708044d6fb4a71a1463242520-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a981f2b708044d6fb4a71a1463242520-Metadata.json},
 openalex = {W2096497636},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf},
 publisher = {MIT Press},
 title = {Unsupervised and Supervised Clustering: The Mutual Information between Parameters and Observations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/a981f2b708044d6fb4a71a1463242520-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_aa486f25,
 abstract = {We analyze the asymptotic behavior of autoregressive neural network (AR-NN) processes using techniques from Markov chains and non-linear time series analysis. It is shown that standard AR-NNs without shortcut connections are asymptotically stationary. If linear shortcut connections are allowed, only the shortcut weights determine whether the overall system is stationary, hence standard conditions for linear AR processes can be used.},
 author = {Leisch, Friedrich and Trapletti, Adrian and Hornik, Kurt},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/aa486f25175cbdc3854151288a645c19-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/aa486f25175cbdc3854151288a645c19-Metadata.json},
 openalex = {W2101417051},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/aa486f25175cbdc3854151288a645c19-Paper.pdf},
 publisher = {MIT Press},
 title = {Stationarity and Stability of Autoregressive Neural Network Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/aa486f25175cbdc3854151288a645c19-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_ab541d87,
 abstract = {Structure in a visual scene can be described at many levels of granularity. At a coarse level, the scene is composed of objects; at a finer level, each object is made up of parts, and the parts of subparts. In this work, I propose a simple principle by which such hierarchical structure can be extracted from visual scenes: Regularity in the relations among different parts of an object is weaker than in the internal structure of a part. This principle can be applied recursively to define part-whole relationships among elements in a scene. The principle does not make use of object models, categories, or other sorts of higher-level knowledge; rather, part-whole relationships can be established based on the statistics of a set of sample visual scenes. I illustrate with a model that performs unsupervised decomposition of simple scenes. The model can account for the results from a human learning experiment on the ontogeny of part-whole relationships.},
 author = {Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ab541d874c7bc19ab77642849e02b89f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ab541d874c7bc19ab77642849e02b89f-Metadata.json},
 openalex = {W2155578289},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Principle for Unsupervised Hierarchical Decomposition of Visual Scenes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/ab541d874c7bc19ab77642849e02b89f-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_af5afd7f,
 abstract = {A simple learning rule is derived, the VAPS algorithm, which can be instantiated to generate a wide range of new reinforcement-learning algorithms. These algorithms solve a number of open problems, define several new approaches to reinforcement learning, and unify different approaches to reinforcement learning under a single theory. These algorithms all have guaranteed convergence, and include modifications of several existing algorithms that were known to fail to converge on simple MOPs. These include Q-learning, SARSA, and advantage learning. In addition to these value-based algorithms it also generates pure policy-search reinforcement-learning algorithms, which learn optimal policies without learning a value function. In addition, it allows policy-search and value-based algorithms to be combined, thus unifying two very different approaches to reinforcement learning into a single Value and Policy Search (VAPS) algorithm. And these algorithms converge for POMDPs without requiring a proper belief state. Simulations results are given, and several areas for future research are discussed.},
 author = {Baird, Leemon and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/af5afd7f7c807171981d443ad4f4f648-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/af5afd7f7c807171981d443ad4f4f648-Metadata.json},
 openalex = {W2164056559},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/af5afd7f7c807171981d443ad4f4f648-Paper.pdf},
 publisher = {MIT Press},
 title = {Gradient Descent for General Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/af5afd7f7c807171981d443ad4f4f648-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_aff0a6a4,
 abstract = {In High Energy Physics experiments one has to sort through a high flux of events, at a rate of tens of MHz, and select the few that are of interest. One of the key factors in making this decision is the location of the vertex where the interaction, that led to the event, took place. Here we present a novel solution to the problem of finding the location of the vertex, based on two feedforward neural networks with fixed architectures, whose parameters are chosen so as to obtain a high accuracy. The system is tested on simulated data sets, and is shown to perform better than conventional algorithms.},
 author = {Dror, Gideon and Abramowicz, Halina and Horn, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/aff0a6a4521232970b2c1cf539ad0a19-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/aff0a6a4521232970b2c1cf539ad0a19-Metadata.json},
 openalex = {W2130224879},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/aff0a6a4521232970b2c1cf539ad0a19-Paper.pdf},
 publisher = {MIT Press},
 title = {Vertex Identification in High Energy Physics Experiments},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/aff0a6a4521232970b2c1cf539ad0a19-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_b132ecc1,
 abstract = {We present a probabilistic latent-variable framework for data visualisation, a key feature of which is its applicability to binary and categorical data types for which few established methods exist. A variational approximation to the likelihood is exploited to derive a fast algorithm for determining the model parameters. Illustrations of application to real and synthetic binary data sets are given.},
 author = {Tipping, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b132ecc1609bfcf302615847c1caa69a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b132ecc1609bfcf302615847c1caa69a-Metadata.json},
 openalex = {W2135561481},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b132ecc1609bfcf302615847c1caa69a-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Visualisation of High-Dimensional Binary Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/b132ecc1609bfcf302615847c1caa69a-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_b2dd1403,
 abstract = {In 1986, Tanner and Mead [1] implemented an interesting constraint satisfaction circuit for global motion sensing in a VLSI. We report here a new and improved a VLSI implementation that provides smooth optical flow as well as global motion in a two dimensional visual field. The computation of optical flow is an ill-posed problem, which expresses itself as the aperture problem. However, the optical flow can be estimated by the use of regularization methods, in which additional constraints are introduced in terms of a global energy functional that must be minimized . We show how the algorithmic constraints of Hom and Schunck [2] on computing smooth optical flow can be mapped onto the physical constraints of an equivalent electronic network.},
 author = {Stocker, Alan A and Douglas, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b2dd140336c9df867c087a29b2e66034-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b2dd140336c9df867c087a29b2e66034-Metadata.json},
 openalex = {W2117543255},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b2dd140336c9df867c087a29b2e66034-Paper.pdf},
 publisher = {MIT Press},
 title = {Computation of Smooth Optical Flow in a Feedback Connected Analog Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/b2dd140336c9df867c087a29b2e66034-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_b5488aef,
 abstract = {We have previously presented a coarse-to-fine hierarchical pyramid/ neural network (HPNN) architecture which combines multi-scale image processing techniques with neural networks. In this paper we present applications of this general architecture to two problems in mammographic Computer-Aided Diagnosis (CAD). The first application is the detection of microcalcifications. The coarse-to-fine HPNN was designed to learn large-scale context information for detecting small objects like microcalcifications. Receiver operating characteristic (ROC) analysis suggests that the hierarchical architecture improves detection performance of a well established CAD system by roughly 50%. The second application is to detect mammographic masses directly. Since masses are large, extended objects, the coarse-to-fine HPNN architecture is not suitable for this problem. Instead we construct a fine-to-coarse HPNN architecture which is designed to learn small-scale detail structure associated with the extended objects. Our initial results applying the fine-to-coarse HPNN to mass detection are encouraging, with detection performance improvements of about 36%. We conclude that the ability of the HPNN architecture to integrate information across scales, both coarse-to-fine and fine-to-coarse, makes it well suited for detecting objects which may have contextual clues or detail structure occurring at scales other than the natural scale of the object.},
 author = {Spence, Clay and Sajda, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b5488aeff42889188d03c9895255cecc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b5488aeff42889188d03c9895255cecc-Metadata.json},
 openalex = {W2097744143},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b5488aeff42889188d03c9895255cecc-Paper.pdf},
 publisher = {MIT Press},
 title = {Applications of Multi-Resolution Neural Networks to Mammography},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/b5488aeff42889188d03c9895255cecc-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_b5a1fc20,
 abstract = {Gain control by divisive inhibition, a.k.a. divisive normalization, has been proposed to be a general mechanism throughout the visual cortex. We explore in this study the statistical properties of this normalization in the presence of noise. Using simulations, we show that divisive normalization is a close approximation to a maximum likelihood estimator, which, in the context of population coding, is the same as an ideal observer. We also demonstrate analytically that this is a general property of a large class of nonlinear recurrent networks with line attractors. Our work suggests that divisive normalization plays a critical role in noise filtering, and that every cortical layer may be an ideal observer of the activity in the preceding layer.},
 author = {Den\`{e}ve, Sophie and Pouget, Alexandre and Latham, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b5a1fc2085986034e448d2ccc5bb9703-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b5a1fc2085986034e448d2ccc5bb9703-Metadata.json},
 openalex = {W2098069024},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b5a1fc2085986034e448d2ccc5bb9703-Paper.pdf},
 publisher = {MIT Press},
 title = {Divisive Normalization, Line Attractor Networks and Ideal Observers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/b5a1fc2085986034e448d2ccc5bb9703-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_b60c5ab6,
 abstract = {This paper applies the Mixture of Gaussians probabilistic model, combined with Expectation Maximization optimization to the task of summarizing three dimensional range data for a mobile robot. This provides a flexible way of dealing with uncertainties in sensor information, and allows the introduction of prior knowledge into low-level perception modules. Problems with the basic approach were solved in several ways: the mixture of Gaussians was reparameterized to reflect the types of objects expected in the scene, and priors on model parameters were included in the optimization process. Both approaches force the optimization to find 'interesting' objects, given the sensor and object characteristics. A higher level classifier was used to interpret the results provided by the model, and to reject spurious solutions.},
 author = {Williamson, Matthew and Murray-Smith, Roderick and Hansen, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b60c5ab647a27045b462934977ccad9a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b60c5ab647a27045b462934977ccad9a-Metadata.json},
 openalex = {W2147620251},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b60c5ab647a27045b462934977ccad9a-Paper.pdf},
 publisher = {MIT Press},
 title = {Robot Docking Using Mixtures of Gaussians},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/b60c5ab647a27045b462934977ccad9a-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_b7109157,
 abstract = {We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach.},
 author = {Bennett, Kristin and Demiriz, Ayhan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b710915795b9e9c02cf10d6d2bdb688c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b710915795b9e9c02cf10d6d2bdb688c-Metadata.json},
 openalex = {W2107968230},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf},
 publisher = {MIT Press},
 title = {Semi-Supervised Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/b710915795b9e9c02cf10d6d2bdb688c-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_bc573864,
 abstract = {There has been much recent work on measuring image statistics and on learning probability distributions on images. We observe that the mapping from images to statistics is many-to-one and show it can be quantified by a phase space factor. This phase space approach throws light on the Minimax Entropy technique for learning Gibbs distributions on images with potentials derived from image statistics and elucidates the ambiguities that are inherent to determining the potentials. In addition, it shows that if the phase factor can be approximated by an analytic distribution then this approximation yields a swift Minutemax algorithm that vastly reduces the computation time for Minimax entropy learning. An illustration of this concept, using a Gaussian to approximate the phase factor, gives a good approximation to the results of Zhu and Mumford (1997) in just seconds of CPU time. The phase space approach also gives insight into the multi-scale potentials found by Zhu and Mumford (1997) and suggests that the forms of the potentials are influenced greatly by phase space considerations. Finally, we prove that probability distributions learned in feature space alone are equivalent to Minimax Entropy learning with a multinomial approximation of the phase factor.},
 author = {Coughlan, James and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bc573864331a9e42e4511de6f678aa83-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bc573864331a9e42e4511de6f678aa83-Metadata.json},
 openalex = {W2160377604},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bc573864331a9e42e4511de6f678aa83-Paper.pdf},
 publisher = {MIT Press},
 title = {A Phase Space Approach to Minimax Entropy Learning and the Minutemax Approximations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/bc573864331a9e42e4511de6f678aa83-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_bc731692,
 abstract = {We compute upper and lower bounds on the VC dimension and pseudo-dimension of feedforward neural networks composed of piecewise polynomial activation functions. We show that if the number of layers is fixed, then the VC dimension and pseudo-dimension grow as WlogW, where W is the number of parameters in the network. This result stands in opposition to the case where the number of layers is unbounded, in which case the VC dimension and pseudo-dimension grow as W2. We combine our results with recently established approximation error rates and determine error bounds for the problem of regression estimation by piecewise polynomial networks with unbounded weights.},
 author = {Bartlett, Peter and Maiorov, Vitaly and Meir, Ron},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bc7316929fe1545bf0b98d114ee3ecb8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bc7316929fe1545bf0b98d114ee3ecb8-Metadata.json},
 openalex = {W2115881141},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf},
 publisher = {MIT Press},
 title = {Almost Linear VC-Dimension Bounds for Piecewise Polynomial Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/bc7316929fe1545bf0b98d114ee3ecb8-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_bcb41ccd,
 abstract = {In previous work [6, 9, 10], we advanced a new technique for direct visual matching of images for the purposes of face recognition and image retrieval, using a probabilistic measure of similarity based primarily on a Bayesian (MAP) analysis of image differences, leading to a dual basis similar to eigenfaces [13]. The performance advantage of this probabilistic matching technique over standard Euclidean nearest-neighbor eigenface matching was recently demonstrated using results from DARPA's 1996 FERET face recognition competition, in which this probabilistic matching algorithm was found to be the top performer. We have further developed a simple method of replacing the costly compution of nonlinear (online) Bayesian similarity measures by the relatively inexpensive computation of linear (offline) subspace projections and simple (online) Euclidean norms, thus resulting in a significant computational speed-up for implementation with very large image databases as typically encountered in real-world applications.},
 author = {Moghaddam, Baback and Jebara, Tony and Pentland, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bcb41ccdc4363c6848a1d760f26c28a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bcb41ccdc4363c6848a1d760f26c28a0-Metadata.json},
 openalex = {W2125204498},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bcb41ccdc4363c6848a1d760f26c28a0-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Modeling of Facial Similarity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/bcb41ccdc4363c6848a1d760f26c28a0-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_bffc9834,
 abstract = {We propose a new in-sample cross validation based method (randomized GACV) for choosing smoothing or bandwidth parameters that govern the bias-variance or fit-complexity tradeoff in 'soft' classification. Soft classification refers to a learning procedure which estimates the probability that an example with a given attribute vector is in class 1 vs class O. The target for optimizing the the tradeoff is the Kullback-Liebler distance between the estimated probability distribution and the 'true' probability distribution, representing knowledge of an infinite population. The method uses a randomized estimate of the trace of a Hessian and mimics cross validation at the cost of a single relearning with perturbed outcome data.},
 author = {Wahba, Grace and Lin, Xiwu and Gao, Fangyu and Xiang, Dong and Klein, Ronald and Klein, Barbara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bffc98347ee35b3ead06728d6f073c68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bffc98347ee35b3ead06728d6f073c68-Metadata.json},
 openalex = {W2111611307},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/bffc98347ee35b3ead06728d6f073c68-Paper.pdf},
 publisher = {MIT Press},
 title = {The Bias-Variance Tradeoff and the Randomized GACV},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/bffc98347ee35b3ead06728d6f073c68-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_c559da2b,
 abstract = {Lateral competition within a layer of neurons sharpens and localizes the response to an input stimulus. Here, we investigate a model for the activity dependent development of ocular dominance maps which allows to vary the degree of lateral competition. For weak competition, it resembles a correlation-based learning model and for strong competition, it becomes a self-organizing map. Thus, in the regime of weak competition the receptive fields are shaped by the second order statistics of the input patterns, whereas in the regime of strong competition, the higher moments and features of the individual patterns become important. When correlated localized stimuli from two eyes drive the cortical development we find (i) that a topographic map and binocular, localized receptive fields emerge when the degree of competition exceeds a critical value and (ii) that receptive fields exhibit eye dominance beyond a second critical value. For anti-correlated activity between the eyes, the second order statistics drive the system to develop ocular dominance even for weak competition, but no topography emerges. Topography is established only beyond a critical degree of competition.},
 author = {Piepenbrock, Christian and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c559da2ba967eb820766939a658022c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c559da2ba967eb820766939a658022c8-Metadata.json},
 openalex = {W2170298440},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c559da2ba967eb820766939a658022c8-Paper.pdf},
 publisher = {MIT Press},
 title = {The Role of Lateral Cortical Competition in Ocular Dominance Development},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/c559da2ba967eb820766939a658022c8-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_c7af0926,
 abstract = {Based on a simple convexity lemma, we develop bounds for different types of Bayesian prediction errors for regression with Gaussian processes. The basic bounds are formulated for a fixed training set. Simpler expressions are obtained for sampling from an input distribution which equals the weight function of the covariance kernel, yielding asymptotically tight results. The results are compared with numerical experiments.},
 author = {Opper, Manfred and Vivarelli, Francesco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c7af0926b294e47e52e46cfebe173f20-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c7af0926b294e47e52e46cfebe173f20-Metadata.json},
 openalex = {W2158451137},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c7af0926b294e47e52e46cfebe173f20-Paper.pdf},
 publisher = {MIT Press},
 title = {General Bounds on Bayes Errors for Regression with Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/c7af0926b294e47e52e46cfebe173f20-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_c88d8d0a,
 abstract = {The technique of principal component analysis (PCA) has recently been expressed as the maximum likelihood solution for a generative latent variable model. In this paper we use this probabilistic reformulation as the basis for a Bayesian treatment of PCA. Our key result is that effective dimensionality of the latent space (equivalent to the number of retained principal components) can be determined automatically as part of the Bayesian inference procedure. An important application of this framework is to mixtures of probabilistic PCA models, in which each component can determine its own effective complexity.},
 author = {Bishop, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Metadata.json},
 openalex = {W2294460823},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/c88d8d0a6097754525e02c2246d8d27f-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_cb8acb1d,
 abstract = {We consider recurrent analog neural nets where each gate is subject to Gaussian noise, or any other common noise distribution whose probability density function is nonzero on a large set. We show that many regular languages cannot be recognized by networks of this type, for example the language {w e {O, I}*| w begins with O}, and we give a precise characterization of those languages which can be recognized. This result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise. On the other hand we present a method for constructing feed forward analog neural nets that are robust with regard to analog noise of this type.},
 author = {Maass, Wolfgang and Sontag, Eduardo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/cb8acb1dc9821bf74e6ca9068032d623-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/cb8acb1dc9821bf74e6ca9068032d623-Metadata.json},
 openalex = {W2115027004},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/cb8acb1dc9821bf74e6ca9068032d623-Paper.pdf},
 publisher = {MIT Press},
 title = {A Precise Characterization of the Class of Languages Recognized by Neural Nets under Gaussian and Other Common Noise Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/cb8acb1dc9821bf74e6ca9068032d623-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_cda72177,
 abstract = {We suggest a working definition of texture: Texture is stuff that is more compactly represented by its statistics than by specifying the configuration of its parts. This definition suggests that to find texture we look for outliers to the local statistics, and label as texture the regions with no outliers. We present a method, based upon this idea, for labeling points in natural scenes as belonging to texture regions, while simultaneously allowing us to label lowlevel, bottom-up cues for visual attention. This method is based upon recent psychophysics results on processing of texture and popout.},
 author = {Rosenholtz, Ruth},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/cda72177eba360ff16b7f836e2754370-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/cda72177eba360ff16b7f836e2754370-Metadata.json},
 openalex = {W2141170618},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/cda72177eba360ff16b7f836e2754370-Paper.pdf},
 publisher = {MIT Press},
 title = {General-Purpose Localization of Textured Image Regions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/cda72177eba360ff16b7f836e2754370-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_ced556cd,
 abstract = {This paper describes a simple and efficient method to make template-based object classification invariant to in-plane rotations. The task is divided into two parts: orientation discrimination and classification. The key idea is to perform the orientation discrimination before the classification. This can be accomplished by hypothesizing, in turn, that the input image belongs to each class of interest. The image can then be rotated to maximize its similarity to the training images in each class (these contain the prototype object in an upright orientation). This process yields a set of images, at least one of which will have the object in an upright position. The resulting images can then be classified by models which have been trained with only upright examples. This approach has been successfully applied to two real-world vision-based tasks: rotated handwritten digit recognition and rotated face detection in cluttered scenes.},
 author = {Baluja, Shumeet},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ced556cd9f9c0c8315cfbe0744a3baf0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ced556cd9f9c0c8315cfbe0744a3baf0-Metadata.json},
 openalex = {W2166645434},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ced556cd9f9c0c8315cfbe0744a3baf0-Paper.pdf},
 publisher = {MIT Press},
 title = {Making Templates Rotationally Invariant. An Application to Rotated Digit Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/ced556cd9f9c0c8315cfbe0744a3baf0-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_cfa53013,
 abstract = {Adaptive Ridge is a special form of Ridge regression, balancing the quadratic penalization on each parameter of the model. It was shown to be equivalent to Lasso (least absolute shrinkage and selection operator), in the sense that both procedures produce the same estimate. Lasso can thus be viewed as a particular quadratic penalizer.

From this observation, we derive a fixed point algorithm to compute the Lasso solution. The analogy provides also a new hyper-parameter for tuning effectively the model complexity. We finally present a series of possible extensions of lasso performing sparse regression in kernel smoothing, additive modeling and neural net training.},
 author = {Grandvalet, Yves and Canu, St\'{e}phane},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/cfa5301358b9fcbe7aa45b1ceea088c6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/cfa5301358b9fcbe7aa45b1ceea088c6-Metadata.json},
 openalex = {W2133100596},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/cfa5301358b9fcbe7aa45b1ceea088c6-Paper.pdf},
 publisher = {MIT Press},
 title = {Outcomes of the Equivalence of Adaptive Ridge with Least Absolute Shrinkage},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/cfa5301358b9fcbe7aa45b1ceea088c6-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_d010396c,
 abstract = {I consider the problem of learning concepts from small numbers of positive examples, a feat which humans perform routinely but which computers are rarely capable of. Bridging machine learning and cognitive science perspectives, I present both theoretical analysis and an empirical study with human subjects for the simple task oflearning concepts corresponding to axis-aligned rectangles in a multidimensional feature space. Existing learning models, when applied to this task, cannot explain how subjects generalize from only a few examples of the concept. I propose a principled Bayesian model based on the assumption that the examples are a random sample from the concept to be learned. The model gives precise fits to human behavior on this simple task and provides qualitative insights into more complex, realistic cases of concept learning.},
 author = {Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d010396ca8abf6ead8cacc2c2f2f26c7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d010396ca8abf6ead8cacc2c2f2f26c7-Metadata.json},
 openalex = {W2138138984},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Modeling of Human Concept Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/d010396ca8abf6ead8cacc2c2f2f26c7-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_d1a69640,
 abstract = {We present the CEM (Conditional Expectation Maximization) algorithm as an extension of the EM (Expectation Maximization) algorithm to conditional density estimation under missing data. A bounding and maximization process is given to specifically optimize conditional likelihood instead of the usual joint likelihood. We apply the method to conditioned mixture models and use bounding techniques to derive the model's update rules. Monotonic convergence, computational efficiency and regression results superior to EM are demonstrated.},
 author = {Jebara, Tony and Pentland, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d1a69640d53a32a9fb13e93d1c8f3104-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d1a69640d53a32a9fb13e93d1c8f3104-Metadata.json},
 openalex = {W2154834035},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d1a69640d53a32a9fb13e93d1c8f3104-Paper.pdf},
 publisher = {MIT Press},
 title = {Maximum Conditional Likelihood via Bound Maximization and the CEM Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/d1a69640d53a32a9fb13e93d1c8f3104-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_d1dc3a82,
 abstract = {This paper reveals a previously ignored connection between two important fields: regularization and independent component analysis (ICA). We show that at least one representative of a broad class of algorithms (regularizers that reduce network complexity) extracts independent features as a by-product. This algorithm is Flat Minimum Search (FMS), a recent general method for finding low-complexity networks with high generalization capability. FMS works by minimizing both training error and required weight precision. According to our theoretical analysis the hidden layer of an FMS-trained autoassociator attempts at coding each input by a sparse code with as few simple features as possible. In experiments the method extracts optimal codes for difficult versions of the noisy bars benchmark problem by separating the underlying sources, whereas ICA and PCA fail. Real world images are coded with fewer bits per pixel than by ICA or PCA.},
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d1dc3a8270a6f9394f88847d7f0050cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d1dc3a8270a6f9394f88847d7f0050cf-Metadata.json},
 openalex = {W2150042189},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d1dc3a8270a6f9394f88847d7f0050cf-Paper.pdf},
 publisher = {MIT Press},
 title = {Source Separation as a By-Product of Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/d1dc3a8270a6f9394f88847d7f0050cf-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_d63fbf8c,
 abstract = {We present an analog very large scale integration (VLSI) cellular architecture implementing a version of the boundary contour system (BCS) for real-time focal-plane image processing. Inspired by neuromorphic models across the retina and several layers of visual cortex, the design integrates in each pixel the functions of phototransduction and simple cells, complex cells, hypercomplex cells, and bipole cells in each of three directions interconnected on a hexagonal grid. Analog current-mode complementary metal-oxide-semiconductor (CMOS) circuits are used throughout to perform edge detection, local inhibition, directionally selective long-range diffusive kernels, and renormalizing global gain control. Experimental results from a fabricated 12/spl times/10 pixel prototype in a 1.2-/spl mu/m CMOS process are included, demonstrating the robustness of the implemented BCS model in selecting image contours in a cluttered and noisy background.},
 author = {Cauwenberghs, Gert and Waskiewicz, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d63fbf8c3173730f82b150c5ef38b8ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d63fbf8c3173730f82b150c5ef38b8ff-Metadata.json},
 openalex = {W2163633330},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d63fbf8c3173730f82b150c5ef38b8ff-Paper.pdf},
 publisher = {MIT Press},
 title = {Focal-plane analog VLSI cellular implementation of the boundary contour system},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/d63fbf8c3173730f82b150c5ef38b8ff-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_d72fbbcc,
 abstract = {We present an unsupervised classification algorithm based on an ICA mixture model. The ICA mixture model assumes that the observed data can be categorized into several mutually exclusive data classes in which the components in each class are generated by a linear mixture of independent sources. The algorithm finds the independent sources, the mixing matrix for each class and also computes the class membership probability for each data point. This approach extends the Gaussian mixture model so that the classes can have non-Gaussian structure. We demonstrate that this method can learn efficient codes to represent images of natural scenes and text. The learned classes of basis functions yield a better approximation of the underlying distributions of the data, and thus can provide greater coding efficiency. We believe that this method is well suited to modeling structure in high-dimensional data and has many potential applications.},
 author = {Lee, Te-Won and Lewicki, Michael and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d72fbbccd9fe64c3a14f85d225a046f4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d72fbbccd9fe64c3a14f85d225a046f4-Metadata.json},
 openalex = {W2153302588},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d72fbbccd9fe64c3a14f85d225a046f4-Paper.pdf},
 publisher = {MIT Press},
 title = {Unsupervised Classification with Non-Gaussian Mixture Models Using ICA},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/d72fbbccd9fe64c3a14f85d225a046f4-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_d961e9f2,
 abstract = {In order to find the optimal control of continuous state-space and time reinforcement learning (RL) problems, we approximate the value function (VF) with a particular class of functions called the barycentric interpolators. We establish sufficient conditions under which a RL algorithm converges to the optimal VF, even when we use approximate models of the state dynamics and the reinforcement functions.},
 author = {Munos, R\'{e}mi and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d961e9f236177d65d21100592edb0769-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d961e9f236177d65d21100592edb0769-Metadata.json},
 openalex = {W2142409110},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/d961e9f236177d65d21100592edb0769-Paper.pdf},
 publisher = {MIT Press},
 title = {Barycentric Interpolators for Continuous Space and Time Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/d961e9f236177d65d21100592edb0769-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_db191505,
 abstract = {Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis.},
 author = {Jaakkola, Tommi and Haussler, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/db1915052d15f7815c8b88e879465a1e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/db1915052d15f7815c8b88e879465a1e-Metadata.json},
 openalex = {W2166473218},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/db1915052d15f7815c8b88e879465a1e-Paper.pdf},
 publisher = {MIT Press},
 title = {Exploiting Generative Models in Discriminative Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/db1915052d15f7815c8b88e879465a1e-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_dc5c768b,
 abstract = {A directed generative model for binary data using a small number of hidden continuous units is investigated. A clipping nonlinearity distinguishes the model from conventional principal components analysis. The relationships between the correlations of the underlying continuous Gaussian variables and the binary output variables are utilized to learn the appropriate weights of the network. The advantages of this approach are illustrated on a translationally invariant binary distribution and on handwritten digit images.},
 author = {Lee, Daniel and Sompolinsky, Haim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/dc5c768b5dc76a084531934b34601977-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/dc5c768b5dc76a084531934b34601977-Metadata.json},
 openalex = {W2124304383},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/dc5c768b5dc76a084531934b34601977-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning a Continuous Hidden Variable Model for Binary Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/dc5c768b5dc76a084531934b34601977-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_dca5672f,
 abstract = {As a benchmark task, the spiral problem is well known in neural networks. Unlike previous work that emphasizes learning, we approach the problem from a generic perspective that does not involve learning. We point out that the spiral problem is intrinsically connected to the inside/ outside problem. A generic solution to both problems is proposed based on oscillatory correlation using a time delay network. Our simulation results are qualitatively consistent with human performance, and we interpret human limitations in terms of synchrony and time delays, both biologically plausible. As a special case, our network without time delays can always distinguish these figures regardless of shape, position, size, and orientation.},
 author = {Chen, Ke and Wang, DeLiang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/dca5672ff3444c7e997aa9a2c4eb2094-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/dca5672ff3444c7e997aa9a2c4eb2094-Metadata.json},
 openalex = {W2126485605},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/dca5672ff3444c7e997aa9a2c4eb2094-Paper.pdf},
 publisher = {MIT Press},
 title = {Perceiving without Learning: From Spirals to Inside/Outside Relations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/dca5672ff3444c7e997aa9a2c4eb2094-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_df12ecd0,
 abstract = {Following recent results[9, 8] showing the importance of the fat-shattering dimension in explaining the beneficial effect of a large margin on generalization performance, the current paper investigates the implications of these results for the case of imbalanced datasets and develops two approaches to setting the threshold. The approaches are incorporated into ThetaBoost, a boosting algorithm for dealing with unequal loss functions. The performance of ThetaBoost and the two approaches are tested experimentally.},
 author = {Karakoulas, Grigoris and Shawe-Taylor, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/df12ecd077efc8c23881028604dbb8cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/df12ecd077efc8c23881028604dbb8cc-Metadata.json},
 openalex = {W2160468005},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/df12ecd077efc8c23881028604dbb8cc-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimizing Classifers for Imbalanced Training Sets},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/df12ecd077efc8c23881028604dbb8cc-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_dfa92d8f,
 abstract = {In this paper we present a novel approach to multichannel blind separation/generalized deconvolution, assuming that both mixing and demixing models are described by stable linear state-space systems. We decompose the blind separation problem into two process: separation and state estimation. Based on the minimization of Kullback-Leibler Divergence, we develop a novel learning algorithm to train the matrices in the output equation. To estimate the state of the demixing model, we introduce a new concept, called hidden innovation, to numerically implement the Kalman filter. Computer simulations are given to show the validity and high effectiveness of the state-space approach.},
 author = {Zhang, Liqing and Cichocki, Andrzej},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/dfa92d8f817e5b08fcaafb50d03763cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/dfa92d8f817e5b08fcaafb50d03763cf-Metadata.json},
 openalex = {W2107585460},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/dfa92d8f817e5b08fcaafb50d03763cf-Paper.pdf},
 publisher = {MIT Press},
 title = {Blind Separation of Filtered Sources Using State-Space Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/dfa92d8f817e5b08fcaafb50d03763cf-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_e60e81c4,
 abstract = {Cortical amplification has been proposed as a mechanism for enhancing the selectivity of neurons in the primary visual cortex. Less appreciated is the fact that the same form of amplification can also be used to detune or broaden selectivity. Using a network model with recurrent cortical circuitry, we propose that the spatial phase invariance of complex cell responses arises through recurrent amplification of feedforward input. Neurons in the network respond like simple cells at low gain and complex ceUs at high gain. Similar recurrent mechanisms may play a role in generating invariant representations of feedforward input elsewhere in the visual processing pathway.},
 author = {Chance, Frances and Nelson, Sacha and Abbott, L.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/e60e81c4cbe5171cd654662d9887aec2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/e60e81c4cbe5171cd654662d9887aec2-Metadata.json},
 openalex = {W2098324370},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/e60e81c4cbe5171cd654662d9887aec2-Paper.pdf},
 publisher = {MIT Press},
 title = {Recurrent Cortical Amplification Produces Complex Cell Responses},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/e60e81c4cbe5171cd654662d9887aec2-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_e655c771,
 abstract = {Classifier systems are now viewed disappointing because of their problems such as the rule strength vs rule set performance problem and the credit assignment problem. In order to solve the problems, we have developed a hybrid classifier system: GLS (Generalization Learning System). In designing GLS, we view CSs as model free learning in POMDPs and take a hybrid approach to finding the best generalization, given the total number of rules. GLS uses the policy improvement procedure by Jaakkola et al. for an locally optimal stochastic policy when a set of rule conditions is given. GLS uses GA to search for the best set of rule conditions.},
 author = {Hayashi, Akira and Suematsu, Nobuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/e655c7716a4b3ea67f48c6322fc42ed6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/e655c7716a4b3ea67f48c6322fc42ed6-Metadata.json},
 openalex = {W2166696997},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/e655c7716a4b3ea67f48c6322fc42ed6-Paper.pdf},
 publisher = {MIT Press},
 title = {Viewing Classifier Systems as Model Free Learning in POMDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/e655c7716a4b3ea67f48c6322fc42ed6-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_e9fd7c2c,
 abstract = {In this article, we propose a new reinforcement learning (RL) method based on an actor-critic architecture. The actor and the critic are approximated by Normalized Gaussian Networks (NGnet), which are networks of local linear regression units. The NGnet is trained by the on-line EM algorithm proposed in our previous paper. We apply our RL method to the task of swinging-up and stabilizing a single pendulum and the task of balancing a double pendulum near the upright position. The experimental results show that our RL method can be applied to optimal control problems having continuous state/action spaces and that the method achieves good control with a small number of trial-and-errors.},
 author = {Sato, Masa-aki and Ishii, Shin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/e9fd7c2c6623306db59b6aef5c0d5cac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/e9fd7c2c6623306db59b6aef5c0d5cac-Metadata.json},
 openalex = {W2121174064},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/e9fd7c2c6623306db59b6aef5c0d5cac-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning Based on On-Line EM Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/e9fd7c2c6623306db59b6aef5c0d5cac-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_ebb71045,
 abstract = {Standard techniques (eg. Yule-Walker) are available for learning Auto-Regressive process models of simple, directly observable, dynamical processes. When sensor noise means that dynamics are observed only approximately, learning can still been achieved via Expectation-Maximisation (EM) together with Kalman Filtering. However, this does not handle more complex dynamics, involving multiple classes of motion. For that problem, we show here how EM can be combined with the CONDENSATION algorithm, which is based on propagation of random sample-sets. Experiments have been performed with visually observed juggling, and plausible dynamical models are found to emerge from the learning process.},
 author = {Blake, Andrew and North, Ben and Isard, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ebb71045453f38676c40deb9864f811d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ebb71045453f38676c40deb9864f811d-Metadata.json},
 openalex = {W2123917177},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ebb71045453f38676c40deb9864f811d-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Multi-Class Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/ebb71045453f38676c40deb9864f811d-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_ebd6d2f5,
 abstract = {We investigate a probabilistic framework for automatic speech recognition based on the intrinsic geometric properties of curves. In particular, we analyze the setting in which two variables-one continuous (x), one discrete (s)-evolve jointly in time. We suppose that the vector x traces out a smooth multidimensional curve and that the variable s evolves stochastically as a function of the arc length traversed along this curve. Since arc length does not depend on the rate at which a curve is traversed, this gives rise to a family of Markov processes whose predictions, Pr[s|x], are invariant to nonlinear warpings of time. We describe the use of such models, known as Markov processes on curves (MPCs), for automatic speech recognition, where x are acoustic feature trajectories and s are phonetic transcriptions. On two tasks--recognizing New Jersey town names and connected alpha-digits--we find that MPCs yield lower word error rates than comparably trained hidden Markov models.},
 author = {Saul, Lawrence and Rahim, Mazin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Metadata.json},
 openalex = {W2118847434},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf},
 publisher = {MIT Press},
 title = {Markov Processes on Curves for Automatic Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/ebd6d2f5d60ff9afaeda1a81fc53e2d0-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_ed422773,
 author = {Vivarelli, Francesco and Williams, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ed4227734ed75d343320b6a5fd16ce57-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ed4227734ed75d343320b6a5fd16ce57-Metadata.json},
 openalex = {W2163057507},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/ed4227734ed75d343320b6a5fd16ce57-Paper.pdf},
 publisher = {MIT Press},
 title = {Discovering Hidden Features with Gaussian Processes Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/ed4227734ed75d343320b6a5fd16ce57-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_efb76cff,
 abstract = {Historically, connectionist systems have not excelled at representing and manipulating complex structures. How can a system composed of simple neuron-like computing elements encode complex relations? Recently, researchers have begun to appreciate that representations can extend in both time and space. Many researchers have proposed that the synchronous firing of units can encode complex representations. I identify the limitations of this approach and present an asynchronous model of binding that effectively represents complex structures. The asynchronous model extends the synchronous approach. I argue that our cognitive architecture utilizes a similar mechanism.},
 author = {Love, Bradley C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/efb76cff97aaf057654ef2f38cd77d73-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/efb76cff97aaf057654ef2f38cd77d73-Metadata.json},
 openalex = {W2154301299},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/efb76cff97aaf057654ef2f38cd77d73-Paper.pdf},
 publisher = {MIT Press},
 title = {Utilizing lime: Asynchronous Binding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/efb76cff97aaf057654ef2f38cd77d73-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_f187a23c,
 abstract = {Clustering is important in many fields including manufacturing, biology, finance, and astronomy. Mixture models are a popular approach due to their statistical foundations, and EM is a very popular method for finding mixture models. EM, however, requires many accesses of the data, and thus has been dismissed as impractical (e.g. [9]) for data mining of enormous datasets. We present a new algorithm, based on the multiresolution kd-trees of [5], which dramatically reduces the cost of EM-based clustering, with savings rising linearly with the number of datapoints. Although presented here for maximum likelihood estimation of Gaussian mixture models, it is also applicable to non-Gaussian models (provided class densities are monotonic in Mahalanobis distance), mixed categorical/ numeric clusters. and Bayesian methods such as Antoclass [1].},
 author = {Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f187a23c3ee681ef6913f31fd6d6446b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f187a23c3ee681ef6913f31fd6d6446b-Metadata.json},
 openalex = {W2152255870},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f187a23c3ee681ef6913f31fd6d6446b-Paper.pdf},
 publisher = {MIT Press},
 title = {Very Fast EM-Based Mixture Model Clustering Using Multiresolution Kd-Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/f187a23c3ee681ef6913f31fd6d6446b-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_f18a6d1c,
 abstract = {O(ws(s log d+log(dqh/s))) and O(ws((h/s) log q) +log(dqh/s)) are upper bounds for the VC-dimension of a set of neural networks of units with piecewise polynomial activation functions, where s is the depth of the network, h is the number of hidden units, w is the number of adjustable parameters, q is the maximum of the number of polynomial segments of the activation function, and d is the maximum degree of the polynomials; also Ω (ws log(dqh/s)) is a lower bound for the VC-dimension of such a network set, which are tight for the cases s = Θ(h) and s is constant. For the special case q = 1, the VC-dimension is Θ(ws log d).},
 author = {Sakurai, Akito},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f18a6d1cde4b205199de8729a6637b42-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f18a6d1cde4b205199de8729a6637b42-Metadata.json},
 openalex = {W2154946043},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f18a6d1cde4b205199de8729a6637b42-Paper.pdf},
 publisher = {MIT Press},
 title = {Tight Bounds for the VC-Dimension of Piecewise Polynomial Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/f18a6d1cde4b205199de8729a6637b42-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_f3173935,
 abstract = {Agents acting in the real world are confronted with the problem of making good decisions with limited knowledge of the environment. Partially observable Markov decision processes (POMDPs) model decision problems in which an agent tries to maximize its reward in the face of limited sensor feedback. Recent work has shown empirically that a reinforcement learning (RL) algorithm called Sarsa(λ) can efficiently find optimal memoryless policies, which map current observations to actions, for POMDP problems (Loch and Singh 1998). The Sarsa(λ) algorithm uses a form of short-term memory called an eligibility trace, which distributes temporally delayed rewards to observation-action pairs which lead up to the reward. This paper explores the effect of eligibility traces on the ability of the Sarsa(λ) algorithm to find optimal memoryless policies. A variant of Sarsa(λ) called k-step truncated Sarsa(λ) is applied to four test problems taken from the recent work of Littman, Littman, Cassandra and Kaelbling, Parr and Russell, and Chrisman. The empirical results show that eligibility traces can be significantly truncated without affecting the ability of Sarsa(λ) to find optimal memoryless policies for POMDPs.},
 author = {Loch, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f3173935ed8ac4bf073c1bcd63171f8a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f3173935ed8ac4bf073c1bcd63171f8a-Metadata.json},
 openalex = {W2134309497},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f3173935ed8ac4bf073c1bcd63171f8a-Paper.pdf},
 publisher = {MIT Press},
 title = {The Effect of Eligibility Traces on Finding Optimal Memoryless Policies in Partially Observable Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/f3173935ed8ac4bf073c1bcd63171f8a-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_f337d999,
 abstract = {Two developments of nonlinear latent variable models based on radial basis functions are discussed: in the first, the use of priors or constraints on allowable models is considered as a means of preserving data structure in low-dimensional representations for visualisation purposes. Also, a resampling approach is introduced which makes more effective use of the latent samples in evaluating the likelihood.},
 author = {Marrs, Alan and Webb, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f337d999d9ad116a7b4f3d409fcc6480-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f337d999d9ad116a7b4f3d409fcc6480-Metadata.json},
 openalex = {W2145426834},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf},
 publisher = {MIT Press},
 title = {Exploratory Data Analysis Using Radial Basis Function Latent Variable Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/f337d999d9ad116a7b4f3d409fcc6480-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_f60bb6bb,
 abstract = {Visual search is the task of finding a target in an image against a background of distractors. Unique features of targets enable them to pop out against the background, while targets defined by lacks of features or conjunctions of features are more difficult to spot. It is known that the ease of target detection can change when the roles of figure and ground are switched. The mechanisms underlying the ease of pop out and asymmetry in visual search have been elusive. This paper shows that a model of segmentation in V1 based on intracortical interactions can explain many of the qualitative aspects of visual search.},
 author = {Li, Zhaoping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Metadata.json},
 openalex = {W2127266719},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/f60bb6bb4c96d4df93c51bd69dcc15a0-Paper.pdf},
 publisher = {MIT Press},
 title = {A V1 Model of Pop Out and Asymmetty in Visual Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/f60bb6bb4c96d4df93c51bd69dcc15a0-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_fa1e9c96,
 abstract = {This paper presents a novel and fast k-NN classifier that is based on a binary CMM (Correlation Matrix Memory) neural network. A robust encoding method is developed to meet CMM input requirements. A hardware implementation of the CMM is described, which gives over 200 times the speed of a current mid-range workstation, and is scaleable to very large problems. When tested on several benchmarks and compared with a simple k-NN method, the CMM classifier gave less than 1% lower accuracy and over 4 and 12 times speed-up in software and hardware respectively.},
 author = {Zhou, Ping and Austin, Jim and Kennedy, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/fa1e9c965314ccd7810fb5ea838303e5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/fa1e9c965314ccd7810fb5ea838303e5-Metadata.json},
 openalex = {W2118361959},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/fa1e9c965314ccd7810fb5ea838303e5-Paper.pdf},
 publisher = {MIT Press},
 title = {A High Performance k-NN Classifier Using a Binary Correlation Matrix Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/fa1e9c965314ccd7810fb5ea838303e5-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_faafda66,
 abstract = {Learning Real-Time A* (LRTA*) is a popular control method that interleaves planning and plan execution and has been shown to solve search problems in known environments efficiently. In this paper, we apply LRTA * to the problem of getting to a given goal location in an initially unknown environment. Uninformed LRTA * with maximal lookahead always moves on a shortest path to the closest unvisited state, that is, to the closest potential goal state. This was believed to be a good exploration heuristic, but we show that it does not minimize the worst-case plan-execution time compared to other uninformed exploration methods. This result is also of interest to reinforcement-learning researchers since many reinforcement learning methods use asynchronous dynamic programming, interleave planning and plan execution, and exhibit optimism in the face of uncertainty, just like LRTA *.},
 author = {Koenig, Sven},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/faafda66202d234463057972460c04f5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/faafda66202d234463057972460c04f5-Metadata.json},
 openalex = {W2137351138},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/faafda66202d234463057972460c04f5-Paper.pdf},
 publisher = {MIT Press},
 title = {Exploring Unknown Environments with Real-Time Search or Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/faafda66202d234463057972460c04f5-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_facf9f74,
 abstract = {We study probabilistic inference in large, layered Bayesian networks represented as directed acyclic graphs. We show that the intractability of exact inference in such networks does not preclude their effective use. We give algorithms for approximate probabilistic inference that exploit averaging phenomena occurring at nodes with large numbers of parents. We show that these algorithms compute rigorous lower and upper bounds on marginal probabilities of interest, prove that these bounds become exact in the limit of large networks, and provide rates of convergence.},
 author = {Kearns, Michael and Saul, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/facf9f743b083008a894eee7baa16469-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/facf9f743b083008a894eee7baa16469-Metadata.json},
 openalex = {W2146547746},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/facf9f743b083008a894eee7baa16469-Paper.pdf},
 publisher = {MIT Press},
 title = {Inference in Multilayer Networks via Large Deviation Bounds},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/facf9f743b083008a894eee7baa16469-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_fc528592,
 abstract = {A wavelet basis selection procedure is presented for wavelet regression. Both the basis and threshold are selected using cross-validation. The method includes the capability of incorporating prior knowledge on the smoothness (or shape of the basis functions) into the basis selection procedure. The results of the method are demonstrated using widely published sampled functions. The results of the method are contrasted with other basis function based methods.},
 author = {Wheeler, Kevin and Dhawan, Atam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/fc528592c3858f90196fbfacc814f235-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/fc528592c3858f90196fbfacc814f235-Metadata.json},
 openalex = {W2110022960},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/fc528592c3858f90196fbfacc814f235-Paper.pdf},
 publisher = {MIT Press},
 title = {Basis Selection for Wavelet Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/fc528592c3858f90196fbfacc814f235-Abstract.html},
 volume = {11},
 year = {1998}
}

@inproceedings{NIPS1998_fcdf25d6,
 abstract = {Visually-guided arm reaching movements are produced by distributed neural networks within parietal and frontal regions of the cerebral cortex. Experimental data indicate that (1) single neurons in these regions are broadly tuned to parameters of movement; (2) appropriate commands are elaborated by populations of neurons; (3) the coordinated action of neurons can be visualized using a neuronal population vector (NPV). However, the NPV provides only a rough estimate of movement parameters (direction, velocity) and may even fail to reflect the parameters of movement when arm posture is changed. We designed a model of the cortical motor command to investigate the relation between the desired direction of the movement, the actual direction of movement and the direction of the NPV in motor cortex. The model is a two-layer self-organizing neural network which combines broadly-tuned (muscular) proprioceptive and (cartesian) visual information to calculate (angular) motor commands for the initial part of the movement of a two-link arm. The network was trained by motor babbling in 5 positions. Simulations showed that (1) the network produced appropriate movement direction over a large part of the workspace; (2) small deviations of the actual trajectory from the desired trajectory existed at the extremities of the workspace; (3) these deviations were accompanied by large deviations of the NPV from both trajectories. These results suggest the NPV does not give a faithful image of cortical processing during arm reaching movements.},
 author = {Baraduc, Pierre and Guigon, Emmanuel and Burnod, Yves},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1998/file/fcdf25d6e191893e705819b177cddea0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1998/file/fcdf25d6e191893e705819b177cddea0-Metadata.json},
 openalex = {W2147565823},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1998/file/fcdf25d6e191893e705819b177cddea0-Paper.pdf},
 publisher = {MIT Press},
 title = {Where does the population vector of motor cortical cells point during reaching movements?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/fcdf25d6e191893e705819b177cddea0-Abstract.html},
 volume = {11},
 year = {1998}
}
