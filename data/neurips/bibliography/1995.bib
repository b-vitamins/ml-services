@inproceedings{NIPS1995_00e26af6,
 abstract = {Visual occlusion events constitute a major source of depth information. This paper presents a self-organizing neural network that learns to detect, represent, and predict the visibility and invisibility relationships that arise during occlusion events, after a period of exposure to motion sequences containing occlusion and disocclusion events. The network develops two parallel opponent channels or chains of lateral excitatory connections for every resolvable motion trajectory. One channel, the chain or chain, is activated when a moving stimulus is visible. The other channel, the chain or chain, carries a persistent, amodal representation that predicts the motion of a formerly visible stimulus that becomes invisible due to occlusion. The learning rule uses disinhibition from the On chain to trigger learning in the Off chain. The On and Off chain neurons can learn separate associations with object depth ordering. The results are closely related to the recent discovery (Assad & Maunsell, 1995) of neurons in macaque monkey posterior parietal cortex that respond selectively to inferred motion of invisible stimuli.},
 author = {Marshall, Jonathan and Alley, Richard and Hubbard, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/00e26af6ac3b1c1c49d7c3d79c60d000-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/00e26af6ac3b1c1c49d7c3d79c60d000-Metadata.json},
 openalex = {W2139340835},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning to Predict Visibility and Invisibility from Occlusion Events},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/00e26af6ac3b1c1c49d7c3d79c60d000-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_021bbc7e,
 abstract = {A technique for segmenting sounds using processing based on mammalian early auditory processing is presented. The technique is based on features in sound which neuron spike recording suggests are detected in the cochlear nucleus. The sound signal is bandpassed and each signal processed to enhance onsets and offsets. The onset and offset signals are compressed, then clustered both in time and across frequency channels using a network of integrate-and-fire neurons. Onsets and offsets are signalled by spikes, and the timing of these spikes used to segment the sound.},
 author = {Smith, Leslie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/021bbc7ee20b71134d53e20206bd6feb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/021bbc7ee20b71134d53e20206bd6feb-Metadata.json},
 openalex = {W2110473427},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf},
 publisher = {MIT Press},
 title = {Onset-based Sound Segmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_03f54461,
 abstract = {Learning how to adjust to an opponent's position is critical to the success of having intelligent agents collaborating towards the achievement of specific tasks in unfriendly environments. This paper describes our work on a Memory-based technique for to choose an action based on a continuous-valued state attribute indicating the position of an opponent. We investigate the question of how an agent performs in nondeterministic variations of the training situations. Our experiments indicate that when the random variations fall within some bound of the initial training, the agent performs better with some initial training rather than from a tabula-rasa.},
 author = {Stone, Peter and Veloso, Manuela},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/03f544613917945245041ea1581df0c2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/03f544613917945245041ea1581df0c2-Metadata.json},
 openalex = {W2096944299},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/03f544613917945245041ea1581df0c2-Paper.pdf},
 publisher = {MIT Press},
 title = {Beating a Defender in Robotic Soccer: Memory-Based Learning of a Continuous Function,},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/03f544613917945245041ea1581df0c2-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_062ddb6c,
 abstract = {We present results on the use of neural network based autoassociators which act as novelty or anomaly detectors to detect imminent motor failures. The autoassociator is trained to reconstruct spectra obtained from the healthy motor. In laboratory tests, we have demonstrated that the trained autoassociator has a small reconstruction error on measurements recorded from healthy motors but a larger error on those recorded from a motor with a fault. We have designed and built a motor monitoring system using an autoassociator for anomaly detection and are in the process of testing the system at three industrial and commercial sites.},
 author = {Petsche, Thomas and Marcantonio, Angelo and Darken, Christian and Hanson, Stephen and Kuhn, Gary and Santoso, N.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/062ddb6c727310e76b6200b7c71f63b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/062ddb6c727310e76b6200b7c71f63b5-Metadata.json},
 openalex = {W2111278976},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neural Network Autoassociator for Induction Motor Failure Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/062ddb6c727310e76b6200b7c71f63b5-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_0768281a,
 abstract = {We define a Gamma multi-layer perceptron (MLP) as an MLP with the usual synaptic weights replaced by gamma filters (as proposed by de Vries and Principe (de Vries and Principe, 1992)) and associated gain terms throughout all layers. We derive gradient descent update equations and apply the model to the recognition of speech phonemes. We find that both the inclusion of gamma filters in all layers, and the inclusion of synaptic gains, improves the performance of the Gamma MLP. We compare the Gamma MLP with TDNN, Back-Tsoi FIR MLP, and Back-Tsoi IIR MLP architectures, and a local approximation scheme. We find that the Gamma MLP results in an substantial reduction in error rates.},
 author = {Lawrence, Steve and Tsoi, Ah and Back, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0768281a05da9f27df178b5c39a51263-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0768281a05da9f27df178b5c39a51263-Metadata.json},
 openalex = {W2170700659},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0768281a05da9f27df178b5c39a51263-Paper.pdf},
 publisher = {MIT Press},
 title = {The Gamma MLP for Speech Phoneme Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/0768281a05da9f27df178b5c39a51263-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_09c6c378,
 abstract = {An application of laterally interconnected self-organizing maps (LISSOM) to handwritten digit recognition is presented. The lateral connections learn the correlations of activity between units on the map. The resulting excitatory connections focus the activity into local patches and the inhibitory connections decorrelate redundant activity on the map. The map thus forms internal representations that are easy to recognize with e.g. a perceptron network. The recognition rate on a subset of NIST database 3 is 4.0% higher with LISSOM than with a regular Self-Organizing Map (SOM) as the front end, and 15.8% higher than recognition of raw input bitmaps directly. These results form a promising starting point for building pattern recognition systems with a LISSOM map as a front end.},
 author = {Choe, Yoonsuck and Sirosh, Joseph and Miikkulainen, Risto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/09c6c3783b4a70054da74f2538ed47c6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/09c6c3783b4a70054da74f2538ed47c6-Metadata.json},
 openalex = {W2130463147},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf},
 publisher = {MIT Press},
 title = {Laterally Interconnected Self-Organizing Maps in Hand-Written Digit Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_0a0a0c8a,
 abstract = {Analog electronic cochlear models need exponentially scaled filters. CMOS Compatible Lateral Bipolar Transistors (CLBTs) can create exponentially scaled currents when biased using a resistive line with a voltage difference between both ends of the line. Since these CLBTs are independent of the CMOS threshold voltage, current sources implemented with CLBTs are much better matched than current sources created with MOS transistors operated in weak inversion. Measurements from integrated test chips are shown to verify the improved matching.},
 author = {van Schaik, Andr\'{e} and Fragni\`{e}re, Eric and Vittoz, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Metadata.json},
 openalex = {W2135709325},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf},
 publisher = {MIT Press},
 title = {Improved Silicon Cochlea using Compatible Lateral Bipolar Transistors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/0a0a0c8aaa00ade50f74a3f0ca981ed7-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_0c048b3a,
 abstract = {It has remained unknown whether one can in principle carry out reliable digital computations with networks of biologically realistic models for neurons. This article presents rigorous constructions for simulating in real-time arbitrary given boolean circuits and finite automata with arbitrarily high reliability by networks of noisy spiking neurons.

In addition we show that with the help of shunting inhibition even networks of very unreliable spiking neurons can simulate in real-time any McCulloch-Pitts neuron (or threshold gate), and therefore any multilayer perceptron (or threshold circuit) in a reliable manner. These constructions provide a possible explanation for the fact that biological neural systems can carry out quite complex computations within 100 msec.

It turns out that the assumption that these constructions require about the shape of the EPSP's and the behaviour of the noise are surprisingly weak.},
 author = {Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0c048b3a434e49e655c1247efb389cec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0c048b3a434e49e655c1247efb389cec-Metadata.json},
 openalex = {W2105239147},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0c048b3a434e49e655c1247efb389cec-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Computational Power of Noisy Spiking Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/0c048b3a434e49e655c1247efb389cec-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_0eec27c4,
 abstract = {This paper describes a policy iteration algorithm for optimizing the performance of a harmonic function-based controller with respect to a user-defined index. Value functions are represented as potential distributions over the problem domain, being control policies represented as gradient fields over the same domain. All intermediate policies are intrinsically safe, i.e. collisions are not promoted during the adaptation process. The algorithm has efficient implementation in parallel SIMD architectures. One potential application - travel distance minimization - illustrates its usefulness.},
 author = {Coelho, Jefferson and Sitaraman, R. and Grupen, Roderic},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0eec27c419d0fe24e53c90338cdc8bc6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0eec27c419d0fe24e53c90338cdc8bc6-Metadata.json},
 openalex = {W2136880235},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0eec27c419d0fe24e53c90338cdc8bc6-Paper.pdf},
 publisher = {MIT Press},
 title = {Parallel Optimization of Motion Controllers via Policy Iteration},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/0eec27c419d0fe24e53c90338cdc8bc6-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_0f2c9a93,
 abstract = {In systems that process sensory data there is frequently a model matching stage where class hypotheses are combined to recognize a complex entity. We introduce a new model of parallelism, the Single Function Multiple Data (SFMD) model, appropriate to this stage. SFMD functionality can be added with small hardware expense to certain existing SIMD architectures, and as an incremental addition to the programming model. Adding SFMD to an SIMD machine will not only allow faster model matching, but also increase its flexibility as a general purpose machine and its scope in performing the initial stages of sensory processing.},
 author = {Rehfuss, Steven and Hammerstrom, Dan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0f2c9a93eea6f38fabb3acb1c31488c6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0f2c9a93eea6f38fabb3acb1c31488c6-Metadata.json},
 openalex = {W2171263092},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf},
 publisher = {MIT Press},
 title = {Model Matching and SFMD Computation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/0f2c9a93eea6f38fabb3acb1c31488c6-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_1019c809,
 abstract = {We study the characteristics of learning with ensembles. Solving exactly the simple model of an ensemble of linear students, we find surprisingly rich behaviour. For learning in large ensembles, it is advantageous to use under-regularized students, which actually over-fit the training data. Globally optimal performance can be obtained by choosing the training set sizes of the students appropriately. For smaller ensembles, optimization of the ensemble weights can yield significant improvements in ensemble generalization performance, in particular if the individual students are subject to noise in the training process. Choosing students with a wide range of regularization parameters makes this improvement robust against changes in the unknown level of noise in the training data.},
 author = {Sollich, Peter and Krogh, Anders},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1019c8091693ef5c5f55970346633f92-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1019c8091693ef5c5f55970346633f92-Metadata.json},
 openalex = {W2158441306},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1019c8091693ef5c5f55970346633f92-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning with ensembles: How overfitting can be useful},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/1019c8091693ef5c5f55970346633f92-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_1579779b,
 abstract = {We present an integrated analog processor for real-time wavelet decomposition and reconstruction of continuous temporal signals covering the audio frequency range. The processor performs complex harmonic modulation and Gaussian lowpass filtering in 16 parallel channels, each clocked at a different rate, producing a multiresolution mapping on a logarithmic frequency scale. Our implementation uses mixed-mode analog and digital circuits, oversampling techniques, and switched-capacitor filters to achieve a wide linear dynamic range while maintaining compact circuit size and low power consumption. We include experimental results on the processor and characterize its components separately from measurements on a single-channel test chip.},
 author = {Edwards, R. and Cauwenberghs, Gert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1579779b98ce9edb98dd85606f2c119d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1579779b98ce9edb98dd85606f2c119d-Metadata.json},
 openalex = {W2111170118},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1579779b98ce9edb98dd85606f2c119d-Paper.pdf},
 publisher = {MIT Press},
 title = {Analog VLSI Processor Implementing the Continuous Wavelet Transform},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/1579779b98ce9edb98dd85606f2c119d-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_16e6a332,
 abstract = {State-of-the-art speech processors in cochlear implants perform channel selection using a spectral maxima strategy. This strategy can lead to confusions when high frequency features are needed to discriminate between sounds. We present in this paper a novel channel selection strategy based upon pattern recognition which allows smart channel selections to be made. The proposed strategy is implemented using multi-layer perceptrons trained on a multispeaker labelled speech database. The input to the network are the energy coefficients of N energy channels. The output of the system are the indices of the M selected channels.

We compare the performance of our proposed system to that of spectral maxima strategy, and show that our strategy can produce significantly better results.},
 author = {Jabri, Marwan and Wang, Raymond},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/16e6a3326dd7d868cbc926602a61e4d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/16e6a3326dd7d868cbc926602a61e4d0-Metadata.json},
 openalex = {W2114278549},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/16e6a3326dd7d868cbc926602a61e4d0-Paper.pdf},
 publisher = {MIT Press},
 title = {A Novel Channel Selection System in Cochlear Implants Using Artificial Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/16e6a3326dd7d868cbc926602a61e4d0-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_18426034,
 abstract = {The choice of an input representation for a neural network can have a profound impact on its accuracy in classifying novel instances. However, neural networks are typically computationally expensive to train, making it difficult to test large numbers of alternative representations. This paper introduces fast quality measures for neural network representations, allowing one to quickly and accurately estimate which of a collection of possible representations for a problem is the best. We show that our measures for ranking representations are more accurate than a previously published measure, based on experiments with three difficult, real-world pattern recognition problems.},
 author = {Cherkauer, Kevin and Shavlik, Jude},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/184260348236f9554fe9375772ff966e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/184260348236f9554fe9375772ff966e-Metadata.json},
 openalex = {W2169006618},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/184260348236f9554fe9375772ff966e-Paper.pdf},
 publisher = {MIT Press},
 title = {Rapid Quality Estimation of Neural Network Input Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/184260348236f9554fe9375772ff966e-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_1896a3bf,
 abstract = {Neural-network ensembles have been shown to be very accurate classification techniques. Previous work has shown that an effective ensemble should consist of networks that are not only highly correct, but ones that make their errors on different parts of the input space as well. Most existing techniques, however, only indirectly address the problem of creating such a set of networks. In this paper we present a technique called ADDEMUP that uses genetic algorithms to directly search for an accurate and diverse set of trained networks. ADDEMUP works by first creating an initial population, then uses genetic operators to continually create new networks, keeping the set of networks that are as accurate as possible while disagreeing with each other as much as possible. Experiments on three DNA problems show that ADDEMUP is able to generate a set of trained networks that is more accurate than several existing approaches. Experiments also show that ADDEMUP is able to effectively incorporate prior knowledge, if available, to improve the quality of its ensemble.},
 author = {Opitz, David and Shavlik, Jude},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1896a3bf730516dd643ba67b4c447d36-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1896a3bf730516dd643ba67b4c447d36-Metadata.json},
 openalex = {W2164544703},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1896a3bf730516dd643ba67b4c447d36-Paper.pdf},
 publisher = {MIT Press},
 title = {Generating Accurate and Diverse Members of a Neural-Network Ensemble},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/1896a3bf730516dd643ba67b4c447d36-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_1b0114c5,
 abstract = {We have recently developed a theory of spatial representations in which the position of an object is not encoded in a particular frame of reference but, instead, involves neurons computing basis functions of their sensory inputs. This type of representation is able to perform nonlinear sensorimotor transformations and is consistent with the response properties of parietal neurons. We now ask whether the same theory could account for the behavior of human patients with parietal lesions. These lesions induce a deficit known as hemineglect that is characterized by a lack of reaction to stimuli located in the hemispace contralateral to the lesion. A simulated lesion in a basis function representation was found to replicate three of the most important aspects of hemineglect: i) The models failed to cross the leftmost lines in line cancellation experiments, ii) the deficit affected multiple frames of reference and, iii) it could be object centered. These results strongly support the basis function hypothesis for spatial representations and provide a computational theory of hemineglect at the single cell level.},
 author = {Pouget, Alexandre and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1b0114c51cc532ed34e1954b5b9e4b58-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1b0114c51cc532ed34e1954b5b9e4b58-Metadata.json},
 openalex = {W2139334884},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1b0114c51cc532ed34e1954b5b9e4b58-Paper.pdf},
 publisher = {MIT Press},
 title = {A Model of Spatial Representations in Parietal Cortex Explains Hemineglect},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/1b0114c51cc532ed34e1954b5b9e4b58-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_1e1d1841,
 abstract = {A continuous-time, continuous-state version of the temporal difference (TD) algorithm is derived in order to facilitate the application of reinforcement learning to real-world control tasks and neurobiological modeling. An optimal nonlinear feedback control law was also derived using the derivatives of the value function. The performance of the algorithms was tested in a task of swinging up a pendulum with limited torque. Both the critic that specifies the paths to the upright position and the actor that works as a nonlinear feedback controller were successfully implemented by radial basis function (RBF) networks.},
 author = {Doya, Kenji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1e1d184167ca7676cf665225e236a3d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1e1d184167ca7676cf665225e236a3d2-Metadata.json},
 openalex = {W2100644648},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1e1d184167ca7676cf665225e236a3d2-Paper.pdf},
 publisher = {MIT Press},
 title = {Temporal Difference Learning in Continuous Time and Space},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/1e1d184167ca7676cf665225e236a3d2-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_1e6e0a04,
 abstract = {Backpropagation learning algorithms typically collapse the network's structure into a single vector of weight parameters to be optimized. We suggest that their performance may be improved by utilizing the structural information instead of discarding it, and introduce a framework for each weight accordingly.

In the tempering model, activation and error signals are treated as approximately independent random variables. The characteristic scale of weight changes is then matched to that of the residuals, allowing structural properties such as a node's fan-in and fan-out to affect the local learning rate and backpropagated error. The model also permits calculation of an upper bound on the global learning rate for batch updates, which in turn leads to different update rules for bias vs. non-bias weights.

This approach yields hitherto unparalleled performance on the family relations benchmark, a deep multi-layer network: for both batch learning with momentum and the delta-bar-delta algorithm, convergence at the optimal learning rate is sped up by more than an order of magnitude.},
 author = {Schraudolph, Nicol and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1e6e0a04d20f50967c64dac2d639a577-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1e6e0a04d20f50967c64dac2d639a577-Metadata.json},
 openalex = {W2127085923},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},
 publisher = {MIT Press},
 title = {Tempering Backpropagation Networks: Not All Weights are Created Equal},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/1e6e0a04d20f50967c64dac2d639a577-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_208e43f0,
 abstract = {Performing policy iteration in dynamic programming should only require knowledge of relative rather than absolute measures of the utility of actions (Werbos, 1991) - what Baird (1993) calls the advantages of actions at states. Nevertheless, most existing methods in dynamic programming (including Baird's) compute some form of absolute utility function. For smooth problems, advantages satisfy two differential consistency conditions (including the requirement that they be free of curl), and we show that enforcing these can lead to appropriate policy improvement solely in terms of advantages.},
 author = {Dayan, Peter and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/208e43f0e45c4c78cafadb83d2888cb6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/208e43f0e45c4c78cafadb83d2888cb6-Metadata.json},
 openalex = {W2112940296},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf},
 publisher = {MIT Press},
 title = {Improving Policies without Measuring Merits},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/208e43f0e45c4c78cafadb83d2888cb6-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_20b5e1cf,
 abstract = {Completely parallel object recognition is NP-complete. Achieving a recognizer with feasible complexity requires a compromise between parallel and sequential processing where a system selectively focuses on parts of a given image, one after another. Successive fixations are generated to sample the image and these samples are processed and abstracted to generate a temporal context in which results are integrated over time. A computational model based on a partially recurrent feedforward network is proposed and made credible by testing on the real-world problem of recognition of handwritten digits with encouraging results.},
 author = {Alpaydin, Ethem},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/20b5e1cf8694af7a3c1ba4a87f073021-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/20b5e1cf8694af7a3c1ba4a87f073021-Metadata.json},
 openalex = {W2135309076},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/20b5e1cf8694af7a3c1ba4a87f073021-Paper.pdf},
 publisher = {MIT Press},
 title = {Selective Attention for Handwritten Digit Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/20b5e1cf8694af7a3c1ba4a87f073021-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_20d135f0,
 abstract = {In this paper the problem of learning appropriate domain-specific bias is addressed. It is shown that this can be achieved by learning many related tasks from the same domain, and a theorem is given bounding the number tasks that must be learnt. A corollary of the theorem is that if the tasks are known to possess a common internal representation or preprocessing then the number of examples required per task for good generalisation when learning n tasks simultaneously scales like O(a + b/n), where O(a) is a bound on the minimum number of examples requred to learn a single task, and O(a + b) is a bound on the number of examples required to learn each task independently. An experiment providing strong qualitative support for the theoretical results is reported.},
 author = {Baxter, Jonathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/20d135f0f28185b84a4cf7aa51f29500-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/20d135f0f28185b84a4cf7aa51f29500-Metadata.json},
 openalex = {W2130980152},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/20d135f0f28185b84a4cf7aa51f29500-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Model Bias},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/20d135f0f28185b84a4cf7aa51f29500-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_2290a738,
 abstract = {A new nearest-neighbor method is described for estimating the Bayes risk of a multiclass pattern classification problem from sample data (e.g., a classified training set). Although it is assumed that the classification problem can be accurately described by sufficiently smooth class-conditional distributions, neither these distributions, nor the corresponding prior probabilities of the classes are required. Thus this method can be applied to practical problems where the underlying probabilities are not known. This method is illustrated using two different pattern recognition problems.},
 author = {Snapp, Robert and Xu, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2290a7385ed77cc5592dc2153229f082-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2290a7385ed77cc5592dc2153229f082-Metadata.json},
 openalex = {W2161880915},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2290a7385ed77cc5592dc2153229f082-Paper.pdf},
 publisher = {MIT Press},
 title = {Estimating the Bayes Risk from Sample Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/2290a7385ed77cc5592dc2153229f082-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_24146db4,
 abstract = {An essential feature of intelligent sensory processing is the ability to focus on the part of the signal of interest against a background of distracting signals, and to be able to direct this focus at will. In this paper the problem of auditory streaming is considered and a model of the early stages of the process is proposed. The behavior of the model is shown to be in agreement with a number of well-known psychophysical results, including the relationship between presentation rate, frequency separation and streaming, the temporal development of streaming, and the effect of background organization on streaming. The principal contribution of this model is that it demonstrates how streaming might result from interactions between the tonotopic patterns of activity of incoming signals and traces of previous activity which feed back and influence the way in which subsequent signals are processed. The significance of these results for auditory scene analysis is considered and a framework for the integration of simultaneous and sequential grouping cues in the perception of auditory objects is proposed.},
 author = {McCabe, Susan and Denham, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/24146db4eb48c718b84cae0a0799dcfc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/24146db4eb48c718b84cae0a0799dcfc-Metadata.json},
 openalex = {W1998530097},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf},
 publisher = {MIT Press},
 title = {A model of auditory streaming},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/24146db4eb48c718b84cae0a0799dcfc-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_27ed0fb9,
 abstract = {A method for incorporating context-dependent phone classes in a connectionist-HMM hybrid speech recognition system is introduced. A modular approach is adopted, where single-layer networks discriminate between different context classes given the phone class and the acoustic data. The context networks are combined with a context-independent (CI) network to generate context-dependent (CD) phone probability estimates. Experiments show an average reduction in word error rate of 16% and 13% from the CI system on ARPA 5,000 word and SQALE 20,000 word tasks respectively. Due to improved modelling, the decoding speed of the CD system is more than twice as fast as the CI system.},
 author = {Kershaw, Dan and Robinson, Anthony and Hochberg, Mike},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/27ed0fb950b856b06e1273989422e7d3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/27ed0fb950b856b06e1273989422e7d3-Metadata.json},
 openalex = {W2115353073},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/27ed0fb950b856b06e1273989422e7d3-Paper.pdf},
 publisher = {MIT Press},
 title = {Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/27ed0fb950b856b06e1273989422e7d3-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_285f89b8,
 abstract = {We develop a refined mean field approximation for inference and learning in probabilistic neural networks. Our mean field theory, unlike most, does not assume that the units behave as independent degrees of freedom; instead, it exploits in a principled way the existence of large substructures that are computationally tractable. To illustrate the advantages of this framework, we show how to incorporate weak higher order interactions into a first-order hidden Markov model, treating the corrections (but not the first order structure) within mean field theory.},
 author = {Saul, Lawrence and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/285f89b802bcb2651801455c86d78f2a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/285f89b802bcb2651801455c86d78f2a-Metadata.json},
 openalex = {W2104163628},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/285f89b802bcb2651801455c86d78f2a-Paper.pdf},
 publisher = {MIT Press},
 title = {Exploiting Tractable Substructures in Intractable Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/285f89b802bcb2651801455c86d78f2a-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_299a23a2,
 abstract = {A statistical theory for overtraining is proposed. The analysis treats realizable stochastic neural networks, trained with Kullback-Leibler loss in the asymptotic case. It is shown that the asymptotic gain in the generalization error is small if we perform early stopping, even if we have access to the optimal stopping time. Considering cross-validation stopping we answer the question: In what ratio the examples should be divided into training and testing sets in order to obtain the optimum performance. In the nonasymptotic region cross-validated early stopping always decreases the generalization error. Our large scale simulations done on a CM5 are in nice agreement with our analytical findings.},
 author = {Amari, Shun-ichi and Murata, Noboru and M\"{u}ller, Klaus-Robert and Finke, Michael and Yang, Howard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/299a23a2291e2126b91d54f3601ec162-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/299a23a2291e2126b91d54f3601ec162-Metadata.json},
 openalex = {W2120914564},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/299a23a2291e2126b91d54f3601ec162-Paper.pdf},
 publisher = {MIT Press},
 title = {Statistical Theory of Overtraining - Is Cross-Validation Asymptotically Effective?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/299a23a2291e2126b91d54f3601ec162-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_2b38c2df,
 abstract = {Infants' manipulative exploratory behavior within the environment is a vehicle of cognitive stimulation[McCall 1974]. During this time, infants practice and perfect sensorimotor patterns that become behavioral modules which will be seriated and imbedded in more complex actions. This paper explores the development of such primitive learning systems using an embodied light-weight hand which will be used for a humanoid being developed at the MIT Artificial Intelligence Laboratory [Brooks and Stein 1993]. Primitive grasping procedures are learned from sensory inputs using a connectionist reinforcement algorithm while two submodules preprocess sensory data to recognize the hardness of objects and detect shear using competitive learning and back-propagation algorithm strategies, respectively. This system is not only consistent and quick during the initial learning stage, but also adaptable to new situations after training is completed.},
 author = {Matsuoka, Yoky},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2b38c2df6a49b97f706ec9148ce48d86-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2b38c2df6a49b97f706ec9148ce48d86-Metadata.json},
 openalex = {W2154589016},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf},
 publisher = {MIT Press},
 title = {Primitive Manipulation Learning with Connectionism},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/2b38c2df6a49b97f706ec9148ce48d86-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_2cfd4560,
 abstract = {We are developing special-purpose, low-power analog-to-digital converters for speech and music applications, that feature analog circuit models of biological audition to process the audio signal before conversion. This paper describes our most recent converter design, and a working system that uses several copies of the chip to compute multiple representations of sound from an analog input. This multi-representation system demonstrates the plausibility of inexpensively implementing an auditory scene analysis approach to sound processing.},
 author = {Lazzaro, John and Wawrzynek, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2cfd4560539f887a5e420412b370b361-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2cfd4560539f887a5e420412b370b361-Metadata.json},
 openalex = {W2112705921},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2cfd4560539f887a5e420412b370b361-Paper.pdf},
 publisher = {MIT Press},
 title = {Silicon Models for Auditory Scene Analysis.},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/2cfd4560539f887a5e420412b370b361-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_2f29b6e3,
 abstract = {We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with another state-of-the-art face detection system are presented; our system has better performance in terms of detection and false-positive rates.},
 author = {Rowley, Henry and Baluja, Shumeet and Kanade, Takeo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2f29b6e3abc6ebdefb55456ea6ca5dc8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2f29b6e3abc6ebdefb55456ea6ca5dc8-Metadata.json},
 openalex = {W2125713050},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/2f29b6e3abc6ebdefb55456ea6ca5dc8-Paper.pdf},
 publisher = {MIT Press},
 title = {Human Face Detection in Visual Scenes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/2f29b6e3abc6ebdefb55456ea6ca5dc8-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_31857b44,
 abstract = {We report on the development of a high-performance system for neural network and other signal processing applications. We have designed and implemented a vector microprocessor and packaged it as an attached processor for a conventional workstation. We present performance comparisons with workstations on neural network backpropagation training. The SPERT-II system demonstrates roughly 15 times the performance of a mid-range workstation and five times the performance of a high-end workstation with extensive hand-optimization of both workstation versions.},
 author = {Wawrzynek, John and Asanovic, Krste and Kingsbury, Brian and Beck, James and Johnson, David and Morgan, Nelson},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/31857b449c407203749ae32dd0e7d64a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/31857b449c407203749ae32dd0e7d64a-Metadata.json},
 openalex = {W2163050299},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/31857b449c407203749ae32dd0e7d64a-Paper.pdf},
 publisher = {MIT Press},
 title = {SPERT-II: a vector microprocessor system and its application to large problems in backpropagation training},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/31857b449c407203749ae32dd0e7d64a-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_3473decc,
 author = {Pedersen, Morten and Hansen, Lars and Larsen, Jan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3473decccb0509fb264818a7512a8b9b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3473decccb0509fb264818a7512a8b9b-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3473decccb0509fb264818a7512a8b9b-Paper.pdf},
 publisher = {MIT Press},
 title = {Pruning with generalization based weight saliencies: \lambda OBD, \lambda OBS},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/3473decccb0509fb264818a7512a8b9b-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_36a1694b,
 abstract = {We investigate the effectiveness of stochastic hillclimbing as a baseline for evaluating the performance of genetic algorithms (GAs) as combinatorial function optimizers. In particular, we address two problems to which GAs have been applied in the literature: Koza's 11-multiplexer problem and the jobshop problem. We demonstrate that simple stochastic hillclimbing methods are able to achieve results comparable or superior to those obtained by the GAs designed to address these two problems. We further illustrate, in the case of the jobshop problem, how insights obtained in the formulation of a stochastic hillclimbing algorithm can lead to improvements in the encoding used by a GA.},
 author = {Juels, Ari and Wattenberg, Martin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/36a1694bce9815b7e38a9dad05ad42e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/36a1694bce9815b7e38a9dad05ad42e0-Metadata.json},
 openalex = {W2124403533},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/36a1694bce9815b7e38a9dad05ad42e0-Paper.pdf},
 publisher = {MIT Press},
 title = {Stochastic Hillclimbing as a Baseline Method for Evaluating Genetic Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/36a1694bce9815b7e38a9dad05ad42e0-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_36a16a25,
 abstract = {A patient visits the doctor; the doctor reviews the patient's history, asks questions, makes basic measurements (blood pressure, ...), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk--patients at higher risk are given more and faster attention. It is also sequential--it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable.},
 author = {Caruana, Rich and Baluja, Shumeet and Mitchell, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/36a16a2505369e0c922b6ea7a23a56d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/36a16a2505369e0c922b6ea7a23a56d2-Metadata.json},
 openalex = {W2145365639},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/36a16a2505369e0c922b6ea7a23a56d2-Paper.pdf},
 publisher = {MIT Press},
 title = {Using the Future to "Sort Out" the Present: Rankprop and Multitask Learning for Medical Risk Evaluation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/36a16a2505369e0c922b6ea7a23a56d2-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_3806734b,
 abstract = {We show that for a single neuron with the logistic function as the transfer function the number of local minima of the error function based on the square loss can grow exponentially in the dimension.},
 author = {Auer, Peter and Herbster, Mark and Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3806734b256c27e41ec2c6bffa26d9e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3806734b256c27e41ec2c6bffa26d9e7-Metadata.json},
 openalex = {W2101762657},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3806734b256c27e41ec2c6bffa26d9e7-Paper.pdf},
 publisher = {MIT Press},
 title = {Exponentially many local minima for single neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/3806734b256c27e41ec2c6bffa26d9e7-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_38ca8956,
 abstract = {A new learning algorithm is developed for the design of statistical classifiers minimizing the rate of misclassification. The method, which is based on ideas from information theory and analogies to statistical physics, assigns data to classes in probability. The distributions are chosen to minimize the expected classification error while simultaneously enforcing the classifier's structure and a level of randomness measured by Shannon's entropy. Achievement of the classifier structure is quantified by an associated cost. The constrained optimization problem is equivalent to the minimization of a Helmholtz free energy, and the resulting optimization method is a basic extension of the deterministic annealing algorithm that explicitly enforces structural constraints on assignments while reducing the entropy and expected cost with temperature. In the limit of low temperature, the error rate is minimized directly and a hard classifier with the requisite structure is obtained. This learning algorithm can be used to design a variety of classifier structures. The approach is compared with standard methods for radial basis function design and is demonstrated to substantially outperform other design methods on several benchmark examples, while often retaining design complexity comparable to, or only moderately greater than that of strict descent-based methods.},
 author = {Miller, David J and Rao, Ajit and Rose, Kenneth and Gersho, Allen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/38ca89564b2259401518960f7a06f94b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/38ca89564b2259401518960f7a06f94b-Metadata.json},
 openalex = {W2100727305},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/38ca89564b2259401518960f7a06f94b-Paper.pdf},
 publisher = {MIT Press},
 title = {An Information-theoretic Learning Algorithm for Neural Network Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/38ca89564b2259401518960f7a06f94b-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_390e9825,
 abstract = {This paper describes the application of reinforcement learning (RL) to the difficult real world problem of elevator dispatching. The elevator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems. Their states are not fully observable and they are nonstationary due to changing passenger arrival rates. In addition, we use a team of RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility.},
 author = {Crites, Robert and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/390e982518a50e280d8e2b535462ec1f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/390e982518a50e280d8e2b535462ec1f-Metadata.json},
 openalex = {W2117341272},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf},
 publisher = {MIT Press},
 title = {Improving Elevator Performance Using Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/390e982518a50e280d8e2b535462ec1f-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_3a15c7d0,
 abstract = {In recent years, the interest of investors has shifted to computerized asset allocation (portfolio management) to exploit the growing dynamics of the capital markets. In this paper, asset allocation is formalized as a Markovian Decision Problem which can be optimized by applying dynamic programming or reinforcement learning based algorithms. Using an artificial exchange rate, the asset allocation strategy optimized with reinforcement learning (Q-Learning) is shown to be equivalent to a policy computed by dynamic programming. The approach is then tested on the task to invest liquid capital in the German stock market. Here, neural networks are used as value function approximators. The resulting asset allocation strategy is superior to a heuristic benchmark policy. This is a further example which demonstrates the applicability of neural network based reinforcement learning to a problem setting with a high dimensional state space.},
 author = {Neuneier, Ralph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3a15c7d0bbe60300a39f76f8a5ba6896-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3a15c7d0bbe60300a39f76f8a5ba6896-Metadata.json},
 openalex = {W2106214155},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimal Asset Allocation using Adaptive Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_3b712de4,
 abstract = {In this paper, recursive estimation algorithms for dynamic modular networks are developed. The models are based on Gaussian RBF networks and the gating network is considered in two stages: At first, it is simply a time-varying scalar and in the second, it is based on the state, as in the mixture of local experts scheme. The resulting algorithm uses Kalman filter estimation for the model estimation and the gating probability estimation. Both, 'hard' and 'soft' competition based estimation schemes are developed where in the former, the most probable network is adapted and in the latter all networks are adapted by appropriate weighting of the data.},
 author = {Kadirkamanathan, Visakan and Kadirkamanathan, Maha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3b712de48137572f3849aabd5666a4e3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3b712de48137572f3849aabd5666a4e3-Metadata.json},
 openalex = {W2135259393},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf},
 publisher = {MIT Press},
 title = {Recursive Estimation of Dynamic Modular RBF Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/3b712de48137572f3849aabd5666a4e3-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_3eb71f62,
 author = {Platt, John and Allen, Timothy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3eb71f6293a2a31f3569e10af6552658-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3eb71f6293a2a31f3569e10af6552658-Metadata.json},
 openalex = {W2097507475},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3eb71f6293a2a31f3569e10af6552658-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neural Network Classifier for the I100 OCR Chip},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/3eb71f6293a2a31f3569e10af6552658-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_3fe78a8a,
 abstract = {Visual cognition depends critically on the ability to make rapid eye movements known as saccades that orient the fovea over targets of interest in a visual scene. Saccades are known to be ballistic: the pattern of muscle activation for foveating a prespecified target location is computed prior to the movement and visual feedback is precluded. Despite these distinctive properties, there has been no general model of the saccadic targeting strategy employed by the human visual system during visual search in natural scenes. This paper proposes a model for saccadic targeting that uses iconic scene representations derived from oriented spatial filters at multiple scales. Visual search proceeds in a coarse-to-fine fashion with the largest scale filter responses being compared first. The model was empirically tested by comparing its performance with actual eye movement data from human subjects in a natural visual search task; preliminary results indicate substantial agreement between eye movements predicted by the model and those recorded from human subjects.},
 author = {Rao, Rajesh and Zelinsky, Gregory and Hayhoe, Mary and Ballard, Dana},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3fe78a8acf5fda99de95303940a2420c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3fe78a8acf5fda99de95303940a2420c-Metadata.json},
 openalex = {W2143938301},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling Saccadic Targeting in Visual Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/3fe78a8acf5fda99de95303940a2420c-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_41bfd20a,
 abstract = {Despite the phylogenic and structural differences, the visual systems of different species, whether vertebrate or invertebrate, share certain functional properties. The center-surround opponent receptive field (CSRF) mechanism represents one such example. Here, analogous CSRFs are shown to be formed in an artificial neural network which learns to localize contours (edges) of the luminance difference. Furthermore, when the input pattern is corrupted by a background noise, the CSRFs of the hidden units becomes shallower and broader with decrease of the signal-to-noise ratio (SNR). The same kind of SNR-dependent plasticity is present in the CSRF of real visual neurons; in bipolar cells of the carp retina as is shown here experimentally, as well as in large monopolar cells of the fly compound eye as was described by others. Also, analogous SNR-dependent plasticity is shown to be present in the biphasic flash responses (BPFR) of these artificial and biological visual systems. Thus, the spatial (CSRF) and temporal (BPFR) filtering properties with which a wide variety of creatures see the world appear to be optimized for detectability of changes in space and time.},
 author = {Yasui, S. and Furukawa, T. and Yamada, M. and Saito, T.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/41bfd20a38bb1b0bec75acf0845530a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/41bfd20a38bb1b0bec75acf0845530a7-Metadata.json},
 openalex = {W2132122247},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf},
 publisher = {MIT Press},
 title = {Plasticity of Center-Surround Opponent Receptive Fields in Real and Artificial Neural Systems of Vision},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_43baa676,
 abstract = {We have developed a foveated gesture recognition system that runs in an unconstrained office environment with an active camera. Using vision routines previously implemented for an interactive environment, we determine the spatial location of salient body parts of a user and guide an active camera to obtain images of gestures or expressions. A hidden-state reinforcement learning paradigm is used to implement visual attention. The attention module selects targets to foveate based on the goal of successful recognition, and uses a new multiple-model Q-learning formulation. Given a set of target and distractor gestures, our system can learn where to foveate to maximally discriminate a particular gesture.},
 author = {Darrell, Trevor and Pentland, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/43baa6762fa81bb43b39c62553b2970d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/43baa6762fa81bb43b39c62553b2970d-Metadata.json},
 openalex = {W2134429682},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/43baa6762fa81bb43b39c62553b2970d-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Gesture Recognition using Learned Visual Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/43baa6762fa81bb43b39c62553b2970d-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_43dd49b4,
 abstract = {Linear threshold elements are the basic building blocks of artificial neural networks. A linear threshold element computes a function that is a sign of a weighted sum of the input variables. The weights are arbitrary integers; actually, they can be very big integers--exponential in the number of the input variables. However, in practice, it is difficult to implement big weights. In the present literature a distinction is made between the two extreme cases: linear threshold functions with polynomial-size weights as opposed to those with exponential-size weights. The main contribution of this paper is to fill up the gap by further refining that separation. Namely, we prove that the class of linear threshold functions with polynomial-size weights can be divided into subclasses according to the degree of the polynomial. In fact, we prove a more general result--that there exists a minimal weight linear threshold function for any arbitrary number of inputs and any weight size. To prove those results we have developed a novel technique for constructing linear threshold functions with minimal weights.},
 author = {Bohossian, Vasken and Bruck, Jehoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/43dd49b4fdb9bede653e94468ff8df1e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/43dd49b4fdb9bede653e94468ff8df1e-Metadata.json},
 openalex = {W1568278406},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/43dd49b4fdb9bede653e94468ff8df1e-Paper.pdf},
 publisher = {MIT Press},
 title = {On Neural Networks with Minimal Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/43dd49b4fdb9bede653e94468ff8df1e-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_456ac9b0,
 abstract = {This paper shows that neural networks which use continuous activation functions have VC dimension at least as large as the square of the number of weightsw. This results settles a long-standing open question, namely whether the well-knownO(w log w) bound, known for hard-threshold nets, also held for more general sigmoidal nets. Implications for the number of samples needed for valid generalization are discussed.},
 author = {Koiran, Pascal and Sontag, Eduardo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/456ac9b0d15a8b7f1e71073221059886-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/456ac9b0d15a8b7f1e71073221059886-Metadata.json},
 openalex = {W2050946712},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/456ac9b0d15a8b7f1e71073221059886-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Networks with Quadratic VC Dimension},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/456ac9b0d15a8b7f1e71073221059886-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_4588e674,
 author = {Ghahramani, Zoubin and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4588e674d3f0faf985047d4c3f13ed0d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4588e674d3f0faf985047d4c3f13ed0d-Metadata.json},
 openalex = {W4245655784},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf},
 publisher = {MIT Press},
 title = {Factorial Hidden Markov Models.},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/4588e674d3f0faf985047d4c3f13ed0d-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_45f31d16,
 abstract = {A significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans. We present a novel algorithm, TREPAN, for extracting comprehensible, symbolic representations from trained neural networks. Our algorithm uses queries to induce a decision tree that approximates the concept represented by a given network. Our experiments demonstrate that TREPAN is able to produce decision trees that maintain a high level of fidelity to their respective networks while being comprehensible and accurate. Unlike previous work in this area, our algorithm is general in its applicability and scales well to large networks and problems with high-dimensional input spaces.},
 author = {Craven, Mark and Shavlik, Jude},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/45f31d16b1058d586fc3be7207b58053-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/45f31d16b1058d586fc3be7207b58053-Metadata.json},
 openalex = {W2113882472},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf},
 publisher = {MIT Press},
 title = {Extracting Tree-Structured Representations of Trained Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/45f31d16b1058d586fc3be7207b58053-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_46072631,
 abstract = {Central to the performance improvement of a committee relative to individual networks is the error correlation between networks in the committee. We investigated methods of achieving error independence between the networks by training the networks with different resampling sets from the original training set. The methods were tested on the sinwave artificial task and the real-world problems of hepatoma (liver cancer) and breast cancer diagnoses.},
 author = {Parmanto, Bambang and Munro, Paul and Doyle, Howard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/46072631582fc240dd2674a7d063b040-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/46072631582fc240dd2674a7d063b040-Metadata.json},
 openalex = {W2098191394},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/46072631582fc240dd2674a7d063b040-Paper.pdf},
 publisher = {MIT Press},
 title = {Improving Committee Diagnosis with Resampling Techniques},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/46072631582fc240dd2674a7d063b040-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_47a65822,
 abstract = {Recently, several researchers have reported encouraging experimental results when using Gaussian or bump-like activation functions in multilayer perceptrons. Networks of this type usually require fewer hidden layers and units and often learn much faster than typical sigmoidal networks. To explain these results we consider a hyper-ridge network, which is a simple perceptron with no hidden units and a ridge activation function. If we are interested in partitioning p points in d dimensions into two classes then in the limit as d approaches infinity the capacity of a hyper-ridge and a perceptron is identical. However, we show that for p ≫ d, which is the usual case in practice, the ratio of hyper-ridge to perceptron dichotomies approaches p/2(d + 1).},
 author = {Flake, Gary},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/47a658229eb2368a99f1d032c8848542-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/47a658229eb2368a99f1d032c8848542-Metadata.json},
 openalex = {W2120897304},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/47a658229eb2368a99f1d032c8848542-Paper.pdf},
 publisher = {MIT Press},
 title = {The Capacity of a Bump},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/47a658229eb2368a99f1d032c8848542-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_4a08142c,
 abstract = {We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning.},
 author = {Drucker, Harris and Cortes, Corinna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4a08142c38dbe374195d41c04562d9f8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4a08142c38dbe374195d41c04562d9f8-Metadata.json},
 openalex = {W2104364170},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4a08142c38dbe374195d41c04562d9f8-Paper.pdf},
 publisher = {MIT Press},
 title = {Boosting Decision Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/4a08142c38dbe374195d41c04562d9f8-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_4a213d37,
 abstract = {In this paper we consider probabilities of different asymptotics of convergent unlearning algorithm for the Hopfield-type neural network (Plakhov & Semenov, 1994) treating the case of unbiased random patterns. We show also that failed unlearning results in total memory breakdown.},
 author = {Semenov, Serguei and Shuvalova, Irina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4a213d37242bdcad8e7300e202e7caa4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4a213d37242bdcad8e7300e202e7caa4-Metadata.json},
 openalex = {W2144655316},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4a213d37242bdcad8e7300e202e7caa4-Paper.pdf},
 publisher = {MIT Press},
 title = {Some results on convergent unlearning algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/4a213d37242bdcad8e7300e202e7caa4-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_4ca82782,
 abstract = {This paper describes the training of a recurrent neural network as the letter posterior probability estimator for a hidden Markov model, off-line handwriting recognition system. The network estimates posterior distributions for each of a series of frames representing sections of a handwritten word. The supervised training algorithm, backpropagation through time, requires target outputs to be provided for each frame. Three methods for deriving these targets are presented. A novel method based upon the forward-backward algorithm is found to result in the recognizer with the lowest error rate.},
 author = {Senior, Andrew W and Robinson, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4ca82782c5372a547c104929f03fe7a9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4ca82782c5372a547c104929f03fe7a9-Metadata.json},
 openalex = {W2117141689},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf},
 publisher = {MIT Press},
 title = {Forward-backward retraining of recurrent neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/4ca82782c5372a547c104929f03fe7a9-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_4da04049,
 author = {Shustorovich, Alexander and Thrasher, Christopher W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4da04049a062f5adfe81b67dd755cecc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4da04049a062f5adfe81b67dd755cecc-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf},
 publisher = {MIT Press},
 title = {KODAK lMAGELINK\texttrademark OCR Alphanumeric Handprint Module},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/4da04049a062f5adfe81b67dd755cecc-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_4e2545f8,
 abstract = {In this paper, we propose a memory-based Q-Iearning algorithm called predictive Q-routing (PQ-routing) for adaptive traffic control. We attempt to address two problems encountered in Q-routing (Boyan & Littman, 1994), namely, the inability to fine-tune routing policies under low network load and the inability to learn new optimal policies under decreasing load conditions. Unlike other memory-based reinforcement learning algorithms in which memory is used to keep past experiences to increase learning speed, PQ-routing keeps the best experiences learned and reuses them by predicting the traffic trend. The effectiveness of PQ-routing has been verified under various network topologies and traffic conditions. Simulation results show that PQ-routing is superior to Q-routing in terms of both learning speed and adaptability.},
 author = {Choi, Samuel and Yeung, Dit-Yan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4e2545f819e67f0615003dd7e04a6087-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4e2545f819e67f0615003dd7e04a6087-Metadata.json},
 openalex = {W2144658257},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4e2545f819e67f0615003dd7e04a6087-Paper.pdf},
 publisher = {MIT Press},
 title = {Predictive Q-Routing: A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/4e2545f819e67f0615003dd7e04a6087-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_4f16c818,
 author = {Zhang, Wei and Dietterich, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4f16c818875d9fcb6867c7bdc89be7eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4f16c818875d9fcb6867c7bdc89be7eb-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/4f16c818875d9fcb6867c7bdc89be7eb-Paper.pdf},
 publisher = {MIT Press},
 title = {High-Performance Job-Shop Scheduling With A Time-Delay TD(\lambda ) Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/4f16c818875d9fcb6867c7bdc89be7eb-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_5055cbf4,
 abstract = {Family is the task of learning the dimension and structure of a parameterized family of stochastic models. It is especially appropriate when the training examples are partitioned into episodes of samples drawn from a single parameter value. We present three family discovery algorithms based on surface learning and show that they significantly improve performance over two alternatives on a parameterized classification task.},
 author = {Omohundro, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/5055cbf43fac3f7e2336b27310f0b9ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/5055cbf43fac3f7e2336b27310f0b9ef-Metadata.json},
 openalex = {W2294330583},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/5055cbf43fac3f7e2336b27310f0b9ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Family Discovery},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/5055cbf43fac3f7e2336b27310f0b9ef-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_522a9ae9,
 author = {Redish, A. and Touretzky, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/522a9ae9a99880d39e5daec35375e999-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/522a9ae9a99880d39e5daec35375e999-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/522a9ae9a99880d39e5daec35375e999-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling Interactions of the Rat\textquotesingle s Place and Head Direction Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/522a9ae9a99880d39e5daec35375e999-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_537d9b6c,
 abstract = {No finite sample is sufficient to determine the density, and therefore the entropy, of a signal directly. Some assumption about either the functional form of the density or about its smoothness is necessary. Both amount to a prior over the space of possible density functions. By far the most common approach is to assume that the density has a parametric form.

By contrast we derive a differential learning rule called EMMA that optimizes entropy by way of kernel density estimation. Entropy and its derivative can then be calculated by sampling from this density estimate. The resulting parameter update rule is surprisingly simple and efficient.

We will show how EMMA can be used to detect and correct corruption in magnetic resonance images (MRI). This application is beyond the scope of existing parametric entropy models.},
 author = {Viola, Paul and Schraudolph, Nicol and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/537d9b6c927223c796cac288cced29df-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/537d9b6c927223c796cac288cced29df-Metadata.json},
 openalex = {W2126678392},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/537d9b6c927223c796cac288cced29df-Paper.pdf},
 publisher = {MIT Press},
 title = {Empirical Entropy Manipulation for Real-World Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/537d9b6c927223c796cac288cced29df-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_53adaf49,
 abstract = {A neural network model of 3-D lightness perception is presented which builds upon the FACADE Theory Boundary Contour System/ Feature Contour System of Grossberg and colleagues. Early ratio encoding by retinal ganglion neurons as well as psychophysical results on constancy across different backgrounds (background constancy) are used to provide functional constraints to the theory and suggest a contrast negation hypothesis which states that ratio measures between coplanar regions are given more weight in the determination of lightness of the respective regions. Simulations of the model address data on lightness perception, including the coplanar ratio hypothesis, the Benary cross, and White's illusion.},
 author = {Pessoa, Luiz and Ross, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/53adaf494dc89ef7196d73636eb2451b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/53adaf494dc89ef7196d73636eb2451b-Metadata.json},
 openalex = {W2097692712},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neural Network Model of 3-D Lightness Perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/53adaf494dc89ef7196d73636eb2451b-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_53c04118,
 abstract = {Both vertebrate and invertebrate retinas are highly efficient in extracting contrast independent of the background intensity over five or more decades. This efficiency has been rendered possible by the adaptation of the DC operating point to the background intensity while maintaining high gain transient responses. The center-surround properties of the retina allows the system to extract information at the edges in the image. This silicon retina models the adaptation properties of the receptors and the antagonistic center-surround properties of the laminar cells of the invertebrate retina and the outer-plexiform layer of the vertebrate retina. We also illustrate the spatio-temporal responses of the silicon retina on moving bars. The chip has 59×64 pixels on a 6.9×6.8mm2 die and it is fabricated in 2 µm n-well technology.},
 author = {Liu, Shih-Chii and Boahen, Kwabena},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/53c04118df112c13a8c34b38343b9c10-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/53c04118df112c13a8c34b38343b9c10-Metadata.json},
 openalex = {W2112980516},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive Retina with Center-Surround Receptive Field},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_55b1927f,
 abstract = {The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a relatively efficient method of fitting a multilayer stochastic generative model to high-dimensional data. In addition to the top-down connections in the generative model, it makes use of bottom-up connections for approximating the probability distribution over the hidden units given the data, and it trains these bottom-up connections using a simple delta rule. We use a variety of synthetic and real data sets to compare the performance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit.},
 author = {Frey, Brendan J and Hinton, Geoffrey E and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/55b1927fdafef39c48e5b73b5d61ea60-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/55b1927fdafef39c48e5b73b5d61ea60-Metadata.json},
 openalex = {W2170858534},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf},
 publisher = {MIT Press},
 title = {Does the Wake-sleep Algorithm Produce Good Density Estimators?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/55b1927fdafef39c48e5b73b5d61ea60-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_571e0f7e,
 abstract = {There is currently considerable interest in developing general nonlinear density models based on latent, or hidden, variables. Such models have the ability to discover the presence of a relatively small number of underlying 'causes' which, acting in combination, give rise to the apparent complexity of the observed data set. Unfortunately, to train such models generally requires large computational effort. In this paper we introduce a novel latent variable algorithm which retains the general non-linear capabilities of previous models but which uses a training procedure based on the EM algorithm. We demonstrate the performance of the model on a toy problem and on data from flow diagnostics for a multi-phase oil pipeline.},
 author = {Bishop, Christopher and Svens\'{e}n, Markus and Williams, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/571e0f7e2d992e738adff8b1bd43a521-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/571e0f7e2d992e738adff8b1bd43a521-Metadata.json},
 openalex = {W2138265369},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/571e0f7e2d992e738adff8b1bd43a521-Paper.pdf},
 publisher = {MIT Press},
 title = {EM Optimization of Latent-Variable Density Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/571e0f7e2d992e738adff8b1bd43a521-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_58c54802,
 abstract = {We describe the use of modern analytical techniques in solving the dynamics of symmetric and nonsymmetric recurrent neural networks near saturation. These explicitly take into account the correlations between the post-synaptic potentials, and thereby allow for a reliable prediction of transients.},
 author = {Coolen, A.C.C. and Laughton, S. and Sherrington, D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/58c54802a9fb9526cd0923353a34a7ae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/58c54802a9fb9526cd0923353a34a7ae-Metadata.json},
 openalex = {W2117434052},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf},
 publisher = {MIT Press},
 title = {Modern Analytic Techniques to Solve the Dynamics of Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/58c54802a9fb9526cd0923353a34a7ae-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_678a1491,
 abstract = {In this paper we examine a perceptron learning task. The task is realizable since it is provided by another perceptron with identical architecture. Both perceptrons have nonlinear sigmoid output functions. The gain of the output function determines the level of nonlinearity of the learning task. It is observed that a high level of nonlinearity leads to overfitting. We give an explanation for this rather surprising observation and develop a method to avoid the overfitting. This method has two possible interpretations, one is learning with noise, the other cross-validated early stopping.},
 author = {B\"{o}s, Siegfried},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/678a1491514b7f1006d605e9161946b1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/678a1491514b7f1006d605e9161946b1-Metadata.json},
 openalex = {W2150029999},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/678a1491514b7f1006d605e9161946b1-Paper.pdf},
 publisher = {MIT Press},
 title = {A Realizable Learning Task which Exhibits Overfitting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/678a1491514b7f1006d605e9161946b1-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_69a5b599,
 abstract = {Several regions of the rat brain contain neurons known as head-direction cells, which encode the animal's directional heading during spatial navigation. This paper presents a biophysical model of head-direction cell activity, which suggests that a thalamocortical circuit might compute the rat's head direction by integrating the angular velocity of the head over time. The model was implemented using the neural simulator NEURON, and makes testable predictions about the structure and function of the rat head-direction circuit.},
 author = {Blair, Hugh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/69a5b5995110b36a9a347898d97a610e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/69a5b5995110b36a9a347898d97a610e-Metadata.json},
 openalex = {W2125647716},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf},
 publisher = {MIT Press},
 title = {Simulation of a Thalamocortical Circuit for Computing Directional Heading in the Rat},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/69a5b5995110b36a9a347898d97a610e-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_6a2feef8,
 abstract = {A new approach for clustering is proposed. This method is based on an analogy to a physical model; the ferromagnetic Potts model at thermal equilibrium is used as an analog computer for this hard optimization problem. We do not assume any structure of the underlying distribution of the data. Phase space of the Potts model is divided into three regions; ferromagnetic, super-paramagnetic and paramagnetic phases. The region of interest is that corresponding to the super-paramagnetic one, where domains of aligned spins appear. The range of temperatures where these structures are stable is indicated by a non-vanishing magnetic susceptibility. We use a very efficient Monte Carlo algorithm to measure the susceptibility and the spin spin correlation function. The values of the spin spin correlation function, at the super-paramagnetic phase, serve to identify the partition of the data points into clusters.},
 author = {Blatt, Marcelo and Wiseman, Shai and Domany, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Metadata.json},
 openalex = {W2148686234},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6a2feef8ed6a9fe76d6b3f30f02150b4-Paper.pdf},
 publisher = {MIT Press},
 title = {Clustering data through an analogy to the Potts model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/6a2feef8ed6a9fe76d6b3f30f02150b4-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_6c340f25,
 abstract = {Compliant control is a standard method for performing fine manipulation tasks, like grasping and assembly, but it requires estimation of the state of contact between the robot arm and the objects involved. Here we present a method to learn a model of the movement from measured data. The method requires little or no prior knowledge and the resulting model explicitly estimates the state of contact. The current state of contact is viewed as the hidden state variable of a discrete HMM. The control dependent transition probabilities between states are modeled as parametrized functions of the measurement We show that their parameters can be estimated from measurements concurrently with the estimation of the parameters of the movement in each state of contact. The learning algorithm is a variant of the EM procedure. The E step is computed exactly; solving the M step exactly would require solving a set of coupled nonlinear algebraic equations in the parameters. Instead, gradient ascent is used to produce an increase in likelihood.},
 author = {Meila, Marina and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6c340f25839e6acdc73414517203f5f0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6c340f25839e6acdc73414517203f5f0-Metadata.json},
 openalex = {W2120603682},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6c340f25839e6acdc73414517203f5f0-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Fine Motion by Markov Mixtures of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/6c340f25839e6acdc73414517203f5f0-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_6d70cb65,
 abstract = {We consider the solution to large stochastic control problems by means of methods that rely on compact representations and a variant of the value iteration algorithm to compute approximate cost-togo functions. While such methods are known to be unstable in general, we identify a new class of problems for which convergence, as well as graceful error bounds, are guaranteed. This class involves linear parameterizations of the cost-to-go function together with an assumption that the dynamic programming operator is a contraction with respect to the Euclidean norm when applied to functions in the parameterized class. We provide a special case where this assumption is satisfied, which relies on the locality of transitions in a state space. Other cases will be discussed in a full length version of this paper.},
 author = {Van Roy, Benjamin and Tsitsiklis, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6d70cb65d15211726dcce4c0e971e21c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6d70cb65d15211726dcce4c0e971e21c-Metadata.json},
 openalex = {W2115849850},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf},
 publisher = {MIT Press},
 title = {Stable LInear Approximations to Dynamic Programming for Stochastic Control Problems with Local Transitions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/6d70cb65d15211726dcce4c0e971e21c-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_6e7d2da6,
 abstract = {We propose a way of using boolean circuits to perform real valued computation in a way that naturally extends their boolean functionality. The functionality of multiple fan in threshold gates in this model is shown to mimic that of a hardware implementation of continuous Neural Networks. A Vapnik-Chervonenkis dimension and sample size analysis for the systems is performed giving best known sample sizes for a real valued Neural Network. Experimental results confirm the conclusion that the sample sizes required for the networks are significantly smaller than for sigmoidal networks.},
 author = {Shawe-Taylor, John and Zhao, Jieyu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6e7d2da6d3953058db75714ac400b584-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6e7d2da6d3953058db75714ac400b584-Metadata.json},
 openalex = {W2129887682},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalisation of A Class of Continuous Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/6e7d2da6d3953058db75714ac400b584-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_708f3cf8,
 abstract = {We report on the development of the modular neural system SEE-EAGLE for the visual guidance of robot pick-and-place actions. Several neural networks are integrated to a single system that visually recognizes human hand pointing gestures from stereo pairs of color video images. The output of the hand recognition stage is processed by a set of color-sensitive neural networks to determine the cartesian location of the target object that is referenced by the pointing gesture. Finally, this information is used to guide a robot to grab the target object and put it at another location that can be specified by a second pointing gesture. The accuracy of the current system allows to identify the location of the referenced target object to an accuracy of 1 cm in a workspace area of 50×50 cm. In our current environment, this is sufficient to pick and place arbitrarily positioned target objects within the workspace. The system consists of neural networks that perform the tasks of image segmentation, estimation of hand location, estimation of 3D-pointing direction, object recognition, and necessary coordinate transforms. Drawing heavily on the use of learning algorithms, the functions of all network modules were created from data examples only.},
 author = {Littmann, Enno and Drees, Andrea and Ritter, Helge},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/708f3cf8100d5e71834b1db77dfa15d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/708f3cf8100d5e71834b1db77dfa15d6-Metadata.json},
 openalex = {W2110730726},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/708f3cf8100d5e71834b1db77dfa15d6-Paper.pdf},
 publisher = {MIT Press},
 title = {Visual gesture-based robot guidance with a modular neural system},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/708f3cf8100d5e71834b1db77dfa15d6-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_731c83db,
 abstract = {Statistically independent features can be extracted by finding a factorial representation of a signal distribution. Principal Component Analysis (PCA) accomplishes this for linear correlated and Gaussian distributed signals. Independent Component Analysis (ICA), formalized by Comon (1994), extracts features in the case of linear statistical dependent but not necessarily Gaussian distributed signals. Nonlinear Component Analysis finally should find a factorial representation for nonlinear statistical dependent distributed signals. This paper proposes for this task a novel feed-forward, information conserving, nonlinear map - the explicit symplectic transformations. It also solves the problem of non-Gaussian output distributions by considering single coordinate higher order statistics.},
 author = {Parra, Lucas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/731c83db8d2ff01bdc000083fd3c3740-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/731c83db8d2ff01bdc000083fd3c3740-Metadata.json},
 openalex = {W2163718582},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf},
 publisher = {MIT Press},
 title = {Symplectic Nonlinear Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/731c83db8d2ff01bdc000083fd3c3740-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_754dda4b,
 abstract = {Because of the distance between the skull and brain and their different resistivities, electroencephalographic (EEG) data collected from any point on the human scalp includes activity generated within a large brain area. This spatial smearing of EEG data by volume conduction does not involve significant time delays, however, suggesting that the Independent Component Analysis (ICA) algorithm of Bell and Sejnowski [1] is suitable for performing blind source separation on EEG data. The ICA algorithm separates the problem of source identification from that of source localization. First results of applying the ICA algorithm to EEG and event-related potential (ERP) data collected during a sustained auditory detection task show: (1) ICA training is insensitive to different random seeds. (2) ICA may be used to segregate obvious artifactual EEG components (line and muscle noise, eye movements) from other sources. (3) ICA is capable of isolating overlapping EEG phenomena, including alpha and theta bursts and spatially-separable ERP components, to separate ICA channels. (4) Nonstationarities in EEG and behavioral state can be tracked using ICA via changes in the amount of residual correlation between ICA-filtered output channels.},
 author = {Makeig, Scott and Bell, Anthony and Jung, Tzyy-Ping and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/754dda4b1ba34c6fa89716b85d68532b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/754dda4b1ba34c6fa89716b85d68532b-Metadata.json},
 openalex = {W2157765880},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/754dda4b1ba34c6fa89716b85d68532b-Paper.pdf},
 publisher = {MIT Press},
 title = {Independent Component Analysis of Electroencephalographic Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/754dda4b1ba34c6fa89716b85d68532b-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_7c9d0b1f,
 abstract = {In this paper we examine the practical use of hardware neural networks in an autonomous mobile robot. We have developed a hardware neural system based around a custom VLSI chip, EPSILON II, designed specifically for embedded hardware neural applications. We present here a demonstration application of an autonomous mobile robot that highlights the flexibility of this system. This robot gains basic mobility competence in very few training epochs using an instinct-rule training methodology.},
 author = {Jackson, Geoffrey and Murray, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Metadata.json},
 openalex = {W2166754567},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf},
 publisher = {MIT Press},
 title = {Competence Acquisition in an Autonomous Mobile Robot using Hardware Neural Techniques},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_7cce53cf,
 abstract = {The Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions. In this paper we investigate the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of challenging problems and have produced excellent results.},
 author = {Williams, Christopher and Rasmussen, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/7cce53cf90577442771720a370c3c723-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/7cce53cf90577442771720a370c3c723-Metadata.json},
 openalex = {W2161767008},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/7cce53cf90577442771720a370c3c723-Paper.pdf},
 publisher = {MIT Press},
 title = {Gaussian Processes for Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/7cce53cf90577442771720a370c3c723-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_7fec306d,
 abstract = {Topographic maps in primary areas of mammalian cerebral cortex reorganise as a result of behavioural training. The nature of this reorganisation seems consistent with the behaviour of competitive neural networks, as has been demonstrated in the past by computer simulation. We model tactile training on the hand representation in primate somatosensory cortex, using the Neural Field Theory of Amari and his colleagues. Expressions for changes in both receptive field size and magnification factor are derived, which are consistent with owl monkey experiments and make a prediction which goes beyond them.},
 author = {Petersen, Rasmus and Taylor, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/7fec306d1e665bc9c748b5d2b99a6e97-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/7fec306d1e665bc9c748b5d2b99a6e97-Metadata.json},
 openalex = {W2107182359},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf},
 publisher = {MIT Press},
 title = {Reorganisation of Somatosensory Cortex after Tactile Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/7fec306d1e665bc9c748b5d2b99a6e97-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_818f4654,
 abstract = {Most current methods for prediction of protein secondary structure use a small window of the protein sequence to predict the structure of the central amino acid. We describe a new method for prediction of the non-local structure called β-sheet, which consists of two or more β-strands that are connected by hydrogen bonds. Since β-strands are often widely separated in the protein chain, a network with two windows is introduced. After training on a set of proteins the network predicts the sheets well, but there are many false positives. By using a global energy function the β-sheet prediction is combined with a local prediction of the three secondary structures α-helix, β-strand and coil. The energy function is minimized using simulated annealing to give a final prediction.},
 author = {Krogh, Anders and Riis, Soren},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/818f4654ed39a1c147d1e51a00ffb4cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/818f4654ed39a1c147d1e51a00ffb4cb-Metadata.json},
 openalex = {W2129216500},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf},
 publisher = {MIT Press},
 title = {Prediction of Beta Sheets in Proteins},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/818f4654ed39a1c147d1e51a00ffb4cb-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_8248a99e,
 abstract = {This paper proposes new methods for generating input locations actively in gathering training data, aiming at solving problems unique to multilayer perceptrons. One of the problems is that optimum input locations, which are calculated deterministically, sometimes distribute densely around the same point and cause local minima in backpropagation training. Two probabilistic active learning methods, which utilize the statistical variance of locations, are proposed to solve this problem. One is parametric active learning and the other is multipoint-search active learning. Another serious problem in applying active learning to multilayer perceptrons is that a Fisher information matrix can be singular, while many methods, including the proposed ones, assume its regularity. A technique of pruning redundant hidden units is proposed to keep the Fisher information matrix regular. Combined with this technique, active learning can be applied stably to multilayer perceptrons. The effectiveness of the proposed methods is demonstrated through computer simulations on simple artificial problems and a real-world problem of color conversion.},
 author = {Fukumizu, Kenji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8248a99e81e752cb9b41da3fc43fbe7f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8248a99e81e752cb9b41da3fc43fbe7f-Metadata.json},
 openalex = {W2148924730},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8248a99e81e752cb9b41da3fc43fbe7f-Paper.pdf},
 publisher = {MIT Press},
 title = {Statistical active learning in multilayer perceptrons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/8248a99e81e752cb9b41da3fc43fbe7f-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_82b8a343,
 abstract = {A model of human motion perception is presented. The model contains two stages of direction selective units. The first stage contains broadly tuned units, while the second stage contains units that are narrowly tuned. The model accounts for the motion aftereffect through adapting units at the first stage and inhibitory interactions at the second stage. The model explains how two populations of dots moving in slightly different directions are perceived as a single population moving in the direction of the vector sum, and how two populations moving in strongly different directions are perceived as transparent motion. The model also explains why the motion aftereffect in both cases appears as non-transparent motion.},
 author = {Grunewald, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/82b8a3434904411a9fdc43ca87cee70c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/82b8a3434904411a9fdc43ca87cee70c-Metadata.json},
 openalex = {W2109886545},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/82b8a3434904411a9fdc43ca87cee70c-Paper.pdf},
 publisher = {MIT Press},
 title = {A model of transparent motion and non-transparent motion aftereffects},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/82b8a3434904411a9fdc43ca87cee70c-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_83fa5a43,
 abstract = {We compare two regularization methods which can be used to improve the generalization capabilities of Gaussian mixture density estimates. The first method uses a Bayesian prior on the parameter space. We derive EM (Expectation Maximization) update rules which maximize the a posterior parameter probability. In the second approach we apply ensemble averaging to density estimation. This includes Breiman's bagging, which recently has been found to produce impressive results for classification networks.},
 author = {Ormoneit, Dirk and Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/83fa5a432ae55c253d0e60dbfa716723-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/83fa5a432ae55c253d0e60dbfa716723-Metadata.json},
 openalex = {W2115092992},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf},
 publisher = {MIT Press},
 title = {Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_84d2004b,
 abstract = {A practical method for Bayesian training of feed-forward neural networks using sophisticated Monte Carlo methods is presented and evaluated. In reasonably small amounts of computer time this approach outperforms other state-of-the-art methods on 5 data-limited tasks from real world domains.},
 author = {Rasmussen, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/84d2004bf28a2095230e8e14993d398d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/84d2004bf28a2095230e8e14993d398d-Metadata.json},
 openalex = {W2110237331},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf},
 publisher = {MIT Press},
 title = {A Practical Monte Carlo Implementation of Bayesian Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/84d2004bf28a2095230e8e14993d398d-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_8597a6cf,
 abstract = {We derive a smoothing regularizer for dynamic network models by requiring robustness in prediction performance to perturbations of the training data. The regularizer can be viewed as a generalization of the first-order Tikhonov stabilizer to dynamic models. For two layer networks with recurrent connections described by the training criterion with the regularizer is where Φ = {U, V, W} is the network parameter set, Z(t) are the targets, I(t) = {X(s), s = 1,2, …, t} represents the current and all historical input information, N is the size of the training data set, [Formula: see text] is the regularizer, and λ is a regularization parameter. The closed-form expression for the regularizer for time-lagged recurrent networks is where ‖ ‖ is the Euclidean matrix norm and γ is a factor that depends upon the maximal value of the first derivatives of the internal unit activations f(). Simplifications of the regularizer are obtained for simultaneous recurrent nets (τ ↦ 0), two-layer feedforward nets, and one layer linear nets. We have successfully tested this regularizer in a number of case studies and found that it performs better than standard quadratic weight decay.},
 author = {Wu, Lizhong and Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8597a6cfa74defcbde3047c891d78f90-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8597a6cfa74defcbde3047c891d78f90-Metadata.json},
 openalex = {W1971812378},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf},
 publisher = {MIT Press},
 title = {A Smoothing Regularizer for Feedforward and Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/8597a6cfa74defcbde3047c891d78f90-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_883e881b,
 abstract = {In this paper, we introduce REMAP, an approach for the training and estimation of posterior probabilities using a recursive algorithm that is reminiscent of the EM-based Forward-Backward (Liporace 1982) algorithm for the estimation of sequence likelihoods. Although very general, the method is developed in the context of a statistical model for transition-based speech recognition using Artificial Neural Networks (ANN) to generate probabilities for Hidden Markov Models (HMMs). In the new approach, we use local conditional posterior probabilities of transitions to estimate global posterior probabilities of word sequences. Although we still use ANNs to estimate posterior probabilities, the network is trained with targets that are themselves estimates of local posterior probabilities. An initial experimental result shows a significant decrease in error-rate in comparison to a baseline system.},
 author = {Konig, Yochai and Bourlard, Herv\'{e} and Morgan, Nelson},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/883e881bb4d22a7add958f2d6b052c9f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/883e881bb4d22a7add958f2d6b052c9f-Metadata.json},
 openalex = {W2147511255},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/883e881bb4d22a7add958f2d6b052c9f-Paper.pdf},
 publisher = {MIT Press},
 title = {REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities - Application to Transition-Based Connectionist Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/883e881bb4d22a7add958f2d6b052c9f-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_884d7996,
 abstract = {Harmony networks have been proposed as a means by which connectionist models can perform symbolic computation. Indeed, proponents claim that a harmony network can be built that constructs parse trees for strings in a context free language. This paper shows that harmony networks do not work in the following sense: they construct many outputs that are not valid parse trees.},
 author = {Gourley, Ren\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/884d79963bd8bc0ae9b13a1aa71add73-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/884d79963bd8bc0ae9b13a1aa71add73-Metadata.json},
 openalex = {W2131829879},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/884d79963bd8bc0ae9b13a1aa71add73-Paper.pdf},
 publisher = {MIT Press},
 title = {Harmony Networks Do Not Work},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/884d79963bd8bc0ae9b13a1aa71add73-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_8a1e808b,
 abstract = {We introduce a new algorithm designed to learn sparse perceptrons over input representations which include high-order features. Our algorithm, which is based on a hypothesis-boosting method, is able to PAC-learn a relatively natural class of target concepts. Moreover, the algorithm appears to work well in practice: on a set of three problem domains, the algorithm produces classifiers that utilize small numbers of features yet exhibit good generalization performance. Perhaps most importantly, our algorithm generates concept descriptions that are easy for humans to understand.},
 author = {Jackson, Jeffrey and Craven, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8a1e808b55fde9455cb3d8857ed88389-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8a1e808b55fde9455cb3d8857ed88389-Metadata.json},
 openalex = {W2126177819},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Sparse Perceptrons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/8a1e808b55fde9455cb3d8857ed88389-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_8a3363ab,
 abstract = {An extended version of the dual constraint model of motor end-plate morphogenesis is presented that includes activity dependent and independent competition. It is supported by a wide range of recent neurophysiological evidence that indicates a strong relationship between synaptic efficacy and survival. The computational model is justified at the molecular level and its predictions match the developmental and regenerative behaviour of real synapses.},
 author = {Joseph, Samuel and Willshaw, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8a3363abe792db2d8761d6403605aeb7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8a3363abe792db2d8761d6403605aeb7-Metadata.json},
 openalex = {W2106460814},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf},
 publisher = {MIT Press},
 title = {The Role of Activity in Synaptic Competition at the Neuromuscular Junction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_8b406655,
 abstract = {For a given recurrent neural network, a discrete-time model may have asymptotic dynamics different from the one of a related continuous-time model. In this article, we consider a discrete-time model that discretizes the continuous-time leaky integrat or model and study its parallel, sequential, block-sequential, and distributed dynamics for symmetric networks. We provide sufficient (and in many cases necessary) conditions for the discretized model to have the same cycle-free dynamics of the corresponding continuous-time model in symmetric networks.},
 author = {Wang, Xin and Jagota, Arun and Botelho, Fernanda and Garzon, Max},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8b4066554730ddfaa0266346bdc1b202-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8b4066554730ddfaa0266346bdc1b202-Metadata.json},
 openalex = {W2167652445},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8b4066554730ddfaa0266346bdc1b202-Paper.pdf},
 publisher = {MIT Press},
 title = {Absence of Cycles in Symmetric Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/8b4066554730ddfaa0266346bdc1b202-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_8ce6790c,
 abstract = {Following Shrager and Johnson (1995) we study growth of logical function complexity in a network swept by two overlapping waves: one of pruning, and the other of Hebbian reinforcement of connections. Results indicate a significant spatial gradient in the appearance of both linearly separable and non linearly separable functions of the two inputs of the network; the n.l.s. cells are much sparser and their slope of appearance is sensitive to parameters in a highly non-linear way.},
 author = {Rebotier, Thomas and Elman, Jeffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8ce6790cc6a94e65f17f908f462fae85-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8ce6790cc6a94e65f17f908f462fae85-Metadata.json},
 openalex = {W2098613006},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8ce6790cc6a94e65f17f908f462fae85-Paper.pdf},
 publisher = {MIT Press},
 title = {Explorations with the Dynamic Wave Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/8ce6790cc6a94e65f17f908f462fae85-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_8f1d4362,
 abstract = {On large problems, reinforcement learning systems must use parameterized function approximators such as neural networks in order to generalize between similar situations and actions. In these cases there are no strong theoretical results on the accuracy of convergence, and computational results have been mixed. In particular, Boyan and Moore reported at last year's meeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces. In this paper, we present positive results for all the control tasks they attempted, and for one that is significantly larger. The most important differences are that we used sparse-coarse-coded function approximators (CMACs) whereas they used mostly global function approximators, and that we learned online whereas they learned offline. Boyan and Moore and others have suggested that the problems they encountered could be solved by using actual outcomes (rollouts), as in classical Monte Carlo methods, and as in the TD(λ) algorithm when λ = 1. However, in our experiments this always resulted in substantially poorer performance. We conclude that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general λ.},
 author = {Sutton, Richard S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8f1d43620bc6bb580df6e80b0dc05c48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8f1d43620bc6bb580df6e80b0dc05c48-Metadata.json},
 openalex = {W2124175081},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_93d65641,
 abstract = {We investigate the optimization of neural networks governed by general objective functions. Practical formulations of such objectives are notoriously difficult to solve; a common problem is the poor local extrema that result by any of the applied methods. In this paper, a novel framework is introduced for the solution of largescale optimization problems. It assumes little about the objective function and can be applied to general nonlinear, non-convex functions; objectives in thousand of variables are thus efficiently minimized by a combination of techniques - deterministic annealing, multiscale optimization, attention mechanisms and trust region optimization methods.},
 author = {Tsioutsias, Dimitris and Mjolsness, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/93d65641ff3f1586614cf2c1ad240b6c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/93d65641ff3f1586614cf2c1ad240b6c-Metadata.json},
 openalex = {W2134197137},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf},
 publisher = {MIT Press},
 title = {A Multiscale Attentional Framework for Relaxation Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/93d65641ff3f1586614cf2c1ad240b6c-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_995e1fda,
 abstract = {A one dimensional model of primate smooth pursuit mechanism has been implemented in 2 µm CMOS VLSI. The model consolidates Robinson's negative feedback model with Wyatt and Pola's positive feedback scheme, to produce a smooth pursuit system which zero's the velocity of a target on the retina. Furthermore, the system uses the current eye motion as a predictor for future target motion. Analysis, stability and biological correspondence of the system are discussed. For implementation at the focal plane, a local correlation based visual motion detection technique is used. Velocity measurements, ranging over 4 orders of magnitude with < 15% variation, provides the input to the smooth pursuit system. The system performed successful velocity tracking for high contrast scenes. Circuit design and performance of the complete smooth pursuit system is presented.},
 author = {Etienne-Cummings, Ralph and Van der Spiegel, Jan and Mueller, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/995e1fda4a2b5f55ef0df50868bf2a8f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/995e1fda4a2b5f55ef0df50868bf2a8f-Metadata.json},
 openalex = {W2170526990},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/995e1fda4a2b5f55ef0df50868bf2a8f-Paper.pdf},
 publisher = {MIT Press},
 title = {VLSI Model of Primate Visual Smooth Pursuit},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/995e1fda4a2b5f55ef0df50868bf2a8f-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_9ac403da,
 abstract = {We present a new algorithm for associative reinforcement learning. The algorithm is based upon the idea of matching a network's output probability with a probability distribution derived from the environment's reward signal. This Probability Matching algorithm is shown to perform faster and be less susceptible to local minima than previously existing algorithms. We use Probability Matching to train mixture of experts networks, an architecture for which other reinforcement learning rules fail to converge reliably on even simple problems. This architecture is particularly well suited for our algorithm as it can compute arbitrarily complex functions yet calculation of the output probability is simple.},
 author = {Sabes, Philip N and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9ac403da7947a183884c18a67d3aa8de-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9ac403da7947a183884c18a67d3aa8de-Metadata.json},
 openalex = {W2152726590},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9ac403da7947a183884c18a67d3aa8de-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning by Probability Matching},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/9ac403da7947a183884c18a67d3aa8de-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_9c3b1830,
 abstract = {We propose a new learning method, Generalized Learning Vector Quantization (GLVQ), in which reference vectors are updated based on the steepest descent method in order to minimize the cost function. The cost function is determined so that the obtained learning rule satisfies the convergence condition. We prove that Kohonen's rule as used in LVQ does not satisfy the convergence condition and thus degrades recognition ability. Experimental results for printed Chinese character recognition reveal that GLVQ is superior to LVQ in recognition ability.},
 author = {Sato, Atsushi and Yamada, Keiji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9c3b1830513cc3b8fc4b76635d32e692-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9c3b1830513cc3b8fc4b76635d32e692-Metadata.json},
 openalex = {W2123749980},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9c3b1830513cc3b8fc4b76635d32e692-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalized Learning Vector Quantization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/9c3b1830513cc3b8fc4b76635d32e692-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_9da187a7,
 abstract = {We present a Bayesian framework for inferring the parameters of a mixture of experts model based on ensemble learning by variational free energy minimisation. The Bayesian approach avoids the over-fitting and noise level under-estimation problems of traditional maximum likelihood inference. We demonstrate these methods on artificial problems and sunspot time series prediction.},
 author = {Waterhouse, Steve and MacKay, David and Robinson, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9da187a7a191431db943a9a5a6fec6f4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9da187a7a191431db943a9a5a6fec6f4-Metadata.json},
 openalex = {W2105658140},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9da187a7a191431db943a9a5a6fec6f4-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Methods for Mixtures of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/9da187a7a191431db943a9a5a6fec6f4-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_9f36407e,
 abstract = {Current environmental monitoring systems assume particles to be spherical, and do not attempt to classify them. A laser-based system developed at the University of Hertfordshire aims at classifying airborne particles through the generation of two-dimensional scattering profiles. The pedormances of template matching, and two types of neural network (HyperNet and semi-linear units) are compared for image classification. The neural network approach is shown to be capable of comparable recognition pedormance, while offering a number of advantages over template matching.},
 author = {Ferguson, Alistair and Sabisch, Theo and Kaye, Paul and Dixon, Laurence and Bolouri, Hamid},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9f36407ead0629fc166f14dde7970f68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9f36407ead0629fc166f14dde7970f68-Metadata.json},
 openalex = {W2139269690},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/9f36407ead0629fc166f14dde7970f68-Paper.pdf},
 publisher = {MIT Press},
 title = {High-Speed Airborne Particle Monitoring Using Artificial Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/9f36407ead0629fc166f14dde7970f68-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a0160709,
 abstract = {We describe and analyze a mixture model for supervised learning of probabilistic transducers. We devise an online learning algorithm that efficiently infers the structure and estimates the parameters of each probabilistic transducer in the mixture. Theoretical analysis and comparative simulations indicate that the learning algorithm tracks the best transducer from an arbitrarily large (possibly infinite) pool of models. We also present an application of the model for inducing a noun phrase recognizer.},
 author = {Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a0160709701140704575d499c997b6ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a0160709701140704575d499c997b6ca-Metadata.json},
 openalex = {W1963571628},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a0160709701140704575d499c997b6ca-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive Mixtures of Probabilistic Transducers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a0160709701140704575d499c997b6ca-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a0e2a2c5,
 abstract = {A view-based, high-dimensional feature-space recognition system called SEEMORE was developed as a testbed to explore the representational trade-offs that arise when a simple feedforward neural architecture is challenged with a difficult 3D object recognition problem. Particular emphasis was placed on designing an object representation that could: 1) cope with a large number of real 3D objects of many different types; 2) operate directly on input images without shift, scale, or other object pre-normalization steps; 3) integrate multiple visual cues; and 4) recognize objects over 6 degrees of freedom of viewpoint, gross non-rigid shape distortions, and/or partial occulsion. Recognition results were obtained using a set of 102 color and shape feature channels, each designed to be invariant to image plane shifts and rotations, and only modestly sensitive to orientation in depth. In response to a test set of 600 novel test views of 100 objects presented individually in color video images, SEEMORE identified the object correctly 97% of the time using a nearest neighbour classifier. Similar levels of performance were obtained for the subset of 15 non-rigid objects.},
 author = {Mel, Bartlett},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a0e2a2c563d57df27213ede1ac4ac780-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a0e2a2c563d57df27213ede1ac4ac780-Metadata.json},
 openalex = {W2050167372},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a0e2a2c563d57df27213ede1ac4ac780-Paper.pdf},
 publisher = {MIT Press},
 title = {SEEMORE: a view-based approach to 3-D object recognition using multiple visual cues},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a0e2a2c563d57df27213ede1ac4ac780-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a113c1ec,
 abstract = {Conventional binary classification trees such as CART either split the data using axis-aligned hyperplanes or they perform a computationally expensive search in the continuous space of hyperplanes with unrestricted orientations. We show that the limitations of the former can be overcome without resorting to the latter. For every pair of training data-points, there is one hyperplane that is orthogonal to the line joining the data-points and bisects this line. Such hyperplanes are plausible candidates for splits. In a comparison on a suite of 12 datasets we found that this method of generating candidate splits outperformed the standard methods, particularly when the training sets were small.},
 author = {Hinton, Geoffrey E and Revow, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a113c1ecd3cace2237256f4c712f61b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a113c1ecd3cace2237256f4c712f61b5-Metadata.json},
 openalex = {W2141036804},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a113c1ecd3cace2237256f4c712f61b5-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Pairs of Data-Points to Define Splits for Decision Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a113c1ecd3cace2237256f4c712f61b5-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a1519de5,
 abstract = {We consider the problem of on-line gradient descent learning for general two-layer neural networks. An analytic solution is presented and used to investigate the role of the learning rate in controlling the evolution and convergence of the learning process.},
 author = {Saad, David and Solla, Sara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a1519de5b5d44b31a01de013b9b51a80-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a1519de5b5d44b31a01de013b9b51a80-Metadata.json},
 openalex = {W2168263526},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamics of On-Line Gradient Descent Learning for Multilayer Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a1519de5b5d44b31a01de013b9b51a80-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a1d50185,
 abstract = {A theory of early stopping as applied to linear models is presented. The backpropagation learning algorithm is modeled as gradient descent in continuous time. Given a training set and a validation set, all weight vectors found by early stopping must lie on a certain quadric surface, usually an ellipsoid. Given a training set and a candidate early stopping weight vector, all validation sets have least-squares weights lying on a certain plane. This latter fact can be exploited to estimate the probability of stopping at any given point along the trajectory from the initial weight vector to the least-squares weights derived from the training set, and to estimate the probability that training goes on indefinitely. The prospects for extending this theory to nonlinear models are discussed.},
 author = {Dodier, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a1d50185e7426cbb0acad1e6ca74b9aa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a1d50185e7426cbb0acad1e6ca74b9aa-Metadata.json},
 openalex = {W2100081266},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf},
 publisher = {MIT Press},
 title = {Geometry of Early Stopping in Linear Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a1d50185e7426cbb0acad1e6ca74b9aa-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a2137a2a,
 abstract = {A Bayesian-Kullback learning scheme, called Ying-Yang Machine, is proposed based on the two complement but equivalent Bayesian representations for joint density and their Kullback divergence. Not only the scheme unifies existing major supervised and unsupervised learnings, including the classical maximum likelihood or least square learning, the maximum information preservation, the EM & em algorithm and information geometry, the recent popular Helmholtz machine, as well as other learning methods with new variants and new results; but also the scheme provides a number of new learning models.},
 author = {Xu, Lei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a2137a2ae8e39b5002a3f8909ecb88fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a2137a2ae8e39b5002a3f8909ecb88fe-Metadata.json},
 openalex = {W2170986055},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a2137a2ae8e39b5002a3f8909ecb88fe-Paper.pdf},
 publisher = {MIT Press},
 title = {A Unified Learning Scheme: Bayesian-Kullback Ying-Yang Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a2137a2ae8e39b5002a3f8909ecb88fe-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a26398dc,
 abstract = {We report here that changes in the normalized electroencephalographic (EEG) cross-spectrum can be used in conjunction with feedforward neural networks to monitor changes in alertness of operators continuously and in near-real time. Previously, we have shown that EEG spectral amplitudes covary with changes in alertness as indexed by changes in behavioral error rate on an auditory detection task [6,4]. Here, we report for the first time that increases in the frequency of detection errors in this task are also accompanied by patterns of increased and decreased spectral coherence in several frequency bands and EEG channel pairs. Relationships between EEG coherence and performance vary between subjects, but within subjects, their topographic and spectral profiles appear stable from session to session. Changes in alertness also covary with changes in correlations among EEG waveforms recorded at different scalp sites, and neural networks can also estimate alertness from correlation changes in spontaneous and unobtrusively-recorded EEG signals.},
 author = {Makeig, Scott and Jung, Tzyy-Ping and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a26398dca6f47b49876cbaffbc9954f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a26398dca6f47b49876cbaffbc9954f9-Metadata.json},
 openalex = {W2107025871},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a26398dca6f47b49876cbaffbc9954f9-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Feedforward Neural Networks to Monitor Alertness from Changes in EEG Correlation and Coherence},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a26398dca6f47b49876cbaffbc9954f9-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a34bacf8,
 abstract = {The vestibulo-ocular reflex (VOR) stabilizes images on the retina during rapid head motions. The gain of the VOR (the ratio of eye to head rotation velocity) is typically around -1 when the eyes are focused on a distant target. However, to stabilize images accurately, the VOR gain must vary with context (eye position, eye vergence and head translation). We first describe a kinematic model of the VOR which relies solely on sensory information available from the semicircular canals (head rotation), the otoliths (head translation), and neural correlates of eye position and vergence angle. We then propose a dynamical model and compare it to the eye velocity responses measured in monkeys. The dynamical model reproduces the observed amplitude and time course of the modulation of the VOR and suggests one way to combine the required neural signals within the cerebellum and the brain stem. It also makes predictions for the responses of neurons to multiple inputs (head rotation and translation, eye position, etc.) in the oculomotor system.},
 author = {Coenen, Olivier and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a34bacf839b923770b2c360eefa26748-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a34bacf839b923770b2c360eefa26748-Metadata.json},
 openalex = {W2122940913},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a34bacf839b923770b2c360eefa26748-Paper.pdf},
 publisher = {MIT Press},
 title = {A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a34bacf839b923770b2c360eefa26748-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a3fb4fbf,
 abstract = {We present two additions to the hierarchical mixture of experts (HME) architecture. By applying a likelihood splitting criteria to each expert in the HME we grow the tree adaptively during training. Secondly, by considering only the most probable path through the tree we may prune branches away, either temporarily, or permanently if they become redundant. We demonstrate results for the growing and path pruning algorithms which show significant speed ups and more efficient use of parameters over the standard fixed structure in discriminating between two interlocking spirals and classifying 8-bit parity patterns.},
 author = {Waterhouse, Steve and Robinson, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Metadata.json},
 openalex = {W2169210279},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Paper.pdf},
 publisher = {MIT Press},
 title = {Constructive Algorithms for Hierarchical Mixtures of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a5e0ff62,
 abstract = {We study Bayesian networks for continuous variables using nonlinear conditional density estimators. We demonstrate that useful structures can be extracted from a data set in a self-organized way and we present sampling techniques for belief update based on Markov blanket conditional density models.},
 author = {Hofmann, Reimar and Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a5e0ff62be0b08456fc7f1e88812af3d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a5e0ff62be0b08456fc7f1e88812af3d-Metadata.json},
 openalex = {W2105130030},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf},
 publisher = {MIT Press},
 title = {Discovering Structure in Continuous Variables Using Bayesian Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a5e0ff62be0b08456fc7f1e88812af3d-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a8240cb8,
 abstract = {Binaural coincidence detection is essential for the localization of external sounds and requires auditory signal processing with high temporal precision. We present an integrate-and-fire model of spike processing in the auditory pathway of the barn owl. It is shown that a temporal precision in the microsecond range can be achieved with neuronal time constants which are at least one magnitude longer. An important feature of our model is an unsupervised Hebbian learning rule which leads to a temporal fine tuning of the neuronal connections.},
 author = {Kempter, Richard and Gerstner, Wulfram and van Hemmen, J. and Wagner, Hermann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a8240cb8235e9c493a0c30607586166c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a8240cb8235e9c493a0c30607586166c-Metadata.json},
 openalex = {W2136953607},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a8240cb8235e9c493a0c30607586166c-Paper.pdf},
 publisher = {MIT Press},
 title = {Temporal coding in the sub-millisecond range: Model of barn owl auditory pathway},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a8240cb8235e9c493a0c30607586166c-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_a89cf525,
 abstract = {A stability criterion for dynamic parameter adaptation is given. In the case of the learning rate of backpropagation, a class of stable algorithms is presented and studied, including a convergence proof.},
 author = {R\"{u}ger, Stefan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a89cf525e1d9f04d16ce31165e139a4b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a89cf525e1d9f04d16ce31165e139a4b-Metadata.json},
 openalex = {W2116007328},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf},
 publisher = {MIT Press},
 title = {Stable Dynamic Parameter Adaption},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/a89cf525e1d9f04d16ce31165e139a4b-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_aace49c7,
 abstract = {Topographic mappings occur frequently in the brain. A popular approach to understanding the structure of such mappings is to map points representing input features in a space of a few dimensions to points in a 2 dimensional space using some self-organizing algorithm. We argue that a more general approach may be useful, where similarities between features are not constrained to be geometric distances, and the objective function for topographic matching is chosen explicitly rather than being specified implicitly by the self-organizing algorithm. We investigate analytically an example of this more general approach applied to the structure of interdigitated mappings, such as the pattern of ocular dominance columns in primary visual cortex.},
 author = {Goodhill, Geoffrey and Finch, Steven and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/aace49c7d80767cffec0e513ae886df0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/aace49c7d80767cffec0e513ae886df0-Metadata.json},
 openalex = {W2145324398},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/aace49c7d80767cffec0e513ae886df0-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimizing Cortical Mappings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/aace49c7d80767cffec0e513ae886df0-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_abea47ba,
 abstract = {This paper describes a neural network based controller for allocating capacity in a telecommunications network. This system was proposed in order to overcome a real response constraint. Two basic architectures are evaluated: 1) a feedforward network-heuristic and; 2) a feedforward network-recurrent network. These architectures are compared against a linear programming (LP) optimiser as a benchmark. This LP optimiser was also used as a teacher to label the data samples for the feedforward neural network training algorithm. It is found that the systems are able to provide a traffic throughput of 99% and 95%, respectively, of the throughput obtained by the linear programming solution. Once trained, the neural network based solutions are found in a fraction of the time required by the LP optimiser.},
 author = {Campbell, Peter and Dale, Michael and Ferr\'{a}, Herman and Kowalczyk, Adam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/abea47ba24142ed16b7d8fbf2c740e0d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/abea47ba24142ed16b7d8fbf2c740e0d-Metadata.json},
 openalex = {W2168472875},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf},
 publisher = {MIT Press},
 title = {Experiments with Neural Networks for Real Time Implementation of Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/abea47ba24142ed16b7d8fbf2c740e0d-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_af21d0c9,
 abstract = {Selective suppression of transmission at feedback synapses during learning is proposed as a mechanism for combining associative feedback with self-organization of feed forward synapses. Experimental data demonstrates cholinergic suppression of synaptic transmission in layer I (feedback synapses), and a lack of suppression in layer IV (feedforward synapses). A network with this feature uses local rules to learn mappings which are not linearly separable. During learning, sensory stimuli and desired response are simultaneously presented as input. Feedforward connections form self-organized representations of input, while suppressed feedback connections learn the transpose of feedforward connectivity. During recall, suppression is removed, sensory input activates the self-organized representation, and activity generates the learned response.},
 author = {Hasselmo, Michael and Cekic, Milos},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/af21d0c97db2e27e13572cbf59eb343d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/af21d0c97db2e27e13572cbf59eb343d-Metadata.json},
 openalex = {W2119424459},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},
 publisher = {MIT Press},
 title = {Cholinergic suppression of transmission may allow combined associative memory function and self-organization in the neocortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_afdec700,
 abstract = {In consideration of attention as a means for goal-directed behavior in non-stationary environments, we argue that the dynamics of attention should satisfy two opposing demands: long-term maintenance and quick transition. These two characteristics are contradictory within the linear domain. We propose the near saddle-node bifurcation behavior of a sigmoidal unit with self-connection as a candidate of dynamical mechanism that satisfies both of these demands. We further show in simulations of the 'bug-eat-food' tasks that the near saddle-node bifurcation behavior of recurrent networks can emerge as a functional property for survival in non-stationary environments.},
 author = {Nakahara, Hiroyuki and Doya, Kenji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/afdec7005cc9f14302cd0474fd0f3c96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/afdec7005cc9f14302cd0474fd0f3c96-Metadata.json},
 openalex = {W2113864751},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamics of Attention as Near Saddle-Node Bifurcation Behavior},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_b1563a78,
 abstract = {A new technique, termed soft assign, is applied for the first time to two classic combinatorial optimization problems, the traveling salesman problem and graph partitioning. Soft assign, which has emerged from the recurrent neural network/statistical physics framework, enforces two-way (assignment) constraints without the use of penalty terms in the energy functions. The soft assign can also be generalized from two-way winner-take-all constraints to multiple membership constraints which are required for graph partitioning. The soft assign technique is compared to the softmax (Potts glass). Within the statistical physics framework, softmax and a penalty term has been a widely used method for enforcing the two-way constraints common within many combinatorial optimization problems. The benchmarks present evidence that soft assign has clear advantages in accuracy, speed, parallelizability and algorithmic simplicity over softmax and a penalty term in optimization problems with two-way constraints.},
 author = {Gold, Steven and Rangarajan, Anand},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b1563a78ec59337587f6ab6397699afc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b1563a78ec59337587f6ab6397699afc-Metadata.json},
 openalex = {W2108109322},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b1563a78ec59337587f6ab6397699afc-Paper.pdf},
 publisher = {MIT Press},
 title = {Softassign versus Softmax: Benchmarks in Combinatorial Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/b1563a78ec59337587f6ab6397699afc-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_b4d168b4,
 abstract = {We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning.},
 author = {Schaal, Stefan and Atkeson, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b4d168b48157c623fbd095b4a565b5bb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b4d168b48157c623fbd095b4a565b5bb-Metadata.json},
 openalex = {W2144104178},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf},
 publisher = {MIT Press},
 title = {From Isolation to Cooperation: An Alternative View of a System of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/b4d168b48157c623fbd095b4a565b5bb-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_b59c67bf,
 abstract = {Sigmoid type belief networks, a class of probabilistic neural networks, provide a natural framework for compactly representing probabilistic information in a variety of unsupervised and supervised learning problems. Often the parameters used in these networks need to be learned from examples. Unfortunately, estimating the parameters via exact probabilistic calculations (i.e, the EM-algorithm) is intractable even for networks with fairly small numbers of hidden units. We propose to avoid the infeasibility of the E step by bounding likelihoods instead of computing them exactly. We introduce extended and complementary representations for these networks and show that the estimation of the network parameters can be made fast (reduced to quadratic optimization) by performing the estimation in either of the alternative domains. The complementary networks can be used for continuous density estimation as well.},
 author = {Jaakkola, Tommi and Saul, Lawrence and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b59c67bf196a4758191e42f76670ceba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b59c67bf196a4758191e42f76670ceba-Metadata.json},
 openalex = {W2100266635},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b59c67bf196a4758191e42f76670ceba-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Learning by Bounding Likelihoods in Sigmoid Type Belief Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/b59c67bf196a4758191e42f76670ceba-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_b9141aff,
 author = {Yu, Ssu-Hsin and Annaswamy, Anuradha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b9141aff1412dc76340b3822d9ea6c72-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b9141aff1412dc76340b3822d9ea6c72-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b9141aff1412dc76340b3822d9ea6c72-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Control for Nonlinear Dynamic Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/b9141aff1412dc76340b3822d9ea6c72-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_b9d487a3,
 abstract = {Recurrent perceptron classifiers generalize the usual perceptron model. They correspond to linear transformations of input vectors obtained by means of "autoregressive moving-average schemes", or infinite impulse response filters, and take into account those correlations and dependences among input coordinates which arise from linear digital filtering. This paper provides tight bounds on the sample complexity associated to the fitting of such models to experimental data. The results are expressed in the context of the theory of probably approximately correct (PAC) learning.},
 author = {DasGupta, Bhaskar and Sontag, Eduardo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b9d487a30398d42ecff55c228ed5652b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b9d487a30398d42ecff55c228ed5652b-Metadata.json},
 openalex = {W2156582723},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/b9d487a30398d42ecff55c228ed5652b-Paper.pdf},
 publisher = {MIT Press},
 title = {Sample complexity for learning recurrent perceptron mappings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/b9d487a30398d42ecff55c228ed5652b-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_bdb106a0,
 abstract = {This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks.},
 author = {Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/bdb106a0560c4e46ccc488ef010af787-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/bdb106a0560c4e46ccc488ef010af787-Metadata.json},
 openalex = {W2133013156},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf},
 publisher = {MIT Press},
 title = {Is Learning The n-th Thing Any Easier Than Learning The First?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/bdb106a0560c4e46ccc488ef010af787-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_c21002f4,
 abstract = {We describe two parallel analog VLSI architectures that integrate optical flow data obtained from arrays of elementary velocity sensors to estimate heading direction and time-to-contact. For heading direction computation, we performed simulations to evaluate the most important qualitative properties of the optical flow field and determine the best functional operators for the implementation of the architecture. For time-to-contact we exploited the divergence theorem to integrate data from all velocity sensors present in the architecture and average out possible errors.},
 author = {Indiveri, Giacomo and Kramer, J\"{o}rg and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c21002f464c5fc5bee3b98ced83963b8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c21002f464c5fc5bee3b98ced83963b8-Metadata.json},
 openalex = {W2137032375},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf},
 publisher = {MIT Press},
 title = {Parallel analog VLSI architectures for computation of heading direction and time-to-contact},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/c21002f464c5fc5bee3b98ced83963b8-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_c3e0c62e,
 author = {Tani, Jun and Fukumura, Naohiro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c3e0c62ee91db8dc7382bde7419bb573-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c3e0c62ee91db8dc7382bde7419bb573-Metadata.json},
 openalex = {W2462625107},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c3e0c62ee91db8dc7382bde7419bb573-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamical Systems Approach in Learnable Autonomous Robots},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/c3e0c62ee91db8dc7382bde7419bb573-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_c6036a69,
 abstract = {Recent experiments show that the neural codes at work in a wide range of creatures share some common features. At first sight, these observations seem unrelated. However, we show that these features arise naturally in a linear filtered threshold crossing model when we set the threshold to maximize the transmitted information. This maximization process requires neural adaptation to not only the DC signal level, as in conventional light and dark adaptation, but also to the statistical structure of the signal and noise distributions. We also present a new approach for calculating the mutual information between a neuron's output spike train and any aspect of its input signal which does not require reconstruction of the input signal. This formulation is valid provided the correlations in the spike train are small, and we provide a procedure for checking this assumption. This paper is based on joint work (DeWeese M 1995 Optimization principles for the neural code, Dissertation, Princeton University). Preliminary results from the linear filtered threshold crossing model appeared in a previous proceedings (DeWeese M and Bialek W 1995 Information flow in sensory neurons, Nuovo Cimento D 17 733–8), and the conclusions we reached at that time have been reaffirmed by further analysis of the model.* This paper was presented at the Workshop on Information Theory and the Brain, held at the University of Stirling, UK, on 4–5 September 1995.},
 author = {DeWeese, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c6036a69be21cb660499b75718a3ef24-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c6036a69be21cb660499b75718a3ef24-Metadata.json},
 openalex = {W2102848185},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c6036a69be21cb660499b75718a3ef24-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimization principles for the neural code},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/c6036a69be21cb660499b75718a3ef24-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_c60d060b,
 abstract = {Matching feature point sets lies at the core of many approaches to object recognition. We present a framework for non-rigid matching that begins with a skeleton module, affine point matching, and then integrates multiple features to improve correspondence and develops an object representation based on spatial regions to model local transformations. The algorithm for feature matching iteratively updates the transformation parameters and the correspondence solution, each in turn. The affine mapping is solved in closed form, which permits its use for data of any dimension. The correspondence is set via a method for two-way constraint satisfaction, called softassign, which has recently emerged from the neural network/statistical physics realm. The complexity of the non-rigid matching algorithm with multiple features is the same as that of the affine point matching algorithm. Results for synthetic and real world data are provided for point sets in 2D and 3D, and for 2D data with multiple types of features and parts.},
 author = {Pappu, Suguna and Gold, Steven and Rangarajan, Anand},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c60d060b946d6dd6145dcbad5c4ccf6f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c60d060b946d6dd6145dcbad5c4ccf6f-Metadata.json},
 openalex = {W2166856522},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Framework for Non-rigid Matching and Correspondence},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/c60d060b946d6dd6145dcbad5c4ccf6f-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_c667d53a,
 abstract = {We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.},
 author = {Hihi, Salah and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c667d53acd899a97a85de0c201ba99be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c667d53acd899a97a85de0c201ba99be-Metadata.json},
 openalex = {W2099257174},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c667d53acd899a97a85de0c201ba99be-Paper.pdf},
 publisher = {MIT Press},
 title = {Hierarchical Recurrent Neural Networks for Long-Term Dependencies},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/c667d53acd899a97a85de0c201ba99be-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_c6bff625,
 abstract = {This paper relates the computational power of Fahlman's Recurrent Cascade Correlation (RCC) architecture to that of finite state automata (FSA). While some recurrent networks are FSA equivalent, RCC is not. The paper presents a theoretical analysis of the RCC architecture in the form of a proof describing a large class of FSA which cannot be realized by RCC.},
 author = {Kremer, Stefan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c6bff625bdb0393992c9d4db0c6bbe45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c6bff625bdb0393992c9d4db0c6bbe45-Metadata.json},
 openalex = {W2126741029},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c6bff625bdb0393992c9d4db0c6bbe45-Paper.pdf},
 publisher = {MIT Press},
 title = {Finite State Automata that Recurrent Cascade-Correlation Cannot Represent},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/c6bff625bdb0393992c9d4db0c6bbe45-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_c7635bfd,
 abstract = {In this paper we introduce new algorithms for optimizing noisy plants in which each experiment is very expensive. The algorithms build a global non-linear model of the expected output at the same time as using Bayesian linear regression analysis of locally weighted polynomial models. The local model answers queries about confidence, noise, gradient and Hessians, and use them to make automated decisions similar to those made by a practitioner of Response Surface Methodology. The global and local models are combined naturally as a locally weighted regression. We examine the question of whether the global model can really help optimization, and we extend it to the case of time-varying functions. We compare the new algorithms with a highly tuned higher-order stochastic optimization algorithm on randomly-generated functions and a simulated manufacturing task. We note significant improvements in total regret, time to converge, and final solution quality.},
 author = {Moore, Andrew and Schneider, Jeff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c7635bfd99248a2cdef8249ef7bfbef4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c7635bfd99248a2cdef8249ef7bfbef4-Metadata.json},
 openalex = {W2118948426},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf},
 publisher = {MIT Press},
 title = {Memory-based Stochastic Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_c9f95a0a,
 abstract = {A hybrid and contextual radial basis function network/hidden Markov model off-line handwritten word recognition system is presented. The task assigned to the radial basis function networks is the estimation of emission probabilities associated to Markov states. The model is contextual because the estimation of emission probabilities takes into account the left context of the current image segment as represented by its predecessor in the sequence. The new system does not outperform the previous system without context but acts differently.},
 author = {Lemari\'{e}, Bernard and Gilloux, Michel and Leroux, Manuel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c9f95a0a5af052bffce5c89917335f67-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c9f95a0a5af052bffce5c89917335f67-Metadata.json},
 openalex = {W2134581054},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf},
 publisher = {MIT Press},
 title = {Handwritten Word Recognition using Contextual Hybrid Radial Basis Function Network/Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/c9f95a0a5af052bffce5c89917335f67-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_cd89fef7,
 abstract = {Natural and artificial neural circuits must be capable of traversing specific state space trajectories. A natural approach to this problem is to learn the relevant trajectories from examples. Unfortunately, gradient descent learning of complex trajectories in amorphous networks is unsuccessful. We suggest a possible approach where trajectories are realized by combining simple oscillators, in various modular ways. We contrast two regimes of fast and slow oscillations. In all cases, we show that banks of oscillators with bounded frequencies have universal approximation properties. Open questions are also discussed briefly.},
 author = {Baldi, Pierre and Hornik, Kurt},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/cd89fef7ffdd490db800357f47722b20-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/cd89fef7ffdd490db800357f47722b20-Metadata.json},
 openalex = {W2135263068},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/cd89fef7ffdd490db800357f47722b20-Paper.pdf},
 publisher = {MIT Press},
 title = {Universal Approximation and Learning of Trajectories Using Oscillators},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/cd89fef7ffdd490db800357f47722b20-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_ce5140df,
 abstract = {We have analyzed the relationship between correlated spike count and the peak in the cross-correlation of spike trains for pairs of simultaneously recorded neurons from a previous study of area MT in the macaque monkey (Zohary et al., 1994). We conclude that common input, responsible for creating peaks on the order of ten milliseconds wide in the spike train cross-correlograms (CCGs), is also responsible for creating the correlation in spike count observed at the two second time scale of the trial. We argue that both common excitation and inhibition may play significant roles in establishing this correlation.},
 author = {Bair, Wyeth and Zohary, Ehud and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/ce5140df15d046a66883807d18d0264b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/ce5140df15d046a66883807d18d0264b-Metadata.json},
 openalex = {W2168762781},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/ce5140df15d046a66883807d18d0264b-Paper.pdf},
 publisher = {MIT Press},
 title = {Correlated Neuronal Response: Time Scales and Mechanisms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/ce5140df15d046a66883807d18d0264b-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_d6723e7c,
 abstract = {We analyze and compare the well-known Gradient Descent algorithm and a new algorithm, called the Exponentiated Gradient algorithm, for training a single neuron with an arbitrary transfer function. Both algorithms are easily generalized to larger neural networks, and the generalization of Gradient Descent is the standard back-propagation algorithm. In this paper we prove worst-case loss bounds for both algorithms in the single neuron case. Since local minima make it difficult to prove worst-case bounds for gradient-based algorithms, we must use a loss function that prevents the formation of spurious local minima. We define such a matching loss function for any strictly increasing differentiable transfer function and prove worst-case loss bound for any such transfer function and its corresponding matching loss. For example, the matching loss for the identity function is the square loss and the matching loss for the logistic sigmoid is the entropic loss. The different structure of the bounds for the two algorithms indicates that the new algorithm out-performs Gradient Descent when the inputs contain a large number of irrelevant components.},
 author = {Helmbold, David and Kivinen, Jyrki and Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d6723e7cd6735df68d1ce4c704c29a04-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d6723e7cd6735df68d1ce4c704c29a04-Metadata.json},
 openalex = {W2102054292},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d6723e7cd6735df68d1ce4c704c29a04-Paper.pdf},
 publisher = {MIT Press},
 title = {Worst-case Loss Bounds for Single Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/d6723e7cd6735df68d1ce4c704c29a04-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_d6ef5f7f,
 abstract = {This paper discusses the use of multilayer feed forward neural networks for predicting a stock's excess return based on its exposure to various technical and fundamental factors. To demonstrate the effectiveness of the approach a hedged portfolio which consists of equally capitalized long and short positions is constructed and its historical returns are benchmarked against T-bill returns and the S&P500 index.},
 author = {Levin, Asriel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d6ef5f7fa914c19931a55bb262ec879c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d6ef5f7fa914c19931a55bb262ec879c-Metadata.json},
 openalex = {W2135550012},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf},
 publisher = {MIT Press},
 title = {Stock Selection via Nonlinear Multi-Factor Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/d6ef5f7fa914c19931a55bb262ec879c-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_d736bb10,
 author = {Handzel, Amir and Flash, Tamar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d736bb10d83a904aefc1d6ce93dc54b8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d736bb10d83a904aefc1d6ce93dc54b8-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d736bb10d83a904aefc1d6ce93dc54b8-Paper.pdf},
 publisher = {MIT Press},
 title = {The Geometry of Eye Rotations and Listing\textquotesingle s Law},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/d736bb10d83a904aefc1d6ce93dc54b8-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_d8700cbd,
 abstract = {In the Poisson neuron model, the output is a rate-modulated Poisson process (Snyder and Miller, 1991); the time varying rate parameter r(t) is an instantaneous function G[.] of the stimulus, r(t) = G[s(t)]. In a Poisson neuron, then, r(t) gives the instantaneous firing rate--the instantaneous probability of firing at any instant t--and the output is a stochastic function of the input. In part because of its great simplicity, this model is widely used (usually with the addition of a refractory period), especially in in vivo single unit electrophysiological studies, where s(t) is usually taken to be the value of some sensory stimulus. In the integrate-and-fire neuron model, by contrast, the output is a filtered and thresholded function of the input: the input is passed through a low-pass filter (determined by the membrane time constant τ) and integrated until the membrane potential v(t) reaches threshold θ, at which point v(t) is reset to its initial value. By contrast with the Poisson model, in the integrate-and-fire model the ouput is a deterministic function of the input. Although the integrate-and-fire model is a caricature of real neural dynamics, it captures many of the qualitative features, and is often used as a starting point for conceptualizing the biophysical behavior of single neurons. Here we show how a slightly modified Poisson model can be derived from the integrate-and-fire model with noisy inputs y(t) = s(t) + n(t). In the modified model, the transfer function G[.] is a sigmoid (erf) whose shape is determined by the noise variance σn2. Understanding the equivalence between the dominant in vivo and in vitro simple neuron models may help forge links between the two levels.},
 author = {Stevens, Charles and Zador, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d8700cbd38cc9f30cecb34f0c195b137-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d8700cbd38cc9f30cecb34f0c195b137-Metadata.json},
 openalex = {W2147522710},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d8700cbd38cc9f30cecb34f0c195b137-Paper.pdf},
 publisher = {MIT Press},
 title = {When is an Integrate-and-fire Neuron like a Poisson Neuron?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/d8700cbd38cc9f30cecb34f0c195b137-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_d91d1b4d,
 abstract = {We examine the issue of evaluation of model specific parameters in a modified VC-formalism. Two examples are analyzed: the 2-dimensional homogeneous perceptron and the 1-dimensional higher order neuron. Both models are solved theoretically, and their learning curves are compared against true learning curves. It is shown that the formalism has the potential to generate a variety of learning curves, including ones displaying phase transitions.},
 author = {Kowalczyk, Adam and Szymanski, Jacek and Bartlett, Peter and Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d91d1b4d82419de8a614abce9cc0e6d4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d91d1b4d82419de8a614abce9cc0e6d4-Metadata.json},
 openalex = {W2155409512},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/d91d1b4d82419de8a614abce9cc0e6d4-Paper.pdf},
 publisher = {MIT Press},
 title = {Examples of learning curves from a modified VC-formalism},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/d91d1b4d82419de8a614abce9cc0e6d4-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_db2b4182,
 abstract = {Many classification problems have the property that the only costly part of obtaining examples is the class label. This paper suggests a simple method for using distribution information contained in unlabeled examples to augment labeled examples in a supervised training framework. Empirical tests show that the technique described in this paper can significantly improve the accuracy of a supervised learner when the learner is well below its asymptotic accuracy level.},
 author = {Towell, Geoffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/db2b4182156b2f1f817860ac9f409ad7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/db2b4182156b2f1f817860ac9f409ad7-Metadata.json},
 openalex = {W2115959931},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Unlabeled Data for Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/db2b4182156b2f1f817860ac9f409ad7-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_db576a7d,
 author = {Mansour, Yishay and Sahar, Sigal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/db576a7d2453575f29eab4bac787b919-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/db576a7d2453575f29eab4bac787b919-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/db576a7d2453575f29eab4bac787b919-Paper.pdf},
 publisher = {MIT Press},
 title = {Implementation Issues in the Fourier Transform Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/db576a7d2453575f29eab4bac787b919-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_dc58e3a3,
 abstract = {We give a theoretical and experimental analysis of the generalization error of cross validation using two natural measures of the problem under consideration. The approximation rate measures the accuracy to which the target function can be ideally approximated as a function of the number of parameters, and thus captures the complexity of the target function with respect to the hypothesis model. The estimation rate measures the deviation between the training and generalization errors as a function of the number of parameters, and thus captures the extent to which the hypothesis model suffers from overfitting. Using these two measures, we give a rigorous and general bound on the error of the simplest form of cross validation. The bound clearly shows the dangers of making γ —the fraction of data saved for testing—too large or too small. By optimizing the bound with respect to γ, we then argue that the following qualitative properties of cross-validation behavior should be quite robust to significant changes in the underlying model selection problem: When the target function complexity is small compared to the sample size, the performance of cross validation is relatively insensitive to the choice of γ. The importance of choosing γ optimally increases, and the optimal value for γ decreases, as the target function becomes more complex relative to the sample size. There is nevertheless a single fixed value for γ that works nearly optimally for a wide range of target function complexity.},
 author = {Kearns, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/dc58e3a306451c9d670adcd37004f48f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/dc58e3a306451c9d670adcd37004f48f-Metadata.json},
 openalex = {W2043076104},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Bound on the Error of Cross Validation Using the Approximation and Estimation Rates, with Consequences for the Training-Test Split},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/dc58e3a306451c9d670adcd37004f48f-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_dd77279f,
 abstract = {The facial action coding system (FAGS) is an objective method for quantifying facial movement in terms of component actions. This paper explores and compares techniques for automatically recognizing facial actions in sequences of images. These techniques include: analysis of facial motion through estimation of optical flow; holistic spatial analysis, such as principal component analysis, independent component analysis, local feature analysis, and linear discriminant analysis; and methods based on the outputs of local filters, such as Gabor wavelet representations and local principal components. Performance of these systems is compared to naive and expert human subjects. Best performances were obtained using the Gabor wavelet representation and the independent component representation, both of which achieved 96 percent accuracy for classifying 12 facial actions of the upper and lower face. The results provide converging evidence for the importance of using local filters, high spatial frequencies, and statistical independence for classifying facial actions.},
 author = {Bartlett, Marian and Viola, Paul and Sejnowski, Terrence J and Golomb, Beatrice and Larsen, Jan and Hager, Joseph and Ekman, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/dd77279f7d325eec933f05b1672f6a1f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/dd77279f7d325eec933f05b1672f6a1f-Metadata.json},
 openalex = {W2133180260},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/dd77279f7d325eec933f05b1672f6a1f-Paper.pdf},
 publisher = {MIT Press},
 title = {Classifying facial actions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/dd77279f7d325eec933f05b1672f6a1f-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_df0aab05,
 author = {Marchand, Mario and Hadjifaradji, Saeed},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/df0aab058ce179e4f7ab135ed4e641a9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/df0aab058ce179e4f7ab135ed4e641a9-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf},
 publisher = {MIT Press},
 title = {Strong Unimodality and Exact Learning of Constant Depth \mathrm{\mu}-Perceptron Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/df0aab058ce179e4f7ab135ed4e641a9-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_e17184bc,
 abstract = {The process of machine learning can be considered in two stages: model selection and parameter estimation. In this paper a technique is presented for constructing dynamical systems with desired qualitative properties. The approach is based on the fact that an n-dimensional nonlinear dynamical system can be decomposed into one gradient and (n - 1) Hamiltonian systems. Thus, the model selection stage consists of choosing the gradient and Hamiltonian portions appropriately so that a certain behavior is obtainable. To estimate the parameters, a stably convergent learning rule is presented. This algorithm has been proven to converge to the desired system trajectory for all initial conditions and system inputs. This technique can be used to design neural network models which are guaranteed to solve the trajectory learning problem.},
 author = {Howse, James and Abdallah, Chaouki and Heileman, Gregory},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e17184bcb70dcf3942c54e0b537ffc6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e17184bcb70dcf3942c54e0b537ffc6d-Metadata.json},
 openalex = {W2148101780},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e17184bcb70dcf3942c54e0b537ffc6d-Paper.pdf},
 publisher = {MIT Press},
 title = {Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/e17184bcb70dcf3942c54e0b537ffc6d-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_e19347e1,
 abstract = {A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations.},
 author = {Amari, Shun-ichi and Cichocki, Andrzej and Yang, Howard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e19347e1c3ca0c0b97de5fb3b690855a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e19347e1c3ca0c0b97de5fb3b690855a-Metadata.json},
 openalex = {W2133069808},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf},
 publisher = {MIT Press},
 title = {A New Learning Algorithm for Blind Signal Separation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_e2231217,
 abstract = {An adaptive back-propagation algorithm is studied and compared with gradient descent (standard back-propagation) for on-line learning in two-layer neural networks with an arbitrary number of hidden units. Within a statistical mechanics framework, both numerical studies and a rigorous analysis show that the adaptive back-propagation method results in faster training by breaking the symmetry between hidden units more efficiently and by providing faster convergence to optimal generalization than gradient descent.},
 author = {West, Ansgar and Saad, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e22312179bf43e61576081a2f250f845-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e22312179bf43e61576081a2f250f845-Metadata.json},
 openalex = {W2122464180},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e22312179bf43e61576081a2f250f845-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive Back-Propagation in On-Line Learning of Multilayer Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/e22312179bf43e61576081a2f250f845-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_e515df0d,
 abstract = {A unique architecture of winner search hardware has been developed using a novel neuron-like high functionality device called Neuron MOS transistor (or vMOS in short) [1,2] as a key circuit element. The circuits developed in this work can find the location of the maximum (or minimum) signal among a number of input data on the continuous-time basis, thus enabling real-time winner tracking as well as fully-parallel sorting of multiple input data. We have developed two circuit schemes. One is an ensemble of self-loop-selecting vMOS ring oscillators finding the winner as an oscillating node. The other is an ensemble of vMOS variable threshold inverters receiving a common ramp-voltage for competitive excitation where data sorting is conducted through consecutive winner search actions. Test circuits were fabricated by a double-polysilicon CMOS process and their operation has been experimentally verified.},
 author = {Shibata, Tadashi and Nakai, Tsutomu and Morimoto, Tatsuo and Kaihara, Ryu and Yamashita, Takeo and Ohmi, Tadahiro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e515df0d202ae52fcebb14295743063b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e515df0d202ae52fcebb14295743063b-Metadata.json},
 openalex = {W2103541480},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e515df0d202ae52fcebb14295743063b-Paper.pdf},
 publisher = {MIT Press},
 title = {Neuron-MOS Temporal Winner Search Hardware for Fully-Parallel Data Processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/e515df0d202ae52fcebb14295743063b-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_e58cc5ca,
 abstract = {Whereas optical character recognition (OCR) systems learn to classify single characters; people learn to classify long character strings in parallel, within a single fixation. This difference is surprising because high dimensionality is associated with poor classification learning. This paper suggests that the human reading system avoids these problems because the number of to-be-classified images is reduced by consistent and optimal eye fixation positions, and by character sequence regularities.},
 author = {Martin, Gale},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e58cc5ca94270acaceed13bc82dfedf7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e58cc5ca94270acaceed13bc82dfedf7-Metadata.json},
 openalex = {W2098018975},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf},
 publisher = {MIT Press},
 title = {Human Reading and the Curse of Dimensionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/e58cc5ca94270acaceed13bc82dfedf7-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_e5e63da7,
 abstract = {When a sensory system constructs a model of the environment from its input, it might need to verify the model's accuracy. One method of verification is multivariate time-series prediction: a good model could predict the near-future activity of its inputs, much as a good scientific theory predicts future data. Such a predicting model would require copious top-down connections to compare the predictions with the input. That feedback could improve the model's performance in two ways: by biasing internal activity toward expected patterns, and by generating specific error signals if the predictions fail. A proof-of-concept model-an event-driven, computationally efficient layered network, incorporating cortical features like all-excitatory synapses and local inhibition--was constructed to make near-future predictions of a simple, moving stimulus. After unsupervised learning, the network contained units not only tuned to obvious features of the stimulus like contour orientation and motion, but also to contour discontinuity (end-stopping) and illusory contours.},
 author = {Softky, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Metadata.json},
 openalex = {W2145316930},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf},
 publisher = {MIT Press},
 title = {Unsupervised Pixel-prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/e5e63da79fcd2bebbd7cb8bf1c1d0274-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_e8b1cbd0,
 abstract = {Intermediate and higher vision processes require selection of a subset of the available sensory information before further processing. Usually, this selection is implemented in the form of a spatially circumscribed region of the visual field, the so-called of which scans the visual scene dependent on the input and on the attentional state of the subject. We here present a model for the control of the focus of attention in primates, based on a saliency map. This mechanism is not only expected to model the functionality of biological vision but also to be essential for the understanding of complex scenes in machine vision.},
 author = {Niebur, Ernst and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e8b1cbd05f6e6a358a81dee52493dd06-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e8b1cbd05f6e6a358a81dee52493dd06-Metadata.json},
 openalex = {W2117686553},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e8b1cbd05f6e6a358a81dee52493dd06-Paper.pdf},
 publisher = {MIT Press},
 title = {Control of Selective Visual Attention: Modeling the "Where" Pathway},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/e8b1cbd05f6e6a358a81dee52493dd06-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_eddb904a,
 abstract = {The dynamics of complex neural networks must include the aspects of long and short-term memory. The behaviour of the network is characterized by an equation of neural activity as a fast phenomenon and by an equation of synaptic modification as a slow part of the neural system. We present a quadratic-type Lyapunov function for the flow of a competitive neural system with fast and slow dynamic variables.},
 author = {Meyer-B\"{a}se, Anke},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/eddb904a6db773755d2857aacadb1cb0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/eddb904a6db773755d2857aacadb1cb0-Metadata.json},
 openalex = {W1596514494},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf},
 publisher = {MIT Press},
 title = {Quadratic-type Lyapunov functions for competitive neural networks with different time-scales},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/eddb904a6db773755d2857aacadb1cb0-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_f197002b,
 abstract = {It has recently been shown that gradient descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies. In this paper we explore this problem for a class of architectures called NARX networks, which have powerful representational capabilities. Previous work reported that gradient descent learning is more effective in NARX networks than in recurrent networks with hidden states. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on such problems. We present some experimental 'results that show that NARX networks can often retain information for two to three times as long as conventional recurrent networks.},
 author = {Lin, Tsungnan and Horne, Bill and Ti\~{n}o, Peter and Giles, C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/f197002b9a0853eca5e046d9ca4663d5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/f197002b9a0853eca5e046d9ca4663d5-Metadata.json},
 openalex = {W2097682555},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning long-term dependencies is not as difficult with NARX networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/f197002b9a0853eca5e046d9ca4663d5-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_f4dd765c,
 abstract = {The additive clustering (ADCLUS) model (Shepard & Arabie, 1979) treats the similarity of two stimuli as a weighted additive measure of their common features. Inspired by recent work in unsupervised learning with multiple cause models, we propose a new, statistically well-motivated algorithm for discovering the structure of natural stimulus classes using the ADCLUS model, which promises substantial gains in conceptual simplicity, practical efficiency, and solution quality over earlier efforts. We also present preliminary results with artificial data and two classic similarity data sets.},
 author = {Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/f4dd765c12f2ef67f98f3558c282a9cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/f4dd765c12f2ef67f98f3558c282a9cd-Metadata.json},
 openalex = {W2156002312},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/f4dd765c12f2ef67f98f3558c282a9cd-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning the Structure of Similarity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/f4dd765c12f2ef67f98f3558c282a9cd-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_f7f580e1,
 abstract = {We propose a hierarchical scheme for rapid learning of context dependent skills that is based on the recently introduced Parameterized Self-Organizing Map (PSOM). The underlying idea is to first invest some learning effort to specialize the system into a rapid learner for a more restricted range of contexts.

The specialization is carried out by a prior learning stage, during which the system acquires a set of basis mappings or skills for a set of prototypical contexts. Adaptation of a skill to a new context can then be achieved by interpolating in the space of the basis mappings and thus can be extremely rapid.

We demonstrate the potential of this approach for the task of a 3D visuomotor map for a Puma robot and two cameras. This includes the forward and backward robot kinematics in 3D end effector coordinates, the 2D+2D retina coordinates and also the 6D joint angles. After the investment phase the transformation can be learned for a new camera set-up with a single observation.},
 author = {Walter, J\"{o}rg and Ritter, Helge},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/f7f580e11d00a75814d2ded41fe8e8fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/f7f580e11d00a75814d2ded41fe8e8fe-Metadata.json},
 openalex = {W2148962516},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/f7f580e11d00a75814d2ded41fe8e8fe-Paper.pdf},
 publisher = {MIT Press},
 title = {Investment Learning with Hierarchical PSOMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/f7f580e11d00a75814d2ded41fe8e8fe-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_fc2c7c47,
 abstract = {A computational model of song learning in the song sparrow (Melospiza melodia) learns to categorize the different syllables of a song sparrow song and uses this categorization to train itself to reproduce song. The model fills a crucial gap in the computational explanation of birdsong learning by exploring the organization of perception in songbirds. It shows how competitive learning may lead to the organization of a specific nucleus in the bird brain, replicates the song production results of a previous model (Doya and Sejnowski, 1995), and demonstrates how perceptual learning can guide production through reinforcement learning.},
 author = {Fry, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fc2c7c47b918d0c2d792a719dfb602ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fc2c7c47b918d0c2d792a719dfb602ef-Metadata.json},
 openalex = {W2156717522},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf},
 publisher = {MIT Press},
 title = {How Perception Guides Production in Birdsong Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/fc2c7c47b918d0c2d792a719dfb602ef-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_fd06b8ea,
 abstract = {We describe the reinforcement learning problem, motivate algorithms which seek an approximation to the Q function, and present new convergence results for two such algorithms.},
 author = {Gordon, Geoffrey J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Metadata.json},
 openalex = {W2101533993},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Paper.pdf},
 publisher = {MIT Press},
 title = {Stable Fitted Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/fd06b8ea02fe5b1c2496fe1700e9d16c-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_fd2c5e46,
 abstract = {While it is generally agreed that neurons transmit information about their synaptic inputs through spike trains, the code by which this information is transmitted is not well understood. An upper bound on the information encoded is obtained by hypothesizing that the precise timing of each spike conveys information. Here we develop a general approach to quantifying the information carried by spike trains under this hypothesis, and apply it to the leaky integrate-and-fire (IF) model of neuronal dynamics. We formulate the problem in terms of the probability distribution p(T) of interspike intervals (ISIs), assuming that spikes are detected with arbitrary but finite temporal resolution. In the absence of added noise, all the variability in the ISIs could encode information, and the information rate is simply the entropy of the ISI distribution, H(T)= 〈-p(T) log2 p(T)〉, times the spike rate. H(T) thus provides an exact expression for the information rate. The methods developed here can be used to determine experimentally the information carried by spike trains, even when the lower bound of the information rate provided by the stimulus reconstruction method is not tight. In a preliminary series of experiments, we have used these methods to estimate information rates of hippocampal neurons in slice in response to somatic current injection. These pilot experiments suggest information rates as high as 6.3 bits/spike.},
 author = {Stevens, Charles and Zador, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fd2c5e4680d9a01dba3aada5ece22270-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fd2c5e4680d9a01dba3aada5ece22270-Metadata.json},
 openalex = {W2133869598},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fd2c5e4680d9a01dba3aada5ece22270-Paper.pdf},
 publisher = {MIT Press},
 title = {Information through a Spiking Neuron},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/fd2c5e4680d9a01dba3aada5ece22270-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_fe709c65,
 abstract = {Nearest neighbor classification expects the class conditional probabilities to be locally constant, and suffers from bias in high dimensions We propose a locally adaptive form of nearest neighbor classification to try to finesse this curse of dimensionality. We use a local linear discriminant analysis to estimate an effective metric for computing neighborhoods. We determine the local decision boundaries from centroid information, and then shrink neighborhoods in directions orthogonal to these local decision boundaries, and elongate them parallel to the boundaries. Thereafter, any neighborhood-based classifier can be employed, using the modified neighborhoods. We also propose a method for global dimension reduction, that combines local dimension information. We indicate how these techniques can be extended to the regression problem.},
 author = {Hastie, Trevor and Tibshirani, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fe709c654eac84d5239d1a12a4f71877-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fe709c654eac84d5239d1a12a4f71877-Metadata.json},
 openalex = {W2113646052},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/fe709c654eac84d5239d1a12a4f71877-Paper.pdf},
 publisher = {MIT Press},
 title = {Discriminant Adaptive Nearest Neighbor Classification and Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/fe709c654eac84d5239d1a12a4f71877-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_feab05aa,
 abstract = {We present a hypothesis about how the cerebellum could participate in regulating movement in the presence of significant feedback delays without resorting to a forward model of the motor plant. We show how a simplified cerebellar model can learn to control end-point positioning of a nonlinear spring-mass system with realistic delays in both afferent and efferent pathways. The model's operation involves prediction, but instead of predicting sensory input, it directly regulates movement by reacting in an anticipatory fashion to input patterns that include delayed sensory feedback.},
 author = {Barto, Andrew and Houk, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/feab05aa91085b7a8012516bc3533958-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/feab05aa91085b7a8012516bc3533958-Metadata.json},
 openalex = {W2146258231},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/feab05aa91085b7a8012516bc3533958-Paper.pdf},
 publisher = {MIT Press},
 title = {A Predictive Switching Model of Cerebellar Movement Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/feab05aa91085b7a8012516bc3533958-Abstract.html},
 volume = {8},
 year = {1995}
}

@inproceedings{NIPS1995_ffeed84c,
 abstract = {In this paper we propose recurrent neural networks with feedback into the input units for handling two types of data analysis problems. On the one hand, this scheme can be used for static data when some of the input variables are missing. On the other hand, it can also be used for sequential data, when some of the input variables are missing or are available at different frequencies. Unlike in the case of probabilistic models (e.g. Gaussian) of the missing variables, the network does not attempt to model the distribution of the missing variables given the observed variables. Instead it is a more discriminant approach that fills in the missing variables for the sole purpose of minimizing a learning criterion (e.g., to minimize an output error).},
 author = {Bengio, Yoshua and Gingras, Francois},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1995/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1995/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Metadata.json},
 openalex = {W2139772141},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Paper.pdf},
 publisher = {MIT Press},
 title = {Recurrent Neural Networks for Missing or Asynchronous Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1995/hash/ffeed84c7cb1ae7bf4ec4bd78275bb98-Abstract.html},
 volume = {8},
 year = {1995}
}
