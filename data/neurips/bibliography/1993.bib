@inproceedings{NIPS1993_013a006f,
 abstract = {The back propagation algorithm has been modified to work without any multiplications and to tolerate computations with a low resolution, which makes it. more attractive for a hardware implementation. Numbers are represented in floating point format with 1 bit mantissa and 3 bits in the exponent for the states, and 1 bit mantissa and 5 bit exponent. for the gradients, while the weights are 16 bit fixed-point numbers. In this way, all the computations can be executed with shift and add operations. Large networks with over 100,000 weights were trained and demonstrated the same performance as networks computed with full precision. An estimate of a circuit implementation shows that a large network can be placed on a single chip, reaching more than 1 billion weight updates per second. A speedup is also obtained on any machine where a multiplication is slower than a shift operation.},
 author = {Simard, Patrice and Graf, Hans},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/013a006f03dbc5392effeb8f18fda755-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/013a006f03dbc5392effeb8f18fda755-Metadata.json},
 openalex = {W2149428536},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Backpropagation without Multiplication},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_02a32ad2,
 abstract = {Selecting a good model of a set of input points by cross validation is a computationally intensive process, especially if the number of possible models or the number of training points is high. Techniques such as gradient descent are helpful in searching through the space of models, but problems such as local minima, and more importantly, lack of a distance metric between various models reduce the applicability of these search methods. Hoeffding Races is a technique for finding a good model for the data by quickly discarding bad models, and concentrating the computational effort at differentiating between the better ones. This paper focuses on the special case of leave-one-out cross validation applied to memory-based learning algorithms, but we also argue that it is applicable to any class of model selection problems.},
 author = {Maron, Oded and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/02a32ad2669e6fe298e607fe7cc0e1a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/02a32ad2669e6fe298e607fe7cc0e1a0-Metadata.json},
 openalex = {W2165962877},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/02a32ad2669e6fe298e607fe7cc0e1a0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/02a32ad2669e6fe298e607fe7cc0e1a0-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_0537fb40,
 abstract = {The past several years have seen a tremendous growth in the complexity of the recognition, estimation and control tasks expected of neural networks. In solving these tasks, one is faced with a large variety of learning algorithms and a vast selection of possible network architectures. After all the training, how does one know which is the best network? This decision is further complicated by the fact that standard techniques can be severely limited by problems such as over-fitting, data sparsity and local optima. The usual solution to these problems is a winner-take-all cross-validatory model selection. However, recent experimental and theoretical work indicates that we can improve performance by considering methods for combining neural networks.},
 author = {Perrone, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/0537fb40a68c18da59a35c2bfe1ca554-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/0537fb40a68c18da59a35c2bfe1ca554-Metadata.json},
 openalex = {W2142671416},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/0537fb40a68c18da59a35c2bfe1ca554-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Putting It All Together: Methods for Combining Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/0537fb40a68c18da59a35c2bfe1ca554-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_06997f04,
 abstract = {Integrated Mean Squared Error (IMSE) is a version of the usual mean squared error criterion, averaged over all possible training sets of a given size. If it could be observed, it could be used to determine optimal network complexity or optimal data subsets for efficient training. We show that two common methods of cross-validating average squared error deliver unbiased estimates of IMSE, converging to IMSE with probability one. These estimates thus make possible approximate IMSE-based choice of network complexity. We also show that two variants of cross validation measure provide unbiased IMSE-based estimates potentially useful for selecting optimal data subsets.},
 author = {Plutowski, Mark and Sakata, Shinichi and White, Halbert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/06997f04a7db92466a2baa6ebc8b872d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/06997f04a7db92466a2baa6ebc8b872d-Metadata.json},
 openalex = {W2171359060},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/06997f04a7db92466a2baa6ebc8b872d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Cross-Validation Estimates IMSE},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/06997f04a7db92466a2baa6ebc8b872d-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_08419be8,
 abstract = {We propose a learning algorithm for a variable memory length Markov process. Human communication, whether given as text, handwriting, or speech, has multi characteristic time scales. On short scales it is characterized mostly by the dynamics that generate the process, whereas on large scales, more syntactic and semantic information is carried. For that reason the conventionally used fixed memory Markov models cannot capture effectively the complexity of such structures. On the other hand using long memory models uniformly is not practical even for as short memory as four. The algorithm we propose is based on minimizing the statistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small. We demonstrate the algorithm by learning the structure of natural English text and applying the learned model to the correction of corrupted text. Using less than 3000 states the model's performance is far superior to that of fixed memory models with similar number of states. We also show how the algorithm can be applied to intergenic E. coli DNA base prediction with results comparable to HMM based methods.},
 author = {Ron, Dana and Singer, Yoram and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/08419be897405321542838d77f855226-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/08419be897405321542838d77f855226-Metadata.json},
 openalex = {W2157140289},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/08419be897405321542838d77f855226-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Power of Amnesia},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/08419be897405321542838d77f855226-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_0d3180d6,
 abstract = {We prove that the so called for (recurrent) neural networks is unsolvable. This extends several results which already demonstrated that training and related design problems for neural networks are (at least) NP-complete. Our result also implies that it is impossible to find or to formulate a universal training algorithm, which for any neural network architecture could determine a correct set of weights. For the simple proof of this, we will just show that the loading problem is equivalent to Hilbert's tenth which is known to be unsolvable.},
 author = {Wiklicky, Herbert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/0d3180d672e08b4c5312dcdafdf6ef36-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/0d3180d672e08b4c5312dcdafdf6ef36-Metadata.json},
 openalex = {W2125383297},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {On the Non-Existence of a Universal Learning Algorithm for Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/0d3180d672e08b4c5312dcdafdf6ef36-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_1141938b,
 abstract = {We present an algorithm for the training of feedforward and recurrent neural networks. It detects internal representation conflicts and uses these conflicts in a constructive manner to add new neurons to the network. The advantages are twofold: (1) starting with a small network neurons are only allocated when required; (2) by detecting and resolving internal conflicts at an early stage learning time is reduced. Empirical results on two real-world problems substantiate the faster learning speed; when applied to the training of a recurrent network on a well researched sequence recognition task (the Reber grammar), training times are significantly less than previously reported.},
 author = {Leerink, Laurens and Jabri, Marwan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1141938ba2c2b13f5505d7c424ebae5f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1141938ba2c2b13f5505d7c424ebae5f-Metadata.json},
 openalex = {W2145586232},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Constructive Learning Using Internal Representation Conflicts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/1141938ba2c2b13f5505d7c424ebae5f-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_185e65bc,
 abstract = {This paper presents a simple algorithm to learn trajectories with a continuous time, continuous activation version of the Boltzmann machine. The algorithm takes advantage of intrinsic Brownian noise in the network to easily compute gradients using entirely local computations. The algorithm may be ideal for parallel hardware implementations.},
 author = {Movellan, Javier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/185e65bc40581880c4f2c82958de8cfe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/185e65bc40581880c4f2c82958de8cfe-Metadata.json},
 openalex = {W2160918807},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/185e65bc40581880c4f2c82958de8cfe-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Local Algorithm to Learn Trajectories with Stochastic Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/185e65bc40581880c4f2c82958de8cfe-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_1905aeda,
 abstract = {We show how randomly scrambling the output classes of various fractions of the training data may be used to improve predictive accuracy of a classification algorithm. We present a method for calculating the sensitivity of a learning algorithm which is based on scrambling the output classes. This signature can be used to indicate a good match between the complexity of the classifier and the complexity of the data. Use of noise sensitivity signatures is distinctly different from other schemes to avoid overtraining, such as cross-validation, which uses only part of the training data, or various penalty functions, which are not data-adaptive. Noise sensitivity signature methods use all of the training data and are manifestly data-adaptive and non-parametric. They are well suited for situations with limited training data.},
 author = {Grossman, Tal and Lapedes, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1905aedab9bf2477edc068a355bba31a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1905aedab9bf2477edc068a355bba31a-Metadata.json},
 openalex = {W2235280489},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1905aedab9bf2477edc068a355bba31a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Use of Bad Training Data for Better Predictions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/1905aedab9bf2477edc068a355bba31a-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_19b65066,
 abstract = {We have developed an artificial neural network based gaze tracking system which can be customized to individual users. A three layer feed forward network, trained with standard error back propagation, is used to determine the position of a user''s gaze from the appearance of the user''s eye. Unlike other gaze trackers, which normally require the user to wear cumbersome headgear, or to use a chin rest to ensure head immobility, our system is entirely non-intrusive. Currently, the best intrusive gaze tracking systems are accurate to approximately 0.75 degrees. In our experiments, we have been able to achieve an accuracy of 1.5 degrees, while allowing head mobility. In its current implementation, our system works at 15 hz. In this paper we present an empirical analysis of the performance of a large number of artificial neural network architectures for this task. Suggestions for further explorations for neurally based gaze trackers are presented, and are related to other similar artificial neural network applications such as autonomous road following.},
 author = {Baluja, Shumeet and Pomerleau, Dean},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/19b650660b253761af189682e03501dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/19b650660b253761af189682e03501dd-Metadata.json},
 openalex = {W2103289133},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/19b650660b253761af189682e03501dd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Non-Intrusive Gaze Tracking Using Artificial Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/19b650660b253761af189682e03501dd-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_1aa48fc4,
 abstract = {Training classifiers on large databases is computationally demanding. It is desirable to develop efficient procedures for a reliable prediction of a classifier's suitability for implementing a given task, so that resources can be assigned to the most promising candidates or freed for exploring new classifier candidates. We propose such a practical and principled predictive method. Practical because it avoids the costly procedure of training poor classifiers on the whole training set, and principled because of its theoretical foundation. The effectiveness of the proposed procedure is demonstrated for both single- and multi-layer networks.},
 author = {Cortes, Corinna and Jackel, L. D. and Solla, Sara and Vapnik, Vladimir and Denker, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Metadata.json},
 openalex = {W2134429390},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Curves: Asymptotic Values and Rate of Convergence},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/1aa48fc4880bb0c9b8a3bf979d3b917e-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_1efa39bc,
 abstract = {What is the 'correct' theoretical description of neuronal activity? The analysis of the dynamics of a globally connected network of spiking neurons (the Spike Response Model) shows that a description by mean firing rates is possible only if active neurons fire incoherently. If firing occurs coherently or with spatio-temporal correlations, the spike structure of the neural code becomes relevant. Alternatively, neurons can be gathered into local or distributed ensembles or 'assemblies'. A description based on the mean ensemble activity is, in principle, possible but the interaction between different assemblies becomes highly nonlinear. A description with spikes should therefore be preferred.},
 author = {Gerstner, Wulfram and van Hemmen, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1efa39bcaec6f3900149160693694536-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1efa39bcaec6f3900149160693694536-Metadata.json},
 openalex = {W2120023793},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1efa39bcaec6f3900149160693694536-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {How to Describe Neuronal Activity: Spikes, Rates, or Assemblies?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/1efa39bcaec6f3900149160693694536-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_1fc21400,
 abstract = {We built a high-speed, digital mean-field Boltzmann chip and SBus board for general problems in constraint satisfaction and learning. Each chip has 32 neural processors and 4 weight update processors, supporting an arbitrary topology of up to 160 functional neurons. On-chip learning is at a theoretical maximum rate of 3.5 × 108 connection updates/sec; recall is 12000 patterns/sec for typical conditions. The chip's high speed is due to parallel computation of inner products, limited (but adequate) precision for weights and activations (5 bits), fast clock (125 MHz), and several design insights.},
 author = {Murray, Michael and Leung, Ming-Tak and Boonyanit, Kan and Kritayakirana, Kong and Burg, James and Wolff, Gregory and Watanabe, Tokahiro and Schwartz, Edward and Stork, David and Peterson, Allen M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1fc214004c9481e4c8073e85323bfd4b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1fc214004c9481e4c8073e85323bfd4b-Metadata.json},
 openalex = {W2104628291},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/1fc214004c9481e4c8073e85323bfd4b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Digital Boltzmann VLSI for constraint satisfaction and learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/1fc214004c9481e4c8073e85323bfd4b-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_217eedd1,
 abstract = {The feature correspondence problem is a classic hurdle in visual object-recognition concerned with determining the correct mapping between the features measured from the image and the features expected by the model. In this paper we show that determining good correspondences requires information about the joint probability density over the image features. We propose based correspondence matching as a general principle for selecting optimal correspondences. The approach is applicable to non-rigid models, allows nonlinear perspective transformations, and can optimally deal with occlusions and missing features. Experiments with rigid and non-rigid 3D hand gesture recognition support the theory. The likelihood based techniques show almost no decrease in classification performance when compared to performance with perfect correspondence knowledge.},
 author = {Ahmad, Subutai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/217eedd1ba8c592db97d0dbe54c7adfc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/217eedd1ba8c592db97d0dbe54c7adfc-Metadata.json},
 openalex = {W2119837152},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/217eedd1ba8c592db97d0dbe54c7adfc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Feature Densities are Required for Computing Feature Correspondences},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/217eedd1ba8c592db97d0dbe54c7adfc-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_22ac3c5a,
 abstract = {A variant of the encoder architecture, where units at the input and output layers represent nodes on a graph, is applied to the task of mapping locations to sets of neighboring locations. The degree to which the resulting internal (i.e. hidden unit) representations reflect global properties of the environment depends upon several parameters of the learning procedure. Architectural bottlenecks, noise, and incremental learning of landmarks are shown to be important factors in maintaining topographic relationships at a global scale.},
 author = {Ghiselli-Crippa, Thea and Munro, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/22ac3c5a5bf0b520d281c122d1490650-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/22ac3c5a5bf0b520d281c122d1490650-Metadata.json},
 openalex = {W2117869415},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/22ac3c5a5bf0b520d281c122d1490650-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Emergence of Global Structure from Local Associations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/22ac3c5a5bf0b520d281c122d1490650-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_22fb0cee,
 abstract = {The goal of this workshop was to discuss two major issues: efficient exploration of a learner's state space, and learning in continuous domains. The common themes that emerged in presentations and in discussion were the importance of choosing one's domain assumptions carefully, mixing controllers/strategies, avoidance of catastrophic failure, new approaches with difficulties with reinforcement learning, and the importance of task transfer.},
 author = {Cohn, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/22fb0cee7e1f3bde58293de743871417-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/22fb0cee7e1f3bde58293de743871417-Metadata.json},
 openalex = {W2132879544},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Robot Learning: Exploration and Continuous Domains},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/22fb0cee7e1f3bde58293de743871417-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_2823f479,
 abstract = {Using a quasi-realistic model of the feedback inhibition ofmotoneurons (MNs) by Renshaw cells, we show that weak inhibition is sufficient to maximally desynchronize MNs, with negligible effects on total MN activity. MN synchrony can produce a 20 - 30 Hz peak in the force power spectrum, which may cause instability in feedback loops.},
 author = {Maltenfort, Mitchell and Druzinsky, Robert and Heckman, C. and Rymer, W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2823f4797102ce1a1aec05359cc16dd9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2823f4797102ce1a1aec05359cc16dd9-Metadata.json},
 openalex = {W2111117992},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2823f4797102ce1a1aec05359cc16dd9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Lower Boundaries of Motoneuron Desynchronization via Renshaw Interneurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_28267ab8,
 abstract = {Catastrophic forgetting occurs when connectionist networks learn new information, and by so doing, forget all previously learned information. This workshop focused primarily on the causes of catastrophic interference, the techniques that have been developed to reduce it, the effect of these techniques on the networks' ability to generalize, and the degree to which prediction of catastrophic forgetting is possible. The speakers were Robert French, Phil Hetherington (Psychology Department, McGill University, het@blaise.psych.mcgill.ca), and Stephan Lewandowsky (Psychology Department, University of Oklahoma, lewan@constellation.ecn.uoknor.edu).},
 author = {French, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/28267ab848bcf807b2ed53c3a8f8fc8a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/28267ab848bcf807b2ed53c3a8f8fc8a-Metadata.json},
 openalex = {W2159246181},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/28267ab848bcf807b2ed53c3a8f8fc8a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Catastrophic interference in connectionist networks: Can It Be predicted, can It be prevented?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/28267ab848bcf807b2ed53c3a8f8fc8a-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_288cc0ff,
 author = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S\"{a}ckinger, Eduard and Shah, Roopak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Signature Verification using a "Siamese" Time Delay Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_2a084e55,
 abstract = {This workshop explored machine learning approaches to 3 topics: (1) finding structure in music (analysis, continuation, and completion of an unfinished piece), (2) modeling perception of time (extraction of musical meter, explanation of human data on timing), and (3) interpolation in timbre space.},
 author = {Weigend, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Metadata.json},
 openalex = {W2122753397},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connectionism for Music and Audition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/2a084e55c87b1ebcdaad1f62fdbbac8e-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_2afe4567,
 abstract = {Data clustering amounts to a combinatorial optimization problem to reduce the complexity of a data representation and to increase its precision. Central and pairwise data clustering are studied in the maximum entropy framework. For central clustering we derive a set of reestimation equations and a minimization procedure which yields an optimal number of clusters, their centers and their cluster probabilities. A meanfield approximation for pairwise clustering is used to estimate assignment probabilities. A selfconsistent solution to multidimensional scaling and pairwise clustering is derived which yields an optimal embedding and clustering of data points in a d-dimensional Euclidian space.},
 author = {Buhmann, Joachim and Hofmann, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2afe4567e1bf64d32a5527244d104cea-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2afe4567e1bf64d32a5527244d104cea-Metadata.json},
 openalex = {W2134725327},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Central and Pairwise Data Clustering by Competitive Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/2afe4567e1bf64d32a5527244d104cea-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_2ca65f58,
 abstract = {Visual spatial information is projected from the retina to the brain in a highly topographic fashion, so that 2-D visual space is represented in a simple retinotopic map. Auditory spatial information, by contrast, has to be computed from binaural time and intensity differences as well as from monaural spectral cues produced by the head and ears. Evaluation of these cues in the central nervous system leads to the generation of neurons that are sensitive to the location of a sound source in space (spatial tuning) and, in some animal species, to auditory space maps where spatial location is encoded as a 2-D map just like in the visual system. The brain structures thought to be involved in the multimodal integration of visual and auditory spatial integration are the superior colliculus in the midbrain and the inferior parietal lobe in the cerebral cortex.},
 author = {Rauschecker, Josef and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Metadata.json},
 openalex = {W2121208559},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Processing of Visual and Auditory Space and Its Modification by Experience},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_2dace78f,
 author = {Rosen, Daniel and Rumelhart, David and Knudsen, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2dace78f80bc92e6d7493423d729448e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2dace78f80bc92e6d7493423d729448e-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Connectionist Model of the Owl\textquotesingle s Sound Localization System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/2dace78f80bc92e6d7493423d729448e-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_2e65f2f2,
 abstract = {Functional complexity of a software module can be measured in terms of static complexity metrics of the program text. Classifying software modules, based on their static complexity measures, into different fault-prone categories is a difficult problem in software engineering. This research investigates the applicability of neural network classifiers for identifying fault-prone software modules using a data set from a commercial software system. A preliminary empirical comparison is performed between a minimum distance based Gaussian classifier, a perceptron classifier and a multilayer layer feed-forward network classifier constructed using a modified Cascade-Correlation algorithm. The modified version of the Cascade-Correlation algorithm constrains the growth of the network size by incorporating a cross-validation check during the output layer training phase. Our preliminary results suggest that a multilayer feed-forward network can be used as a tool for identifying fault-prone software modules early during the development cycle. Other issues such as representation of software metrics and selection of a proper training samples are also discussed.},
 author = {Karunanithi, N.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2e65f2f2fdaf6c699b223c61b1b5ab89-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2e65f2f2fdaf6c699b223c61b1b5ab89-Metadata.json},
 openalex = {W2103364859},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/2e65f2f2fdaf6c699b223c61b1b5ab89-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Identifying Fault-Prone Software Modules Using Feed-Forward Networks: A Case Study},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/2e65f2f2fdaf6c699b223c61b1b5ab89-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_301ad0e3,
 abstract = {A fast event-driven software simulator has been developed for simulating large networks of spiking neurons and synapses. The primitive network elements are designed to exhibit biologically realistic behaviors, such as spiking, refractoriness, adaptation, axonal delays, summation of post-synaptic current pulses, and tonic current inputs. The efficient event-driven representation allows large networks to be simulated in a fraction of the time that would be required for a full compartmental-model simulation. Corresponding analog CMOS VLSI circuit primitives have been designed and characterized, so that large-scale circuits may be simulated prior to fabrication.},
 author = {Watts, Lloyd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/301ad0e3bd5cb1627a2044908a42fdc2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/301ad0e3bd5cb1627a2044908a42fdc2-Metadata.json},
 openalex = {W2115205567},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Event-Driven Simulation of Networks of Spiking Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/301ad0e3bd5cb1627a2044908a42fdc2-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_31839b03,
 abstract = {We show how an Elman network architecture, constructed from recurrently connected oscillatory associative memory network modules, can employ selective attentional control of synchronization to direct the flow of communication and computation within the architecture to solve a grammatical inference problem.

previously we have shown how the discrete time Elman network algorithm can be implemented in a network completely described by continuous ordinary differential equations. The time steps (machine cycles) of the system are implemented by rhythmic variation (clocking) of a bifurcation parameter. In this architecture, oscillation amplitude codes the information content or activity of a module (unit), whereas phase and frequency are used to softwire the network. Only synchronized modules communicate by exchanging amplitude information; the activity of non-resonating modules contributes incoherent crosstalk noise.

Attentional control is modeled as a special subset of the hidden modules with ouputs which affect the resonant frequencies of other hidden modules. They control synchrony among the other modules and direct the flow of computation (attention) to effect transitions between two subgraphs of a thirteen state automaton which the system emulates to generate a Reber grammar. The internal crosstalk noise is used to drive the required random transitions of the automaton.},
 author = {Baird, Bill and Troyer, Todd and Eeckman, Frank},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/31839b036f63806cba3f47b93af8ccb5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/31839b036f63806cba3f47b93af8ccb5-Metadata.json},
 openalex = {W2116801250},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Grammatical Inference by Attentional Control of Synchronization in an Oscillating Elman Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/31839b036f63806cba3f47b93af8ccb5-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_32b30a25,
 abstract = {Recent work by Becker and Hinton (Becker and Hinton, 1992) shows a promising mechanism, based on maximizing mutual information assuming spatial coherence, by which a system can self-organize itself to learn visual abilities such as binocular stereo. We introduce a more general criterion, based on Bayesian probability theory, and thereby demonstrate a connection to Bayesian theories of visual perception and to other organization principles for early vision (Atick and Redlich, 1990). Methods for implementation using variants of stochastic learning are described and, for the special case of linear filtering, we derive an analytic expression for the output.},
 author = {Yuille, Alan L and Smirnakis, Stelios and Xu, Lei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/32b30a250abd6331e03a2a1f16466346-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/32b30a250abd6331e03a2a1f16466346-Metadata.json},
 openalex = {W2165576249},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bayesian Self-Organization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/32b30a250abd6331e03a2a1f16466346-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_35240722,
 abstract = {We use mean-field theory methods from Statistical Mechanics to derive the softmax nonlinearity from the discontinuous winner-take-all (WTA) mapping. We give two simple ways of implementing softmax as a multiterminal network element. One of these has a number of important network-theoretic properties. It is a reciprocal, passive, incrementally passive, nonlinear, resistive multiterminal element with a content function having the form of information-theoretic entropy. These properties should enable one to use this element in nonlinear RC networks with such other reciprocal elements as resistive fuses and constraint boxes to implement very high speed analog optimization algorithms using a minimum of hardware.},
 author = {Elfadel, I. M. and Wyatt, Jr., J. L.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/352407221afb776e3143e8a1a0577885-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/352407221afb776e3143e8a1a0577885-Metadata.json},
 openalex = {W2169207244},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/352407221afb776e3143e8a1a0577885-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The "Softmax" Nonlinearity: Derivation Using Statistical Mechanics and Useful Properties as a Multiterminal Analog Circuit Element},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/352407221afb776e3143e8a1a0577885-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_35cf8659,
 abstract = {In this paper, it is shown that the conventional back-propagation (BPP) algorithm for neural network regression is robust to leverages (data with x corrupted), but not to outliers (data with y corrupted). A robust model is to model the error as a mixture of normal distribution. The influence function for this mixture model is calculated and the condition for the model to be robust to outliers is given. EM algorithm [5] is used to estimate the parameter. The usefulness of model selection criteria is also discussed. Illustrative simulations are performed.},
 author = {Liu, Yong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/35cf8659cfcb13224cbd47863a34fc58-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/35cf8659cfcb13224cbd47863a34fc58-Metadata.json},
 openalex = {W2110301375},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/35cf8659cfcb13224cbd47863a34fc58-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Robust Parameter Estimation and Model Selection for Neural Network Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/35cf8659cfcb13224cbd47863a34fc58-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_3621f145,
 abstract = {We consider the problem of how the CNS learns to control dynamics of a mechanical system. By using a paradigm where a subject's hand interacts with a virtual mechanical environment, we show that learning control is via composition of a model of the imposed dynamics. Some properties of the computational elements with which the CNS composes this model are inferred through the generalization capabilities of the subject outside the training data.},
 author = {Shadmehr, Reza and Mussa-Ivaldi, Ferdinando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3621f1454cacf995530ea53652ddf8fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3621f1454cacf995530ea53652ddf8fb-Metadata.json},
 openalex = {W2116576179},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Computational Elements of the Adaptive Controller of the Human Arm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_362e80d4,
 abstract = {Transition point dynamic programming (TPDP) is a memory-based, reinforcement learning, direct dynamic programming approach to adaptive optimal control that can reduce the learning time and memory usage required for the control of continuous stochastic dynamic systems. TPDP does so by determining an ideal set of transition points (TPs) which specify only the control action changes necessary for optimal control. TPDP converges to an ideal TP set by using a variation of Q-learning to assess the merits of adding, swapping and removing TPs from states throughout the state space. When applied to a race track problem, TPDP learned the optimal control policy much sooner than conventional Q-learning, and was able to do so using less memory.},
 author = {Buckland, Kenneth and Lawrence, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/362e80d4df43b03ae6d3f8540cd63626-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/362e80d4df43b03ae6d3f8540cd63626-Metadata.json},
 openalex = {W2163908189},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/362e80d4df43b03ae6d3f8540cd63626-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Transition Point Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/362e80d4df43b03ae6d3f8540cd63626-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_3a835d32,
 abstract = {The fovea of a mammal retina was simulated with its detailed biological properties to study the local preprocessing of images. The direct visual pathway (photoreceptors, bipolar and ganglion cells) and the horizontal units, as well as the D-amacrine cells were simulated. The computer program simulated the analog nonspiking transmission between photoreceptor and bipolar cells, and between bipolar and ganglion cells, as well as the gap-junctions between horizontal cells, and the release of dopamine by D-amacrine cells and its diffusion in the extra-cellular space. A 64 × 64 photoreceptors retina, containing 16,448 units, was carried out. This retina displayed contour extraction with a Mach effect, and adaptation to brightness. The simulation showed that the dopaminergic amacrine cells were necessary to ensure adaptation to local brightness.},
 author = {Boussard, Eric and Vibert, Jean-Fran\c{c}ois},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3a835d3215755c435ef4fe9965a3f2a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3a835d3215755c435ef4fe9965a3f2a0-Metadata.json},
 openalex = {W2151693891},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dopaminergic Neuromodulation Brings a Dynamical Plasticity to the Retina},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/3a835d3215755c435ef4fe9965a3f2a0-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_3b3dbaf6,
 abstract = {We describe the relationship between certain reinforcement learning (RL) methods based on dynamic programming (DP) and a class of unorthodox Monte Carlo methods for solving systems of linear equations proposed in the 1950's. These methods recast the solution of the linear system as the expected value of a statistic suitably defined over sample paths of a Markov chain. The significance of our observations lies in arguments (Curtiss, 1954) that these Monte Carlo methods scale better with respect to state-space size than do standard, iterative techniques for solving systems of linear equations. This analysis also establishes convergence rate estimates. Because methods used in RL systems for approximating the evaluation function of a fixed control policy also approximate solutions to systems of linear equations, the connection to these Monte Carlo methods establishes that algorithms very similar to TD algorithms (Sutton, 1988) are asymptotically more efficient in a precise sense than other methods for evaluating policies. Further, all DP-based RL methods have some of the properties of these Monte Carlo algorithms, which suggests that although RL is often perceived to be slow, for sufficiently large problems, it may in fact be more efficient than other known classes of methods capable of producing the same results.},
 author = {Barto, Andrew and Duff, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3b3dbaf68507998acd6a5a5254ab2d76-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3b3dbaf68507998acd6a5a5254ab2d76-Metadata.json},
 openalex = {W2126217565},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Monte Carlo Matrix Inversion and Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/3b3dbaf68507998acd6a5a5254ab2d76-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_3b5dca50,
 abstract = {We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution annotated images where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network which can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors.},
 author = {Bengio, Yoshua and LeCun, Yann and Henderson, Donnie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3b5dca501ee1e6d8cd7b905f4e1bf723-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3b5dca501ee1e6d8cd7b905f4e1bf723-Metadata.json},
 openalex = {W2148695089},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3b5dca501ee1e6d8cd7b905f4e1bf723-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Globally Trained Handwritten Word Recognizer using Spatial Representation, Convolutional Neural Networks, and Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/3b5dca501ee1e6d8cd7b905f4e1bf723-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_3d8e28ca,
 abstract = {While exploring to find better solutions, an agent performing online reinforcement learning (RL) can perform worse than is acceptable. In some cases, exploration might have unsafe, or even catastrophic, results, often modeled in terms of reaching 'failure' states of the agent's environment. This paper presents a method that uses domain knowledge to reduce the number of failures during exploration. This method formulates the set of actions from which the RL agent composes a control policy to ensure that exploration is conducted in a policy space that excludes most of the unacceptable policies. The resulting action set has a more abstract relationship to the task being solved than is common in many applications of RL. Although the cost of this added safety is that learning may result in a suboptimal solution, we argue that this is an appropriate tradeoff in many problems. We illustrate this method in the domain of motion planning.},
 author = {Singh, Satinder and Barto, Andrew and Grupen, Roderic and Connolly, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3d8e28caf901313a554cebc7d32e67e5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3d8e28caf901313a554cebc7d32e67e5-Metadata.json},
 openalex = {W2149398074},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/3d8e28caf901313a554cebc7d32e67e5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Robust Reinforcement Learning in Motion Planning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/3d8e28caf901313a554cebc7d32e67e5-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_43fa7f58,
 abstract = {We study the problem of when to stop learning a class of feedforward networks - networks with linear outputs neuron and fixed input weights - when they are trained with a gradient descent algorithm on a finite number of examples. Under general regularity conditions, it is shown that there are in general three distinct phases in the generalization performance in the learning process, and in particular, the network has better generalization performance when learning is stopped at a certain time before the global minimum of the empirical error is reacherd. A notion of effective size of a machine is defined and used to explain the trade-off between the complexity of the machine and the training error in the learning process. The study leads naturally to a network size selection criterion, which turns out to be a generalization of Akaike's Information Criterion for the learning process. It is shown that stopping learning before the global minimum of the empirical error has the effect of network size selection.},
 author = {Wang, Changfeng and Venkatesh, Santosh and Judd, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/43fa7f58b7eac7ac872209342e62e8f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/43fa7f58b7eac7ac872209342e62e8f1-Metadata.json},
 openalex = {W2153743018},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/43fa7f58b7eac7ac872209342e62e8f1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Stopping and Effective Machine Complexity in Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/43fa7f58b7eac7ac872209342e62e8f1-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_43feaeee,
 abstract = {Recently, Ott, Grebogi and Yorke (OGY) [6] found an effective method to control chaotic systems to unstable fixed points by using only small control forces; however, OGY's method is based on and limited to a linear theory and requires considerable knowledge of the dynamics of the system to be controlled. In this paper we use two radial basis function networks: one as a model of an unknown plant and the other as the controller. The controller is trained with a recurrent learning algorithm to minimize a novel objective function such that the controller can locate an unstable fixed point and drive the system into the fixed point with no a priori knowledge of the system dynamics. Our results indicate that the neural controller offers many advantages over OGY's technique.},
 author = {Flake, Gary and Sun, Guo-Zhen and Lee, Yee-Chun},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/43feaeeecd7b2fe2ae2e26d917b6477d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/43feaeeecd7b2fe2ae2e26d917b6477d-Metadata.json},
 openalex = {W2140105758},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/43feaeeecd7b2fe2ae2e26d917b6477d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Exploiting Chaos to Control the Future},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/43feaeeecd7b2fe2ae2e26d917b6477d-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_4558dbb6,
 abstract = {The fundamental backpropagation (BP) algorithm for training artificial neural networks is cast as a deterministic nonmonotone perturbed gradient method. Under certain natural assumptions, such as the series of learning rates diverging while the series of their squares converging, it is established that every accumulation point of the online BP iterates is a stationary point of the BP error function. The results presented cover serial and parallel online BP, modified BP with a momentum term, and BP with weight decay.},
 author = {Mangasarian, O. L. and Solodov, M. V.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Metadata.json},
 openalex = {W2160257547},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Backpropagation Convergence Via Deterministic Nonmonotone Perturbed Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/4558dbb6f6f8bb2e16d03b85bde76e2c-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_470e7a4f,
 author = {Kolen, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/470e7a4f017a5476afb7eeb3f8b96f9b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/470e7a4f017a5476afb7eeb3f8b96f9b-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Fool\textquotesingle s Gold: Extracting Finite State Machines from Recurrent Network Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/470e7a4f017a5476afb7eeb3f8b96f9b-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_49c9adb1,
 abstract = {The Singular Value (SVD) is an important tool for linear algebra and can be used to invert or approximate matrices. Although many authors use synonymously with Decomposition or Principal Components Transform, it is important to realize that these other methods apply only to symmetric matrices, while the SVD can be applied to arbitrary nonsquare matrices. This property is important for applications to signal transmission and control.

I propose two new algorithms for iterative computation of the SVD given only sample inputs and outputs from a matrix. Although there currently exist many algorithms for Eigenvector (Sanger 1989, for example), these are the first true sample-based SVD algorithms.},
 author = {Sanger, Terence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/49c9adb18e44be0711a94e827042f630-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/49c9adb18e44be0711a94e827042f630-Metadata.json},
 openalex = {W2106863551},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/49c9adb18e44be0711a94e827042f630-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Two Iterative Algorithms for Computing the Singular Value Decomposition from Input/Output Samples},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/49c9adb18e44be0711a94e827042f630-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_4b0a59dd,
 abstract = {By their very nature, memory based algorithms such as KNN or Parzen windows require a computationally expensive search of a large database of prototypes. In this paper we optimize the searching process for tangent distance (Simard, LeCun and Denker, 1993) to improve speed performance. The closest prototypes are found by recursively searching included subsets of the database using distances of increasing complexity. This is done by using a hierarchy of tangent distances (increasing the Humber of tangent. vectors from 0 to its maximum) and multiresolution (using wavelets). At each stage, a confidence level of the classification is computed. If the confidence is high enough, the computation of more complex distances is avoided. The resulting algorithm applied to character recognition is close to three orders of magnitude faster than computing the full tangent distance on every prototypes.},
 author = {Simard, Patrice},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4b0a59ddf11c58e7446c9df0da541a84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4b0a59ddf11c58e7446c9df0da541a84-Metadata.json},
 openalex = {W2151861435},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4b0a59ddf11c58e7446c9df0da541a84-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Efficient Computation of Complex Distance Metrics Using Hierarchical Filtering},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/4b0a59ddf11c58e7446c9df0da541a84-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_4d5b9953,
 abstract = {We study the statistics of an ensemble of images taken in the woods. Distributions of local quantities such as contrast are scale invariant and have nearly exponential tails. Power spectra exhibit scaling with a nontrivial exponent. These data limit the information content of natural images and point to the importance of gain-control strategies in visual processing.},
 author = {Ruderman, Daniel and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4d5b995358e7798bc7e9d9db83c612a5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4d5b995358e7798bc7e9d9db83c612a5-Metadata.json},
 openalex = {W2042755403},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Statistics of natural images: Scaling in the woods},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/4d5b995358e7798bc7e9d9db83c612a5-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_4e0928de,
 abstract = {Imagine you have designed a neural network that successfully learns a complex classification task. What are the relevant input features the classifier relies on and how are these features combined to produce the classification decisions? There are applications where a deeper insight into the structure of an adaptive system and thus into the underlying classification problem may well be as important as the system's performance characteristics, e.g. in economics or medicine. GDS is a backpropagation-based training scheme that produces networks transformable into an equivalent and concise set of IF-THEN rules. This is achieved by imposing penalty terms on the network parameters that adapt the network to the expressive power of this class of rules. Thus during training we simultaneously minimize classification and transformation error. Some real-world tasks demonstrate the viability of our approach.},
 author = {Blasig, Reinhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4e0928de075538c593fbdabb0c5ef2c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4e0928de075538c593fbdabb0c5ef2c3-Metadata.json},
 openalex = {W2124640585},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {GDS: Gradient Descent Generation of Symbolic Classification Rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/4e0928de075538c593fbdabb0c5ef2c3-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_4ea06fbc,
 abstract = {This paper describes the Q-routing algorithm for packet routing, in which a reinforcement learning module is embedded into each node of a switching network. Only local communication is used by each node to keep accurate statistics on which routing decisions lead to minimal delivery times. In simple experiments involving a 36-node, irregularly connected network, Q-routing proves superior to a nonadaptive algorithm based on precomputed shortest paths and is able to route efficiently even when critical aspects of the simulation, such as the network load, are allowed to vary dynamically. The paper concludes with a discussion of the tradeoff between discovering shortcuts and maintaining stable policies.},
 author = {Boyan, Justin and Littman, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4ea06fbc83cdd0a06020c35d50e1e89a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4ea06fbc83cdd0a06020c35d50e1e89a-Metadata.json},
 openalex = {W2156666755},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/4ea06fbc83cdd0a06020c35d50e1e89a-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_50c3d761,
 abstract = {This thesis describes the MM32k, a massively parallel SIMD computer which is easy to program, high in performance, low in cost and effective for implementing highly parallel neural network architectures. The MM32k has 32768 bit serial processing elements, each of which has 512 bits of memory, and all of which are interconnected by a switching network. The entire system resides on a single PC-AT compatible card. It is programmed from the host computer using a C++ language class library which supports variable precision vector arithmetic. The MM32k also supports direct video input and output for machine vision applications.},
 author = {Glover, Michael and Miller, W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/50c3d7614917b24303ee6a220679dab3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/50c3d7614917b24303ee6a220679dab3-Metadata.json},
 openalex = {W2149588176},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/50c3d7614917b24303ee6a220679dab3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Massively-Parallel SIMD Processor for Neural Network and Machine Vision Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/50c3d7614917b24303ee6a220679dab3-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_51ef186e,
 abstract = {We study the complexity problem in artificial feedforward neural networks designed to approximate real valued functions of several real variables; i.e., we estimate the number of neurons in a network required to ensure a given degree of approximation to every function in a given function class. We indicate how to construct networks with the indicated number of neurons evaluating standard activation functions. Our general theorem shows that the smoother the activation function, the better the rate of approximation.},
 author = {Mhaskar, H. N. and Micchelli, C. A..},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/51ef186e18dc00c2d31982567235c559-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/51ef186e18dc00c2d31982567235c559-Metadata.json},
 openalex = {W2133545078},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/51ef186e18dc00c2d31982567235c559-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {How to Choose an Activation Function},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/51ef186e18dc00c2d31982567235c559-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_5751ec3e,
 abstract = {We present a neural network simulation which we implemented on the massively parallel Connection Machine 2. In contrast to previous work, this simulator is based on biologically realistic neurons with nontrivial single-cell dynamics, high connectivity with a structure modelled in agreement with biological data, and preservation of the temporal dynamics of spike interactions. We simulate neural networks of 16,384 neurons coupled by about 1000 synapses per neuron, and estimate the performance for much larger systems. Communication between neurons is identified as the computationally most demanding task and we present a novel method to overcome this bottleneck. The simulator has already been used to study the primary visual system of the cat.},
 author = {Niebur, Ernst and Brettle, Dean},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5751ec3e9a4feab575962e78e006250d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5751ec3e9a4feab575962e78e006250d-Metadata.json},
 openalex = {W2168378381},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Efficient Simulation of Biological Neural Networks on Massively Parallel Supercomputers with Hypercube Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_5807a685,
 abstract = {Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD(λ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD(λ) and Q-learning belong.},
 author = {Jaakkola, Tommi and Jordan, Michael and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5807a685d1a9ab3b599035bc566ce2b9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5807a685d1a9ab3b599035bc566ce2b9-Metadata.json},
 openalex = {W2165131254},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {On the Convergence of Stochastic Iterative Dynamic Programming Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/5807a685d1a9ab3b599035bc566ce2b9-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_59c33016,
 abstract = {The performance requirements in experimental research on artificial neural nets often exceed the capability of workstations and PCs by a great amount. But speed is not the only requirement. Flexibility and implementation time for new algorithms are usually of equal importance. This paper describes the simulation of neural nets on the MUSIC parallel supercomputer, a system that shows a good balance between the three issues and therefore made many research projects possible that were unthinkable before. (MUSIC stands for Multiprocessor System with Intelligent Communication).},
 author = {M\"{u}ller, Urs A. and Kocheisen, Michael and Gunzinger, Anton},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/59c33016884a62116be975a9bb8257e3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/59c33016884a62116be975a9bb8257e3-Metadata.json},
 openalex = {W2167212012},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/59c33016884a62116be975a9bb8257e3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {High Performance Neural Net Simulation on a Multiprocessor System with "Intelligent" Communication},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/59c33016884a62116be975a9bb8257e3-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_5c572eca,
 abstract = {I detail the design and construction of an analog VLSI model of the neural system responsible for swimming behaviors of the leech. Why the leech? The biological network is small and relatively well understood, and the silicon model can therefore span three levels of organization in the leech nervous system (neuron, ganglion, system); it represents one of the first comprehensive models of leech swimming operating in real-time. The circuit employs biophysically motivated analog neurons networked to form multiple biologically inspired silicon ganglia. These ganglia are coupled using known interganglionic connections. Thus the model retains the flavor of its biological counterpart, and though simplified, the output of the silicon circuit is similar to the output of the leech swim central pattern generator. The model operates on the same time- and spatial-scale as the leech nervous system and will provide an excellent platform with which to explore real-time adaptive locomotion in the leech and other simple invertebrate nervous systems.},
 author = {Siegel, Micah},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5c572eca050594c7bc3c36e7e8ab9550-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5c572eca050594c7bc3c36e7e8ab9550-Metadata.json},
 openalex = {W2158541896},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5c572eca050594c7bc3c36e7e8ab9550-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Analog VLSI Model of Central Pattern Generation in the Leech},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/5c572eca050594c7bc3c36e7e8ab9550-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_5ec91aac,
 abstract = {Neurons learning under an unsupervised Hebbian learning rule can perform a nonlinear generalization of principal component analysis. This relationship between nonlinear PCA and nonlinear neurons is reviewed. The stable fixed points of the neuron learning dynamics correspond to the maxima of the statistic optimized under nonlinear PCA. However, in order to predict. what the neuron learns, knowledge of the basins of attractions of the neuron dynamics is required. Here the correspondence between nonlinear PCA and neural networks breaks down. This is shown for a simple model. Methods of statistical mechanics can be used to find the optima of the objective function of non-linear PCA. This determines what the neurons can learn. In order to find how the solutions are partitioned amoung the neurons, however, one must solve the dynamics.},
 author = {Shapiro, Jonathan and Pr\"{u}gel-Bennett, Adam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5ec91aac30eae62f4140325d09b9afd0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5ec91aac30eae62f4140325d09b9afd0-Metadata.json},
 openalex = {W2136716272},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5ec91aac30eae62f4140325d09b9afd0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Non-Linear Statistical Analysis and Self-Organizing Hebbian Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/5ec91aac30eae62f4140325d09b9afd0-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_5f0f5e5f,
 abstract = {Four versions of a k-nearest neighbor algorithm with locally adaptive k are introduced and compared to the basic k-nearest neighbor algorithm (kNN). Locally adaptive kNN algorithms choose the value of k that should be used to classify a query by consulting the results of cross-validation computations in the local neighborhood of the query. Local kNN methods are shown to perform similar to kNN in experiments with twelve commonly used data sets. Encouraging results in three constructed tasks show that local methods can significantly outperform kNN in specific applications. Local methods can be recommended for on-line learning and for applications where different regions of the input space are covered by patterns solving different sub-tasks.},
 author = {Wettschereck, Dietrich and Dietterich, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5f0f5e5f33945135b874349cfbed4fb9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5f0f5e5f33945135b874349cfbed4fb9-Metadata.json},
 openalex = {W2160675518},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5f0f5e5f33945135b874349cfbed4fb9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Locally Adaptive Nearest Neighbor Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/5f0f5e5f33945135b874349cfbed4fb9-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_5f2c22cb,
 abstract = {MacKay's Bayesian framework for backpropagation is conceptually appealing as well as practical. It automatically adjusts the weight decay parameters during training, and computes the evidence for each trained network. The evidence is proportional to our belief in the model. The networks with highest evidence turn out to generalise well. In this paper, the framework is extended to pruned nets, leading to an Ockham Factor for tuning the architecture to the data. A committee of networks, selected by their high evidence, is a natural Bayesian construction. The evidence of a committee is computed. The framework is illustrated on real-world data from a near infrared spectrometer used to determine the fat content in minced meat. Error bars are computed, including the contribution from the dissent of the committee members.},
 author = {Thodberg, Hans},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5f2c22cb4a5380af7ca75622a6426917-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5f2c22cb4a5380af7ca75622a6426917-Metadata.json},
 openalex = {W2097468217},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/5f2c22cb4a5380af7ca75622a6426917-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bayesian Backprop in Action: Pruning, Committees, Error Bars and an Application to Spectroscopy},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/5f2c22cb4a5380af7ca75622a6426917-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_61b4a64b,
 abstract = {Elements of Artificial Neural Networks provides a clearly organized general introduction, focusing on a broad range of algorithms, for students and others who want to use neural networks rather than simply study them.The authors, who have been developing and team teaching the material in a one-semester course over the past six years, describe most of the basic neural network models (with several detailed solved examples) and discuss the rationale and advantages of the models, as well as their limitations. The approach is practical and open-minded and requires very little mathematical or technical background. Written from a computer science and statistics point of view, the text stresses links to contiguous fields and can easily serve as a first course for students in economics and management.The opening chapter sets the stage, presenting the basic concepts in a clear and objective way and tackling important—yet rarely addressed—questions related to the use of neural networks in practical situations. Subsequent chapters on supervised learning (single layer and multilayer networks), unsupervised learning, and associative models are structured around classes of problems to which networks can be applied. Applications are discussed along with the algorithms. A separate chapter takes up optimization methods.The most frequently used algorithms, such as backpropagation, are introduced early on, right after perceptrons, so that these can form the basis for initiating course projects. Algorithms published as late as 1995 are also included. All of the algorithms are presented using block-structured pseudo-code, and exercises are provided throughout. Software implementing many commonly used neural network algorithms is available at the book's website.Transparency masters, including abbreviated text and figures for the entire book, are available for instructors using the text.},
 author = {Watanabe, Sumio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/61b4a64be663682e8cb037d9719ad8cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/61b4a64be663682e8cb037d9719ad8cd-Metadata.json},
 openalex = {W2093090592},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/61b4a64be663682e8cb037d9719ad8cd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Elements of Artificial Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/61b4a64be663682e8cb037d9719ad8cd-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_632cee94,
 abstract = {Bumptrees are geometric data structures introduced by Omohundro (1991) to provide efficient access to a collection of functions on a Euclidean space of interest. We describe a modified bumptree structure that has been employed as a neural network classifier, and compare its performance on several classification tasks against that of radial basis function networks and the standard mutli-layer perceptron.},
 author = {Bostock, Richard and Harget, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/632cee946db83e7a52ce5e8d6f0fed35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/632cee946db83e7a52ce5e8d6f0fed35-Metadata.json},
 openalex = {W2116191433},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Comparative Study of a Modified Bumptree Neural Network with Radial Basis Function Networks and the Standard Multi Layer Perceptron},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/632cee946db83e7a52ce5e8d6f0fed35-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_670e8a43,
 abstract = {For two layer networks with n sigmoidal hidden units, the generalization error is shown to be bounded by O(E 1/K) + O((EK)d/N log N), where d and N are the input dimension and the number of training samples, respectively. E represents the expectation on random number K of hidden units (1 ≤ K ≤ n). The probability Pr(K = k) (1 ≤ k ≤ n) is determined by a prior distribution of weights, which corresponds to a Gibbs distribution of a regularizer. This relationship makes it possible to characterize explicitly how a regularization term affects bias/variance of networks. The bound can be obtained analytically for a large class of commonly used priors. It can also be applied to estimate the expected network complexity EK in practice. The result provides a quantitative explanation on how large networks can generalize well.},
 author = {Ji, Chuanyi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/670e8a43b246801ca1eaca97b3e19189-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/670e8a43b246801ca1eaca97b3e19189-Metadata.json},
 openalex = {W2129496809},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/670e8a43b246801ca1eaca97b3e19189-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generalization Error and the Expected Network Complexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/670e8a43b246801ca1eaca97b3e19189-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_677e0972,
 abstract = {We propose a method for improving the performance of any network designed to predict the next value of a time series. We advocate analyzing the deviations of the network's predictions from the data in the training set. This can be carried out by a secondary network trained on the time series of these residuals. The combined system of the two networks is viewed as the new predictor. We demonstrate the simplicity and success of this method, by applying it to the sunspots data. The small corrections of the secondary network can be regarded as resulting from a Taylor expansion of a complex network which includes the combined system. We find that the complex network is more difficult to train and performs worse than the two-step procedure of the combined system.},
 author = {Ginzburg, Iris and Horn, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/677e09724f0e2df9b6c000b75b5da10d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/677e09724f0e2df9b6c000b75b5da10d-Metadata.json},
 openalex = {W2171786406},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/677e09724f0e2df9b6c000b75b5da10d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Combined Neural Networks for Time Series Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/677e09724f0e2df9b6c000b75b5da10d-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_67d16d00,
 abstract = {Changes in lighting conditions strongly effect the performance and reliability of computer vision systems. We report face recognition results under drastically changing lighting conditions for a computer vision system which concurrently uses a contrast sensitive silicon retina and a conventional, gain controlled CCO camera. For both input devices the face recognition system employs an elastic matching algorithm with wavelet based features to classify unknown faces. To assess the effect of analog on-chip preprocessing by the silicon retina the CCO images have been digitally preprocessed with a bandpass filter to adjust the power spectrum. The silicon retina with its ability to adjust sensitivity increases the recognition rate up to 50 percent. These comparative experiments demonstrate that preprocessing with an analog VLSI silicon retina generates image data enriched with object-constant features.},
 author = {Buhmann, Joachim and Lades, Martin and Eeckman, Frank},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/67d16d00201083a2b118dd5128dd6f59-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/67d16d00201083a2b118dd5128dd6f59-Metadata.json},
 openalex = {W2163971208},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Illumination-invariant face recognition with a contrast sensitive silicon retina},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_67d96d45,
 abstract = {We developed a system for finding address blocks on mail pieces that can process four images per second. Besides locating the address block, our system also determines the writing style, handwritten or machine printed, and moreover, it measures the skew angle of the text lines and cleans noisy images. A layout analysis of all the elements present in the image is performed in order to distinguish drawings and dirt from text and to separate text of advertisement from that of the destination address.

A speed of more than four images per second is obtained on a modular hardware platform, containing a board with two of the NET32K neural net chips, a SPARC2 processor board, and a board with 2 digital signal processors. The system has been tested with more than 100,000 images. Its performance depends on the quality of the images, and lies between 85% correct location in very noisy images to over 98% in cleaner images.},
 author = {Graf, Hans and Cosatto, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/67d96d458abdef21792e6d8e590244e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/67d96d458abdef21792e6d8e590244e7-Metadata.json},
 openalex = {W2130835828},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Address Block Location with a Neural Net System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/67d96d458abdef21792e6d8e590244e7-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_68053af2,
 abstract = {An approach is presented to learning high dimensional functions in the case where the learning algorithm can affect the generation of new data. A local modeling algorithm, locally weighted regression, is used to represent the learned function. Architectural parameters of the approach, such as distance metrics, are also localized and become a function of the query point instead of being global. Statistical tests are given for when a local model is good enough and sampling should be moved to a new area. Our methods explicitly deal with the case where prediction accuracy requirements exist during exploration: By gradually shifting a center of and controlling the speed of the shift with local prediction accuracy, a goal-directed exploration of state space takes place along the fringes of the current data support until the task goal is achieved. We illustrate this approach with simulation results and results from a real robot learning a complex juggling task.},
 author = {Schaal, Stefan and Atkeson, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/68053af2923e00204c3ca7c6a3150cf7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/68053af2923e00204c3ca7c6a3150cf7-Metadata.json},
 openalex = {W2151924253},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Assessing the Quality of Learned Local Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_6bc24fc1,
 abstract = {We investigate a model for neural activity that generates long range temporal correlations, 1/f noise, and oscillations in global activity. The model consists of a two-dimensional sheet of leaky integrate-and-fire neurons with feedback connectivity consisting of local excitation and surround inhibition. Each neuron is independently driven by homogeneous external noise. Spontaneous symmetry breaking occurs, resulting in the formation of hotspots of activity in the network. These localized patterns of excitation appear as clusters that coalesce, disintegrate, or fluctuate in size while simultaneously moving in a random walk constrained by the interaction with other clusters. The emergent cross-correlation functions have a dual structure, with a sharp peak around zero on top of a much broader hill. The power spectrum associated with single units shows a 1/f decay for small frequencies and is flat at higher frequencies, while the power spectrum of the spiking activity averaged over many cells--equivalent to the local field potential--shows no 1/f decay but a prominent peak around 40 Hz.},
 author = {Stemmler, Martin and Usher, Marius and Koch, Christof and Olami, Zeev},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6bc24fc1ab650b25b4114e93a98f1eba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6bc24fc1ab650b25b4114e93a98f1eba-Metadata.json},
 openalex = {W2103021791},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6bc24fc1ab650b25b4114e93a98f1eba-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Synchronization, oscillations, and 1/f noise in networks of spiking neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/6bc24fc1ab650b25b4114e93a98f1eba-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_6c29793a,
 abstract = {Computational models of the hippocampal-region provide an important method for understanding the functional role of this brain system in learning and memory. The presentations in this workshop focused on how modeling can lead to a unified understanding of the interplay among hippocampal physiology, anatomy, and behavior. Several approaches were presented. One approach can be characterized as top-down analyses of the neuropsychology of memory, drawing upon brain-lesion studies in animals and humans. Other models take a bottom-up approach, seeking to infer emergent computational and functional properties from detailed analyses of circuit connectivity and physiology (see Gluck & Granger, 1993, for a review). Among the issues discussed were: (1) integration of physiological and behavioral theories of hippocampal function, (2) similarities and differences between animal and human studies, (3) representational vs. temporal properties of hippocampal-dependent behaviors, (4) rapid vs. incremental learning, (5) mUltiple vs. unitary memory systems, (5) spatial navigation and memory, and (6) hippocampal interaction with other brain systems.},
 author = {Gluck, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6c29793a140a811d0c45ce03c1c93a28-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6c29793a140a811d0c45ce03c1c93a28-Metadata.json},
 openalex = {W2098951378},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6c29793a140a811d0c45ce03c1c93a28-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {What Does the Hippocampus Compute?: A Precis of the 1993 NIPS Workshop},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/6c29793a140a811d0c45ce03c1c93a28-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_6cd67d9b,
 abstract = {This paper presents a formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data. Unlike the standard mixture model, a multiple cause model accounts for observed data by combining assertions from many hidden causes, each of which can pertain to varying degree to any subset of the observable dimensions. A crucial issue is the mixing-function for combining beliefs from different cluster-centers in order to generate data reconstructions whose errors are minimized both during recognition and learning. We demonstrate a weakness inherent to the popular weighted sum followed by sigmoid squashing, and offer an alternative form of the nonlinearity. Results are presented demonstrating the algorithm's ability successfully to discover coherent multiple causal representations of noisy test data and in images of printed characters.},
 author = {Saund, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6cd67d9b6f0150c77bda2eda01ae484c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6cd67d9b6f0150c77bda2eda01ae484c-Metadata.json},
 openalex = {W2118363651},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6cd67d9b6f0150c77bda2eda01ae484c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Unsupervised Learning of Mixtures of Multiple Causes in Binary Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/6cd67d9b6f0150c77bda2eda01ae484c-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_6e0721b2,
 abstract = {A gradient descent algorithm for parameter estimation which is similar to those used for continuous-time recurrent neural networks was derived for Hodgkin-Huxley type neuron models. Using membrane potential trajectories as targets, the parameters (maximal conductances, thresholds and slopes of activation curves, time constants) were successfully estimated. The algorithm was applied to modeling slow non-spike oscillation of an identified neuron in the lobster stomatogastric ganglion. A model with three ionic currents was trained with experimental data. It revealed a novel role of A-current for slow oscillation below -50 mV.},
 author = {Doya, Kenji and Selverston, Allen and Rowat, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6e0721b2c6977135b916ef286bcb49ec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6e0721b2c6977135b916ef286bcb49ec-Metadata.json},
 openalex = {W2118869063},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6e0721b2c6977135b916ef286bcb49ec-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Hodgkin-Huxley Type Neuron Model That Learns Slow Non-Spike Oscillation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/6e0721b2c6977135b916ef286bcb49ec-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_6e7b33fd,
 abstract = {Memory-based learning methods operate by storing all (or most) of the training data and deferring analysis of that data until run time (i.e., when a query is presented and a decision or prediction must be made). When a query is received, these methods generally answer the query by retrieving and analyzing a small subset of the training data--namely, data in the immediate neighborhood of the query point. In short, memory-based methods are lazy (they wait until the query) and (they use only a local neighborhood). The purpose of this workshop was to review the state-of-the-art in memory-based methods and to understand their relationship to eager and global learning algorithms such as batch backpropagation.},
 author = {Dietterich, Thomas and Wettschereck, Dietrich and Atkeson, Chris G. and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6e7b33fdea3adc80ebd648fffb665bb8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6e7b33fdea3adc80ebd648fffb665bb8-Metadata.json},
 openalex = {W2172019843},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Memory-Based Methods for Regression and Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/6e7b33fdea3adc80ebd648fffb665bb8-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_6f2268bd,
 abstract = {In a talk entitled Trajectory Control of Convergent Networks with applications to TSP, Natan Peterfreund (Computer Science, Technion) dealt with the problem of controlling the trajectories of continuous convergent neural networks models for solving optimization problems, without affecting their equilibria set and their convergence properties. Natan presented a class of feedback control functions which achieve this objective, while also improving the convergence rates. A modified Hopfield and Tank neural network model, developed through the proposed feedback approach, was found to substantially improve the results of the original model in solving the Traveling Salesman Problem. The proposed feedback overcame the 2n symmetric property of the TSP problem.},
 author = {Jagota, Arun},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6f2268bd1d3d3ebaabb04d6b5d099425-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6f2268bd1d3d3ebaabb04d6b5d099425-Metadata.json},
 openalex = {W2106096998},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Methods for Optimization Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/6f2268bd1d3d3ebaabb04d6b5d099425-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_7143d7fb,
 abstract = {In drug activity prediction (as in handwritten character recognition), the features extracted to describe a training example depend on the pose (location, orientation, etc.) of the example. In handwritten character recognition, one of the best techniques for addressing this problem is the tangent distance method of Simard, LeCun and Denker (1993). Jain, et al. (1993a; 1993b) introduce a new technique-dynamic reposing--that also addresses this problem. Dynamic reposing iteratively learns a neural network and then reposes the examples in an effort to maximize the predicted output values. New models are trained and new poses computed until models and poses converge. This paper compares dynamic reposing to the tangent distance method on the task of predicting the biological activity of musk compounds. In a 20-fold cross-validation, dynamic reposing attains 91% correct compared to 79% for the tangent distance method, 75% for a neural network with standard poses, and 75% for the nearest neighbor method.},
 author = {Dietterich, Thomas and Jain, Ajay and Lathrop, Richard and Lozano-P\'{e}rez, Tom\'{a}s},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7143d7fbadfa4693b9eec507d9d37443-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7143d7fbadfa4693b9eec507d9d37443-Metadata.json},
 openalex = {W2158822024},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Comparison of Dynamic Reposing and Tangent Distance for Drug Activity Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/7143d7fbadfa4693b9eec507d9d37443-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_71ad16ad,
 abstract = {We have recently shown that the widely known LMS algorithm is an H∞ optimal estimator. The H∞ criterion has been introduced, initially in the control theory literature, as a means to ensure robust performance in the face of model uncertainties and lack of statistical information on the exogenous signals. We extend here our analysis to the nonlinear setting often encountered in neural networks, and show that the backpropagation algorithm is locally H∞ optimal. This fact provides a theoretical justification of the widely observed excellent robustness properties of the LMS and backpropagation algorithms. We further discuss some implications of these results.},
 author = {Hassibi, Babak and Sayed, Ali H and Kailath, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/71ad16ad2c4d81f348082ff6c4b20768-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/71ad16ad2c4d81f348082ff6c4b20768-Metadata.json},
 openalex = {W2128668667},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/71ad16ad2c4d81f348082ff6c4b20768-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Hoo Optimality Criteria for LMS and Backpropagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/71ad16ad2c4d81f348082ff6c4b20768-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_7250eb93,
 abstract = {We present a method for learning, tracking, and recognizing human hand gestures recorded by a conventional CCD camera without any special gloves or other sensors. A view-based representation is used to model aspects of the hand relevant to the trained gestures, and is found using an unsupervised clustering technique. We use normalized correlation networks, with dynamic time warping in the temporal domain, as a distance function for unsupervised clustering. Views are computed separably for space and time dimensions; the distributed response of the combination of these units characterizes the input data with a low dimensional representation. A supervised classification stage uses labeled outputs of the spatio-temporal units as training data. Our system can correctly classify gestures in real time with a low-cost image processing accelerator.},
 author = {Darrell, Trevor J. and Pentland, Alex P.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7250eb93b3c18cc9daa29cf58af7a004-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7250eb93b3c18cc9daa29cf58af7a004-Metadata.json},
 openalex = {W2104652184},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Classifying Hand Gestures with a View-Based Distributed Representation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/7250eb93b3c18cc9daa29cf58af7a004-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_72da7fd6,
 abstract = {(Masino and Knudsen 1990) showed some remarkable results which suggest that head motion in the barn owl is controlled by distinct circuits coding for the horizontal and vertical components of movement. This implies the existence of a set of orthogonal internal coordinates that are related to meaningful coordinates of the external world. No coherent computational theory has yet been proposed to explain this finding. I have proposed a simple model which provides a framework for a theory of low-level motor learning. I show that the theory predicts the observed microstimulation results in the barn owl. The model rests on the concept of Optimal Unsupervised Motor Learning, which provides a set of criteria that predict optimal internal representations. I describe two iterative Neural Network algorithms which find the optimal solution and demonstrate possible mechanisms for the development of internal representations in animals.},
 author = {Sanger, Terence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/72da7fd6d1302c0a159f6436d01e9eb0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/72da7fd6d1302c0a159f6436d01e9eb0-Metadata.json},
 openalex = {W2156252257},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Unsupervised Motor Learning Predicts the Internal Representation of Barn Owl Head Movements},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/72da7fd6d1302c0a159f6436d01e9eb0-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_788d9869,
 abstract = {1. Computer simulations were used to study the effect of voltage-dependent calcium and potassium conductances in the apical dendritic tree of a pyramidal cell on the synaptic efficacy of apical synaptic input. The apical tuft in layers 1 and 2 is the target of feedback projections from other cortical areas. 2. The current, Isoma, flowing into the soma in response to synaptic input was used to assess synaptic efficacy. This measure takes full account of all the relevant nonlinearities in the dendrities and can be used during spiking activity. Isoma emphasizes current flowing in response to synaptic input rather than synaptically induced voltage change. This measure also permits explicit characterization of the input-output relationship of the entire neuron by computing the relationship between presynaptic input and postsynaptic output frequency. 3. Simulations were based on two models. The first was a biophysically detailed 400-compartment model of a morphologically characterized layer 5 pyramidal cell from striate cortex of an adult cat. In this model eight voltage-dependent conductances were incorporated into the somatic membrane to provide the observed firing behavior of a regular spiking cell. The second model was a highly simplified three-compartment equivalent electrical circuit. 4. If the dendritic tree is entirely passive, excitatory synaptic input of the non-N-methyl-D-aspartate (non-NMDA) type to layers 1, 2, and 3 saturate at very moderate input rates, because of the high input impedance of the apical tuft. Layers 1 and 2 together can deliver only 0.25 nA current to the soma. This modest effect is surprising in view of the important afferents that synapse on the apical tuft and is inconsistent with experimental data indicating a more powerful effect. 5. We introduced in a controlled manner a voltage-dependent potassium conductance in the apical tuft, gK, to prevent saturation of the synaptic response. This conductance was designed to linearize the relationship between presynaptic input frequency and the somatic current. We also introduced a voltage-dependent calcium conductance along the apical trunk, gCa, to amplify the apical signal, i.e., the synaptic current reaching the soma. 6. To arrive at a specific relationship between the presynaptic input rate and the somatic current delivered by the synaptic input, we derived the activation curves of gK and gCa either analytically or numerically. The resultant voltage-dependent behavior of both conductances was similar to experimentally measured activation curves.(ABSTRACT TRUNCATED AT 400 WORDS)},
 author = {Bernander, \"{O}jvind and Koch, Christof and Douglas, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/788d986905533aba051261497ecffcbb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/788d986905533aba051261497ecffcbb-Metadata.json},
 openalex = {W2106614406},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/788d986905533aba051261497ecffcbb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Amplification and linearization of distal synaptic input to cortical pyramidal cells},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/788d986905533aba051261497ecffcbb-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_795c7a7a,
 abstract = {Online cursive handwriting recognition is currently one of the most intriguing challenges in pattern recognition. This study presents a novel approach to this problem which is composed of two complementary phases. The first is dynamic encoding of the writing trajectory into a compact sequence of discrete motor control symbols. In this compact representation we largely remove the redundancy of the script, while preserving most of its intelligible components. In the second phase these control sequences are used to train adaptive probabilistic acyclic automata (PAA) for the important ingredients of the writing trajectories, e.g. letters. We present a new and efficient learning algorithm for such stochastic automata, and demonstrate its utility for spotting and segmentation of cursive scripts. Our experiments show that over 90% of the letters are correctly spotted and identified, prior to any higher level language model. Moreover, both the training and recognition algorithms are very efficient compared to other modeling methods, and the models are 'on-line' adaptable to other writers and styles.},
 author = {Singer, Yoram and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/795c7a7a5ec6b460ec00c5841019b9e9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/795c7a7a5ec6b460ec00c5841019b9e9-Metadata.json},
 openalex = {W2168122360},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/795c7a7a5ec6b460ec00c5841019b9e9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Decoding Cursive Scripts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/795c7a7a5ec6b460ec00c5841019b9e9-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_7a53928f,
 author = {Montague, P. and Dayan, Peter and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7a53928fa4dd31e82c6ef826f341daec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7a53928fa4dd31e82c6ef826f341daec-Metadata.json},
 openalex = {W2005866069},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7a53928fa4dd31e82c6ef826f341daec-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bee foraging in uncertain environments using predictive hebbian learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/7a53928fa4dd31e82c6ef826f341daec-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_7c590f01,
 abstract = {Short term memory is indispensable for the processing of time varying information with artificial neural networks. In this paper a model for linear memories is presented, and ways to include memories in connectionist topologies are discussed. A comparison is drawn among different memory types, with indication of what is the salient characteristic of each memory model.},
 author = {Principe, Jose C. and Hsu, Hui-H. and Kuo, Jyh-Ming},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7c590f01490190db0ed02a5070e20f01-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7c590f01490190db0ed02a5070e20f01-Metadata.json},
 openalex = {W2166033949},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7c590f01490190db0ed02a5070e20f01-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analysis of Short Term Memories for Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/7c590f01490190db0ed02a5070e20f01-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_7f1171a7,
 abstract = {Learning to recognize or predict sequences using long-term context has many applications. However, practical and theoretical problems are found in training recurrent neural networks to perform tasks in which input/output dependencies span long intervals. Starting from a mathematical analysis of the problem, we consider and compare alternative algorithms and architectures on tasks for which the span of the input/output dependencies can be controlled. Results on the new algorithms show performance qualitatively superior to that obtained with backpropagation.},
 author = {Bengio, Yoshua and Frasconi, Paolo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Metadata.json},
 openalex = {W2131387845},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/7f1171a78ce0780a2142a6eb7bc4f3c8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Credit Assignment through Time: Alternatives to Backpropagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/7f1171a78ce0780a2142a6eb7bc4f3c8-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_81e74d67,
 abstract = {The non-linear complexities of neural networks make network solutions difficult to understand. Sanger's contribution analysis is here extended to the analysis of networks automatically generated by the cascade-correlation learning algorithm. Because such networks have cross connections that supersede hidden layers, standard analyses of hidden unit activation patterns are insufficient. A contribution is defined as the product of an output weight and the associated activation on the sending unit, whether that sending unit is an input or a hidden unit, multiplied by the sign of the output target for the current input pattern. Intercorrelations among contributions, as gleaned from the matrix of contributions x input patterns, can be subjected to principal components analysis (PCA) to extract the main features of variation in the contributions. Such an analysis is applied to three problems, continuous XOR, arithmetic comparison, and distinguishing between two interlocking spirals. In all three cases, this technique yields useful insights into network solutions that are consistent across several networks.},
 author = {Shultz, Thomas and Elman, Jeffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/81e74d678581a3bb7a720b019f4f1a93-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/81e74d678581a3bb7a720b019f4f1a93-Metadata.json},
 openalex = {W2171970443},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/81e74d678581a3bb7a720b019f4f1a93-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analyzing Cross-Connected Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/81e74d678581a3bb7a720b019f4f1a93-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_82489c97,
 abstract = {This paper introduces a new recognition-based segmentation approach to recognizing on-line cursive handwriting from a database of 10,000 English words. The original input stream of x, y pen coordinates is encoded as a sequence of uniform stroke descriptions that are processed by six feed-forward neural-networks, each designed to recognize letters of different sizes. Words are then recognized by performing best-first search over the space of all possible segmentations. Results demonstrate that the method is effective at both writer dependent recognition (1.7% to 15.5% error rate) and writer independent recognition (5.2% to 31.1% error rate).},
 author = {Flann, Nicholas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/82489c9737cc245530c7a6ebef3753ec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/82489c9737cc245530c7a6ebef3753ec-Metadata.json},
 openalex = {W2102836637},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/82489c9737cc245530c7a6ebef3753ec-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Recognition-based Segmentation of On-Line Cursive Handwriting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/82489c9737cc245530c7a6ebef3753ec-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_82f2b308,
 abstract = {We will present the implementation of intelligent electronic circuits realized for the first time using a new functional device called Neuron MOS Transistor (neuMOS or vMOS in short) simulating the behavior of biological neurons at a single transistor level. Search for the most resembling data in the memory cell array, for instance, can be automatically carried out on hardware without any software manipulation. Soft Hardware, which we named, can arbitrarily change its logic function in real time by external control signals without any hardware modification. Implementation of a neural network equipped with an on-chip self-learning capability is also described. Through the studies of vMOS intelligent circuit implementation, we noticed an interesting similarity in the architectures of vMOS logic circuitry and biological systems.},
 author = {Shibata, Tadashi and Kotani, Koji and Yamashita, Takeo and Ishii, Hiroshi and Kosaka, Hideo and Ohmi, Tadahiro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/82f2b308c3b01637c607ce05f52a2fed-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/82f2b308c3b01637c607ce05f52a2fed-Metadata.json},
 openalex = {W2142659398},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/82f2b308c3b01637c607ce05f52a2fed-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Implementing Intelligence on Silicon Using Neuron-Like Functional MOS Transistors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/82f2b308c3b01637c607ce05f52a2fed-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_846c260d,
 abstract = {This paper describes probabilistic methods for novelty detection when using pattern recognition methods for fault monitoring of dynamic systems. The problem of novelty detection is particularly acute when prior knowledge and training data only allow one to construct an incomplete classification model. Allowance must be made in model design so that the classifier will be robust to data generated by classes not included in the training phase. For diagnosis applications one practical approach is to construct both an input density model and a discriminative class model. Using Bayes' rule and prior estimates of the relative likelihood of data of known and unknown origin the resulting classification equations are straightforward. The paper describes the application of this method in the context of hidden Markov models for online fault monitoring of large ground antennas for spacecraft tracking, with particular application to the detection of transient behaviour of unknown origin.},
 author = {Smyth, Padhraic},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/846c260d715e5b854ffad5f70a516c88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/846c260d715e5b854ffad5f70a516c88-Metadata.json},
 openalex = {W2163522236},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Probabilistic Anomaly Detection in Dynamic Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_847cc55b,
 abstract = {The most commonly used neural network models are not well suited to direct digital implementations because each node needs to perform a large number of operations between floating point values. Fortunately, the ability to learn from examples and to generalize is not restricted to networks of this type. Indeed, networks where each node implements a simple Boolean function (Boolean networks) can be designed in such a way as to exhibit similar properties. Two algorithms that generate Boolean networks from examples are presented. The results show that these algorithms generalize very well in a class of problems that accept compact Boolean network descriptions. The techniques described are general and can be applied to tasks that are not known to have that characteristic. Two examples of applications are presented: image reconstruction and hand-written character recognition.},
 author = {Oliveira, Arlindo and Sangiovanni-Vincentelli, Alberto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/847cc55b7032108eee6dd897f3bca8a5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/847cc55b7032108eee6dd897f3bca8a5-Metadata.json},
 openalex = {W2147404448},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/847cc55b7032108eee6dd897f3bca8a5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Complex Boolean Functions: Algorithms and Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/847cc55b7032108eee6dd897f3bca8a5-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_84f7e699,
 abstract = {Although recurrent neural nets have been moderately successful in learning to emulate finite-state machines (FSMs), the continuous internal state dynamics of a neural net are not well matched to the discrete behavior of an FSM. We describe an architecture, called DOLCE, that allows discrete states to evolve in a net as learning progresses. DOLCE consists of a standard recurrent neural net trained by gradient descent and an adaptive clustering technique that quantizes the state space. DOLCE is based on the assumption that a finite set of discrete internal states is required for the task, and that the actual network state belongs to this set but has been corrupted by noise due to inaccuracy in the weights. DOLCE learns to recover the discrete state with maximum a posteriori probability from the noisy state. Simulations show that DOLCE leads to a significant improvement in generalization performance over earlier neural net approaches to FSM induction.},
 author = {Das, Sreerupa and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/84f7e69969dea92a925508f7c1f9579a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/84f7e69969dea92a925508f7c1f9579a-Metadata.json},
 openalex = {W2165301751},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/84f7e69969dea92a925508f7c1f9579a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/84f7e69969dea92a925508f7c1f9579a-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_86b122d4,
 abstract = {Reinforcement Learning methods based on approximating dynamic programming (DP) are receiving increased attention due to their utility in forming reactive control policies for systems embedded in dynamic environments. Environments are usually modeled as controlled Markov processes, but when the environment model is not known a priori, adaptive methods are necessary. Adaptive control methods are often classified as being direct or indirect. Direct methods directly adapt the control policy from experience, whereas indirect methods adapt a model of the controlled process and compute control policies based on the latest model. Our focus is on indirect adaptive DP-based methods in this paper. We present a convergence result for indirect adaptive asynchronous value iteration algorithms for the case in which a look-up table is used to store the value function. Our result implies convergence of several existing reinforcement learning algorithms such as adaptive real-time dynamic programming (ARTDP) (Barto, Bradtke, & Singh, 1993) and prioritized sweeping (Moore & Atkeson, 1993). Although the emphasis of researchers studying DP-based reinforcement learning has been on direct adaptive methods such as Q-Learning (Watkins, 1989) and methods using TD algorithms (Sutton, 1988), it is not clear that these direct methods are preferable in practice to indirect methods such as those analyzed in this paper.},
 author = {Gullapalli, Vijaykumar and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/86b122d4358357d834a87ce618a55de0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/86b122d4358357d834a87ce618a55de0-Metadata.json},
 openalex = {W2118219918},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/86b122d4358357d834a87ce618a55de0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Convergence of Indirect Adaptive Asynchronous Value Iteration Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/86b122d4358357d834a87ce618a55de0-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_872488f8,
 abstract = {We present a new algorithm for eliminating excess parameters and improving network generalization after supervised training. The method, Principal Components Pruning (PCP), is based on principal component analysis of the node activations of successive layers of the network. It is simple, cheap to implement, and effective. It requires no network retraining, and does not involve calculating the full Hessian of the cost function. Only the weight and the node activity correlation matrices for each layer of nodes are required. We demonstrate the efficacy of the method on a regression problem using polynomial basis functions, and on an economic time series prediction problem using a two-layer, feedforward network.},
 author = {Levin, Asriel and Leen, Todd and Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/872488f88d1b2db54d55bc8bba2fad1b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/872488f88d1b2db54d55bc8bba2fad1b-Metadata.json},
 openalex = {W2156913264},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Fast Pruning Using Principal Components},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/872488f88d1b2db54d55bc8bba2fad1b-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_88ae6372,
 author = {Baldi, Pierre and Brunak, S\o ren and Chauvin, Yves and Engelbrecht, Jacob and Krogh, Anders},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/88ae6372cfdc5df69a976e893f4d554b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/88ae6372cfdc5df69a976e893f4d554b-Metadata.json},
 openalex = {W2102301373},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/88ae6372cfdc5df69a976e893f4d554b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Hidden Markov Models for Human Genes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/88ae6372cfdc5df69a976e893f4d554b-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_8c235f89,
 abstract = {This paper describes the use of a convolutional neural network to perform address block location on machine-printed mail pieces. Locating the address block is a difficult object recognition problem because there is often a large amount of extraneous printing on a mail piece and because address blocks vary dramatically in size and shape.

We used a convolutional locator network with four outputs, each trained to find a different corner of the address block. A simple set of rules was used to generate ABL candidates from the network output. The system performs very well: when allowed five guesses, the network will tightly bound the address delivery information in 98.2% of the cases.},
 author = {Wolf, Ralph and Platt, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8c235f89a8143a28a1d6067e959dd858-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8c235f89a8143a28a1d6067e959dd858-Metadata.json},
 openalex = {W2166559794},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Postal Address Block Location Using a Convolutional Locator Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/8c235f89a8143a28a1d6067e959dd858-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_8c6744c9,
 abstract = {In an effort to understand saccadic eye movements and their relation to visual attention and other forms of eye movements, we - in collaboration with a number of other laboratories - are carrying out a large-scale effort to design and build a complete primate oculomotor system using analog CMOS VLSI technology. Using this technology, a low power, compact, multi-chip system has been built which works in real-time using real-world visual inputs. We describe in this paper the performance of an early version of such a system including a 1-D array of photoreceptors mimicking the retina, a circuit computing the mean location of activity representing the superior colliculus, a saccadic burst generator, and a one degree-of-freedom rotational platform which models the dynamic properties of the primate oculomotor plant.},
 author = {Horiuchi, Timothy and Bishofberger, Brooks and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8c6744c9d42ec2cb9e8885b54ff744d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8c6744c9d42ec2cb9e8885b54ff744d0-Metadata.json},
 openalex = {W2133700812},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Analog VLSI Saccadic Eye Movement System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/8c6744c9d42ec2cb9e8885b54ff744d0-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_8d317bdc,
 abstract = {This paper proposes a practical optimization method for layered neural networks, by which the optimal model and parameter can be found simultaneously. We modify the conventional information criterion into a differentiable function of parameters, and then, minimize it, while controlling it back to the ordinary form. Effectiveness of this method is discussed theoretically and experimentally.},
 author = {Watanabe, Sumio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8d317bdcf4aafcfc22149d77babee96d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8d317bdcf4aafcfc22149d77babee96d-Metadata.json},
 openalex = {W2130306774},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8d317bdcf4aafcfc22149d77babee96d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Optimization Method of Layered Neural Networks based on the Modified Information Criterion},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/8d317bdcf4aafcfc22149d77babee96d-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_8e82ab72,
 abstract = {We propose a trajectory planning and control theory for continuous movements such as connected cursive handwriting and continuous natural speech. Its hardware is based on our previously proposed forward-inverse-relaxation neural network (Wada & Kawato, 1993). Computationally, its optimization principle is the minimum torque-change criterion. Regarding the representation level, hard constraints satisfied by a trajectory are represented as a set of via-points extracted from a handwritten character. Accordingly, we propose a via-point estimation algorithm that estimates via-points by repeating the trajectory formation of a character and the via-point extraction from the character. In experiments, good quantitative agreement is found between human handwriting data and the trajectories generated by the theory. Finally, we propose a recognition schema based on the movement generation. We show a result in which the recognition schema is applied to the handwritten character recognition and can be extended to the phoneme timing estimation of natural speech.},
 author = {Wada, Yasuhiro and Koike, Yasuharu and Vatikiotis-Bateson, Eric and Kawato, Mitsuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8e82ab7243b7c66d768f1b8ce1c967eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8e82ab7243b7c66d768f1b8ce1c967eb-Metadata.json},
 openalex = {W2171403244},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8e82ab7243b7c66d768f1b8ce1c967eb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Computational Model for Cursive Handwriting Based on the Minimization Principle},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/8e82ab7243b7c66d768f1b8ce1c967eb-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_8f7d807e,
 abstract = {The problem of learning from examples in multilayer networks is studied within the framework of statistical mechanics. Using the replica formalism we calculate the average generalization error of a fully connected committee machine in the limit of a large number of hidden units. If the number of training examples is proportional to the number of inputs in the network, the generalization error as a function of the training set size approaches a finite value. If the number of training examples is proportional to the number of weights in the network we find first-order phase transitions with a discontinuous drop in the generalization error for both binary and continuous weights.},
 author = {Schwarze, H. and Hertz, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8f7d807e1f53eff5f9efbe5cb81090fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8f7d807e1f53eff5f9efbe5cb81090fb-Metadata.json},
 openalex = {W2143004559},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Discontinuous Generalization in Large Committee Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/8f7d807e1f53eff5f9efbe5cb81090fb-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_912d2b1c,
 abstract = {Progress has been made in computational implementation of speech production based on physiological data. An inverse dynamics model of the speech articulator's musculo-skeletal system, which is the mapping from articulator trajectories to electromyographic (EMG) signals, was modeled using the acquired forward dynamics model and temporal (smoothness of EMG activation) and range constraints. This inverse dynamics model allows the use of a faster speech motor control scheme, which can be applied to phoneme-to-speech synthesis via musclo-skeletal system dynamics, or to future use in speech recognition. The forward acoustic model, which is the mapping from articulator trajectories to the acoustic parameters, was improved by adding velocity and voicing information inputs to distinguish acoustic parameter differences caused by changes in source characteristics.},
 author = {Hirayama, Makoto and Vatikiotis-Bateson, Eric and Kawato, Mitsuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/912d2b1c7b2826caf99687388d2e8f7c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/912d2b1c7b2826caf99687388d2e8f7c-Metadata.json},
 openalex = {W2136654058},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Inverse Dynamics of Speech Motor Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/912d2b1c7b2826caf99687388d2e8f7c-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_92fb0c6d,
 abstract = {Hybrid connectionist/HMM systems model time both using a Markov chain and through properties of a connectionist network. In this paper, we discuss the nature of the time dependence currently employed in our systems using recurrent networks (RNs) and feed-forward multi-layer perceptrons (MLPs). In particular, we introduce local recurrences into a MLP to produce an enhanced input representation. This is in the form of an adaptive gamma filter and incorporates an automatic approach for learning temporal dependencies. We have experimented on a speaker-independent phone recognition task using the TIMIT database. Results using the gamma filtered input representation have shown improvement over the baseline MLP system. Improvements have also been obtained through merging the baseline and gamma filter models.},
 author = {Renals, Steve and Hochberg, Mike and Robinson, Tony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/92fb0c6d1758261f10d052e6e2c1123c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/92fb0c6d1758261f10d052e6e2c1123c-Metadata.json},
 openalex = {W2117881873},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/92fb0c6d1758261f10d052e6e2c1123c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Temporal Dependencies in Connectionist Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/92fb0c6d1758261f10d052e6e2c1123c-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_959a557f,
 abstract = {Based on precise anatomical data of the bee's olfactory system, we propose an investigation of the possible mechanisms of modulation and control between the two levels of olfactory information processing: the antennallobe glomeruli and the mushroom bodies. We use simplified neurons, but realistic architecture. As a first conclusion, we postulate that the feature extraction performed by the antennallobe (glomeruli and interneurons) necessitates central input from the mushroom bodies for fine tuning. The central input thus facilitates the evolution from fuzzy olfactory images in the glomerular layer towards more focussed images upon odor presentation.},
 author = {Linster, Christiane and Marsan, David and Masson, Claudine and Kerszberg, Michel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/959a557f5f6beb411fd954f3f34b21c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/959a557f5f6beb411fd954f3f34b21c3-Metadata.json},
 openalex = {W2138442796},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/959a557f5f6beb411fd954f3f34b21c3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Odor processing in the bee: a preliminary study of the role of central input to the antennal lobe},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/959a557f5f6beb411fd954f3f34b21c3-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_96b9bff0,
 abstract = {Most connectionist research has focused on learning mappings from one space to another (eg. classification and regression). This paper introduces the more general task of learning constraint surfaces. It describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data. We demonstrate the technique on low dimensional synthetic surfaces and compare it to nearest neighbor approaches. We then show its utility in learning the space of lip images in a system for improving speech recognition by lip reading. This learned surface is used to improve the visual tracking performance during recognition.},
 author = {Bregler, Christoph and Omohundro, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/96b9bff013acedfb1d140579e2fbeb63-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/96b9bff013acedfb1d140579e2fbeb63-Metadata.json},
 openalex = {W2132747970},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/96b9bff013acedfb1d140579e2fbeb63-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Surface Learning with Applications to Lipreading},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/96b9bff013acedfb1d140579e2fbeb63-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_96ea64f3,
 abstract = {Spotting tasks require detection of target patterns from a background of richly varied non-target inputs. The performance measure of interest for these tasks, called the figure of merit (FOM), is the detection rate for target patterns when the false alarm rate is in an acceptable range. A new approach to training spotters is presented which computes the FOM gradient for each input pattern and then directly maximizes the FOM using backpropagation. This eliminates the need for thresholds during training. It also uses network resources to model Bayesian a posteriori probability functions accurately only for patterns which have a significant effect on the detection accuracy over the false alarm rate of interest. FOM training increased detection accuracy by 5 percentage points for a hybrid radial basis function (RBF) - hidden Markov model (HMM) wordspotter on the credit-card speech corpus.},
 author = {Chang, Eric and Lippmann, Richard P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Metadata.json},
 openalex = {W2116400298},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Figure of Merit Training for Detection and Spotting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/96ea64f3a1aa2fd00c72faacf0cb8ac9-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_98d6f58a,
 author = {Kamimura, Ryotaro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/98d6f58ab0dafbb86b083a001561bb34-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/98d6f58ab0dafbb86b083a001561bb34-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/98d6f58ab0dafbb86b083a001561bb34-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generation of Internal Representation by \alpha -Transformation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/98d6f58ab0dafbb86b083a001561bb34-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_9e3cfc48,
 abstract = {An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.},
 author = {Hinton, Geoffrey E and Zemel, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/9e3cfc48eccf81a0d57663e129aef3cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/9e3cfc48eccf81a0d57663e129aef3cb-Metadata.json},
 openalex = {W2102409316},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Autoencoders, Minimum Description Length and Helmholtz Free Energy},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_a1d33d0d,
 abstract = {We propose that the binding and segmentation of visual features is mediated by two complementary mechanisms; a low resolution, spatial-based, resource-free process and a high resolution, temporal-based, resource-limited process. In the visual cortex, the former depends upon the orderly topographic organization in striate and extrastriate areas while the latter may be related to observed temporal relationships between neuronal activities. Computer simulations illustrate the role the two mechanisms play in figure/ground discrimination, depth-from-occlusion, and the vividness of perceptual completion.},
 author = {Sajda, Paul and Finkel, Leif},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a1d33d0dfec820b41b54430b50e96b5c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a1d33d0dfec820b41b54430b50e96b5c-Metadata.json},
 openalex = {W2113827717},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a1d33d0dfec820b41b54430b50e96b5c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dual Mechanisms for Neural Binding and Segmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/a1d33d0dfec820b41b54430b50e96b5c-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_a5cdd4aa,
 abstract = {The satisfiability of random CNF formulae with precisely k variables per clause (k-SAT) is a popular testbed for the performance of search algorithms. Formulae have M clauses from N variables, randomly negated, keeping the ratio α = M/N fixed. For k = 2, this model has been proven to have a sharp threshold at α = 1 between formulae which are almost aways satisfiable and formulae which are almost never satisfiable as N → ∞. Computer experiments for k = 2, 3, 4, 5 and 6, (carried out in collaboration with B. Selman of ATT Bell Labs). show similar threshold behavior for each value of k. Finite-size scaling, a theory of the critical point phenomena used in statistical physics, is shown to characterize the size dependence near the threshold. Annealed and replica-based mean field theories give a good account of the results.},
 author = {Kirkpatrick, Scott and Gy\"{o}rgyi, G\'{e}za and Tishby, Naftali and Troyansky, Lidror},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Metadata.json},
 openalex = {W2142971622},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Statistical Mechanics of k-Satisfaction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/a5cdd4aa0048b187f7182f1b9ce7a6a7-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_a67f0968,
 abstract = {We have developed visual preprocessing algorithms for extracting phonologically relevant features from the grayscale video image of a speaker, to provide speaker-independent inputs for an automatic lipreading (speechreading) system. Visual features such as mouth open/closed, tongue visible/not-visible, teeth visible/notvisible, and several shape descriptors of the mouth and its motion are all rapidly computable in a manner quite insensitive to lighting conditions. We formed a hybrid speechreading system consisting of two time delay neural networks (video and acoustic) and integrated their responses by means of independent opinion pooling - the Bayesian optimal method given conditional independence, which seems to hold for our data. This hybrid system had an error rate 25% lower than that of the acoustic subsystem alone on a five-utterance speaker-independent task, indicating that video can be used to improve speech recognition.},
 author = {Wolff, Gregory and Prasad, K. and Stork, David and Hennecke, Marcus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a67f096809415ca1c9f112d96d27689b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a67f096809415ca1c9f112d96d27689b-Metadata.json},
 openalex = {W2095729891},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a67f096809415ca1c9f112d96d27689b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Lipreading by neural networks: Visual preprocessing, learning, and sensory integration},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/a67f096809415ca1c9f112d96d27689b-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_a8e864d0,
 abstract = {Biological neurons have a variety of intrinsic properties because of the large number of voltage dependent currents that control their activity. Neuromodulatory substances modify both the balance of conductances that determine intrinsic properties and the strength of synapses. These mechanisms alter circuit dynamics, and suggest that functional circuits exist only in the modulatory environment in which they operate.},
 author = {Marder, Eve},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a8e864d04c95572d1aece099af852d0a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a8e864d04c95572d1aece099af852d0a-Metadata.json},
 openalex = {W2161743565},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dynamic Modulation of Neurons and Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/a8e864d04c95572d1aece099af852d0a-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_a8ecbaba,
 abstract = {We analyze how data with uncertain or missing input features can be incorporated into the training of a neural network. The general solution requires a weighted integration over the unknown or uncertain input although computationally cheaper closed-form solutions can be found for certain Gaussian Basis Function (GBF) networks. We also discuss cases in which heuristical solutions such as substituting the mean of an unknown input can be harmful.},
 author = {Tresp, Volker and Ahmad, Subutai and Neuneier, Ralph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a8ecbabae151abacba7dbde04f761c37-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a8ecbabae151abacba7dbde04f761c37-Metadata.json},
 openalex = {W2100206501},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/a8ecbabae151abacba7dbde04f761c37-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Training Neural Networks with Deficient Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/a8ecbabae151abacba7dbde04f761c37-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_ab88b157,
 abstract = {We analyze a simple hill-climbing algorithm (RMHC) that was previously shown to outperform a genetic algorithm (GA) on a simple Royal Road function. We then analyze an idealized genetic algorithm (IGA) that is significantly faster than RMHC and that gives a lower bound for GA speed. We identify the features of the IGA that give rise to this speedup, and discuss how these features can be incorporated into a real GA.},
 author = {Mitchell, Melanie and Holland, John and Forrest, Stephanie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ab88b15733f543179858600245108dd8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ab88b15733f543179858600245108dd8-Metadata.json},
 openalex = {W2166897600},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ab88b15733f543179858600245108dd8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {When will a Genetic Algorithm Outperform Hill Climbing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/ab88b15733f543179858600245108dd8-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_aba3b6fd,
 abstract = {We describe a number of learning rules that can be used to train unsupervised parallel feature extraction systems. The learning rules are derived using gradient ascent of a quality function. We consider a number of quality functions that are rational functions of higher order moments of the extracted feature values. We show that one system learns the principle components of the correlation matrix. Principal component analysis systems are usually not optimal feature extractors for classification. Therefore we design quality functions which produce feature vectors that support unsupervised classification. The properties of the different systems are compared with the help of different artificially designed datasets and a database consisting of all Munsell color spectra.},
 author = {\"{O}sterberg, Mats and Lenz, Reiner},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/aba3b6fd5d186d28e06ff97135cade7f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/aba3b6fd5d186d28e06ff97135cade7f-Metadata.json},
 openalex = {W2103755514},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Unsupervised Parallel Feature Extraction from First Principles},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/aba3b6fd5d186d28e06ff97135cade7f-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_addfa9b7,
 abstract = {In this paper, we will consider the problem of classifying electroencephalogram (EEG) signals of normal subjects, and subjects suffering from psychiatric disorder, e.g., obsessive compulsive disorder, schizophrenia, using a class of artificial neural networks, viz., multi-layer perceptron. It is shown that the multilayer perceptron is capable of classifying unseen test EEG signals to a high degree of accuracy.},
 author = {Tsoi, A C and So, D S C and Sergejew, A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/addfa9b7e234254d26e9c7f2af1005cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/addfa9b7e234254d26e9c7f2af1005cb-Metadata.json},
 openalex = {W2160837129},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Classification of Electroencephalogram using Artificial Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_aeb3135b,
 abstract = {Airline companies usually schedule their flights and crews well in advance to optimize their crew pools activities. Many events such as flight delays or the absence of a member require the crew pool rescheduling team to change the initial schedule (rescheduling). In this paper, we show that the neural network comparison paradigm applied to the backgammon game by Tesauro (Tesauro and Sejnowski, 1989) can also be applied to the rescheduling problem of an aircrew pool. Indeed both problems correspond to choosing the best solution from a set of possible ones without ranking them (called here best choice problem). The paper explains from a mathematical point of view the architecture and the learning strategy of the backpropagation neural network used for the best choice problem. We also show how the learning phase of the network can be accelerated. Finally we apply the neural network model to some real rescheduling problems for the Belgian Airline (Sabena).},
 author = {Keymeulen, Didier and de Gerlache, Martine},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/aeb3135b436aa55373822c010763dd54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/aeb3135b436aa55373822c010763dd54-Metadata.json},
 openalex = {W2099528334},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/aeb3135b436aa55373822c010763dd54-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Comparison Training for a Rescheduling Problem in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/aeb3135b436aa55373822c010763dd54-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_afda3322,
 abstract = {In this paper the efficiency of recurrent neural network implementations of m-state finite state machines will be explored. Specifically, it will be shown that the node complexity for the unrestricted case can be bounded above by O(√m). It will also be shown that the node complexity is O(m log m) when the weights and thresholds are restricted to the set {−1,1} and O(m) when the fan-in is restricted to two. Matching lower bounds will be provided for each of these upper bounds assuming that the state of the FSM can be encoded in a subset of the nodes of size [log m].},
 author = {Horne, Bill and Hush, Don},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/afda332245e2af431fb7b672a68b659d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/afda332245e2af431fb7b672a68b659d-Metadata.json},
 openalex = {W1971476487},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/afda332245e2af431fb7b672a68b659d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bounds on the complexity of recurrent neural network implementations of finite state machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/afda332245e2af431fb7b672a68b659d-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_aff16212,
 abstract = {A new neural network, the Binary Diamond, is presented and its use as a classifier is demonstrated and evaluated. The network is of the feed-forward type. It learns from examples in the 'one shot' mode, and recruits new neurons as needed. It was tested on the problem of pixel classification, and performed well. Possible applications of the network in associative memories are outlined.},
 author = {Salu, Yehuda},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/aff1621254f7c1be92f64550478c56e6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/aff1621254f7c1be92f64550478c56e6-Metadata.json},
 openalex = {W2123808976},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Classification of Multi-Spectral Pixels by the Binary Diamond Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/aff1621254f7c1be92f64550478c56e6-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_b056eb15,
 abstract = {We extend Optimal Brain Surgeon (OBS) - a second-order method for pruning networks - to allow for general error measures, and explore a reduced computational and storage implementation via a dominant eigenspace decomposition. Simulations on nonlinear, noisy pattern classification problems reveal that OBS does lead to improved generalization, and performs favorably in comparison with Optimal Brain Damage (OBD). We find that the required retraining steps in OBD may lead to inferior generalization, a result that can be interpreted as due to injecting noise back into the system. A common technique is to stop training of a large network at the minimum validation error. We found that the test error could be reduced even further by means of OBS (but not OBD) pruning. Our results justify the t → 0 approximation used in OBS and indicate why retraining in a highly pruned network may lead to inferior performance.},
 author = {Hassibi, Babak and Stork, David and Wolff, Gregory},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b056eb1587586b71e2da9acfe4fbd19e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b056eb1587586b71e2da9acfe4fbd19e-Metadata.json},
 openalex = {W2141155619},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b056eb1587586b71e2da9acfe4fbd19e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Surgeon: Extensions and performance comparisons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/b056eb1587586b71e2da9acfe4fbd19e-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_b0b183c2,
 abstract = {We consider learning on multilayer neural nets with piecewise polynomial activation functions and a fixed number k of numerical inputs. We exhibit arbitrarily large network architectures for which efficient and provably successful learning algorithms exist in the rather realistic refinement of Valiant's model for probably approximately correct learning ("PAC learning") where no a priori assumptions are required about the "target function" (agnostic learning), arbitrary noise is permitted in the training sample, and the target outputs as well as the network outputs may be arbitrary reals. The number of computation steps of the learning algorithm LEARN that we construct is bounded by a polynomial in the bit-length n of the fixed number of input variables, in the bound s for the allowed bit-length of weights, in 1/ε, where ε is some arbitrary given bound for the true error of the neural net after training, and in 1/δ where δ is some arbitrary given bound for the probability that the learning algorithm fails for a randomly drawn training sample. However, the computation time of LEARN is exponential in the number of weights of the considered network architecture, and therefore only of interest for neural nets of small size. This article provides details to the previously published extended abstract (Maass 1994).},
 author = {Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b0b183c207f46f0cca7dc63b2604f5cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b0b183c207f46f0cca7dc63b2604f5cc-Metadata.json},
 openalex = {W2124076322},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b0b183c207f46f0cca7dc63b2604f5cc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Agnostic PAC Learning of Functions on Analog Neural Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/b0b183c207f46f0cca7dc63b2604f5cc-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_b137fdd1,
 abstract = {We describe an extension to the Mixture of Experts architecture for modelling and controlling dynamical systems which exhibit multiple modes of behavior. This extension is based on a Markov process model, and suggests a recurrent network for gating a set of linear or non-linear controllers. The new architecture is demonstrated to be capable of learning effective control strategies for jump linear and non-linear plants with multiple modes of behavior.},
 author = {Cacciatore, Timothy and Nowlan, Steven},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b137fdd1f79d56c7edf3365fea7520f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b137fdd1f79d56c7edf3365fea7520f2-Metadata.json},
 openalex = {W2172116616},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b137fdd1f79d56c7edf3365fea7520f2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Mixtures of Controllers for Jump Linear and Non-Linear Plants},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/b137fdd1f79d56c7edf3365fea7520f2-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_b6edc1cd,
 abstract = {A neurocomputer was implemented using radial basis functions and a combination of analog and digital VLSI circuits. The hybrid system uses custom analog circuits for the input layer and a digital signal processing board for the hidden and output layers. The system combines the advantages of both analog and digital circuits, featuring low power consumption while minimizing overall system error. The analog circuits have been fabricated and tested, the system has been built, and several applications have been executed on the system. One application provides significantly better results for a remote sensing problem than have been previously obtained using conventional methods.},
 author = {Watkins, Steven and Chau, Paul and Tawel, Raoul and Lambrigtsen, Bjorn and Plutowski, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b6edc1cd1f36e45daf6d7824d7bb2283-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b6edc1cd1f36e45daf6d7824d7bb2283-Metadata.json},
 openalex = {W2108116494},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b6edc1cd1f36e45daf6d7824d7bb2283-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Hybrid Radial Basis Function Neurocomputer and Its Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/b6edc1cd1f36e45daf6d7824d7bb2283-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_b7ee6f5f,
 author = {Diamantaras, K. I. and Geiger, D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Resolving motion ambiguities},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/b7ee6f5f9aa5cd17ca1aea43ce848496-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_b86e8d03,
 author = {Zemel, Richard and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b86e8d03fe992d1b0e19656875ee557c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b86e8d03fe992d1b0e19656875ee557c-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Developing Population Codes by Minimizing Description Length},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_beb22fb6,
 author = {Greenspan, Hayit},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/beb22fb694d513edcf5533cf006dfeae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/beb22fb694d513edcf5533cf006dfeae-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/beb22fb694d513edcf5533cf006dfeae-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning in Computer Vision and Image Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/beb22fb694d513edcf5533cf006dfeae-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_bf62768c,
 abstract = {We use two co-evolving neural networks to determine new classes of protein secondary structure which are significantly more predictable from local amino sequence than the conventional secondary structure classification. Accurate prediction of the conventional secondary structure classes: alpha helix, beta strand, and coil, from primary sequence has long been an important problem in computational molecular biology. Neural networks have been a popular method to attempt to predict these conventional secondary structure classes. Accuracy has been disappointingly low. The algorithm presented here uses neural networks to similtaneously examine both sequence and structure data, and to evolve new classes of secondary structure that can be predicted from sequence with significantly higher accuracy than the conventional classes. These new classes have both similarities to, and differences with the conventional alpha helix, beta strand and coil.},
 author = {Lapedes, Alan and Steeg, Evan and Farber, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/bf62768ca46b6c3b5bea9515d1a1fc45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/bf62768ca46b6c3b5bea9515d1a1fc45-Metadata.json},
 openalex = {W2138799519},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Definitions of Highly Predictable Protein Secondary Structure Classes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/bf62768ca46b6c3b5bea9515d1a1fc45-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_c0f168ce,
 abstract = {A simple model of coupled dynamics of fast neurons and slow interactions, modelling self-organization in recurrent neural networks, leads naturally to an effective statistical mechanics characterized by a partition function which is an average over a replicated system. This is reminiscent of the replica trick used to study spin-glasses, but with the difference that the number of replicas has a physical meaning as the ratio of two temperatures and can be varied throughout the whole range of real values. The model has interesting phase consequences as a function of varying this ratio and external stimuli, and can be extended to a range of other models.},
 author = {Coolen, A.C.C. and Penney, R. and Sherrington, D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c0f168ce8900fa56e57789e2a2f2c9d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c0f168ce8900fa56e57789e2a2f2c9d0-Metadata.json},
 openalex = {W2117083346},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Coupled Dynamics of Fast Neurons and Slow Interactions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/c0f168ce8900fa56e57789e2a2f2c9d0-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_c15da1f2,
 abstract = {Dynamic programming provides a methodology to develop planners and controllers for nonlinear systems. However, general dynamic programming is computationally intractable. We have developed procedures that allow more complex planning and control problems to be solved. We use second order local trajectory optimization to generate locally optimal plans and local models of the value function and its derivatives. We maintain global consistency of the local models of the value function, guaranteeing that our locally optimal plans are actually globally optimal, up to the resolution of our search procedures.},
 author = {Atkeson, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c15da1f2b5e5ed6e6837a3802f0d1593-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c15da1f2b5e5ed6e6837a3802f0d1593-Metadata.json},
 openalex = {W2105038027},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c15da1f2b5e5ed6e6837a3802f0d1593-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Using Local Trajectory Optimizers to Speed Up Global Optimization in Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/c15da1f2b5e5ed6e6837a3802f0d1593-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_c2626d85,
 abstract = {The introduction of specialized hardware platforms for connectionist modeling (connectionist supercomputer) has created a number of research topics. Some of these issues are controversial, e.g. the efficient implementation of incremental learning techniques, the need for the dynamic reconfiguration of networks and possible programming environments for these machines.},
 author = {Diederich, Joachim and Tsoi, Ah},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c2626d850c80ea07e7511bbae4c76f4b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c2626d850c80ea07e7511bbae4c76f4b-Metadata.json},
 openalex = {W2162367273},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connectionist Modeling and Parallel Architectures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/c2626d850c80ea07e7511bbae4c76f4b-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_c8ed21db,
 abstract = {This paper introduces GNARL, an evolutionary program which induces recurrent neural networks that are structurally unconstrained. In contrast to constructive and destructive algorithms, GNARL employs a population of networks and uses a fitness function's unsupervised feedback to guide search through network space. Annealing is used in generating both gaussian weight changes and structural modifications. Applying GNARL to a complex search and collection task demonstrates that the system is capable of inducing networks with complex internal dynamics.},
 author = {Saunders, Gregory and Angeline, Peter and Pollack, Jordan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c8ed21db4f678f3b13b9d5ee16489088-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c8ed21db4f678f3b13b9d5ee16489088-Metadata.json},
 openalex = {W2097060141},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/c8ed21db4f678f3b13b9d5ee16489088-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Structural and Behavioral Evolution of Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/c8ed21db4f678f3b13b9d5ee16489088-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_ca8155f4,
 abstract = {We present a Mean Field Theory method for locating two-dimensional objects that have undergone rigid transformations. The resulting algorithm is a form of coarse-to-fine correlation matching. We first consider problems of matching synthetic point data, and derive a point matching objective function. A tractable line segment matching objective function is derived by considering each line segment as a dense collection of points, and approximating it by a sum of Gaussians. The algorithm is tested on real images from which line segments are extracted and matched.},
 author = {Lu, Chien-Ping and Mjolsness, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ca8155f4d27f205953f9d3d7974bdd70-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ca8155f4d27f205953f9d3d7974bdd70-Metadata.json},
 openalex = {W2137397854},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Two-Dimensional Object Localization by Coarse-to-Fine Correlation Matching},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/ca8155f4d27f205953f9d3d7974bdd70-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_ccb09896,
 abstract = {This paper describes a low power analogue VLSI neural network called Wattle. Wattle is a 10:6:4 three layer perceptron with multiplying DAC synapses and on chip switched capacitor neurons fabricated in 1.2µm CMOS. The on chip neurons facillitate variable gain per neuron and lower energy/connection than for previous designs. The intended application of this chip is Intra Cardiac Electrogram classification as part of an implantable pacemaker/defibrillator system. Measurements of the chip indicate that 10pJ per connection is achievable as part of an integrated system. Wattle has been successfully trained in loop on parity 4 and ICEG morphology classification problems.},
 author = {Coggins, Richard and Jabri, Marwan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ccb0989662211f61edae2e26d58ea92f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ccb0989662211f61edae2e26d58ea92f-Metadata.json},
 openalex = {W2108619824},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {WATTLE: A Trainable Gain Analogue VLSI Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/ccb0989662211f61edae2e26d58ea92f-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_ce78d1da,
 abstract = {The purpose of this workshop was to discuss both recent experimental findings and computational models of the neurobiological implementation of selective attention. Recent experimental results were presented in two of the four presentations given (C.E. Connor, Washington University and B.C. Motter, SUNY and V.A. Medical Center, Syracuse), while the other two talks were devoted to computational models (E. Niebur, Caltech, and B. Olshausen, Washington University).},
 author = {Niebur, Ernst and Olshausen, Bruno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ce78d1da254c0843eb23951ae077ff5f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ce78d1da254c0843eb23951ae077ff5f-Metadata.json},
 openalex = {W2120898744},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ce78d1da254c0843eb23951ae077ff5f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neurobiology, Psychophysics, and Computational Models of Visual Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/ce78d1da254c0843eb23951ae077ff5f-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_d4c2e4a3,
 abstract = {The conventional Bayesian justification for backprop is that it finds the MAP weight vector. As this paper shows, to find the MAP i-o function instead, one must add a correction term to backprop. That term biases one towards i-o functions with small description lengths, and in particular favors (some kinds of) feature-selection, pruning, and weight-sharing. This can be viewed as an {\it a priori} argument in favor of those techniques.},
 author = {Wolpert, David H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d4c2e4a3297fe25a71d030b67eb83bfc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d4c2e4a3297fe25a71d030b67eb83bfc-Metadata.json},
 openalex = {W2117087049},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d4c2e4a3297fe25a71d030b67eb83bfc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bayesian Backpropagation Over I-O Functions Rather Than Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/d4c2e4a3297fe25a71d030b67eb83bfc-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_d554f7bb,
 abstract = {We present a fast algorithm for non-linear dimension reduction. The algorithm builds a local linear model of the data by merging PCA with clustering based on a new distortion measure. Experiments with speech and image data indicate that the local linear algorithm produces encodings with lower distortion than those built by five layer auto-associative networks. The local linear algorithm is also more than an order of magnitude faster to train.},
 author = {Kambhatla, Nanda and Leen, Todd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d554f7bb7be44a7267068a7df88ddd20-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d554f7bb7be44a7267068a7df88ddd20-Metadata.json},
 openalex = {W2141666957},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Fast Non-Linear Dimension Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/d554f7bb7be44a7267068a7df88ddd20-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_d58072be,
 abstract = {We prove that except possibly for small exceptional sets, discrete-time analog neural nets are globally observable, i.e. all their corrupted pseudo-orbits on computer simulations actually reflect the true dynamical behavior of the network. Locally finite discrete (boolean) neural networks are observable without exception.},
 author = {Garzon, Max and Botelho, Fernanda},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d58072be2820e8682c0a27c0518e805e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d58072be2820e8682c0a27c0518e805e-Metadata.json},
 openalex = {W2104209250},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d58072be2820e8682c0a27c0518e805e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Observability of Neural Network Behavior},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/d58072be2820e8682c0a27c0518e805e-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_d5cfead9,
 abstract = {In a recent paper we presented a methodological framework describing the two iteration performance of Hopfield-like attractor neural networks with history-dependent Bayesian dynamics. We now extend this analysis in a number of directions: input patterns applied to small subsets of neurons, general connectivity architectures and more efficient use of history. We show that the optimal signal (activation) function has a slanted sigmoidal shape, and provide an intuitive mount of activation functions with a non-monotone shape. This function endows the analytical model with some properties characteristic of cortical neurons' firing.},
 author = {Meilijson, Isaac and Ruppin, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d5cfead94f5350c12c322b5b664544c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d5cfead94f5350c12c322b5b664544c1-Metadata.json},
 openalex = {W2130261842},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimal signalling in attractor neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/d5cfead94f5350c12c322b5b664544c1-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_d840cc5d,
 abstract = {I consider the question "How should one act when the only goal is to learn as much as possible?". Building on the theoretical results of Fedorov (1972, Theory of Optimal Experiments, Academic Press) and MacKay (1992, Neural Computation, 4, 590-604), I apply techniques from optimal experiment design (OED) to guide the query/action selection of a neural network learner. I demonstrate that these techniques allow the learner to minimize its generalization error by exploring its domain efficiently and completely. I conclude that, while not a panacea, OED-based query/action selection has much to offer, especially in domains where its high computational costs can be tolerated. Copyright 1996 Elsevier Science Ltd},
 author = {Cohn, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d840cc5d906c3e9c84374c8919d2074e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d840cc5d906c3e9c84374c8919d2074e-Metadata.json},
 openalex = {W2003381716},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/d840cc5d906c3e9c84374c8919d2074e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Exploration Using Optimal Experiment Design},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/d840cc5d906c3e9c84374c8919d2074e-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_da8ce53c,
 abstract = {We provide a computational description of the function of the Mauthner system. This is the brainstem circuit which initiates fast-start escapes in teleost fish in response to sounds. Our simulations, using backpropagation in a realistically constrained feedforward network, have generated hypotheses which are directly interpretable in terms of the activity of the auditory nerve fibers, the principle cells of the system and their associated inhibitory neurons.},
 author = {Guzik, Audrey and Eaton, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/da8ce53cf0240070ce6c69c48cd588ee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/da8ce53cf0240070ce6c69c48cd588ee-Metadata.json},
 openalex = {W2154643487},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/da8ce53cf0240070ce6c69c48cd588ee-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Directional Hearing by the Mauthner System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/da8ce53cf0240070ce6c69c48cd588ee-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_dc568979,
 abstract = {Motivated by mathematical modeling, analog implementation and distributed simulation of neural networks, we present a definition of asynchronous dynamics of general CT dynamical systems defined by ordinary differential equations, based on notions of local times and communication times. We provide some preliminary results on globally asymptotical convergence of asynchronous dynamics for contractive and monotone CT dynamical systems. When applying the results to neural networks, we obtain some conditions that ensure additive-type neural networks to be asynchronizable.},
 author = {Wang, Xin and Li, Qingnan and Blum, Edward},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/dc5689792e08eb2e219dce49e64c885b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/dc5689792e08eb2e219dce49e64c885b-Metadata.json},
 openalex = {W2120265816},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/dc5689792e08eb2e219dce49e64c885b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Asynchronous Dynamics of Continuous Time Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/dc5689792e08eb2e219dce49e64c885b-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_dd45045f,
 abstract = {Maps of orientation preference and ocular dominance were recorded optically from the cortices of 5 infant macaque monkeys, ranging in age from 3.5 to 14 weeks. In agreement with previous observations, we found that basic features of orientation and ocular dominance maps, as well as correlations between them, are present and robust by 3.5 weeks of age. We did observe changes in the strength of ocular dominance signals, as well as in the spacing of ocular dominance bands, both of which increased steadily between 3.5 and 14 weeks of age. The latter finding suggests that the adult spacing of ocular dominance bands depends on cortical growth in neonatal animals. Since we found no corresponding increase in the spacing of orientation preferences, however, there is a possibility that the orientation preferences of some cells change as the cortical surface expands. Since correlations between the patterns of orientation selectivity and ocular dominance are present at an age, when the visual system is still immature, it seems more likely that their development may be an innate process and may not require extensive visual experience.},
 author = {Obermayer, Klaus and Kiorpes, Lynne and Blasdel, Gary},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/dd45045f8c68db9f54e70c67048d32e8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/dd45045f8c68db9f54e70c67048d32e8-Metadata.json},
 openalex = {W2145881979},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Development of Orientation and Ocular Dominance Columns in Infant Macaques},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/dd45045f8c68db9f54e70c67048d32e8-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_df263d99,
 abstract = {We show that a randomly selected N-tuple x→ of points of Rn with probability > 0 is such that any multi-layer percept ron with the first hidden layer composed of h1 threshold logic units can implement exactly 2 Σi=0h1n (N-1 i) different dichotomies of x→. If N > h1n then such a perceptron must have all units of the first hidden layer fully connected to inputs. This implies the maximal capacities (in the sense of Cover) of 2n input patterns per hidden unit and 2 input patterns per synaptic weight of such networks (both capacities are achieved by networks with single hidden layer and are the same as for a single neuron). Comparing these results with recent estimates of VC-dimension we find that in contrast to the single neuron case, for sufficiently large n, and h1 the VC-dimension exceeds Cover's capacity.},
 author = {Kowalczyk, Adam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/df263d996281d984952c07998dc54358-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/df263d996281d984952c07998dc54358-Metadata.json},
 openalex = {W2134941668},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/df263d996281d984952c07998dc54358-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Counting function theorem for multi-layer networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/df263d996281d984952c07998dc54358-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_df7f28ac,
 abstract = {We present a new incremental radial basis function network suitable for classification and regression problems. Center positions are continuously updated through soft competitive learning. The width of the radial basis functions is derived from the distance to topological neighbors. During the training the observed error is accumulated locally and used to determine where to insert the next unit. This leads (in case of classification problems) to the placement of units near class borders rather than near frequency peaks as is done by most existing methods. The resulting networks need few training epochs and seem to generalize very well. This is demonstrated by examples.},
 author = {Fritzke, Bernd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Metadata.json},
 openalex = {W2126598917},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Supervised Learning with Growing Cell Structures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/df7f28ac89ca37bf1abd2f6c184fe1cf-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_e0741335,
 abstract = {We present experimental results on supervised learning of dynamical features in an analog VLSI neural network chip. The recurrent network, containing six continuous-time analog neurons and 42 free parameters (connection strengths and thresholds), is trained to generate time-varying outputs approximating given periodic signals presented to the network. The chip implements a stochastic perturbative algorithm, which observes the error gradient along random directions in the parameter space for error-descent learning. In addition to the integrated learning functions and the generation of pseudo-random perturbations, the chip provides for teacher forcing and long-term storage of the volatile parameters. The network learns a 1 kHz circular trajectory in 100 sec. The chip occupies 2mm × 2mm in a 2µm CMOS process, and dissipates 1.2 mW.},
 author = {Cauwenberghs, Gert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e07413354875be01a996dc560274708e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e07413354875be01a996dc560274708e-Metadata.json},
 openalex = {W2161574926},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e07413354875be01a996dc560274708e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Learning Analog Neural Network Chip with Continuous-Time Recurrent Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/e07413354875be01a996dc560274708e-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_e0cf1f47,
 abstract = {This workshop reviewed and classified the various models which have emerged from the general concept of selective attention and context dependency, and sought to identify their commonalities. It was concluded that the motivation and mechanism of these models are efficiency and factoring, respectively. The workshop focused on computational models of selective attention and context dependency within the realm of neural networks. We treated only functional models; computational models of biological neural systems, and symbolic or rule-based systems were omitted from the discussion.},
 author = {Hildebrandt, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e0cf1f47118daebc5b16269099ad7347-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e0cf1f47118daebc5b16269099ad7347-Metadata.json},
 openalex = {W2151871305},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e0cf1f47118daebc5b16269099ad7347-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Functional Models of Selective Attention and Context Dependency},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/e0cf1f47118daebc5b16269099ad7347-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_e0ec453e,
 abstract = {One of the advantages of supervised learning is that the final error metric is available during training. For classifiers, the algorithm can directly reduce the number of misclassifications on the training set. Unfortunately, when modeling human learning or constructing classifiers for autonomous robots, supervisory labels are often not available or too expensive. In this paper we show that we can substitute for the labels by making use of structure between the pattern distributions to different sensory modalities. We show that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality, and leads to similar results. Using the Peterson-Barney vowel dataset we show that the algorithm performs well in finding appropriate placement for the codebook vectors particularly when the confuseable classes are different for the two modalities.},
 author = {de Sa, Virginia},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e0ec453e28e061cc58ac43f91dc2f3f0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e0ec453e28e061cc58ac43f91dc2f3f0-Metadata.json},
 openalex = {W2113896236},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e0ec453e28e061cc58ac43f91dc2f3f0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Classification with Unlabeled Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/e0ec453e28e061cc58ac43f91dc2f3f0-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_e2a2dcc3,
 abstract = {The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely difficult. Development of conventional Go programs is hampered by their knowledge-intensive nature. We demonstrate a viable alternative by training networks to evaluate Go positions via temporal difference (TD) learning.

Our approach is based on network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play. These techniques yield far better performance than undifferentiated networks trained by selfplay alone. A network with less than 500 weights learned within 3,000 games of 9×9 Go a position evaluation function that enables a primitive one-ply search to defeat a commercial Go program at a low playing level.},
 author = {Schraudolph, Nicol and Dayan, Peter and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e2a2dcc36a08a345332c751b2f2e476c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e2a2dcc36a08a345332c751b2f2e476c-Metadata.json},
 openalex = {W2159920598},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e2a2dcc36a08a345332c751b2f2e476c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Temporal Difference Learning of Position Evaluation in the Game of Go},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/e2a2dcc36a08a345332c751b2f2e476c-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_e49b8b40,
 abstract = {We study feed-forward nets with arbitrarily many layers, using the standard sigmoid, tanh x. Aside from technicalities, our theorems are: 1. Complete knowledge of the output of a neural net for arbitrary inputs uniquely specifies the architecture, weights and thresholds; and 2. There are only finitely many critical points on the error surface for a generic training problem.},
 author = {Fefferman, Charles and Markel, Scott},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e49b8b4053df9505e1f48c3a701c0682-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e49b8b4053df9505e1f48c3a701c0682-Metadata.json},
 openalex = {W2134828648},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e49b8b4053df9505e1f48c3a701c0682-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Recovering a Feed-Forward Net From Its Output},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/e49b8b4053df9505e1f48c3a701c0682-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_e57c6b95,
 abstract = {Stochastic optimization algorithms typically use learning rate schedules that behave asymptotically as µ(t) = µ0/t. The ensemble dynamics (Leen and Moody, 1993) for such algorithms provides an easy path to results on mean squared weight error and asymptotic normality. We apply this approach to stochastic gradient algorithms with momentum. We show that at late times, learning is governed by an effective learning rate µeff = µ0/(1 - β) where β is the momentum parameter. We describe the behavior of the asymptotic weight error and give conditions on µeff that insure optimal convergence speed. Finally, we use the results to develop an adaptive form of momentum that achieves optimal convergence speed independent of µ0.},
 author = {Leen, Todd and Orr, Genevieve},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e57c6b956a6521b28495f2886ca0977a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e57c6b956a6521b28495f2886ca0977a-Metadata.json},
 openalex = {W2159967940},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Stochastic Search and Adaptive Momentum},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/e57c6b956a6521b28495f2886ca0977a-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_e94550c9,
 abstract = {Parti-game is a new algorithm for learning feasible trajectories to goal regions in high dimensional continuous state-spaces. In high dimensions it is essential that neither planning nor exploration occurs uniformly over a state-space. Parti-game maintains a decision-tree partitioning of state-space and applies techniques from game-theory and computational geometry to efficiently and adaptively concentrate high resolution only on critical areas. The current version of the algorithm is designed to find feasible paths or trajectories to goal regions in high dimensional spaces. Future versions will be designed to find a solution that optimizes a real-valued criterion. Many simulated problems have been rested, ranging from two-dimensional to nine-dimensional state-spaces, including mazes, path planning, non-linear dynamics, and planar snake robots in restricted spaces. In all cases, a good solution is found in less than ten trials and a few minutes.},
 author = {Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e94550c93cd70fe748e6982b3439ad3b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e94550c93cd70fe748e6982b3439ad3b-Metadata.json},
 openalex = {W4231226883},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e94550c93cd70fe748e6982b3439ad3b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/e94550c93cd70fe748e6982b3439ad3b-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_e97ee205,
 abstract = {The general goal of this workshop was to bring together researchers working toward developing a theoretical framework for the analysis and design of neural networks. The technical focus of the workshop was to address recent developments in understanding the capabilities and limitations of various models for neural computation and learning. The primary topics addressed the following three areas: 1) Computational complexity issues in neural networks, 2) Complexity issues in learning, and 3) Convergence and numerical properties of learning algorithms. Other topics included experimental/simulation results on neural networks, which seemed to pose some open problems in the areas of learning and generalization properties of feedforward networks.},
 author = {Roychowdhury, V. P. and Siu, K.-Y.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e97ee2054defb209c35fe4dc94599061-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e97ee2054defb209c35fe4dc94599061-Metadata.json},
 openalex = {W2135322281},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Complexity Issues in Neural Computation and Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/e97ee2054defb209c35fe4dc94599061-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_e995f98d,
 abstract = {The theme was the effect of perturbations of the defining parameters of a neural network due to: 1) measurements (particularly with analog networks); 2) discretization due to a) digital implementation of analog nets; b) bounded-precision implementation of digital networks; or c) inaccurate evaluation of the transfer function(s); 3) noise in or incomplete input and/or output of the net or individual cells (particularly with analog networks).

The workshop presentations address these problems in various ways. Some develop models to understand the influence of errors/perturbation in the output, learning and general behavior of the net (probabilistic in Piche and Tresp; optimization in Rojas; dynamical systems in Botelho & Garzon). Others attempt to identify desirable properties that are to be preserved by neural network solutions (equilibria under faster convergence in Peterfreund & Baram; decision regions in Cohen). Of particular interest is to develop networks that compute robustly, in the sense that small perturbations of their parameters do not affect their dynamical and observable behavior (stability in biological networks in Chauvet & Chauvet; oscillation stability in learning in Rojas; hysterectic finite-state machine simulation in Casey). In particular, understand how biological networks cope with uncertainty and errors (Chauvet & Chauvet) through the type of stability that they exhibit.},
 author = {Garzon, Max and Botelho, Fernanda},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e995f98d56967d946471af29d7bf99f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e995f98d56967d946471af29d7bf99f1-Metadata.json},
 openalex = {W2134795035},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Stability and Observability},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/e995f98d56967d946471af29d7bf99f1-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_ede7e2b6,
 abstract = {The goal of this work was to investigate the role of primate MT neurons in solving the structure from motion (SFM) problem. Three types of receptive field (RF) surrounds found in area MT neurons (K.Tanaka et al., 1986; Allman et al., 1985) correspond, as our analysis suggests, to the 0th, 1st and 2nd order fuzzy space-differential operators. The large surround/center radius ratio (≥ 7) allows both differentiation of smooth velocity fields and discontinuity detection at boundaries of objects. The model is in agreement with recent psychophysical data on surface interpolation involvement in SFM. We suggest that area MT partially segregates information about object shape from information about spatial relations necessary for navigation and manipulation.},
 author = {Buracas, G. and Albright, T.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Metadata.json},
 openalex = {W2161915817},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Role of MT Neuron Receptive Field Surrounds in Computing Object Shape from Velocity Fields},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/ede7e2b6d13a41ddf9f4bdef84fdc737-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_edfbe1af,
 abstract = {Models of analog retrieval require a computationally cheap method of estimating similarity between a probe and the candidates in a large pool of memory items. The vector dot-product operation would be ideal for this purpose if it were possible to encode complex structures as vector representations in such a way that the superficial similarity of vector representations reflected underlying structural similarity. This paper describes how such an encoding is provided by Holographic Reduced Representations (HRRs), which are a method for encoding nested relational structures as fixed-width distributed representations. The conditions under which structural similarity is reflected in the dot-product rankings of HRRs are discussed.},
 author = {Plate, Tony A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/edfbe1afcf9246bb0d40eb4d8027d90f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/edfbe1afcf9246bb0d40eb4d8027d90f-Metadata.json},
 openalex = {W2157142092},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Estimating analogical similarity by dot-products of Holographic Reduced Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/edfbe1afcf9246bb0d40eb4d8027d90f-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_eefc9e10,
 abstract = {Previously, we had developed the concept of a Segmental Neural Net (SNN) for phonetic modeling in continuous speech recognition (CSR). This kind of neural network technology advanced the state-of-the-art of large-vocabulary CSR, which employs Hidden Markov Models (HMM), for the ARPA 1000-word Resource Management corpus. More Recently, we started porting the neural net system to a larger, more challenging corpus - the ARPA 20,000-word Wall Street Journal (WSJ) corpus. During the porting, we explored the following research directions to refine the system: i) training context-dependent models with a regularization method; ii) training SNN with projection pursuit; and ii) combining different models into a hybrid system. When tested on both a development set and an independent test set, the resulting neural net system alone yielded a performance at the level of the HMM system, and the hybrid SNN/HMM system achieved a consistent 10-15% word error reduction over the HMM system. This paper describes our hybrid system, with emphasis on the optimization methods employed.},
 author = {Zhao, Ying and Schwartz, Richard and Makhoul, John and Zavaliagkos, George},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Metadata.json},
 openalex = {W2164567263},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Segmental Neural Net Optimization for Continuous Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/eefc9e10ebdc4a2333b42b2dbb8f27b6-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_f0adc883,
 abstract = {Performance of many nonparametric methods critically depends on the strategy for positioning knots along the regression surface. Constrained Topological Mapping algorithm is a novel method that achieves adaptive knot placement by using a neural network based on Kohonen's self-organizing maps. We present a modification to the original algorithm that provides knot placement according to the estimated second derivative of the regression surface},
 author = {Najafi, Hossein L. and Cherkassky, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f0adc8838f4bdedde4ec2cfad0515589-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f0adc8838f4bdedde4ec2cfad0515589-Metadata.json},
 openalex = {W2137863930},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive knot Placement for Nonparametric Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/f0adc8838f4bdedde4ec2cfad0515589-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_f1c15925,
 abstract = {Identifying and classifying action potential shapes in extracellular neural waveforms have long been the subject of research, and although several algorithms for this purpose have been successfully applied, their use has been limited by some outstanding problems. The first is how to determine shapes of the action potentials in the waveform and, second, how to decide how many shapes are distinct. A harder problem is that action potentials frequently overlap making difficult both the determination of the shapes and the classification of the spikes. In this report, a solution to each of these problems is obtained by applying Bayesian probability theory. By defining a probabilistic model of the waveform, the probability of both the form and number of spike shapes can be quantified. In addition, this framework is used to obtain an efficient algorithm for the decomposition of arbitrarily complex overlap sequences. This algorithm can extract many times more information than previous methods and facilitates the extracellular investigation of neuronal classes and of interactions within neuronal circuits.},
 author = {Lewicki, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f1c1592588411002af340cbaedd6fc33-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f1c1592588411002af340cbaedd6fc33-Metadata.json},
 openalex = {W2025462430},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bayesian Modeling and Classification of Neural Signals},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_f2201f51,
 abstract = {Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data set.s. We use mixture models for the density estimates and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm--EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark--the iris data set--are presented.},
 author = {Ghahramani, Zoubin and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f2201f5191c4e92cc5af043eebfd0946-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f2201f5191c4e92cc5af043eebfd0946-Metadata.json},
 openalex = {W2128221272},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Supervised learning from incomplete data via an EM approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/f2201f5191c4e92cc5af043eebfd0946-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_f7e9050c,
 abstract = {Most theoretical investigations of large recurrent networks focus on the properties of the macroscopic order parameters such as population averaged activities or average overlaps with memories. However, the statistics of the fluctuations in the local activities may be an important testing ground for comparison between models and observed cortical dynamics. We evaluated the neuronal correlation functions in a stochastic network comprising of excitatory and inhibitory populations. We show that when the network is in a stationary state, the cross-correlations are relatively weak, i.e., their amplitude relative to that of the auto-correlations are of order of 1/N, N being the size of the interacting population. This holds except in the neighborhoods of bifurcations to nonstationary states. As a bifurcation point is approached the amplitude of the cross-correlations grows and becomes of order 1 and the decay time-constant diverges. This behavior is analogous to the phenomenon of critical slowing down in systems at thermal equilibrium near a critical point. Near a Hopf bifurcation the cross-correlations exhibit damped oscillations.},
 author = {Ginzburg, Iris and Sompolinsky, Haim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f7e9050c92a851b0016442ab604b0488-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f7e9050c92a851b0016442ab604b0488-Metadata.json},
 openalex = {W2125534101},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f7e9050c92a851b0016442ab604b0488-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Correlation Functions in a Large Stochastic Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/f7e9050c92a851b0016442ab604b0488-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_f9028fae,
 abstract = {With a point matching distance measure which is invariant under translation, rotation and permutation, we learn 2-D point-set objects, by clustering noisy point-set images. Unlike traditional clustering methods which use distance measures that operate on feature vectors - a representation common to most problem domains - this object-based clustering technique employs a distance measure specific to a type of object within a problem domain. Formulating the clustering problem as two nested objective functions, we derive optimization dynamics similar to the Expectation-Maximization algorithm used in mixture models.},
 author = {Gold, Steven and Mjolsness, Eric and Rangarajan, Anand},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f9028faec74be6ec9b852b0a542e2f39-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f9028faec74be6ec9b852b0a542e2f39-Metadata.json},
 openalex = {W2133003874},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f9028faec74be6ec9b852b0a542e2f39-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Clustering with a Domain-Specific Distance Measure},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/f9028faec74be6ec9b852b0a542e2f39-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_f9a40a47,
 abstract = {A new classifier is presented for text-independent speaker recognition. The new classifier is called the modified neural tree network (MNTN). The NTN is a hierarchical classifier that combines the properties of decision trees and feed-forward neural networks. The MNTN differs from the standard NTN in that a new learning rule based on discriminant learning is used, which minimizes the classification error as opposed to a norm of the approximation error. The MNTN also uses leaf probability measures in addition to the class labels. The MNTN is evaluated for several speaker identification experiments and is compared to multilayer perceptrons (MLPs), decision trees, and vector quantization (VQ) classifiers. The VQ classifier and MNTN demonstrate comparable performance and perform significantly better than the other classifiers for this task. Additionally, the MNTN provides a logarithmic saving in retrieval time over that of the VQ classifier. The MNTN and VQ classifiers are also compared for several speaker verification experiments where the MNTN is found to outperform the VQ classifier.},
 author = {Farrell, Kevin and Mammone, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f9a40a4780f5e1306c46f1c8daecee3b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f9a40a4780f5e1306c46f1c8daecee3b-Metadata.json},
 openalex = {W2144402804},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Speaker Recognition Using Neural Tree Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/f9a40a4780f5e1306c46f1c8daecee3b-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_fa14d4fe,
 abstract = {This study explores the extent to which a network that learns the temporal relationships within and between the component features of Western tonal music can account for music theoretic and psychological phenomena such as the tonal hierarchy and rhythmic expectancies. Predicted and generated sequences were recorded as the representation of a 153-note waltz melody was learnt by a predictive, recurrent network. The network learned transitions and relations between and within pitch and timing components: accent and duration values interacted in the development of rhythmic and metric structures and, with training, the network developed chordal expectancies in response to the activation of individual tones. Analysis of the hidden unit representation revealed that musical sequences are represented as transitions between states in hidden unit space.},
 author = {Stevens, Catherine and Wiles, Janet},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fa14d4fe2f19414de3ebd9f63d5c0169-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fa14d4fe2f19414de3ebd9f63d5c0169-Metadata.json},
 openalex = {W2124608834},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fa14d4fe2f19414de3ebd9f63d5c0169-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Tonal Music as a Componential Code: Learning Temporal Relationships Between and Within Pitch and Timing Components},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/fa14d4fe2f19414de3ebd9f63d5c0169-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_fa3a3c40,
 abstract = {Although the visual and auditory systems share the same basic tasks of informing an organism about its environment, most connectionist work on hearing to date has been devoted to the very different problem of speech recognition. We believe that the most fundamental task of the auditory system is the analysis of acoustic signals into components corresponding to individual sound sources, which Bregman has called auditory scene analysis. Computational and connectionist work on auditory scene analysis is reviewed, and the outline of a general model that includes these approaches is described.},
 author = {Duda, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fa3a3c407f82377f55c19c5d403335c7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fa3a3c407f82377f55c19c5d403335c7-Metadata.json},
 openalex = {W2172002874},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fa3a3c407f82377f55c19c5d403335c7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connectionist Models for Auditory Scene Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/fa3a3c407f82377f55c19c5d403335c7-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_fa83a11a,
 abstract = {I propose a learning algorithm for learning hierarchical models for object recognition. The model architecture is a compositional hierarchy that represents part-whole relationships: parts are described in the local context of substructures of the object. The focus of this report is learning hierarchical models from data, i.e. inducing the structure of model prototypes from observed exemplars of an object. At each node in the hierarchy, a probability distribution governing its parameters must be learned. The connections between nodes reflects the structure of the object. The formulation of substructures is encouraged such that their parts become conditionally independent. The resulting model can be interpreted as a Bayesian Belief Network and also is in many respects similar to the stochastic visual grammar described by Mjolsness.},
 author = {Utans, Joachim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fa83a11a198d5a7f0bf77a1987bcd006-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fa83a11a198d5a7f0bf77a1987bcd006-Metadata.json},
 openalex = {W2132516041},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fa83a11a198d5a7f0bf77a1987bcd006-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning in Compositional Hierarchies: Inducing the Structure of Objects from Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/fa83a11a198d5a7f0bf77a1987bcd006-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_fb89705a,
 abstract = {Recent physiological research has shown that synchronization of oscillatory responses in striate cortex may code for relationships between visual features of objects. A VLSI circuit has been designed to provide rapid phase-locking synchronization of multiple oscillators to allow for further exploration of this neural mechanism. By exploiting the intrinsic random transistor mismatch of devices operated in subthreshold, large groups of phase-locked oscillators can be readily partitioned into smaller phase-locked groups. A multiple target tracker for binary images is described utilizing this phase-locking architecture. A VLSI chip has been fabricated and tested to verify the architecture. The chip employs Pulse Amplitude Modulation (PAM) to encode the output at the periphery of the system.},
 author = {Andreou, Andreas and Edwards, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fb89705ae6d743bf1e848c206e16a1d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fb89705ae6d743bf1e848c206e16a1d7-Metadata.json},
 openalex = {W2117863300},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fb89705ae6d743bf1e848c206e16a1d7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {VLSI Phase Locking Architectures for Feature Linking in Multiple Target Tracking Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/fb89705ae6d743bf1e848c206e16a1d7-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_fc3cf452,
 abstract = {We propose a computational model for how the cortex discriminates shape and depth from texture. The model consists of four stages: (1) extraction of local spatial frequency, (2) frequency characterization, (3) detection of texture compression by normalization, and (4) integration of the normalized frequency over space. The model accounts for a number of psychophysical observations including experiments based on novel random textures. These textures are generated from white noise and manipulated in Fourier domain in order to produce specific frequency spectra. Simulations with a range of stimuli, including real images, show qualitative and quantitative agreement with human perception.},
 author = {Sakai, K\^{o} and Finkel, Leif},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fc3cf452d3da8402bebb765225ce8c0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fc3cf452d3da8402bebb765225ce8c0e-Metadata.json},
 openalex = {W2114034150},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fc3cf452d3da8402bebb765225ce8c0e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Network Mechanism for the Determination of Shape-From-Texture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/fc3cf452d3da8402bebb765225ce8c0e-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_fc49306d,
 abstract = {In this paper we propose an extension to the RAAM by Pollack. This extension, the Labeling RAAM (LRAAM), can encode labeled graphs with cycles by representing pointers explicitly. Data encoded in an LRAAM can be accessed by pointer as well as by content. Direct access by content can be achieved by transforming the encoder network of the LRAAM into an analog Hopfield network with hidden units. Different access procedures can be defined depending on the access key. Sufficient conditions on the asymptotical stability of the associated Hopfield network are briefly introduced.},
 author = {Sperduti, Alessandro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fc49306d97602c8ed1be1dfbf0835ead-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fc49306d97602c8ed1be1dfbf0835ead-Metadata.json},
 openalex = {W2102738208},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fc49306d97602c8ed1be1dfbf0835ead-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Encoding Labeled Graphs by Labeling RAAM},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/fc49306d97602c8ed1be1dfbf0835ead-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_fc8001f8,
 abstract = {We apply active exemplar selection (Plutowski & White, 1991; 1993) to predicting a chaotic time series. Given a fixed set of examples, the method chooses a concise subset for training. Fitting these exemplars results in the entire set being fit as well as desired. The algorithm incorporates a method for regulating network complexity, automatically adding exemplars and hidden units as needed. Fitting examples generated from the Mackey-Glass equation with fractal dimension 2.1 to an rmse of 0.01 required about 25 exemplars and 3 to 6 hidden units. The method requires an order of magnitude fewer floating point operations than training on the entire set of examples, is significantly cheaper than two contending exemplar selection techniques, and suggests a simpler active selection technique that performs comparably.},
 author = {Plutowski, Mark and Cottrell, Garrison and White, Halbert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fc8001f834f6a5f0561080d134d53d29-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fc8001f834f6a5f0561080d134d53d29-Metadata.json},
 openalex = {W2147543047},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Mackey-Glass from 25 examples, Plus or Minus 2},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/fc8001f834f6a5f0561080d134d53d29-Abstract.html},
 volume = {6},
 year = {1993}
}

@inproceedings{NIPS1993_fe8c15fe,
 author = {Wahba, Grace and Wang, Yuedong and Gu, Chong and Klein, MD, Ronald and Klein, MD, Barbara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fe8c15fed5f808006ce95eddb7366e35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fe8c15fed5f808006ce95eddb7366e35-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1993/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Structured Machine Learning for \textquotesingle Soft\textquotesingle Classification with Smoothing Spline ANOVA and Stacked Tuning, Testing and Evaluation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/hash/fe8c15fed5f808006ce95eddb7366e35-Abstract.html},
 volume = {6},
 year = {1993}
}
