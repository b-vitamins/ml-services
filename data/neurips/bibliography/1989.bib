@inproceedings{NIPS1989_01161aaa,
 abstract = {In the development of an image segmentation system for time image processing applications, we apply the classical decision analysis paradigm by viewing image segmentation as a pixel classification task. We use supervised training to derive a classifier for our system from a set of examples of a particular pixel classification problem. In this study, we test the suitability of a connectionist method against two statistical methods, Gaussian maximum likelihood classifier and first, second, and third degree polynomial classifiers, for the solution of a real world image segmentation problem taken from combustion research. Classifiers are derived using all three methods, and the performance of all of the classifiers on the training data set as well as on 3 separate entire test images is measured.},
 author = {Gish, Sheri and Blanz, W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/01161aaa0b6d1345dd8fe4e481144d84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/01161aaa0b6d1345dd8fe4e481144d84-Metadata.json},
 openalex = {W2138983350},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Comparing the Performance of Connectionist and Statistical Classifiers on an Image Segmentation Problem},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/01161aaa0b6d1345dd8fe4e481144d84-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_0266e33d,
 abstract = {This paper explores whether analog circuitry can adequately perform constrained optimization. Constrained optimization circuits are designed using the differential multiplier method. These circuits fulfill time-varying constraints correctly. Example circuits include a quadratic programming circuit and a constrained flip-flop.},
 author = {Platt, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0266e33d3f546cb5436a10798e657d97-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0266e33d3f546cb5436a10798e657d97-Metadata.json},
 openalex = {W2138566027},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0266e33d3f546cb5436a10798e657d97-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analog Circuits for Constrained Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/0266e33d3f546cb5436a10798e657d97-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_0336dcba,
 abstract = {One of the attractions of neural network approaches to pattern recognition is the use of a discrimination-based training method. We show that once we have modified the output layer of a multilayer perceptron to provide mathematically correct probability distributions, and replaced the usual squared error criterion with a probability-based score, the result is equivalent to Maximum Mutual Information training, which has been used successfully to improve the performance of hidden Markov models for speech recognition. If the network is specially constructed to perform the recognition computations of a given kind of stochastic model based classifier then we obtain a method for discrimination-based training of the parameters of the models. Examples include an HMM-based word discriminator, which we call an 'Alphanet'.},
 author = {Bridle, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0336dcbab05b9d5ad24f4333c7658a0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0336dcbab05b9d5ad24f4333c7658a0e-Metadata.json},
 openalex = {W2140766383},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_03c6b069,
 author = {Softky, William and Kammen, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/03c6b06952c750899bb03d998e631860-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/03c6b06952c750899bb03d998e631860-Metadata.json},
 openalex = {W2166054640},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/03c6b06952c750899bb03d998e631860-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Can Simple Cells Learn Curves? A Hebbian Model in a Structured Environment},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/03c6b06952c750899bb03d998e631860-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_060ad924,
 abstract = {We outline a computational model of the development and regeneration of specific eye-brain circuits. The model comprises a self-organizing map-forming network which uses local Hebb rules. constrained by molecular markers. Various simulations of the development of eye-brain maps in fish and frogs are described.},
 author = {Cowan, Jack and Friedman, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/060ad92489947d410d897474079c1477-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/060ad92489947d410d897474079c1477-Metadata.json},
 openalex = {W2110397774},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/060ad92489947d410d897474079c1477-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Development and Regeneration of Eye-Brain Maps: A Computational Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/060ad92489947d410d897474079c1477-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_06138bc5,
 author = {Rogers, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/06138bc5af6023646ede0e1f7c1eac75-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/06138bc5af6023646ede0e1f7c1eac75-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/06138bc5af6023646ede0e1f7c1eac75-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Predicting Weather Using a Genetic Memory: A Combination of Kanerva\textquotesingle s Sparse Distributed Memory with Holland\textquotesingle s Genetic Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_077e29b1,
 abstract = {Interneurons in leech ganglia receive multiple sensory inputs and make synaptic contacts with many motor neurons. These units coordinate several different behaviors. We used physiological and anatomical constraints to construct a model of the local bending reflex. Dynamical networks were trained on experimentally derived input-output patterns using recurrent back-propagation. Units in the model were modified to include electrical synapses and multiple synaptic time constants. The properties of the hidden units that emerged in the simulations matched those in the leech. The model and data support distributed rather than localist representations in the local bending reflex. These results also explain counterintuitive aspects of the local bending circuitry.},
 author = {Lockery, Shawn and Fang, Yan and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/077e29b11be80ab57e1a2ecabb7da330-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/077e29b11be80ab57e1a2ecabb7da330-Metadata.json},
 openalex = {W2115579699},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/077e29b11be80ab57e1a2ecabb7da330-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Analysis of Distributed Representations of Dynamical Sensory-Motor Transformations in the Leech},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/077e29b11be80ab57e1a2ecabb7da330-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_084b6fbb,
 abstract = {This paper presents the results of a simulation of the spatial relationship between the inferior olivary nucleus and folium crus IIA of the lateral hemisphere of the rat cerebellum. The principal objective of this modeling effort was to resolve an apparent conflict between a proposed zonal organization of olivary projections to cerebellar cortex suggested by anatomical tract-tracing experiments (Brodal & Kawamura 1980; Campbell & Armstrong 1983) and a more patchy organization apparent with physiological mapping (Robertson 1987). The results suggest that several unique features of the olivocerebellar circuit may contribute to the appearance of zonal organization using anatomical techniques, but that the detailed patterns of patchy tactile projections seen with physiological techniques are a more accurate representation of the afferent organization of this region of cortex.},
 author = {Lee, Maurice and Bower, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Metadata.json},
 openalex = {W2131048599},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Computer Modeling Approach to Understanding the Inferior Olive and Its Relationships to the Cerebellar Cortex in Rats},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/084b6fbb10729ed4da8c3d3f5a3ae7c9-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_091d584f,
 abstract = {In associative reinforcement learning, an environment generates input vectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output pairs. The task is to discover and remember input-output pairs that generate rewards. Especially difficult cases occur when rewards are rare, since the expected time for any algorithm can grow exponentially with the size of the problem. Nonetheless, if a reinforcement function possesses regularities, and a learning algorithm exploits them, learning time can be reduced below that of non-generalizing algorithms. This paper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results on problems designed to offer differing opportunities for generalization.},
 author = {Ackley, David and Littman, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/091d584fced301b442654dd8c23b3fc9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/091d584fced301b442654dd8c23b3fc9-Metadata.json},
 openalex = {W2116585932},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generalization and Scaling in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_0aa1883c,
 abstract = {Given a set of input-output training samples, we describe a procedure for determining the time sequence of weights for a dynamic neural network to model an arbitrary input-output process. We formulate the input-output mapping problem as an optimal control problem, defining a performance index to be minimized as a function of time-varying weights. We solve the resulting nonlinear two-point-boundary-value problem, and this yields the training rule. For the performance index chosen, this rule turns out to be a continuous time generalization of the outer product rule earlier suggested heuristically by Hopfield for designing associative memories. Learning curves for the new technique are presented.},
 author = {Farotimi, O. and Dembo, Amir and Kailath, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0aa1883c6411f7873cb83dacb17b0afc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0aa1883c6411f7873cb83dacb17b0afc-Metadata.json},
 openalex = {W2163590698},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0aa1883c6411f7873cb83dacb17b0afc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Weight Matrix Synthesis Using Optimal Control Techniques},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/0aa1883c6411f7873cb83dacb17b0afc-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_0e01938f,
 abstract = {The firing patterns of populations of cells in the cat visual cortex can exhibit oscillatory responses in the range of 35 - 85 Hz. Furthermore, groups of neurons many mm's apart can be highly synchronized as long as the cells have similar orientation tuning. We investigate two basic network architectures that incorporate either nearest-neighbor or global feedback interactions and conclude that non-local feedback plays a fundamental role in the initial synchronization and dynamic stability of the oscillations.},
 author = {Kammen, Daniel and Koch, Christof and Holmes, Philip},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0e01938fc48a2cfb5f2217fbfb00722d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0e01938fc48a2cfb5f2217fbfb00722d-Metadata.json},
 openalex = {W2168217598},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0e01938fc48a2cfb5f2217fbfb00722d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Collective Oscillations in the Visual Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/0e01938fc48a2cfb5f2217fbfb00722d-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_0e65972d,
 abstract = {A short account is given of various investigations of neural network properties, beginning with the classic work of McCulloch & Pitts. Early work on neurodynamics and statistical mechanics, analogies with magnetic materials, fault tolerance via parallel distributed processing, memory, learning, and pattern recognition, is described.},
 author = {Cowan, Jack},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0e65972dce68dad4d52d063967f0a705-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0e65972dce68dad4d52d063967f0a705-Metadata.json},
 openalex = {W2147005945},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Networks: The Early Days},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/0e65972dce68dad4d52d063967f0a705-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_0f49c89d,
 abstract = {The performance sensitivity of Albus' CMAC network was studied for the scenario in which faults are introduced into the adjustable weights after training has been accomplished. It was found that fault sensitivity was reduced with increased generalization when loss of weight faults were considered, but sensitivity was increased for saturated weight faults.},
 author = {Carter, Michael and Rudolph, Franklin and Nucci, Adam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0f49c89d1e7298bb9930789c8ed59d48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0f49c89d1e7298bb9930789c8ed59d48-Metadata.json},
 openalex = {W2155652844},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/0f49c89d1e7298bb9930789c8ed59d48-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Operational Fault Tolerance of CMAC Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/0f49c89d1e7298bb9930789c8ed59d48-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_115f8950,
 abstract = {Spiking neurons which integrate to threshold and fire were used to study the transmission of frequency modulated (FM) signals through layered networks. Firing correlations between cells in the input layer were found to modulate the transmission of FM signals under certain dynamical conditions. A tonic level of activity was maintained by providing each cell with a source of Poisson-distributed synaptic input. When the average membrane depolarization produced by the synaptic input was sufficiently below threshold, the firing correlations between cells in the input layer could greatly amplify the signal present in subsequent layers. When the depolarization was sufficiently close to threshold, however, the firing synchrony between cells in the initial layers could no longer effect the propagation of FM signals. In this latter case, integrate-and-fire neurons could be effectively modeled by simpler analog elements governed by a linear input-output relation.},
 author = {Kenyon, G. and Fetz, Eberhard and Puff, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/115f89503138416a242f40fb7d7f338e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/115f89503138416a242f40fb7d7f338e-Metadata.json},
 openalex = {W2103729411},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/115f89503138416a242f40fb7d7f338e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Effects of Firing Synchrony on Signal Propagation in Layered Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/115f89503138416a242f40fb7d7f338e-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_13fe9d84,
 abstract = {The echolocating bat, Eptesicus fuscus, perceives the distance to sonar targets from the delay of echoes and the shape of targets from the spectrum of echoes. However, shape is perceived in terms of the target's range profile. The time separation of echo components from parts of the target located at different distances is reconstructed from the echo spectrum and added to the estimate of absolute delay already derived from the arrival-time of echoes. The bat thus perceives the distance to targets and depth within targets along the same psychological range dimension, which is computed. The image corresponds to the crosscorrelation function of echoes. Fusion of physiologically distinct time- and frequency-domain representations into a final, common time-domain image illustrates the binding of within-modality features into a unified, whole image. To support the structure of images along the dimension of range, bats can perceive echo delay with a hyperacuity of 10 nanoseconds.},
 author = {Simmons, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/13fe9d84310e77f13a6d184dbf1232f3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/13fe9d84310e77f13a6d184dbf1232f3-Metadata.json},
 openalex = {W2131007731},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Acoustic-Imaging Computations by Echolocating Bats: Unification of Diversely-Represented Stimulus Features into Whole Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/13fe9d84310e77f13a6d184dbf1232f3-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_1534b76d,
 abstract = {A comparison of algorithms that minimize error functions to train the trajectories of recurrent networks, reveals how complexity is traded off for causality. These algorithms are also related to time-independent formalisms. It is suggested that causal and scalable algorithms are possible when the activation dynamics of adaptive neurons is fast compared to the behavior to be learned. Standard continuous-time recurrent backpropagation is used in an example.},
 author = {Pineda, Fernando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/1534b76d325a8f591b52d302e7181331-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/1534b76d325a8f591b52d302e7181331-Metadata.json},
 openalex = {W2115515053},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/1534b76d325a8f591b52d302e7181331-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Time Dependent Adaptive Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/1534b76d325a8f591b52d302e7181331-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_16a5cdae,
 abstract = {We have developed graphics to visualize static and dynamic information in layered neural network learning systems. Emphasis was placed on creating new visuals that make use of spatial arrangements, size information, animation and color. We applied these tools to the study of back-propagation learning of simple Boolean predicates, and have obtained new insights into the dynamics of the learning process.},
 author = {Wejchert, Jakub and Tesauro, Gerald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/16a5cdae362b8d27a1d8f8c7b78b4330-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/16a5cdae362b8d27a1d8f8c7b78b4330-Metadata.json},
 openalex = {W2130569024},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Visualization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/16a5cdae362b8d27a1d8f8c7b78b4330-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_17000029,
 abstract = {Consider a robot wandering around an unfamiliar environment, performing actions and observing the consequences. The robot's task is to construct an internal model of its environment, a model that will allow it to predict the effects of its actions and to determine what sequences of actions to take to reach particular goal states. Rivest and Schapire (1987a,b; Schapire 1988) have studied this problem and have designed a symbolic algorithm to strategically explore and infer the structure of “finite state” environments. The heart of this algorithm is a clever representation of the environment called an update graph. We have developed a connectionist implementation of the update graph using a highly specialized network architecture. With backpropagation learning and a trivial exploration strategy — choosing random actions — the connectionist network can outperform the Rivest and Schapire algorithm on simple problems. Our approach has additional virtues, including the fact that the network can accommodate stochastic environments and that it suggests generalizations of the update graph representation that do not arise from a traditional, symbolic perspective.},
 author = {Mozer, Michael C and Bachrach, Jonathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/1700002963a49da13542e0726b7bb758-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/1700002963a49da13542e0726b7bb758-Metadata.json},
 openalex = {W1982898256},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/1700002963a49da13542e0726b7bb758-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Discovering the Structure of a Reactive Environment by Exploration},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/1700002963a49da13542e0726b7bb758-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_19f3cd30,
 abstract = {The CMAC storage scheme has been used as a basis for a software implementation of an associative memory system AMS, which itself is a major part of the learning control loop LERNAS. A major disadvantage of this CMAC-concept is that the degree of local generalization (area of interpolation) is fixed. This paper deals with an algorithm for self-organizing variable generalization for the AKS, based on ideas of T. Kohonen.},
 author = {Hormel, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/19f3cd308f1455b3fa09a282e0d496f4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/19f3cd308f1455b3fa09a282e0d496f4-Metadata.json},
 openalex = {W2146042914},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Self-organizing Associative Memory System for Control Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/19f3cd308f1455b3fa09a282e0d496f4-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_20f07591,
 abstract = {Thomas Kailath Inform. Systems Lab. Stanford University Stanford, Calif. 94305 A rigorous analysis on the finite precision computational 1, that are uniformly distributed, with high probability the perceptron can perform all possible binary classifications of the patterns. Moreover, the resulting neural network requires a vanishingly small proportion O(log n/n) of the memory that would be required for complete storage of the patterns. Further, the perceptron algorithm takes O(n2) arithmetic operations with high probability, whereas other methods such as linear programming takes O(n3 .5 ) in the worst case. We also indicate some mathematical connections with VLSI circuit testing and the theory of random matrices.},
 author = {Dembo, Amir and Siu, Kai-Yeung and Kailath, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/20f07591c6fcb220ffe637cda29bb3f6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/20f07591c6fcb220ffe637cda29bb3f6-Metadata.json},
 openalex = {W2140296504},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/20f07591c6fcb220ffe637cda29bb3f6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Complexity of finite precision neural network classifier},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/20f07591c6fcb220ffe637cda29bb3f6-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_274ad478,
 abstract = {In this paper, we discuss a current attempt at applying the organizational principle Edelman calls Neuronal Group Selection to the control of a real, two-link robotic manipulator. We begin by motivating the need for an alternative to the position-control paradigm of classical robotics, and suggest that a possible avenue is to look at the primitive animal limb 'neurologically ballistic' control mode. We have been considering a selectionist approach to coordinating a simple perception-action task.},
 author = {Donnett, Jim and Smithers, Tim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/274ad4786c3abca69fa097b85867d9a4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/274ad4786c3abca69fa097b85867d9a4-Metadata.json},
 openalex = {W2109535355},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/274ad4786c3abca69fa097b85867d9a4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neuronal Group Selection Theory: A Grounding in Robotics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/274ad4786c3abca69fa097b85867d9a4-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_289dff07,
 abstract = {It is well known that when an automatic learning algorithm is applied to a fixed corpus of data, the size of the corpus places an upper bound on the number of degrees of freedom that the model can contain if it is to generalize well. Because the amount of hardware in a neural network typically increases with the dimensionality of its inputs, it can be challenging to build a high-performance network for classifying large input patterns. In this paper, several techniques for addressing this problem are discussed in the context of an isolated word recognition task.},
 author = {Lang, Kevin and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/289dff07669d7a23de0ef88d2f7129e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/289dff07669d7a23de0ef88d2f7129e7-Metadata.json},
 openalex = {W2145173023},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/289dff07669d7a23de0ef88d2f7129e7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dimensionality Reduction and Prior Knowledge in E-Set Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/289dff07669d7a23de0ef88d2f7129e7-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_335f5352,
 abstract = {We explore representation of 3D objects in which several distinct 2D views are stored for each object. We demonstrate the ability of a two-layer network of thresholded summation units to support such representations. Using unsupervised Hebbian relaxation, the network learned to recognize ten objects from different viewpoints. The training process led to the emergence of compact representations of the specific input views. When tested on novel views of the same objects, the network exhibited a substantial generalization capability. In simulated psychophysical experiments, the network's behavior was qualitatively similar to that of human subjects.},
 author = {Weinshall, Daphna and Edelman, Shimon and B\"{u}lthoff, Heinrich},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/335f5352088d7d9bf74191e006d8e24c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/335f5352088d7d9bf74191e006d8e24c-Metadata.json},
 openalex = {W2105598323},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/335f5352088d7d9bf74191e006d8e24c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A self-organizing multiple-view representation of 3D objects},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/335f5352088d7d9bf74191e006d8e24c-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_3644a684,
 abstract = {A generic model of oscillating cortex, which assumes minimal coupling justified by known anatomy, is shown to function as an associative memory, using previously developed theory. The network has explicit excitatory neurons with local inhibitory interneuron feedback that forms a set of nonlinear oscillators coupled only by long range excitatory connections. Using a local Hebb-like learning rule for primary and higher order synapses at the ends of the long range connections, the system learns to store the kinds of oscillation amplitude patterns observed in olfactory and visual cortex. This rule is derived from a more general projection algorithm for recurrent analog networks, that analytically guarantees content addressable memory storage of continuous periodic sequences - capacity: N/2 Fourier components for an N node network - no spurious attractors.},
 author = {Baird, Bill},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/3644a684f98ea8fe223c713b77189a77-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/3644a684f98ea8fe223c713b77189a77-Metadata.json},
 openalex = {W2137385901},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/3644a684f98ea8fe223c713b77189a77-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Associative Memory in a Simple Model of Oscillating Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/3644a684f98ea8fe223c713b77189a77-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_36660e59,
 abstract = {A methodology for faster supervised learning in dynamical nonlinear neural networks is presented. It exploits the concept of adjoint operators to enable computation of changes in the network's response due to perturbations in all system parameters, using the solution of a single set of appropriately constructed linear equations. The lower bound on speedup per learning iteration over conventional methods for calculating the neuromorphic energy gradient is O(N2), where N is the number of neurons in the network.},
 author = {Barhen, Jacob and Toomarian, Nikzad and Gulati, Sandeep},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/36660e59856b4de58a219bcf4e27eba3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/36660e59856b4de58a219bcf4e27eba3-Metadata.json},
 openalex = {W2134257360},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/36660e59856b4de58a219bcf4e27eba3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adjoint Operator Algorithms for Faster Learning in Dynamical Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/36660e59856b4de58a219bcf4e27eba3-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_38db3aed,
 abstract = {It has been known for many years that specific regions of the working cerebral cortex display periodic variations in correlated cellular activity. While the olfactory system has been the focus of much of this work, similar behavior has recently been observed in primary visual cortex. We have developed models of both the olfactory and visual cortex which replicate the observed oscillatory properties of these networks. Using these models we have examined the dependence of oscillatory behavior on single cell properties and network architectures. We discuss the idea that the oscillatory events recorded from cerebral cortex may be intrinsic to the architecture of cerebral cortex as a whole, and that these rhythmic patterns may be important in coordinating neuronal activity during sensory processing.},
 author = {Wilson, Matthew and Bower, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/38db3aed920cf82ab059bfccbd02be6a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/38db3aed920cf82ab059bfccbd02be6a-Metadata.json},
 openalex = {W2118951009},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Computer Simulation of Oscillatory Behavior in Cerebral Cortical Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/38db3aed920cf82ab059bfccbd02be6a-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_39059724,
 abstract = {In recent years many researchers have investigated the use of Markov Random Fields (MRFs) for computer vision. They can be applied for example to reconstruct surfaces from sparse and noisy depth data coming from the output of a visual process, or to integrate early vision processes to label physical discontinuities. In this paper we show that by applying mean field theory to those MRFs models a class of neural networks is obtained. Those networks can speed up the solution for the MRFs models. The method is not restricted to computer vision.},
 author = {Geiger, Davi and Girosi, Federico},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/39059724f73a9969845dfe4146c5660e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/39059724f73a9969845dfe4146c5660e-Metadata.json},
 openalex = {W2104288852},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/39059724f73a9969845dfe4146c5660e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Coupled Markov Random Fields and Mean Field Theory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/39059724f73a9969845dfe4146c5660e-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_3b8a6142,
 abstract = {We announce new CMOS synapse circuits using only three and four MOSFETs/synapse. Neural states are asynchronous pulse streams, upon which arithmetic is performed directly. Chips implementing over 100 fully programmable synapses are described and projections to networks of hundreds of neurons are made.},
 author = {Brownlow, Michael and Tarassenko, Lionel and Murray, Alan and Hamilton, Alister and Han, Il and Reekie, H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/3b8a614226a953a8cd9526fca6fe9ba5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/3b8a614226a953a8cd9526fca6fe9ba5-Metadata.json},
 openalex = {W2129360572},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/3b8a614226a953a8cd9526fca6fe9ba5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Pulse-Firing Neural Chips for Hundreds of Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/3b8a614226a953a8cd9526fca6fe9ba5-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_45fbc6d3,
 abstract = {We present two connectionist architectures for chunking of symbolic rewrite rules. One uses backpropagation learning, the other competitive learning. Although they were developed for chunking the same sorts of rules, the two differ in their representational abilities and learning behaviors.},
 author = {Touretzky, David and Elvgreen, Gillette},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/45fbc6d3e05ebd93369ce542e8f2322d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/45fbc6d3e05ebd93369ce542e8f2322d-Metadata.json},
 openalex = {W2135557657},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/45fbc6d3e05ebd93369ce542e8f2322d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Rule Representations in a Connectionist Chunker},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/45fbc6d3e05ebd93369ce542e8f2322d-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_46ba9f2a,
 abstract = {This paper describes a neural network algorithm that (1) performs temporal pattern matching in real-time, (2) is trained on-line, with a single pass, (3) requires only a single template for training of each representative class, (4) is continuously adaptable to changes in background noise, (5) deals with transient signals having low signal-to-noise ratios, (6) works in the presence of non-Gaussian noise, (7) makes use of context dependencies and (8) outputs Bayesian probability estimates. The algorithm has been adapted to the problem of passive sonar signal detection and classification. It runs on a Connection Machine and correctly classifies, within 500 ms of onset, signals embedded in noise and subject to considerable uncertainty.},
 author = {Malkoff, Donald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/46ba9f2a6976570b0353203ec4474217-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/46ba9f2a6976570b0353203ec4474217-Metadata.json},
 openalex = {W2151478919},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/46ba9f2a6976570b0353203ec4474217-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Neural Network for Real-Time Signal Processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/46ba9f2a6976570b0353203ec4474217-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_4734ba6f,
 abstract = {We attempt to combine neural networks with knowledge from speech science to build a speaker independent speech recognition system. This knowledge is utilized in designing the preprocessing, input coding, output coding, output supervision and architectural constraints. To handle the temporal aspect of speech we combine delays, copies of activations of hidden and output units at the input level, and Back-Propagation for Sequences (BPS), a learning algorithm for networks with local self-loops. This strategy is demonstrated in several experiments, in particular a nasal discrimination task for which the application of a speech theory hypothesis dramatically improved generalization.},
 author = {Bengio, Yoshua and de Mori, Renato and Cardin, R\'{e}gis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/4734ba6f3de83d861c3176a6273cac6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/4734ba6f3de83d861c3176a6273cac6d-Metadata.json},
 openalex = {W2119341451},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/4734ba6f3de83d861c3176a6273cac6d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Speaker Independent Speech Recognition with Neural Networks and Speech Knowledge},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/4734ba6f3de83d861c3176a6273cac6d-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_48aedb88,
 abstract = {Analytic solutions to the information-theoretic evolution equation of the connection strength of a three-layer feedforward neural net for visual information processing are presented. The results are (1) the receptive fields of the feature-analysing cells correspond to the eigenvector of the maximum eigenvalue of the Fredholm integral equation of the first kind derived from the evolution equation of the connection strength; (2) a symmetry-breaking mechanism (parity-violation) has been identified to be responsible for the changes of the morphology of the receptive field; (3) the conditions for the formation of different morphologies are explicitly identified.},
 author = {Tang, Dun-Sung},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/48aedb8880cab8c45637abc7493ecddd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/48aedb8880cab8c45637abc7493ecddd-Metadata.json},
 openalex = {W2156368267},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/48aedb8880cab8c45637abc7493ecddd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analytic Solutions to the Formation of Feature-Analysing Cells of a Three-Layer Feedforward Visual Information Processing Neural Net},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/48aedb8880cab8c45637abc7493ecddd-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_502e4a16,
 abstract = {The vestibulo-ocular reflex (VOR) is the primary mechanism that controls the compensatory eye movements that stabilize retinal images during rapid head motion. The primary pathways of this system are feed-forward, with inputs from the semicircular canals and outputs to the oculomotor system. Since visual feedback is not used directly in the VOR computation, the system must exploit motor learning to perform correctly. Lisberger(1988) has proposed a model for adapting the VOR gain using image-slip information from the retina. We have designed and tested analog very largescale integrated (VLSI) circuitry that implements a simplified version of Lisberger's adaptive VOR model.},
 author = {DeWeerth, Stephen and Mead, Carver},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/502e4a16930e414107ee22b6198c578f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/502e4a16930e414107ee22b6198c578f-Metadata.json},
 openalex = {W2128249685},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/502e4a16930e414107ee22b6198c578f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Analog VLSI Model of Adaptation in the Vestibulo-Ocular Reflex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/502e4a16930e414107ee22b6198c578f-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_539fd53b,
 abstract = {1024 distributed-neuron synapses have been integrated in an active area of 6.1 mm × 3.3 mm using a 0.9 µm, double-metal, single-poly, n-well CMOS technology. The distributed-neuron synapses are arranged in blocks of 16, which we call '4 × 4 tiles'. Switch matrices are interleaved between each of these tiles to provide programmability of interconnections. With a small area overhead (15 %), the 1024 units of the network can be rearranged in various configurations. Some of the possible configurations are, a 12-32-12 network, a 16-12-12-16 network, two 12-32 networks etc. (the numbers separated by dashes indicate the number of units per layer, including the input layer). Weights are stored in analog form on MOS capacitors. The synaptic weights are usable to a resolution of 1 % of their full scale value. The limitation arises due to charge injection from the access switch and charge leakage. Other parameters like gain and shape of nonlinearity are also programmable.},
 author = {Satyanarayana, Srinagesh and Tsividis, Yannis and Graf, Hans},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/539fd53b59e3bb12d203f45a912eeaf2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/539fd53b59e3bb12d203f45a912eeaf2-Metadata.json},
 openalex = {W2098711133},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/539fd53b59e3bb12d203f45a912eeaf2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Reconfigurable Analog VLSI Neural Network Chip},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/539fd53b59e3bb12d203f45a912eeaf2-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_53c3bce6,
 abstract = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.},
 author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Metadata.json},
 openalex = {W2154579312},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Handwritten Digit Recognition with a Back-Propagation Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_555d6702,
 abstract = {Cascadable, CMOS synapse chips containing a cross-bar array of 32×32 (1024) programmable synapses have been fabricated as building blocks for fully parallel implementation of neural networks. The synapses are based on a hybrid digital-analog design which utilizes on-Chip 7-bit data latches to store quantized weights and two-quadrant multiplying DAC's to compute weighted outputs. The synapses exhibit 6-bit resolution and excellent monotonicity and consistency in their transfer characteristics. A 64-neuron hardware incorporating four synapse chips has been fabricated to investigate the performance of feedback networks in optimization problem solving. In this study, a 7×7, one-to-one assignment net and the Hop field-Tank 8-city traveling salesman problem net have been implemented in the hardware. The network's ability to obtain optimum or near optimum solutions in real time has been demonstrated.},
 author = {Moopenn, Alexander and Duong, T. and Thakoor, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/555d6702c950ecb729a966504af0a635-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/555d6702c950ecb729a966504af0a635-Metadata.json},
 openalex = {W2124967612},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/555d6702c950ecb729a966504af0a635-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Digital-Analog Hybrid Synapse Chips for Electronic Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/555d6702c950ecb729a966504af0a635-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_577ef115,
 abstract = {It is well-known that neural responses in particular brain regions are spatially organized, but no general principles have been developed that relate the structure of a brain map to the nature of the associated computation. On parallel computers, maps of a sort quite similar to brain maps arise when a computation is distributed across multiple processors. In this paper we will discuss the relationship between maps and computations on these computers and suggest how similar considerations might also apply to maps in the brain.},
 author = {Nelson, Mark and Bower, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/577ef1154f3240ad5b9b413aa7346a1e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/577ef1154f3240ad5b9b413aa7346a1e-Metadata.json},
 openalex = {W2098624395},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/577ef1154f3240ad5b9b413aa7346a1e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Computational Efficiency: A Common Organizing Principle for Parallel Computer Maps and Brain Maps?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/577ef1154f3240ad5b9b413aa7346a1e-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_57aeee35,
 abstract = {We introduce a cost function for learning in feed-forward neural networks which is an explicit function of the internal representation in addition to the weights. The learning problem can then be formulated as two simple perceptrons and a search for internal representations. Back-propagation is recovered as a limit. The frequency of successful solutions is better for this algorithm than for back-propagation when weights and hidden units are updated on the same timescale i.e. once every learning step.},
 author = {Krogh, Anders and Thorbergsson, C. and Hertz, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/57aeee35c98205091e18d1140e9f38cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/57aeee35c98205091e18d1140e9f38cf-Metadata.json},
 openalex = {W2117243393},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Cost Function for Internal Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/57aeee35c98205091e18d1140e9f38cf-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_58a2fc6e,
 abstract = {Two approaches were explored which integrate neural net classifiers with Hidden Markov Model (HMM) speech recognizers. Both attempt to improve speech pattern discrimination while retaining the temporal processing advantages of HMMs. One approach used neural nets to provide second-stage discrimination following an HMM recognizer. On a small vocabulary task, Radial Basis Function (RBF) and back-propagation neural nets reduced the error rate substantially (from 7.9% to 4.2% for the RBF classifier). In a larger vocabulary task, neural net classifiers did not reduce the error rate. They, however, outperformed Gaussian, Gaussian mixture, and k-nearest neighbor (KNN) classifiers. In another approach, neural nets functioned as low-level acoustic-phonetic feature extractors. When classifying phonemes based on single 10 msec. frames, discriminant RBF neural net classifiers outperformed Gaussian mixture classifiers. Performance, however, differed little when classifying phones by accumulating scores across all frames in phonetic segments using a single node HMM recognizer.},
 author = {Huang, William and Lippmann, Richard P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/58a2fc6ed39fd083f55d4182bf88826d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/58a2fc6ed39fd083f55d4182bf88826d-Metadata.json},
 openalex = {W2101098655},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/58a2fc6ed39fd083f55d4182bf88826d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {HMM Speech Recognition with Neural Net Discrimination},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/58a2fc6ed39fd083f55d4182bf88826d-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_621bf66d,
 abstract = {The effects of parameter modifications imposed by hardware constraints on a self-organizing feature map algorithm were examined. Performance was measured by the error rate of a speech recognition system which included this algorithm as part of the front-end processing. System parameters which were varied included weight (connection strength) quantization, adaptation quantization, distance measures and circuit approximations which include device characteristics and process variability. Experiments using the TI isolated word database for 16 speakers demonstrated degradation in performance when weight quantization fell below 8 bits. The competitive nature of the algorithm relaxes constraints on uniformity and linearity which makes it an excellent candidate for a fully analog circuit implementation. Prototype circuits have been fabricated and characterized following the constraints established through the simulation efforts.},
 author = {Mann, Jim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/621bf66ddb7c962aa0d22ac97d69b793-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/621bf66ddb7c962aa0d22ac97d69b793-Metadata.json},
 openalex = {W2171934033},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Effects of Circuit Integration on a Feature Map Vector Quantizer},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/621bf66ddb7c962aa0d22ac97d69b793-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_63923f49,
 abstract = {We have done an empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization performance. Two experiments are reported. In one, we use simulated data sets with well-controlled parameters, such as the signal-to-noise ratio of continuous-valued data. In the second, we train the network on vector-quantized mel cepstra from real speech samples. In each case, we use back-propagation to train the feedforward net to discriminate in a multiple class pattern classification problem. We report the results of these studies, and show the application of cross-validation techniques to prevent overfitting.},
 author = {Morgan, N. and Bourlard, H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/63923f49e5241343aa7acb6a06a751e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/63923f49e5241343aa7acb6a06a751e7-Metadata.json},
 openalex = {W2113457868},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/63923f49e5241343aa7acb6a06a751e7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generalization and Parameter Estimation in Feedforward Nets: Some Experiments},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/63923f49e5241343aa7acb6a06a751e7-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_63dc7ed1,
 abstract = {In this paper we describe the VLSI design and testing of a high capacity associative memory which we call the exponential correlation associative memory (ECAM). The prototype 3µ-CMOS programmable chip is capable of storing 32 memory patterns of 24 bits each. The high capacity of the ECAM is partly due to the use of special exponentiation neurons, which are implemented via sub-threshold MOS transistors in this design. The prototype chip is capable of performing one associative recall in 3 µs.},
 author = {Chiueh, Tzi-Dar and Goodman, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/63dc7ed1010d3c3b8269faf0ba7491d4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/63dc7ed1010d3c3b8269faf0ba7491d4-Metadata.json},
 openalex = {W2140080802},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/63dc7ed1010d3c3b8269faf0ba7491d4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {VLSI Implementation of a High-Capacity Neural Network Associative Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/63dc7ed1010d3c3b8269faf0ba7491d4-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_69adc1e1,
 abstract = {Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.},
 author = {Fahlman, Scott and Lebiere, Christian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/69adc1e107f7f7d035d7baf04342e1ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/69adc1e107f7f7d035d7baf04342e1ca-Metadata.json},
 openalex = {W2109779438},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Cascade-Correlation Learning Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_6a9aeddf,
 abstract = {The goal in this work has been to identify the neuronal elements of the cortical column that are most likely to support the learning of nonlinear associative maps. We show that a particular style of network learning algorithm based on locally-tuned receptive fields maps naturally onto cortical hardware, and gives coherence to a variety of features of cortical anatomy, physiology, and biophysics whose relations to learning remain poorly understood.},
 author = {Mel, Bartlett and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Metadata.json},
 openalex = {W2148028763},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Sigma-Pi Learning: On Radial Basis Functions and Cortical Associative Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_6c9882bb,
 abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Metadata.json},
 openalex = {W2114766824},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_6da9003b,
 abstract = {In our effort to develop a modular neural system for invariant learning and recognition of 3D objects, we introduce here a new module architecture called an aspect network constructed around adaptive axo-axo-dendritic synapses. This builds upon our existing system (Seibert & Waxman, 1989) which processes 20 shapes and classifies them into view categories (i.e., aspects) invariant to illumination, position, orientation, scale, and projective deformations. From a sequence of views, the aspect network learns the transitions between these aspects, crystallizing a graph-like structure from an initially amorphous network. Object recognition emerges by accumulating evidence over multiple views which activate competing object hypotheses.},
 author = {Seibert, Michael and Waxman, Allen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6da9003b743b65f4c0ccd295cc484e57-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6da9003b743b65f4c0ccd295cc484e57-Metadata.json},
 openalex = {W2127324578},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6da9003b743b65f4c0ccd295cc484e57-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Aspect Graph Representations from View Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/6da9003b743b65f4c0ccd295cc484e57-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_6f3ef77a,
 abstract = {At the level of individual neurons, catecholamine release increases the responsivity of cells to excitatory and inhibitory inputs. We present a model of catecholamine effects in a network of neural-like elements. We argue that changes in the responsivity of individual elements do not affect their ability to detect a signal and ignore noise. However, the same changes in cell responsivity in a network of such elements do improve the signal detection performance of the network as a whole. We show how this result can be used in a computer simulation of behavior to account for the effect of CNS stimulants on the signal detection performance of human subjects.},
 author = {Servan-Schreiber, David and Printz, Harry and Cohen, Jonathan D},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6f3ef77ac0e3619e98159e9b6febf557-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6f3ef77ac0e3619e98159e9b6febf557-Metadata.json},
 openalex = {W2143710034},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Effect of Catecholamines on Performance: From Unit to System Behavior},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/6f3ef77ac0e3619e98159e9b6febf557-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_705f2172,
 abstract = {A central problem in connectionist modelling is the control of network and architectural resources during learning. In the present approach, weights reflect a coarse prediction history as coded by a distribution of values and parameterized in the mean and standard deviation of these weight distributions. Weight updates are a function of both the mean and standard deviation of each connection in the network and vary as a function of the error signal (stochastic delta rule; Hanson, 1990). Consequently, the weights maintain information on their central tendency and their uncertainty in prediction. Such information is useful in establishing a policy concerning the size of the nodal complexity of the network and growth of new nodes. For example, during problem solving the present network can undergo meiosis, producing two nodes where there was one overtaxed node as measured by its coefficient of variation. It is shown in a number of benchmark problems that meiosis networks can find minimal architectures, reduce computational complexity, and overall increase the efficiency of the representation learning interaction.},
 author = {Hanson, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/705f2172834666788607efbfca35afb3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/705f2172834666788607efbfca35afb3-Metadata.json},
 openalex = {W2295027001},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/705f2172834666788607efbfca35afb3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Meiosis Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/705f2172834666788607efbfca35afb3-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_74db120f,
 abstract = {The properties of a cluster of multiple back-propagation (BP) networks are examined and compared to the performance of a single BP network. The underlying idea is that a synergistic effect within the cluster improves the performance and fault tolerance. Five networks were initially trained to perform the same input-output mapping. Following training, a cluster was created by computing an average of the outputs generated by the individual networks. The output of the cluster can be used as the desired output during training by feeding it back to the individual networks. In comparison to a single BP network, a cluster of multiple BP's generalization and significant fault tolerance. It appear that cluster advantage follows from simple maxim can fool some of the single BP's in a cluster all of the but you cannot fool all of them all of the time {Lincoln}.},
 author = {Lincoln, William and Skrzypek, Josef},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/74db120f0a8e5646ef5a30154e9f6deb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/74db120f0a8e5646ef5a30154e9f6deb-Metadata.json},
 openalex = {W2110593694},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/74db120f0a8e5646ef5a30154e9f6deb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Synergy of Clustering Multiple Back Propagation Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/74db120f0a8e5646ef5a30154e9f6deb-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_757b505c,
 abstract = {We present a novel, modular, recurrent connectionist network architecture which learns to robustly perform incremental parsing of complex sentences. From sequential input, one word at a time, our networks learn to do semantic role assignment, noun phrase attachment, and clause structure recognition for sentences with passive constructions and center embedded clauses. The networks make syntactic and semantic predictions at every point in time, and previous predictions are revised as expectations are affirmed or violated with the arrival of new information. Our networks induce their own grammar rules for dynamically transforming an input sequence of words into a syntactic/semantic interpretation. These networks generalize and display tolerance to input which has been corrupted in ways common in spoken language.},
 author = {Jain, Ajay and Waibel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/757b505cfd34c64c85ca5b5690ee5293-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/757b505cfd34c64c85ca5b5690ee5293-Metadata.json},
 openalex = {W2138708605},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/757b505cfd34c64c85ca5b5690ee5293-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Incremental Parsing by Modular Recurrent Connectionist Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/757b505cfd34c64c85ca5b5690ee5293-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_7a614fd0,
 abstract = {Traditional approaches to neural coding characterize the encoding of known stimuli in average neural responses. Organisms face nearly the opposite task--extracting information about an unknown time-dependent stimulus from short segments of a spike train. Here the neural code was characterized from the point of view of the organism, culminating in algorithms for real-time stimulus estimation based on a single example of the spike train. These methods were applied to an identified movement-sensitive neuron in the fly visual system. Such decoding experiments determined the effective noise level and fault tolerance of neural computation, and the structure of the decoding algorithms suggested a simple model for real-time analog signal processing with spiking neurons.},
 author = {Bialek, William and Rieke, Fred and van Steveninck, Robert and Warland, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/7a614fd06c325499f1680b9896beedeb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/7a614fd06c325499f1680b9896beedeb-Metadata.json},
 openalex = {W1984931175},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/7a614fd06c325499f1680b9896beedeb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Reading a Neural Code},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/7a614fd06c325499f1680b9896beedeb-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_7eabe3a1,
 abstract = {A method for storing analog vectors in Hopfield's continuous feedback model is proposed. By analog vectors we mean vectors whose components are real-valued. The vectors to be stored are set as equilibria of the network. The network model consists of one layer of visible neurons and one layer of hidden neurons. We propose a learning algorithm, which results in adjusting the positions of the equilibria, as well as guaranteeing their stability. Simulation results confirm the effectiveness of the method.},
 author = {Atiya, Amir and Abu-Mostafa, Yaser},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Metadata.json},
 openalex = {W2098208185},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Method for the Associative Storage of Analog Vectors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/7eabe3a1649ffa2b3ff8c02ebfd5659f-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_839ab468,
 abstract = {This paper explores the use of a model neural network for motor learning. Steinbuch and Taylor presented neural network designs to do nearest neighbor lookup in the early 1960s. In this paper their nearest neighbor network is augmented with a local model network, which fits a local model to a set of nearest neighbors. The network design is equivalent to local regression. This network architecture can represent smooth nonlinear functions, yet has simple training rules with a single global optimum. The network has been used for motor learning of a simulated arm and a simulated running machine.},
 author = {Atkeson, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/839ab46820b524afda05122893c2fe8e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/839ab46820b524afda05122893c2fe8e-Metadata.json},
 openalex = {W2142751562},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/839ab46820b524afda05122893c2fe8e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Using Local Models to Control Movement},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/839ab46820b524afda05122893c2fe8e-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_84d9ee44,
 abstract = {The forward modeling approach is a methodology for learning control when data is available in distal coordinate systems. We extend previous work by considering how this methodology can be applied to the optimization of quantities that are distal not only in space but also in time.},
 author = {Jordan, Michael and Jacobs, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/84d9ee44e457ddef7f2c4f25dc8fa865-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/84d9ee44e457ddef7f2c4f25dc8fa865-Metadata.json},
 openalex = {W2118426468},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/84d9ee44e457ddef7f2c4f25dc8fa865-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning to Control an Unstable System with Forward Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/84d9ee44e457ddef7f2c4f25dc8fa865-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_854d6fae,
 abstract = {If neurons sum up their inputs in a non-linear way, as some simulations suggest, how is this distributed fine-grained non-linearity exploited during learning? How are all the small sigmoids in synapse, spine and dendritic tree lined up in the right areas of their respective input spaces? In this report, I show how an abstract a temporal highly nested tree structure with a quadratic transfer function associated with each branchpoint, can self organise using only a single global reinforcement scalar, to perform binary classification tasks. The procedure works well, solving the 6-multiplexer and a difficult phoneme classification task as well as back-propagation does, and faster. Furthermore, it does not calculate an error gradient, but uses a statistical scheme to build moving models of the reinforcement signal.},
 author = {Bell, Tony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/854d6fae5ee42911677c739ee1734486-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/854d6fae5ee42911677c739ee1734486-Metadata.json},
 openalex = {W2102835722},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/854d6fae5ee42911677c739ee1734486-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning in Higher-Order "Artificial Dendritic Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/854d6fae5ee42911677c739ee1734486-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_85d8ce59,
 author = {Chauvin, Yves},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/85d8ce590ad8981ca2c8286f79f59954-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/85d8ce590ad8981ca2c8286f79f59954-Metadata.json},
 openalex = {W2133943238},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dynamic Behavior of Constained Back-Propagation Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_8c19f571,
 abstract = {We present a general and systematic method for neural network design based on the genetic algorithm. The technique works in conjunction with network learning rules, addressing aspects of the network's gross architecture, connectivity, and learning rule parameters. Networks can be optimized for various application-specific criteria, such as learning speed, generalilation, robustness and connectivity. The approach is model-independent. We describe a prototype system, NeuroGENESYS, that employs the backpropagation learning rule. Experiments on several small problems have been conducted. In each case, NeuroGENESYS has produced networks that perform significantly better than the randomly generated networks of its initial population. The computational feasibility of our approach is discussed.},
 author = {Harp, Steven and Samad, Tariq and Guha, Aloke},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/8c19f571e251e61cb8dd3612f26d5ecf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/8c19f571e251e61cb8dd3612f26d5ecf-Metadata.json},
 openalex = {W2123568654},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/8c19f571e251e61cb8dd3612f26d5ecf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Designing Application-Specific Neural Networks Using the Genetic Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/8c19f571e251e61cb8dd3612f26d5ecf-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_8f121ce0,
 abstract = {The phonological structure of human languages is intricate, yet highly constrained. Through a combination of connectionist modeling and linguistic analysis, we are attempting to develop a computational basis for the nature of phonology. We present a connectionist architecture that performs multiple simultaneous insertion, deletion, and mutation operations on sequences of phonemes, and introduce a novel additional primitive, clustering. Clustering provides an interesting alternative to both iterative and relaxation accounts of assimilation processes such as vowel harmony. Our resulting model is efficient because it processes utterances entirely in parallel using only feed-forward circuitry.},
 author = {Touretzky, David and Wheeler, Deirdre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/8f121ce07d74717e0b1f21d122e04521-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/8f121ce07d74717e0b1f21d122e04521-Metadata.json},
 openalex = {W2096103549},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Computational Basis for Phonology},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/8f121ce07d74717e0b1f21d122e04521-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_918317b5,
 abstract = {The brain represents the skin surface as a topographic map in the somatosensory cortex. This map has been shown experimentally to be modifiable in a use-dependent fashion throughout life. We present a neural network simulation of the competitive dynamics underlying this cortical plasticity by detailed analysis of receptive field properties of model neurons during simulations of skin coactivation, cortical lesion, digit amputation and nerve section.},
 author = {Grajski, Kamil and Merzenich, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/918317b57931b6b7a7d29490fe5ec9f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/918317b57931b6b7a7d29490fe5ec9f9-Metadata.json},
 openalex = {W2139566064},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/918317b57931b6b7a7d29490fe5ec9f9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Simulation of Somatosensory Representational Plasticity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/918317b57931b6b7a7d29490fe5ec9f9-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_9188905e,
 author = {Intrator, Nathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9188905e74c28e489b44e954ec0b9bca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9188905e74c28e489b44e954ec0b9bca-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9188905e74c28e489b44e954ec0b9bca-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Neural Network for Feature Extraction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/9188905e74c28e489b44e954ec0b9bca-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_92c8c96e,
 abstract = {A nonlinear neural framework, called the Generalized Hopfield network, is proposed, which is able to solve in a parallel distributed manner systems of nonlinear equations. The method is applied to the general nonlinear optimization problem. We demonstrate GHNs implementing the three most important optimization algorithms, namely the Augmented Lagrangian, Generalized Reduced Gradient and Successive Quadratic Programming methods. The study results in a dynamic view of the optimization problem and offers a straightforward model for the parallelization of the optimization computations, thus significantly extending the practical limits of problems that can be formulated as an optimization problem and which can gain from the introduction of nonlinearities in their structure (eg. pattern recognition, supervised learning, design of content-addressable memories).},
 author = {Reklaitis, Gintaras and Tsirukis, Athanasios and Tenorio, Manoel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/92c8c96e4c37100777c7190b76d28233-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/92c8c96e4c37100777c7190b76d28233-Metadata.json},
 openalex = {W2099448462},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/92c8c96e4c37100777c7190b76d28233-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generalized Hopfield Networks and Nonlinear Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/92c8c96e4c37100777c7190b76d28233-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_979d472a,
 abstract = {We present a number of Time-Delay Neural Network (TDNN) based architectures for multi-speaker phoneme recognition (/b,d,g/ task). We use speech of two females and four males to compare the performance of the various architectures against a baseline recognition rate of 95.9% for a single IDNN on the six-speaker /b,d,g/ task. This series of modular designs leads to a highly modular multi-network architecture capable of performing the six-speaker recognition task at the speaker dependent rate of 98.4%. In addition to its high recognition rate, the so-called Meta-Pi architecture learns - without direct supervision - to recognize the speech of one particular male speaker using internal models of other male speakers exclusively.},
 author = {Hampshire, John and Waibel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/979d472a84804b9f647bc185a877a8b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/979d472a84804b9f647bc185a877a8b5-Metadata.json},
 openalex = {W2149967315},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/979d472a84804b9f647bc185a877a8b5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connectionist Architectures for Multi-Speaker Phoneme Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/979d472a84804b9f647bc185a877a8b5-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_9b04d152,
 abstract = {In this paper we develop a Bayes criterion which includes the Rissanen complexity, for inferring regular grammar models. We develop two methods for regular grammar Bayesian inference. The first method is based on treating the regular grammar as a 1-dimensional Markov source, and the second is based on the combinatoric characteristics of the regular grammar itself. We apply the resulting Bayes criteria to a particular example in order to show the efficiency of each method.},
 author = {Smith, Kurt and Miller, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9b04d152845ec0a378394003c96da594-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9b04d152845ec0a378394003c96da594-Metadata.json},
 openalex = {W2161822448},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9b04d152845ec0a378394003c96da594-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bayesian Inference of Regular Grammar and Markov Source Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/9b04d152845ec0a378394003c96da594-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_9c838d2e,
 abstract = {Dataflow architectures are general computation engines optimized for the execution of fine-grain parallel algorithms. Neural networks can be simulated on these systems with certain advantages. In this paper, we review dataflow architectures, examine neural network simulation performance on a new generation dataflow machine, compare that performance to other simulation alternatives, and discuss the benefits and drawbacks of the dataflow approach.},
 author = {Smotroff, Ira},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9c838d2e45b2ad1094d42f4ef36764f6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9c838d2e45b2ad1094d42f4ef36764f6-Metadata.json},
 openalex = {W2101960124},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dataflow Architectures: Flexible Platforms for Neural Network Simulation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/9c838d2e45b2ad1094d42f4ef36764f6-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_9cfdf10e,
 author = {Baum, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Perceptron Algorithm Is Fast for Non-Malicious Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/9cfdf10e8fc047a44b08ed031e1f0ed1-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_a4f23670,
 abstract = {A new form of the deterministic Boltzmann machine (DBM) learning procedure is presented which can efficiently train network to discriminate between input vectors according to some criterion. The new technique directly utilizes the free energy of these field modules to represent the probability that the criterion is met, the free energy being readily manipulated by the learning procedure. Although conventional deterministic Boltzmann learning fails to extract the higher order feature of shift at a network bottleneck, combining the new mean field with the mutual information objective function rapidly produces that perfectly extract this important higher order feature without direct external supervision.},
 author = {Galland, Conrad and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/a4f23670e1833f3fdb077ca70bbd5d66-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/a4f23670e1833f3fdb077ca70bbd5d66-Metadata.json},
 openalex = {W2150042549},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/a4f23670e1833f3fdb077ca70bbd5d66-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Discovering High Order Features with Mean Field Modules},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/a4f23670e1833f3fdb077ca70bbd5d66-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_a597e505,
 abstract = {Decision making tasks that involve delayed consequences are very common yet difficult to address with supervised learning methods. If there is an accurate model of the underlying dynamical system, then these tasks can be formulated as sequential decision problems and solved by Dynamic Programming. This paper discusses reinforcement learning in terms of the sequential decision framework and shows how a learning algorithm similar to the one implemented by the Adaptive Critic Element used in the pole-balancer of Barto, Sutton, and Anderson (1983), and further developed by Sutton (1984), fits into this framework. Adaptive neural networks can play significant roles as modules for approximating the functions required for solving sequential decision problems.},
 author = {Barto, A. G. and Sutton, R. S. and Watkins, C. J. C. H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/a597e50502f5ff68e3e25b9114205d4a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/a597e50502f5ff68e3e25b9114205d4a-Metadata.json},
 openalex = {W2135630072},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/a597e50502f5ff68e3e25b9114205d4a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Sequential Decision Problems and Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/a597e50502f5ff68e3e25b9114205d4a-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_ac1dd209,
 abstract = {We have calculated, both analytically and in simulations, the rate of convergence at long times in the backpropagation learning algorithm for networks with and without hidden units. Our basic finding for units using the standard sigmoid transfer function is 1/t convergence of the error for large t, with at most logarithmic corrections for networks with hidden units. Other transfer functions may lead to a slower polynomial rate of convergence. Our analytic calculations were presented in (Tesauro, He & Ahamd, 1989). Here we focus in more detail on our empirical measurements of the convergence rate in numerical simulations, which confirm our analytic results.},
 author = {Ahmad, Subutai and Tesauro, Gerald and He, Yu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Metadata.json},
 openalex = {W2154556964},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Asymptotic Convergence of Backpropagation: Numerical Experiments},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_b1a59b31,
 abstract = {Selective is a form of directed search that can greatly increase the ability of a connectionist network to generalize accurately. Based on information from previous batches of samples, a network may be trained on data selectively sampled from regions in the domain that are unknown. This is realizable in cases when the distribution is known, or when the cost of drawing points from the target distribution is negligible compared to the cost of labeling them with the proper classification. The approach is justified by its applicability to the problem of training a network for power system security analysis. The benefits of selective sampling are studied analytically, and the results are confirmed experimentally.},
 author = {Atlas, Les and Cohn, David and Ladner, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/b1a59b315fc9a3002ce38bbe070ec3f5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/b1a59b315fc9a3002ce38bbe070ec3f5-Metadata.json},
 openalex = {W2140535827},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/b1a59b315fc9a3002ce38bbe070ec3f5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Training Connectionist Networks with Queries and Selective Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/b1a59b315fc9a3002ce38bbe070ec3f5-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_b1d10e7b,
 abstract = {A new concept for unsupervised learning based upon examples introduced to the neural network is proposed. Each example is considered as an interpolation node of the velocity field in the phase space. The velocities at these nodes are selected such that all the streamlines converge to an attracting set imbedded in the subspace occupied by the cluster of examples. The synaptic interconnections are found from learning procedure providing selected field. The theory is illustrated by examples.},
 author = {Zak, Michail and Toomarian, Nikzad},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Metadata.json},
 openalex = {W2159729261},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Unsupervised Learning in Neurodynamics Using the Phase Velocity Field Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/b1d10e7bafa4421218a51b1e1f1b0ba2-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_bcbe3365,
 abstract = {We are developing a phoneme based, speaker-dependent continuous speech recognition system embedding a Multilayer Perceptron (MLP) (i.e., a feedforward Artificial Neural Network), into a Hidden Markov Model (HMM) approach. In [Bourlard & Wellekens], it was shown that MLPs were approximating Maximum a Posteriori (MAP) probabilities and could thus be embedded as an emission probability estimator in HMMs. By using contextual information from a sliding window on the input frames, we have been able to improve frame or phoneme classification performance over the corresponding performance for Simple Maximum Likelihood (ML) or even MAP probabilities that are estimated without the benefit of context. However, recognition of words in continuous speech was not so simply improved by the use of an MLP, and several modifications of the original scheme were necessary for getting acceptable performance. It is shown here that word recognition performance for a simple discrete density HMM system appears to be somewhat better when MLP methods are used to estimate the emission probabilities.},
 author = {Bourlard, Herv\'{e} and Morgan, Nelson},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/bcbe3365e6ac95ea2c0343a2395834dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/bcbe3365e6ac95ea2c0343a2395834dd-Metadata.json},
 openalex = {W2102988002},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/bcbe3365e6ac95ea2c0343a2395834dd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Continuous Speech Recognition System Embedding MLP into HMM},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/bcbe3365e6ac95ea2c0343a2395834dd-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_bd686fd6,
 author = {MacKay, David and Miller, Kenneth},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/bd686fd640be98efaae0091fa301e613-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/bd686fd640be98efaae0091fa301e613-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/bd686fd640be98efaae0091fa301e613-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analysis of Linsker\textquotesingle s Simulations of Hebbian Rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/bd686fd640be98efaae0091fa301e613-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_be83ab3e,
 abstract = {Experimental evidence has shown analog neural networks to be ex~mely fault-tolerant; in particular. their performance does not appear to be significantly impaired when precision is limited. Analog neurons with limited precision essentially compute k-ary weighted multilinear threshold functions. which divide R into k regions with k-l hyperplanes. The behaviour of k-ary neural networks is investigated. There is no canonical set of threshold values for k>3. although they exist for binary and ternary neural networks. The weights can be made integers of only 0 «z +k ) log (z +k » bits. where z is the number of processors. without increasing hardware or running time. The weights can be made ±1 while increasing running time by a constant multiple and hardware by a small polynomial in z and k. Binary neurons can be used if the running time is allowed to increase by a larger constant multiple and the hardware is allowed to increase by a slightly larger polynomial in z and k. Any symmetric k-ary function can be computed in constant depth and size o (n k1/(k-2)!). and any k-ary function can be computed in constant depth and size 0 (nk). The alternating neural networks of Olafsson and Abu-Mostafa. and the quantized neural networks of Fleisher are closely related to this model. Analog Neural Networks of Limited Precision I 703},
 author = {Obradovic, Zoran and Parberry, Ian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/be83ab3ecd0db773eb2dc1b0a17836a1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/be83ab3ecd0db773eb2dc1b0a17836a1-Metadata.json},
 openalex = {W2115831891},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analog Neural Networks of Limited Precision I: Computing with Multilinear Threshold Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_c0e190d8,
 abstract = {The input/output properties of a 2 compartment model neuron are systematically explored. Taken from the work of MacGregor (MacGregor, 1987), the model neuron compartments contain several active conductances, including a potassium conductance in the dendritic compartment driven by the accumulation of intradendritic calcium. Dynamics of the conductances and potentials are governed by a set of coupled first order differential equations which are integrated numerically. There are a set of 17 internal parameters to this model, specificying conductance rate constants, time constants, thresholds, etc.

To study parameter sensitivity, a set of trials were run in which the input driving the neuron is kept fixed while each internal parameter is varied with all others left fixed.

To study the input/output relation, the input to the dendrite (a square wave) was varied (in frequency and magnitude) while all internal parameters of the system were left fixed, and the resulting output firing rate and bursting rate was counted.

The input/output relation of the model neuron studied turns out to be much more sensitive to modulation of certain dendritic potassium current parameters than to plasticity of synapse efficacy per se (the amount of current influx due to synapse activation). This would in turn suggest, as has been recently observed experimentally, that the potassium current may be as or more important a focus of neural plasticity than synaptic efficacy.},
 author = {Rhodes, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/c0e190d8267e36708f955d7ab048990d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/c0e190d8267e36708f955d7ab048990d-Metadata.json},
 openalex = {W2099559868},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/c0e190d8267e36708f955d7ab048990d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Systematic Study of the Input/Output Properties of a 2 Compartment Model Neuron With Active Membranes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/c0e190d8267e36708f955d7ab048990d-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_c24cd76e,
 abstract = {Recurrent nets are more powerful than feedforward nets because they allow simulation of dynamical systems. Everything from sine wave generators through computers to the brain are potential candidates, but to use recurrent nets to emulate dynamical systems we need learning algorithms to program them. Here I describe a new twist on an old algorithm for recurrent nets and compare it to its predecessors.},
 author = {Zipser, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/c24cd76e1ce41366a4bbe8a49b02a028-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/c24cd76e1ce41366a4bbe8a49b02a028-Metadata.json},
 openalex = {W2098618477},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Subgrouping Reduces Complexity and Speeds Up Learning in Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_c52f1bd6,
 abstract = {Most complex behaviors appear to be governed by internal motivational states or drives that modify an animal's responses to its environment. It is therefore of considerable interest to understand the neural basis of these motivational states. Drawing upon work on the neural basis of feeding in the marine mollusc Aplysia, we have developed a heterogeneous artificial neural network for controlling the feeding behavior of a simulated insect. We demonstrate that feeding in this artificial insect shares many characteristics with the motivated behavior of natural animals.},
 author = {Beer, Randall and Chiel, Hillel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/c52f1bd66cc19d05628bd8bf27af3ad6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/c52f1bd66cc19d05628bd8bf27af3ad6-Metadata.json},
 openalex = {W2147830683},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Implementation of Motivated Behavior: Feeding in an Artificial Insect},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/c52f1bd66cc19d05628bd8bf27af3ad6-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_ca46c1b9,
 abstract = {In order to detect the presence and location of immunoglobulin (Ig) domains from amino acid sequences we built a system based on a neural network with one hidden layer trained with back propagation. The program was designed to efficiently identify proteins exhibiting such domains, characterized by a few localized conserved regions and a low overall homology. When the National Biomedical Research Foundation (NBRF) NEW protein sequence database was scanned to evaluate the program's performance, we obtained very low rates of false negatives coupled with a moderate rate of false positives.},
 author = {Bengio, Yoshua and Bengio, Samy and Pouliot, Yannick and Agin, Patrick},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/ca46c1b9512a7a8315fa3c5a946e8265-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/ca46c1b9512a7a8315fa3c5a946e8265-Metadata.json},
 openalex = {W2140790266},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Neural Network to Detect Homologies in Proteins},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_cb70ab37,
 abstract = {A higher order single layer recursive network easily learns to simulate a deterministic finite state machine and recognize regular grammars. When an enhanced version of this neural net state machine is connected through a common error term to an external analog stack memory, the combination can be interpreted as a neural net pushdown automata. The neural net finite state machine is given the primitives, push and POP, and is able to read the top of the stack. Through a gradient descent learning rule derived from the common error function, the hybrid network learns to effectively use the stack actions to manipulate the stack memory and to learn simple contextfree grammars.},
 author = {Giles, C. and Sun, Guo-Zheng and Chen, Hsing-Hen and Lee, Yee-Chun and Chen, Dong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/cb70ab375662576bd1ac5aaf16b3fca4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/cb70ab375662576bd1ac5aaf16b3fca4-Metadata.json},
 openalex = {W2116664051},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Higher Order Recurrent Networks and Grammatical Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_cfa0860e,
 abstract = {Eight neural net and conventional pattern classifiers (Bayesian-unimodal Gaussian, k-nearest neighbor, standard back-propagation, adaptive-stepsize back-propagation, hypersphere, feature-map, learning vector quantizer, and binary decision tree) were implemented on a serial computer and compared using two speech recognition and two artificial tasks. Error rates were statistically equivalent on almost all tasks, but classifiers differed by orders of magnitude in memory requirements, training time, classification time, and ease of adaptivity. Nearest-neighbor classifiers trained rapidly but required the most memory. Tree classifiers provided rapid classification but were complex to adapt. Back-propagation classifiers typically required long training times and had intermediate memory requirements. These results suggest that classifier selection should often depend more heavily on practical considerations concerning memory and computation resources, and restrictions on training and classification times than on error rate.},
 author = {Lee, Yuchun and Lippmann, Richard P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/cfa0860e83a4c3a763a7e62d825349f7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/cfa0860e83a4c3a763a7e62d825349f7-Metadata.json},
 openalex = {W2106957943},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/cfa0860e83a4c3a763a7e62d825349f7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Practical Characteristics of Neural Network and Conventional Pattern Classifiers on Artificial and Speech Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/cfa0860e83a4c3a763a7e62d825349f7-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_cfecdb27,
 abstract = {Contour maps provide a general method for recognizing two-dimensional shapes. All but blank images give rise to such maps, and people are good at recognizing objects and shapes from them. The maps are encoded easily in long feature vectors that are suitable for recognition by an associative memory. These properties of contour maps suggest a role for them in early visual perception. The prevalence of direction-sensitive neurons in the visual cortex of mammals supports this view.},
 author = {Kanerva, Pentti},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/cfecdb276f634854f3ef915e2e980c31-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/cfecdb276f634854f3ef915e2e980c31-Metadata.json},
 openalex = {W2163020087},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/cfecdb276f634854f3ef915e2e980c31-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Contour-Map Encoding of Shape for Early Vision},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/cfecdb276f634854f3ef915e2e980c31-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_d1c38a09,
 abstract = {One popular class of unsupervised algorithms are competitive algorithms. In the traditional view of competition, only one competitor, the winner, adapts for any given case. I propose to view competitive adaptation as attempting to fit a blend of simple probability generators (such as gaussians) to a set of data-points. The maximum likelihood fit of a model of this type suggests a softer form of competition, in which all competitors adapt in proportion to the relative probability that the input came from each competitor. I investigate one application of the soft competitive model, placement of radial basis function centers for function interpolation, and show that the soft model can give better performance with little additional computational cost.},
 author = {Nowlan, Steven},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d1c38a09acc34845c6be3a127a5aacaf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d1c38a09acc34845c6be3a127a5aacaf-Metadata.json},
 openalex = {W2097863906},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d1c38a09acc34845c6be3a127a5aacaf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Maximum Likelihood Competitive Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/d1c38a09acc34845c6be3a127a5aacaf-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_d3957710,
 abstract = {The existence of modularity in the organization of nervous systems (e.g. cortical columns and olfactory glomeruli) is well known. We show that localized activity patterns in a layer of cells, collective excitations, can induce the formation of modular structures in the anatomical connections via a Hebbian learning mechanism. The networks are spatially homogeneous before learning, but the spontaneous emergence of localized collective excitations and subsequently modularity in the connection patterns breaks translational symmetry. This spontaneous symmetry breaking phenomenon is similar to those which drive pattern formation in reaction-diffusion systems. We have identified requirements on the patterns of lateral connections and on the gains of internal units which are essential for the development of modularity. These essential requirements will most likely remain operative when more complicated (and biologically realistic) models are considered.},
 author = {Chernajvsky, Alex and Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d395771085aab05244a4fb8fd91bf4ee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d395771085aab05244a4fb8fd91bf4ee-Metadata.json},
 openalex = {W2154134045},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d395771085aab05244a4fb8fd91bf4ee-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Note on Development of Modularity in Simple Cortical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/d395771085aab05244a4fb8fd91bf4ee-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_d6baf65e,
 abstract = {To achieve high-rate image data compression while maintainig a high quality reconstructed image, a good image model and an efficient way to represent the specific data of each image must be introduced. Based on the physiological knowledge of multi - channel characteristics and inhibitory interactions between them in the human visual system, a mathematically coherent parallel architecture for image data compression which utilizes the Markov random field Image model and interactions between a vast number of filter banks, is proposed.},
 author = {Okamoto, Toshiaki and Kawato, Mitsuo and Inui, Toshio and Miyake, Sei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d6baf65e0b240ce177cf70da146c8dc8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d6baf65e0b240ce177cf70da146c8dc8-Metadata.json},
 openalex = {W2134064344},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d6baf65e0b240ce177cf70da146c8dc8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Model Based Image Compression and Adaptive Data Representation by Interacting Filter Banks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/d6baf65e0b240ce177cf70da146c8dc8-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_d947bf06,
 abstract = {Minimization of energy or error functions has proved to be a useful principle in the design and analysis of neural networks and neural algorithms. A brief list of examples include: the backpropagation algorithm, the use of optimization methods in computational vision, the application of analog networks to the approximate solution of NP complete problems and the Hopfield model of associative memory.},
 author = {Baldi, Pierre and Rinott, Yosef and Stein, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d947bf06a885db0d477d707121934ff8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d947bf06a885db0d477d707121934ff8-Metadata.json},
 openalex = {W2128403207},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d947bf06a885db0d477d707121934ff8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {On the Distribution of the Number of Local Minima of a Random Function on a Graph},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/d947bf06a885db0d477d707121934ff8-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_d96409bf,
 abstract = {The use of a neural network to classify ECG signals directly, without parametrization, is discussed. The input to such a network must be translation-invariant, since the distinctive features of the ECG may appear anywhere in the input window. The input must also be insensitive to the episode-to-episode and patient-to-patient variability in the rhythm pattern. A simple transformation of the ECG time-series input that meets both criteria is considered. A set of internally recorded, transcardiac ECG signals obtained from 54 patients was used to train and test the network. For each type of rhythm presented to the network, the network could output normal sinus rhythm (NSR), ventricular tachycardia (VT), and ventricular fibrillation (VF) (for networks trained to discriminate between VT and VF), or ambiguous (AMB).< <ETX xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">&gt;</ETX>},
 author = {Lee, Susan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d96409bf894217686ba124d7356686c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d96409bf894217686ba124d7356686c9-Metadata.json},
 openalex = {W2104624599},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/d96409bf894217686ba124d7356686c9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Using a translation-invariant neural network to diagnose heart arrhythmia},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/d96409bf894217686ba124d7356686c9-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_db8e1af0,
 abstract = {Networks of spiking neurons in which spikes are fired as a Poisson process are studied. The state of a cell is determined by the instantaneous firing rate, and in the limit of high firing rates the model reduces to that studied by Hopfield. The inclusion of spiking results in several features including a noise-induced asymmetry between on and off states, and probability currents which destroy the usual description of network dynamics in terms of energy surfaces. Taking account of spikes also allows calibration of network parameters such as synaptic weights against experiments on real synapses. Realistic forms of the post-synaptic response alter the network dynamics, which suggests a dynamical learning mechanism.< <ETX xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">&gt;</ETX>},
 author = {Crair, Michael and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/db8e1af0cb3aca1ae2d0018624204529-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/db8e1af0cb3aca1ae2d0018624204529-Metadata.json},
 openalex = {W1774321452},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/db8e1af0cb3aca1ae2d0018624204529-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Non-Boltzmann dynamics in networks of spiking neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/db8e1af0cb3aca1ae2d0018624204529-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_e1654211,
 abstract = {A simple method for training the dynamical behavior of a neural network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. A straightforward adaptation of this method to feedforward networks offers an alternative to training by conventional back-propagation. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past.},
 author = {Rohwer, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e165421110ba03099a1c0393373c5b43-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e165421110ba03099a1c0393373c5b43-Metadata.json},
 openalex = {W2167607759},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e165421110ba03099a1c0393373c5b43-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The “moving targets” training algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/e165421110ba03099a1c0393373c5b43-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_e2c0be24,
 abstract = {Multi-layer perceptrons and trained classification trees are two very different techniques which have recently become popular. Given enough data and time, both methods are capable of performing arbitrary non-linear classification. We first consider the important differences between multi-layer perceptrons and classification trees and conclude that there is not enough theoretical basis for the clear-cut superiority of one technique over the other. For this reason, we performed a number of empirical tests on three real-world problems in power system load forecasting, power system security prediction, and speaker-independent vowel identification. In all cases, even for piecewise-linear trees, the multi-layer perceptron performed as well as or better than the trained classification trees.},
 author = {Atlas, Les and Cole, Ronald and Connor, Jerome and El-Sharkawi, Mohamed and Marks, Robert and Muthusamy, Yeshwant and Barnard, Etienne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e2c0be24560d78c5e599c2a9c9d0bbd2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e2c0be24560d78c5e599c2a9c9d0bbd2-Metadata.json},
 openalex = {W2104873529},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e2c0be24560d78c5e599c2a9c9d0bbd2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Performance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/e2c0be24560d78c5e599c2a9c9d0bbd2-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_e3796ae8,
 abstract = {In this paper, we present a novel implementation of the widely used Back-propagation neural net learning algorithm on the Connection Machine CM-2 - a general purpose, massively parallel computer with a hypercube topology. This implementation runs at about 180 million interconnections per second (IPS) on a 64K processor CM- 2. The main interprocessor communication operation used is 2D nearest neighbor communication. The techniques developed here can be easily extended to implement other algorithms for layered neural nets on the CM-2, or on other massively parallel computers which have 2D or higher degree connections among their processors.},
 author = {Zhang, Xiru and McKenna, Michael and Mesirov, Jill and Waltz, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e3796ae838835da0b6f6ea37bcf8bcb7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e3796ae838835da0b6f6ea37bcf8bcb7-Metadata.json},
 openalex = {W2112972442},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e3796ae838835da0b6f6ea37bcf8bcb7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Efficient Implementation of the Back-propagation Algorithm on the Connection Machine CM-2},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/e3796ae838835da0b6f6ea37bcf8bcb7-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_e4a6222c,
 abstract = {In this paper we present upper bounds for the learning rates for hybrid models that employ a combination of both self-organized and supervised learning, using radial basis functions to build receptive field representations in the hidden units. The learning performance in such networks with nearest neighbor heuristic can be improved upon by multiplying the individual receptive field widths by a suitable overlap factor. We present results indicating optimal values for such overlap factors. We also present a new algorithm for determining receptive field centers. This method negotiates more hidden units in the regions of the input space as a function of the output and is conducive to better learning when the number of patterns (hidden units) is small.},
 author = {Saha, Avijit and Keeler, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e4a6222cdb5b34375400904f03d8e6a5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e4a6222cdb5b34375400904f03d8e6a5-Metadata.json},
 openalex = {W2108400112},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e4a6222cdb5b34375400904f03d8e6a5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Algorithms for Better Representation and Faster Learning in Radial Basis Function Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/e4a6222cdb5b34375400904f03d8e6a5-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_e56954b4,
 abstract = {This work introduces a new method called Self Organizing Neural Network (SONN) algorithm and compares its performance with Back Propagation in a signal separation application. The problem is to separate two signals; a modem data signal and a male speech signal, added and transmitted through a 4 khz channel. The signals are sampled at 8 khz, and using supervised learning, an attempt is made to reconstruct them. The SONN is an algorithm that constructs its own network topology during training, which is shown to be much smaller than the BP network, faster to trained, and free from the trial-and-error network design that characterize BP.},
 author = {Kassebaum, John and Tenorio, Manoel and Schaefers, Christoph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e56954b4f6347e897f954495eab16a88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e56954b4f6347e897f954495eab16a88-Metadata.json},
 openalex = {W2157686457},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e56954b4f6347e897f954495eab16a88-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Cocktail Party Problem: Speech/Data Signal Separation Comparison between Backpropagation and SONN},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/e56954b4f6347e897f954495eab16a88-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_e96ed478,
 abstract = {The long-term goal of our laboratory is the development of analog resistive network-based VLSI implementations of early and intermediate vision algorithms. We demonstrate an experimental circuit for smoothing and segmenting noisy and sparse depth data using the resistive fuse and a 1-D edge-detection circuit for computing zero-crossings using two resistive grids with different space-constants. To demonstrate the robustness of our algorithms and of the fabricated analog CMOS VLSI chips, we are mounting these circuits onto small mobile vehicles operating in a real-time, laboratory environment.},
 author = {Koch, Christof and Bair, Wyeth and Harris, John and Horiuchi, Timothy and Hsu, Andrew and Luo, Jin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e96ed478dab8595a7dbda4cbcbee168f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e96ed478dab8595a7dbda4cbcbee168f-Metadata.json},
 openalex = {W2163800821},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/e96ed478dab8595a7dbda4cbcbee168f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Real-Time Computer Vision and Robotics Using Analog VLSI Circuits},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/e96ed478dab8595a7dbda4cbcbee168f-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_eae27d77,
 abstract = {A new learning algorithm, Learning by Choice of Internal Represetations (CHIR), was recently introduced. Whereas many algorithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal representations as the fundamental entities to be determined. The algorithm applies a search procedure in the space of internal representations, and a cooperative adaptation of the weights (e.g. by using the perceptron learning rule). Since the introduction of its basic, single output version, the CHIR algorithm was generalized to train any feed forward network of binary neurons. Here we present the generalised version of the CHIR algorithm, and further demonstrate its versatility by describing how it can be modified in order to train networks with binary (±1) weights. Preliminary tests of this binary version on the random teacher problem are also reported.},
 author = {Grossman, Tal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/eae27d77ca20db309e056e3d2dcd7d69-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/eae27d77ca20db309e056e3d2dcd7d69-Metadata.json},
 openalex = {W2137293139},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The CHIR Algorithm for Feed Forward Networks with Binary Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/eae27d77ca20db309e056e3d2dcd7d69-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_eb163727,
 abstract = {We propose a new way to construct a large-scale neural network for 3.000 handwritten Kanji characters recognition. This neural network consists of 3 parts: a collection of small-scale networks which are trained individually on a small number of Kanji characters; a network which integrates the output from the small-scale networks, and a process to facilitate the integration of these neworks. The recognition rate of the total system is comparable with those of the small-scale networks. Our results indicate that the proposed method is effective for constructing a large-scale network without loss of recognition performance.},
 author = {Mori, Yoshihiro and Joe, Kazuki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/eb163727917cbba1eea208541a643e74-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/eb163727917cbba1eea208541a643e74-Metadata.json},
 openalex = {W2104462656},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/eb163727917cbba1eea208541a643e74-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Large-Scale Neural Network Which Recognizes Handwritten Kanji Characters},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/eb163727917cbba1eea208541a643e74-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_ec8ce6ab,
 abstract = {The pyloric Central Pattern Generator of the crustacean stomatogastric ganglion is a well-defined biological neural network. This 14-neuron network is modulated by many inputs. These inputs reconfigure the network to produce multiple output patterns by three simple mechanisms: 1) detennining which cells are active; 2) modulating the synaptic efficacy; 3) changing the intrinsic response properties of individual neurons. The importance of modifiable intrinsic response properties of neurons for network function and modulation is discussed.},
 author = {Harris-Warrick, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/ec8ce6abb3e952a85b8551ba726a1227-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/ec8ce6abb3e952a85b8551ba726a1227-Metadata.json},
 openalex = {W2154353427},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/ec8ce6abb3e952a85b8551ba726a1227-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Mechanisms for Neuromodulation of Biological Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/ec8ce6abb3e952a85b8551ba726a1227-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_eda80a3d,
 abstract = {We are developing a hand-printed character recognition system using a multi-layered neural net trained through backpropagation. We report on results of training nets with samples of hand-printed digits scanned off of bank checks and hand-printed letters interactively entered into a computer through a stylus digitizer. Given a large training set, and a net with sufficient capacity to achieve high performance on the training set, nets typically achieved error rates of 4-5% at a 0% reject rate and 1-2% at a 10% reject rate. The topology and capacity of the system, as measured by the number of connections in the net, have surprisingly little effect on generalization. For those developing practical pattern recognition systems, these results suggest that a large and representative training sample may be the single, most important factor in achieving high recognition accuracy. From a scientific standpoint, these results raise doubts about the relevance to backpropagation of learning models that estimate the likelihood of high generalization from estimates of capacity. Reducing capacity does have other benefits however, especially when the reduction is accomplished by using local receptive fields with shared weights. In this latter case, we find the net evolves feature detectors resembling those in visual cortex and Linsker's orientation-selective nodes.},
 author = {Martin, Gale and Pittman, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/eda80a3d5b344bc40f3bc04f65b7a357-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/eda80a3d5b344bc40f3bc04f65b7a357-Metadata.json},
 openalex = {W2141690599},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/eda80a3d5b344bc40f3bc04f65b7a357-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Recognizing Hand-Printed Letters and Digits},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/eda80a3d5b344bc40f3bc04f65b7a357-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_f340f1b1,
 abstract = {We describe a model that can recognize two-dimensional shapes in an unsegmented image, independent of their orientation, position, and scale. The model, called TRAFFIC, efficiently represents the structural relation between an object and each of its component features by encoding the fixed viewpoint-invariant transformation from the feature's reference frame to the object's in the weights of a connectionist network. Using a hierarchy of such transformations, with increasing complexity of features at each successive layer, the network can recognize multiple objects in parallel. An implementation of TRAFFIC is described, along with experimental results demonstrating the network's ability to recognize constellations of stars in a viewpoint-invariant manner.},
 author = {Zemel, Richard and Mozer, Michael C and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f340f1b1f65b6df5b5e3f94d95b11daf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f340f1b1f65b6df5b5e3f94d95b11daf-Metadata.json},
 openalex = {W2110803095},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {TRAFFIC: Recognizing Objects Using Hierarchical Reference Frame Transformations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/f340f1b1f65b6df5b5e3f94d95b11daf-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_f718499c,
 abstract = {The mapping of the back-propagation and mean field theory learning algorithms onto a generic 2-D SIMD computer is described. This architecture proves to be very adequate for these applications since efficiencies close to the optimum can be attained. Expressions to find the learning rates are given and then particularized to the DAP array procesor.},
 author = {Nu\~{n}ez, Fernando and Fortes, Jos\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f718499c1c8cef6730f9fd03c8125cab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f718499c1c8cef6730f9fd03c8125cab-Metadata.json},
 openalex = {W2135999692},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f718499c1c8cef6730f9fd03c8125cab-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Performance of Connectionist Learning Algorithms on 2-D SIMD Processor Arrays},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/f718499c1c8cef6730f9fd03c8125cab-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_f7664060,
 abstract = {We have constructed a two axis camera positioning system which is roughly analogous to a single human eye. This Artificial-Eye (A-eye) combines the signals generated by two rate gyroscopes with motion information extracted from visual analysis to stabilize its camera. This stabilization process is similar to the vestibulo-ocular response (VOR); like the VOR, A-eye learns a system model that can be incrementally modified to adapt to changes in its structure, performance and environment. A-eye is an example of a robust sensory system that performs computations that can be of significant use to the designers of mobile robots.},
 author = {Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f7664060cc52bc6f3d620bcedc94a4b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f7664060cc52bc6f3d620bcedc94a4b6-Metadata.json},
 openalex = {W2167689886},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neurally Inspired Plasticity in Oculomotor Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_f90f2aca,
 abstract = {Acoustic speech recognition degrades in the presence of noise. Compensatory information is available from the visual speech signals around the speaker's mouth. Previous attempts at using these visual speech signals to improve automatic speech recognition systems have combined the acoustic and visual speech information at a symbolic level using heuristic rules. In this paper, we demonstrate an alternative approach to fusing the visual and acoustic speech information by training feedforward neural networks to map the visual signal onto the corresponding short-term spectral amplitude envelope (STSAE) of the acoustic signal. This information can be directly combined with the degraded acoustic STSAE. Significant improvements are demonstrated in vowel recognition from noise-degraded acoustic signals. These results are compared to the performance of humans, as well as other pattern matching and estimation algorithms.},
 author = {Sejnowski, Terrence J and Yuhas, Ben and Goldstein, Moise and Jenkins, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f90f2aca5c640289d0a29417bcb63a37-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f90f2aca5c640289d0a29417bcb63a37-Metadata.json},
 openalex = {W2115116459},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/f90f2aca5c640289d0a29417bcb63a37-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Combining Visual and Acoustic Speech Signals with a Neural Network Improves Intelligibility},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/f90f2aca5c640289d0a29417bcb63a37-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{NIPS1989_fe131d7f,
 abstract = {The midbrain of the barn owl contains a map-like representation of sound source direction which is used to precisely orient the head toward targets of interest. Elevation is computed from the interaural difference in sound level. We present models and computer simulations of two stages of level difference processing which qualitatively agree with known anatomy and physiology, and make several striking predictions.},
 author = {Spence, Clay D. and Pearson, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1989/file/fe131d7f5a6b38b23cc967316c13dae2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1989/file/fe131d7f5a6b38b23cc967316c13dae2-Metadata.json},
 openalex = {W2136399412},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1989/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Computation of Sound Source Elevation in the Barn Owl},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/hash/fe131d7f5a6b38b23cc967316c13dae2-Abstract.html},
 volume = {2},
 year = {1989}
}
