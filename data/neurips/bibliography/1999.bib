@inproceedings{NIPS1999_01e00f2f,
 abstract = {The performance of regular and irregular Gallager-type error-correcting code is investigated via methods of statistical physics. The transmitted codeword comprises products of the original message bits selected by two randomly-constructed sparse matrices; the number of non-zero row/column elements in these matrices constitutes a family of codes. We show that Shannon's channel capacity may be saturated in equilibrium for many of the regular codes while slightly lower performance is obtained for others which may be of higher practical relevance. Decoding aspects are considered by employing the TAP approach which is identical to the commonly used belief-propagation-based decoding. We show that irregular codes may saturate Shannon's capacity but with improved dynamical properties.},
 author = {Kabashima, Yoshiyuki and Murayama, Tatsuto and Saad, David and Vicente, Renato},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/01e00f2f4bfcbb7505cb641066f2859b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/01e00f2f4bfcbb7505cb641066f2859b-Metadata.json},
 openalex = {W2138364277},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/01e00f2f4bfcbb7505cb641066f2859b-Paper.pdf},
 publisher = {MIT Press},
 title = {Regular and Irregular Gallager-zype Error-Correcting Codes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/01e00f2f4bfcbb7505cb641066f2859b-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_02f03905,
 abstract = {Human reaction times during sensory-motor tasks vary considerably. To begin to understand how this variability arises, we examined neuronal populational response time variability at early versus late visual processing stages. The conventional view is that precise temporal information is gradually lost as information is passed through a layered network of mean-rate We tested in humans whether neuronal populations at different processing stages behave like mean-rate units. A blind source separation algorithm was applied to MEG signals from sensory-motor integration tasks. Response time latency and variability for multiple visual sources were estimated by detecting single-trial stimulus-locked events for each source. In two subjects tested on four visual reaction time tasks, we reliably identified sources belonging to early and late visual processing stages. The standard deviation of response latency was smaller for early rather than late processing stages. This supports the hypothesis that human populational response time variability increases from early to late visual processing stages.},
 author = {Tang, Akaysha and Pearlmutter, Barak and Hely, Tim and Zibulevsky, Michael and Weisend, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/02f039058bd48307e6f653a2005c9dd2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/02f039058bd48307e6f653a2005c9dd2-Metadata.json},
 openalex = {W2137242579},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/02f039058bd48307e6f653a2005c9dd2-Paper.pdf},
 publisher = {MIT Press},
 title = {An MEG Study of Response Latency and Variability in the Human Visual System During a Visual-Motor Integration Task},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/02f039058bd48307e6f653a2005c9dd2-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_08e6bea8,
 abstract = {Effective methods of capacity control via uniform convergence bounds for function expansions have been largely limited to Support Vector machines, where good bounds are obtainable by the entropy number approach. We extend these methods to systems with expansions in terms of arbitrary (parametrized) basis functions and a wide range of regularization methods covering the whole range of general linear additive models. This is achieved by a data dependent analysis of the eigenvalues of the corresponding design matrix.},
 author = {Smola, Alex and Shawe-Taylor, John and Sch\"{o}lkopf, Bernhard and Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/08e6bea8e90ba87af3c9554d94db6579-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/08e6bea8e90ba87af3c9554d94db6579-Metadata.json},
 openalex = {W2128768232},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/08e6bea8e90ba87af3c9554d94db6579-Paper.pdf},
 publisher = {MIT Press},
 title = {The Entropy Regularization Information Criterion},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/08e6bea8e90ba87af3c9554d94db6579-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_0efbe980,
 abstract = {We incorporate prior knowledge to construct nonlinear algorithms for invariant feature extraction and discrimination. Employing a unified framework in terms of a nonlinear variant of the Rayleigh coefficient, we propose non-linear generalizations of Fisher's discriminant and oriented PCA using Support Vector kernel functions. Extensive simulations show the utility of our approach.},
 author = {Mika, Sebastian and R\"{a}tsch, Gunnar and Weston, Jason and Sch\"{o}lkopf, Bernhard and Smola, Alex and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/0efbe98067c6c73dba1250d2beaa81f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/0efbe98067c6c73dba1250d2beaa81f9-Metadata.json},
 openalex = {W2096925994},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf},
 publisher = {MIT Press},
 title = {Invariant Feature Extraction and Classification in Kernel Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_10c272d0,
 abstract = {Graphical models, such as Bayesian networks and Markov random fields, represent statistical dependencies of variables by a graph. Local "belief propagation" rules of the sort proposed by Pearl (1988) are guaranteed to converge to the correct posterior probabilities in singly connected graphs. Recently, good performance has been obtained by using these same rules on graphs with loops, a method we refer to as loopy belief propagation. Perhaps the most dramatic instance is the near Shannon-limit performance of "Turbo codes," whose decoding algorithm is equivalent to loopy propagation. Except for the case of graphs with a single loop, there has been little theoretical understanding of loopy propagation. Here we analyze belief propagation in networks with arbitrary topologies when the nodes in the graph describe jointly gaussian random variables. We give an analytical formula relating the true posterior probabilities with those calculated using loopy propagation. We give sufficient conditions for convergence and show that when belief propagation converges, it gives the correct posterior means for all graph topologies, not just networks with a single loop. These results motivate using the powerful belief propagation algorithm in a broader class of networks and help clarify the empirical performance results.},
 author = {Weiss, Yair and Freeman, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/10c272d06794d3e5785d5e7c5356e9ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/10c272d06794d3e5785d5e7c5356e9ff-Metadata.json},
 openalex = {W2158122241},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf},
 publisher = {MIT Press},
 title = {Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary Topology},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/10c272d06794d3e5785d5e7c5356e9ff-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_11c484ea,
 abstract = {We study a population decoding paradigm in which the maximum likelihood inference is based on an unfaithful decoding model (UMLI). This is usually the case for neural population decoding because the encoding process of the brain is not exactly known, or because a simplified decoding model is preferred for saving computational cost. We consider an unfaithful decoding model which neglects the pair-wise correlation between neuronal activities, and prove that UMLI is asymptotically efficient when the neuronal correlation is uniform or of limited-range. The performance of UMLI is compared with that of the maximum likelihood inference based on a faithful model and that of the center of mass decoding method. It turns out that UMLI has advantages of decreasing the computational complexity remarkablely and maintaining a high-level decoding accuracy at the same time. The effect of correlation on the decoding accuracy is also discussed.},
 author = {Wu, Si and Nakahara, Hiroyuki and Murata, Noboru and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/11c484ea9305ea4c7bb6b2e6d570d466-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/11c484ea9305ea4c7bb6b2e6d570d466-Metadata.json},
 openalex = {W2153647085},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/11c484ea9305ea4c7bb6b2e6d570d466-Paper.pdf},
 publisher = {MIT Press},
 title = {Population Decoding Based on an Unfaithful Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/11c484ea9305ea4c7bb6b2e6d570d466-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_11d0e628,
 abstract = {N wideband sources recorded using N closely spaced receivers can feasibly be separated based only on second order statistics when using a physical model of the mixing process. In this case we show that the parameter estimation problem can be essentially reduced to considering directions of arrival and attenuations of each signal. The paper presents two demixing methods operating in the time and frequency domain and experimentally shows that it is always possible to demix signals arriving at different angles. Moreover, one can use spatial cues to solve the channel selection problem and a post-processing Wiener filter to ameliorate the artifacts caused by demixing.},
 author = {Rosca, Justinian and Ruanaidh, Joseph and Jourjine, Alexander and Rickard, Scott},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/11d0e6287202fced83f79975ec59a3a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/11d0e6287202fced83f79975ec59a3a6-Metadata.json},
 openalex = {W2119890508},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf},
 publisher = {MIT Press},
 title = {Broadband Direction-Of-Arrival Estimation Based on Second Order Statistics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_14851003,
 abstract = {Independent component analysis of natural images leads to emergence of simple cell properties, i.e. linear filters that resemble wavelets or Gabor functions. In this paper, we extend ICA to explain further properties of V1 cells. First, we decompose natural images into independent subspaces instead of scalar components. This model leads to emergence of phase and shift invariant features, similar to those in V1 complex cells. Second, we define a topography between the linear components obtained by ICA. The topographic distance between two components is defined by their higher-order correlations, so that two components are close to each other in the topography if they are strongly dependent on each other. This leads to simultaneous emergence of both topography and invariances similar to complex cell properties.},
 author = {Hyv\"{a}rinen, Aapo and Hoyer, Patrik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/148510031349642de5ca0c544f31b2ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/148510031349642de5ca0c544f31b2ef-Metadata.json},
 openalex = {W2154594384},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/148510031349642de5ca0c544f31b2ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Emergence of Topography and Complex Cell Properties from Natural Images using Extensions of ICA},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/148510031349642de5ca0c544f31b2ef-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_158fc2dd,
 abstract = {The problem of developing good policies for partially observable Markov decision problems (POMDPs) remains one of the most challenging areas of research in stochastic planning. One line of research in this area involves the use of reinforcement learning with belief states, probability distributions over the underlying model states. This is a promising method for small problems, but its application is limited by the intractability of computing or representing a full belief state for large problems. Recent work shows that, in many settings, we can maintain an approximate belief state, which is fairly close to the true belief state. In particular, great success has been shown with approximate belief states that marginalize out correlations between state variables. In this paper, we investigate two methods of full belief state reinforcement learning and one novel method for reinforcement learning using factored approximate belief states. We compare the performance of these algorithms on several well-known problem from the literature. Our results demonstrate the importance of approximate belief state representations for large problems.},
 author = {Rodriguez, Andres and Parr, Ronald and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/158fc2ddd52ec2cf54d3c161f2dd6517-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/158fc2ddd52ec2cf54d3c161f2dd6517-Metadata.json},
 openalex = {W2146398222},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/158fc2ddd52ec2cf54d3c161f2dd6517-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning Using Approximate Belief States},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/158fc2ddd52ec2cf54d3c161f2dd6517-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_17ed8abe,
 abstract = {We propose a novel approach for building finite memory predictive models similar in spirit to variable memory length Markov models (VLMMs). The models are constructed by first transforming the n-block structure of the training sequence into a spatial structure of points in a unit hypercube, such that the longer is the common suffix shared by any two n-blocks, the closer lie their point representations. Such a transformation embodies a Markov assumption - n-blocks with long common suffixes are likely to produce similar continuations. Finding a set of prediction contexts is formulated as a resource allocation problem solved by vector quantizing the spatial n-block representation. We compare our model with both the classical and variable memory length Markov models on three data sets with different memory and stochastic components. Our models have a superior performance, yet, their construction is fully automatic, which is shown to be problematic in the case of VLMMs.},
 author = {Ti\~{n}o, Peter and Dorffner, Georg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/17ed8abedc255908be746d245e50263a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/17ed8abedc255908be746d245e50263a-Metadata.json},
 openalex = {W2126228757},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/17ed8abedc255908be746d245e50263a-Paper.pdf},
 publisher = {MIT Press},
 title = {Building Predictive Models from Fractal Representations of Symbolic Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/17ed8abedc255908be746d245e50263a-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_1c54985e,
 abstract = {Everybody knows that neural networks need more than a single layer of nonlinear units to compute interesting functions. We show that this is false if one employs winner-take-all as nonlinear unit: • Any boolean function can be computed by a single k-winner-take-all unit applied to weighted sums of the input variables. • Any continuous function can be approximated arbitrarily well by a single soft winner-take-all unit applied to weighted sums of the input variables. • Only positive weights are needed in these (linear) weighted sums. This may be of interest from the point of view of neurophysiology, since only 15% of the synapses in the cortex are inhibitory. In addition it is widely believed that there are special microcircuits in the cortex that compute winner-take-all. • Our results support the view that winner-take-all is a very useful basic computational unit in Neural VLSI: □ it is wellknown that winner-take-all of n input variables can be computed very efficiently with 2n transistors (and a total wire length and area that is linear in n) in analog VLSI [Lazzaro et at., 1989] □ we show that winner-take-all is not just useful for special purpose computations, but may serve as the only nonlinear unit for neural circuits with universal computational power □ we show that any multi-layer perceptron needs quadratically in n many gates to compute winner-take-all for n input variables, hence winner-take-all provides a substantially more powerful computational unit than a perceptron (at about the same cost of implementation in analog VLSI). Complete proofs and further details to these results can be found in [Maass, 2000].},
 author = {Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/1c54985e4f95b7819ca0357c0cb9a09f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/1c54985e4f95b7819ca0357c0cb9a09f-Metadata.json},
 openalex = {W2141700268},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/1c54985e4f95b7819ca0357c0cb9a09f-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Computation with Winner-Take-All as the Only Nonlinear Operation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/1c54985e4f95b7819ca0357c0cb9a09f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_207f8801,
 abstract = {A new method for multivariate density estimation is developed based on the Support Vector Method (SVM) solution of inverse ill-posed problems. The solution has the form of a mixture of densities. This method with Gaussian kernels compared favorably to both Parzen's method and the Gaussian Mixture Model method. For synthetic data we achieve more accurate estimates for densities of 2, 6, 12, and 40 dimensions.},
 author = {Vapnik, Vladimir and Mukherjee, Sayan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/207f88018f72237565570f8a9e5ca240-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/207f88018f72237565570f8a9e5ca240-Metadata.json},
 openalex = {W2159026946},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/207f88018f72237565570f8a9e5ca240-Paper.pdf},
 publisher = {MIT Press},
 title = {Support Vector Method for Multivariate Density Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/207f88018f72237565570f8a9e5ca240-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_21be9a4b,
 abstract = {We describe an iterative algorithm for building vector machines used in classification tasks. The algorithm builds on ideas from support vector machines, boosting, and generalized additive models. The algorithm can be used with various continuously differential functions that bound the discrete (0-1) classification loss and is very simple to implement. We test the proposed algorithm with two different loss functions on synthetic and natural data. We also describe a norm-penalized version of the algorithm for the exponential loss function used in AdaBoost. The performance of the algorithm on natural data is comparable to support vector machines while typically its running time is shorter than of SVM.},
 author = {Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/21be9a4bd4f81549a9d1d241981cec3c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/21be9a4bd4f81549a9d1d241981cec3c-Metadata.json},
 openalex = {W2120350760},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf},
 publisher = {MIT Press},
 title = {Leveraged Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/21be9a4bd4f81549a9d1d241981cec3c-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_231141b3,
 abstract = {The problem of reinforcement learning in a non-Markov environment is explored using a dynamic Bayesian network, where conditional independence assumptions between random variables are compactly represented by network parameters. The parameters are learned on-line, and approximations are used to perform inference and to compute the optimal value function. The relative effects of inference and value function approximations on the quality of the final policy are investigated, by learning to solve a moderately difficult driving task. The two value function approximations, linear and quadratic, were found to perform similarly, but the quadratic model was more sensitive to initialization. Both performed below the level of human performance on the task. The dynamic Bayesian network performed comparably to a model using a localist hidden state representation, while requiring exponentially fewer parameters.},
 author = {Sallans, Brian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/231141b34c82aa95e48810a9d1b33a79-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/231141b34c82aa95e48810a9d1b33a79-Metadata.json},
 openalex = {W2114501936},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/231141b34c82aa95e48810a9d1b33a79-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Factored Representations for Partially Observable Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/231141b34c82aa95e48810a9d1b33a79-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_24510415,
 abstract = {We present an algorithm that infers the model structure of a mixture of factor analysers using an efficient and deterministic variational approximation to full Bayesian integration over model parameters. This procedure can automatically determine the optimal number of components and the local dimensionality of each component (i.e. the number of factors in each factor analyser). Alternatively it can be used to infer posterior distributions over number of components and dimensionalities. Since all parameters are integrated out the method is not prone to overfitting. Using a stochastic procedure for adding components it is possible to perform the variational optimisation incrementally and to avoid local maxima. Results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples.

By importance sampling from the variational approximation we show how to obtain unbiased estimates of the true evidence, the exact predictive density, and the KL divergence between the variational posterior and the true posterior, not only in this model but for variational approximations in general.},
 author = {Ghahramani, Zoubin and Beal, Matthew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2451041557a22145b3701b0184109cab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2451041557a22145b3701b0184109cab-Metadata.json},
 openalex = {W2151454335},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2451041557a22145b3701b0184109cab-Paper.pdf},
 publisher = {MIT Press},
 title = {Variational Inference for Bayesian Mixtures of Factor Analysers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/2451041557a22145b3701b0184109cab-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_25e2a30f,
 abstract = {Invariance to topographic transformations such as translation and shearing in an image has been successfully incorporated into feed-forward mechanisms, >e.g., convolutional neural networks, tangent propagation. We describe a way to add transformation invariance to a generative density model by approximating the nonlinear transformation manifold by a discrete set of transformations. An EM algorithm for the original model can be extended to the new model by computing expectations over the set of transformations. We show how to add a discrete transformation variable to Gaussian mixture modeling, factor analysis and mixtures of factor analysis. We give results on filtering microscopy images, face and facial pose clustering, and handwritten digit modeling and recognition.},
 author = {Jojic, Nebojsa and Frey, Brendan J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/25e2a30f44898b9f3e978b1786dcd85c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/25e2a30f44898b9f3e978b1786dcd85c-Metadata.json},
 openalex = {W2104591426},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/25e2a30f44898b9f3e978b1786dcd85c-Paper.pdf},
 publisher = {MIT Press},
 title = {Topographic Transformation as a Discrete Latent Variable},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/25e2a30f44898b9f3e978b1786dcd85c-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_2612aa89,
 abstract = {Stochastic fluctuations of voltage-gated ion channels generate current and voltage noise in neuronal membranes. This noise may be a critical determinant of the efficacy of information processing within neural systems. Using Monte-Carlo simulations, we carry out a systematic investigation of the relationship between channel kinetics and the resulting membrane voltage noise using a stochastic Markov version of the Mainen-Sejnowski model of dendritic excitability in cortical neurons. Our simulations show that kinetic parameters which lead to an increase in membrane excitability (increasing channel densities, decreasing temperature) also lead to an increase in the magnitude of the subthreshold voltage noise. Noise also increases as the membrane is depolarized from rest towards threshold. This suggests that channel fluctuations may interfere with a neuron's ability to function as an integrator of its synaptic inputs and may limit the reliability and precision of neural information processing.},
 author = {Manwani, Amit and Steinmetz, Peter and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2612aa892d962d6f8056b195ca6e550d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2612aa892d962d6f8056b195ca6e550d-Metadata.json},
 openalex = {W2136854392},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2612aa892d962d6f8056b195ca6e550d-Paper.pdf},
 publisher = {MIT Press},
 title = {Channel Noise in Excitable Neural Membranes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/2612aa892d962d6f8056b195ca6e550d-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_26751be1,
 abstract = {We present three simple approximations for the calculation of the posterior mean in Gaussian Process classification. The first two methods are related to mean field ideas known in Statistical Physics. The third approach is based on Bayesian online approach which was motivated by recent results in the Statistical Mechanics of Neural Networks. We present simulation results showing: 1. that the mean field Bayesian evidence may be used for hyperparameter tuning and 2. that the online approach may achieve a low training error fast.},
 author = {Csat\'{o}, Lehel and Fokou\'{e}, Ernest and Opper, Manfred and Schottky, Bernhard and Winther, Ole},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/26751be1181460baf78db8d5eb7aad39-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/26751be1181460baf78db8d5eb7aad39-Metadata.json},
 openalex = {W2100494227},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/26751be1181460baf78db8d5eb7aad39-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient Approaches to Gaussian Process Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/26751be1181460baf78db8d5eb7aad39-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_270edd69,
 abstract = {I consider a topographic projection between two neuronal layers with different densities of neurons. Given the number of output neurons connected to each input neuron (divergence) and the number of input neurons synapsing on each output neuron (convergence), I determine the widths of axonal and dendritic arbors which minimize the total volume of axons and dendrites. Analytical results for one-dimensional and two-dimensional projections can be summarized qualitatively in the following rule: neurons of the sparser layer should have arbors wider than those of the denser layer. This agrees with the anatomic data for retinal, cerebellar, olfactory bulb, and neocortical neurons the morphology and connectivity of which are known. The rule may be used to infer connectivity of neurons from their morphology.},
 author = {Chklovskii, Dmitri},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/270edd69788dce200a3b395a6da6fdb7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/270edd69788dce200a3b395a6da6fdb7-Metadata.json},
 openalex = {W2125472588},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/270edd69788dce200a3b395a6da6fdb7-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimal Sizes of Dendritic and Axonal Arbors in a Topographic Projection},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/270edd69788dce200a3b395a6da6fdb7-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_28dc6b0e,
 abstract = {AdaBoost and other ensemble methods have successfully been applied to a number of classification tasks, seemingly defying problems of overfitting. AdaBoost performs gradient descent in an error function with respect to the margin, asymptotically concentrating on the patterns which are hardest to learn. For very noisy problems, however, this can be disadvantageous. Indeed, theoretical analysis has shown that the margin distribution, as opposed to just the minimal margin, plays a crucial role in understanding this phenomenon. Loosely speaking, some outliers should be tolerated if this has the benefit of substantially increasing the margin on the remaining points. We propose a new boosting algorithm which allows for the possibility of a pre-specified fraction of points to lie in the margin area Or even on the wrong side of the decision boundary.},
 author = {R\"{a}tsch, Gunnar and Sch\"{o}lkopf, Bernhard and Smola, Alex and M\"{u}ller, Klaus-Robert and Onoda, Takashi and Mika, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/28dc6b0e1b33769b4b94685e4f4d1e5c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/28dc6b0e1b33769b4b94685e4f4d1e5c-Metadata.json},
 openalex = {W2105310066},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/28dc6b0e1b33769b4b94685e4f4d1e5c-Paper.pdf},
 publisher = {MIT Press},
 title = {v-Arc: Ensemble Learning in the Presence of Outliers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/28dc6b0e1b33769b4b94685e4f4d1e5c-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_29957047,
 abstract = {We present a Monte Carlo algorithm for learning to act in partially observable Markov decision processes (POMDPs) with real-valued state and action spaces. Our approach uses importance sampling for representing beliefs, and Monte Carlo approximation for belief propagation. A reinforcement learning algorithm, value iteration, is employed to learn value functions over belief states. Finally, a sample-based version of nearest neighbor is used to generalize across states. Initial empirical results suggest that our approach works well in practical applications.},
 author = {Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/299570476c6f0309545110c592b6a63b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/299570476c6f0309545110c592b6a63b-Metadata.json},
 openalex = {W2113889826},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/299570476c6f0309545110c592b6a63b-Paper.pdf},
 publisher = {MIT Press},
 title = {Monte Carlo POMDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/299570476c6f0309545110c592b6a63b-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_2a27b814,
 abstract = {A very simple model of two reciprocally connected attractor neural networks is studied analytically in situations similar to those encountered in delay match-to-sample tasks with intervening stimuli and in tasks of memory guided attention. The model qualitatively reproduces many of the experimental data on these types of tasks and provides a framework for the understanding of the experimental observations in the context of the attractor neural network scenario.},
 author = {Renart, Alfonso and Parga, N\'{e}stor and Rolls, Edmund},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2a27b8144ac02f67687f76782a3b5d8f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2a27b8144ac02f67687f76782a3b5d8f-Metadata.json},
 openalex = {W2126515524},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2a27b8144ac02f67687f76782a3b5d8f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Recurrent Model of the Interaction Between Prefrontal and Inferotemporal Cortex in Delay Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/2a27b8144ac02f67687f76782a3b5d8f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_2cb6b103,
 abstract = {We examine a psychophysical law that describes the influence of stimulus and context on perception. According to this law choice probability ratios factorize into components independently controlled by stimulus and context. It has been argued that this pattern of results is incompatible with feedback models of perception. In this paper we examine this claim using neural network models defined via stochastic differential equations. We show that the law is related to a condition named channel separability and has little to do with the existence of feedback connections. In essence, channels are separable if they converge into the response units without direct lateral connections to other channels and if their sensors are not directly contaminated by external inputs to the other channels. Implications of the analysis for cognitive and computational neurosicence are discussed.},
 author = {Movellan, Javier and McClelland, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2cb6b10338a7fc4117a80da24b582060-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2cb6b10338a7fc4117a80da24b582060-Metadata.json},
 openalex = {W2162656889},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2cb6b10338a7fc4117a80da24b582060-Paper.pdf},
 publisher = {MIT Press},
 title = {Information Factorization in Connectionist Models of Perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/2cb6b10338a7fc4117a80da24b582060-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_365d1777,
 abstract = {We formulate a model for probability distributions on image spaces. We show that any distribution of images can be factored exactly into conditional distributions of feature vectors at one resolution (pyramid level) conditioned on the image information at lower resolutions. We would like to factor this over positions in the pyramid levels to make it tractable, but such factoring may miss long-range dependencies. To fix this, we introduce hidden class labels at each pixel in the pyramid. The result is a hierarchical mixture of conditional probabilities, similar to a hidden Markov model on a tree. The model parameters can be found with maximum likelihood estimation using the EM algorithm. We have obtained encouraging preliminary results on the problems of detecting various objects in SAR images and target recognition in optical aerial images.},
 author = {Spence, Clay and Parra, Lucas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/365d17770080c807a0e47ae9118d8641-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/365d17770080c807a0e47ae9118d8641-Metadata.json},
 openalex = {W2158704084},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/365d17770080c807a0e47ae9118d8641-Paper.pdf},
 publisher = {MIT Press},
 title = {Hierarchical Image Probability (H1P) Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/365d17770080c807a0e47ae9118d8641-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_36d75342,
 abstract = {Recently, a number of authors have proposed treating dialogue systems as Markov decision processes (MDPs). However, the practical application of MDP algorithms to dialogue systems faces a number of severe technical challenges. We have built a general software tool (RLDS, for Reinforcement Learning for Dialogue Systems) based on the MDP framework, and have applied it to dialogue corpora gathered from two dialogue systems built at AT&T Labs. Our experiments demonstrate that RLDS holds promise as a tool for browsing and understanding correlations in complex, temporally dependent dialogue corpora.},
 author = {Singh, Satinder and Kearns, Michael and Litman, Diane and Walker, Marilyn},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/36d7534290610d9b7e9abed244dd2f28-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/36d7534290610d9b7e9abed244dd2f28-Metadata.json},
 openalex = {W2163068732},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning for Spoken Dialogue Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/36d7534290610d9b7e9abed244dd2f28-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_375c7134,
 abstract = {We investigate the behavior of a Hebbian cell assembly of spiking neurons formed via a temporal synaptic learning curve. This learning function is based on recent experimental findings. It includes potentiation for short time delays between pre - and postsynaptic neuronal spiking, and depression for spiking events occuring in the reverse order. The coupling between the dynamics of the synaptic learning and of the neuronal activation leads to interesting results. We find that the cell assembly can fire asynchronously, but may also function in complete synchrony, or in distributed synchrony. The latter implies spontaneous division of the Hebbian cell assembly into groups of cells that fire in a cyclic manner. We invetigate the behavior of distributed synchrony both by simulations and by analytic calculations of the resulting synaptic distributions.},
 author = {Horn, David and Levy, Nir and Meilijson, Isaac and Ruppin, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/375c71349b295fbe2dcdca9206f20a06-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/375c71349b295fbe2dcdca9206f20a06-Metadata.json},
 openalex = {W2165985821},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf},
 publisher = {MIT Press},
 title = {Distributed Synchrony of Spiking Neurons in a Hebbian Cell Assembly},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/375c71349b295fbe2dcdca9206f20a06-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_393c55ae,
 abstract = {The Facial Action Coding System (FACS) (9) is an objective method for quantifying facial movement in terms of component actions. This system is widely used in behavioral investigations of emotion, cognitive processes, and social interaction. The coding is presently performed by highly trained human experts. This paper explores and compares techniques for automatically recognizing facial actions in sequences of images. These methods include unsupervised learning techniques for finding basis images such as principal component analysis, independent component analysis and local feature analysis, and supervised learning techniques such as Fisher's linear discriminants. These data-driven bases are compared to Gabor wavelets, in which the basis images are predefined. Best performances were obtained using the Gabor wavelet representation and the independent component representation, both of which achieved 96% accuracy for classifying 12 facial actions. The ICA representation employs 2 orders of magnitude fewer basis images than the Gabor representation and takes 90% less CPU time to compute for new images. The results provide converging support for using local basis images, high spatial frequencies, and statistical independence for classifying facial actions.},
 author = {Bartlett, Marian and Donato, Gianluca and Movellan, Javier and Hager, Joseph and Ekman, Paul and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/393c55aea738548df743a186d15f3bef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/393c55aea738548df743a186d15f3bef-Metadata.json},
 openalex = {W2100866252},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/393c55aea738548df743a186d15f3bef-Paper.pdf},
 publisher = {MIT Press},
 title = {Image Representations for Facial Expression Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/393c55aea738548df743a186d15f3bef-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_3c1e4bd6,
 abstract = {A latent variable generative model with finite noise is used to describe several different algorithms for Independent Components Analysis (ICA). In particular, the Fixed Point ICA algorithm is shown to be equivalent to the Expectation-Maximization algorithm for maximum likelihood under certain constraints, allowing the conditions for global convergence to be elucidated. The algorithms can also be explained by their generic behavior near a singular point where the size of the optimal generative bases vanishes. An expansion of the likelihood about this singular point indicates the role of higher order correlations in determining the features discovered by ICA. The application and convergence of these algorithms are demonstrated on a simple illustrative example.},
 author = {Lee, Daniel and Rokni, Uri and Sompolinsky, Haim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3c1e4bd67169b8153e0047536c9f541e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3c1e4bd67169b8153e0047536c9f541e-Metadata.json},
 openalex = {W2156969267},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3c1e4bd67169b8153e0047536c9f541e-Paper.pdf},
 publisher = {MIT Press},
 title = {Algorithms for Independent Components Analysis and Higher Order Statistics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/3c1e4bd67169b8153e0047536c9f541e-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_3e15cc11,
 abstract = {A novel learning approach for human face detection using a network of linear units is presented. The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features. A wide range of face images in different poses, with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces. Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and others. Furthermore, learning and evaluation using the SNoW-based method are significantly more efficient than with other methods.},
 author = {Yang, Ming-Hsuan and Roth, Dan and Ahuja, Narendra},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3e15cc11f979ed25912dff5b0669f2cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3e15cc11f979ed25912dff5b0669f2cd-Metadata.json},
 openalex = {W2138560582},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3e15cc11f979ed25912dff5b0669f2cd-Paper.pdf},
 publisher = {MIT Press},
 title = {A SNoW-Based Face Detector},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_3e7e0224,
 abstract = {I describe a silicon network consisting of a group of excitatory neurons and a global inhibitory neuron. The output of the inhibitory neuron is normalized with respect to the input strengths. This output models the normalization property of the wide-field direction-selective cells in the fly visual system. This normalizing property is also useful in any system where we wish the output signal to code only the strength of the inputs, and not be dependent on the number of inputs. The circuitry in each neuron is equivalent to that in Lazzaro's winner-take-all (WTA) circuit with one additional transistor and a voltage reference. Just as in Lazzaro's circuit, the outputs of the excitatory neurons code the neuron with the largest input. The difference here is that multiple winners can be chosen. By varying the voltage reference of the neuron, the network can transition between a soft-max behavior and a hard WTA behavior. I show results from a fabricated chip of 20 neurons in a 1.2 µm CMOS technology.},
 author = {Liu, Shih-Chii},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3e7e0224018ab3cf51abb96464d518cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3e7e0224018ab3cf51abb96464d518cd-Metadata.json},
 openalex = {W2116905890},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3e7e0224018ab3cf51abb96464d518cd-Paper.pdf},
 publisher = {MIT Press},
 title = {A Winner-Take-All Circuit with Controllable Soft Max Property},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/3e7e0224018ab3cf51abb96464d518cd-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_404dcc91,
 abstract = {We present a variational Bayesian method for model selection over families of kernels classifiers like Support Vector machines or Gaussian processes. The algorithm needs no user interaction and is able to adapt a large number of parameters to given data without having to sacrifice training cases for validation. This opens the possibility to use sophisticated families of kernels in situations where the small standard kernel classes are clearly inappropriate. We relate the method to other work done on Gaussian processes and clarify the relation between Support Vector machines and certain Gaussian process models.},
 author = {Seeger, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/404dcc91b2aeaa7caa47487d1483e48a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/404dcc91b2aeaa7caa47487d1483e48a-Metadata.json},
 openalex = {W2129869373},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Model Selection for Support Vector Machines, Gaussian Processes and Other Kernel Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/404dcc91b2aeaa7caa47487d1483e48a-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_418ef612,
 abstract = {Three contributions to developing an algorithm for assisting engineers in designing analog circuits are provided in this paper. First, a method for representing highly nonlinear and noncontinuous analog circuits using Kirchoff current law potential functions within the context of a Markov field is described. Second, a relatively efficient algorithm for optimizing the Markov field objective function is briefly described and the convergence proof is briefly sketched. And third, empirical results illustrating the strengths and limitations of the approach are provided within the context of a JFET transistor design problem. The proposed algorithm generated a set of circuit components for the JFET circuit model that accurately generated the desired characteristic curves.},
 author = {Golden, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/418ef6127e44214882c61e372e866691-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/418ef6127e44214882c61e372e866691-Metadata.json},
 openalex = {W2166145198},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/418ef6127e44214882c61e372e866691-Paper.pdf},
 publisher = {MIT Press},
 title = {Kirchoff Law Markov Fields for Analog Circuit Design},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/418ef6127e44214882c61e372e866691-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_442cde81,
 abstract = {When a visual image consists of a figure against a background, V1 cells are physiologically observed to give higher responses to image regions corresponding to the figure relative to their responses to the background. The medial axis of the figure also induces relatively higher responses compared to responses to other locations in the figure (except for the boundary between the figure and the background). Since the receptive fields of V1 cells are very small compared with the global scale of the figure-ground and medial axis effects, it has been suggested that these effects may be caused by feedback from higher visual areas. I show how these effects can be accounted for by V1 mechanisms when the size of the figure is small or is of a certain scale. They are a manifestation of the processes of pre-attentive segmentation which detect and highlight the boundaries between homogeneous image regions.},
 author = {Li, Zhaoping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/442cde81694ca09a626eeddefd1b74ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/442cde81694ca09a626eeddefd1b74ca-Metadata.json},
 openalex = {W2149545164},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/442cde81694ca09a626eeddefd1b74ca-Paper.pdf},
 publisher = {MIT Press},
 title = {Can VI Mechanisms Account for Figure-Ground and Medial Axis Effects?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/442cde81694ca09a626eeddefd1b74ca-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_464d828b,
 abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
 author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Metadata.json},
 openalex = {W2155027007},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},
 publisher = {MIT Press},
 title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_4921f95b,
 abstract = {We calculate lower bounds on the size of sigmoidal neural networks that approximate continuous functions. In particular, we show that for the approximation of polynomials the network size has to grow as Ω((logk)1/4) where k is the degree of the polynomials. This bound is valid for any input dimension, i.e. independently of the number of variables. The result is obtained by introducing a new method employing upper bounds on the Vapnik-Chervonenkis dimension for proving lower bounds on the size of networks that approximate continuous functions.},
 author = {Schmitt, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4921f95baf824205e1b13f22d60357a1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4921f95baf824205e1b13f22d60357a1-Metadata.json},
 openalex = {W1593405531},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4921f95baf824205e1b13f22d60357a1-Paper.pdf},
 publisher = {MIT Press},
 title = {Lower Bounds on the Complexity of Approximating Continuous Functions by Sigmoidal Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/4921f95baf824205e1b13f22d60357a1-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_4a2ddf14,
 abstract = {Recent theories suggest that language acquisition is assisted by the evolution of languages towards forms that are easily learnable. In this paper, we evolve combinatorial languages which can be learned by a recurrent neural network quickly and from relatively few examples. Additionally, we evolve languages for generalization in different worlds, and for generalization from specific examples. We find that languages can be evolved to facilitate different forms of impressive generalization for a minimally biased, general purpose learner. The results provide empirical support for the theory that the language itself, as well as the language environment of a learner, plays a substantial role in learning: that there is far more to language acquisition than the language acquisition device.},
 author = {Tonkes, Bradley and Blair, Alan and Wiles, Janet},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4a2ddf148c5a9c42151a529e8cbdcc06-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4a2ddf148c5a9c42151a529e8cbdcc06-Metadata.json},
 openalex = {W2160047837},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4a2ddf148c5a9c42151a529e8cbdcc06-Paper.pdf},
 publisher = {MIT Press},
 title = {Evolving Learnable Languages},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/4a2ddf148c5a9c42151a529e8cbdcc06-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_4abe17a1,
 abstract = {We present a new learning architecture: the Decision Directed Acyclic Graph (DDAG), which is used to combine many two-class classifiers into a multiclass classifier. For an N-class problem, the DDAG contains N(N - 1)/2 classifiers, one for each pair of classes. We present a VC analysis of the case when the node classifiers are hyperplanes; the resulting bound on the test error depends on N and on the margin achieved at the nodes, but not on the dimension of the space. This motivates an algorithm, DAGSVM, which operates in a kernel-induced feature space and uses two-class maximal margin hyperplanes at each decision-node of the DDAG. The DAGSVM is substantially faster to train and evaluate than either the standard algorithm or Max Wins, while maintaining comparable accuracy to both of these algorithms.},
 author = {Platt, John and Cristianini, Nello and Shawe-Taylor, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4abe17a1c80cbdd2aa241b70840879de-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4abe17a1c80cbdd2aa241b70840879de-Metadata.json},
 openalex = {W2157239837},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4abe17a1c80cbdd2aa241b70840879de-Paper.pdf},
 publisher = {MIT Press},
 title = {Large Margin DAGs for Multiclass Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/4abe17a1c80cbdd2aa241b70840879de-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_4f398cb9,
 abstract = {We consider the problem of reliably choosing a near-best strategy from a restricted class of strategies II in a partially observable Markov decision process (POMDP). We assume we are given the ability to simulate the POMDP, and study what might be called the sample complexity -- that is, the amount of data one must generate in the POMDP in order to choose a good strategy. We prove upper bounds on the sample complexity showing that, even for infinitely large and arbitrarily complex POMDPs, the amount of data needed can be finite, and depends only linearly on the complexity of the restricted strategy class II, and exponentially on the horizon time. This latter dependence can be eased in a variety of ways, including the application of gradient and local search algorithms. Our measure of complexity generalizes the classical supervised learning notion of VC dimension to the settings of reinforcement learning and planning.},
 author = {Kearns, Michael and Mansour, Yishay and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4f398cb9d6bc79ae567298335b51ba8a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4f398cb9d6bc79ae567298335b51ba8a-Metadata.json},
 openalex = {W2161521419},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4f398cb9d6bc79ae567298335b51ba8a-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximate Planning in Large POMDPs via Reusable Trajectories},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/4f398cb9d6bc79ae567298335b51ba8a-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_4fa53be9,
 abstract = {We present a general framework for discriminative estimation based on the maximum entropy principle and its extensions. All calculations involve distributions over structures and/or parameters rather than specific settings and reduce to relative entropy projections. This holds even when the data is not separable within the chosen parametric class, in the context of anomaly detection rather than classification, or when the labels in the training set are uncertain or incomplete. Support vector machines are naturally subsumed under this class and we provide several extensions. We are also able to estimate exactly and efficiently discriminative distributions over tree structures of class-conditional models within this framework. Preliminary experimental results are indicative of the potential in these techniques.},
 author = {Jaakkola, Tommi and Meila, Marina and Jebara, Tony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4fa53be91b4933d536748a60458b9797-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4fa53be91b4933d536748a60458b9797-Metadata.json},
 openalex = {W2161813919},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4fa53be91b4933d536748a60458b9797-Paper.pdf},
 publisher = {MIT Press},
 title = {Maximum Entropy Discrimination},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/4fa53be91b4933d536748a60458b9797-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_515ab26c,
 author = {Li, Yi and Long, Philip},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/515ab26c135e92ed8bf3a594d67e4ade-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/515ab26c135e92ed8bf3a594d67e4ade-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/515ab26c135e92ed8bf3a594d67e4ade-Paper.pdf},
 publisher = {MIT Press},
 title = {The Relaxed Online Maximum Margin Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/515ab26c135e92ed8bf3a594d67e4ade-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_52c51893,
 abstract = {We present a Hidden Markov Model (HMM) for inferring the hidden psychological state (or neural activity) during single trial fMRI activation experiments with blocked task paradigms. Inference is based on Bayesian methodology, using a combination of analytical and a variety of Markov Chain Monte Carlo (MCMC) sampling techniques. The advantage of this method is that detection of short time learning effects between repeated trials is possible since inference is based only on single trial experiments.},
 author = {H\o jen-S\o rensen, Pedro and Hansen, Lars and Rasmussen, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/52c5189391854c93e8a0e1326e56c14f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/52c5189391854c93e8a0e1326e56c14f-Metadata.json},
 openalex = {W2167040121},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/52c5189391854c93e8a0e1326e56c14f-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Modelling of fMRI lime Series},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/52c5189391854c93e8a0e1326e56c14f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_52d080a3,
 abstract = {Bayesian predictions are stochastic just like predictions of any other inference scheme that generalize from a finite sample. While a simple variational argument shows that Bayes averaging is generalization optimal given that the prior matches the teacher parameter distribution the situation is less clear if the teacher distribution is unknown. I define a class of averaging procedures, the temperated likelihoods, including both Bayes averaging with a uniform prior and maximum likelihood estimation as special cases. I show that Bayes is generalization optimal in this family for any teacher distribution for two learning problems that are analytically tractable: learning the mean of a Gaussian and asymptotics of smooth learners.},
 author = {Hansen, Lars},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/52d080a3e172c33fd6886a37e7288491-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/52d080a3e172c33fd6886a37e7288491-Metadata.json},
 openalex = {W2150388061},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/52d080a3e172c33fd6886a37e7288491-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Averaging is Well-Temperated},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/52d080a3e172c33fd6886a37e7288491-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_54e36c5f,
 abstract = {We propose a new approach to the problem of searching a space of stochastic controllers for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP). Following several other authors, our approach is based on searching in parameterized families of policies (for example, via gradient descent) to optimize solution quality. However, rather than trying to estimate the values and derivatives of a policy directly, we do so indirectly using estimates for the probability densities that the policy induces on states at the different points in time. This enables our algorithms to exploit the many techniques for efficient and robust approximate density propagation in stochastic systems. We show how our techniques can be applied both to deterministic propagation schemes (where the MDP's dynamics are given explicitly in compact form,) and to stochastic propagation schemes (where we have access only to a generative model, or simulator, of the MDP). We present empirical results for both of these variants on complex problems.},
 author = {Ng, Andrew and Parr, Ronald and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/54e36c5ff5f6a1802925ca009f3ebb68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/54e36c5ff5f6a1802925ca009f3ebb68-Metadata.json},
 openalex = {W2162401674},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/54e36c5ff5f6a1802925ca009f3ebb68-Paper.pdf},
 publisher = {MIT Press},
 title = {Policy Search via Density Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/54e36c5ff5f6a1802925ca009f3ebb68-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_54f5f407,
 abstract = {This paper examines the application of reinforcement learning to a wireless communication problem. The problem requires that channel utility be maximized while simultaneously minimizing battery usage. We present a solution to this multi-criteria problem that is able to significantly reduce power consumption. The solution uses a variable discount factor to capture the effects of battery usage.},
 author = {Brown, Timothy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/54f5f4071faca32ad5285fef87b78646-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/54f5f4071faca32ad5285fef87b78646-Metadata.json},
 openalex = {W2157758004},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/54f5f4071faca32ad5285fef87b78646-Paper.pdf},
 publisher = {MIT Press},
 title = {Low Power Wireless Communication via Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/54f5f4071faca32ad5285fef87b78646-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_5a142a55,
 abstract = {We describe a class of probabilistic models that we call credibility networks. Using parse trees as internal representations of images, credibility networks are able to perform segmentation and recognition simultaneously, removing the need for ad hoc segmentation heuristics. Promising results in the problem of segmenting handwritten digits were obtained.},
 author = {Hinton, Geoffrey E and Ghahramani, Zoubin and Teh, Yee Whye},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5a142a55461d5fef016acfb927fee0bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5a142a55461d5fef016acfb927fee0bd-Metadata.json},
 openalex = {W2125240890},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning to Parse Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/5a142a55461d5fef016acfb927fee0bd-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_5cf21ce3,
 abstract = {In many classification tasks, recognition accuracy is low because input patterns are corrupted by noise or are spatially or temporally overlapping. We propose an approach to overcoming these limitations based on a model of human selective attention. The model, an early selection filter guided by top-down attentional control, entertains each candidate output class in sequence and adjusts attentional gain coefficients in order to produce a strong response for that class. The chosen class is then the one that obtains the strongest response with the least modulation of attention. We present simulation results on classification of corrupted and superimposed handwritten digit patterns, showing a significant improvement in recognition rates. The algorithm has also been applied in the domain of speech recognition, with comparable results.},
 author = {Lee, Soo-Young and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5cf21ce30208cfffaa832c6e44bb567d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5cf21ce30208cfffaa832c6e44bb567d-Metadata.json},
 openalex = {W2120294624},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5cf21ce30208cfffaa832c6e44bb567d-Paper.pdf},
 publisher = {MIT Press},
 title = {Robust Recognition of Noisy and Superimposed Patterns via Selective Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_5d79099f,
 abstract = {In recent years, Bayesian networks have become highly successful tool for diagnosis, analysis, and decision making in real-world domains. We present an efficient algorithm for learning Bayes networks from data. Our approach constructs Bayesian networks by first identifying each node's Markov blankets, then connecting nodes in a maximally consistent way. In contrast to the majority of work, which typically uses hill-climbing approaches that may produce dense and causally incorrect nets, our approach yields much more compact causal networks by heeding independencies in the data. Compact causal networks facilitate fast inference and are also easier to understand. We prove that under mild assumptions, our approach requires time polynomial in the size of the data and the number of nodes. A randomized variant, also presented here, yields comparable results at much higher speeds.},
 author = {Margaritis, Dimitris and Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5d79099fcdf499f12b79770834c0164a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5d79099fcdf499f12b79770834c0164a-Metadata.json},
 openalex = {W2129564794},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5d79099fcdf499f12b79770834c0164a-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Network Induction via Local Neighborhoods},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/5d79099fcdf499f12b79770834c0164a-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_62889e73,
 abstract = {We first show how to represent sharp posterior probability distributions using real valued coefficients on broadly-tuned basis functions. Then we show how the precise times of spikes can be used to convey the real-valued coefficients on the basis functions quickly and accurately. Finally we describe a simple simulation in which spiking neurons learn to model an image sequence by fitting a dynamic generative model.},
 author = {Hinton, Geoffrey E and Brown, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/62889e73828c756c961c5a6d6c01a463-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/62889e73828c756c961c5a6d6c01a463-Metadata.json},
 openalex = {W2122225791},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/62889e73828c756c961c5a6d6c01a463-Paper.pdf},
 publisher = {MIT Press},
 title = {Spiking Boltzmann Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/62889e73828c756c961c5a6d6c01a463-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_6449f44a,
 abstract = {Many complex decision making problems like scheduling in manufacturing systems, portfolio management in finance, admission control in communication networks etc., with clear and precise objectives, can be formulated as stochastic dynamic programming problems in which the objective of decision making is to maximize a single “overall” reward. In these formulations, finding an optimal decision policy involves computing a certain “value function” which assigns to each state the optimal reward one would obtain if the system was started from that state. This function then naturally prescribes the optimal policy, which is to take decisions that drive the system to states with maximum value. 
For many practical problems, the computation of the exact value function is intractable, analytically and numerically, due to the enormous size of the state space. Therefore one has to resort to one of the following approximation methods to find a good sub-optimal policy: (1) Approximate the value function. (2) Restrict the search for a good policy to a smaller family of policies. 
In this thesis, we propose and study actor-critic algorithms which combine the above two approaches with simulation to find the best policy among a parameterized class of policies. Actor-critic algorithms have two learning units: an actor and a critic. An actor is a decision maker with a tunable parameter. A critic is a function approximator. The critic tries to approximate the value function of the policy used by the actor, and the actor in turn tries to improve its policy based on the current approximation provided by the critic. Furthermore, the critic evolves on a faster time-scale than the actor. 
We propose several variants of actor-critic algorithms. In all the variants, the critic uses Temporal Difference (TD) learning with linear function approximation. Some of the variants are inspired by a new geometric interpretation of the formula for the gradient of the overall reward with respect to the actor parameters. This interpretation suggests a natural set of basis functions for the critic, determined by the family of policies parameterized by the actor's parameters. We concentrate on the average expected reward criterion but we also show how the algorithms can be modified for other objective criteria. We prove convergence of the algorithms for problems with general (finite, countable, or continuous) state and decision spaces. 
To compute the rate of convergence (ROC) of our algorithms, we develop a general theory of the ROC of two-time-scale algorithms and we apply it to study our algorithms. In the process, we study the ROC of TD learning and compare it with related methods such as Least Squares TD (LSTD). We study the effect of the basis functions used for linear function approximation on the ROC of TD. We also show that the ROC of actor-critic algorithms does not depend on the actual basis functions used in the critic but depends only on the subspace spanned by them and study this dependence. 
Finally, we compare the performance of our algorithms with other algorithms that optimize over a parameterized family of policies. We show that when only the “natural” basis functions are used for the critic, the rate of convergence of the actor critic algorithms is the same as that of certain stochastic gradient descent algorithms. However, with appropriate additional basis functions for the critic, we show that our algorithms outperform the existing ones in terms of ROC. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)},
 author = {Konda, Vijay and Tsitsiklis, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Metadata.json},
 openalex = {W2156737235},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
 publisher = {MIT Press},
 title = {Actor-critic algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_647c722b,
 abstract = {In this paper, we consider the problem of active learning in trigonometric polynomial networks and give a necessary and sufficient condition of sample points to provide the optimal generalization capability. By analyzing the condition from the functional analytic point of view, we clarify the mechanism of achieving the optimal generalization capability. We also show that a set of training examples satisfying the condition does not only provide the optimal generalization but also reduces the computational complexity and memory required for the calculation of learning results. Finally, examples of sample points satisfying the condition are given and computer simulations are performed to demonstrate the effectiveness of the proposed active learning method.},
 author = {Sugiyama, Masashi and Ogawa, Hidemitsu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/647c722bf90a49140184672e0d3723e3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/647c722bf90a49140184672e0d3723e3-Metadata.json},
 openalex = {W2115633849},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/647c722bf90a49140184672e0d3723e3-Paper.pdf},
 publisher = {MIT Press},
 title = {Training Data Selection for Optimal Generalization in Trigonometric Polynomial Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/647c722bf90a49140184672e0d3723e3-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_64f1f27b,
 abstract = {We propose a new and efficient technique for incorporating contextual information into object classification. Most of the current techniques face the problem of exponential computation cost. In this paper, we propose a new general framework that incorporates partial context at a linear cost. This technique is applied to microscopic urinalysis image recognition, resulting in a significant improvement of recognition rate over the context free approach. This gain would have been impossible using conventional context incorporation techniques.},
 author = {Song, Xubo and Sill, Joseph and Abu-Mostafa, Yaser and Kasdan, Harvey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/64f1f27bf1b4ec22924fd0acb550c235-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/64f1f27bf1b4ec22924fd0acb550c235-Metadata.json},
 openalex = {W2116478415},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf},
 publisher = {MIT Press},
 title = {Image Recognition in Context: Application to Microscopic Urinalysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/64f1f27bf1b4ec22924fd0acb550c235-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_66be31e4,
 abstract = {We consider the problem of learning a grid-based map using a robot with noisy sensors and actuators. We compare two approaches: online EM, where the map is treated as a fixed parameter, and Bayesian inference, where the map is a (matrix-valued) random variable. We show that even on a very simple example, online EM can get stuck in local minima, which causes the robot to get lost and the resulting map to be useless. By contrast, the Bayesian approach, by maintaining multiple hypotheses, is much more robust. We then introduce a method for approximating the Bayesian solution, called Rao-Blackwellised particle filtering. We show that this approximation, when coupled with an active learning strategy, is fast but accurate.},
 author = {Murphy, Kevin P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/66be31e4c40d676991f2405aaecc6934-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/66be31e4c40d676991f2405aaecc6934-Metadata.json},
 openalex = {W2112793821},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/66be31e4c40d676991f2405aaecc6934-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Map Learning in Dynamic Environments},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/66be31e4c40d676991f2405aaecc6934-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_6709e8d6,
 abstract = {This paper describes bidirectional recurrent mixture density networks, which can model multi-modal distributions of the type P(xt|y1T) and P(xt|x1, x2,..., xt-1, y1T) without any explicit assumptions about the use of context. These expressions occur frequently in pattern recognition problems with sequential data, for example in speech recognition. Experiments show that the proposed generative models give a higher likelihood on test data compared to a traditional modeling approach, indicating that they can summarize the statistical properties of the data better.},
 author = {Schuster, Mike},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6709e8d64a5f47269ed5cea9f625f7ab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6709e8d64a5f47269ed5cea9f625f7ab-Metadata.json},
 openalex = {W2112656927},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6709e8d64a5f47269ed5cea9f625f7ab-Paper.pdf},
 publisher = {MIT Press},
 title = {Better Generative Models for Sequential Data Problems: Bidirectional Recurrent Mixture Density Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/6709e8d64a5f47269ed5cea9f625f7ab-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_673271cc,
 abstract = {An important issue in neural computing concerns the description of learning dynamics with macroscopic dynamical variables. Recent progress on on-line learning only addresses the often unrealistic case of an infinite training set. We introduce a new framework to model batch learning of restricted sets of examples, widely applicable to any learning cost function, and fully taking into account the temporal correlations introduced by the recycling of the examples. For illustration we analyze the effects of weight decay and early stopping during the learning of teacher-generated examples.},
 author = {Li, Song and Wong, K. Y. Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/673271cc47c1a4e77f57e239ed4d28a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/673271cc47c1a4e77f57e239ed4d28a7-Metadata.json},
 openalex = {W2141064830},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/673271cc47c1a4e77f57e239ed4d28a7-Paper.pdf},
 publisher = {MIT Press},
 title = {Statistical Dynamics of Batch Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/673271cc47c1a4e77f57e239ed4d28a7-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_6a5dfac4,
 abstract = {The statistics of photographic images, when represented using multiscale (wavelet) bases, exhibit two striking types of non-Gaussian behavior. First, the marginal densities of the coefficients have extended heavy tails. Second, the joint densities exhibit variance dependencies not captured by second-order models. We examine properties of the class of Gaussian scale mixtures, and show that these densities can accurately characterize both the marginal and joint distributions of natural image wavelet coefficients. This class of model suggests a Markov structure, in which wavelet coefficients are linked by hidden scaling variables corresponding to local image structure. We derive an estimator for these hidden variables, and show that a nonlinear normalization procedure can be used to Gaussianize the coefficients.},
 author = {Wainwright, Martin J and Simoncelli, Eero},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6a5dfac4be1502501489fc0f5a24b667-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6a5dfac4be1502501489fc0f5a24b667-Metadata.json},
 openalex = {W2119938170},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6a5dfac4be1502501489fc0f5a24b667-Paper.pdf},
 publisher = {MIT Press},
 title = {Scale Mixtures of Gaussians and the Statistics of Natural Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/6a5dfac4be1502501489fc0f5a24b667-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_6a81681a,
 abstract = {We present a new technique for time series analysis based on dynamic probabilistic networks. In this approach, the observed data are modeled in terms of unobserved, mutually independent factors, as in the recently introduced technique of Independent Factor Analysis (IFA). However, unlike in IFA, the factors are not i.i.d.; each factor has its own temporal statistical characteristics. We derive a family of EM algorithms that learn the structure of the underlying factors and their relation to the data. These algorithms perform source separation and noise reduction in an integrated manner, and demonstrate superior performance compared to IFA.},
 author = {Attias, Hagai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6a81681a7af700c6385d36577ebec359-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6a81681a7af700c6385d36577ebec359-Metadata.json},
 openalex = {W2110763772},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6a81681a7af700c6385d36577ebec359-Paper.pdf},
 publisher = {MIT Press},
 title = {Independent Factor Analysis with Temporally Structured Sources},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/6a81681a7af700c6385d36577ebec359-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_6e62a992,
 abstract = {We develop a hierarchical generative model to study cue combination. The model maps a global shape parameter to local cue-specific parameters, which in turn generate an intensity image. Inferring shape from images is achieved by inverting this model. Inference produces a probability distribution at each level; using distributions rather than a single value of underlying variables at each stage preserves information about the validity of each local cue for the given image. This allows the model, unlike standard combination models, to adaptively weight each cue based on general cue reliability and specific image context. We describe the results of a cue combination psychophysics experiment we conducted that allows a direct comparison with the model. The model provides a good fit to our data and a natural account for some interesting aspects of cue combination.},
 author = {Yang, Zhiyong and Zemel, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6e62a992c676f611616097dbea8ea030-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6e62a992c676f611616097dbea8ea030-Metadata.json},
 openalex = {W2114349498},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6e62a992c676f611616097dbea8ea030-Paper.pdf},
 publisher = {MIT Press},
 title = {Managing Uncertainty in Cue Combination},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/6e62a992c676f611616097dbea8ea030-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_70ece1e1,
 abstract = {Recent interpretations of the Adaboost algorithm view it as performing a gradient descent on a potential function. Simply changing the potential function allows one to create new algorithms related to AdaBoost. However, these new algorithms are generally not known to have the formal boosting property. This paper examines the question of which potential functions lead to new algorithms that are boosters. The two main results are general sets of conditions on the potential; one set implies that the resulting algorithm is a booster, while the other implies that the algorithm is not. These conditions are applied to previously studied potential functions, such as those used by LogitBoost and Doom II.},
 author = {Duffy, Nigel and Helmbold, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/70ece1e1e0931919438fcfc6bd5f199c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/70ece1e1e0931919438fcfc6bd5f199c-Metadata.json},
 openalex = {W2294105907},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/70ece1e1e0931919438fcfc6bd5f199c-Paper.pdf},
 publisher = {MIT Press},
 title = {Potential Boosters},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/70ece1e1e0931919438fcfc6bd5f199c-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_712a3c98,
 abstract = {We study here a simple stochastic single neuron model with delayed self-feedback capable of generating spike trains. Simulations show that its spike trains exhibit resonant behavior between and delay. In order to gain insight into this resonance, we simplify the model and study a stochastic binary element whose transition probability depends on its state at a fixed interval in the past. With this simplified model we can analytically compute interspike interval histograms, and show how the resonance between noise and delay arises. The resonance is also observed when such elements are coupled through delayed interaction.},
 author = {Ohira, Toru and Sato, Yuzuru and Cowan, Jack},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/712a3c9878efeae8ff06d57432016ceb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/712a3c9878efeae8ff06d57432016ceb-Metadata.json},
 openalex = {W2167735111},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/712a3c9878efeae8ff06d57432016ceb-Paper.pdf},
 publisher = {MIT Press},
 title = {Resonance in a Stochastic Neuron Model with Delayed Interaction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/712a3c9878efeae8ff06d57432016ceb-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_7137debd,
 abstract = {The complexity of cortical circuits may be characterized by the number of synapses per neuron. We study the dependence of complexity on the fraction of the cortical volume that is made up of (that is, of axons and dendrites), and find that complexity is maximized when wire takes up about 60% of the cortical volume. This prediction is in good agreement with experimental observations. A consequence of our arguments is that any rearrangement of neurons that takes more wire would sacrifice computational power.},
 author = {Chklovskii, Dmitri and Stevens, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7137debd45ae4d0ab9aa953017286b20-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7137debd45ae4d0ab9aa953017286b20-Metadata.json},
 openalex = {W2166141945},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf},
 publisher = {MIT Press},
 title = {Wiring Optimization in the Brain},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_7283518d,
 abstract = {We formulate the problem of retrieving images from visual databases as a problem of Bayesian inference. This leads to natural and effective solutions for two of the most challenging issues in the design of a retrieval system: providing support for region-based queries without requiring prior image segmentation, and accounting for user-feedback during a retrieval session. We present a new learning algorithm that relies on belief propagation to account for both positive and negative examples of the user's interests.},
 author = {Vasconcelos, Nuno and Lippman, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7283518d47a05a09d33779a17adf1707-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7283518d47a05a09d33779a17adf1707-Metadata.json},
 openalex = {W2097005463},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7283518d47a05a09d33779a17adf1707-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning from User Feedback in Image Retrieval Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/7283518d47a05a09d33779a17adf1707-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_7437d136,
 abstract = {Stochastic meta-descent (SMD) is a new technique for online adaptation of local learning rates in arbitrary twice-differentiable systems. Like matrix momentum it uses full second-order information while retaining O(n) computational complexity by exploiting the efficient computation of Hessian-vector products. Here we apply SMD to independent component analysis, and employ the resulting algorithm for the blind separation of time-varying mixtures. By matching individual learning rates to the rate of change in each source signal's mixture coefficients, our technique is capable of simultaneously tracking sources that move at very different, a priori unknown speeds.},
 author = {Schraudolph, Nicol and Giannakopoulos, Xavier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7437d136770f5b35194cb46c1653efaa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7437d136770f5b35194cb46c1653efaa-Metadata.json},
 openalex = {W2120957588},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7437d136770f5b35194cb46c1653efaa-Paper.pdf},
 publisher = {MIT Press},
 title = {Online Independent Component Analysis with Local Learning Rate Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/7437d136770f5b35194cb46c1653efaa-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_74563ba2,
 abstract = {This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non-Gaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation.},
 author = {Attias, Hagai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/74563ba21a90da13dacf2a73e3ddefa7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/74563ba21a90da13dacf2a73e3ddefa7-Metadata.json},
 openalex = {W2172085063},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf},
 publisher = {MIT Press},
 title = {A Variational Baysian Framework for Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/74563ba21a90da13dacf2a73e3ddefa7-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_752d25a1,
 abstract = {Hierarchical learning machines are non-regular and nonidentifiable statistical models, whose true parameter sets are analytic sets with singularities. Using algebraic analysis, we rigorously prove that the stochastic complexity of a non-identifiable learning machine is asymptotically equal to λ1 log n - (m1 - 1) log log n + const., where n is the number of training samples. Moreover we show that the rational number λ1 and the integer m1 can be algorithmically calculated using resolution of singularities in algebraic geometry. Also we obtain inequalities 0 < λ1 ≤ d2/ and 1 ≤ m1 ≤ d, where d is the number of parameters.},
 author = {Watanabe, Sumio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/752d25a1f8dbfb2d656bac3094bfb81c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/752d25a1f8dbfb2d656bac3094bfb81c-Metadata.json},
 openalex = {W2129429921},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/752d25a1f8dbfb2d656bac3094bfb81c-Paper.pdf},
 publisher = {MIT Press},
 title = {Algebraic Analysis for Non-regular Learning Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/752d25a1f8dbfb2d656bac3094bfb81c-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_757f843a,
 abstract = {Unsupervised learning algorithms are designed to extract structure from data samples. Reliable and robust inference requires a guarantee that extracted structures are typical for the data source, i.e., similar structures have to be inferred from a second sample set of the same data source. The overfitting phenomenon in maximum entropy based annealing algorithms is exemplarily studied for a class of histogram clustering models. Bernstein's inequality for large deviations is used to determine the maximally achievable approximation quality parameterized by a minimal temperature. Monte Carlo simulations support the proposed model selection criterion by finite temperature annealing.},
 author = {Buhmann, Joachim and Held, Marcus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/757f843a169cc678064d9530d12a1881-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/757f843a169cc678064d9530d12a1881-Metadata.json},
 openalex = {W2121557504},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/757f843a169cc678064d9530d12a1881-Paper.pdf},
 publisher = {MIT Press},
 title = {Model Selection in Clustering by Uniform Convergence Bounds},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/757f843a169cc678064d9530d12a1881-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_798ed7d4,
 abstract = {In hyperspectral imagery one pixel typically consists of a mixture of the reflectance spectra of several materials, where the mixture coefficients correspond to the abundances of the constituting materials. We assume linear combinations of reflectance spectra with some additive normal sensor noise and derive a probabilistic MAP framework for analyzing hyperspectral data. As the material reflectance characteristics are not know a priori, we face the problem of unsupervised linear unmixing. The incorporation of different prior information (e.g. positivity and normalization of the abundances) naturally leads to a family of interesting algorithms, for example in the noise-free case yielding an algorithm that can be understood as constrained independent component analysis (ICA). Simulations underline the usefulness of our theory.},
 author = {Parra, Lucas and Spence, Clay and Sajda, Paul and Ziehe, Andreas and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/798ed7d4ee7138d49b8828958048130a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/798ed7d4ee7138d49b8828958048130a-Metadata.json},
 openalex = {W2120248066},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/798ed7d4ee7138d49b8828958048130a-Paper.pdf},
 publisher = {MIT Press},
 title = {Unmixing Hyperspectral Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/798ed7d4ee7138d49b8828958048130a-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_7c4ede33,
 abstract = {Recently, sample complexity bounds have been derived for problems involving linear functions such as neural networks and support vector machines. In this paper, we extend some theoretical results in this area by deriving dimensional independent covering number bounds for regularized linear functions under certain regularization conditions. We show that such bounds lead to a class of new methods for training linear classifiers with similar theoretical advantages of the support vector machine. Furthermore, we also present a theoretical analysis for these new methods from the asymptotic statistical point of view. This technique provides better description for large sample behaviors of these algorithms.},
 author = {Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7c4ede33a62160a19586f6e26eaefacf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7c4ede33a62160a19586f6e26eaefacf-Metadata.json},
 openalex = {W2107685997},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7c4ede33a62160a19586f6e26eaefacf-Paper.pdf},
 publisher = {MIT Press},
 title = {Some Theoretical Results Concerning the Convergence of Compositions of Regularized Linear Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/7c4ede33a62160a19586f6e26eaefacf-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_7d12b66d,
 abstract = {We perform a theoretical investigation of the variance of the cross-validation estimate of the generalization error that takes into account the variability due to the choice of training sets and test examples. This allows us to propose two new estimators of this variance. We show, via simulations, that these new statistics perform well relative to the statistics considered in Dietterich (1998). In particular, tests of hypothesis based on these don't tend to be too liberal like other tests currently available, and have good power. Nous considerons l'estimation par validation croisee de l'erreur de generalisation. Nous effectuons une etude theorique de la variance de ect estimateur en tenant compte de la variabilite due au choix des ensembles d'entrainement et des exemples de test. Cela nous permet de proposer deux nouveaux estimateurs de cette variance. Nous montrons, via des simulations, que ces nouvelles statistiques performent bien par rapport aux statistiques considerees dans Dietterich (1998). En particulier, ces nouvelles statistiques se demarquent des autres presentement utilisees par le fait qu'elles menent a des tests d'hypotheses qui sont puissants sans avoir tendance a etre trop liberaux.},
 author = {Nadeau, Claude and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7d12b66d3df6af8d429c1a357d8b9e1a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7d12b66d3df6af8d429c1a357d8b9e1a-Metadata.json},
 openalex = {W3123294050},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7d12b66d3df6af8d429c1a357d8b9e1a-Paper.pdf},
 publisher = {MIT Press},
 title = {Inference for the Generalization Error},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/7d12b66d3df6af8d429c1a357d8b9e1a-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_7dd0240c,
 abstract = {Long-term potentiation (LTP) has long been held as a biological substrate for associative learning. Recently, evidence has emerged that long-term depression (LTD) results when the presynaptic cell fires after the postsynaptic cell. The computational utility of LTD is explored here. Synaptic modification kernels for both LTP and LTD have been proposed by other laboratories based studies of one postsynaptic unit. Here, the interaction between time-dependent LTP and LTD is studied in small networks.},
 author = {Munro, Paul and Hern\'{a}ndez, Gerardina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7dd0240cd412efde8bc165e864d3644f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7dd0240cd412efde8bc165e864d3644f-Metadata.json},
 openalex = {W2162890749},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7dd0240cd412efde8bc165e864d3644f-Paper.pdf},
 publisher = {MIT Press},
 title = {LTD Facilitates Learning in a Noisy Environment},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/7dd0240cd412efde8bc165e864d3644f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_7e230522,
 abstract = {A system emulating the functionality of a moving eye-hence the name oculo-motor system-has been built and successfully tested. It is made of an optical device for shifting the field of view of an image sensor by up to 45° in any direction, four neuromorphic analog VLSI circuits implementing an oculo-motor control loop, and some off-the-shelf electronics. The custom integrated circuits communicate with each other primarily by non-arbitrated address-event buses. The system implements the behaviors of saliency-based saccadic exploration, and smooth pursuit of light spots. The duration of saccades ranges from 45 ms to 100 ms, which is comparable to human eye performance. Smooth pursuit operates on light sources moving at up to 50°/s in the visual field.},
 author = {Landolt, Oliver and Gyger, Steve},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7e230522657ecdc50e4249581b861f8e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7e230522657ecdc50e4249581b861f8e-Metadata.json},
 openalex = {W2098081011},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7e230522657ecdc50e4249581b861f8e-Paper.pdf},
 publisher = {MIT Press},
 title = {An Oculo-Motor System with Multi-Chip Neuromorphic Analog VLSI Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/7e230522657ecdc50e4249581b861f8e-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_7eb7eabb,
 abstract = {In this article we study the effects of introducing structure in the input distribution of the data to be learnt by a simple perceptron. We determine the learning curves within the framework of Statistical Mechanics. Stepwise generalization occurs as a function of the number of examples when the distribution of patterns is highly anisotropic. Although extremely simple, the model seems to capture the relevant features of a class of Support Vector Machines which was recently shown to present this behavior.},
 author = {Risau-Gusman, Sebastian and Gordon, Mirta},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7eb7eabbe9bd03c2fc99881d04da9cbd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7eb7eabbe9bd03c2fc99881d04da9cbd-Metadata.json},
 openalex = {W2097244193},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7eb7eabbe9bd03c2fc99881d04da9cbd-Paper.pdf},
 publisher = {MIT Press},
 title = {Understanding Stepwise Generalization of Support Vector Machines: a Toy Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/7eb7eabbe9bd03c2fc99881d04da9cbd-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_7fea637f,
 author = {Crisp, David and Burges, Christopher J. C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7fea637fd6d02b8f0adf6f7dc36aed93-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7fea637fd6d02b8f0adf6f7dc36aed93-Metadata.json},
 openalex = {W2156736119},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf},
 publisher = {MIT Press},
 title = {A Geometric Interpretation of v-SVM Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/7fea637fd6d02b8f0adf6f7dc36aed93-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_81c650ca,
 abstract = {A fundamental problem with the modeling of chaotic time series data is that minimizing short-term prediction errors does not guarantee a match between the reconstructed attractors of model and experiments. We introduce a modeling paradigm that simultaneously learns to short-term predict and to locate the outlines of the attractor by a new way of nonlinear principal component analysis. Closed-loop predictions are constrained to stay within these outlines, to prevent divergence from the attractor. Learning is exceptionally fast: parameter estimation for the 1000 sample laser data from the 1991 Santa Fe time series competition took less than a minute on a 166 MHz Pentium PC.},
 author = {Bakker, Rembrandt and Schouten, Jaap and Coppens, Marc-Olivier and Takens, Floris and Giles, C. and van den Bleek, Cor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/81c650caac28cdefce4de5ddc18befa0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/81c650caac28cdefce4de5ddc18befa0-Metadata.json},
 openalex = {W2150637077},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf},
 publisher = {MIT Press},
 title = {Robust Learning of Chaotic Attractors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/81c650caac28cdefce4de5ddc18befa0-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_8303a79b,
 abstract = {Greedy importance sampling is an unbiased estimation technique that reduces the variance of standard importance sampling by explicitly searching for modes in the estimation objective. Previous work has demonstrated the feasibility of implementing this method and proved that the technique is unbiased in both discrete and continuous domains. In this paper we present a reformulation of greedy importance sampling that eliminates the free parameters from the original estimator, and introduces a new regularization strategy that further reduces variance without compromising unbiasedness. The resulting estimator is shown to be effective for difficult estimation problems arising in Markov random field inference. In particular, improvements are achieved over standard MCMC estimators when the distribution has multiple peaked modes.},
 author = {Schuurmans, Dale},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8303a79b1e19a194f1875981be5bdb6f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8303a79b1e19a194f1875981be5bdb6f-Metadata.json},
 openalex = {W2164968235},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8303a79b1e19a194f1875981be5bdb6f-Paper.pdf},
 publisher = {MIT Press},
 title = {Regularized Greedy Importance Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/8303a79b1e19a194f1875981be5bdb6f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_831c2f88,
 abstract = {We investigate the short term dynamics of the recurrent competition and neural activity in the primary visual cortex in terms of information processing and in the context of orientation selectivity. We propose that after stimulus onset, the strength of the recurrent excitation decreases due to fast synaptic depression. As a consequence, the network shifts from an initially highly nonlinear to a more linear operating regime. Sharp orientation tuning is established in the first highly competitive phase. In the second and less competitive phase, precise signaling of multiple orientations and long range modulation, e.g., by intra - and inter-areal connections becomes possible (surround effects). Thus the network first extracts the salient features from the stimulus, and then starts to process the details. We show that this signal processing strategy is optimal if the neurons have limited bandwidth and their objective is to transmit the maximum amount of information in any time interval beginning with the stimulus onset.},
 author = {Adorj\'{a}n, P\'{e}ter and Schwabe, Lars and Piepenbrock, Christian and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/831c2f88a604a07ca94314b56a4921b8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/831c2f88a604a07ca94314b56a4921b8-Metadata.json},
 openalex = {W2109335998},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/831c2f88a604a07ca94314b56a4921b8-Paper.pdf},
 publisher = {MIT Press},
 title = {Recurrent Cortical Competition: Strengthen or Weaken?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/831c2f88a604a07ca94314b56a4921b8-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_84c6494d,
 abstract = {By thinking of each state in a hidden Markov model as corresponding to some spatial region of a fictitious topology space it is possible to naturally define neighbouring states as those which are connected in that space. The transition matrix can then be constrained to allow transitions only between neighbours; this means that all valid state sequences correspond to connected paths in the topology space. I show how such constrained HMMs can learn to discover underlying structure in complex sequences of high dimensional data, and apply them to the problem of recovering mouth movements from acoustics in continuous speech.},
 author = {Roweis, Sam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/84c6494d30851c63a55cdb8cb047fadd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/84c6494d30851c63a55cdb8cb047fadd-Metadata.json},
 openalex = {W2134195709},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/84c6494d30851c63a55cdb8cb047fadd-Paper.pdf},
 publisher = {MIT Press},
 title = {Constrained Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/84c6494d30851c63a55cdb8cb047fadd-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_84f0f204,
 abstract = {We present a class of approximate inference algorithms for graphical models of the QMR-DT type. We give convergence rates for these algorithms and for the Jaakkola and Jordan (1999) algorithm, and verify these theoretical predictions empirically. We also present empirical results on the difficult QMR-DT network problem, obtaining performance of the new algorithms roughly comparable to the Jaakkola and Jordan algorithm.},
 author = {Ng, Andrew and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/84f0f20482cde7e5eacaf7364a643d33-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/84f0f20482cde7e5eacaf7364a643d33-Metadata.json},
 openalex = {W2163525478},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/84f0f20482cde7e5eacaf7364a643d33-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximate Inference A lgorithms for Two-Layer Bayesian Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/84f0f20482cde7e5eacaf7364a643d33-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_851300ee,
 abstract = {This paper presents a computational model for perceptual organization. A figure-ground segregation network is proposed based on a novel boundary pair representation. The system solves the figure-ground segregation problem through temporal evolution. Gestalt-like grouping rules are incorporated by modulating connections, which determines the temporal behavior and thus the perception of the system. The results are then fed to a surface completion module based on local diffusion. Different perceptual phenomena, such as modal and a modal completion, virtual contours, grouping and shape decomposition are explained by the model with a fixed set of parameters. Computationally, the system eliminates combinatorial optimization, which is common to many existing computational approaches. It also accounts for more examples that are consistent with psychological experiments. In addition, the boundary-pair representation is consistent with well-known on- and off-center cell responses and thus biologically more plausible.},
 author = {Liu, Xiuwen and Wang, DeLiang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/851300ee84c2b80ed40f51ed26d866fc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/851300ee84c2b80ed40f51ed26d866fc-Metadata.json},
 openalex = {W1833224072},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/851300ee84c2b80ed40f51ed26d866fc-Paper.pdf},
 publisher = {MIT Press},
 title = {Perceptual organization based on temporal dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/851300ee84c2b80ed40f51ed26d866fc-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_8698ff92,
 abstract = {We discuss an information theoretic approach for categorizing and modeling dynamic processes. The approach can learn a compact and informative statistic which summarizes past states to predict future observations. Furthermore, the uncertainty of the prediction is characterized nonparametrically by a joint density over the learned statistic and present observation. We discuss the application of the technique to both noise driven dynamical systems and random processes sampled from a density which is conditioned on the past. In the first case we show results in which both the dynamics of random walk and the statistics of the driving noise are captured. In the second case we present results in which a summarizing statistic is learned on noisy random telegraph waves with differing dependencies on past states. In both cases the algorithm yields a principled approach for discriminating processes with differing dynamics and/or dependencies. The method is grounded in ideas from information theory and nonparametric statistics.},
 author = {Fisher III, John W and Ihler, Alexander and Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8698ff92115213ab187d31d4ee5da8ea-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8698ff92115213ab187d31d4ee5da8ea-Metadata.json},
 openalex = {W2110413319},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8698ff92115213ab187d31d4ee5da8ea-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Informative Statistics: A Nonparametnic Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/8698ff92115213ab187d31d4ee5da8ea-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_86d7c8a0,
 abstract = {This paper argues that two apparently distinct modes of generalizing concepts - abstracting rules and computing similarity to exemplars - should both be seen as special cases of a more general Bayesian learning framework. Bayes explains the specific workings of these two modes - which rules are abstracted, how similarity is measured - as well as why generalization should appear rule - or similarity-based in different situations. This analysis also suggests why the rules/similarity distinction, even if not computationally fundamental, may still be useful at the algorithmic level as part of a principled approximation to fully Bayesian learning.},
 author = {Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/86d7c8a08b4aaa1bc7c599473f5dddda-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/86d7c8a08b4aaa1bc7c599473f5dddda-Metadata.json},
 openalex = {W2119084537},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/86d7c8a08b4aaa1bc7c599473f5dddda-Paper.pdf},
 publisher = {MIT Press},
 title = {Rules and Similarity in Concept Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/86d7c8a08b4aaa1bc7c599473f5dddda-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_8725fb77,
 abstract = {Suppose you are given some dataset drawn from an underlying probability distribution P and you want to estimate a simple subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified ν between 0 and 1.

We propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm.

The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.},
 author = {Sch\"{o}lkopf, Bernhard and Williamson, Robert C and Smola, Alex and Shawe-Taylor, John and Platt, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Metadata.json},
 openalex = {W2105497548},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Paper.pdf},
 publisher = {MIT Press},
 title = {Support Vector Method for Novelty Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_89f03f7d,
 abstract = {We describe a Bayesian approach to model selection in unsupervised learning that determines both the feature set and the number of clusters. We then evaluate this scheme (based on marginal likelihood) and one based on cross-validated likelihood. For the Bayesian scheme we derive a closed-form solution of the marginal likelihood by assuming appropriate forms of the likelihood function and prior. Extensive experiments compare these approaches and all results are verified by comparison against ground truth. In these experiments the Bayesian scheme using our objective function gave better results than cross-validation.},
 author = {Vaithyanathan, Shivakumar and Dom, Byron},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/89f03f7d02720160f1b04cf5b27f5ccb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/89f03f7d02720160f1b04cf5b27f5ccb-Metadata.json},
 openalex = {W2099005519},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/89f03f7d02720160f1b04cf5b27f5ccb-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalized Model Selection for Unsupervised Learning in High Dimensions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/89f03f7d02720160f1b04cf5b27f5ccb-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_8b570001,
 abstract = {A new decomposition algorithm for training regression Support Vector Machines (SVM) is presented. The algorithm builds on the basic principles of decomposition proposed by Osuna et. al., and addresses the issue of optimal working set selection. The new criteria for testing optimality of a working set are derived. Based on these criteria, the principle of maximal inconsistency is proposed to form (approximately) optimal working sets. Experimental results show superior performance of the new algorithm in comparison with traditional training of regression SVM without decomposition. Similar results have been previously reported on decomposition algorithms for pattern recognition SVM. The new algorithm is also applicable to advanced SVM formulations based on regression, such as density estimation and integral equation SVM.},
 author = {Laskov, Pavel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8b5700012be65c9da25f49408d959ca0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8b5700012be65c9da25f49408d959ca0-Metadata.json},
 openalex = {W2167280730},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8b5700012be65c9da25f49408d959ca0-Paper.pdf},
 publisher = {MIT Press},
 title = {An Improved Decomposition Algorithm for Regression Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/8b5700012be65c9da25f49408d959ca0-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_8b6a80c3,
 abstract = {This paper presents an electronic system that extracts the periodicity of a sound. It uses three analogue VLSI building blocks: a silicon cochlea, two inner-hair-cell circuits and two spiking neuron chips. The silicon cochlea consists of a cascade of filters. Because of the delay between two outputs from the silicon cochlea, spike trains created at these outputs are synchronous only for a narrow range of periodicities. In contrast to traditional bandpass filters, where an increase in selectivity has to be traded off against a decrease in response time, the proposed system responds quickly, independent of selectivity.},
 author = {van Schaik, Andr\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8b6a80c3cf2cbd5f967063618dc54f39-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8b6a80c3cf2cbd5f967063618dc54f39-Metadata.json},
 openalex = {W2138872356},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8b6a80c3cf2cbd5f967063618dc54f39-Paper.pdf},
 publisher = {MIT Press},
 title = {An Analog VLSI Model of Periodicity Extraction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/8b6a80c3cf2cbd5f967063618dc54f39-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_8bb88f80,
 abstract = {We provide preliminary evidence that existing algorithms for inferring small-scale gene regulation networks from gene expression data can be adapted to large-scale gene expression data coming from hybridization microarrays. The essential steps are (1) clustering many genes by their expression time-course data into a minimal set of clusters of co-expressed genes, (2) theoretically modeling the various conditions under which the time-courses are measured using a continious-time analog recurrent neural network for the cluster mean time-courses, (3) fitting such a regulatory model to the cluster mean time courses by simulated annealing with weight decay, and (4) analysing several such fits for commonalities in the circuit parameter sets including the connection matrices. This procedure can be used to assess the adequacy of existing and future gene expression time-course data sets for determining transcriptional regulatory relationships such as coregulation.},
 author = {Mjolsness, Eric and Mann, Tobias and Casta\~{n}o, Rebecca and Wold, Barbara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8bb88f80d334b1869781beb89f7b73be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8bb88f80d334b1869781beb89f7b73be-Metadata.json},
 openalex = {W2125393150},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8bb88f80d334b1869781beb89f7b73be-Paper.pdf},
 publisher = {MIT Press},
 title = {From Coexpression to Coregulation: An Approach to Inferring Transcriptional Regulation among Gene Classes from Large-Scale Expression Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/8bb88f80d334b1869781beb89f7b73be-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_8c01a759,
 abstract = {Data visualization and feature selection methods are proposed based on the joint mutual information and ICA. The visualization methods can find many good 2-D projections for high dimensional data interpretation, which cannot be easily found by the other existing methods. The new variable selection method is found to be better in eliminating redundancy in the inputs than other methods based on simple mutual information. The efficacy of the methods is illustrated on a radar signal analysis problem to find 2-D viewing coordinates for data visualization and to select inputs for a neural network classifier.},
 author = {Yang, Howard and Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8c01a75941549a705cf7275e41b21f0d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8c01a75941549a705cf7275e41b21f0d-Metadata.json},
 openalex = {W2155344811},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8c01a75941549a705cf7275e41b21f0d-Paper.pdf},
 publisher = {MIT Press},
 title = {Data Visualization and Feature Selection: New Algorithms for Nongaussian Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/8c01a75941549a705cf7275e41b21f0d-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_8d420fa3,
 abstract = {In this paper, we propose that information maximization can provide a unified framework for understanding saccadic eye movements. In this framework, the mutual information among the cortical representations of the retinal image, the priors constructed from our long term visual experience, and a dynamic short-term internal representation constructed from recent saccades provides a map for guiding eye navigation. By directing the eyes to locations of maximum complexity in neuronal ensemble responses at each step, the automatic saccadic eye movement system greedily collects information about the external world, while modifying the neural representations in the process. This framework attempts to connect several psychological phenomena, such as pop-out and inhibition of return, to long term visual experience and short term working memory. It also provides an interesting perspective on contextual computation and formation of neural representation in the visual system.},
 author = {Lee, Tai Sing and Yu, Stella},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8d420fa35754d1f1c19969c88780314d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8d420fa35754d1f1c19969c88780314d-Metadata.json},
 openalex = {W2127850450},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8d420fa35754d1f1c19969c88780314d-Paper.pdf},
 publisher = {MIT Press},
 title = {An Information-Theoretic Framework for Understanding Saccadic Eye Movements},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/8d420fa35754d1f1c19969c88780314d-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_94e4451a,
 author = {Siegelmann, Hava and Roitershtein, Alexander and Ben-Hur, Asa},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/94e4451ad23909020c28b26ca3a13cb8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/94e4451ad23909020c28b26ca3a13cb8-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/94e4451ad23909020c28b26ca3a13cb8-Paper.pdf},
 publisher = {MIT Press},
 title = {Noisy Neural Networks and Generalizations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/94e4451ad23909020c28b26ca3a13cb8-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_955a1584,
 abstract = {The nonnegative Boltzmann machine (NNBM) is a recurrent neural network model that can describe multimodal nonnegative data. Application of maximum likelihood estimation to this model gives a learning rule that is analogous to the binary Boltzmann machine. We examine the utility of the mean field approximation for the NNBM, and describe how Monte Carlo sampling techniques can be used to learn its parameters. Reflective slice sampling is particularly well-suited for this distribution, and can efficiently be implemented to sample the distribution. We illustrate learning of the NNBM on a transiationally invariant distribution, as well as on a generative model for images of human faces.},
 author = {Downs, Oliver and MacKay, David and Lee, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/955a1584af63a546588caae4d23840b3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/955a1584af63a546588caae4d23840b3-Metadata.json},
 openalex = {W2105556268},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/955a1584af63a546588caae4d23840b3-Paper.pdf},
 publisher = {MIT Press},
 title = {The Nonnegative Boltzmann Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/955a1584af63a546588caae4d23840b3-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_96a93ba8,
 abstract = {We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions.},
 author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter and Frean, Marcus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Metadata.json},
 openalex = {W2108263314},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Paper.pdf},
 publisher = {MIT Press},
 title = {Boosting Algorithms as Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/96a93ba89a5b5c6c226e49b88973f46e-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_96de2547,
 abstract = {Ever since Pearl's probability propagation algorithm in graphs with cycles was shown to produce excellent results for error-correcting decoding a few years ago, we have been curious about whether local probability propagation could be used successfully for machine learning. One of the simplest adaptive models is the factor analyzer, which is a two-layer network that models bottom layer sensory inputs as a linear combination of top layer factors plus independent Gaussian sensor noise. We show that local probability propagation in the factor analyzer network usually takes just a few iterations to perform accurate inference, even in networks with 320 sensors and 80 factors. We derive an expression for the algorithm's fixed point and show that this fixed point matches the exact solution in a variety of networks, even when the fixed point is unstable. We also show that this method can be used successfully to perform inference for approximate EM and we give results on an online face recognition task.},
 author = {Frey, Brendan J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/96de2547f44254c97f5f4f1f402711c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/96de2547f44254c97f5f4f1f402711c1-Metadata.json},
 openalex = {W2168790308},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/96de2547f44254c97f5f4f1f402711c1-Paper.pdf},
 publisher = {MIT Press},
 title = {Local Probability Propagation for Factor Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/96de2547f44254c97f5f4f1f402711c1-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_973a5f0c,
 abstract = {There are many hierarchical clustering algorithms available, but these lack a firm statistical basis. Here we set up a hierarchical probabilistic mixture model, where data is generated in a hierarchical tree-structured manner. Markov chain Monte Carlo (MCMC) methods are demonstrated which can be used to sample from the posterior distribution over trees containing variable numbers of hidden units.},
 author = {Williams, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/973a5f0ccbc4ee3524ccf035d35b284b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/973a5f0ccbc4ee3524ccf035d35b284b-Metadata.json},
 openalex = {W2160157087},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/973a5f0ccbc4ee3524ccf035d35b284b-Paper.pdf},
 publisher = {MIT Press},
 title = {A MCMC Approach to Hierarchical Mixture Modelling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/973a5f0ccbc4ee3524ccf035d35b284b-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_97d98119,
 abstract = {In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the right number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling.},
 author = {Rasmussen, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/97d98119037c5b8a9663cb21fb8ebf47-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/97d98119037c5b8a9663cb21fb8ebf47-Metadata.json},
 openalex = {W2120636621},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf},
 publisher = {MIT Press},
 title = {The Infinite Gaussian Mixture Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/97d98119037c5b8a9663cb21fb8ebf47-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_9a440050,
 abstract = {We consider the problem of reconstructing a temporal discrete sequence of multidimensional real vectors when part of the data is missing, under the assumption that the sequence was generated by a continuous process. A particular case of this problem is multivariate regression, which is very difficult when the underlying mapping is one-to-many. We propose an algorithm based on a joint probability model of the variables of interest, implemented using a nonlinear latent variable model. Each point in the sequence is potentially reconstructed as any of the modes of the conditional distribution of the missing variables given the present variables (computed using an exhaustive mode search in a Gaussian mixture). Mode selection is determined by a dynamic programming search that minimises a geometric measure of the reconstructed sequence, derived from continuity constraints. We illustrate the algorithm with a toy example and apply it to a real-world inverse problem, the acoustic-to-articulatory mapping. The results show that the algorithm outperforms conditional mean imputation and multilayer perceptrons.},
 author = {Carreira-Perpi\~{n}\'{a}n, Miguel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9a4400501febb2a95e79248486a5f6d3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9a4400501febb2a95e79248486a5f6d3-Metadata.json},
 openalex = {W2108186306},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9a4400501febb2a95e79248486a5f6d3-Paper.pdf},
 publisher = {MIT Press},
 title = {Reconstruction of Sequential Data with Probabilistic Models and Continuity Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/9a4400501febb2a95e79248486a5f6d3-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_9d268236,
 abstract = {The project pursued in this paper is to develop from first information-geometric principles a general method for learning the similarity between text documents. Each individual document is modeled as a memoryless information source. Based on a latent class decomposition of the term-document matrix, a low-dimensional (curved) multinomial subfamily is learned. From this model a canonical similarity function - known as the Fisher kernel-is derived. Our approach can be applied for unsupervised and supervised learning problems alike. This in particular covers interesting cases where both, labeled and unlabeled data are available. Experiments in automated indexing and text categorization verify the advantages of the proposed method.},
 author = {Hofmann, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9d2682367c3935defcb1f9e247a97c0d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9d2682367c3935defcb1f9e247a97c0d-Metadata.json},
 openalex = {W2166023018},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning the Similarity of Documents: An Information-Geometric Approach to Document Retrieval and Categorization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_9fe97fff,
 abstract = {The three-dimensional motion of humans is underdetermined when the observation is limited to a single camera, due to the inherent 3D ambiguity of 2D video. We present a system that reconstructs the 3D motion of human subjects from single-camera video, relying on prior knowledge about human motion, learned from training data, to resolve those ambiguities. After initialization in 2D, the tracking and 3D reconstruction is automatic; we show results for several video sequences. The results show the power of treating 3D body tracking as an inference problem.},
 author = {Howe, Nicholas and Leventon, Michael and Freeman, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9fe97fff97f089661135d0487843108e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9fe97fff97f089661135d0487843108e-Metadata.json},
 openalex = {W2098813977},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9fe97fff97f089661135d0487843108e-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Reconstruction of 3D Human Motion from Single-Camera Video},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/9fe97fff97f089661135d0487843108e-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_a0f3601d,
 abstract = {Gaussian mixtures (or so-called radial basis function networks) for density estimation provide a natural counterpart to sigmoidal neural networks for function fitting and approximation. In both cases, it is possible to give simple expressions for the iterative improvement of performance as components of the network are introduced one at a time. In particular, for mixture density estimation we show that a k-component mixture estimated by maximum likelihood (or by an iterative likelihood improvement that we introduce) achieves log-likelihood within order 1/k of the log-likelihood achievable by any convex combination. Consequences for approximation and estimation using Kullback-Leibler risk are also given. A Minimum Description Length principle selects the optimal number of components k that minimizes the risk bound.},
 author = {Li, Jonathan and Barron, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a0f3601dc682036423013a5d965db9aa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a0f3601dc682036423013a5d965db9aa-Metadata.json},
 openalex = {W2168227362},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a0f3601dc682036423013a5d965db9aa-Paper.pdf},
 publisher = {MIT Press},
 title = {Mixture Density Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/a0f3601dc682036423013a5d965db9aa-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_a40511ca,
 abstract = {The reliability and accuracy of spike trains have been shown to depend on the nature of the stimulus that the neuron encodes. Adding ion channel stochasticity to neuronal models results in a macroscopic behavior that replicates the input-dependent reliability and precision of real neurons. We calculate the amount of information that an ion channel based stochastic Hodgkin-Huxley (HH) neuron model can encode about a wide set of stimuli. We show that both the information rate and the information per spike of the stochastic model are similar to the values reported experimentally. Moreover, the amount of information that the neuron encodes is correlated with the amplitude of fluctuations in the input, and less so with the average firing rate of the neuron. We also show that for the HH ion channel density, the information capacity is robust to changes in the density of ion channels in the membrane, whereas changing the ratio between the Na+ and K+ ion channels has a considerable effect on the information that the neuron can encode. Finally, we suggest that neurons may maximize their information capacity by appropriately balancing the density of the different ion channels that underlie neuronal excitability.},
 author = {Schneidman, Elad and Segev, Idan and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a40511cad8383e5ae8ddd8b855d135da-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a40511cad8383e5ae8ddd8b855d135da-Metadata.json},
 openalex = {W2166261868},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a40511cad8383e5ae8ddd8b855d135da-Paper.pdf},
 publisher = {MIT Press},
 title = {Information Capacity and Robustness of Stochastic Neuron Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/a40511cad8383e5ae8ddd8b855d135da-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_a51c896c,
 abstract = {Transduction is an inference principle that takes a training sample and aims at estimating the values of a function at given points contained in the so-called working sample as opposed to the whole of input space for induction. Transduction provides a confidence measure on single predictions rather than classifiers - a feature particularly important for risk-sensitive applications. The possibly infinite number of functions is reduced to a finite number of equivalence classes on the working sample. A rigorous Bayesian analysis reveals that for standard classification loss we cannot benefit from considering more than one test point at a time. The probability of the label of a given test point is determined as the posterior measure of the corresponding subset of hypothesis space. We consider the PAC setting of binary classification by linear discriminant functions (perceptrons) in kernel space such that the probability of labels is determined by the volume ratio in version space. We suggest to sample this region by an ergodic billiard. Experimental results on real world data indicate that Bayesian Transduction compares favourably to the well-known Support Vector Machine, in particular if the posterior probability of labellings is used as a confidence measure to exclude test points of low confidence.},
 author = {Graepel, Thore and Herbrich, Ralf and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a51c896c9cb81ecb5a199d51ac9fc3c5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a51c896c9cb81ecb5a199d51ac9fc3c5-Metadata.json},
 openalex = {W2293789987},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a51c896c9cb81ecb5a199d51ac9fc3c5-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Transduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/a51c896c9cb81ecb5a199d51ac9fc3c5-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_a588a619,
 abstract = {The committee approach has been proposed for reducing model uncertainty and improving generalization performance. The advantage of committees depends on (1) the performance of individual members and (2) the correlational structure of errors between members. This paper presents an input grouping technique for designing a heterogeneous committee With this technique, all input variables are first grouped based on their mutual information. Statistically similar variables are assigned to the same group. Each member's input set is then formed by input variables extracted from different groups. Our designed committees have less error correlation between its members, since each member observes different input variable combinations. The individual member's feature sets contain less redundant information, because highly correlated variables will not be combined together. The member feature sets contain almost complete information, since each set contains a feature from each information group. An empirical study for a noisy and nonstationary economic forecasting problem shows that committees constructed by our proposed technique outperform committees formed using several existing techniques.},
 author = {Liao, Yuansong and Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a588a6199feff5ba48402883d9b72700-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a588a6199feff5ba48402883d9b72700-Metadata.json},
 openalex = {W2140994537},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a588a6199feff5ba48402883d9b72700-Paper.pdf},
 publisher = {MIT Press},
 title = {Constructing Heterogeneous Committees Using Input Feature Grouping: Application to Economic Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/a588a6199feff5ba48402883d9b72700-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_a63fc8c5,
 abstract = {We provide an analysis of the turbo decoding algorithm (TDA) in a setting involving Gaussian densities. In this context, we are able to show that the algorithm converges and that - somewhat surprisingly - though the density generated by the TDA may differ significantly from the desired posterior density, the means of these two densities coincide.},
 author = {Rusmevichientong, Paat and Van Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a63fc8c5d915e1f1a40f40e6c7499863-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a63fc8c5d915e1f1a40f40e6c7499863-Metadata.json},
 openalex = {W2150964576},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a63fc8c5d915e1f1a40f40e6c7499863-Paper.pdf},
 publisher = {MIT Press},
 title = {An Analysis of Turbo Decoding with Gaussian Densities},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/a63fc8c5d915e1f1a40f40e6c7499863-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_a941493e,
 abstract = {I describe a framework for interpreting Support Vector Machines (SVMs) as maximum a posteriori (MAP) solutions to inference problems with Gaussian Process priors. This can provide intuitive guidelines for choosing a 'good' SVM kernel. It can also assign (by evidence maximization) optimal values to parameters such as the noise level C which cannot be determined unambiguously from properties of the MAP solution alone (such as cross-validation error). I illustrate this using a simple approximate expression for the SVM evidence. Once C has been determined, error bars on SVM predictions can also be obtained.},
 author = {Sollich, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a941493eeea57ede8214fd77d41806bc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a941493eeea57ede8214fd77d41806bc-Metadata.json},
 openalex = {W2115438515},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a941493eeea57ede8214fd77d41806bc-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Methods for Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/a941493eeea57ede8214fd77d41806bc-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_ab2b41c6,
 abstract = {This paper examines the role of biological constraints in the human auditory localization process. A psychophysical and neural system modeling approach was undertaken in which performance comparisons between competing models and a human subject explore the relevant biologically plausible "realism constraints." The directional acoustical cues, upon which sound localization is based, were derived from the human subject's head-related transfer functions (HRTFs). Sound stimuli were generated by convolving bandpass noise with the HRTFs and were presented to both the subject and the model. The input stimuli to the model were processed using the Auditory Image Model of cochlear processing. The cochlear data were then analyzed by a time-delay neural network which integrated temporal and spectral information to determine the spatial location of the sound source. The combined cochlear model and neural network provided a system model of the sound localization process. Aspects of humanlike localization performance were qualitatively achieved for broadband and bandpass stimuli when the model architecture incorporated frequency division (i.e., the progressive integration of information across the different frequency channels) and was trained using variable bandwidth and center-frequency sounds. Results indicate that both issues are relevant to human sound localization performance.},
 author = {Jin, Craig and Carlile, Simon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ab2b41c63853f0a651ba9fbf502b0cd8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ab2b41c63853f0a651ba9fbf502b0cd8-Metadata.json},
 openalex = {W2057823727},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ab2b41c63853f0a651ba9fbf502b0cd8-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural system identification model of human sound localization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/ab2b41c63853f0a651ba9fbf502b0cd8-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_acab0116,
 abstract = {We have developed and tested an analog/digital VLSI system that models the coordination of biological segmental oscillators underlying axial locomotion in animals such as leeches and lampreys. In its current form the system consists of a chain of twelve pattern generating circuits that are capable of arbitrary contralateral inhibitory synaptic coupling. Each pattern generating circuit is implemented with two independent silicon Morris-Lecar neurons with a total of 32 programmable (floating-gate based) inhibitory synapses, and an asynchronous address-event interconnection element that provides synaptic connectivity and implements axonal delay. We describe and analyze the data from a set of experiments exploring the system behavior in terms of synaptic coupling.},
 author = {Patel, Girish and Brown, Edgar and DeWeerth, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/acab0116c354964a558e65bdd07ff047-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/acab0116c354964a558e65bdd07ff047-Metadata.json},
 openalex = {W2151050136},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/acab0116c354964a558e65bdd07ff047-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neuromorphic VLSI System for Modeling the Neural Control of Axial Locomotion},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/acab0116c354964a558e65bdd07ff047-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_ae614c55,
 abstract = {We introduce a novel method of constructing language models, which avoids some of the problems associated with recurrent neural networks. The method of creating a Prediction Fractal Machine (PFM) [1] is briefly described and some experiments are presented which demonstrate the suitability of PFMs for language modeling. PFMs distinguish reliably between minimal pairs, and their behavior is consistent with the hypothesis [4] that wellformedness is 'graded' not absolute. A discussion of their potential to offer fresh insights into language acquisition and processing follows.},
 author = {Parfitt, Shan and Ti\~{n}o, Peter and Dorffner, Georg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ae614c557843b1df326cb29c57225459-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ae614c557843b1df326cb29c57225459-Metadata.json},
 openalex = {W2123198242},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
 publisher = {MIT Press},
 title = {Graded Grammaticality in Prediction Fractal Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/ae614c557843b1df326cb29c57225459-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b0f2ad44,
 abstract = {We describe a method for learning an overcomplete set of basis functions for the purpose of modeling sparse structure in images. The sparsity of the basis function coefficients is modeled with a mixture-of-Gaussians distribution. One Gaussian captures nonactive coefficients with a small-variance distribution centered at zero, while one or more other Gaussians capture active coefficients with a large-variance distribution. We show that when the prior is in such a form, there exist efficient methods for learning the basis functions as well as the parameters of the prior. The performance of the algorithm is demonstrated on a number of test cases and also on natural images. The basis functions learned on natural images are similar to those obtained with other methods, but the sparse form of the coefficient distribution is much better described. Also, since the parameters of the prior are adapted to the data, no assumption about sparse structure in the images need be made a priori, rather it is learned from the data.},
 author = {Olshausen, Bruno and Millman, K.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b0f2ad44d26e1a6f244201fe0fd864d1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b0f2ad44d26e1a6f244201fe0fd864d1-Metadata.json},
 openalex = {W2109742299},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b0f2ad44d26e1a6f244201fe0fd864d1-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Sparse Codes with a Mixture-of-Gaussians Prior},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b0f2ad44d26e1a6f244201fe0fd864d1-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b147a61c,
 abstract = {In this paper we discuss the semi parametric statistical model for blind deconvolution. First we introduce a Lie Group to the manifold of noncausal FIR filters. Then blind deconvolution problem is formulated in the framework of a semiparametric model, and a family of estimating functions is derived for blind deconvolution. A natural gradient learning algorithm is developed for training noncausal filters. Stability of the natural gradient algorithm is also analyzed in this framework.},
 author = {Zhang, Liqing and Amari, Shun-ichi and Cichocki, Andrzej},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b147a61c1d07c1c999560f62add6dbc7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b147a61c1d07c1c999560f62add6dbc7-Metadata.json},
 openalex = {W2163556507},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b147a61c1d07c1c999560f62add6dbc7-Paper.pdf},
 publisher = {MIT Press},
 title = {Semiparametric Approach to Multichannel Blind Deconvolution of Nonminimum Phase Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b147a61c1d07c1c999560f62add6dbc7-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b2531e7b,
 abstract = {In the analysis of data recorded by optical imaging from intrinsic signals (measurement of changes of light reflectance from cortical tissue) the removal of noise and artifacts such as blood vessel patterns is a serious problem. Often bandpass filtering is used, but the underlying assumption that a spatial frequency exists, which separates the mapping component from other components (especially the global signal), is questionable. Here we propose alternative ways of processing optical imaging data, using blind source separation techniques based on the spatial decorrelation of the data. We first perform benchmarks on artificial data in order to select the way of processing, which is most robust with respect to sensor noise. We then apply it to recordings of optical imaging experiments from macaque primary visual cortex. We show that our BSS technique is able to extract ocular dominance and orientation preference maps from single condition stacks, for data, where standard post-processing procedures fail. Artifacts, especially blood vessel patterns, can often be completely removed from the maps. In summary, our method for blind source separation using extended spatial decorrelation is a superior technique for the analysis of optical recording data.},
 author = {Schoner, Holger and Stetter, Martin and Schie\ss l, Ingo and Mayhew, John and Lund, Jennifer and McLoughlin, Niall and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b2531e7bb29bf22e1daae486fae3417a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b2531e7bb29bf22e1daae486fae3417a-Metadata.json},
 openalex = {W2129466809},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b2531e7bb29bf22e1daae486fae3417a-Paper.pdf},
 publisher = {MIT Press},
 title = {Application of Blind Separation of Sources to Optical Recording of Brain Activity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b2531e7bb29bf22e1daae486fae3417a-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b29eed44,
 abstract = {The differential contribution of the monaural and interaural spectral cues to human sound localization was examined using a combined psychophysical and analytical approach. The cues to a sound's location were correlated on an individual basis with the human localization responses to a variety of spectrally manipulated sounds. The spectral cues derive from the acoustical filtering of an individual's auditory periphery which is characterized by the measured head-related transfer functions (HRTFs). Auditory localization performance was determined in virtual auditory space (VAS). Psychoacoustical experiments were conducted in which the amplitude spectra of the sound stimulus was varied independently at each ear while preserving the normal timing cues, an impossibility in the free-field environment. Virtual auditory noise stimuli were generated over earphones for a specified target direction such that there was a false flat spectrum at the left eardrum. Using the subject's HRTFs, the sound spectrum at the right eardrum was then adjusted so that either the true right monaural spectral cue or the true interaural spectral cue was preserved. All subjects showed systematic mislocalizations in both the true right and true interaural spectral conditions which was absent in their control localization performance. The analysis of the different cues along with the subjects' localization responses suggests there are significant differences in the use of the monaural and interaural spectral cues and that the auditory system's reliance on the spectral cues varies with the sound condition.},
 author = {Jin, Craig and Corderoy, Anna and Carlile, Simon and van Schaik, Andr\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b29eed44276144e4e8103a661f9a78b7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b29eed44276144e4e8103a661f9a78b7-Metadata.json},
 openalex = {W2103270712},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b29eed44276144e4e8103a661f9a78b7-Paper.pdf},
 publisher = {MIT Press},
 title = {Spectral Cues in Human Sound Localization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b29eed44276144e4e8103a661f9a78b7-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b3b43aee,
 abstract = {In this paper, we propose a full Bayesian model for neural networks. This model treats the model dimension (number of neurons), model parameters, regularisation parameters and noise parameters as random variables that need to be estimated. We then propose a reversible jump Markov chain Monte Carlo (MCMC) method to perform the necessary computations. We find that the results are not only better than the previously reported ones, but also appear to be robust with respect to the prior specification. Moreover, we present a geometric convergence theorem for the algorithm.},
 author = {Andrieu, Christophe and de Freitas, Jo\~{a}o and Doucet, Arnaud},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b3b43aeeacb258365cc69cdaf42a68af-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b3b43aeeacb258365cc69cdaf42a68af-Metadata.json},
 openalex = {W2123256163},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf},
 publisher = {MIT Press},
 title = {Robust Full Bayesian Methods for Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b3b43aeeacb258365cc69cdaf42a68af-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b3bbccd6,
 abstract = {The psychophysical evidence for originates mainly from visual search experiments. In this work, we formulate a hierarchical system of interconnected modules consisting in populations of neurons for modeling the underlying mechanisms involved in selective visual attention. We demonstrate that our neural system for visual search works across the visual field in parallel but due to the different intrinsic dynamics can show the two experimentally observed modes of visual attention, namely: the serial and the parallel search mode. In other words, neither explicit model of a focus of attention nor saliencies maps are used. The focus of attention appears as an emergent property of the dynamic behavior of the system. The neural population dynamics are handled in the framework of the mean-field approximation. Consequently, the whole process can be expressed as a system of coupled differential equations.},
 author = {Deco, Gustavo and Zihl, Josef},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b3bbccd6c008e727785cb81b1aa08ac5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b3bbccd6c008e727785cb81b1aa08ac5-Metadata.json},
 openalex = {W2120707780},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b3bbccd6c008e727785cb81b1aa08ac5-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neurodynamical Approach to Visual Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b3bbccd6c008e727785cb81b1aa08ac5-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b59a51a3,
 abstract = {We generalize a recent formalism to describe the dynamics of supervised learning in layered neural networks, in the regime where data recycling is inevitable, to the case of noisy teachers. Our theory generates reliable predictions for the evolution in time of training - and generalization errors, and extends the class of mathematically solvable learning processes in large neural networks to those situations where overfitting can occur.},
 author = {Coolen, Anthony and Mace, C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b59a51a3c0bf9c5228fde841714f523a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b59a51a3c0bf9c5228fde841714f523a-Metadata.json},
 openalex = {W2170853645},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b59a51a3c0bf9c5228fde841714f523a-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamics of Supervised Learning with Restricted Training Sets and Noisy Teachers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b59a51a3c0bf9c5228fde841714f523a-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b618c321,
 abstract = {Psychophysical and physiological evidence shows that sound localization of acoustic signals is strongly influenced by their synchrony with visual signals. This effect, known as ventriloquism, is at work when sound coming from the side of a TV set feels as if it were coming from the mouth of the actors. The ventriloquism effect suggests that there is important information about sound location encoded in the synchrony between the audio and video signals. In spite of this evidence, audiovisual synchrony is rarely used as a source of information in computer vision tasks. In this paper we explore the use of audio visual synchrony to locate sound sources. We developed a system that searches for regions of the visual landscape that correlate highly with the acoustic signals and tags them as likely to contain an acoustic source. We discuss our experience implementing the system, present results on a speaker localization task and discuss potential applications of the approach.},
 author = {Hershey, John and Movellan, Javier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b618c3210e934362ac261db280128c22-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b618c3210e934362ac261db280128c22-Metadata.json},
 openalex = {W2164899449},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b618c3210e934362ac261db280128c22-Paper.pdf},
 publisher = {MIT Press},
 title = {Audio Vision: Using Audio-Visual Synchrony to Locate Sounds},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b618c3210e934362ac261db280128c22-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b865367f,
 abstract = {Neocortical circuits are dominated by massive excitatory feedback: more than eighty percent of the synapses made by excitatory cortical neurons are onto other excitatory cortical neurons. Why is there such massive recurrent excitation in the neocortex and what is its role in cortical computation? Recent neurophysiological experiments have shown that the plasticity of recurrent neocortical synapses is governed by a temporally asymmetric Hebbian learning rule. We describe how such a rule may allow the cortex to modify recurrent synapses for prediction of input sequences. The goal is to predict the next cortical input from the recent past based on previous experience of similar input sequences. We show that a temporal difference learning rule for prediction used in conjunction with dendritic back-propagating action potentials reproduces the temporally asymmetric Hebbian plasticity observed physiologically. Biophysical simulations demonstrate that a network of cortical neurons can learn to predict moving stimuli and develop direction selective responses as a consequence of learning. The space-time response properties of model neurons are shown to be similar to those of direction selective cells in alert monkey V1.},
 author = {Rao, Rajesh and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b865367fc4c0845c0682bd466e6ebf4c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b865367fc4c0845c0682bd466e6ebf4c-Metadata.json},
 openalex = {W2154039946},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf},
 publisher = {MIT Press},
 title = {Predictive Sequence Learning in Recurrent Neocortical Circuits},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b865367fc4c0845c0682bd466e6ebf4c-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_b9f94c77,
 abstract = {For many problems, the correct behavior of a model depends not only on its input-output mapping but also on properties of its Jacobian matrix, the matrix of partial derivatives of the model's outputs with respect to its inputs. We introduce the J-prop algorithm, an efficient general method for computing the exact partial derivatives of a variety of simple functions of the Jacobian of a model with respect to its free parameters. The algorithm applies to any parametrized feedforward model, including nonlinear regression, multilayer perceptrons, and radial basis function networks.},
 author = {Flake, Gary and Pearlmutter, Barak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b9f94c77652c9a76fc8a442748cd54bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b9f94c77652c9a76fc8a442748cd54bd-Metadata.json},
 openalex = {W2109860336},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf},
 publisher = {MIT Press},
 title = {Differentiating Functions of the Jacobian with Respect to the Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/b9f94c77652c9a76fc8a442748cd54bd-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_ba1b3eba,
 abstract = {Spatial information comes in two forms: direct spatial information (for example, retinal position) and indirect temporal contiguity information, since objects encountered sequentially are in general spatially close. The acquisition of spatial information by a neural network is investigated here. Given a spatial layout of several objects, networks are trained on a prediction task. Networks using temporal sequences with no direct spatial information are found to develop internal representations that show distances correlated with distances in the external layout. The influence of spatial information is analyzed by providing direct spatial information to the system during training that is either consistent with the layout or inconsistent with it. This approach allows examination of the relative contributions of spatial and temporal contiguity.},
 author = {Ghiselli-Crippa, Thea and Munro, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ba1b3eba322eab5d895aa3023fe78b9c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ba1b3eba322eab5d895aa3023fe78b9c-Metadata.json},
 openalex = {W2154051454},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ba1b3eba322eab5d895aa3023fe78b9c-Paper.pdf},
 publisher = {MIT Press},
 title = {Effects of Spatial and Temporal Contiguity on the Acquisition of Spatial Information},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/ba1b3eba322eab5d895aa3023fe78b9c-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_be3e9d3f,
 abstract = {We introduce a novel distributional clustering algorithm that maximizes the mutual information per cluster between data and given categories. This algorithm can be considered as a bottom up hard version of the recently introduced Information Bottleneck Method. The algorithm is compared with the top-down soft version of the information bottleneck method and a relationship between the hard and soft results is established. We demonstrate the algorithm on the 20 Newsgroups data set. For a subset of two news-groups we achieve compression by 3 orders of magnitudes loosing only 10% of the original mutual information.},
 author = {Slonim, Noam and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/be3e9d3f7d70537357c67bb3f4086846-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/be3e9d3f7d70537357c67bb3f4086846-Metadata.json},
 openalex = {W2103723258},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/be3e9d3f7d70537357c67bb3f4086846-Paper.pdf},
 publisher = {MIT Press},
 title = {Agglomerative Information Bottleneck},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/be3e9d3f7d70537357c67bb3f4086846-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_c0560792,
 abstract = {We analyze the conditions under which synaptic learning rules based on action potential timing can be approximated by learning rules based on firing rates. In particular, we consider a form of plasticity in which synapses depress when a presynaptic spike is followed by a postsynaptic spike, and potentiate with the opposite temporal ordering. Such differential anti-Hebbian plasticity can be approximated under certain conditions by a learning rule that depends on the time derivative of the postsynaptic firing rate. Such a learning rule acts to stabilize persistent neural activity patterns in recurrent neural networks.},
 author = {Xie, Xiaohui and Seung, H. Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c0560792e4a3c79e62f76cbf9fb277dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c0560792e4a3c79e62f76cbf9fb277dd-Metadata.json},
 openalex = {W2121392865},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c0560792e4a3c79e62f76cbf9fb277dd-Paper.pdf},
 publisher = {MIT Press},
 title = {Spike-based Learning Rules and Stabilization of Persistent Neural Activity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/c0560792e4a3c79e62f76cbf9fb277dd-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_c0d0e461,
 abstract = {Fishers linear discriminant analysis (LDA) is a classical multivariate technique both for dimension reduction and classification. The data vectors are transformed into a low dimensional subspace such that the class centroids are spread out as much as possible. In this subspace LDA works as a simple prototype classifier with linear decision boundaries. However, in many applications the linear boundaries do not adequately separate the classes. We present a nonlinear generalization of discriminant analysis that uses the kernel trick of representing dot products by kernel functions. The presented algorithm allows a simple formulation of the EM-algorithm in terms of kernel functions which leads to a unique concept for unsupervised mixture analysis, supervised discriminant analysis and semi-supervised discriminant analysis with partially unlabelled observations in feature spaces.},
 author = {Roth, Volker and Steinhage, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c0d0e461de8d0024aebcb0a7c68836df-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c0d0e461de8d0024aebcb0a7c68836df-Metadata.json},
 openalex = {W2108146080},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c0d0e461de8d0024aebcb0a7c68836df-Paper.pdf},
 publisher = {MIT Press},
 title = {Nonlinear Discriminant Analysis Using Kernel Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/c0d0e461de8d0024aebcb0a7c68836df-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_c1fea270,
 abstract = {In this paper we will treat input selection for a radial basis function (RBF) like classifier within a Bayesian framework. We approximate the a-posteriori distribution over both model coefficients and input subsets by samples drawn with Gibbs updates and reversible jump moves. Using some public datasets, we compare the classification accuracy of the method with a conventional ARD scheme. These datasets are also used to infer the a-posteriori probabilities of different input subsets.},
 author = {Sykacek, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c1fea270c48e8079d8ddf7d06d26ab52-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c1fea270c48e8079d8ddf7d06d26ab52-Metadata.json},
 openalex = {W2170063046},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf},
 publisher = {MIT Press},
 title = {On Input Selection with Reversible Jump Markov Chain Monte Carlo Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/c1fea270c48e8079d8ddf7d06d26ab52-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_c4492cbe,
 abstract = {We give necessary and sufficient conditions for uniqueness of the support vector solution for the problems of pattern recognition and regression estimation, for a general class of cost functions. We show that if the solution is not unique, all support vectors are necessarily at bound, and we give some simple examples of non-unique solutions. We note that uniqueness of the primal (dual) solution does not necessarily imply uniqueness of the dual (primal) solution. We show how to compute the threshold b when the solution is unique, but when all support vectors are at bound, in which case the usual method for determining b does not work.},
 author = {Burges, Christopher J. C. and Crisp, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c4492cbe90fbdbf88a5aec486aa81ed5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c4492cbe90fbdbf88a5aec486aa81ed5-Metadata.json},
 openalex = {W2168420538},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c4492cbe90fbdbf88a5aec486aa81ed5-Paper.pdf},
 publisher = {MIT Press},
 title = {Uniqueness of the SVM Solution},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/c4492cbe90fbdbf88a5aec486aa81ed5-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_c59b469d,
 abstract = {Imagine that you wish to classify data consisting of tens of thousands of examples residing in a twenty thousand dimensional space. How can one apply standard machine learning algorithms? We describe the Parallel Problems Server (PPServer) and MATLAB*P. In tandem they allow users of networked computers to work transparently on large data sets from within Matlab. This work is motivated by the desire to bring the many benefits of scientific computing algorithms and computational power to machine learning researchers.

We demonstrate the usefulness of the system on a number of tasks. For example, we perform independent components analysis on very large text corpora consisting of tens of thousands of documents, making minimal changes to the original Bell and Sejnowski Matlab source (Bell and Sejnowski, 1995). Applying ML techniques to data previously beyond their reach leads to interesting analyses of both data and algorithms.},
 author = {Isbell, Charles and Husbands, Parry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c59b469d724f7919b7d35514184fdc0f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c59b469d724f7919b7d35514184fdc0f-Metadata.json},
 openalex = {W2127556726},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c59b469d724f7919b7d35514184fdc0f-Paper.pdf},
 publisher = {MIT Press},
 title = {The Parallel Problems Server: an Interactive Tool for Large Scale Machine Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/c59b469d724f7919b7d35514184fdc0f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_cc42acc8,
 abstract = {In this paper, we use mutual information to characterize the distributions of phonetic and speaker/channel information in a time-frequency space. The mutual information (MI) between the phonetic label and one feature, and the joint mutual information (JMI) between the phonetic label and two or three features are estimated. The Miller's bias formulas for entropy and mutual information estimates are extended to include higher order terms. The MI and the JMI for speaker/channel recognition are also estimated. The results are complementary to those for phonetic classification. Our results show how the phonetic information is locally spread and how the speaker/channel information is globally spread in time and frequency.},
 author = {Yang, Howard and Hermansky, Hynek},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/cc42acc8ce334185e0193753adb6cb77-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/cc42acc8ce334185e0193753adb6cb77-Metadata.json},
 openalex = {W2129223887},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/cc42acc8ce334185e0193753adb6cb77-Paper.pdf},
 publisher = {MIT Press},
 title = {Search for Information Bearing Components in Speech},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/cc42acc8ce334185e0193753adb6cb77-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_cdf1035c,
 author = {Brown, Guy and Wang, DeLiang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/cdf1035c34ec380218a8cc9a43d438f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/cdf1035c34ec380218a8cc9a43d438f9-Metadata.json},
 openalex = {W2136812504},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/cdf1035c34ec380218a8cc9a43d438f9-Paper.pdf},
 publisher = {MIT Press},
 title = {An Oscillatory Correlation Frame work for Computational Auditory Scene Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/cdf1035c34ec380218a8cc9a43d438f9-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_d2cdf047,
 abstract = {We propose a new Markov Chain Monte Carlo algorithm, which is a generalization of the stochastic dynamics method. The algorithm performs exploration of the state-space using its intrinsic geometric structure, which facilitates efficient sampling of complex distributions. Applied to Bayesian learning in neural networks, our algorithm was found to produce results comparable to the best state-of-the-art method while consuming considerably less time.},
 author = {Zlochin, Mark and Baram, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d2cdf047a6674cef251d56544a3cf029-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d2cdf047a6674cef251d56544a3cf029-Metadata.json},
 openalex = {W2136491832},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d2cdf047a6674cef251d56544a3cf029-Paper.pdf},
 publisher = {MIT Press},
 title = {Manifold Stochastic Dynamics for Bayesian Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/d2cdf047a6674cef251d56544a3cf029-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_d43ab110,
 abstract = {In this paper, we describe an analysis of the nonlinear dynamical phenomenon associated with a silicon neuron. Our silicon neuron in Very Large Scale Integration (VLSI) integrates Hodgkin–Huxley (HH) model formalism, including the membrane voltage dependency of temporal dynamics. Analysis of the bifurcation conditions allow us to identify different regimes in the parameter space that are desirable for biasing our silicon neuron. This approach of studying bifurcations is useful because it is believed that computational properties of neurons are based on the bifurcations exhibited by these dynamical systems in response to some changing stimulus. We describe numerical simulations of the Hopf bifurcation which is characteristic of class 2 excitability in the HH model. We also show experimental measurements of an observed phenomenon in biological neurons and termed excitation block, firing rate and effect of current impulses. Hence, by showing that this silicon neuron has similar bifurcations to a certain class of biological neurons, we can claim that the silicon neuron can also perform similar computations.},
 author = {Patel, Girish and Cymbalyuk, Gennady and Calabrese, Ronald and DeWeerth, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d43ab110ab2489d6b9b2caa394bf920f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d43ab110ab2489d6b9b2caa394bf920f-Metadata.json},
 openalex = {W1965506448},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d43ab110ab2489d6b9b2caa394bf920f-Paper.pdf},
 publisher = {MIT Press},
 title = {Bifurcation analysis in a silicon neuron},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/d43ab110ab2489d6b9b2caa394bf920f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_d860bd12,
 abstract = {The speech waveform can be modelled as a piecewise-stationary linear stochastic state space system, and its parameters can be estimated using an expectation-maximisation (EM) algorithm. One problem is the initialisation of the EM algorithm. Standard initialisation schemes can lead to poor formant trajectories. But these trajectories however are important for vowel intelligibility. The aim of this paper is to investigate the suitability of subspace identification methods to initialise EM.

The paper compares the subspace state space system identification (4SID) method with the EM algorithm. The 4SID and EM methods are similar in that they both estimate a state sequence (but using Kalman filters and Kalman smoothers respectively), and then estimate parameters (but using least-squares and maximum likelihood respectively). The similarity of 4SID and EM motivates the use of 4SID to initialise EM. Also, 4SID is non-iterative and requires no initialisation, whereas EM is iterative and requires initialisation. However 4SID is sub-optimal compared to EM in a probabilistic sense. During experiments on real speech, 4SID methods compare favourably with conventional initialisation techniques. They produce smoother formant trajectories, have greater frequency resolution, and produce higher likelihoods.},
 author = {Smith, Gavin and de Freitas, Jo\~{a}o and Robinson, Tony and Niranjan, Mahesan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d860bd12ce9c026814bbdfc1c573f0f5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d860bd12ce9c026814bbdfc1c573f0f5-Metadata.json},
 openalex = {W2155797931},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d860bd12ce9c026814bbdfc1c573f0f5-Paper.pdf},
 publisher = {MIT Press},
 title = {Speech Modelling Using Subspace and EM Techniques},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/d860bd12ce9c026814bbdfc1c573f0f5-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_d8d31bd7,
 abstract = {Local linear regression performs very well in many low-dimensional forecasting problems. In high-dimensional spaces, its performance typically decays due to the well-known curse-of-dimensionality. A possible way to approach this problem is by varying the shape of the weighting kernel. In this work we suggest a new, data-driven method to estimating the optimal kernel shape. Experiments using an artificially generated data set and data from the UC Irvine repository show the benefits of kernel shaping.},
 author = {Ormoneit, Dirk and Hastie, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d8d31bd778da8bdd536187c36e48892b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d8d31bd778da8bdd536187c36e48892b-Metadata.json},
 openalex = {W2150575761},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d8d31bd778da8bdd536187c36e48892b-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimal kernel shapes for local linear regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/d8d31bd778da8bdd536187c36e48892b-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_db29450c,
 abstract = {We replace the commonly used Gaussian noise model in nonlinear regression by a more flexible noise model based on the Student-t- distribution. The degrees of freedom of the t-distribution can be chosen such that as special cases either the Gaussian distribution or the Cauchy distribution are realized. The latter is commonly used in robust regression. Since the t-distribution can be interpreted as being an infinite mixture of Gaussians, parameters and hyperparameters such as the degrees of freedom of the t-distribution can be learned from the data based on an EM-learning algorithm. We show that modeling using the t-distribution leads to improved predictors on real world data sets. In particular, if outliers are present, the t-distribution is superior to the Gaussian noise model. In effect, by adapting the degrees of freedom, the system can learn to distinguish between outliers and non-outliers. Especially for online learning tasks, one is interested in avoiding inappropriate weight changes due to measurement outliers to maintain stable online learning capability. We show experimentally that using the t-distribution as a noise model leads to stable online learning algorithms and outperforms state-of-the art online learning methods like the extended Kalman filter algorithm.},
 author = {Briegel, Thomas and Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/db29450c3f5e97f97846693611f98c15-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/db29450c3f5e97f97846693611f98c15-Metadata.json},
 openalex = {W2104112579},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/db29450c3f5e97f97846693611f98c15-Paper.pdf},
 publisher = {MIT Press},
 title = {Robust Neural Network Regression for Offline and Online Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/db29450c3f5e97f97846693611f98c15-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_db957c62,
 abstract = {Model Predictive Control (MPC), a control algorithm which uses an optimizer to solve for the optimal control moves over a future time horizon based upon a model of the process, has become a standard control technique in the process industries over the past two decades. In most industrial applications, a linear dynamic model developed using empirical data is used even though the process itself is often nonlinear. Linear models have been used because of the difficulty in developing a generic nonlinear model from empirical data and the computational expense often involved in using nonlinear models. In this paper, we present a generic neural network based technique for developing nonlinear dynamic models from empirical data and show that these models can be efficiently used in a model predictive control framework. This nonlinear MPC based approach has been successfully implemented in a number of industrial applications in the refining, petrochemical, paper and food industries. Performance of the controller on a nonlinear industrial process, a polyethylene reactor, is presented.},
 author = {Piche, Stephen and Keeler, James and Martin, Greg and Boe, Gene and Johnson, Doug and Gerules, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/db957c626a8cd7a27231adfbf51e20eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/db957c626a8cd7a27231adfbf51e20eb-Metadata.json},
 openalex = {W2155353042},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Network Based Model Predictive Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/db957c626a8cd7a27231adfbf51e20eb-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_df9028fc,
 abstract = {The problem that we address in this paper is how a mobile robot can plan in order to arrive at its goal with minimum uncertainty. Traditional motion planning algorithms often assume that a mobile robot can track its position reliably, however, in real world situations, reliable localization may not always be feasible. Partially Observable Markov Decision Processes (POMDPs) provide one way to maximize the certainty of reaching the goal state, but at the cost of computational intractability for large state spaces.

The method we propose explicitly models the uncertainty of the robot's position as a state variable, and generates trajectories through the augmented pose-uncertainty space. By minimizing the positional uncertainty at the goal, the robot reduces the likelihood it becomes lost. We demonstrate experimentally that coastal navigation reduces the uncertainty at the goal, especially with degraded localization.},
 author = {Roy, Nicholas and Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/df9028fcb6b065e000ffe8a4f03eeb38-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/df9028fcb6b065e000ffe8a4f03eeb38-Metadata.json},
 openalex = {W2106237563},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/df9028fcb6b065e000ffe8a4f03eeb38-Paper.pdf},
 publisher = {MIT Press},
 title = {Coastal Navigation with Mobile Robots},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/df9028fcb6b065e000ffe8a4f03eeb38-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e1696007,
 abstract = {It is known that decision tree learning can be viewed as a form of boosting. However, existing boosting theorems for decision tree learning allow only binary-branching trees and the generalization to multi-branching trees is not immediate. Practical decision tree algorithms, such as CART and C4.5, implement a trade-off between the number of branches and the improvement in tree quality as measured by an index function. Here we give a boosting justification for a particular quantitative trade-off curve. Our main theorem states, in essence, that if we require an improvement proportional to the log of the number of branches then top-down greedy construction of decision trees remains an effective boosting algorithm.},
 author = {Mansour, Yishay and McAllester, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e1696007be4eefb81b1a1d39ce48681b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e1696007be4eefb81b1a1d39ce48681b-Metadata.json},
 openalex = {W2123793406},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf},
 publisher = {MIT Press},
 title = {Boosting with Multi-Way Branching in Decision Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e1696007be4eefb81b1a1d39ce48681b-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e22dd5da,
 abstract = {The encoding accuracy of a population of stochastically spiking neurons is studied for different distributions of their tuning widths. The situation of identical radially symmetric receptive fields for all neurons, which is usually considered in the literature, turns out to be disadvantageous from an information-theoretic point of view. Both a variability of tuning widths and a fragmentation of the neural population into specialized subpopulations improve the encoding accuracy.},
 author = {Eurich, Christian and Wilke, Stefan and Schwegler, Helmut},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e22dd5dabde45eda5a1a67772c8e25dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e22dd5dabde45eda5a1a67772c8e25dd-Metadata.json},
 openalex = {W2149720325},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e22dd5dabde45eda5a1a67772c8e25dd-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Representation of Multi-Dimensional Stimuli},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e22dd5dabde45eda5a1a67772c8e25dd-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e449b931,
 abstract = {New functionals for parameter (model) selection of Support Vector Machines are introduced based on the concepts of the span of support vectors and rescaling of the feature space. It is shown that using these functionals, one can both predict the best choice of parameters of the model and the relative quality of performance for any value of parameter.},
 author = {Chapelle, Olivier and Vapnik, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e449b9317dad920c0dd5ad0a2a2d5e49-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e449b9317dad920c0dd5ad0a2a2d5e49-Metadata.json},
 openalex = {W2158078575},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf},
 publisher = {MIT Press},
 title = {Model Selection for Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e449b9317dad920c0dd5ad0a2a2d5e49-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e4873aa9,
 abstract = {Previous biophysical modeling work showed that nonlinear interactions among nearby synapses located on active dendritic trees can provide a large boost in the memory capacity of a cell (Mel, 1992a, 1992b). The aim of our present work is to quantify this boost by estimating the capacity of (1) a neuron model with passive dendritic integration where inputs are combined linearly across the entire cell followed by a single global threshold, and (2) an active dendrite model in which a threshold is applied separately to the output of each branch, and the branch subtotals are combined linearly. We focus here on the limiting case of binary-valued synaptic weights, and derive expressions which measure model capacity by estimating the number of distinct input-output functions available to both neuron types. We show that (1) the application of a fixed nonlinearity to each dendritic compartment substantially increases the model's flexibility, (2) for a neuron of realistic size, the capacity of the nonlinear cell can exceed that of the same-sized linear cell by more than an order of magnitude, and (3) the largest capacity boost occurs for cells with a relatively large number of dendritic subunits of relatively small size. We validated the analysis by empirically measuring memory capacity with randomized two-class classification problems, where a stochastic delta rule was used to train both linear and nonlinear models. We found that large capacity boosts predicted for the nonlinear dendritic model were readily achieved in practice.},
 author = {Poirazi, Panayiota and Mel, Bartlett},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e4873aa9a05cc5ed839561d121516766-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e4873aa9a05cc5ed839561d121516766-Metadata.json},
 openalex = {W2123555956},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e4873aa9a05cc5ed839561d121516766-Paper.pdf},
 publisher = {MIT Press},
 title = {Memory Capacity of Linear vs. Nonlinear Models of Dendritic Integration},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e4873aa9a05cc5ed839561d121516766-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e5a4d6bf,
 abstract = {Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.},
 author = {Dietterich, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e5a4d6bf330f23a8707bb0d6001dfbe8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e5a4d6bf330f23a8707bb0d6001dfbe8-Metadata.json},
 openalex = {W1754881896},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e5a4d6bf330f23a8707bb0d6001dfbe8-Paper.pdf},
 publisher = {MIT Press},
 title = {State Abstraction in MAXQ Hierarchical Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e5a4d6bf330f23a8707bb0d6001dfbe8-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e6384711,
 abstract = {The curse of dimensionality is severe when modeling high-dimensional discrete data: the number of possible combinations of the variables explodes exponentially. In this paper we propose a new architecture for modeling high-dimensional data that requires resources (parameters and computations) that grow only at most as the square of the number of variables, using a multi-layer neural network to represent the joint distribution of the variables as the product of conditional distributions. The neural network can be interpreted as a graphical model without hidden random variables, but in which the conditional distributions are tied through the hidden units. The connectivity of the neural network can be pruned by using dependency tests between the variables. Experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive Bayes and comparable Bayesian networks, and show that significant improvements can be obtained by pruning the network.},
 author = {Bengio, Yoshua and Bengio, Samy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e6384711491713d29bc63fc5eeb5ba4f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e6384711491713d29bc63fc5eeb5ba4f-Metadata.json},
 openalex = {W2103936488},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e721a54a,
 abstract = {Attractor networks, which map an input space to a discrete output space, are useful for pattern completion. However, designing a net to have a given set of attractors is notoriously tricky; training procedures are CPU intensive and often produce spurious afuactors and ill-conditioned attractor basins. These difficulties occur because each connection in the network participates in the encoding of multiple attractors. We describe an alternative formulation of attractor networks in which the encoding of knowledge is local, not distributed. Although localist attractor networks have similar dynamics to their distributed counterparts, they are much easier to work with and interpret. We propose a statistical formulation of localist attract or net dynamics, which yields a convergence proof and a mathematical interpretation of model parameters.},
 author = {Zemel, Richard and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e721a54a8cf18c8543d44782d9ef681f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e721a54a8cf18c8543d44782d9ef681f-Metadata.json},
 openalex = {W2103041741},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e721a54a8cf18c8543d44782d9ef681f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Generative Model for Attractor Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e8d92f99,
 abstract = {Reinforcement learning in nonstationary environments is generally regarded as an important and yet difficult problem. This paper partially addresses the problem by formalizing a subclass of nonstationary environments. The environment model, called hidden-mode Markov decision process (HM-MDP), assumes that environmental changes are always confined to a small number of hidden modes. A mode basically indexes a Markov decision process (MDP) and evolves with time according to a Markov chain. While HM-MDP is a special case of partially observable Markov decision processes (POMDP), modeling an HM-MDP environment via the more general POMDP model unnecessarily increases the problem complexity. A variant of the Baum-Welch algorithm is developed for model learning requiring less data and time.},
 author = {Choi, Samuel and Yeung, Dit-Yan and Zhang, Nevin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e8d92f99edd25e2cef48eca48320a1a5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e8d92f99edd25e2cef48eca48320a1a5-Metadata.json},
 openalex = {W2156371714},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e8d92f99edd25e2cef48eca48320a1a5-Paper.pdf},
 publisher = {MIT Press},
 title = {An Environment Model for Nonstationary Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e8d92f99edd25e2cef48eca48320a1a5-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e8fd4a8a,
 author = {Sundararajan, S. and Keerthi, S.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e8fd4a8a5bab2b3785d794ab51fef55c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e8fd4a8a5bab2b3785d794ab51fef55c-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e8fd4a8a5bab2b3785d794ab51fef55c-Paper.pdf},
 publisher = {MIT Press},
 title = {Predictive App roaches for Choosing Hyperparameters in Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e8fd4a8a5bab2b3785d794ab51fef55c-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_e9b73bcc,
 abstract = {Quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning. However, most models have simply ignored these data; the few that have attempted to address them have failed by at least an order of magnitude. We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli play a crucial role.},
 author = {Kakade, Sham and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e9b73bccd1762555582b513ff9d02492-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e9b73bccd1762555582b513ff9d02492-Metadata.json},
 openalex = {W2112376384},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e9b73bccd1762555582b513ff9d02492-Paper.pdf},
 publisher = {MIT Press},
 title = {Acquisition in Autoshaping},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/e9b73bccd1762555582b513ff9d02492-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_ef2a4be5,
 abstract = {We introduce an algorithm for estimating the values of a function at a set of test points xl+1,..., xl+m given a set of training points (x1, Y1),...,(xl, yl) without estimating (as an intermediate step) the regression function. We demonstrate that this direct (transductive) way for estimating values of the regression (or classification in pattern recognition) can be more accurate than the traditional one based on two steps, first estimating the function and then calculating the values of this function at the points of interest.},
 author = {Chapelle, Olivier and Vapnik, Vladimir and Weston, Jason},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ef2a4be5473ab0b3cc286e67b1f59f44-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ef2a4be5473ab0b3cc286e67b1f59f44-Metadata.json},
 openalex = {W2145366000},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ef2a4be5473ab0b3cc286e67b1f59f44-Paper.pdf},
 publisher = {MIT Press},
 title = {Transductive Inference for Estimating Values of Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/ef2a4be5473ab0b3cc286e67b1f59f44-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_f0bda020,
 abstract = {This paper revisits the classical neuroscience paradigm of Hebbian learning. We find that a necessary requirement for effective associative memory learning is that the efficacies of the incoming synapses should be uncorrelated. This requirement is difficult to achieve in a robust manner by Hebbian synaptic learning, since it depends on network level information. Effective learning can yet be obtained by a neuronal process that maintains a zero sum of the incoming synaptic efficacies. This normalization drastically improves the memory capacity of associative networks, from an essentially bounded capacity to one that linearly scales with the network's size. It also enables the effective storage of patterns with heterogeneous coding levels in a single network. Such neuronal normalization can be successfully carried out by activity-dependent homeostasis of the neuron's synaptic efficacies, which was recently observed in cortical tissue. Thus, our findings strongly suggest that effective associative learning with Hebbian synapses alone is biologically implausible and that Hebbian synapses must be continuously remodeled by neuronally-driven regulatory processes in the brain.},
 author = {Chechik, Gal and Meilijson, Isaac and Ruppin, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f0bda020d2470f2e74990a07a607ebd9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f0bda020d2470f2e74990a07a607ebd9-Metadata.json},
 openalex = {W2112126127},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf},
 publisher = {MIT Press},
 title = {Effective Learning Requires Neuronal Remodeling of Hebbian Synapses},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/f0bda020d2470f2e74990a07a607ebd9-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_f3144cef,
 abstract = {The support vector machine (SVM) is a state-of-the-art technique for regression and classification, combining excellent generalisation properties with a sparse kernel representation. However, it does suffer from a number of disadvantages, notably the absence of probabilistic outputs, the requirement to estimate a trade-off parameter and the need to utilise 'Mercer' kernel functions. In this paper we introduce the Relevance Vector Machine (RVM), a Bayesian treatment of a generalised linear model of identical functional form to the SVM. The RVM suffers from none of the above disadvantages, and examples demonstrate that for comparable generalisation performance, the RVM requires dramatically fewer kernel functions.},
 author = {Tipping, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f3144cefe89a60d6a1afaf7859c5076b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f3144cefe89a60d6a1afaf7859c5076b-Metadata.json},
 openalex = {W2137512539},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f3144cefe89a60d6a1afaf7859c5076b-Paper.pdf},
 publisher = {MIT Press},
 title = {The Relevance Vector Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/f3144cefe89a60d6a1afaf7859c5076b-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_f50a6c02,
 abstract = {Dual estimation refers to the problem of simultaneously estimating the state of a dynamic system and the model which gives rise to the dynamics. Algorithms include expectation-maximization (EM), dual Kalman filtering, and joint Kalman methods. These methods have recently been explored in the context of nonlinear modeling, where a neural network is used as the functional form of the unknown model. Typically, an extended Kalman filter (EKF) or smoother is used for the part of the algorithm that estimates the clean state given the current estimated model. An EKF may also be used to estimate the weights of the network. This paper points out the flaws in using the EKF, and proposes an improvement based on a new approach called the unscented transformation (UT) [3]. A substantial performance gain is achieved with the same order of computational complexity as that of the standard EKF. The approach is illustrated on several dual estimation methods.},
 author = {Wan, Eric and van der Merwe, Rudolph and Nelson, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f50a6c02a3fc5a3a5d4d9391f05f3efc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f50a6c02a3fc5a3a5d4d9391f05f3efc-Metadata.json},
 openalex = {W2147929628},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f50a6c02a3fc5a3a5d4d9391f05f3efc-Paper.pdf},
 publisher = {MIT Press},
 title = {Dual Estimation and the Unscented Transformation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/f50a6c02a3fc5a3a5d4d9391f05f3efc-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_f63f65b5,
 abstract = {In this paper, we question the necessity of levels of expert-guided abstraction in learning hard, statistically neutral classification tasks. We focus on two tasks, date calculation and parity-12, that are claimed to require intermediate levels of abstraction that must be defined by a human expert. We challenge this claim by demonstrating empirically that a single hidden-layer BP-SOM network can learn both tasks without guidance. Moreover, we analyze the network's solution for the parity-12 task and show that its solution makes use of an elegant intermediary checksum computation.},
 author = {Weijters, Ton and van den Bosch, Antal and Postma, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f63f65b503e22cb970527f23c9ad7db1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f63f65b503e22cb970527f23c9ad7db1-Metadata.json},
 openalex = {W2135999057},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f63f65b503e22cb970527f23c9ad7db1-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Statistically Neutral Tasks without Expert Guidance},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_f670ef5d,
 abstract = {Layered Sigmoid Belief Networks are directed graphical models in which the local conditional probabilities are parameterised by weighted sums of parental states. Learning and inference in such networks are generally intractable, and approximations need to be considered. Progress in learning these networks has been made by using variational procedures. We demonstrate, however, that variational procedures can be inappropriate for the equally important issue of inference - that is, calculating marginals of the network. We introduce an alternative procedure, based on assuming that the weighted input to a node is approximately Gaussian distributed. Our approach goes beyond previous Gaussian field assumptions in that we take into account correlations between parents of nodes. This procedure is specialized for calculating marginals and is significantly faster and simpler than the variational procedure.},
 author = {Barber, David and Sollich, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f670ef5d2d6bdf8f29450a970494dd64-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f670ef5d2d6bdf8f29450a970494dd64-Metadata.json},
 openalex = {W2109849264},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f670ef5d2d6bdf8f29450a970494dd64-Paper.pdf},
 publisher = {MIT Press},
 title = {Gaussian Fields for Approximate Inference in Layered Sigmoid Belief Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/f670ef5d2d6bdf8f29450a970494dd64-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_fc6709bf,
 abstract = {In this paper, we present Committee, a new multi-class learning algorithm related to the Winnow family of algorithms. Committee is an algorithm for combining the predictions of a set of sub-experts in the online mistake-bounded model of learning. A sub-expert is a special type of attribute that predicts with a distribution over a finite number of classes. Committee learns a linear function of sub-experts and uses this function to make class predictions. We provide bounds for Committee that show it performs well when the target can be represented by a few relevant sub-experts. We also show how Committee can be used to solve more traditional problems composed of attributes. This leads to a natural extension that learns on multi-class problems that contain both traditional attributes and sub-experts.},
 author = {Mesterharm, Chris},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fc6709bfdf0572f183c1a84ce5276e96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fc6709bfdf0572f183c1a84ce5276e96-Metadata.json},
 openalex = {W2141462139},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fc6709bfdf0572f183c1a84ce5276e96-Paper.pdf},
 publisher = {MIT Press},
 title = {A Multi-class Linear Learning Algorithm Related to Winnow},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/fc6709bfdf0572f183c1a84ce5276e96-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_fc9b003b,
 abstract = {Competition in the wireless telecommunications industry is rampant. To maintain profitability, wireless carriers must control churn, the loss of subscribers who switch from one carrier to another. We explore statistical techniques for churn prediction and, based on these predictions. an optimal policy for identifying customers to whom incentives should be offered to increase retention. Our experiments are based on a data base of nearly 47,000 U.S. domestic subscribers, and includes information about their usage, billing, credit, application, and complaint history. We show that under a wide variety of assumptions concerning the cost of intervention and the retention rate resulting from intervention, churn prediction and remediation can yield significant savings to a carrier. We also show the importance of a data representation crafted by domain experts.},
 author = {Mozer, Michael C and Wolniewicz, Richard and Grimes, David and Johnson, Eric and Kaushansky, Howard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fc9b003bb003a298c2ad0d05e4342bdc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fc9b003bb003a298c2ad0d05e4342bdc-Metadata.json},
 openalex = {W2106817624},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fc9b003bb003a298c2ad0d05e4342bdc-Paper.pdf},
 publisher = {MIT Press},
 title = {Churn Reduction in the Wireless Industry},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/fc9b003bb003a298c2ad0d05e4342bdc-Abstract.html},
 volume = {12},
 year = {1999}
}

@inproceedings{NIPS1999_fddd7938,
 abstract = {Virtual reality (VR) provides immersive and controllable experimental environments. It expands the bounds of possible evoked potential (EP) experiments by providing complex, dynamic environments in order to study cognition without sacrificing environmental control. VR also serves as a safe dynamic testbed for brain-computer interface (BCI) research. However, there has been some concern about detecting EP signals in a complex VR environment. This paper shows that EPs exist at red, green, and yellow lights in a virtual driving environment. Experimental results show the existence of the P3 EP at go and stop lights and the contingent negative variation (CNY) EP at lights. In order to test the feasibility of on-line recognition in VR, we looked at recognizing the P3 EP at red tights and the absence of this signal at yellow slow down lights. Recognition results show that the P3 may successfully be used to control the brakes of a VR car at lights.},
 author = {Bayliss, Jessica and Ballard, Dana},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fddd7938a71db5f81fcc621673ab67b7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fddd7938a71db5f81fcc621673ab67b7-Metadata.json},
 openalex = {W2151790668},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fddd7938a71db5f81fcc621673ab67b7-Paper.pdf},
 publisher = {MIT Press},
 title = {Recognizing Evoked Potentials in a Virtual Environment},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/fddd7938a71db5f81fcc621673ab67b7-Abstract.html},
 volume = {12},
 year = {1999}
}
