@inproceedings{NIPS1988_006f52e9,
 abstract = {We present a new hypothesis that the cerebellum plays a key role in actively controlling the acquisition of sensory information by the nervous system. In this paper we explore this idea by examining the function of a simple cerebellar-related behavior, the vestibulo-ocular reflex or VOR, in which eye movements are generated to minimize image slip on the retina during rapid head movements. Considering this system from the point of view of statistical estimation theory, our results suggest that the transfer function of the VOR, often regarded as a static or slowly modifiable feature of the system, should actually be continuously and rapidly changed during head movements. We further suggest that these changes are under the direct control of the cerebellar cortex and propose experiments to test this hypothesis.},
 author = {Paulin, Michael and Nelson, Mark and Bower, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/006f52e9102a8d3be2fe5614f42ba989-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/006f52e9102a8d3be2fe5614f42ba989-Metadata.json},
 openalex = {W2126497304},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/006f52e9102a8d3be2fe5614f42ba989-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Control of Sensory Acquisition: The Vestibulo-Ocular Reflex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/006f52e9102a8d3be2fe5614f42ba989-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_013d4071,
 abstract = {The olfactory bulb of mammals aids in the discrimination of odors. A mathematical model based on the bulbar anatomy and electrophysiology is described. Simulations produce a 35-60 Hz modulated activity coherent across the bulb, mimicing the observed field potentials. The decision states (for the odor information) here can be thought of as stable cycles, rather than point stable states typical of simpler neuro-computing models. Analysis and simulations show that a group of coupled non-linear oscillators are responsible for the oscillatory activities determined by the odor input, and that the bulb, with appropriate inputs from higher centers, can enhance or suppress the sensitivity to particular odors. The model provides a framework in which to understand the transform between odor input and the bulbar output to olfactory cortex.},
 author = {Li, Zhaoping and Hopfield, John J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/013d407166ec4fa56eb1e1f8cbe183b9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/013d407166ec4fa56eb1e1f8cbe183b9-Metadata.json},
 openalex = {W2114495760},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Modeling the Olfactory Bulb - Coupled Nonlinear Oscillators},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/013d407166ec4fa56eb1e1f8cbe183b9-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_02522a2b,
 abstract = {Parallelizable optimization techniques are applied to the problem of learning in feedforward neural networks. In addition to having superior convergence properties, optimization techniques such as the Polak-Ribiere method are also significantly more efficient than the Backpropagation algorithm. These results are based on experiments performed on small boolean learning problems and the noisy real-valued learning problem of hand-written character recognition.},
 author = {Kramer, Alan and Sangiovanni-Vincentelli, Alberto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/02522a2b2726fb0a03bb19f2d8d9524d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/02522a2b2726fb0a03bb19f2d8d9524d-Metadata.json},
 openalex = {W2112957975},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Efficient Parallel Learning Algorithms for Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/02522a2b2726fb0a03bb19f2d8d9524d-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_045117b0,
 abstract = {The concept of the stochastic Boltzmann machine (BM) is attractive for decision making and pattern classification purposes since the probability of attaining the network states is a function of the network energy. Hence, the probability of attaining particular energy minima may be associated with the probabilities of making certain decisions (or classifications). However, because of its stochastic nature, the complexity of the BM is fairly high and therefore such networks are not very likely to be used in practice. In this paper we suggest a way to alleviate this drawback by converting the stochastic BM into a deterministic network which we call the Boltzmann Perceptron Network (BPN). The BPN is functionally equivalent to the BM but has a feed-forward structure and low complexity. No annealing is required. The conditions under which such a conversion is feasible are given. A learning algorithm for the BPN based on the conjugate gradient method is also provided which is somewhat akin to the backpropagation algorithm.},
 author = {Yair, Eyal and Gersho, Allen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/045117b0e0a11a242b9765e79cbf113f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/045117b0e0a11a242b9765e79cbf113f-Metadata.json},
 openalex = {W2116849507},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Boltzmann Perceptron Network: A Multi-Layered Feed-Forward Network Equivalent to the Boltzmann Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/045117b0e0a11a242b9765e79cbf113f-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_06409663,
 abstract = {A neural network is applied to the problem of recognizing Kanji characters. Using a back propagation network learning algorithm, a three-layered, feed-forward network is trained to recognize similar handwritten Kanji characters. In addition, two new methods are utilized to make training effective. The recognition accuracy was higher than that of conventional methods. An analysis of connection weights showed that trained networks can discern the hierarchical structure of Kanji characters. This strategy of trained networks makes high recognition accuracy possible. Our results suggest that neural networks are very effective for Kanji character recognition.},
 author = {Mori, Yoshihiro and Yokosawa, Kazuhiko},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/06409663226af2f3114485aa4e0a23b4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/06409663226af2f3114485aa4e0a23b4-Metadata.json},
 openalex = {W2156867668},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/06409663226af2f3114485aa4e0a23b4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Networks that Learn to Discriminate Similar Kanji Characters},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/06409663226af2f3114485aa4e0a23b4-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_069059b7,
 author = {Alkon, Daniel and Quek, Francis and Vogl, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/069059b7ef840f0c74a814ec9237b6ec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/069059b7ef840f0c74a814ec9237b6ec-Metadata.json},
 openalex = {W2107977004},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/069059b7ef840f0c74a814ec9237b6ec-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Computer Modeling of Associative Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/069059b7ef840f0c74a814ec9237b6ec-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_0777d5c1,
 abstract = {The statistical use of a particular classic form of a connectionist system, the multilayer perceptron (MLP), is described in the context of the recognition of continuous speech. A discriminant hidden Markov model (HMM) is defined, and it is shown how a particular MLP with contextual and extra feedback input units can be considered as a general form of such a Markov model. A link between these discriminant HMMs, trained along the Viterbi algorithm, and any other approach based on least mean square minimization of an error function (LMSE) is established. It is shown theoretically and experimentally that the outputs of the MLP (when trained along the LMSE or the entropy criterion) approximate the probability distribution over output classes conditioned on the input, i.e. the maximum a posteriori probabilities. Results of a series of speech recognition experiments are reported. The possibility of embedding MLP into HMM is described. Relations with other recurrent networks are also explained.< <ETX xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">&gt;</ETX>},
 author = {Bourlard, Herv\'{e} and Wellekens, C. J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0777d5c17d4066b82ab86dff8a46af6f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0777d5c17d4066b82ab86dff8a46af6f-Metadata.json},
 openalex = {W2140539590},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0777d5c17d4066b82ab86dff8a46af6f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Links between Markov models and multilayer perceptrons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/0777d5c17d4066b82ab86dff8a46af6f-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_07e1cd7d,
 abstract = {This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and improving its performance. The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units. This skeletonization technique can be used to simplify networks by eliminating units that convey redundant information; to improve learning performance by first learning with spare hidden units and then trimming the unnecessary ones away, thereby constraining generalization; and to understand the behavior of networks in terms of minimal rules.},
 author = {Mozer, Michael C and Smolensky, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/07e1cd7dca89a1678042477183b7ac3f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/07e1cd7dca89a1678042477183b7ac3f-Metadata.json},
 openalex = {W2134273960},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/07e1cd7dca89a1678042477183b7ac3f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/07e1cd7dca89a1678042477183b7ac3f-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_0a09c884,
 abstract = {We present a simplified model of the micromechanics of the human cochlea, realized with electrical elements. Simulation of the model shows that it retains four signal processing features whose importance we argue on the basis of engineering logic and evolutionary evidence. Furthermore, just as the cochlea does, the model achieves massively parallel signal processing in a structurally economic way, by means of shared elements. By extracting what we believe are the five essential features of the cochlea, we hope to design a useful front-end filter to process acoustic images and to obtain a better understanding of the auditory system.},
 author = {Feld, David and Eisenberg, Joe and Lewis, Edwin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0a09c8844ba8f0936c20bd791130d6b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0a09c8844ba8f0936c20bd791130d6b6-Metadata.json},
 openalex = {W2109204217},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0a09c8844ba8f0936c20bd791130d6b6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Passive Shared Element Analog Electrical Cochlea},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/0a09c8844ba8f0936c20bd791130d6b6-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_0f28b5d4,
 abstract = {We propose a new neural network model and its learning algorithm. The proposed neural network consists of four layers - input, hidden, output and final output layers. The hidden and output layers are multiple. Using the proposed SICL(Spread Pattern Information and Cooperative Learning) algorithm, it is possible to learn analog data accurately and to obtain smooth outputs. Using this neural network, we have developed a speech production system consisting of a phonemic symbol production subsystem and a speech parameter production subsystem. We have succeeded in producing natural speech waves with high accuracy.},
 author = {Komura, Mitsuo and Tanaka, Akio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0f28b5d49b3020afeecd95b4009adf4c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0f28b5d49b3020afeecd95b4009adf4c-Metadata.json},
 openalex = {W2118445167},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0f28b5d49b3020afeecd95b4009adf4c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Speech Production Using A Neural Network with a Cooperative Learning Mechanism},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/0f28b5d49b3020afeecd95b4009adf4c-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_1385974e,
 abstract = {ALVIS is a reinforcement-based connectionist architecture that learns associative maps in continuous multidimensional environments. The discovered locations of positive and negative reinforcements are recorded in do be and don't be subnetworks, respectively. The outputs of the subnetworks relevant to the current goal are combined and compared with the current location to produce an error vector. This vector is backpropagated through a motor-perceptual mapping network to produce an action vector that leads the system towards do-be locations and away from don't-be locations. ALVIS is demonstrated with a simulated robot posed a target-seeking task.},
 author = {Ackley, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1385974ed5904a438616ff7bdb3f7439-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1385974ed5904a438616ff7bdb3f7439-Metadata.json},
 openalex = {W2153057117},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Associative Learning via Inhibitory Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/1385974ed5904a438616ff7bdb3f7439-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_140f6969,
 abstract = {We have fabricated a test chip in 2 micron CMOS that can perform supervised learning in a manner similar to the Boltzmann machine. Patterns can be presented to it at 100,000 per second. The chip learns to solve the XOR problem in a few milliseconds. We also have demonstrated the capability to do unsupervised competitive learning with it. The functions of the chip components are examined and the performance is assessed.},
 author = {Alspector, Joshua and Gupta, Bhusan and Allen, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/140f6969d5213fd0ece03148e62e461e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/140f6969d5213fd0ece03148e62e461e-Metadata.json},
 openalex = {W1731410993},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Performance of a stochastic learning microchip},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_149e9677,
 author = {Konishi, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/149e9677a5989fd342ae44213df68868-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/149e9677a5989fd342ae44213df68868-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/149e9677a5989fd342ae44213df68868-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Song Learning in Birds},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/149e9677a5989fd342ae44213df68868-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_1afa34a7,
 abstract = {We have used analog VLSI technology to model a class of small oscillating biological neural circuits known as central pattern generators (CPG). These circuits generate rhythmic patterns of activity which drive locomotor behaviour in the animal. We have designed, fabricated, and tested a model neuron circuit which relies on many of the same mechanisms as a biological central pattern generator neuron, such as delays and internal feedback. We show that this neuron can be used to build several small circuits based on known biological CPG circuits, and that these circuits produce patterns of output which are very similar to the observed biological patterns.},
 author = {Ryckebusch, Sylvie and Bower, James and Mead, Carver},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1afa34a7f984eeabdbb0a7d494132ee5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1afa34a7f984eeabdbb0a7d494132ee5-Metadata.json},
 openalex = {W2126362583},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Modeling Small Oscillating Biological Networks in Analog VLSI},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/1afa34a7f984eeabdbb0a7d494132ee5-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_1c9ac015,
 abstract = {Rumelhart (1987), has proposed a method for choosing minimal or representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units, (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart's minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general, the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima.},
 author = {Hanson, Stephen and Pratt, Lorien},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Metadata.json},
 openalex = {W2120972216},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Comparing Biases for Minimal Network Construction with Back-Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/1c9ac0159c94d8d0cbedc973445af2da-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_1d7f7abc,
 abstract = {We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 &lt; ∊ ≤ 1/8. We show that if m ≥ O(W/∊ log N/∊) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 − ∊/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 − ∊ of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than Ω(W/∊) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 − ∊ fraction of the future test examples.},
 author = {Baum, Eric and Haussler, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1d7f7abc18fcb43975065399b0d1e48e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1d7f7abc18fcb43975065399b0d1e48e-Metadata.json},
 openalex = {W2165758113},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1d7f7abc18fcb43975065399b0d1e48e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {What Size Net Gives Valid Generalization?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/1d7f7abc18fcb43975065399b0d1e48e-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_1ff8a7b5,
 abstract = {This paper describes a CMOS artificial neuron. The circuit is directly derived from the voltage-gated channel model of neural membrane, has low power dissipation, and small layout geometry. The principal motivations behind this work include a desire for high performance, more accurate neuron emulation, and the need for higher density in practical neural network implementations.},
 author = {Meador, Jack and Cole, Clint},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Metadata.json},
 openalex = {W2109212534},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Low-Power CMOS Circuit Which Emulates Temporal Electrical Properties of Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_202cb962,
 abstract = {What follows extends some of our results of [1] on learning from examples in layered feed-forward networks of units. In particular we examine what happens when the ntunber of layers is large or when the connectivity between layers is local and investigate some of the properties of an autoassociative algorithm. Notation will be as in [1] where additional motivations and references can be found. It is usual to criticize networks because linear functions do not compute and because several layers can always be reduced to one by the proper multiplication of matrices. However this is not the point of view adopted here. It is assumed that the architecture of the network is given (and could perhaps depend on external constraints) and the purpose is to understand what happens during the learning phase, what strategies are adopted by a synaptic weights modifying algorithm, ... [see also Cottrell et al. (1988) for an example of an application and the work of Linsker (1988) on the emergence of feature detecting units in networks}.},
 author = {Baldi, Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/202cb962ac59075b964b07152d234b70-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/202cb962ac59075b964b07152d234b70-Metadata.json},
 openalex = {W2171902495},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/202cb962ac59075b964b07152d234b70-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Linear Learning: Landscapes and Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/202cb962ac59075b964b07152d234b70-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_2723d092,
 abstract = {This paper is concerced with the use of error back-propagation in phonetic classification. Our objective is to investigate the basic characteristics of back-propagation, and study how the framework of multi-layer perceptrons can be exploited in phonetic recognition. We explore issues such as integration of heterogeneous sources of information, conditions that can affect performance of phonetic classification, internal representations, comparisons with traditional pattern classification techniques, comparisons of different error metrics, and initialization of the network. Our investigation is performed within a set of experiments that attempts to recognize the 16 vowels in American English independent of speaker. Our results are comparable to human performance.},
 author = {Leung, Hong and Zue, Victor W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2723d092b63885e0d7c260cc007e8b9d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2723d092b63885e0d7c260cc007e8b9d-Metadata.json},
 openalex = {W2153066151},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2723d092b63885e0d7c260cc007e8b9d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Applications of Error Back-Propagation to Phonetic Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/2723d092b63885e0d7c260cc007e8b9d-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_2a79ea27,
 abstract = {A design for a fully analog version of a self-organizing feature map neural network has been completed. Several parts of this design are in fabrication. The feature map algorithm was modified to accommodate circuit solutions to the various computations required. Performance effects were measured by simulating the design as part of a frontend for a speech recognition system. Circuits are included to implement both activation computations and weight adaption or learning. External access to the analog weight values is provided to facilitate weight initialization, testing and static storage. This fully analog implementation requires an order of magnitude less area than a comparable digital/analog hybrid version developed earlier.},
 author = {Mann, James and Gilbert, Sheldon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2a79ea27c279e471f4d180b08d62b00a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2a79ea27c279e471f4d180b08d62b00a-Metadata.json},
 openalex = {W2111352882},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Analog Self-Organizing Neural Network Chip},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/2a79ea27c279e471f4d180b08d62b00a-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_2b24d495,
 abstract = {A new learning algorithm for the storage of static and periodic attractors in biologically inspired recurrent analog neural networks is introduced. For a network of n nodes, n static or n/2 periodic attractors may be stored. The algorithm allows programming of the network vector field independent of the patterns to be stored. Stability of patterns, basin geometry, and rates of convergence may be controlled. For orthonormal patterns, the learning operation reduces to a kind of periodic outer product rule that allows local, additive, commutative, incremental learning. Standing or traveling wave cycles may be stored to mimic the kind of oscillating spatial patterns that appear in the neural activity of the olfactory bulb and prepyriform cortex during inspiration and suffice, in the bulb, to predict the pattern recognition behavior of rabbits in classical conditioning experiments. These attractors arise, during simulated inspiration, through a multiple Hopf bifurcation, which can act as a critical decision point for their selection by a very small input pattern.},
 author = {Baird, Bill},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2b24d495052a8ce66358eb576b8912c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2b24d495052a8ce66358eb576b8912c8-Metadata.json},
 openalex = {W2101602849},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Bifurcation Theory Approach to the Programming of Periodic Attractors in Network Models of Olfactory Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_2b44928a,
 abstract = {Research in artificial neural networks has generally emphasized homogeneous architectures. In contrast, the nervous systems of natural animals exhibit great heterogeneity in both their elements and patterns of interconnection. This heterogeneity is crucial to the flexible generation of behavior which is essential for survival in a complex, dynamic environment. It may also provide powerful insights into the design of artificial neural networks. In this paper, we describe a heterogeneous neural network for controlling the walking of a simulated insect. This controller is inspired by the neuroethological and neurobiological literature on insect locomotion. It exhibits a variety of statically stable gaits at different speeds simply by varying the tonic activity of a single cell. It can also adapt to perturbations as a natural consequence of its design.},
 author = {Beer, Randall and Chiel, Hillel and Sterling, Leon S.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2b44928ae11fb9384c4cf38708677c48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2b44928ae11fb9384c4cf38708677c48-Metadata.json},
 openalex = {W2106433364},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2b44928ae11fb9384c4cf38708677c48-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Heterogeneous Neural Networks for Adaptive Behavior in Dynamic Environments},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/2b44928ae11fb9384c4cf38708677c48-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_31fefc0e,
 abstract = {We describe pulse - stream firing integrated circuits that implement asynchronous analog neural networks. Synaptic weights are stored dynamically, and weighting uses time-division of the neural pulses from a signalling neuron to a receiving neuron. MOS transistors in their ON state act as variable resistors to control a capacitive discharge, and time-division is thus achieved by a small synapse circuit cell. The VLSI chip set design uses 2.5 µm CMOS technology.},
 author = {Hamilton, Alister and Murray, Alan and Tarassenko, Lionel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/31fefc0e570cb3860f2a6d4b38c6490d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/31fefc0e570cb3860f2a6d4b38c6490d-Metadata.json},
 openalex = {W2143310639},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Programmable Analog Pulse-Firing Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_36366388,
 abstract = {Reconstructing a surface from sparse sensory data is a well-known problem in computer vision. This paper describes an experimental analog VLSI chip for smooth surface interpolation from sparse depth data. An eight-node ID network was designed in 3µm CMOS and successfully tested. The network minimizes a second-order or thin-plate energy of the surface. The circuit directly implements the coupled depth/slope model of surface reconstruction (Harris, 1987). In addition, this chip can provide Gaussian-like smoothing of images.},
 author = {Harris, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3636638817772e42b59d74cff571fbb3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3636638817772e42b59d74cff571fbb3-Metadata.json},
 openalex = {W2143899083},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3636638817772e42b59d74cff571fbb3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Analog VLSI Chip for Thin-Plate Surface Interpolation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/3636638817772e42b59d74cff571fbb3-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_37a749d8,
 abstract = {The weakly electric fish, Gnathonemus petersii, explores its environment by generating pulsed electric fields and detecting small perturbations in the fields resulting from nearby objects. Accordingly, the fish detects and discriminates objects on the basis of a sequence of electric images whose temporal and spatial properties depend on the timing of the fish's electric organ discharge and its body position relative to objects in its environment. We are interested in investigating how these fish utilize timing and body-position during exploration to aid in object discrimination. We have developed a finite-element simulation of the fish's self-generated electric fields so as to reconstruct the electrosensory consequences of body position and electric organ discharge timing in the fish. This paper describes this finite-element simulation system and presents preliminary electric field measurements which are being used to tune the simulation.},
 author = {Rasnow, Brian and Assad, Christopher and Nelson, Mark and Bower, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/37a749d808e46495a8da1e5352d03cae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/37a749d808e46495a8da1e5352d03cae-Metadata.json},
 openalex = {W2104589364},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Simulation and Measurement of the Electric Fields Generated by Weakly Electric Fish},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/37a749d808e46495a8da1e5352d03cae-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_38b3eff8,
 abstract = {A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. al. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two-dimensional examples.},
 author = {Singhal, Sharad and Wu, Lance},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/38b3eff8baf56627478ec76a704e9b52-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/38b3eff8baf56627478ec76a704e9b52-Metadata.json},
 openalex = {W2112462566},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/38b3eff8baf56627478ec76a704e9b52-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Training Multilayer Perceptrons with the Extended Kalman Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/38b3eff8baf56627478ec76a704e9b52-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_3988c7f8,
 abstract = {We describe an electronic photoreceptor circuit that is sensitive to small changes in incident light intensity. The sensitivity to changes in the intensity is achieved by feeding back to the input a filtered version of the output. The feedback loop includes a hysteretic element. The circuit behaves in a manner reminiscent of the gain control properties and temporal responses of a variety of retinal cells, particularly retinal bipolar cells. We compare the thresholds for detection of intensity increments by a human and by the circuit. Both obey Weber's law and for both the temporal contrast sensitivities are nearly identical.},
 author = {Delbr\"{u}ck, Tobi and Mead, C. A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3988c7f88ebcb58c6ce932b957b6f332-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3988c7f88ebcb58c6ce932b957b6f332-Metadata.json},
 openalex = {W2143935935},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3988c7f88ebcb58c6ce932b957b6f332-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Electronic Photoreceptor Sensitive to Small Changes in Intensity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/3988c7f88ebcb58c6ce932b957b6f332-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_3def184a,
 abstract = {We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with nonlinear functions such as sigmoids.},
 author = {Blum, Avrim and Rivest, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3def184ad8f4755ff269862ea77393dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3def184ad8f4755ff269862ea77393dd-Metadata.json},
 openalex = {W196871588},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Training a 3-node neural network is NP-complete},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/3def184ad8f4755ff269862ea77393dd-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_42a0e188,
 abstract = {This research involves a method for finding global maxima in constraint satisfaction networks. It is an annealing process but, unlike most others, requires no annealing schedule. Temperature is instead determined locally by units at each update, and thus all processing is done at the unit level. There are two major practical benefits to processing this way: 1) processing can continue in 'bad' areas of the network, while 'good' areas remain stable, and 2) processing continues in the 'bad' areas, as long as the constraints remain poorly satisfied (i.e. it does not stop after some predetermined number of cycles). As a result, this method not only avoids the kludge of requiring an externally determined annealing schedule, but it also finds global maxima more quickly and consistently than externally scheduled systems (a comparison to the Boltzmann machine (Ackley et al, 1985) is made). Finally, implementation of this method is computationally trivial.},
 author = {Leinbach, Jared},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/42a0e188f5033bc65bf8d78622277c4e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/42a0e188f5033bc65bf8d78622277c4e-Metadata.json},
 openalex = {W2137517408},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Automatic Local Annealing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/42a0e188f5033bc65bf8d78622277c4e-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_47d1e990,
 abstract = {A nonlinearity is required before matched filtering in minimum error receivers when additive noise is present which is impulsive and highly non-Gaussian. Experiments were performed to determine whether the correct clipping nonlinearity could be provided by a single-input single-output multi-layer perceptron trained with back propagation. It was found that a multi-layer perceptron with one input and output node, 20 nodes in the first hidden layer, and 5 nodes in the second hidden layer could be trained to provide a clipping nonlinearity with fewer than 5,000 presentations of noiseless and corrupted waveform samples. A network trained at a relatively high signal-to-noise (S/N) ratio and then used as a front end for a linear matched filter detector greatly reduced the probability of error. The clipping nonlinearity formed by this network was similar to that used in current receivers designed for impulsive noise and provided similar substantial improvements in performance.},
 author = {Lippmann, Richard P and Beckman, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/47d1e990583c9c67424d369f3414728e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/47d1e990583c9c67424d369f3414728e-Metadata.json},
 openalex = {W2167733724},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/47d1e990583c9c67424d369f3414728e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive Neural Net Preprocessing for Signal Detection in Non-Gaussian Noise},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/47d1e990583c9c67424d369f3414728e-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_4c56ff4c,
 abstract = {The primate visual system learns to recognize the true direction of pattern motion using local detectors only capable of detecting the component of motion perpendicular to the orientation of the moving edge. A multilayer feedforward network model similar to Linsker's model was presented with input patterns each consisting of randomly oriented contours moving in a particular direction. Input layer units are granted component direction and speed tuning curves similar to those recorded from neurons in primate visual area VI that project to area MT. The network is trained on many such patterns until most weights saturate. A proportion of the units in the second layer solve the aperture problem (e.g., show the same direction-tuning curve peak to plaids as to gratings), resembling pattern-direction selective neurons, which first appear in area MT.},
 author = {Sereno, Martin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/4c56ff4ce4aaf9573aa5dff913df997a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/4c56ff4ce4aaf9573aa5dff913df997a-Metadata.json},
 openalex = {W2129436656},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning the Solution to the Aperture Problem for Pattern Motion with a Hebb Rule},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/4c56ff4ce4aaf9573aa5dff913df997a-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_4c5bde74,
 abstract = {We have developed a graphically oriented, general purpose simulation system to facilitate the modeling of neural networks. The simulator is implemented under UNIX and X-windows and is designed to support simulations at many levels of detail. Specifically, it is intended for use in both applied network modeling and in the simulation of detailed, realistic, biologically-based models. Examples of current models developed under this system include mammalian olfactory bulb and cortex, invertebrate central pattern generators, as well as more abstract connectionist simulations.},
 author = {Wilson, Matthew and Bhalla, Upinder and Uhley, John and Bower, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/4c5bde74a8f110656874902f07378009-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/4c5bde74a8f110656874902f07378009-Metadata.json},
 openalex = {W2152274602},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/4c5bde74a8f110656874902f07378009-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {GENESIS: A System for Simulating Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/4c5bde74a8f110656874902f07378009-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_5878a7ab,
 abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
 author = {Braitenberg, Valentino},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5878a7ab84fb43402106c575658472fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5878a7ab84fb43402106c575658472fa-Metadata.json},
 openalex = {W2946547492},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5878a7ab84fb43402106c575658472fa-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Architecture Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/5878a7ab84fb43402106c575658472fa-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_5ef05993,
 abstract = {We introduce a learning algorithm for multilayer neural networks composed of binary linear threshold elements. Whereas existing algorithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal representations as the fundamental entities to be determined. Once a correct set of internal representations is arrived at, the weights are found by the local and biologically plausible Perceptron Learning Rule (PLR). We tested our learning algorithm on four problems: adjacency, symmetry, parity and combined symmetry-parity.},
 author = {Grossman, Tal and Meir, Ronny and Domany, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5ef059938ba799aaa845e1c2e8a762bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5ef059938ba799aaa845e1c2e8a762bd-Metadata.json},
 openalex = {W2146704595},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5ef059938ba799aaa845e1c2e8a762bd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning by Choice of Internal Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/5ef059938ba799aaa845e1c2e8a762bd-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_5f93f983,
 abstract = {MOS charge storage has been demonstrated as an effective method to store the weights in VLSI implementations of neural network models by several workers. However, to achieve the full power of a VLSI implementation of an adaptive algorithm, the learning operation must built into the circuit. We have fabricated and tested a circuit ideal for this purpose by connecting a pair of capacitors with a CCD like structure, allowing for variable size weight changes as well as a weight decay operation. A 2.5µ CMOS version achieves better than 10 bits of dynamic range in a 140µ × 350µ area. A 1.25µ chip based upon the same cell has 1104 weights on a 3.5mm × 6.0mm die and is capable of peak learning rates of at least 2 × 109 weight changes per second.},
 author = {Schwartz, Daniel and Howard, R. and Hubbard, Wayne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5f93f983524def3dca464469d2cf9f3e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5f93f983524def3dca464469d2cf9f3e-Metadata.json},
 openalex = {W2108498433},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive Neural Networks Using MOS Charge Storage},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/5f93f983524def3dca464469d2cf9f3e-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_5fd0b37c,
 abstract = {I will describe my recent results on the automatic development of fixed-width recursive distributed representations of variable-sized hierarchal data structures. One implication of this work is that certain types of AI-style data-structures can now be represented in fixed-width analog vectors. Simple inferences can be performed using the type of pattern associations that neural networks excel at Another implication arises from noting that these representations become self-similar in the limit. Once this door to chaos is opened, many interesting new questions about the representational basis of intelligence emerge, and can (and will) be discussed.},
 author = {Pollack, Jordan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Metadata.json},
 openalex = {W2101618401},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Implications of Recursive Distributed Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/5fd0b37cd7dbbb00f97ba6ce92bf5add-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_65b9eea6,
 abstract = {A pool of handwritten signatures is used to train a neural network for the task of deciding whether or not a given signature is a forgery. The network is a feedforward net, with a binary image as input. There is a hidden layer, with a single unit output layer. The weights are adjusted according to the backpropagation algorithm. The signatures are entered into a C software program through the use of a Datacopy Electronic Digitizing Camera. The binary signatures are normalized and centered. The performance is examined as a function of the training set and network structure. The best scores are on the order of 2% true signature rejection with 2-4% false signature acceptance.},
 author = {Wilkinson, Timothy and Mighell, Dorothy and Goodman, Joseph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Metadata.json},
 openalex = {W2121671444},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Backpropagation and Its Application to Handwritten Signature Verification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_65ded535,
 abstract = {We have mathematically shown that cortical maps in the primary sensory cortices can be reproduced by using three hypotheses which have physiological basis and meaning. Here, our main focus is on ocular dominance column formation in the primary visual cortex. Monte Carlo simulations on the segregation of ipsilateral and contralateral afferent terminals are carried out. Based on these, we show that almost all the physiological experimental results concerning the ocular dominance patterns of cats and monkeys reared under normal or various abnormal visual conditions can be explained from a viewpoint of the phase transition phenomena.},
 author = {Tanaka, Shigeru},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/65ded5353c5ee48d0b7d48c591b8f430-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/65ded5353c5ee48d0b7d48c591b8f430-Metadata.json},
 openalex = {W2101787322},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Theory of Self-Organization of Cortical Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/65ded5353c5ee48d0b7d48c591b8f430-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_6974ce5a,
 abstract = {We describe an adaptive network, TIN2, that learns the transition function of a sequential system from observations of its behavior. It integrates two subnets, TIN-1 (Winter, Ryan and Turner, 1987) and TIN-2. TIN-2 constructs state representations from examples of system behavior, and its dynamics are the main topics of the paper. TIN-1 abstracts transition functions from noisy state representations and environmental data during training, while in operation it produces sequences of transitions in response to variations in input. Dynamics of both nets are based on the Adaptive Resonance Theory of Carpenter and Grossberg (1987). We give results from an experiment in which TIN2 learned the behavior of a system that recognizes strings with an even number of 1's.},
 author = {Winter, C.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6974ce5ac660610b44d9b9fed0ff9548-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6974ce5ac660610b44d9b9fed0ff9548-Metadata.json},
 openalex = {W2105315226},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Adaptive Network That Learns Sequences of Transitions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/6974ce5ac660610b44d9b9fed0ff9548-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_698d51a1,
 abstract = {A time delay in the response of the neurons in a network can induce sustained oscillation and chaos. We present a stability criterion based on local stability analysis to prevent sustained oscillation in symmetric delay networks, and show an example of chaotic dynamics in a non-symmetric delay network.},
 author = {Marcus, Charles and Westervelt, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/698d51a19d8a121ce581499d7b701668-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/698d51a19d8a121ce581499d7b701668-Metadata.json},
 openalex = {W2157970746},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/698d51a19d8a121ce581499d7b701668-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dynamics of Analog Neural Networks with Time Delay},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/698d51a19d8a121ce581499d7b701668-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_6c4b761a,
 abstract = {We present and rigorously analyze a generalization of the Winner-Take-All Network: the K-Winners-Take-All Network. This network identifies the K largest of a set of N real numbers. The network model used is the continuous Hopfield model.},
 author = {Majani, E. and Erlanson, Ruth and Abu-Mostafa, Yaser},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6c4b761a28b734fe93831e3fb400ce87-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6c4b761a28b734fe93831e3fb400ce87-Metadata.json},
 openalex = {W2099896495},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6c4b761a28b734fe93831e3fb400ce87-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {On the K-Winners-Take-All Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/6c4b761a28b734fe93831e3fb400ce87-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_6cdd60ea,
 abstract = {An improved learning paradigm that offers a significant reduction in computation time during the supervised learning phase is described. It is based on extending the role that the neuron plays in artificial neural systems. Prior work has regarded the neuron as a strictly passive, non-linear processing element, and the synapse on the other hand as the primary source of information processing and knowledge retention. In this work, the role of the neuron is extended insofar as allowing its parameters to adaptively participate in the learning phase. The temperature of the sigmoid function is an example of such a parameter. During learning, both the synaptic interconnection weights wijm and the neuronal temperatures Tim are optimized so as to capture the knowledge contained within the training set. The method allows each neuron to possess and update its own characteristic local temperature. This algorithm has been applied to logic type of problems such as the XOR or parity problem, resulting in a significant decrease in the required number of training cycles.},
 author = {Tawel, Raoul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6cdd60ea0045eb7a6ec44c54d29ed402-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6cdd60ea0045eb7a6ec44c54d29ed402-Metadata.json},
 openalex = {W2158037496},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Does the Neuron "Learn" like the Synapse?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/6cdd60ea0045eb7a6ec44c54d29ed402-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_73278a4a,
 abstract = {Preliminary results on speaker-independant speech recognition are reported. A method that combines expertise on neural networks with expertise on speech recognition is used to build the recognition systems. For transient sounds, event-driven property extractors with variable resolution in the time and frequency domains are used. For sonorant speech, a model of the human auditory system is preferred to FFT as a front-end module.},
 author = {Bengio, Yoshua and Cardin, R\'{e}gis and de Mori, Renato and Cosi, Piero},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/73278a4a86960eeb576a8fd4c9ec6997-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/73278a4a86960eeb576a8fd4c9ec6997-Metadata.json},
 openalex = {W2140846220},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Use of Multi-Layered Networks for Coding Speech with Phonetic Features},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/73278a4a86960eeb576a8fd4c9ec6997-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_76dc611d,
 abstract = {We discuss synthetic receptors for haptic sensing. These are based on magnetic field sensors (Hall effect structures) fabricated using standard CMOS technologies. These receptors, biased with a small permanent magnet can detect the presence of ferro or ferri-magnetic objects in the vicinity of the sensor. They can also detect the magnitude and direction of the magnetic field.},
 author = {Andreou, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/76dc611d6ebaafc66cc0879c71b5db5c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/76dc611d6ebaafc66cc0879c71b5db5c-Metadata.json},
 openalex = {W2155663744},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Electronic Receptors for Tactile/Haptic Sensing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/76dc611d6ebaafc66cc0879c71b5db5c-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_7e7757b1,
 abstract = {This report describes the design of a programmable general purpose analog neural computer and simulator. It is intended primarily for real-world real-time computations such as analysis of visual or acoustical patterns, robotics and the development of special purpose neural nets. The machine is scalable and composed of interconnected modules containing arrays of neurons, modifiable synapses and switches. It runs entirely in analog mode but connection architecture, synaptic gains and time constants as well as neuron parameters are set digitally. Each neuron has a limited number of inputs and can be connected to any but not all other neurons. For the determination of synaptic gains and the implementation of learning algorithms the neuron outputs are multiplexed, A/D converted and stored in digital memory. Even at moderate size of 103 to 105 neurons computational speed is expected to exceed that of any current digital computer.},
 author = {Mueller, Paul and Van der Spiegel, Jan and Blackman, David and Chiu, Timothy and Clare, Thomas and Dao, Joseph and Donham, Christopher and Hsieh, Tzu-pu and Loinaz, Marc},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7e7757b1e12abcb736ab9a754ffb617a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7e7757b1e12abcb736ab9a754ffb617a-Metadata.json},
 openalex = {W2114248009},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7e7757b1e12abcb736ab9a754ffb617a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Programmable Analog Neural Computer and Simulator},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/7e7757b1e12abcb736ab9a754ffb617a-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_7ef605fc,
 abstract = {We discuss in this paper architectures for executing probabilistic rule-bases in a parallel manner, using as a theoretical basis recently introduced information-theoretic models. We will begin by describing our (non-neural) learning algorithm and theory of quantitative rule modelling, followed by a discussion on the exact nature of two particular models. Finally we work through an example of our approach, going from database to rules to inference network, and compare the network's performance with the theoretical limits for specific problems.},
 author = {Goodman, Rodney and Miller, John and Smyth, Padhraic},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7ef605fc8dba5425d6965fbd4c8fbe1f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7ef605fc8dba5425d6965fbd4c8fbe1f-Metadata.json},
 openalex = {W2105381734},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7ef605fc8dba5425d6965fbd4c8fbe1f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Information Theoretic Approach to Rule-Based Connectionist Expert Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/7ef605fc8dba5425d6965fbd4c8fbe1f-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_7f1de29e,
 abstract = {The Space Environment Laboratory in Boulder has collaborated with the University of Colorado to construct a small expert system for solar flare forecasting, called THEO. It performed as well as a skilled human forecaster. We have constructed TheoNet, a three-layer back-propagation connectionist network that learns to forecast flares as well as THEO does. TheoNet's success suggests that a connectionist network can perform the task of knowledge engineering automatically. A study of the internal representations constructed by the network may give insights to the microstructure of reasoning processes in the human brain.},
 author = {Fozzard, Richard and Bradshaw, Gary and Ceci, Louis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7f1de29e6da19d22b51c68001e7e0e54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7f1de29e6da19d22b51c68001e7e0e54-Metadata.json},
 openalex = {W2166574517},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7f1de29e6da19d22b51c68001e7e0e54-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Connectionist Expert System that Actually Works},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/7f1de29e6da19d22b51c68001e7e0e54-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_7f6ffaa6,
 abstract = {A great deal of interest has recently been focused on theories concerning parallel distributed processing in central nervous systems. In particular, many researchers have become very interested in the structure and function of maps in sensory systems. As defined in a recent review (Knudsen et al, 1987), a is an array of nerve cells, within which there is a systematic variation in the tuning of neighboring cells for a particular parameter. For example, the projection from retina to visual cortex is a relatively simple topographic map; each cortical hypercolumn itself contains a more complex computational map of preferred line orientation representing the angle of tilt of a simple line stimulus.},
 author = {Miller, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7f6ffaa6bb0b408017b62254211691b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7f6ffaa6bb0b408017b62254211691b5-Metadata.json},
 openalex = {W2147363389},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7f6ffaa6bb0b408017b62254211691b5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Cricket Wind Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/7f6ffaa6bb0b408017b62254211691b5-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_812b4ba2,
 abstract = {ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand.},
 author = {Pomerleau, Dean A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Metadata.json},
 openalex = {W2167224731},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {ALVINN: An Autonomous Land Vehicle in a Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_82161242,
 abstract = {A class of fast, supervised learning algorithms is presented. They use local representations, hashing, and multiple scales of resolution to approximate functions which are piece-wise continuous. Inspired by Albus's CMAC model, the algorithms learn orders of magnitude more rapidly than typical implementations of back propagation, while often achieving comparable qualities of generalization. Furthermore, unlike most traditional function approximation methods, the algorithms are well suited for use in real time adaptive signal processing. Unlike simpler adaptive systems, such as linear predictive coding, the adaptive linear combiner, and the Kalman filter, the new algorithms are capable of efficiently capturing the structure of complicated non-linear systems. As an illustration, the algorithm is applied to the prediction of a chaotic timeseries.},
 author = {Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/82161242827b703e6acf9c726942a1e4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/82161242827b703e6acf9c726942a1e4-Metadata.json},
 openalex = {W2127385318},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/82161242827b703e6acf9c726942a1e4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Fast Learning in Multi-Resolution Hierarchies},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/82161242827b703e6acf9c726942a1e4-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_82aa4b0a,
 abstract = {Classifier systems are machine learning systems incotporating a genetic algorithm as the learning mechanism. Although they respond to inputs that neural networks can respond to, their internal structure, representation formalisms, and learning mechanisms differ markedly from those employed by neural network researchers in the same sorts of domains. As a result, one might conclude that these two types of machine learning formalisms are intrinsically different. This is one of two papers that, taken together, prove instead that classifier systems and neural networks are equivalent. In this paper, half of the equivalence is demonstrated through the description of a transformation procedure that will map classifier systems into neural networks that are isomorphic in behavior. Several alterations on the commonly-used paradigms employed by neural network researchers are required in order to make the transformation work. These alterations are noted and their appropriateness is discussed. The paper concludes with a discussion of the practical import of these results, and with comments on their extensibility.},
 author = {Davis, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/82aa4b0af34c2313a562076992e50aa3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/82aa4b0af34c2313a562076992e50aa3-Metadata.json},
 openalex = {W2149256131},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/82aa4b0af34c2313a562076992e50aa3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Mapping Classifier Systems Into Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/82aa4b0af34c2313a562076992e50aa3-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_8d5e957f,
 abstract = {We propose a parallel network of simple processors to find color boundaries irrespective of spatial changes in illumination, and to spread uniform colors within marked regions.},
 author = {Hurlbert, Anya and Poggio, Tomaso},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8d5e957f297893487bd98fa830fa6413-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8d5e957f297893487bd98fa830fa6413-Metadata.json},
 openalex = {W2141244323},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8d5e957f297893487bd98fa830fa6413-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Network for Image Segmentation Using Color},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/8d5e957f297893487bd98fa830fa6413-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_8f53295a,
 abstract = {Hardware implementation of neuromorphic algorithms is hampered by high degrees of connectivity. Functionally equivalent feedforward networks may be formed by using limited fan-in nodes and additional layers, but this complicates procedures for determining weight magnitudes. No direct mapping of weights exists between fully and limited-interconnect nets. Low-level nonlinearities prevent the formation of internal representations of widely separated spatial features and the use of gradient descent methods to minimize output error is hampered by error magnitude dissipation. The judicious use of linear summations or collection units is proposed as a solution.},
 author = {Walker, M. and Haghighi, S. and Afghan, A. and Akers, Larry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8f53295a73878494e9bc8dd6c3c7104f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8f53295a73878494e9bc8dd6c3c7104f-Metadata.json},
 openalex = {W2160153012},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8f53295a73878494e9bc8dd6c3c7104f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Training a Limited-Interconnect, Synthetic Neural IC},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/8f53295a73878494e9bc8dd6c3c7104f-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_8f855179,
 abstract = {Heiligenberg (1987) recently proposed a model to explain how sensory maps could enhance resolution through orderly arrangement of broadly tuned receptors. We have extended this model to the general case of polynomial weighting schemes and proved that the response function is also a polynomial of the same order. We further demonstrated that the Hermitian polynomials are eigenfunctions of the system. Finally we suggested a biologically plausible mechanism for sensory representation of external stimuli with resolution far exceeding the inter-receptor separation.},
 author = {Zhang, Jun and Miller, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8f85517967795eeef66c225f7883bdcb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8f85517967795eeef66c225f7883bdcb-Metadata.json},
 openalex = {W2136322951},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Model for Resolution Enhancement (Hyperacuity) in Sensory Representation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/8f85517967795eeef66c225f7883bdcb-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_903ce922,
 abstract = {SYREN is a connectionist model that uses temporal information in a speech signal for syllable recognition. It classifies the rates and directions of formant center transitions, and uses an adaptive method to associate transition events with each syllable. The system uses explicit spatial temporal representations through delay lines. SYREN uses implicit parametric temporal representations in formant transition classification through node activation onset, decay, and transition delays in sub-networks analogous to visual motion detector cells. SYREN recognizes 79% of six repetitions of 24 consonant-vowel syllables when tested on unseen data, and recognizes 100% of its training syllables.},
 author = {Smythe, Erich},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/903ce9225fca3e988c2af215d4e544d3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/903ce9225fca3e988c2af215d4e544d3-Metadata.json},
 openalex = {W2126295316},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/903ce9225fca3e988c2af215d4e544d3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Temporal Representations in a Connectionist Speech System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/903ce9225fca3e988c2af215d4e544d3-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_96da2f59,
 abstract = {Currently, the most complex spacecraft attitude determination and control tasks are ultimately governed by ground-based systems and personnel. Conventional on-board systems face severe computational bottlenecks introduced by serial microprocessors operating on inherently parallel problems. New computer architectures based on the anatomy of the human brain seem to promise high speed and fault-tolerant solutions to the limitations of serial processing. This paper discusses the latest applications of artificial neural networks to the problem of star pattern recognition for spacecraft attitude determination.},
 author = {Alvelda, Phillip and San Martin, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/96da2f590cd7246bbde0051047b0d6f7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/96da2f590cd7246bbde0051047b0d6f7-Metadata.json},
 openalex = {W2147370460},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Star Pattern Recognition for Spacecraft Attitude Determination and Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/96da2f590cd7246bbde0051047b0d6f7-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_9766527f,
 abstract = {The Parsing and Learning System (PALS) is a massively parallel self-tuning context-free parser. It is capable of parsing sentences of unbounded length mainly due to its parse-tree representation scheme. The system is capable of improving its parsing performance through the presentation of training examples.},
 author = {Santos, Eugene},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9766527f2b5d3e95d4a733fcfb77bd7e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9766527f2b5d3e95d4a733fcfb77bd7e-Metadata.json},
 openalex = {W2168696917},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Massively Parallel Self-Tuning Context-Free Parser},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/9766527f2b5d3e95d4a733fcfb77bd7e-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_9872ed9f,
 abstract = {The application of neural networks to the demodulation of spread-spectrum signals in a multiple-access environment is considered. This study is motivated in large part by the fact that, in a multiuser system, the conventional (matched filter) receiver suffers severe performance degradation as the relative powers of the interfering signals become large (the problem). Furthermore, the optimum receiver, which alleviates the near-far problem, is too complex to be of practical use. Receivers based on multi-layer perceptrons are considered as a simple and robust alternative to the optimum solution. The optimum receiver is used to benchmark the performance of the neural net receiver; in particular, it is proven to be instrumental in identifying the decision regions of the neural networks. The back-propagation algorithm and a modified version of it are used to train the neural net. An importance sampling technique is introduced to reduce the number of simulations necessary to evaluate the performance of neural nets. In all examples considered the proposed neural net receiver significantly outperforms the conventional receiver.},
 author = {Paris, Bernd-Peter and Orsak, Geoffrey and Varanasi, Mahesh and Aazhang, Behnaam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9872ed9fc22fc182d371c3e9ed316094-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9872ed9fc22fc182d371c3e9ed316094-Metadata.json},
 openalex = {W2111134378},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Net Receivers in Multiple Access-Communications},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/9872ed9fc22fc182d371c3e9ed316094-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_98dce83d,
 abstract = {We analyze a mathematical model for retinal directionally selective cells based on recent electrophysiological data, and show that its computation of motion direction is robust against noise and speed.},
 author = {Grzywacz, Norberto and Amthor, Franklin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/98dce83da57b0395e163467c9dae521b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/98dce83da57b0395e163467c9dae521b-Metadata.json},
 openalex = {W2143409073},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/98dce83da57b0395e163467c9dae521b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Computationally Robust Anatomical Model for Retinal Directional Selectivity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/98dce83da57b0395e163467c9dae521b-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_9b861925,
 author = {Rogers, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9b8619251a19057cff70779273e95aa6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9b8619251a19057cff70779273e95aa6-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9b8619251a19057cff70779273e95aa6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Statistical Prediction with Kanerva\textquotesingle s Sparse Distributed Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/9b8619251a19057cff70779273e95aa6-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_9dcb88e0,
 abstract = {We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t-1, together with element t, to predict element t+1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. Cluster analyses of the hidden-layer patterns of activation showed that they encode prediction-relevant information about the entire path traversed through the network. We illustrate the phases of learning with cluster analyses performed at different points during training.},
 author = {Servan-Schreiber, David and Cleeremans, Axel and McClelland, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9dcb88e0137649590b755372b040afad-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9dcb88e0137649590b755372b040afad-Metadata.json},
 openalex = {W2119796132},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9dcb88e0137649590b755372b040afad-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Sequential Structure in Simple Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/9dcb88e0137649590b755372b040afad-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_9fc3d715,
 abstract = {This paper presents a variation of the back-propagation algorithm that makes optimal use of a network hidden units by decrasing an energy term written as a function of the squared activations of these hidden units. The algorithm can automatically find optimal or nearly optimal architectures necessary to solve known Boolean functions, facilitate the interpretation of the activation of the remaining hidden units and automatically estimate the complexity of architectures appropriate for phonetic labeling problems. The general principle of the algorithm can also be adapted to different tasks: for example, it can be used to eliminate the [0, 0] local minimum of the [-1. +1] logistic activation function while preserving a much faster convergence and forcing binary activations over the set of hidden units.},
 author = {Chauvin, Yves},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9fc3d7152ba9336a670e36d0ed79bc43-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9fc3d7152ba9336a670e36d0ed79bc43-Metadata.json},
 openalex = {W2096764219},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Back-Propagation Algorithm with Optimal Use of Hidden Units},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/9fc3d7152ba9336a670e36d0ed79bc43-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_a0a080f4,
 abstract = {Learning procedures that measure how random perturbations of unit activities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities affect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforcement procedures but is more efficient. GEMINI injects noise only at the first hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing unknown non-linearities in the system. Two simulations demonstrate the effectiveness of GEMINI.},
 author = {Le Cun, Yann and Galland, Conrad and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a0a080f42e6f13b3a2df133f073095dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a0a080f42e6f13b3a2df133f073095dd-Metadata.json},
 openalex = {W2115386659},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {GEMINI: Gradient Estimation Through Matrix Inversion After Noise Injection},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/a0a080f42e6f13b3a2df133f073095dd-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_a2557a7b,
 author = {Hartstein, Allan and Koch, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a2557a7b2e94197ff767970b67041697-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a2557a7b2e94197ff767970b67041697-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a2557a7b2e94197ff767970b67041697-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Self-Learning Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/a2557a7b2e94197ff767970b67041697-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_a3c65c29,
 abstract = {We introduce an optimization approach for solving problems in computer vision that involve multiple levels of abstraction. Our objective functions include compositional and specialization hierarchies. We cast vision problems as inexact graph matching problems, formulate graph matching in terms of constrained optimization, and use analog neural networks to perform the optimization. The method is applicable to perceptual grouping and model matching. Preliminary experimental results are shown.},
 author = {Mjolsness, Eric and Gindi, Gene and Anandan, P.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a3c65c2974270fd093ee8a9bf8ae7d0b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a3c65c2974270fd093ee8a9bf8ae7d0b-Metadata.json},
 openalex = {W2104906822},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a3c65c2974270fd093ee8a9bf8ae7d0b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Networks for Model Matching and Perceptual Organization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/a3c65c2974270fd093ee8a9bf8ae7d0b-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_a4a042cf,
 abstract = {The barn owl has fused visual/auditory/motor representations of space in its midbrain which are used to orient the head so that visual or auditory stimuli are centered in the visual field of view. We present models and computer simulations of these structures which address various problems, including the construction of a map of space from auditory sensory information, and the problem of driving the motor system from these maps. We compare the results with biological data.},
 author = {Spence, Clay D. and Pearson, John and Gelfand, J. and Peterson, R. and Sullivan, W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a4a042cf4fd6bfb47701cbc8a1653ada-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a4a042cf4fd6bfb47701cbc8a1653ada-Metadata.json},
 openalex = {W2172135639},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a4a042cf4fd6bfb47701cbc8a1653ada-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neuronal Maps for Sensory-Motor Control in the Barn Owl},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/a4a042cf4fd6bfb47701cbc8a1653ada-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_a5e00132,
 abstract = {This study evaluates the performance of the multilayer-perceptron and the frequency-sensitive competitive learning network in identifying five commercial aircraft from radar backscatter measurements. The performance of the neural network classifiers is compared with that of the nearest-neighbor and maximum-likelihood classifiers. Our results indicate that for this problem, the neural network classifiers are relatively insensitive to changes in the network topology, and to the noise level in the training data. While, for this problem, the traditional algorithms outperform these simple neural classifiers, we feel that neural networks show the potential for improved performance.},
 author = {Ahalt, Stanley and Garber, F. and Jouny, I. and Krishnamurthy, Ashok},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a5e00132373a7031000fd987a3c9f87b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a5e00132373a7031000fd987a3c9f87b-Metadata.json},
 openalex = {W2110824690},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Performance of Synthetic Neural Network Classification of Noisy Radar Signals},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_a8baa565,
 abstract = {A new training called the paradigm, is introduced for tasks in which a network must learn to choose a preferred pattern from a set of n alternatives, based on examples of human expert preferences. In this the input to the network consists of two of the n alternatives, and the trained output is the expert's judgement of which pattern is better. This paradigm is applied to the learning of backgammon, a difficult board game in which the expert selects a move from a set of legal moves. With comparison training, much higher levels of performance can be achieved, with networks that are much smaller, and with coding schemes that are much simpler and easier to understand. Furthermore, it is possible to set up the network so that it always produces consistent rank-orderings.},
 author = {Tesauro, Gerald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a8baa56554f96369ab93e4f3bb068c22-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a8baa56554f96369ab93e4f3bb068c22-Metadata.json},
 openalex = {W2151368309},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connectionist Learning of Expert Preferences by Comparison Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_a8f15eda,
 abstract = {We have designed, fabricated, and tested a series of compact CMOS integrated circuits that realize the winner-take-all function. These analog, continuous-time circuits use only O(n) of interconnect to perform this function. We have also modified the winner-take-all circuit, realizing a circuit that computes local nonlinear inhibition.},
 author = {Lazzaro, J. and Ryckebusch, S. and Mahowald, M.A. and Mead, C. A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a8f15eda80c50adb0e71943adc8015cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a8f15eda80c50adb0e71943adc8015cf-Metadata.json},
 openalex = {W2139016795},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a8f15eda80c50adb0e71943adc8015cf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Winner-Take-All Networks of O(N) Complexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/a8f15eda80c50adb0e71943adc8015cf-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_a97da629,
 abstract = {This paper describes the construction of a system that recognizes handprinted digits, using a combination of classical techniques and neural-net methods. The system has been trained and tested on real-world data, derived from zip codes seen on actual U.S. Mail. The system rejects a small percentage of the examples as unclassifiable, and achieves a very low error rate on the remaining examples. The system compares favorably with other state-of-the art recognizers. While some of the methods are specific to this task, it is hoped that many of the techniques will be applicable to a wide range of recognition tasks.},
 author = {Denker, John and Gardner, W. and Graf, Hans and Henderson, Donnie and Howard, R. and Hubbard, W. and Jackel, L. D. and Baird, Henry and Guyon, Isabelle},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a97da629b098b75c294dffdc3e463904-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a97da629b098b75c294dffdc3e463904-Metadata.json},
 openalex = {W2157475639},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a97da629b098b75c294dffdc3e463904-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Recognizer for Hand-Written Zip Code Digits},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/a97da629b098b75c294dffdc3e463904-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_ac627ab1,
 abstract = {An extremely compact, all analog and fully parallel implementation of a class of shunting recurrent neural networks that is applicable to a wide variety of FET-based integration technologies is proposed. While the contrast enhancement, data compression, and adaptation to mean input intensity capabilities of the network are well suited for processing of sensory information or feature extraction for a content addressable memory (CAM) system, the network also admits a global Liapunov function and can thus achieve stable CAM storage itself. In addition the model can readily function as a front-end processor to an analog adaptive resonance circuit.},
 author = {Nabet, Bahram and Darling, Robert and Pinter, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ac627ab1ccbdb62ec96e702f07f6425b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ac627ab1ccbdb62ec96e702f07f6425b-Metadata.json},
 openalex = {W2116361423},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analog Implementation of Shunting Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/ac627ab1ccbdb62ec96e702f07f6425b-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_b3e3e393,
 abstract = {A new optimization strategy, Mean Field Annealing, is presented. Its application to MAP restoration of noisy range images is derived and experimentally verified.},
 author = {Bilbro, Griff and Snyder, Wesley},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Metadata.json},
 openalex = {W2104727838},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Range Image Restoration Using Mean Field Annealing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/b3e3e393c77e35a4a3f3cbd1e429b5dc-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_b73ce398,
 abstract = {DCPS (the Distributed Connectionist Production System) is a neural network with complex dynamical properties. Visualizing the energy landscapes of some of its component modules leads to a better intuitive understanding of the model, and suggests ways in which its dynamics can be controlled in order to improve performance on difficult cases.},
 author = {Touretzky, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/b73ce398c39f506af761d2277d853a92-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/b73ce398c39f506af761d2277d853a92-Metadata.json},
 openalex = {W2125210227},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/b73ce398c39f506af761d2277d853a92-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analyzing the Energy Landscapes of Distributed Winner-Take-All Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/b73ce398c39f506af761d2277d853a92-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_bd4c9ab7,
 abstract = {A self-organizing Hopfield network has been developed in the context of Vector Quantization, aiming at compression of television images. The metastable states of the spin glass-like network are used as an extra storage resource using the Minimal Overlap learning rule (Krauth and Mezard 1987) to optimize the organization of the attractors. The self-organizing scheme that we have devised results in the generation of an adaptive codebook for any qiven TV image.},
 author = {Naillon, Martine and Theeten, Jean-Bernard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/bd4c9ab730f5513206b999ec0d90d1fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/bd4c9ab730f5513206b999ec0d90d1fb-Metadata.json},
 openalex = {W2139326808},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/bd4c9ab730f5513206b999ec0d90d1fb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Approach for TV Image Compression Using a Hopfield Type Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/bd4c9ab730f5513206b999ec0d90d1fb-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_bf822969,
 abstract = {Automatic Speech Recognition (ASR) is an artificial perception problem: the input is raw, continuous patterns (no symbols!) and the desired output, which may be words, phonemes, meaning or text, is symbolic. The most successful approach to automatic speech recognition is based on stochastic models. A stochastic model is a theoretical system whose internal state and output undergo a series of transformations governed by probabilistic [1]. In the application to speech recognition the unknown patterns of sound are treated as if they were outputs of a stochastic system [18,2]. Information about the classes of patterns is encoded as the structure of these laws and the probabilities that govern their operation. The most popular type of SM for ASR is also known as a hidden Markov model.},
 author = {Bridle, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/bf8229696f7a3bb4700cfddef19fa23f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/bf8229696f7a3bb4700cfddef19fa23f-Metadata.json},
 openalex = {W2152894887},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/bf8229696f7a3bb4700cfddef19fa23f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Speech Recognition: Statistical and Neural Information Processing Approaches},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/bf8229696f7a3bb4700cfddef19fa23f-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_c45147de,
 abstract = {The Boltzmann Machine has been introduced as a means to perform global optimization for multimodal objective functions using the principles of simulated annealing. In this paper we consider its utility as a spurious-free content-addressable memory, and provide bounds on its performance in this context. We show how to exploit the machine's ability to escape local minima, in order to use it, at a constant temperature, for unambiguous associative pattern-retrieval in noisy environments. An association rule, which creates a sphere of influence around each stored pattern, is used along with the Machine's dynamics to match the machine's noisy input with one of the pre-stored patterns. Spurious fixed points, whose regions of attraction are not recognized by the rule, are skipped, due to the Machine's finite probability to escape from any state. The results apply to the Boltzmann machine and to the asynchronous net of binary threshold elements ('Hopfield model'). They provide the network designer with worst-case and best-case bounds for the network's performance, and allow polynomial-time tradeoff studies of design parameters.},
 author = {Kam, Moshe and Cheng, Roger},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c45147dee729311ef5b5c3003946c48f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c45147dee729311ef5b5c3003946c48f-Metadata.json},
 openalex = {W2135002618},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c45147dee729311ef5b5c3003946c48f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Convergence and Pattern-Stabilization in the Boltzmann Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/c45147dee729311ef5b5c3003946c48f-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_c8ffe9a5,
 abstract = {We have previously developed a simple mathematical model for formation of ocular dominance columns in mammalian visual cortex. The model provides a common framework in which a variety of activity-dependent biological machanisms can be studied. Analytic and computational results together now reveal the following: if inputs specific to each eye are locally correlated in their firing, and are not anticorrelated within an arbor radius, monocular cells will robustly form and be organized by intra-cortical interactions into columns. Broader correlations within each eye, or anti-correlations between the eyes, create a more purely monocular cortex; positive correlation over an arbor radius yields an almost perfectly monocular cortex. Most features of the model can be understood analytically through decomposition into eigenfunctions and linear stability analysis. This allows prediction of the widths of the columns and other features from measurable biological parameters.},
 author = {Miller, Kenneth and Keller, Joseph and Stryker, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c8ffe9a587b126f152ed3d89a146b445-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c8ffe9a587b126f152ed3d89a146b445-Metadata.json},
 openalex = {W2146989116},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c8ffe9a587b126f152ed3d89a146b445-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Models of Ocular Dominance Column Formation: Analytical and Computational Results},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/c8ffe9a587b126f152ed3d89a146b445-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_c9e1074f,
 abstract = {A digital realisation of two-dimensional self-organising feature maps is presented. The method is based on subspace classification using an n-tuple technique. Weight vector approximation and orthogonal projections to produce a winner-takes-all network are also discussed. Over one million effective binary weights can be applied in 25ms using a conventional microcomputer. Details of a number of image recognition tasks, including character recognition and object centring, are described.},
 author = {Allinson, Nigel and Johnson, Martin and Moon, Kevin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c9e1074f5b3f9fc8ea15d152add07294-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c9e1074f5b3f9fc8ea15d152add07294-Metadata.json},
 openalex = {W2126976261},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Digital Realisation of Self-Organising Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/c9e1074f5b3f9fc8ea15d152add07294-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_cedebb6e,
 abstract = {MURPHY is a vision-based kinematic controller and path planner based on a connectionist architecture, and implemented with a video camera and Rhino XR-series robot arm. Imitative of the layout of sensory and motor maps in cerebral cortex, MURPHY'S internal representations consist of four coarse-coded populations of simple units representing both static and dynamic aspects of the sensory-motor environment. In previously reported work [4], MURPHY first learned a direct kinematic of his camera-arm system during a period of extended practice, and then used this mental model to heuristically guide his hand to unobstructed visual targets. MURPHY has since been extended in two ways: First, he now learns the inverse differential-kinematics of his arm in addition to ordinary direct kinematics, which allows him to push his hand directly towards a visual target without the need for search. Secondly, he now deals with the much more difficult problem of reaching in the presence of obstacles.},
 author = {Mel, Bartlett},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/cedebb6e872f539bef8c3f919874e9d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/cedebb6e872f539bef8c3f919874e9d7-Metadata.json},
 openalex = {W2170641298},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Further Explorations in Visually-Guided Reaching: Making MURPHY Smarter},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/cedebb6e872f539bef8c3f919874e9d7-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_d1f491a4,
 abstract = {The issues of scaling and generalization have emerged as key issues in current studies of supervised learning from examples in neural networks. Questions such as how many training patterns and training cycles are needed for a problem of a given size and difficulty, how to represent the input and how to choose useful training exemplars, are of considerable theoretical and practical importance. Several intuitive rules of thumb have been obtained from empirical studies, but as yet there are few rigorous results. In this paper we summarize a study of generalization in the simplest possible case-perceptron networks learning linearly separable functions. The task chosen was the majority function (i.e. return a 1 if a majority of the input units are on), a predicate with a number of useful properties. We find that many aspects of generalization in multilayer networks learning large, difficult tasks are reproduced in this simple domain, in which concrete numerical results and even some analytic understanding can be achieved.},
 author = {Ahmad, Subutai and Tesauro, Gerald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/d1f491a404d6854880943e5c3cd9ca25-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/d1f491a404d6854880943e5c3cd9ca25-Metadata.json},
 openalex = {W2139758973},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Scaling and Generalization in Neural Networks: A Case Study},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_da4fb5c6,
 abstract = {A new model of a controlled neuron oscillator, proposed earlier {Kryukov et al, 1986} for the interpretation of the neural activity in various parts of the central nervous system, may have important applications in engineering and in the theory of brain functions. The oscillator has a good stability of the oscillation period, its frequency is regulated linearly in a wide range and it can exhibit arbitrarily long oscillation periods without changing the time constants of its elements. The latter is achieved by using the critical slowdown in the dynamics arising in a network of nonformal excitatory neurons {Kovalenko et al, 1984, Kryukov, 1984}. By changing the parameters of the oscillator one can obtain various functional modes which are necessary to develop a model of higher brain function.},
 author = {Kirillov, Alexandr and Borisyuk, G. N. and Borisyuk, R. M. and Kovalenko, Ye. and Makarenko, V. and Chulaevsky, V. and Kryukov, V.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/da4fb5c6e93e74d3df8527599fa62642-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/da4fb5c6e93e74d3df8527599fa62642-Metadata.json},
 openalex = {W2127195160},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Model of Neural Oscillator for a Unified Submodule},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/da4fb5c6e93e74d3df8527599fa62642-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_e00da03b,
 abstract = {We propose an optimality principle for training an unsupervised feedforward neural network based upon maximal ability to reconstruct the input data from the network outputs. We describe an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity. Examples of applications to the problems of image coding, feature detection, and analysis of random-dot stereograms are presented.},
 author = {Sanger, Terence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/e00da03b685a0dd18fb6a08af0923de0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/e00da03b685a0dd18fb6a08af0923de0-Metadata.json},
 openalex = {W2114187834},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/e00da03b685a0dd18fb6a08af0923de0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Optimality Principle for Unsupervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/e00da03b685a0dd18fb6a08af0923de0-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_e2ef524f,
 abstract = {A new class of neural network aimed at early visual processing is described; we call it a Neural Analog Diffusion-Enhancement Layer or NADEL. The network consists of two levels which are coupled through feedfoward and shunted feedback connections. The lower level is a two-dimensional diffusion map which accepts visual features as input, and spreads activity over larger scales as a function of time. The upper layer is periodically fed the activity from the diffusion layer and locates local maxima in it (an extreme form of contrast enhancement) using a network of local comparators. These local maxima are fed back to the diffusion layer using an on-center/off-surround shunting anatomy. The maxima are also available as output of the network. The network dynamics serves to cluster features on multiple scales as a function of time, and can be used in a variety of early visual processing tasks such as: extraction of corners and high curvature points along edge contours, line end detection, gap filling in contours, generation of fixation points, perceptual grouping on multiple scales, correspondence and path impletion in long-range apparent motion, and building 2-D shape representations that are invariant to location, orientation, scale, and small deformation on the visual field.},
 author = {Waxman, Allen and Seibert, Michael and Cunningham, Robert and Wu, Jian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Metadata.json},
 openalex = {W2162235128},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Analog Diffusion-Enhancement Layer and Spatio-Temporal Grouping in Early Vision},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/e2ef524fbf3d9fe611d5a8e90fefdc9c-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_ec5decca,
 abstract = {Nearly optimal solutions to many combinatorial problems can be found using stochastic simulated annealing. This paper extends the concept of simulated annealing from its original formulation as a Markov process to a new formulation based on mean field theory. Mean field annealing essentially replaces the discrete degrees of freedom in simulated annealing with their average values as computed by the mean field approximation. The net result is that equilibrium at a given temperature is achieved 1-2 orders of magnitude faster than with simulated annealing. A general framework for the mean field annealing algorithm is derived, and its relationship to Hopfield networks is shown. The behavior of MFA is examined both analytically and experimentally for a generic combinatorial optimization problem: graph bipartitioning. This analysis indicates the presence of critical temperatures which could be important in improving the performance of neural networks.},
 author = {Bilbro, Griff and Mann, Reinhold and Miller, Thomas and Snyder, Wesley and van den Bout, David and White, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Metadata.json},
 openalex = {W2119745285},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimization by Mean Field Annealing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/ec5decca5ed3d6b8079e2e7e7bacc9f2-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_ec895663,
 abstract = {This paper addresses the problem of determining the weights for a set of linear filters (model cells) so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors. The quantity that is maximized is the Shannon information rate, or equivalently the average mutual information between input and output. Several models for the role of processing noise are analyzed, and the biological motivation for considering them is described. For simple models in which nearby input signal values (in space or time) are correlated, the cells resulting from this optimization process include center-surround cells and cells sensitive to temporal variations in input signal.},
 author = {Linsker, Ralph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ec8956637a99787bd197eacd77acce5e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ec8956637a99787bd197eacd77acce5e-Metadata.json},
 openalex = {W2135346645},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Application of the Principle of Maximum Information Preservation to Linear Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/ec8956637a99787bd197eacd77acce5e-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_ed3d2c21,
 abstract = {One attempt at explaining human inferencing is that of spreading activation, particularly in the structured connectionist paradigm. This has resulted in the building of systems with semantically nameable nodes which perform inferencing by examining the patterns of activation spread. In this paper we demonstrate that simple structured network inferencing can be performed by passing activation over the weights learned by a distributed algorithm. Thus, an account, is provided which explains a well-behaved relationship between structured and distributed connectionist approaches.},
 author = {Hendler, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ed3d2c21991e3bef5e069713af9fa6ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ed3d2c21991e3bef5e069713af9fa6ca-Metadata.json},
 openalex = {W2133742994},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Spreading Activation over Distributed Microfeatures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/ed3d2c21991e3bef5e069713af9fa6ca-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_eecca5b6,
 abstract = {It is shown that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks. The performance of resulting larger phonetic nets was found to be as good as the performance of the subcomponent nets by themselves. This approach avoids the excessive learning times that would be necessary to train larger networks and allows for incremental learning. Large time-delay neural networks constructed incrementally by applying these modular training techniques achieved a recognition performance of 96.0% for all consonants and 94.7% for all phonemes.< <ETX xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">&gt;</ETX>},
 author = {Waibel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/eecca5b6365d9607ee5a9d336962c534-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/eecca5b6365d9607ee5a9d336962c534-Metadata.json},
 openalex = {W2169163929},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Consonant recognition by modular construction of large phonemic time-delay neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/eecca5b6365d9607ee5a9d336962c534-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_f0935e4c,
 abstract = {The potential of adaptive networks to learn categorization rules and to model human performance is studied by comparing how natural and artificial systems respond to new inputs, i.e., how they generalize. Like humans, networks can learn a deterministic categorization task by a variety of alternative individual solutions. An analysis of the constraints imposed by using networks with the number of hidden units shows that this minimal configuration constraint is not sufficient to explain and predict human performance; only a few solutions were found to be shared by both humans and adaptive networks. A further analysis of human and network generalizations indicates that initial conditions may provide important constraints on generalization. A new technique, which we call reversed learning, is described for finding appropriate initial conditions.},
 author = {Gluck, Mark and Pavel, M. and Henkle, Van},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f0935e4cd5920aa6c7c996a5ee53a70f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f0935e4cd5920aa6c7c996a5ee53a70f-Metadata.json},
 openalex = {W2143945143},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Constraints on Adaptive Networks for Modeling Human Generalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/f0935e4cd5920aa6c7c996a5ee53a70f-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_f2217062,
 abstract = {This work introduces a new method called Self Organizing Neural Network (SONN) algorithm and demonstrates its use in a system identification task. The algorithm constructs the network, chooses the neuron functions, and adjusts the weights. It is compared to the Back-Propagation algorithm in the identification of the chaotic time series. The results shows that SONN constructs a simpler, more accurate model, requiring less training data and epochs. The algorithm can be applied and generalized to appilications as a classifier.},
 author = {Tenorio, Manoel and Lee, Wei-Tsih},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f2217062e9a397a1dca429e7d70bc6ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f2217062e9a397a1dca429e7d70bc6ca-Metadata.json},
 openalex = {W2170038280},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f2217062e9a397a1dca429e7d70bc6ca-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Self Organizing Neural Networks for the Identification Problem},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/f2217062e9a397a1dca429e7d70bc6ca-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_f4b9ec30,
 abstract = {A number of learning models have recently been proposed which involve calculations of temporal differences (or derivatives in continuous-time models). These models, like most adaptive network models, are formulated in terms of frequency (or activation), a useful abstraction of neuronal firing rates. To more precisely evaluate the implications of a neuronal model, it may be preferable to develop a model which transmits discrete pulse-coded information. We point out that many functions and properties of neuronal processing and learning may depend, in subtle ways, on the pulse-coded nature of the information coding and transmission properties of neuron systems. When compared to formulations in terms of activation, computing with temporal derivatives (or differences) as proposed by Kosko (1986), Klopf (1988), and Sutton (1988), is both more stable and easier when reformulated for a more neuronally realistic pulse-coded system. In reformulating these models in terms of pulse-coding, our motivation has been to enable us to draw further parallels and connections between real-time behavioral models of learning and biological circuit models of the substrates underlying learning and memory.},
 author = {Parker, David and Gluck, Mark and Reifsnider, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f4b9ec30ad9f68f89b29639786cb62ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f4b9ec30ad9f68f89b29639786cb62ef-Metadata.json},
 openalex = {W2171022867},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning with Temporal Derivatives in Pulse-Coded Neuronal Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_f7e6c855,
 abstract = {Computing the inverse dynamics of a robot ann is an active area of research in the control literature. We hope to learn the inverse dynamics by training a neural network on the measured response of a physical arm. The input to the network is a temporal window of measured positions; output is a vector of torques. We train the network on data measured from the first two joints of the CMU Direct-Drive Arm II as it moves through a randomly-generated sample of pick-and-place trajectories. We then test generalization with a new trajectory and compare its output with the torque measured at the physical arm. The network is shown to generalize with a root mean square error/standard deviation (RMSS) of 0.10. We interpreted the weights of the network in terms of the velocity and acceleration filters used in conventional control theory.},
 author = {Goldberg, Kenneth and Pearlmutter, Barak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f7e6c85504ce6e82442c770f7c8606f0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f7e6c85504ce6e82442c770f7c8606f0-Metadata.json},
 openalex = {W2151432038},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Using Backpropagation with Temporal Windows to Learn the Dynamics of the CMU Direct-Drive Arm II},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/f7e6c85504ce6e82442c770f7c8606f0-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_f899139d,
 abstract = {In modeling studies or memory based on neural networks, both the selective enhancement and depression or synaptic strengths are required ror efficient storage or inrormation (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et al, 1982; Sejnowski and Tesauro, 1989). We have tested this assumption in the hippocampus, a cortical structure or the brain that is involved in long-term memory. A brief, high-frequency activation or excitatory synapses in the hippocampus produces an increase in synaptic strength known as long-term potentiation, or LTP (Bliss and Lomo, 1973), that can last for many days. LTP is known to be Hebbian since it requires the simultaneous release or neurotransmitter from presynaptic terminals coupled with postsynaptic depolarization (Kelso et al., 1986; Malinow and Miller, 1986; Gustatrson et al, 1987). However, a mechanism ror the persistent reduction of synaptic strength that could balance LTP has not yet been demonstrated. We studied the associative interactions between separate inputs onto the same dendritic trees or hippocampal pyramidal cells or field CA1, and round that a low-frequency input which, by itself, does not persistently change synaptic strength, can either increase (associative LTP) or decrease in strength (associative long-term depression or LTD) depending upon whether it is positively or negatively correlated in time with a second, high-frequency bursting input. LTP of synaptic strength is Hebbian, and LTD is anti-Hebbian since it is elicited by pairing presynaptic firing with postsynaptic hyperpolarization sufficient to block postsynaptic activity. Thus, associative LTP and associative LTD are capable or storing information contained in the covariance between separate, converging hippocampal inputs.},
 author = {Stanton, Patric and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f899139df5e1059396431415e770c6dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f899139df5e1059396431415e770c6dd-Metadata.json},
 openalex = {W2100975869},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f899139df5e1059396431415e770c6dd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Storing Covariance by the Associative Long-Term Potentiation and Depression of Synaptic Strengths in the Hippocampus},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/f899139df5e1059396431415e770c6dd-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_fa7cdfad,
 abstract = {This paper introduces a means to handle the critical problem of nonlocal role-bindings in localist spreading-activation networks. Every conceptual node in the network broadcasts a stable, uniquely-identifying activation pattern, called its signature. A dynamic role-binding is created when a role's binding node has an activation that matches the bound concept's signature. Most importantly, signatures are propagated across long paths of nodes to handle the non-local role-bindings necessary for inferencing. Our localist network model, ROBIN (ROle Binding and Inferencing Network), uses signature activations to robustly represent schemata role-bindings and thus perform the inferencing, plan/goal analysis, schema instantiation, word-sense disambiguation, and dynamic re-interpretation portions of the natural language understanding process.},
 author = {Lange, Trent and Dyer, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Metadata.json},
 openalex = {W2141955856},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dynamic, Non-Local Role Bindings and Inferencing in a Localist Network for Natural Language Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html},
 volume = {1},
 year = {1988}
}

@inproceedings{NIPS1988_fc221309,
 abstract = {This paper provides a systematic analysis of the recurrent backpropagation (RBP) algorithm, introducing a number of new results. The main limitation of the RBP algorithm is that it assumes the convergence of the network to a stable fixed point in order to backpropagate the error signals. We show by experiment and eigenvalue analysis that this condition can be violated and that chaotic behavior can be avoided. Next we examine the advantages of RBP over the standard backpropagation algorithm. RBP is shown to build stable fixed points corresponding to the input patterns. This makes it an appropriate tool for content addressable memories, one-to-many function learning, and inverse problems.},
 author = {Simard, Patrice and Ottaway, Mary and Ballard, Dana},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1988/file/fc221309746013ac554571fbd180e1c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1988/file/fc221309746013ac554571fbd180e1c8-Metadata.json},
 openalex = {W2132868735},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1988/file/fc221309746013ac554571fbd180e1c8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Fixed Point Analysis for Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1988/hash/fc221309746013ac554571fbd180e1c8-Abstract.html},
 volume = {1},
 year = {1988}
}
