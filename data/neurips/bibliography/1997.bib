@inproceedings{NIPS1997_01d8bae2,
 abstract = {We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions.},
 author = {Sch\"{o}lkopf, Bernhard and Simard, Patrice and Smola, Alex and Vapnik, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/01d8bae291b1e4724443375634ccfa0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/01d8bae291b1e4724443375634ccfa0e-Metadata.json},
 openalex = {W2121649666},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/01d8bae291b1e4724443375634ccfa0e-Paper.pdf},
 publisher = {MIT Press},
 title = {Prior Knowledge in Support Vector Kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/01d8bae291b1e4724443375634ccfa0e-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_0245952e,
 abstract = {Until recently, artificial intelligence researchers have frowned upon the application of probability propagation in Bayesian belief networks that have cycles. The probability propagation algorithm is only exact in networks that are cycle-free. However, it has recently been discovered that the two best error-correcting decoding algorithms are actually performing probability propagation in belief networks with cycles.},
 author = {Frey, Brendan J and MacKay, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0245952ecff55018e2a459517fdb40e3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0245952ecff55018e2a459517fdb40e3-Metadata.json},
 openalex = {W2120294885},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0245952ecff55018e2a459517fdb40e3-Paper.pdf},
 publisher = {MIT Press},
 title = {A Revolution: Belief Propagation in Graphs with Cycles},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/0245952ecff55018e2a459517fdb40e3-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_05311655,
 abstract = {In most neural network models, synapses are treated as static weights that change only with the slow time scales of learning. It is well known, however, that synapses are highly dynamic and show use-dependent plasticity over a wide range of time scales. Moreover, synaptic transmission is an inherently stochastic process: a spike arriving at a presynaptic terminal triggers the release of a vesicle of neurotransmitter from a release site with a probability that can be much less than one. We consider a simple model for dynamic stochastic synapses that can easily be integrated into common models for networks of integrate-and-fire neurons (spiking neurons). The parameters of this model have direct interpretations in terms of synaptic physiology. We investigate the consequences of the model for computing with individual spikes and demonstrate through rigorous theoretical results that the computational power of the network is increased through the use of dynamic synapses.},
 author = {Maass, Wolfgang and Zador, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/05311655a15b75fab86956663e1819cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/05311655a15b75fab86956663e1819cd-Metadata.json},
 openalex = {W2102365206},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/05311655a15b75fab86956663e1819cd-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamic Stochastic Synapses as Computational Units},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/05311655a15b75fab86956663e1819cd-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_07042ac7,
 abstract = {Scale invariance is a fundamental property of ensembles of natural images [1]. Their non Gaussian properties [15, 16] are less well understood, but they indicate the existence of a rich statistical structure. In this work we present a detailed study of the marginal statistics of a variable related to the edges in the images. A numerical analysis shows that it exhibits extended self-similarity [3, 4, 5]. This is a scaling property stronger than self-similarity: all its moments can be expressed as a power of any given moment. More interesting, all the exponents can be predicted in terms of a multiplicative log-Poisson process. This is the very same model that was used very recently to predict the correct exponents of the structure functions of turbulent flows [6]. These results allow us to study the underlying multifractal singularities. In particular we find that the most singular structures are one-dimensional: the most singular manifold consists of sharp edges.},
 author = {Turiel, Antonio and Mato, Germ\'{a}n and Parga, N\'{e}stor and Nadal, Jean-Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/07042ac7d03d3b9911a00da43ce0079a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/07042ac7d03d3b9911a00da43ce0079a-Metadata.json},
 openalex = {W2141441975},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/07042ac7d03d3b9911a00da43ce0079a-Paper.pdf},
 publisher = {MIT Press},
 title = {Self-similarity Properties of Natural Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/07042ac7d03d3b9911a00da43ce0079a-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_09662890,
 abstract = {We introduce a new Boolean computing element related to the Linear Threshold element, which is the Boolean version of the neuron. Instead of the sign function, it computes an arbitrary (with polynomialy many transitions) Boolean function of the weighted sum of its inputs. We call the new computing element an LTM element, which stands for Linear Threshold with Multiple transitions.

The paper consists of the following main contributions related to our study of LTM circuits: (i) the creation of efficient designs of LTM circuits for the addition of a multiple number of integers and the product of two integers. In particular, we show how to compute the addition of m integers with a single layer of LTM elements. (ii) a proof that the area of the VLSI layout is reduced from O(n2) in LT circuits to O(n) in LTM circuits, for n inputs symmetric Boolean functions, and (iii) the characterization of the computing power of LTM relative to LT circuits.},
 author = {Bohossian, Vasken and Bruck, Jehoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0966289037ad9846c5e994be2a91bafa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0966289037ad9846c5e994be2a91bafa-Metadata.json},
 openalex = {W2149840875},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0966289037ad9846c5e994be2a91bafa-Paper.pdf},
 publisher = {MIT Press},
 title = {Multiple Threshold Neural Logic},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/0966289037ad9846c5e994be2a91bafa-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_0a1bf96b,
 abstract = {A 80 Ã— 78 pixel general purpose vision chip for spatial focal plane processing is presented. The size and configuration of the processing receptive field are programmable. The chip's architecture allows the photoreceptor cells to be small and densely packed by performing all computation on the read-out, away from the array. In addition to the raw intensity image, the chip outputs four processed images in parallel. Also presented is an application of the chip to line segment orientation detection, as found in the retinal receptive fields of toads.},
 author = {Etienne-Cummings, Ralph and Cai, Donghui},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0a1bf96b7165e962e90cb14648c9462d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0a1bf96b7165e962e90cb14648c9462d-Metadata.json},
 openalex = {W2145751485},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf},
 publisher = {MIT Press},
 title = {A General Purpose Image Processing Chip: Orientation Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/0a1bf96b7165e962e90cb14648c9462d-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_0c0a7566,
 abstract = {This paper considers the problem of learning the ranking of a set of alternatives based upon incomplete information (e.g., a limited number of observations). We describe two algorithms for hypothesis ranking and their application for probably approximately correct (PAC) and expected loss (EL) learning criteria. Empirical results are provided to demonstrate the effectiveness of these ranking procedures on both synthetic datasets and real-world data from a spacecraft design optimization problem.},
 author = {Chien, Steve and Stechert, Andre and Mutz, Darren},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0c0a7566915f4f24853fc4192689aa7e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0c0a7566915f4f24853fc4192689aa7e-Metadata.json},
 openalex = {W2160480776},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0c0a7566915f4f24853fc4192689aa7e-Paper.pdf},
 publisher = {MIT Press},
 title = {On Efficient Heuristic Ranking of Hypotheses},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/0c0a7566915f4f24853fc4192689aa7e-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_0e3a37aa,
 abstract = {One approach to invariant object recognition employs a recurrent neural network as an associative memory. In the standard depiction of the network's state space, memories of objects are stored as attractive fixed points of the dynamics. I argue for a modification of this picture: if an object has a continuous family of instantiations, it should be represented by a continuous attractor. This idea is illustrated with a network that learns to complete patterns. To perform the task of filling in missing information, the network develops a continuous attractor that models the manifold from which the patterns are drawn. From a statistical view-point, the pattern completion task allows a formulation of unsupervised learning in terms of regression rather than density estimation.},
 author = {Seung, H. Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0e3a37aa85a14e359df74fa77eded3f6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0e3a37aa85a14e359df74fa77eded3f6-Metadata.json},
 openalex = {W2140622310},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0e3a37aa85a14e359df74fa77eded3f6-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Continuous Attractors in Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/0e3a37aa85a14e359df74fa77eded3f6-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_0e4e9466,
 abstract = {We present a new approximate learning algorithm for Boltzmann Machines, using a systematic expansion of the Gibbs free energy to second order in the weights. The linear response correction to the correlations is given by the Hessian of the Gibbs free energy. The computational complexity of the algorithm is cubic in the number of neurons. We compare the performance of the exact BM learning algorithm with first order (Weiss) mean field theory and second order (TAP) mean field theory. The learning task consists of a fully connected Ising spin glass model on 10 neurons. We conclude that 1) the method works well for paramagnetic problems 2) the TAP correction gives a significant improvement over the Weiss mean field theory, both for paramagnetic and spin glass problems and 3) that the inclusion of diagonal weights improves the Weiss approximation for paramagnetic problems, but not for spin glass problems.},
 author = {Kappen, Hilbert and de Borja Rodr\'{\i}guez Ortiz, Francisco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0e4e946668cf2afc4299b462b812caca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0e4e946668cf2afc4299b462b812caca-Metadata.json},
 openalex = {W2129363682},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0e4e946668cf2afc4299b462b812caca-Paper.pdf},
 publisher = {MIT Press},
 title = {Boltzmann Machine Learning Using Mean Field Theory and Linear Response Correction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/0e4e946668cf2afc4299b462b812caca-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_0e55666a,
 abstract = {In many applications, such as credit default prediction and medical image recognition, test inputs are available in addition to the labeled training examples. We propose a method to incorporate the test inputs into learning. Our method results in solutions having smaller test errors than that of simple training solution, especially for noisy problems or small training sets.},
 author = {Cataltepe, Zehra and Magdon-Ismail, Malik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0e55666a4ad822e0e34299df3591d979-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0e55666a4ad822e0e34299df3591d979-Metadata.json},
 openalex = {W2158009322},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0e55666a4ad822e0e34299df3591d979-Paper.pdf},
 publisher = {MIT Press},
 title = {Incorporating Test Inputs into Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/0e55666a4ad822e0e34299df3591d979-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_0ed94223,
 abstract = {Applications of Gaussian mixture models occur frequently in the fields of statistics and artificial neural networks. One of the key issues arising from any mixture model application is how to estimate the optimum number of mixture components. This paper extends the Reversible-Jump Markov Chain Monte Carlo (MCMC) algorithm to the case of multivariate spherical Gaussian mixtures using a hierarchical prior model. Using this method the number of mixture components is no longer fixed but becomes a parameter of the model which we shall estimate. The Reversible-Jump MCMC algorithm is capable of moving between parameter subspaces which correspond to models with different numbers of mixture components. As a result a sample from the full joint distribution of all unknown model parameters is generated. The technique is then demonstrated on a simulated example and a well known vowel dataset.},
 author = {Marrs, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0ed9422357395a0d4879191c66f4faa2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0ed9422357395a0d4879191c66f4faa2-Metadata.json},
 openalex = {W2096278047},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/0ed9422357395a0d4879191c66f4faa2-Paper.pdf},
 publisher = {MIT Press},
 title = {An Application of Reversible-Jump MCMC to Multivariate Spherical Gaussian Mixtures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/0ed9422357395a0d4879191c66f4faa2-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_14d9e800,
 abstract = {An asynchronous PDM (Pulse-Density-Modulating) digital neural network system has been developed in our laboratory. It consists of one thousand neurons that are physically interconnected via one million 7-bit synapses. It can solve one thousand simultaneous nonlinear first-order differential equations in a fully parallel and continuous fashion. The performance of this system was measured by a winner-take-all network with one thousand neurons. Although the magnitude of the input and network parameters were identical for each competing neuron, one of them won in 6 milliseconds. This processing speed amounts to 360 billion connections per second. A broad range of neural networks including spatiotemporal filtering, feedforward, and feedback networks can be run by loading appropriate network parameters from a host system.},
 author = {Hirai, Yuzo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/14d9e8007c9b41f57891c48e07c23f57-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/14d9e8007c9b41f57891c48e07c23f57-Metadata.json},
 openalex = {W2154456238},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/14d9e8007c9b41f57891c48e07c23f57-Paper.pdf},
 publisher = {MIT Press},
 title = {A 1, 000-Neuron System with One Million 7-bit Physical Interconnections},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/14d9e8007c9b41f57891c48e07c23f57-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_186a157b,
 abstract = {This paper is concerned with the problem of Reinforcement Learning (RL) for continuous state space and time stochastic control problems. We state the Hamilton-Jacobi-Bellman equation satisfied by the value function and use a Finite-Difference method for designing a convergent approximation scheme. Then we propose a RL algorithm based on this scheme and prove its convergence to the optimal solution.},
 author = {Munos, R\'{e}mi and Bourgine, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/186a157b2992e7daed3677ce8e9fe40f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/186a157b2992e7daed3677ce8e9fe40f-Metadata.json},
 openalex = {W2166356462},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/186a157b2992e7daed3677ce8e9fe40f-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement learning for continuous stochastic control problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_1abb1e1e,
 abstract = {With the rapid expansion of computer networks during the past few years, security has become a crucial issue for modern computer systems. A good way to detect illegitimate use is through monitoring unusual user activity. Methods of intrusion detection based on hand-coded rule sets or predicting commands on-line are laborous to build or not very reliable. This paper proposes a new way of applying neural networks to detect intrusions. We believe that a user leaves a 'print' when using the system; a neural network can be used to learn this print and identify each user much like detectives use thumbprints to place people at crime scenes. If a user's behavior does not match his/her print, the system administrator can be alerted of a possible security breech. A backpropagation neural network called NNID (Neural Network Intrusion Detector) was trained in the identification task and tested experimentally on a system of 10 users. The system was 96% accurate in detecting unusual activity, with 7% false alarm rate. These results suggest that learning user profiles is an effective way for detecting intrusions.},
 author = {Ryan, Jake and Lin, Meng-Jang and Miikkulainen, Risto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/1abb1e1ea5f481b589da52303b091cbb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/1abb1e1ea5f481b589da52303b091cbb-Metadata.json},
 openalex = {W2146196597},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf},
 publisher = {MIT Press},
 title = {Intrusion Detection with Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/1abb1e1ea5f481b589da52303b091cbb-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_1f3202d8,
 abstract = {In this paper we propose a technique to incorporate contextual information into object classification. In the real world there are cases where the identity of an object is ambiguous due to the noise in the measurements based on which the classification should be made. It is helpful to reduce the ambiguity by utilizing extra information referred to as context, which in our case is the identities of the accompanying objects. This technique is applied to white blood cell classification. Comparisons are made against no context approach, which demonstrates the superior classification performance achieved by using context. In our particular application, it significantly reduces false alarm rate and thus greatly reduces the cost due to expensive clinical tests.},
 author = {Song, Xubo and Abu-Mostafa, Yaser and Sill, Joseph and Kasdan, Harvey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/1f3202d820180a39f736f20fce790de8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/1f3202d820180a39f736f20fce790de8-Metadata.json},
 openalex = {W2152550539},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/1f3202d820180a39f736f20fce790de8-Paper.pdf},
 publisher = {MIT Press},
 title = {Incorporating Contextual Information in White Blood Cell Identification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/1f3202d820180a39f736f20fce790de8-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_215a71a1,
 abstract = {We present a computationally efficient algorithm for function approximation with piecewise linear sigmoidal nodes. A one hidden layer network is constructed one node at a time using the method of fitting the residual. The task of fitting individual nodes is accomplished using a new algorithm that searchs for the best fit by solving a sequence of Quadratic Programming problems. This approach offers significant advantages over derivative-based search algorithms (e.g. backpropagation and its extensions). Unique characteristics of this algorithm include: finite step convergence, a simple stopping criterion, a deterministic methodology for seeking local minima, good scaling properties and a robust numerical implementation.},
 author = {Hush, Don and Lozano, Fernando and Horne, Bill},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/215a71a12769b056c3c32e7299f1c5ed-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/215a71a12769b056c3c32e7299f1c5ed-Metadata.json},
 openalex = {W2116297448},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf},
 publisher = {MIT Press},
 title = {Function Approximation with the Sweeping Hinge Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/215a71a12769b056c3c32e7299f1c5ed-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_23ad3e31,
 abstract = {Given a set of objects in the visual field, how does the the visual system learn to attend to a particular object of interest while ignoring the rest? How are occlusions and background clutter so effortlessly discounted for when recognizing a familiar object? In this paper, we attempt to answer these questions in the context of a Kalman filter-based model of visual recognition that has previously proved useful in explaining certain neurophysiological phenomena such as endstopping and related extra-classical receptive field effects in the visual cortex. By using results from the field of robust statistics, we describe an extension of the Kalman filter model that can handle multiple objects in the visual field. The resulting robust Kalman filter model demonstrates how certain forms of attention can be viewed as an emergent property of the interaction between top-down expectations and bottom-up signals. The model also suggests functional interpretations of certain attention-related effects that have been observed in visual cortical neurons. Experimental results are provided to help demonstrate the ability of the model to perform robust segmentation and recognition of objects and image sequences in the presence of varying degrees of occlusions and clutter.},
 author = {Rao, Rajesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/23ad3e314e2a2b43b4c720507cec0723-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/23ad3e314e2a2b43b4c720507cec0723-Metadata.json},
 openalex = {W2138665783},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/23ad3e314e2a2b43b4c720507cec0723-Paper.pdf},
 publisher = {MIT Press},
 title = {Correlates of Attention in a Model of Dynamic Visual Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/23ad3e314e2a2b43b4c720507cec0723-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_28e209b6,
 abstract = {Nonlinear dimensionality reduction is formulated here as the problem of trying to find a Euclidean feature-space embedding of a set of observations that preserves as closely as possible their intrinsic metric structure - the distances between points on the observation manifold as measured along geodesic paths. Our isometric feature mapping procedure, or isomap, is able to reliably recover low-dimensional nonlinear structure in realistic perceptual data sets, such as a manifold of face images, where conventional global mapping methods find only local minima. The recovered map provides a canonical set of globally meaningful features, which allows perceptual transformations such as interpolation, extrapolation, and analogy - highly nonlinear transformations in the original observation space - to be computed with simple linear operations in feature space.},
 author = {Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/28e209b61a52482a0ae1cb9f5959c792-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/28e209b61a52482a0ae1cb9f5959c792-Metadata.json},
 openalex = {W2159174312},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/28e209b61a52482a0ae1cb9f5959c792-Paper.pdf},
 publisher = {MIT Press},
 title = {Mapping a Manifold of Perceptual Observations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/28e209b61a52482a0ae1cb9f5959c792-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_28fc2782,
 abstract = {A simple but powerful modification of the standard Gaussian distribution is studied. The variables of the rectified Gaussian are constrained to be nonnegative, enabling the use of nonconvex energy functions. Two multimodal examples, the competitive and cooperative distributions, illustrate the representational power of the rectified Gaussian. Since the cooperative distribution can represent the translations of a pattern, it demonstrates the potential of the rectified Gaussian for modeling pattern manifolds.},
 author = {Socci, Nicholas and Lee, Daniel and Seung, H. Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/28fc2782ea7ef51c1104ccf7b9bea13d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/28fc2782ea7ef51c1104ccf7b9bea13d-Metadata.json},
 openalex = {W2114279993},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/28fc2782ea7ef51c1104ccf7b9bea13d-Paper.pdf},
 publisher = {MIT Press},
 title = {The Rectified Gaussian Distribution},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/28fc2782ea7ef51c1104ccf7b9bea13d-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_299fb214,
 abstract = {Existing proofs demonstrating the computational limitations of Recurrent Cascade Correlation and similar networks (Fahlman, 1991; Bachrach, 1988; Mozer, 1988) explicitly limit their results to units having sigmoidal or hard-threshold transfer functions (Giles et al., 1995; and Kremer, 1996). The proof given here shows that for any finite, discrete transfer function used by the units of an RCC network, there are finite-state automata (FSA) that the network cannot model, no matter how many units are used. The proof also applies to continuous transfer functions with a finite number of fixed-points, such as sigmoid and radial-basis functions.},
 author = {Ring, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/299fb2142d7de959380f91c01c3a293c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/299fb2142d7de959380f91c01c3a293c-Metadata.json},
 openalex = {W2104084260},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/299fb2142d7de959380f91c01c3a293c-Paper.pdf},
 publisher = {MIT Press},
 title = {RCC Cannot Compute Certain FSA, Even with Arbitrary Transfer Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/299fb2142d7de959380f91c01c3a293c-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_29c4a0e4,
 abstract = {Up-propagation is an algorithm for inverting and learning neural network generative models. Sensory input is processed by inverting a model that generates patterns from hidden variables using top-down connections. The inversion process is iterative, utilizing a negative feedback loop that depends on an error signal propagated by bottom-up connections. The error signal is also used to learn the generative model from examples. The algorithm is benchmarked against principal component analysis in experiments on images of handwritten digits.},
 author = {Oh, Jong-Hoon and Seung, H. Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/29c4a0e4ef7d1969a94a5f4aadd20690-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/29c4a0e4ef7d1969a94a5f4aadd20690-Metadata.json},
 openalex = {W2098361425},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/29c4a0e4ef7d1969a94a5f4aadd20690-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Generative Models with the Up Propagation Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/29c4a0e4ef7d1969a94a5f4aadd20690-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_2bd7f907,
 abstract = {Dual-Route and Connectionist Single-Route models of reading have been at odds over claims as to the correct explanation of the reading process. Recent Dual-Route models predict that subjects should show an increased naming latency for irregular words when the irregularity is earlier in the word (e.g. chef is slower than glow) - a prediction that has been confirmed in human experiments. Since this would appear to be an effect of the left-to-right reading process, Coltheart & Rastle (1994) claim that Single-Route parallel connectionist models cannot account for it. A refutation of this claim is presented here, consisting of network models which do show the interaction, along with orthographic neighborhood statistics that explain the effect.},
 author = {Milostan, Jeanne and Cottrell, Garrison},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/2bd7f907b7f5b6bbd91822c0c7b835f6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/2bd7f907b7f5b6bbd91822c0c7b835f6-Metadata.json},
 openalex = {W2158974447},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/2bd7f907b7f5b6bbd91822c0c7b835f6-Paper.pdf},
 publisher = {MIT Press},
 title = {Serial Order in Reading Aloud: Connectionist Models and Neighborhood Structure},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/2bd7f907b7f5b6bbd91822c0c7b835f6-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_2dffbc47,
 abstract = {Human subjects are known to adapt their motor behavior to a shift of the visual field brought about by wearing prism glasses over their eyes. We have studied the analog of this effect in speech. Using a device that can feedback transformed speech signals in real time, we exposed subjects to alterations of their own speech feedback. We found that speakers learn to adjust their production of a vowel to compensate for feedback alterations that change the vowel's perceived phonetic identity; moreover, the effect generalizes across consonant contexts and to different vowels.},
 author = {Houde, John and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/2dffbc474aa176b6dc957938c15d0c8b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/2dffbc474aa176b6dc957938c15d0c8b-Metadata.json},
 openalex = {W2118962636},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/2dffbc474aa176b6dc957938c15d0c8b-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptation in Speech Motor Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/2dffbc474aa176b6dc957938c15d0c8b-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_309928d4,
 abstract = {We consider the general problem of learning multi-category classification from labeled examples. We present experimental results for a nearest neighbor algorithm which actively selects samples from different pattern classes according to a querying rule instead of the a priori class probabilities. The amount of improvement of this query-based approach over the passive batch approach depends on the complexity of the Bayes rule. The principle on which this algorithm is based is general enough to be used in any learning algorithm which permits a model-selection criterion and for which the error rate of the classifier is calculable in terms of the complexity of the model.},
 author = {Ratsaby, Joel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/309928d4b100a5d75adff48a9bfc1ddb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/309928d4b100a5d75adff48a9bfc1ddb-Metadata.json},
 openalex = {W2135641286},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/309928d4b100a5d75adff48a9bfc1ddb-Paper.pdf},
 publisher = {MIT Press},
 title = {An Incremental Nearest Neighbor Algorithm with Queries},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/309928d4b100a5d75adff48a9bfc1ddb-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_30c8e1ca,
 abstract = {We apply a general algorithm for merging prediction strategies (the Aggregating Algorithm) to the problem of linear regression with the square loss; our main assumption is that the response variable is bounded. It turns out that for this particular problem the Aggregating Algorithm resembles, but is slightly different from, the well-known ridge estimation procedure. From general results about the Aggregating Algorithm we deduce a guaranteed bound on the difference between our algorithm's performance and the best, in some sense, linear regression function's performance. We show that the AA attains the optimal constant in our bound, whereas the constant attained by the ridge regression procedure in general can be 4 times worse.},
 author = {Vovk, Volodya},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/30c8e1ca872524fbf7ea5c519ca397ee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/30c8e1ca872524fbf7ea5c519ca397ee-Metadata.json},
 openalex = {W2096321607},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/30c8e1ca872524fbf7ea5c519ca397ee-Paper.pdf},
 publisher = {MIT Press},
 title = {Competitive On-line Linear Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/30c8e1ca872524fbf7ea5c519ca397ee-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_33ebd5b0,
 abstract = {We address the problem of learning structure in nonlinear Markov networks with continuous variables. This can be viewed as non-Gaussian multidimensional density estimation exploiting certain conditional independencies in the variables. Markov networks are a graphical way of describing conditional independencies well suited to model relationships which do not exhibit a natural causal ordering. We use neural network structures to model the quantitative relationships between variables. The main focus in this paper will be on learning the structure for the purpose of gaining insight into the underlying process. Using two data sets we show that interesting structures can be found using our approach. Inference will be briefly addressed.},
 author = {Hofmann, Reimar and Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/33ebd5b07dc7e407752fe773eed20635-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/33ebd5b07dc7e407752fe773eed20635-Metadata.json},
 openalex = {W2107419735},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/33ebd5b07dc7e407752fe773eed20635-Paper.pdf},
 publisher = {MIT Press},
 title = {Nonlinear Markov Networks for Continuous Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/33ebd5b07dc7e407752fe773eed20635-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_359f3846,
 abstract = {Online learning is one of the most common forms of neural network training. We present an analysis of online learning from finite training sets for non-linear networks (namely, soft-committee machines), advancing the theory to more realistic learning scenarios. Dynamical equations are derived for an appropriate set of order parameters; these are exact in the limiting case of either linear networks or infinite training sets. Preliminary comparisons with simulations suggest that the theory captures some effects of finite training sets, but may not yet account correctly for the presence of local minima.},
 author = {Sollich, Peter and Barber, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/359f38463d487e9e29bd20e24f0c050a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/359f38463d487e9e29bd20e24f0c050a-Metadata.json},
 openalex = {W2096656494},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/359f38463d487e9e29bd20e24f0c050a-Paper.pdf},
 publisher = {MIT Press},
 title = {On-line Learning from Finite Training Sets in Nonlinear Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/359f38463d487e9e29bd20e24f0c050a-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_363763e5,
 abstract = {Initial experiments described here were directed toward using reinforcement learning (RL) to develop an automated recovery system (ARS) for high-agility aircraft. An ARS is an outer-loop flight-control system designed to bring an aircraft from a range of out-of-control states to straight-and-level flight in minimum time while satisfying physical and physiological constraints. Here we report on results for a simple version of the problem involving only single-axis (pitch) simulated recoveries. Through simulated control experience using a medium-fidelity aircraft simulation, the RL system approximates an optimal policy for pitch-stick inputs to produce minimum-time transitions to straight-and-level flight in unconstrained cases while avoiding ground-strike. The RL system was also able to adhere to a pilot-station acceleration constraint while executing simulated recoveries.},
 author = {Monaco, Jeffrey and Ward, David and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/363763e5c3dc3a68b399058c34aecf2c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/363763e5c3dc3a68b399058c34aecf2c-Metadata.json},
 openalex = {W2146054637},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/363763e5c3dc3a68b399058c34aecf2c-Paper.pdf},
 publisher = {MIT Press},
 title = {Automated Aircraft Recovery via Reinforcement Learning: Initial Experiments},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/363763e5c3dc3a68b399058c34aecf2c-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_372d3f30,
 abstract = {We propose local error estimates together with algorithms for adaptive a-posteriori grid and time refinement in reinforcement learning. We consider a deterministic system with continuous state and time with infinite horizon discounted cost functional. For grid refinement we follow the procedure of numerical methods for the Bellman-equation. For time refinement we propose a new criterion, based on consistency estimates of discrete solutions of the Bellman-equation. We demonstrate, that an optimal ratio of time to space discretization is crucial for optimal learning rates and accuracy of the approximate optimal value function.},
 author = {Pareigis, Stephan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/372d3f309fef061977fb2f7ba36d74d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/372d3f309fef061977fb2f7ba36d74d2-Metadata.json},
 openalex = {W2142377262},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/372d3f309fef061977fb2f7ba36d74d2-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive Choice of Grid and Time in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/372d3f309fef061977fb2f7ba36d74d2-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_3bbfdde8,
 abstract = {Abstract Our aim in this paper is to develop a Bayesian framework for matching hierarchical relational models. Such models are widespread in computer vision. The framework that we adopt for this study is provided by iterative discrete relaxation. Here the aim is to assign the discrete matches so as to optimise a global cost function that draws information concerning the consistency of match from different levels of the hierarchy. Our Bayesian development naturally distinguishes between intra-level and inter-level constraints. This allows the impact of reassigning a match to be assessed not only at its own (or peer) level of representation, but also upon its parents and children in the hierarchy. We illustrate the effectiveness of the technique in the matching of line-segment groupings in synthetic aperture radar (SAR) images of rural scenes.},
 author = {Wilson, Richard and Hancock, Edwin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3bbfdde8842a5c44a0323518eec97cbe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3bbfdde8842a5c44a0323518eec97cbe-Metadata.json},
 openalex = {W2140452537},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3bbfdde8842a5c44a0323518eec97cbe-Paper.pdf},
 publisher = {MIT Press},
 title = {Graph matching with hierarchical discrete relaxation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/3bbfdde8842a5c44a0323518eec97cbe-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_3d779cae,
 abstract = {A Lyapunov function for excitatory-inhibitory networks is constructed. The construction assumes symmetric interactions within excitatory and inhibitory populations of neurons, and antisymmetric interactions between populations. The Lyapunov function yields sufficient conditions for the global asymptotic stability of fixed points. If these conditions are violated, limit cycles may be stable. The relations of the Lyapunov function to optimization theory and classical mechanics are revealed by minimax and dissipative Hamiltonian forms of the network dynamics.},
 author = {Seung, H. Sebastian and Richardson, Tom and Lagarias, J. and Hopfield, John J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3d779cae2d46cf6a8a99a35ba4167977-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3d779cae2d46cf6a8a99a35ba4167977-Metadata.json},
 openalex = {W2125215807},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf},
 publisher = {MIT Press},
 title = {Minimax and Hamiltonian Dynamics of Excitatory-Inhibitory Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/3d779cae2d46cf6a8a99a35ba4167977-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_3de2334a,
 abstract = {In this paper, we present a novel hybrid architecture for continuous speech recognition systems. It consists of a continuous HMM system extended by an arbitrary neural network that is used as a preprocessor that takes several frames of the feature vector as input to produce more discriminative feature vectors with respect to the underlying HMM system. This hybrid system is an extension of a state-of-the-art continuous HMM system, and in fact, it is the first hybrid system that really is capable of outperforming these standard systems with respect to the recognition accuracy. Experimental results show an relative error reduction of about 10% that we achieved on a remarkably good recognition system based on continuous HMMs for the Resource Management 1000-word continuous speech recognition task.},
 author = {Willett, Daniel and Rigoll, Gerhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3de2334a314a7a72721f1f74a6cb4cee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3de2334a314a7a72721f1f74a6cb4cee-Metadata.json},
 openalex = {W2117418643},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3de2334a314a7a72721f1f74a6cb4cee-Paper.pdf},
 publisher = {MIT Press},
 title = {Hybrid NN/HMM-Based Speech Recognition with a Discriminant Neural Feature Extraction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/3de2334a314a7a72721f1f74a6cb4cee-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_3e313b9b,
 abstract = {In human object recognition, converging evidence has shown that subjects' performance depends on their familiarity with an object's appearance. The extent of such dependence is a function of the inter-object similarity. The more similar the objects are, the stronger this dependence will be and the more dominant the two-dimensional (2D) image-based information will be. However, the degree to which three-dimensional (3D) model-based information is used remains an area of strong debate. Previously the authors showed that all models with independent 2D templates that allowed 2D rotations in the image plane cannot account for human performance in discriminating novel object views [1]. Here the authors derive an analytic formulation of a Bayesian model that gives rise to the best possible performance under 2D affine transformations and demonstrate that this model cannot account for human performance in 3D object discrimination. Relative to this model, human statistical efficiency is higher for novel views than for learned views, suggesting that human observers have used some 3D structural information.},
 author = {Liu, Zili and Kersten, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3e313b9badf12632cdae5452d20e1af6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3e313b9badf12632cdae5452d20e1af6-Metadata.json},
 openalex = {W2020825282},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3e313b9badf12632cdae5452d20e1af6-Paper.pdf},
 publisher = {MIT Press},
 title = {2D observers for human 3D object recognition?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/3e313b9badf12632cdae5452d20e1af6-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_3ff31b21,
 abstract = {We have constructed an inexpensive, video-based, motorized tracking system that learns to track a head. It uses real time graphical user inputs or an auxiliary infrared detector as supervisory signals to train a convolutional neural network. The inputs to the neural network consist of normalized luminance and chrominance images and motion information from frame differences. Subsampled images are also used to provide scale invariance. During the online training phase, the neural network rapidly adjusts the input weights depending upon the reliability of the different channels in the surrounding environment. This quick adaptation allows the system to robustly track a head even when other objects are moving within a cluttered background.},
 author = {Lee, Daniel D and Seung, H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3ff31b21755de79edf5668a07bd37f81-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3ff31b21755de79edf5668a07bd37f81-Metadata.json},
 openalex = {W2115839579},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/3ff31b21755de79edf5668a07bd37f81-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neural Network Based Head Tracking System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/3ff31b21755de79edf5668a07bd37f81-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_411ae1bf,
 author = {Kvale, Mark and Schreiner, Christoph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/411ae1bf081d1674ca6091f8c59a266f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/411ae1bf081d1674ca6091f8c59a266f-Metadata.json},
 openalex = {W2105848069},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/411ae1bf081d1674ca6091f8c59a266f-Paper.pdf},
 publisher = {MIT Press},
 title = {Perturbative M-Sequences for Auditory Systems Identification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/411ae1bf081d1674ca6091f8c59a266f-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_42ffcf05,
 abstract = {We describe a system for learning J. S. Bach's rules of musical harmony. These rules are learned from examples and are expressed as rule-based neural networks. The rules are then applied in real-time to generate new accompanying harmony for a live performer. Real-time functionality imposes constraints on the learning and harmonizing processes, including limitations on the types of information the system can use as input and the amount of processing the system can perform. We demonstrate algorithms for generating and refining musical rules from examples which meet these constraints. We describe a method for including a priori knowledge into the rules which yields significant performance gains. We then describe techniques for applying these rules to generate new music in real-time. We conclude the paper with an analysis of experimental results.},
 author = {Spangler, Randall and Goodman, Rodney and Hawkins, Jim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/42ffcf057e133f94c1b7b5cf543ef3bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/42ffcf057e133f94c1b7b5cf543ef3bd-Metadata.json},
 openalex = {W2141723886},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/42ffcf057e133f94c1b7b5cf543ef3bd-Paper.pdf},
 publisher = {MIT Press},
 title = {Bach in a Box - Real-Time Harmony},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/42ffcf057e133f94c1b7b5cf543ef3bd-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_46771d1f,
 abstract = {We discuss the development of a Multi-Layer Perceptron neural network classifier for use in preoperative differentiation between benign and malignant ovarian tumors. As the Mean Squared classification Error is not sufficient to make correct and objective assessments about the performance of the neural classifier, the concepts of sensitivity and specificity are introduced and combined in Receiver Operating Characteristic curves. Based on objective observations such as sonomorphologic criteria, color Doppler imaging and results from serum tumor markers, the neural network is able to make reliable predictions with a discriminating performance comparable to that of experienced gynecologists.},
 author = {Verrelst, Herman and Moreau, Yves and Vandewalle, Joos and Timmerman, Dirk},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/46771d1f432b42343f56f791422a4991-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/46771d1f432b42343f56f791422a4991-Metadata.json},
 openalex = {W2170311364},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/46771d1f432b42343f56f791422a4991-Paper.pdf},
 publisher = {MIT Press},
 title = {Use of a Multi-Layer Perceptron to Predict Malignancy in Ovarian Tumors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/46771d1f432b42343f56f791422a4991-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_489d0396,
 abstract = {We derive a learning algorithm for inferring an overcomplete basis by viewing it as probabilistic model of the observed data. Overcomplete bases allow for better approximation of the underlying statistical density. Using a Laplacian prior on the basis coefficients removes redundancy and leads to representations that are sparse and are a nonlinear function of the data. This can be viewed as a generalization of the technique of independent component analysis and provides a method for blind source separation of fewer mixtures than sources. We demonstrate the utility of overcomplete representations on natural speech and show that compared to the traditional Fourier basis the inferred representations potentially have much greater coding efficiency.},
 author = {Lewicki, Michael and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/489d0396e6826eb0c1e611d82ca8b215-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/489d0396e6826eb0c1e611d82ca8b215-Metadata.json},
 openalex = {W2149239179},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/489d0396e6826eb0c1e611d82ca8b215-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Nonlinear Overcomplete Representations for Efficient Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/489d0396e6826eb0c1e611d82ca8b215-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_4dcae38e,
 abstract = {We study several statistically and biologically motivated learning rules using the same visual environment: one made up of natural scenes and the same single-cell neuronal architecture. This allows us to concentrate on the feature extraction and neuronal coding properties of these rules. Included in these rules are kurtosis and skewness maximization, the quadratic form of the Bienenstock-Cooper-Munro (BCM) learning rule, and single-cell independent component analysis. Using a structure removal method, we demonstrate that receptive fields developed using these rules depend on a small portion of the distribution. We find that the quadratic form of the BCM rule behaves in a manner similar to a kurtosis maximization rule when the distribution contains kurtotic directions, although the BCM modification equations are computationally simpler.},
 author = {Blais, Brian and Intrator, Nathan and Shouval, Harel and Cooper, Leon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4dcae38ee11d3a6606cc6cd636a3628b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4dcae38ee11d3a6606cc6cd636a3628b-Metadata.json},
 openalex = {W2167275700},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4dcae38ee11d3a6606cc6cd636a3628b-Paper.pdf},
 publisher = {MIT Press},
 title = {Receptive Field Formation in Natural Scene Environments: Comparison of Single-Cell Learning Rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/4dcae38ee11d3a6606cc6cd636a3628b-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_4e0d67e5,
 abstract = {In integrated service communication networks, an important problem is to exercise call admission control and routing so as to optimally use the network resources. This problem is naturally formulated as a dynamic programming problem, which, however, is too complex to be solved exactly. We use methods of reinforcement learning (RL), together with a decomposition approach, to find call admission control and routing policies. The performance of our policy for a network with approximately 1045 different feature configurations is compared with a commonly used heuristic policy.},
 author = {Marbach, Peter and Mihatsch, Oliver and Schulte, Miriam and Tsitsiklis, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4e0d67e54ad6626e957d15b08ae128a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4e0d67e54ad6626e957d15b08ae128a6-Metadata.json},
 openalex = {W2125132331},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4e0d67e54ad6626e957d15b08ae128a6-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning for Call Admission Control and Routing in Integrated Service Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/4e0d67e54ad6626e957d15b08ae128a6-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_4e8412ad,
 abstract = {A new class of plug in classification techniques have recently been developed in the statistics and machine learning literature. A plug in classification technique (PaCT) is a method that takes a standard classifier (such as LDA or TREES) and plugs it into an algorithm to produce a new classifier. The standard classifier is known as the Plug in Classifier (PiC). These methods often produce large improvements over using a single classifier. In this paper we investigate one of these methods and give some motivation for its success.},
 author = {James, Gareth and Hastie, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4e8412ad48562e3c9934f45c3e144d48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4e8412ad48562e3c9934f45c3e144d48-Metadata.json},
 openalex = {W2096914831},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4e8412ad48562e3c9934f45c3e144d48-Paper.pdf},
 publisher = {MIT Press},
 title = {The Error Coding and Substitution PaCTs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/4e8412ad48562e3c9934f45c3e144d48-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_4edaa105,
 abstract = {We model the responses of cells in visual area V1 during natural vision. Our model consists of a classical energy mechanism whose output is divided by nonclassical gain control and texture contrast mechanisms. We apply this model to review movies, a stimulus sequence that replicates the stimulation a cell receives during free viewing of natural images. Data were collected from three cells using five different review movies, and the model was fit separately to the data from each movie. For the energy mechanism alone we find modest but significant correlations (rE = 0.41, 0.43, 0.59, 0.35) between model and data. These correlations are improved somewhat when we allow for suppressive surround effects (rE+G = 0.42, 0.56, 0.60, 0.37). In one case the inclusion of a delayed suppressive surround dramatically improves the fit to the data by modifying the time course of the model's response.},
 author = {Vinje, William and Gallant, Jack},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4edaa105d5f53590338791951e38c3ad-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4edaa105d5f53590338791951e38c3ad-Metadata.json},
 openalex = {W2100035604},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4edaa105d5f53590338791951e38c3ad-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling Complex Cells in an Awake Macaque during Natural Image Viewing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/4edaa105d5f53590338791951e38c3ad-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_4f87658e,
 abstract = {Recent theoretical results for pattern classification with thresholded real-valued functions (such as support vector machines, sigmoid networks, and boosting) give bounds on misclassification probability that do not depend on the size of the classifier, and hence can be considerably smaller than the bounds that follow from the VC theory. In this paper, we show that these techniques can be more widely applied, by representing other boolean functions as two-layer neural networks (thresholded convex combinations of boolean functions). For example, we show that with high probability any decision tree of depth no more than d that is consistent with m training examples has misclassification probability no more than O((1/m(Neff VCdim(U) log2 m log d))1/2), where U is the class of node decision functions, and Neff â‰¤ N can be thought of as the effective number of leaves (it becomes small as the distribution on the leaves induced by the training data gets far from uniform). This bound is qualitatively different from the VC bound and can be considerably smaller.

We use the same technique to give similar results for DNF formulae.},
 author = {Golea, Mostefa and Bartlett, Peter and Lee, Wee Sun and Mason, Llew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4f87658ef0de194413056248a00ce009-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4f87658ef0de194413056248a00ce009-Metadata.json},
 openalex = {W2135686858},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4f87658ef0de194413056248a00ce009-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalization in Decision Trees and DNF: Does Size Matter?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/4f87658ef0de194413056248a00ce009-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_4fa7c625,
 abstract = {If globally high dimensional data has locally only low dimensional distributions, it is advantageous to perform a local dimensionality reduction before further processing the data. In this paper we examine several techniques for local dimensionality reduction in the context of locally weighted linear regression. As possible candidates, we derive local versions of factor analysis regression, principle component regression, principle component regression on joint distributions, and partial least squares regression. After outlining the statistical bases of these methods, we perform Monte Carlo simulations to evaluate their robustness with respect to violations of their statistical assumptions. One surprising outcome is that locally weighted partial least squares regression offers the best average results, thus outperforming even factor analysis, the theoretically most appealing of our candidate techniques.},
 author = {Schaal, Stefan and Vijayakumar, Sethu and Atkeson, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4fa7c62536118cc404dec4a0ca88d4f6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4fa7c62536118cc404dec4a0ca88d4f6-Metadata.json},
 openalex = {W2138172627},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/4fa7c62536118cc404dec4a0ca88d4f6-Paper.pdf},
 publisher = {MIT Press},
 title = {Local Dimensionality Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/4fa7c62536118cc404dec4a0ca88d4f6-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_512c5cad,
 abstract = {In the developing nervous system, gradients of target-derived diffusible factors play an important role in guiding axons to appropriate targets. In this paper, the shape that such a gradient might have is calculated as a function of distance from the target and the time since the start of factor production. Using estimates of the relevant parameter values from the experimental literature, the spatiotemporal domain in which a growth cone could detect such a gradient is derived. For large times, a value for the maximum guidance range of about 1 mm is obtained. This value fits well with experimental data. For smaller times, the analysis predicts that guidance over longer ranges may be possible. This prediction remains to be tested.},
 author = {Goodhill, Geoffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/512c5cad6c37edb98ae91c8a76c3a291-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/512c5cad6c37edb98ae91c8a76c3a291-Metadata.json},
 openalex = {W2124806160},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf},
 publisher = {MIT Press},
 title = {A Mathematical Model of Axon Guidance by Diffusible Factors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/512c5cad6c37edb98ae91c8a76c3a291-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_536a76f9,
 abstract = {The generalization ability of a neural network can sometimes be improved dramatically by regularization. To analyze the improvement one needs more refined results than the asymptotic distribution of the weight vector. Here we study the simple case of one-dimensional linear regression under quadratic regularization, i.e., ridge regression. We study the random design, misspecified case, where we derive expansions for the optimal regularization parameter and the ensuing improvement. It is possible to construct examples where it is best to use no regularization.},
 author = {Koistinen, Petri},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/536a76f94cf7535158f66cfbd4b113b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/536a76f94cf7535158f66cfbd4b113b6-Metadata.json},
 openalex = {W2149814254},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/536a76f94cf7535158f66cfbd4b113b6-Paper.pdf},
 publisher = {MIT Press},
 title = {Asymptotic Theory for Regularization: One-Dimensional Linear Case},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/536a76f94cf7535158f66cfbd4b113b6-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_54072f48,
 abstract = {Nystagmus is a pattern of eye movement characterized by smooth rotations of the eye in one direction and rapid rotations in the opposite direction that reset eye position. Periodic alternating nystagmus (PAN) is a form of uncontrollable nystagmus that has been described as an unstable but amplitude-limited oscillation. PAN has been observed previously only in subjects with vestibulo-cerebellar damage. We describe results in which PAN can be produced in normal subjects by prolonged rotation in darkness. We propose a new model in which the neural circuits that control eye movement are inherently unstable, but this instability is kept in check under normal circumstances by the cerebellum. Circumstances which alter this cerebellar restraint, such as vestibulocerebellar damage or plasticity due to rotation in darkness, can lead to PAN.},
 author = {Dow, Ernst and Anastasio, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/54072f485cdb7897ebbcaf7525139561-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/54072f485cdb7897ebbcaf7525139561-Metadata.json},
 openalex = {W2123742876},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/54072f485cdb7897ebbcaf7525139561-Paper.pdf},
 publisher = {MIT Press},
 title = {Instabilities in Eye Movement Control: A Model of Periodic Alternating Nystagmus},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/54072f485cdb7897ebbcaf7525139561-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_540ae6b0,
 abstract = {We present a neural model that can perform eye movements to a particular side of an object regardless of the position and orientation of the object in space, a generalization of a task which has been recently used by Olson and Gettner [4] to investigate the neural structure of object-centered representations. Our model uses an intermediate representation in which units have oculocentric receptive fields-just like collicular neurons-whose gain is modulated by the side of the object to which the movement is directed, as well as the orientation of the object. We show that these gain modulations are consistent with Olson and Gettner's single cell recordings in the supplementary eye field. This demonstrates that it is possible to perform an object-centered task without a representation involving an object-centered map, viz., without neurons whose receptive fields are defined in object-centered coordinates. We also show that the same approach can account for object-centered neglect, a situation in which patients with a right parietal lesion neglect the left side of objects regardless of the orientation of the objects.},
 author = {Den\`{e}ve, Sophie and Pouget, Alexandre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/540ae6b0f6ac6e155062f3dd4f0b2b01-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/540ae6b0f6ac6e155062f3dd4f0b2b01-Metadata.json},
 openalex = {W2169117584},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/540ae6b0f6ac6e155062f3dd4f0b2b01-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Basis of Object-Centered Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/540ae6b0f6ac6e155062f3dd4f0b2b01-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_56352739,
 abstract = {This paper presents a new approach to the problem of modelling daily rainfall using neural networks. We first model the conditional distributions of rainfall amounts, in such a way that the model itself determines the order of the process, and the time-dependent shape and scale of the conditional distributions. After integrating over particular weather patterns, we are able to extract seasonal variations and long-term trends.},
 author = {Williams, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/56352739f59643540a3a6e16985f62c7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/56352739f59643540a3a6e16985f62c7-Metadata.json},
 openalex = {W2144070615},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/56352739f59643540a3a6e16985f62c7-Paper.pdf},
 publisher = {MIT Press},
 title = {Modelling Seasonality and Trends in Daily Rainfall Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/56352739f59643540a3a6e16985f62c7-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_56468d56,
 abstract = {The storage capacity, that is the number of patterns which can be stored per weight, is calculated for the fully-connected committee machine with real couplings and K hidden units from the vanishing of the entropy of the internal representations, and it is found to diverge as .},
 author = {Xiong, Yuansheng and Kwon, Chulan and Oh, Jong-Hoon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/56468d5607a5aaf1604ff5e15593b003-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/56468d5607a5aaf1604ff5e15593b003-Metadata.json},
 openalex = {W2024367258},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/56468d5607a5aaf1604ff5e15593b003-Paper.pdf},
 publisher = {MIT Press},
 title = {Storage capacity of the fully-connected committee machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/56468d5607a5aaf1604ff5e15593b003-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_571d3a94,
 abstract = {The problem of time series prediction is studied within the uniform convergence framework of Vapnik and Chervonenkis. The dependence inherent in the temporal structure is incorporated into the analysis, thereby generalizing the available theory for memoryless processes. Finite sample bounds are calculated in terms of covering numbers of the approximating class, and the tradeoff between approximation and estimation is discussed. A complexity regularization approach is outlined, based on Vapnik's method of Structural Risk Minimization, and shown to be applicable in the context of mixing stochastic processes.},
 author = {Meir, Ron},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/571d3a9420bfd9219f65b643d0003bf4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/571d3a9420bfd9219f65b643d0003bf4-Metadata.json},
 openalex = {W2149465625},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/571d3a9420bfd9219f65b643d0003bf4-Paper.pdf},
 publisher = {MIT Press},
 title = {Structural Risk Minimization for Nonparametric Time Series Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/571d3a9420bfd9219f65b643d0003bf4-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_59f51fd6,
 abstract = {A simple linear averaging of the outputs of several networks as e.g. in bagging [3], seems to follow naturally from a bias/variance decomposition of the sum-squared error. The sum-squared error of the average model is a quadratic function of the weighting factors assigned to the networks in the ensemble [7], suggesting a quadratic programming algorithm for finding the weighting factors.

If we interpret the output of a network as a probability statement, the sum-squared error corresponds to minus the loglikelihood or the Kullback-Leibler divergence, and linear averaging of the outputs to logarithmic averaging of the probability statements: the logarithmic opinion pool.

The crux of this paper is that this whole story about model averaging, bias/variance decompositions, and quadratic programming to find the optimal weighting factors, is not specific for the sum-squared error, but applies to the combination of probability statements of any kind in a logarithmic opinion pool, as long as the Kullback-Leibler divergence plays the role of the error measure. As examples we treat model averaging for classification models under a cross-entropy error measure and models for estimating variances.},
 author = {Heskes, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/59f51fd6937412b7e56ded1ea2470c25-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/59f51fd6937412b7e56ded1ea2470c25-Metadata.json},
 openalex = {W2148124601},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/59f51fd6937412b7e56ded1ea2470c25-Paper.pdf},
 publisher = {MIT Press},
 title = {Selecting Weighting Factors in Logarithmic Opinion Pools},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/59f51fd6937412b7e56ded1ea2470c25-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_5ca3e9b1,
 abstract = {We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and behavior-based or teleo-reactive approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.},
 author = {Parr, Ronald and Russell, Stuart},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/5ca3e9b122f61f8f06494c97b1afccf3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/5ca3e9b122f61f8f06494c97b1afccf3-Metadata.json},
 openalex = {W2158548602},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning with Hierarchies of Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_5e1b18c4,
 abstract = {For blind source separation, when the Fisher information matrix is used as the Riemannian metric tensor for the parameter space, the steepest descent algorithm to maximize the likelihood function in this Riemannian parameter space becomes the serial updating rule with equivariant property. This algorithm can be further simplified by using the asymptotic form of the Fisher information matrix around the equilibrium.},
 author = {Yang, Howard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Metadata.json},
 openalex = {W2159601469},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Paper.pdf},
 publisher = {MIT Press},
 title = {Multiplicative Updating Rule for Blind Separation Derived from the Method of Scoring},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/5e1b18c4c6a6d31695acbae3fd70ecc6-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_5e76bef6,
 abstract = {We discuss a solution to the problem of separating waveforms produced by multiple cells in an extracellular neural recording. We take an explicitly probabilistic approach, using latent-variable models of varying sophistication to describe the distribution of waveforms produced by a single cell. The models range from a single Gaussian distribution of waveforms for each cell to a mixture of hidden Markov models. We stress the overall statistical structure of the approach, allowing the details of the generative model chosen to depend on the specific neural preparation.},
 author = {Sahani, Maneesh and Pezaris, John and Andersen, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/5e76bef6e019b2541ff53db39f407a98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/5e76bef6e019b2541ff53db39f407a98-Metadata.json},
 openalex = {W2123367539},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/5e76bef6e019b2541ff53db39f407a98-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Separation of Signals from Neighboring Cells in Tetrode Recordings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/5e76bef6e019b2541ff53db39f407a98-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_602d1305,
 abstract = {There is strong evidence that face processing is localized in the brain. The double dissociation between prosopagnosia, a face recognition deficit occurring after brain damage, and visual object agnosia, difficulty recognizing other kinds of complex objects, indicates that face and nonface object recognition may be served by partially independent mechanisms in the brain. Is neural specialization innate or learned? We suggest that this specialization could be tbe result of a competitive learning mechanism that, during development, devotes neural resources to the tasks they are best at performing. Furtber, we suggest that the specialization arises as an interaction between task requirements and developmental constraints. In this paper, we present a feed-forward computational model of visual processing, in which two modules compete to classify input stimuli. When one module receives low spatial frequency information and the other receives high spatial frequency information, and the task is to identify the faces while simply classifying the objects, the low frequency network shows a strong specialization for faces. No other combination of tasks and inputs shows this strong specialization. We take these results as support for the idea that an innately-specified face processing module is unnecessary.},
 author = {Dailey, Matthew and Cottrell, Garrison},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/602d1305678a8d5fdb372271e980da6a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/602d1305678a8d5fdb372271e980da6a-Metadata.json},
 openalex = {W2108979579},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/602d1305678a8d5fdb372271e980da6a-Paper.pdf},
 publisher = {MIT Press},
 title = {Task and Spatial Frequency Effects on Face Specialization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/602d1305678a8d5fdb372271e980da6a-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_674bfc5f,
 abstract = {Severe contamination of electroencephalographic (EEG) activity by eye movements, blinks, muscle, heart and line noise is a serious problem for EEG interpretation and analysis. Rejecting contaminated EEG segments results in a considerable loss of information and may be impractical for clinical data. Many methods have been proposed to remove eye movement and blink artifacts from EEG recordings. Often regression in the time or frequency domain is performed on simultaneous EEG and electrooculographic (EOG) recordings to derive parameters characterizing the appearance and spread of EOG artifacts in the EEG channels. However, EOG records also contain brain signals [1, 2], so regressing out EOG activity inevitably involves subtracting a portion of the relevant EEG signal from each recording as well. Regression cannot be used to remove muscle noise or line noise, since these have no reference channels. Here, we propose a new and generally applicable method for removing a wide variety of artifacts from EEG records. The method is based on an extended version of a previous Independent Component Analysis (ICA) algorithm [3, 4] for performing blind source separation on linear mixtures of independent source signals with either sub-Gaussian or super-Gaussian distributions. Our results show that ICA can effectively detect, separate and remove activity in EEG records from a wide variety of artifactual sources, with results comparing favorably to those obtained using regression-based methods.},
 author = {Jung, Tzyy-Ping and Humphries, Colin and Lee, Te-Won and Makeig, Scott and McKeown, Martin and Iragui, Vicente and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/674bfc5f6b72706fb769f5e93667bd23-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/674bfc5f6b72706fb769f5e93667bd23-Metadata.json},
 openalex = {W2164360360},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/674bfc5f6b72706fb769f5e93667bd23-Paper.pdf},
 publisher = {MIT Press},
 title = {Extended ICA Removes Artifacts from Electroencephalographic Recordings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/674bfc5f6b72706fb769f5e93667bd23-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_6786f3c6,
 abstract = {Bayesian methods have been successfully applied to regression and classification problems in multi-layer perceptrons. We present a novel application of Bayesian techniques to Radial Basis Function networks by developing a Gaussian approximation to the posterior distribution which, for fixed basis function widths, is analytic in the parameters. The setting of regularization constants by cross-validation is wasteful as only a single optimal parameter estimate is retained. We treat this issue by assigning prior distributions to these constants, which are then adapted in light of the data under a simple re-estimation formula.},
 author = {Barber, David and Schottky, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6786f3c62fbf9021694f6e51cc07fe3c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6786f3c62fbf9021694f6e51cc07fe3c-Metadata.json},
 openalex = {W2158204126},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6786f3c62fbf9021694f6e51cc07fe3c-Paper.pdf},
 publisher = {MIT Press},
 title = {Radial Basis Functions: A Bayesian Treatment},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/6786f3c62fbf9021694f6e51cc07fe3c-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_6a5889bb,
 abstract = {We apply information maximization / maximum likelihood blind source separation [2, 6] to complex valued signals mixed with complex valued nonstationary matrices. This case arises in radio communications with baseband signals. We incorporate known source signal distributions in the adaptation, thus making the algorithms less blind. This results in drastic reduction of the amount of data needed for successful convergence. Adaptation to rapidly changing signal mixing conditions, such as to fading in mobile communications, becomes now feasible as demonstrated by simulations.},
 author = {Torkkola, Kari},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6a5889bb0190d0211a991f47bb19a777-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6a5889bb0190d0211a991f47bb19a777-Metadata.json},
 openalex = {W2127742081},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6a5889bb0190d0211a991f47bb19a777-Paper.pdf},
 publisher = {MIT Press},
 title = {Blind Separation of Radio Signals in Fading Channels},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/6a5889bb0190d0211a991f47bb19a777-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_6c1da886,
 abstract = {Most computational engineering based loosely on biology uses continuous variables to represent neural activity. Yet most neurons communicate with action potentials. The engineering view is equivalent to using a rate-code for representing information and for computing. An increasing number of examples are being discovered in which biology may not be using rate codes. Information can be represented using the timing of action potentials, and efficiently computed with in this representation. The problem of odour identification is a simple problem which can be efficiently solved using action potential timing and an underlying rhythm. By using adapting units to effect a fundamental change of representation of a problem, we map the recognition of words (having uniform time-warp) in connected speech into the same analog match problem. We describe the architecture and preliminary results of such a recognition system. Using the fast events of biology in conjunction with an underlying rhythm is one way to overcome the limits of an event-driven view of computation. When the intrinsic hardware is much faster than the time scale of change of inputs, this approach can greatly increase the effective computation per unit time on a given quantity of hardware.},
 author = {Hopfield, John J. and Brody, Carlos and Roweis, Sam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6c1da886822c67822bcf3679d04369fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6c1da886822c67822bcf3679d04369fa-Metadata.json},
 openalex = {W2168756233},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},
 publisher = {MIT Press},
 title = {Computing with Action Potentials},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_6d9c547c,
 abstract = {We derive a first-order approximation of the density of maximum entropy for a continuous 1-D random variable, given a number of simple constraints. This results in a density expansion which is somewhat similar to the classical polynomial density expansions by Gram-Charlier and Edgeworth. Using this approximation of density, an approximation of 1-D differential entropy is derived. The approximation of entropy is both more exact and more robust against outliers than the classical approximation based on the polynomial density expansions, without being computationally more expensive. The approximation has applications, for example, in independent component analysis and projection pursuit.},
 author = {Hyv\"{a}rinen, Aapo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6d9c547cf146054a5a720606a7694467-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6d9c547cf146054a5a720606a7694467-Metadata.json},
 openalex = {W2141622014},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6d9c547cf146054a5a720606a7694467-Paper.pdf},
 publisher = {MIT Press},
 title = {New Approximations of Differential Entropy for Independent Component Analysis and Projection Pursuit},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/6d9c547cf146054a5a720606a7694467-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_6d9cb7de,
 abstract = {We have studied the application of an independent component analysis (ICA) approach to the identification and possible removal of artifacts from a magnetoencephalographic (MEG) recording. This statistical technique separates components according to the kurtosis of their amplitude distributions over time, thus distinguishing between strictly periodical signals, and regularly and irregularly occurring signals. Many artifacts belong to the last category. In order to assess the effectiveness of the method, controlled artifacts were produced, which included saccadic eye movements and blinks, increased muscular tension due to biting and the presence of a digital watch inside the magnetically shielded room. The results demonstrate the capability of the method to identify and clearly isolate the produced artifacts.},
 author = {Vig\'{a}rio, Ricardo and Jousm\"{a}ki, Veikko and H\"{a}m\"{a}l\"{a}inen, Matti and Hari, Riitta and Oja, Erkki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Metadata.json},
 openalex = {W2147618476},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf},
 publisher = {MIT Press},
 title = {Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/6d9cb7de5e8ac30bd5e8734bc96a35c1-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_70feb62b,
 abstract = {We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together.The coupling model is similar to the Bradley-Terry method for paired comparisons.We study the nature of the class probability estimates that arise, and examine the performance of the procedure in real and simulated data sets.Classifiers used include linear discriminants, nearest neighbors, adaptive nonlinear methods and the support vector machine.},
 author = {Hastie, Trevor and Tibshirani, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/70feb62b69f16e0238f741fab228fec2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/70feb62b69f16e0238f741fab228fec2-Metadata.json},
 openalex = {W2019575783},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf},
 publisher = {MIT Press},
 title = {Classification by pairwise coupling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_7895fc13,
 abstract = {A learning system composed of linear control modules, reinforcement learning modules and selection modules (a hybrid reinforcement learning system) is proposed for the fast learning of real-world control problems. The selection modules choose one appropriate control module dependent on the state. This hybrid learning system was applied to the control of a stilt-type biped robot. It learned the control on a sloped floor more quickly than the usual reinforcement learning because it did not need to learn the control on a flat floor, where the linear control module can control the robot. When it was trained by a 2-step learning (during the first learning step, the selection module was trained by a training procedure controlled only by the linear controller), it learned the control more quickly. The average number of trials (about 50) is so small that the learning system is applicable to real robot control.},
 author = {Yamada, Satoshi and Watanabe, Akira and Nakashima, Michio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7895fc13088ee37f511913bac71fa66f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7895fc13088ee37f511913bac71fa66f-Metadata.json},
 openalex = {W2164966957},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7895fc13088ee37f511913bac71fa66f-Paper.pdf},
 publisher = {MIT Press},
 title = {Hybrid Reinforcement Learning and Its Application to Biped Robot Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/7895fc13088ee37f511913bac71fa66f-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_78b9cab1,
 abstract = {Here we analyze synaptic transmission from an information-theoretic perspective. We derive closed-form expressions for the lower-bounds on the capacity of a simple model of a cortical synapse under two explicit coding paradigms. Under the estimation paradigm, we assume the signal to be encoded in the mean firing rate of a Poisson neuron. The performance of an optimal linear estimator of the signal then provides a lower bound on the capacity for signal estimation. Under the detection paradigm, the presence or absence of the signal has to be detected. Performance of the optimal spike detector allows us to compute a lower bound on the capacity for signal detection. We find that single synapses (for empirically measured parameter values) transmit information poorly but significant improvement can be achieved with a small amount of redundancy.},
 author = {Manwani, Amit and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/78b9cab19959e4af8ff46156ee460c74-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/78b9cab19959e4af8ff46156ee460c74-Metadata.json},
 openalex = {W2118372081},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/78b9cab19959e4af8ff46156ee460c74-Paper.pdf},
 publisher = {MIT Press},
 title = {Synaptic Transmission: An Information-Theoretic Perspective},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/78b9cab19959e4af8ff46156ee460c74-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_792c7b5a,
 abstract = {In macaque inferotemporal cortex (IT), neurons have been found to respond selectively to complex shapes while showing broad tuning (invariance) with respect to stimulus transformations such as translation and scale changes and a limited tuning to rotation in depth. Training monkeys with novel, paperclip-like objects, Logothetis et al. could investigate whether these invariance properties are due to experience with exhaustively many transformed instances of an object or if there are mechanisms that allow the cells to show response invariance also to previously unseen instances of that object. They found object-selective cells in anterior IT which exhibited limited invariance to various transformations after training with single object views. While previous models accounted for the tuning of the cells for rotations in depth and for their selectivity to a specific object relative to a population of distractor objects, the model described here attempts to explain in a biologically plausible way the additional properties of translation and size invariance. Using the same stimuli as in the experiment, we find that model IT neurons exhibit invariance properties which closely parallel those of real neurons. Simulations show that the model is capable of unsupervised learning of view-tuned neurons.},
 author = {Riesenhuber, Maximilian and Poggio, Tomaso},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/792c7b5aae4a79e78aaeda80516ae2ac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/792c7b5aae4a79e78aaeda80516ae2ac-Metadata.json},
 openalex = {W2136123750},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/792c7b5aae4a79e78aaeda80516ae2ac-Paper.pdf},
 publisher = {MIT Press},
 title = {Just One View: Invariances in Inferotemporal Cell Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/792c7b5aae4a79e78aaeda80516ae2ac-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_79a49b3e,
 abstract = {Classification of finite sequences without explicit knowledge of their statistical nature is a fundamental problem with many important applications. We propose a new information theoretic approach to this problem which is based on the following ingredients: (i) sequences are similar when they are likely to be generated by the same source; (ii) cross entropies can be estimated via universal compression; (iii) Markovian sequences can be asymptotically-optimally merged.

With these ingredients we design a method for the classification of discrete sequences whenever they can be compressed. We introduce the method and illustrate its application for hierarchical clustering of languages and for estimating similarities of protein sequences.},
 author = {El-Yaniv, Ran and Fine, Shai and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/79a49b3e3762632813f9e35f4ba53d6c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/79a49b3e3762632813f9e35f4ba53d6c-Metadata.json},
 openalex = {W2143079199},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/79a49b3e3762632813f9e35f4ba53d6c-Paper.pdf},
 publisher = {MIT Press},
 title = {Agnostic Classification of Markovian Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/79a49b3e3762632813f9e35f4ba53d6c-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_7a674153,
 abstract = {We implement a model of obstacle avoidance in flying insects on a small, monocular robot. The result is a system that is capable of rapid navigation through a dense obstacle field. The key to the system is the use of zigzag behavior to articulate the body during movement. It is shown that this behavior compensates for a parallax blind spot surrounding the focus of expansion normally found in systems without parallax behavior. The system models the cooperation of several behaviors: halteres-ocular response (similar to VOR), optomotor response, and the parallax field computation and mapping to motor system. The resulting system is neurally plausible, very simple, and should be easily hosted on a VLSI hardware.},
 author = {Lewis, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7a674153c63cff1ad7f0e261c369ab2c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7a674153c63cff1ad7f0e261c369ab2c-Metadata.json},
 openalex = {W2160019354},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7a674153c63cff1ad7f0e261c369ab2c-Paper.pdf},
 publisher = {MIT Press},
 title = {Visual Navigation in a Robot Using Zig-Zag Behavior},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/7a674153c63cff1ad7f0e261c369ab2c-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_7b5b23f4,
 abstract = {Prioritized sweeping is a model-based reinforcement learning method that attempts to focus an agent's limited computational resources to achieve a good estimate of the value of environment states. To choose effectively where to spend a costly planning step, classic prioritized sweeping uses a simple heuristic to focus computation on the states that are likely to have the largest errors. In this paper, we introduce generalized prioritized sweeping, a principled method for generating such estimates in a representation-specific manner. This allows us to extend prioritized sweeping beyond an explicit, state-based representation to deal with compact representations that are necessary for dealing with large state spaces. We apply this method for generalized model approximators (such as Bayesian networks), and describe preliminary experiments that compare our approach with classical prioritized sweeping.},
 author = {Andre, David and Friedman, Nir and Parr, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7b5b23f4aadf9513306bcd59afb6e4c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7b5b23f4aadf9513306bcd59afb6e4c9-Metadata.json},
 openalex = {W2159420891},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7b5b23f4aadf9513306bcd59afb6e4c9-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalized Prioritized Sweeping},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/7b5b23f4aadf9513306bcd59afb6e4c9-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_7d6044e9,
 abstract = {The ability to rely on similarity metrics invariant to image transformations is an important issue for image classification tasks such as face or character recognition. We analyze an invariant metric that has performed well for the latter - the tangent distance - and study its limitations when applied to regular images, showing that the most significant among these (convergence to local minima) can be drastically reduced by computing the distance in a multiresolution setting. This leads to the multi resolution tangent distance, which exhibits significantly higher invariance to image transformations, and can be easily combined with robust estimation procedures.},
 author = {Vasconcelos, Nuno and Lippman, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7d6044e95a16761171b130dcb476a43e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7d6044e95a16761171b130dcb476a43e-Metadata.json},
 openalex = {W2158416517},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7d6044e95a16761171b130dcb476a43e-Paper.pdf},
 publisher = {MIT Press},
 title = {Multiresolution Tangent Distance for Affine-invariant Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/7d6044e95a16761171b130dcb476a43e-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_7f53f8c6,
 abstract = {In normal vision, the inputs from the two eyes are integrated into a single percept. When dissimilar images are presented to the two eyes, however, perceptual integration gives way to alternation between monocular inputs, a phenomenon called binocular rivalry. Although recent evidence indicates that binocular rivalry involves a modulation of neuronal responses in extrastriate cortex, the basic mechanisms responsible for differential processing of conflicting and congruent stimuli remain unclear. Using a neural network that models the mammalian early visual system, I demonstrate here that the desynchronized firing of cortical-like neurons that first receive inputs from the two eyes results in rivalrous activity patterns at later stages in the visual pathway. By contrast, synchronization of firing among these cells prevents such competition. The temporal coordination of cortical activity and its effects on neural competition emerge naturally from the network connectivity and from its dynamics. These results suggest that input-related differences in relative spike timing at an early stage of visual processing may give rise to the phenomena both of perceptual integration and rivalry in binocular vision.},
 author = {Lumer, Erik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7f53f8c6c730af6aeb52e66eb74d8507-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7f53f8c6c730af6aeb52e66eb74d8507-Metadata.json},
 openalex = {W2141636605},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/7f53f8c6c730af6aeb52e66eb74d8507-Paper.pdf},
 publisher = {MIT Press},
 title = {Effects of Spike Timing Underlying Binocular Integration and Rivalry in a Neural Model of Early Visual Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/7f53f8c6c730af6aeb52e66eb74d8507-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_81ca0262,
 abstract = {A model of motion detection is presented. The model contains three stages. The first stage is unoriented and is selective for contrast polarities. The next two stages work in parallel. A phase insensitive stage pools across different contrast polarities through a spatiotemporal filter and thus can detect first and second order motion. A phase sensitive stage keeps contrast polarities separate, each of which is filtered through a spatiotemporal filter, and thus only first order motion can be detected. Differential phase sensitivity can therefore account for the detection of first and second order motion. Phase insensitive detectors correspond to cortical complex cells, and phase sensitive detectors to simple cells.},
 author = {Grunewald, Alexander and Neumann, Heiko},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/81ca0262c82e712e50c580c032d99b60-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/81ca0262c82e712e50c580c032d99b60-Metadata.json},
 openalex = {W2157042729},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/81ca0262c82e712e50c580c032d99b60-Paper.pdf},
 publisher = {MIT Press},
 title = {Detection of First and Second Order Motion},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/81ca0262c82e712e50c580c032d99b60-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_82965d4e,
 abstract = {Multiple-instance learning is a variation on supervised learning, where the task is to learn a concept given positive and negative bags of instances. Each bag may contain many instances, but a bag is labeled positive even if only one of the instances in it falls within the concept. A bag is labeled negative only if all the instances in it are negative. We describe a new general framework, called Diverse Density, for solving multiple-instance learning problems. We apply this framework to learn a simple description of a person from a series of images (bags) containing that person, to a stock selection problem, and to the drug activity prediction problem.},
 author = {Maron, Oded and Lozano-P\'{e}rez, Tom\'{a}s},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/82965d4ed8150294d4330ace00821d77-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/82965d4ed8150294d4330ace00821d77-Metadata.json},
 openalex = {W2154318594},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/82965d4ed8150294d4330ace00821d77-Paper.pdf},
 publisher = {MIT Press},
 title = {A Framework for Multiple-Instance Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/82965d4ed8150294d4330ace00821d77-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_83adc922,
 abstract = {Monotonicity is a constraint which arises in many application domains. We present a machine learning model, the monotonic network, for which monotonicity can be enforced exactly, i.e., by virtue of functional form. A straightforward method for implementing and training a monotonic network is described. Monotonic networks are proven to be universal approximators of continuous, differentiable monotonic functions. We apply monotonic networks to a real-world task in corporate bond rating prediction and compare them to other approaches.},
 author = {Sill, Joseph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/83adc9225e4deb67d7ce42d58fe5157c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/83adc9225e4deb67d7ce42d58fe5157c-Metadata.json},
 openalex = {W2293093810},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/83adc9225e4deb67d7ce42d58fe5157c-Paper.pdf},
 publisher = {MIT Press},
 title = {Monotonic Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/83adc9225e4deb67d7ce42d58fe5157c-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_83cdcec0,
 author = {Kivinen, Jyrki and Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/83cdcec08fbf90370fcf53bdd56604ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/83cdcec08fbf90370fcf53bdd56604ff-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf},
 publisher = {MIT Press},
 title = {Relative Loss Bounds for Multidimensional Regression Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/83cdcec08fbf90370fcf53bdd56604ff-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_856fc816,
 abstract = {The initial activity-independent formation of a topographic map in the retinotectal system has long been thought to rely on the matching of molecular cues expressed in gradients in the retina and the tectum. However, direct experimental evidence for the existence of such gradients has only emerged since 1995. The new data has provoked the discussion of a new set of models in the experimental literature. Here, the capabilities of these models are analyzed, and the gradient shapes they predict in vivo are derived.},
 author = {Goodhill, Geoffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/856fc81623da2150ba2210ba1b51d241-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/856fc81623da2150ba2210ba1b51d241-Metadata.json},
 openalex = {W2139262970},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/856fc81623da2150ba2210ba1b51d241-Paper.pdf},
 publisher = {MIT Press},
 title = {Gradients for Retinotectal Mapping},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/856fc81623da2150ba2210ba1b51d241-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_86109d40,
 abstract = {Motivated by the findings of modular structure in the association cortex, we study a multi-modular model of associative memory that can successfully store memory patterns with different levels of activity. We show that the segregation of synaptic conductances into intra-modular linear and inter-modular nonlinear ones considerably enhances the network's memory retrieval performance. Compared with the conventional, single-module associative memory network, the multi-modular network has two main advantages: It is less susceptible to damage to columnar input, and its response is consistent with the cognitive data pertaining to category specific impairment.},
 author = {Levy, Nir and Horn, David and Ruppin, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/86109d400f0ed29e840b47ed72777c84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/86109d400f0ed29e840b47ed72777c84-Metadata.json},
 openalex = {W2115682540},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/86109d400f0ed29e840b47ed72777c84-Paper.pdf},
 publisher = {MIT Press},
 title = {Multi-modular Associative Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/86109d400f0ed29e840b47ed72777c84-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_861dc9bd,
 abstract = {We present a method for the analysis of nonstationary time series with multiple operating modes. In particular, it is possible to detect and to model both a switching of the dynamics and a less abrupt, time consuming drift from one mode to another. This is achieved in two steps. First, an unsupervised training method provides prediction experts for the inherent dynamical modes. Then, the trained experts are used in a hidden Markov model that allows to model drifts. An application to physiological wake/sleep data demonstrates that analysis and modeling of real-world time series can be improved when the drift paradigm is taken into account.},
 author = {Kohlmorgen, Jens and M\"{u}ller, Klaus-Robert and Pawelzik, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Metadata.json},
 openalex = {W2106030658},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Paper.pdf},
 publisher = {MIT Press},
 title = {Analysis of Drifting Dynamics with Neural Network Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_86e8f7ab,
 abstract = {We explain how the training data can be separated into clean information and unexplainable noise. Analogous to the data, the neural network is separated into a time invariant structure used for forecasting, and a noisy part. We propose a unified theory connecting the optimization algorithms for cleaning and learning together with algorithms that control the data noise and the parameter noise. The combined algorithm allows a data-driven local control of the liability of the network parameters and therefore an improvement in generalization. The approach is proven to be very useful at the task of forecasting the German bond market.},
 author = {Zimmermann, Hans-Georg and Neuneier, Ralph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/86e8f7ab32cfd12577bc2619bc635690-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/86e8f7ab32cfd12577bc2619bc635690-Metadata.json},
 openalex = {W2132222485},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 publisher = {MIT Press},
 title = {The Observer-Observation Dilemma in Neuro-Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_8b0d2689,
 abstract = {An analog model of the first stages of retinal processing has been constructed on a single silicon chip. Each photoreceptor computes the logarithm of the incident light intensity. A resistive network is used to compute a spatially smoothed version of the receptor outputs. An amplified difference between the receptor signals and their smoothed counterparts forms a second-order spatial filter. Measured outputs from an experimental 48 Ã— 48 pixel array show many of the characteristics of the bipolar cells in vertebrate retina.},
 author = {Itti, Laurent and Braun, Jochen and Lee, Dale and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8b0d268963dd0cfb808aac48a549829f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8b0d268963dd0cfb808aac48a549829f-Metadata.json},
 openalex = {W2177690825},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8b0d268963dd0cfb808aac48a549829f-Paper.pdf},
 publisher = {MIT Press},
 title = {A silicon model of early visual processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/8b0d268963dd0cfb808aac48a549829f-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_8b0dc65f,
 abstract = {A neural network approach to stereovision is presented based on aliasing effects of simple disparity estimators and a fast coherence-detection scheme. Within a single network structure, a dense disparity map with an associated validation map and, additionally, the fused cyclopean view of the scene are available. The network operations are based on simple, biological plausible circuitry; the algorithm is fully parallel and non-iterative.},
 author = {Henkel, Rolf},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8b0dc65f996f98fd178a9defd0efa077-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8b0dc65f996f98fd178a9defd0efa077-Metadata.json},
 openalex = {W2128773453},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8b0dc65f996f98fd178a9defd0efa077-Paper.pdf},
 publisher = {MIT Press},
 title = {A Simple and Fast Neural Network Approach to Stereovision},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/8b0dc65f996f98fd178a9defd0efa077-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_8d3369c4,
 abstract = {Perceptron Decision Trees (also known as Linear Machine DTs, etc.) are analysed in order that data-dependent Structural Risk Minimisation can be applied. Data-dependent analysis is performed which indicates that choosing the maximal margin hyperplanes at the decision nodes will improve the generalization. The analysis uses a novel technique to bound the generalization error in terms of the margins at individual nodes. Experiments performed on real data sets confirm the validity of the approach.},
 author = {Shawe-Taylor, John and Cristianini, Nello},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8d3369c4c086f236fabf61d614a32818-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8d3369c4c086f236fabf61d614a32818-Metadata.json},
 openalex = {W2148351311},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8d3369c4c086f236fabf61d614a32818-Paper.pdf},
 publisher = {MIT Press},
 title = {Data-Dependent Structural Risk Minimization for Perceptron Decision Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/8d3369c4c086f236fabf61d614a32818-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_8d9a0adb,
 abstract = {In many real world tasks, only a small fraction of the available inputs are important at any particular time. This paper presents a method for ascertaining the relevance of inputs by exploiting temporal coherence and predictability. The method proposed in this paper dynamically allocates relevance to inputs by using expectations of their future values. As a model of the task is learned, the model is simultaneously extended to create task-specific predictions of the future values of inputs. Inputs which are either not relevant, and therefore not accounted for in the model, or those which contain noise, will not be predicted accurately. These inputs can be de-emphasized, and, in turn, a new, improved, model of the task created. The techniques presented in this paper have yielded significant improvements for the vision-based autonomous control of a land vehicle, vision-based hand tracking in cluttered scenes, and the detection of faults in the etching of semiconductor wafers.},
 author = {Baluja, Shumeet},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8d9a0adb7c204239c9635426f35c9522-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8d9a0adb7c204239c9635426f35c9522-Metadata.json},
 openalex = {W3150356128},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8d9a0adb7c204239c9635426f35c9522-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Expectation to Guide Processing: A Study of Three Real-World Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/8d9a0adb7c204239c9635426f35c9522-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_8edd7215,
 abstract = {An image is often represented by a set of detected features. We get an enormous compression by representing images in this way. Furthermore, we get a representation which is little affected by small amounts of noise in the image. However, features are typically chosen in an ad hoc manner. We show how a good set of features can be obtained using sufficient statistics. The idea of sparse data representation naturally arises. We treat the 1-dimensional and 2-dimensional signal reconstruction problem to make our ideas concrete.},
 author = {Geiger, Davi and Rudra, Archisman and Maloney, Laurance},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8edd72158ccd2a879f79cb2538568fdc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8edd72158ccd2a879f79cb2538568fdc-Metadata.json},
 openalex = {W2134083676},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf},
 publisher = {MIT Press},
 title = {Features as Sufficient Statistics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/8edd72158ccd2a879f79cb2538568fdc-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_8fb21ee7,
 abstract = {The mathematical framework for factorizing equivalence classes of multivariate functions is formulated in this paper. Independent component analysis is shown to be a special case of this decomposition. Using only the local geometric structure of a class representative, we derive an analytic solution for the factorization. We demonstrate the factorization solution with numerical experiments and present a preliminary tie to decorrelation.},
 author = {Lin, Juan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8fb21ee7a2207526da55a679f0332de2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8fb21ee7a2207526da55a679f0332de2-Metadata.json},
 openalex = {W2112304897},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf},
 publisher = {MIT Press},
 title = {Factorizing Multivariate Function Classes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/8fb21ee7a2207526da55a679f0332de2-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_8fb5f8be,
 abstract = {We derive and analyse robust optimization schemes for noisy vector quantization on the basis of deterministic annealing. Starting from a cost function for central clustering that incorporates distortions from channel noise we develop a soft topographic vector quantization algorithm (STVQ) which is based on the maximum entropy principle and which performs a maximum-likelihood estimate in an expectation-maximization (EM) fashion. Annealing in the temperature parameter Î² leads to phase transitions in the existing code vector representation during the cooling process for which we calculate critical temperatures and modes as a function of eigenvectors and eigenvalues of the covariance matrix of the data and the transition matrix of the channel noise. A whole family of vector quantization algorithms is derived from STVQ, among them a deterministic annealing scheme for Kohonen's self-organizing map (SOM). This algorithm, which we call SSOM, is then applied to vector quantization of image data to be sent via a noisy binary symmetric channel. The algorithm's performance is compared to those of LBG and STVQ. While it is naturally superior to LBG, which does not take into account channel noise, its results compare very well to those of STVQ, which is computationally much more demanding.},
 author = {Burger, Matthias and Graepel, Thore and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Metadata.json},
 openalex = {W2151689057},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/8fb5f8be2aa9d6c64a04e3ab9f63feee-Paper.pdf},
 publisher = {MIT Press},
 title = {An Annealed Self-Organizing Map for Source Channel Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/8fb5f8be2aa9d6c64a04e3ab9f63feee-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_90db9da4,
 abstract = {We are frequently called upon to perform multiple tasks that compete for our attention and resource. Often we know the optimal solution to each task in isolation; in this paper, we describe how this knowledge can be exploited to efficiently find good solutions for doing the tasks in parallel. We formulate this problem as that of dynamically merging multiple Markov decision processes (MDPs) into a composite MDP, and present a new theoretically-sound dynamic programming algorithm for finding an optimal policy for the composite MDP. We analyze various aspects of our algorithm and illustrate its use on a simple merging problem.},
 author = {Singh, Satinder and Cohn, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/90db9da4fc5414ab55a9fe495d555c06-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/90db9da4fc5414ab55a9fe495d555c06-Metadata.json},
 openalex = {W2129307307},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/90db9da4fc5414ab55a9fe495d555c06-Paper.pdf},
 publisher = {MIT Press},
 title = {How to Dynamically Merge Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/90db9da4fc5414ab55a9fe495d555c06-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_95151403,
 abstract = {The response of a spiking neuron to a stimulus is often characterized by its time-varying firing rate, estimated from a histogram of spike times. If the cell's firing probability in each small time interval depends only on this firing rate, one predicts a highly variable response to repeated trials, whereas many neurons show much greater fidelity. Furthermore, the neuronal membrane is refractory immediately after a spike, so that the firing probability depends not only on the stimulus but also on the preceding spike train. To connect these observations, we investigated the relationship between the refractory period of a neuron and its firing precision. The light response of retinal ganglion cells was modeled as probabilistic firing combined with a refractory period: the instantaneous firing rate is the product of a "free firing rate, " which depends only on the stimulus, and a "recovery function," which depends only on the time since the last spike. This recovery function vanishes for an absolute refractory period and then gradually increases to unity. In simulations, longer refractory periods were found to make the response more reproducible, eventually matching the precision of measured spike trains. Refractoriness, although often thought to limit the performance of neurons, may in fact benefit neuronal reliability. The underlying free firing rate derived by allowing for the refractory period often exceeded the observed firing rate by an order of magnitude and was found to convey information about the stimulus over a much wider dynamic range. Thus, the free firing rate may be the preferred variable for describing the response of a spiking neuron.},
 author = {Berry, Michael and Meister, Markus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/95151403b0db4f75bfd8da0b393af853-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/95151403b0db4f75bfd8da0b393af853-Metadata.json},
 openalex = {W1694767778},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/95151403b0db4f75bfd8da0b393af853-Paper.pdf},
 publisher = {MIT Press},
 title = {Refractoriness and Neural Precision},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/95151403b0db4f75bfd8da0b393af853-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_9683cc5f,
 abstract = {Active data clustering is a novel technique for clustering of proximity data which utilizes principles from sequential experiment design in order to interleave data generation and data analysis. The proposed active data sampling strategy is based on the expected value of information, a concept rooting in statistical decision theory. This is considered to be an important step towards the analysis of large-scale data sets, because it offers a way to overcome the inherent data sparseness of proximity data. We present applications to unsupervised texture segmentation in computer vision and information retrieval in document databases.},
 author = {Hofmann, Thomas and Buhmann, Joachim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9683cc5f89562ea48e72bb321d9f03fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9683cc5f89562ea48e72bb321d9f03fb-Metadata.json},
 openalex = {W2160066096},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9683cc5f89562ea48e72bb321d9f03fb-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Data Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/9683cc5f89562ea48e72bb321d9f03fb-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_9701a1c1,
 abstract = {It is known that humans can make finer discriminations between familiar sounds (e.g. syllables) than between unfamiliar ones (e.g. different noise segments). Here we show that a corresponding enhancement is present in early auditory processing stages. Based on previous work which demonstrated that natural sounds had robust statistical properties that could be quantified, we hypothesize that the auditory system exploits those properties to construct efficient neural codes. To test this hypothesis, we measure the information rate carried by auditory spike trains on narrow-band stimuli whose amplitude modulation has naturalistic characteristics, and compare it to the information rate on stimuli with nonnaturalistic modulation. We find that naturalistic inputs significantly enhance the rate of transmitted information, indicating that auditiory neural responses are matched to characteristics of natural auditory scenes.},
 author = {Attias, Hagai and Schreiner, Christoph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9701a1c165dd9420816bfec5edd6c2b1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9701a1c165dd9420816bfec5edd6c2b1-Metadata.json},
 openalex = {W2135789708},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9701a1c165dd9420816bfec5edd6c2b1-Paper.pdf},
 publisher = {MIT Press},
 title = {Coding of Naturalistic Stimuli by Auditory Midbrain Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/9701a1c165dd9420816bfec5edd6c2b1-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_970af30e,
 abstract = {This paper enhances the Q-learning algorithm for optimal asset allocation proposed in (Neuneier, 1996 [6]). The new formulation simplifies the approach by using only one value-function for many assets and allows model-free policy-iteration. After testing the new algorithm on real data, the possibility of risk management within the framework of Markov decision problems is analyzed. The proposed methods allows the construction of a multi-period portfolio management system which takes into account transaction costs, the risk preferences of the investor, and several constraints on the allocation.},
 author = {Neuneier, Ralph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/970af30e481057c48f87e101b61e6994-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/970af30e481057c48f87e101b61e6994-Metadata.json},
 openalex = {W2155054353},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/970af30e481057c48f87e101b61e6994-Paper.pdf},
 publisher = {MIT Press},
 title = {Enhancing Q-Learning for Optimal Asset Allocation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/970af30e481057c48f87e101b61e6994-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_97275a23,
 abstract = {This paper describes some of the interactions of model learning algorithms and planning algorithms we have found in exploring model-based reinforcement learning. The paper focuses on how local trajectory optimizers can be used effectively with learned non-parametric models. We find that trajectory planners that are fully consistent with the learned model often have difficulty finding reasonable plans in the early stages of learning. Trajectory planners that balance obeying the learned model with minimizing cost (or maximizing reward) often do better, even if the plan is not fully consistent with the learned model.},
 author = {Atkeson, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/97275a23ca44226c9964043c8462be96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/97275a23ca44226c9964043c8462be96-Metadata.json},
 openalex = {W2115450028},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/97275a23ca44226c9964043c8462be96-Paper.pdf},
 publisher = {MIT Press},
 title = {Nonparametric Model-Based Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/97275a23ca44226c9964043c8462be96-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_980ecd05,
 abstract = {A novel neural network model of pre-attention processing in visual-search tasks is presented. Using displays of line orientations taken from Wolfe's experiments [1992], we study the hypothesis that the distinction between parallel versus serial processes arises from the availability of global information in the internal representations of the visual scene. The model operates in two phases. First, the visual displays are compressed via principal-component-analysis. Second, the compressed data is processed by a target detector module in order to identify the existence of a target in the display. Our main finding is that targets in displays which were found experimentally to be processed in parallel can be detected by the system, while targets in experimentally-serial displays cannot. This fundamental difference is explained via variance analysis of the compressed representations, providing a numerical criterion distinguishing parallel from serial displays. Our model yields a mapping of response-time slopes that is similar to Duncan and Humphreys's search surface [1989], providing an explicit formulation of their intuitive notion of feature similarity. It presents a neural realization of the processing that may underlie the classical metaphorical explanations of visual search.},
 author = {Cohen, Eyal and Ruppin, Eytan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/980ecd059122ce2e50136bda65c25e07-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/980ecd059122ce2e50136bda65c25e07-Metadata.json},
 openalex = {W2112542030},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/980ecd059122ce2e50136bda65c25e07-Paper.pdf},
 publisher = {MIT Press},
 title = {On Parallel versus Serial Processing: A Computational Study of Visual Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/980ecd059122ce2e50136bda65c25e07-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_991de292,
 abstract = {Accounts of neurological disorders often posit damage to a specific functional pathway of the brain. Farah (1990) has proposed an alternative class of explanations involving partial damage to multiple pathways. We explore this explanation for optic aphasia, a disorder in which severe performance deficits are observed when patients are asked to name visually presented objects, but surprisingly, performance is relatively normal on naming objects from auditory cues and on gesturing the appropriate use of visually presented objects. We model this highly specific deficit through partial damage to two pathways-one that maps visual input to semantics, and the other that maps semantics to naming responses. The effect of this damage is superadditive, meaning that tasks which require one pathway or the other show little or no performance deficit, but the damage is manifested when a task requires both pathways (i.e., naming visually presented objects). Our model explains other phenomena associated with optic aphasia, and makes testable experimental predictions.},
 author = {Mozer, Michael C and Sitton, Mark and Farah, Martha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/991de292e76f74f3c285b3f6d57958d5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/991de292e76f74f3c285b3f6d57958d5-Metadata.json},
 openalex = {W2096740195},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/991de292e76f74f3c285b3f6d57958d5-Paper.pdf},
 publisher = {MIT Press},
 title = {A Superadditive-Impairment Theory of Optic Aphasia},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/991de292e76f74f3c285b3f6d57958d5-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_9a1756fd,
 abstract = {The S-Map is a network with a simple learning algorithm that combines the self-organization capability of the Self-Organizing Map (SOM) and the probabilistic interpretability of the Generative Topographic Mapping (GTM). The simulations suggest that the S-Map algorithm has a stronger tendency to self-organize from random initial configuration than the GTM. The S-Map algorithm can be further simplified to employ pure Hebbian learning, without changing the qualitative behaviour of the network.},
 author = {Kiviluoto, Kimmo and Oja, Erkki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9a1756fd0c741126d7bbd4b692ccbd91-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9a1756fd0c741126d7bbd4b692ccbd91-Metadata.json},
 openalex = {W2126941075},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9a1756fd0c741126d7bbd4b692ccbd91-Paper.pdf},
 publisher = {MIT Press},
 title = {S-Map: A Network with a Simple Self-Organization Algorithm for Generative Topographic Mappings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/9a1756fd0c741126d7bbd4b692ccbd91-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_9aa42b31,
 abstract = {Image intensity variations can result from several different object surface effects, including shading from 3-dimensional relief of the object, or paint on the surface itself. An essential problem in vision, which people solve naturally, is to attribute the proper physical cause, e.g. surface relief or paint, to an observed image. We addressed this problem with an approach combining psychophysical and Bayesian computational methods.

We assessed human performance on a set of test images, and found that people made fairly consistent judgements of surface properties. Our computational model assigned simple prior probabilities to different relief or paint explanations for an image, and solved for the most probable interpretation in a Bayesian framework. The ratings of the test images by our algorithm compared surprisingly well with the mean ratings of our subjects.},
 author = {Freeman, William and Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9aa42b31882ec039965f3c4923ce901b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9aa42b31882ec039965f3c4923ce901b-Metadata.json},
 openalex = {W2162248510},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Model of Surface Perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_9cb67ffb,
 abstract = {Boosting is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is AdaBoost [5]. It has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms [4], and decision trees [1, 2, 6]. In this paper we use AdaBoost to improve the performances of neural networks. We compare training methods based on sampling the training set and weighting the cost function. Our system achieves about 1.4% error on a data base of online handwritten digits from more than 200 writers. Adaptive boosting of a multi-layer network achieved 1.5% error on the UCI Letters and 8.1 % error on the UCI satellite data set.},
 author = {Schwenk, Holger and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9cb67ffb59554ab1dabb65bcb370ddd9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9cb67ffb59554ab1dabb65bcb370ddd9-Metadata.json},
 openalex = {W2151802538},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/9cb67ffb59554ab1dabb65bcb370ddd9-Paper.pdf},
 publisher = {MIT Press},
 title = {Training Methods for Adaptive Boosting of Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/9cb67ffb59554ab1dabb65bcb370ddd9-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_a223c6b3,
 abstract = {Flies are capable of rapidly detecting and integrating visual motion information in behaviorly-relevant ways. The first stage of visual motion processing in flies is a retinotopic array of functional units known as elementary motion detectors (EMDs). Several decades ago, Reichardt and colleagues developed a correlation-based model of motion detection that described the behavior of these neural circuits. We have implemented a variant of this model in a 2.0-Âµm analog CMOS VLSI process. The result is a low-power, continuous-time analog circuit with integrated photoreceptors that responds to motion in real time. The responses of the circuit to drifting sinusoidal gratings qualitatively resemble the temporal frequency response, spatial frequency response, and direction selectivity of motion-sensitive neurons observed in insects. In addition to its possible engineering applications, the circuit could potentially be used as a building block for constructing hardware models of higher-level insect motion integration.},
 author = {Harrison, Reid and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a223c6b3710f85df22e9377d6c4f7553-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a223c6b3710f85df22e9377d6c4f7553-Metadata.json},
 openalex = {W2129226992},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a223c6b3710f85df22e9377d6c4f7553-Paper.pdf},
 publisher = {MIT Press},
 title = {An Analog VLSI Model of the Fly Elementary Motion Detector},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/a223c6b3710f85df22e9377d6c4f7553-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_a424ed4b,
 abstract = {This paper reports about an application of Bayes' inferred neural network classifiers in the field of automatic sleep staging. The reason for using Bayesian learning for this task is two-fold. First, Bayesian inference is known to embody regularization automatically. Second, a side effect of Bayesian learning leads to larger variance of network outputs in regions without training data. This results in well known moderation effects, which can be used to detect outliers. In a 5 fold cross-validation experiment the full Bayesian solution found with R. Neals hybrid Monte Carlo algorithm, was not better than a single maximum a-posteriori (MAP) solution found with D.J. MacKay's evidence approximation. In a second experiment we studied the properties of both solutions in rejecting classification of movement artefacts.},
 author = {Sykacek, Peter and Dorffner, Georg and Rappelsberger, Peter and Zeitlhofer, Josef},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a424ed4bd3a7d6aea720b86d4a360f75-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a424ed4bd3a7d6aea720b86d4a360f75-Metadata.json},
 openalex = {W2165977312},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a424ed4bd3a7d6aea720b86d4a360f75-Paper.pdf},
 publisher = {MIT Press},
 title = {Experiences with Bayesian Learning in a Real World Application},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/a424ed4bd3a7d6aea720b86d4a360f75-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_a50abba8,
 abstract = {We study model feed forward networks as time series predictors in the stationary limit. The focus is on complex, yet non-chaotic, behavior. The main question we address is whether the asymptotic behavior is governed by the architecture, regardless the details of the weights. We find hierarchies among classes of architectures with respect to the attractor dimension of the long term sequence they are capable of generating; larger number of hidden units can generate higher dimensional attractors. In the case of a perceptron, we develop the stationary solution for general weights, and show that the flow is typically one dimensional. The relaxation time from an arbitrary initial condition to the stationary solution is found to scale linearly with the size of the network. In multilayer networks, the number of hidden units gives bounds on the number and dimension of the possible attractors. We conclude that long term prediction (in the non-chaotic regime) with such models is governed by attractor dynamics related to the architecture.},
 author = {Priel, Avner and Kanter, Ido and Kessler, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a50abba8132a77191791390c3eb19fe7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a50abba8132a77191791390c3eb19fe7-Metadata.json},
 openalex = {W2099056101},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf},
 publisher = {MIT Press},
 title = {Analytical Study of the Interplay between Architecture and Predictability},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/a50abba8132a77191791390c3eb19fe7-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_a60937eb,
 abstract = {A rich body of data exists showing that recollection of specific information makes an important contribution to recognition memory, which is distinct from the contribution of familiarity, and is not adequately captured by existing unitary memory models. Furthermore, neuropsychological evidence indicates that recollection is subserved by the hippocampus. We present a model, based largely on known features of hippocampal anatomy and physiology, that accounts for the following key characteristics of recollection: 1) false recollection is rare (i.e., participants rarely claim to recollect having studied nonstudied items), and 2) increasing interference leads to less recollection but apparently does not compromise the quality of recollection (i.e., the extent to which recollected information veridically reflects events that occurred at study).},
 author = {O\textquotesingle Reilly, Randall and Norman, Kenneth and McClelland, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a60937eba57758ed45b6d3e91e8659f3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a60937eba57758ed45b6d3e91e8659f3-Metadata.json},
 openalex = {W2121843296},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a60937eba57758ed45b6d3e91e8659f3-Paper.pdf},
 publisher = {MIT Press},
 title = {A Hippocampal Model of Recognition Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/a60937eba57758ed45b6d3e91e8659f3-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_a8f8f602,
 abstract = {In this work, we tackle the problem of time-series modeling of video traffic. Different from the existing methods which model the time-series in the time domain, we model the wavelet coefficients in the wavelet domain. The strength of the wavelet model includes (1) a unified approach to model both the long-range and the short-range dependence in the video traffic simultaneously, (2) a computationally efficient method on developing the model and generating high quality video traffic, and (3) feasibility of performance analysis using the model.},
 author = {Ma, Sheng and Ji, Chuanyi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a8f8f60264024dca151f164729b76c0b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a8f8f60264024dca151f164729b76c0b-Metadata.json},
 openalex = {W2167539063},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a8f8f60264024dca151f164729b76c0b-Paper.pdf},
 publisher = {MIT Press},
 title = {Wavelet Models for Video Time-Series},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/a8f8f60264024dca151f164729b76c0b-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_a9be4c2a,
 abstract = {Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence. In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning. Current model-based reinforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver. This paper generalizes prior work on temporally abstract models [Sutton, 1995] and extends it from the prediction setting to include actions, control, and planning. We introduce a more general form of temporally abstract model, the multi-time model, and establish its suitability for planning and learning by virtue of its relationship to the Bellman equations. This paper summarizes the theoretical framework of multi-time models and illustrates their potential advantages in a grid world planning task.},
 author = {Precup, Doina and Sutton, Richard S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a9be4c2a4041cadbf9d61ae16dd1389e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a9be4c2a4041cadbf9d61ae16dd1389e-Metadata.json},
 openalex = {W2156067405},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/a9be4c2a4041cadbf9d61ae16dd1389e-Paper.pdf},
 publisher = {MIT Press},
 title = {Multi-time Models for Temporally Abstract Planning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/a9be4c2a4041cadbf9d61ae16dd1389e-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_ac796a52,
 abstract = {Some learning techniques for classification tasks work indirectly, by first trying to fit a full probabilistic model to the observed data. Whether this is a good idea or not depends on the robustness with respect to deviations from the postulated model. We study this question experimentally in a restricted, yet non-trivial and interesting case: we consider a conditionally independent attribute (CIA) model which postulates a single binary-valued hidden variable z on which all other attributes (i.e., the target and the observables) depend. In this model, finding the most likely value of anyone variable (given known values for the others) reduces to testing a linear function of the observed values.

We learn CIA with two techniques: the standard EM algorithm, and a new algorithm we develop based on covariances. We compare these, in a controlled fashion, against an algorithm (a version of Winnow) that attempts to find a good linear classifier directly. Our conclusions help delimit the fragility of using the CIA model for classification: once the data departs from this model, performance quickly degrades and drops below that of the directly-learned linear classifier.},
 author = {Grove, Adam and Roth, Dan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ac796a52db3f16bbdb6557d3d89d1c5a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ac796a52db3f16bbdb6557d3d89d1c5a-Metadata.json},
 openalex = {W2112792378},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ac796a52db3f16bbdb6557d3d89d1c5a-Paper.pdf},
 publisher = {MIT Press},
 title = {Linear Concepts and Hidden Variables: An Empirical Study},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/ac796a52db3f16bbdb6557d3d89d1c5a-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_ad3019b8,
 abstract = {We present a study which is concerned with word recognition rates for heavily degraded documents. We compare human with machine reading capabilities in a series of experiments, which explores the interaction of word/non-word recognition, word frequency and legality of nonwords with degradation level. We also study the influence of character segmentation, and compare human performance with that of our artificial neural network model for reading. We found that the proposed computer model uses word context as efficiently as humans, but performs slightly worse on the pure character recognition task.},
 author = {Schenkel, Markus and Latimer, Cyril and Jabri, Marwan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ad3019b856147c17e82a5bead782d2a8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ad3019b856147c17e82a5bead782d2a8-Metadata.json},
 openalex = {W2158487457},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ad3019b856147c17e82a5bead782d2a8-Paper.pdf},
 publisher = {MIT Press},
 title = {Comparison of Human and Machine Word Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/ad3019b856147c17e82a5bead782d2a8-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_ad71c82b,
 abstract = {While the understanding of the functional role of different classes of neurons in the awake primary visual cortex has been extensively studied since the time of Hubel and Wiesel (Hubel and Wiesel, 1962), our understanding of the feature selectivity and functional role of neurons in the primary auditory cortex is much farther from complete. Moving bars have long been recognized as an optimal stimulus for many visual cortical neurons, and this finding has recently been confirmed and extended in detail using reverse correlation methods (Jones and Palmer, 1987; Reid and Alonso, 1995; Reid et al., 1991; Ringach et al., 1997). In this study, we recorded from neurons in the primary auditory cortex of the awake primate, and used a novel reverse correlation technique to compute receptive fields (or preferred stimuli), encompassing both multiple frequency components and ongoing time. These spectrotemporal receptive fields make clear that neurons in the primary auditory cortex, as in the primary visual cortex, typically show considerable structure in their feature processing properties, often including multiple excitatory and inhibitory regions in their receptive fields. These neurons can be sensitive to stimulus edges in frequency composition or in time, and sensitive to stimulus transitions such as changes in frequency. These neurons also show strong responses and selectivity to continuous frequency modulated stimuli analogous to visual drifting gratings.},
 author = {DeCharms, R. and Merzenich, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ad71c82b22f4f65b9398f76d8be4c615-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ad71c82b22f4f65b9398f76d8be4c615-Metadata.json},
 openalex = {W2155695536},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf},
 publisher = {MIT Press},
 title = {Characterizing Neurons in the Primary Auditory Cortex of the Awake Primate Using Reverse Correlation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/ad71c82b22f4f65b9398f76d8be4c615-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_af473271,
 abstract = {Estimating motion in scenes containing multiple moving objects remains a difficult problem in computer vision. A promising approach to this problem involves using mixture models, where the motion of each object is a component in the mixture. However, existing methods typically require specifying in advance the number of components in the mixture, i.e. the number of objects in the scene.

Here we show that the number of objects can be estimated automatically in a maximum likelihood framework, given an assumption about the level of noise in the video sequence. We derive analytical results showing the number of models which maximize the likelihood for a given noise level in a given sequence. We illustrate these results on a real video sequence, showing how the phase transitions correspond to different perceptual organizations of the scene.},
 author = {Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/af4732711661056eadbf798ba191272a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/af4732711661056eadbf798ba191272a-Metadata.json},
 openalex = {W2114746503},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/af4732711661056eadbf798ba191272a-Paper.pdf},
 publisher = {MIT Press},
 title = {Phase Transitions and the Perceptual Organization of Video Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/af4732711661056eadbf798ba191272a-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_afe43465,
 abstract = {Gaussian processes provide natural non-parametric prior distributions over regression functions. In this paper we consider regression problems where there is noise on the output, and the variance of the noise depends on the inputs. If we assume that the noise is a smooth function of the inputs, then it is natural to model the noise variance using a second Gaussian process, in addition to the Gaussian process governing the noise-free output value. We show that prior uncertainty about the parameters controlling both processes can be handled and that the posterior distribution of the noise rate can be sampled from using Markov chain Monte Carlo methods. Our results on a synthetic data set give a posterior noise variance that well-approximates the true variance.},
 author = {Goldberg, Paul and Williams, Christopher and Bishop, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/afe434653a898da20044041262b3ac74-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/afe434653a898da20044041262b3ac74-Metadata.json},
 openalex = {W2170078560},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/afe434653a898da20044041262b3ac74-Paper.pdf},
 publisher = {MIT Press},
 title = {Regression with Input-dependent Noise: A Gaussian Process Treatment},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/afe434653a898da20044041262b3ac74-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_b069b341,
 abstract = {We present a novel generic approach to the problem of Event Related Potential identification and classification, based on a competitive Neural Net architecture. The network weights converge to the embedded signal patterns, resulting in the formation of a matched filter bank. The network performance is analyzed via a simulation study, exploring identification robustness under low SNR conditions and compared to the expected performance from an information theoretic perspective. The classifier is applied to real event-related potential data recorded during a classic odd-ball type paradigm; for the first time, within-session variable signal patterns are automatically identified, dismissing the strong and limiting requirement of a-priori stimulus-related selective grouping of the recorded data.},
 author = {Lange, Daniel and Siegelmann, Hava and Pratt, Hillel and Inbar, Gideon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b069b3415151fa7217e870017374de7c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b069b3415151fa7217e870017374de7c-Metadata.json},
 openalex = {W2130103291},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b069b3415151fa7217e870017374de7c-Paper.pdf},
 publisher = {MIT Press},
 title = {A Generic Approach for Identification of Event Related Brain Potentials via a Competitive Neural Network Structure},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/b069b3415151fa7217e870017374de7c-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_b197ffde,
 abstract = {We demonstrate that the problem of training neural networks with small (average) squared error is computationally intractable. Consider a data set of M points (Xi, Yi), i = 1,2, ..., M, where Xi are input vectors from Rd, Yi are real outputs (Yi âˆˆ R). For a network f0 in some class F of neural networks, (1/M) Î£i=1M (f0(Xi)- Yi)2)1/2 - inffâˆˆF(1/M) Î£i=1M (f(Xi) - Yi)2)1/2 is the (avarage) relative error occurs when one tries to fit the data set by f0. We will prove for several classes F of neural networks that achieving a relative error smaller than some fixed positive threshold (independent from the size of the data set) is NP-hard.},
 author = {Vu, Van H. Vu H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b197ffdef2ddc3308584dce7afa3661b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b197ffdef2ddc3308584dce7afa3661b-Metadata.json},
 openalex = {W2147772771},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b197ffdef2ddc3308584dce7afa3661b-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Infeasibility of Training Neural Networks with Small Squared Errors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/b197ffdef2ddc3308584dce7afa3661b-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_b265ce60,
 abstract = {A new algorithm is presented which approximates the perceived visual similarity between images. The images are initially transformed into a feature space which captures visual structure, texture and color using a tree of filters. Similarity is the inverse of the distance in this perceptual feature space. Using this algorithm we have constructed an image database system which can perform example based retrieval on large image databases. Using carefully constructed target sets, which limit variation to only a single visual characteristic, retrieval rates are quantitatively compared to those of standard methods.},
 author = {De Bonet, Jeremy and Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b265ce60fe4c5384e622b09eb829b8df-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b265ce60fe4c5384e622b09eb829b8df-Metadata.json},
 openalex = {W2153168223},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b265ce60fe4c5384e622b09eb829b8df-Paper.pdf},
 publisher = {MIT Press},
 title = {Structure Driven Image Database Retrieval},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/b265ce60fe4c5384e622b09eb829b8df-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_b7087c1f,
 abstract = {We discuss the problem of catastrophic fusion in multimodal recognition systems. This problem arises in systems that need to fuse different channels in non-stationary environments. Practice shows that when recognition modules within each modality are tested in contexts inconsistent with their assumptions, their influence on the fused product tends to increase, with catastrophic results. We explore a principled solution to this problem based upon Bayesian ideas of competitive models and inference robustification: each sensory channel is provided with simple white-noise context models, and the perceptual hypothesis and context are jointly estimated. Consequently, context deviations are interpreted as changes in white noise contamination strength, automatically adjusting the influence of the module. The approach is tested on a fixed lexicon automatic audiovisual speech recognition problem with very good results.},
 author = {Movellan, Javier and Mineiro, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b7087c1f4f89e63af8d46f3b20271153-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b7087c1f4f89e63af8d46f3b20271153-Metadata.json},
 openalex = {W2148317295},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b7087c1f4f89e63af8d46f3b20271153-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Robustification for Audio Visual Fusion},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/b7087c1f4f89e63af8d46f3b20271153-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_b8c27b7a,
 abstract = {Filial imprinting in domestic chicks is of interest in psychology, biology, and computational modeling because it exemplifies simple, rapid, innately programmed learning which is biased toward learning about some objects. Horn et al. have recently discovered a naive visual preference for heads and necks which develops over the course of the first three days of life. The neurological basis of this predisposition is almost entirely unknown; that of imprinting-related learning is fairly clear. This project is the first model of the predisposition consistent with what is known about learning in imprinting. The model develops the predisposition appropriately, learns to approach a training object, and replicates one interaction between the two processes. Future work will replicate more interactions between imprinting and the predisposition in chicks, and analyze why the system works.},
 author = {Hadden, Lucy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b8c27b7a1c450ffdacb31483454e0b54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b8c27b7a1c450ffdacb31483454e0b54-Metadata.json},
 openalex = {W2138744020},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/b8c27b7a1c450ffdacb31483454e0b54-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neural Network Model of Naive Preference and Filial Imprinting in the Domestic Chick},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/b8c27b7a1c450ffdacb31483454e0b54-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_bad5f337,
 abstract = {Recently, a model for supervised learning of probabilistic transducers represented by suffix trees was introduced. However, this algorithm tends to build very large trees, requiring very large amounts of computer memory. In this paper, we propose a new, more compact, transducer model in which one shares the parameters of distributions associated to contexts yielding similar conditional output distributions. We illustrate the advantages of the proposed algorithm with comparative experiments on inducing a noun phrase recognizer.},
 author = {Bengio, Yoshua and Bengio, Samy and Isabelle, Jean-Franc and Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/bad5f33780c42f2588878a9d07405083-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/bad5f33780c42f2588878a9d07405083-Metadata.json},
 openalex = {W2099876072},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/bad5f33780c42f2588878a9d07405083-Paper.pdf},
 publisher = {MIT Press},
 title = {Shared Context Probabilistic Transducers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/bad5f33780c42f2588878a9d07405083-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_bcc0d400,
 abstract = {Program execution speed on modern computers is sensitive, by a factor of two or more, to the order in which instructions are presented to the processor. To realize potential execution efficiency, an optimizing compiler must employ a heuristic algorithm for instruction scheduling. Such algorithms are painstakingly hand-crafted, which is expensive and time-consuming. We show how to cast the instruction scheduling problem as a learning task, obtaining the heuristic scheduling algorithm automatically. Our focus is the narrower problem of scheduling straight-line code (also called basic blocks of instructions). Our empirical results show that just a few features are adequate for quite good performance at this task for a real modern processor, and that any of several supervised learning methods perform nearly optimally with respect to the features used.},
 author = {Moss, J. and Utgoff, Paul and Cavazos, John and Precup, Doina and Stefanovic, Darko and Brodley, Carla and Scheeff, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Metadata.json},
 openalex = {W2119537359},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning to Schedule Straight-Line Code},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_c0826819,
 abstract = {Exact inference in densely connected Bayesian networks is computationally intractable, and so there is considerable interest in developing effective approximation schemes. One approach which has been adopted is to bound the log likelihood using a mean-field approximating distribution. While this leads to a tractable algorithm, the mean field distribution is assumed to be factorial and hence unimodal. In this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions. We derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learning in sigmoid belief networks. Our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased.},
 author = {Bishop, Christopher and Lawrence, Neil and Jaakkola, Tommi and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c0826819636026dd1f3674774f06c51d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c0826819636026dd1f3674774f06c51d-Metadata.json},
 openalex = {W2109787716},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c0826819636026dd1f3674774f06c51d-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximating Posterior Distributions in Belief Networks Using Mixtures},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/c0826819636026dd1f3674774f06c51d-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_c26820b8,
 abstract = {We prove that the Canonical Distortion Measure (CDM) [2,3] is the optimal distance measure to use for 1 nearest-neighbour (1-NN) classification, and show that it reduces to squared Euclidean distance in feature space for function classes that can be expressed as linear combinations of a fixed set of features. PAC-like bounds are given on the sample-complexity required to learn the CDM. An experiment is presented in which a neural network CDM was learnt for a Japanese OCR environment and then used to do 1-NN classification.},
 author = {Baxter, Jonathan and Bartlett, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c26820b8a4c1b3c2aa868d6d57e14a79-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c26820b8a4c1b3c2aa868d6d57e14a79-Metadata.json},
 openalex = {W2159364980},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf},
 publisher = {MIT Press},
 title = {The Canonical Distortion Measure in Feature Space and 1-NN Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/c26820b8a4c1b3c2aa868d6d57e14a79-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_c5cc17e3,
 abstract = {The observed distribution of natural images is far from uniform. On the contrary, real images have complex and important structure that can be exploited for image processing, recognition and analysis. There have been many proposed approaches to the principled statistical modeling of images, but each has been limited in either the complexity of the models or the complexity of the images. We present a non-parametric multi-scale statistical model for images that can be used for recognition, image de-noising, and in a generative mode to synthesize high quality textures.},
 author = {De Bonet, Jeremy and Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c5cc17e395d3049b03e0f1ccebb02b4d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c5cc17e395d3049b03e0f1ccebb02b4d-Metadata.json},
 openalex = {W2153518451},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c5cc17e395d3049b03e0f1ccebb02b4d-Paper.pdf},
 publisher = {MIT Press},
 title = {A Non-Parametric Multi-Scale Statistical Model for Natural Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/c5cc17e395d3049b03e0f1ccebb02b4d-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_c73dfe6c,
 abstract = {We consider neural network models for stochastic nonlinear dynamical systems where measurements of the variable of interest are only available at irregular intervals i.e. most realizations are missing. Difficulties arise since the solutions for prediction and maximum likelihood learning with missing data lead to complex integrals, which even for simple cases cannot be solved analytically. In this paper we propose a specific combination of a nonlinear recurrent neural predictive model and a linear error model which leads to tractable prediction and maximum likelihood adaptation rules. In particular, the recurrent neural network can be trained using the real-time recurrent learning rule and the linear error model can be trained by an EM adaptation rule, implemented using forward-backward Kalman filter equations. The model is applied to predict the glucose/insulin metabolism of a diabetic patient where blood glucose measurements are only available a few times a day at irregular intervals. The new model shows considerable improvement with respect to both recurrent neural networks trained with teacher forcing or in a free running mode and various linear models.},
 author = {Tresp, Volker and Briegel, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c73dfe6c630edb4c1692db67c510f65c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c73dfe6c630edb4c1692db67c510f65c-Metadata.json},
 openalex = {W2100137028},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c73dfe6c630edb4c1692db67c510f65c-Paper.pdf},
 publisher = {MIT Press},
 title = {A Solution for Missing Data in Recurrent Neural Networks with an Application to Blood Glucose Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/c73dfe6c630edb4c1692db67c510f65c-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_c913303f,
 abstract = {In this paper, we discuss regularisation in online/sequential learning algorithms. In environments where data arrives sequentially, techniques such as cross-validation to achieve regularisation or model selection are not possible. Further, bootstrapping to determine a confidence level is not practical. To surmount these problems, a minimum variance estimation approach that makes use of the extended Kalman algorithm for training multi-layer perceptrons is employed. The novel contribution of this paper is to show the theoretical links between extended Kalman filtering, Sutton's variable learning rate algorithms and Mackay's Bayesian estimation framework. In doing so, we propose algorithms to overcome the need for heuristic choices of the initial conditions and noise covariance matrices in the Kalman approach.},
 author = {de Freitas, Jo\~{a}o and Niranjan, Mahesan and Gee, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c913303f392ffc643f7240b180602652-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c913303f392ffc643f7240b180602652-Metadata.json},
 openalex = {W2161959730},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c913303f392ffc643f7240b180602652-Paper.pdf},
 publisher = {MIT Press},
 title = {Regularisation in Sequential Learning Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/c913303f392ffc643f7240b180602652-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_c930eecd,
 abstract = {A new policy iteration algorithm for partially observable Markov decision processes is presented that is simpler and more efficient than an earlier policy iteration algorithm of Sondik (1971, 1978). The key simplification is representation of a policy as a finite-state controller. This representation makes policy evaluation straightforward. The paper's contribution is to show that the dynamic-programming update used in the policy improvement step can be interpreted as the transformation of a finite-state controller into an improved finite-state controller. The new algorithm consistently outperforms value iteration as an approach to solving infinite-horizon problems.},
 author = {Hansen, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c930eecd01935feef55942cc445f708f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c930eecd01935feef55942cc445f708f-Metadata.json},
 openalex = {W2109393574},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/c930eecd01935feef55942cc445f708f-Paper.pdf},
 publisher = {MIT Press},
 title = {An Improved Policy Iteration Algorithm for Partially Observable MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/c930eecd01935feef55942cc445f708f-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_cd0dce8f,
 abstract = {In this paper we show that for discounted MDPs with discount factor Î³ > 1/2 the asymptotic rate of convergence of Q-learning is O(1/tR(1-Î³)) if R(1 - Î³) 0, where pmin and pmax now become the minimum and maximum state-action occupation frequencies corresponding to the stationary distribution.},
 author = {Szepesv\'{a}ri, Csaba},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cd0dce8fca267bf1fb86cf43e18d5598-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cd0dce8fca267bf1fb86cf43e18d5598-Metadata.json},
 openalex = {W2150147323},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cd0dce8fca267bf1fb86cf43e18d5598-Paper.pdf},
 publisher = {MIT Press},
 title = {The Asymptotic Convergence-Rate of Q-learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/cd0dce8fca267bf1fb86cf43e18d5598-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_cec6f62c,
 abstract = {Singular value decomposition (SVD) can be viewed as a method for unsupervised training of a network that associates two classes of events reciprocally by linear connections through a single hidden layer. SVD was used to learn and represent relations among very large numbers of words (20k-60k) and very large numbers of natural text passages (1k- 70k) in which they occurred. The result was 100-350 dimensional semantic spaces in which any trained or newly added word or passage could be represented as a vector, and similarities were measured by the cosine of the contained angle between vectors. Good accuracy in simulating human judgments and behaviors has been demonstrated by performance on multiple-choice vocabulary and domain knowledge tests, emulation of expert essay evaluations, and in several other ways. Examples are also given of how the kind of knowledge extracted by this method can be applied.},
 author = {Landauer, Thomas and Laham, Darrell and Foltz, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cec6f62cfb44b1be110b7bf70c8362d8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cec6f62cfb44b1be110b7bf70c8362d8-Metadata.json},
 openalex = {W2095764546},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cec6f62cfb44b1be110b7bf70c8362d8-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/cec6f62cfb44b1be110b7bf70c8362d8-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_cf1f78fe,
 abstract = {Hubel and Wiesel (1962) proposed that complex cells in visual cortex are driven by a pool of simple cells with the same preferred orientation but different spatial phases. However, a wide variety of experimental results over the past two decades have challenged the pure hierarchical model, primarily by demonstrating that many complex cells receive monosynaptic input from unoriented LGN cells, or do not depend on simple cell input. We recently showed using a detailed biophysical model that nonlinear interactions among synaptic inputs to an excitable dendritic tree could provide the nonlinear subunit computations that underlie complex cell responses (Mel, Ruderman, & Archie, 1997). This work extends the result to the case of complex cell binocular disparity tuning, by demonstrating in an isolated model pyramidal cell (1) disparity tuning at a resolution much finer than the the overall dimensions of the cell's receptive field, and (2) systematically shifted optimal disparity values for rivalrous pairs of light and dark bars--both in good agreement with published reports (Ohzawa, DeAngelis, & Freeman, 1997). Our results reemphasize the potential importance of intradendritic computation for binocular visual processing in particular, and for cortical neurophysiology in general.},
 author = {Mel, Bartlett and Ruderman, Daniel and Archie, Kevin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cf1f78fe923afe05f7597da2be7a3da8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cf1f78fe923afe05f7597da2be7a3da8-Metadata.json},
 openalex = {W2156378528},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cf1f78fe923afe05f7597da2be7a3da8-Paper.pdf},
 publisher = {MIT Press},
 title = {Toward a Single-Cell Account for Binocular Disparity Tuning: An Energy Model May Be Hiding in Your Dendrites},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/cf1f78fe923afe05f7597da2be7a3da8-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_cf9a242b,
 abstract = {Recently researchers have derived formal complexity analysis of analog computation in the setting of discrete-time dynamical systems. As an empirical constrast, training recurrent neural networks (RNNs) produces self-organized systems that are realizations of analog mechanisms. Previous work showed that a RNN can learn to process a simple context-free language (CFL) by counting. Herein, we extend that work to show that a RNN can learn a harder CFL, a simple palindrome, by organizing its resources into a symbol-sensitive counting solution, and we provide a dynamical systems analysis which demonstrates how the network: can not only count, but also copy and store counting information.},
 author = {Rodriguez, Paul and Wiles, Janet},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cf9a242b70f45317ffd281241fa66502-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cf9a242b70f45317ffd281241fa66502-Metadata.json},
 openalex = {W2161921398},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/cf9a242b70f45317ffd281241fa66502-Paper.pdf},
 publisher = {MIT Press},
 title = {Recurrent Neural Networks Can Learn to Implement Symbol-Sensitive Counting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/cf9a242b70f45317ffd281241fa66502-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_d10ec7c1,
 abstract = {Hidden Markov models (HMMs) for automatic speech recognition rely on high dimensional feature vectors to summarize the short-time properties of speech. Correlations between features can arise when the speech signal is non-stationary or corrupted by noise. We investigate how to model these correlations using factor analysis, a statistical method for dimensionality reduction. Factor analysis uses a small number of parameters to model the covariance structure of high dimensional data. These parameters are estimated by an Expectation-Maximization (EM) algorithm that can be embedded in the training procedures for HMMs. We evaluate the combined use of mixture densities and factor analysis in HMMs that recognize alphanumeric strings. Holding the total number of parameters fixed, we find that these methods, properly combined, yield better models than either method on its own.},
 author = {Saul, Lawrence and Rahim, Mazin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Metadata.json},
 openalex = {W2112571327},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling Acoustic Correlations by Factor Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/d10ec7c16cbe9de8fbb1c42787c3ec26-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_d1ee59e2,
 abstract = {MELONET I is a multi-scale neural network system producing baroque-style melodic variations. Given a melody, the system invents a four-part chorale harmonization and a variation of any chorale voice, after being trained on music pieces of composers like J. S. Bach and J. Pachelbel. Unlike earlier approaches to the learning of melodic structure, the system is able to learn and reproduce high-order structure like harmonic, motif and phrase structure in melodic sequences. This is achieved by using mutually interacting feedforward networks operating at different time scales, in combination with Kohonen networks to classify and recognize musical structure. The results are chorale partitas in the style of J. Pachelbel. Their quality has been judged by experts to be comparable to improvisations invented by an experienced human organist.},
 author = {H\"{o}rnel, Dominik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d1ee59e20ad01cedc15f5118a7626099-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d1ee59e20ad01cedc15f5118a7626099-Metadata.json},
 openalex = {W2131568409},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d1ee59e20ad01cedc15f5118a7626099-Paper.pdf},
 publisher = {MIT Press},
 title = {MELONET I: Neural Nets for Inventing Baroque-Style Chorale Variations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/d1ee59e20ad01cedc15f5118a7626099-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_d8211837,
 abstract = {We provide a model of the standard watermaze task, and of a more challenging task involving novel platform locations, in which rats exhibit one-trial learning after a few days of training. The model uses hippocampal place cells to support reinforcement learning, and also, in an integrated manner, to build and use allocentric coordinates.},
 author = {Foster, David and Morris, Richard and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d82118376df344b0010f53909b961db3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d82118376df344b0010f53909b961db3-Metadata.json},
 openalex = {W2108434894},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d82118376df344b0010f53909b961db3-Paper.pdf},
 publisher = {MIT Press},
 title = {Hippocampal Model of Rat Spatial Abilities Using Temporal Difference Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/d82118376df344b0010f53909b961db3-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_d9731321,
 abstract = {I present an expectation-maximization (EM) algorithm for principal component analysis (PCA). The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data. It is computationally very efficient in space and time. It also naturally accommodates missing information. I also introduce a new variant of PCA called sensible principal component analysis (SPCA) which defines a proper density model in the data space. Learning for SPCA is also done with an EM algorithm. I report results on synthetic and real data showing that these EM algorithms correctly and efficiently find the leading eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions.},
 author = {Roweis, Sam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Metadata.json},
 openalex = {W2103633133},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf},
 publisher = {MIT Press},
 title = {EM Algorithms for PCA and SPCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/d9731321ef4e063ebbee79298fa36f56-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_daa96d96,
 abstract = {We first describe a hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We then show how to incorporate lateral connections into the generative model. The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes, the model develops topographically organised local feature detectors.},
 author = {Ghahramani, Zoubin and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/daa96d9681a21445772454cbddf0cac1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/daa96d9681a21445772454cbddf0cac1-Metadata.json},
 openalex = {W2137141209},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/daa96d9681a21445772454cbddf0cac1-Paper.pdf},
 publisher = {MIT Press},
 title = {Hierarchical Non-linear Factor Analysis and Topographic Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/daa96d9681a21445772454cbddf0cac1-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_db6ebd05,
 abstract = {We propose diffusion networks, a type of recurrent neural network with probabilistic dynamics, as models for learning natural signals that are continuous in time and space. We give a formula for the gradient of the log-likelihood of a path with respect to the drift parameters for a diffusion network. This gradient can be used to optimize diffusion networks in the nonequilibrium regime for a wide variety of problems paralleling techniques which have succeeded in engineering fields such as system identification, state estimation and signal filtering. An aspect of this work which is of particular interest to computational neuroscience and hardware design is that with a suitable choice of activation function, e.g., quasi-linear sigmoidal, the gradient formula is local in space and time.},
 author = {Mineiro, Paul and Movellan, Javier and Williams, Ruth},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/db6ebd0566994d14a1767f14eb6fba81-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/db6ebd0566994d14a1767f14eb6fba81-Metadata.json},
 openalex = {W2128079370},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/db6ebd0566994d14a1767f14eb6fba81-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Path Distributions Using Nonequilibrium Diffusion Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/db6ebd0566994d14a1767f14eb6fba81-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_dc09c97f,
 abstract = {An adaptive on-line algorithm is proposed to estimate hierarchical data structures for non-stationary data sources. The approach is based on the principle of minimum cross entropy to derive a decision tree for data clustering and it employs a metalearning idea (learning to learn) to adapt to changes in data characteristics. Its efficiency is demonstrated by grouping non-stationary artifical data and by hierarchical segmentation of LANDSAT images.},
 author = {Held, Marcus and Buhmann, Joachim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/dc09c97fd73d7a324bdbfe7c79525f64-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/dc09c97fd73d7a324bdbfe7c79525f64-Metadata.json},
 openalex = {W2141685426},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/dc09c97fd73d7a324bdbfe7c79525f64-Paper.pdf},
 publisher = {MIT Press},
 title = {Unsupervised On-line Learning of Decision Trees for Hierarchical Data Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/dc09c97fd73d7a324bdbfe7c79525f64-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_e077e1a5,
 abstract = {This paper describes a new approach to extracting 3D perspective structure from 2D point-sets. The novel feature is to unify the tasks of estimating transformation geometry and identifying point-correspondence matches. Unification is realised by constructing a mixture model over the bi-partite graph representing the correspondence match and by effecting optimisation using the EM algorithm. According to our EM framework the probabilities of structural correspondence gate contributions to the expected likelihood function used to estimate maximum likelihood perspective pose parameters. This provides a means of rejecting structural outliers.},
 author = {Cross, Andrew and Hancock, Edwin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e077e1a544eec4f0307cf5c3c721d944-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e077e1a544eec4f0307cf5c3c721d944-Metadata.json},
 openalex = {W2096007840},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf},
 publisher = {MIT Press},
 title = {Recovering Perspective Pose with a Dual Step EM Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/e077e1a544eec4f0307cf5c3c721d944-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_e11943a6,
 abstract = {There are many applications in which it is desirable to order rather than classify instances. Here we consider the problem of learning how to order instances given feedback in the form of preference judgments, i.e., statements to the effect that one instance should be ranked ahead of another. We outline a two-stage approach in which one first learns by conventional means a binary preference function indicating whether it is advisable to rank one instance before another. Here we consider an on-line algorithm for learning preference functions that is based on Freund and Schapire's 'Hedge' algorithm. In the second stage, new instances are ordered so as to maximize agreement with the learned preference function. We show that the problem of finding the ordering that agrees best with a learned preference function is NP-complete. Nevertheless, we describe simple greedy algorithms that are guaranteed to find a good approximation. Finally, we show how metasearch can be formulated as an ordering problem, and present experimental results on learning a combination of 'search experts', each of which is a domain-specific query expansion strategy for a web search engine.},
 author = {Cohen, William W and Schapire, Robert E and Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e11943a6031a0e6114ae69c257617980-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e11943a6031a0e6114ae69c257617980-Metadata.json},
 openalex = {W3101685505},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e11943a6031a0e6114ae69c257617980-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning to Order Things},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/e11943a6031a0e6114ae69c257617980-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_e48e1320,
 abstract = {We have a developed an analog VLSI system that models the coordination of neurobiological segmental oscillators. We have implemented and tested a system that consists of a chain of eleven pattern generating circuits that are synaptically coupled to their nearest neighbors. Each pattern generating circuit is implemented with two silicon Morris-Lecar neurons that are connected in a reciprocally inhibitory network. We discuss the mechanisms of oscillations in the two-cell network and explore system behavior based on isotropic and anisotropic coupling, and frequency gradients along the chain of oscillators.},
 author = {Patel, Girish and Holleman, Jeremy and DeWeerth, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e48e13207341b6bffb7fb1622282247b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e48e13207341b6bffb7fb1622282247b-Metadata.json},
 openalex = {W2098316235},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e48e13207341b6bffb7fb1622282247b-Paper.pdf},
 publisher = {MIT Press},
 title = {Analog VLSI Model of Intersegmental Coordination with Nearest-Neighbor Coupling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/e48e13207341b6bffb7fb1622282247b-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_e56b06c5,
 abstract = {We present a method for determining the globally optimal on-line learning rule for a soft committee machine under a statistical mechanics framework. This work complements previous results on locally optimal rules, where only the rate of change in generalization error was considered. We maximize the total reduction in generalization error over the whole learning process and show how the resulting rule can significantly outperform the locally optimal rule.},
 author = {Rattray, Magnus and Saad, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e56b06c51e1049195d7b26d043c478a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e56b06c51e1049195d7b26d043c478a0-Metadata.json},
 openalex = {W2160120953},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e56b06c51e1049195d7b26d043c478a0-Paper.pdf},
 publisher = {MIT Press},
 title = {Globally Optimal On-line Learning Rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/e56b06c51e1049195d7b26d043c478a0-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_e816c635,
 abstract = {Bayesian treatments of learning in neural networks are typically based either on local Gaussian approximations to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called ensemble learning, was introduced by Hinton and van Camp (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution. However, the derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and so was unable to capture the posterior correlations between parameters. In this paper, we show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure. Initial results from a standard benchmark problem are encouraging.},
 author = {Barber, David and Bishop, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e816c635cad85a60fabd6b97b03cbcc9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e816c635cad85a60fabd6b97b03cbcc9-Metadata.json},
 openalex = {W2141800987},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/e816c635cad85a60fabd6b97b03cbcc9-Paper.pdf},
 publisher = {MIT Press},
 title = {Ensemble Learning for Multi-Layer Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/e816c635cad85a60fabd6b97b03cbcc9-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_ea8fcd92,
 abstract = {This paper introduces a probability model, the mixture of trees that can account for sparse, dynamically changing dependence relationships. We present a family of efficient algorithms that use EM and the Minimum Spanning Tree algorithm to find the ML and MAP mixture of trees for a variety of priors, including the Dirichlet and the MDL priors.},
 author = {Meila, Marina and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ea8fcd92d59581717e06eb187f10666d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ea8fcd92d59581717e06eb187f10666d-Metadata.json},
 openalex = {W2123306636},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf},
 publisher = {MIT Press},
 title = {Estimating Dependency Structure as a Hidden Variable},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_eaa32c96,
 abstract = {A new learning model based on autoassociative neural networks is developped and applied to face detection. To extend the detection ability in orientation and to decrease the number of false alarms, different combinations of networks are tested: ensemble, conditional ensemble and conditional mixture of networks. The use of a conditional mixture of networks allows to obtain state of the art results on different benchmark face databases.},
 author = {Feraud, Rapha\"{e}l and Bernier, Olivier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/eaa32c96f620053cf442ad32258076b9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/eaa32c96f620053cf442ad32258076b9-Metadata.json},
 openalex = {W2141342076},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/eaa32c96f620053cf442ad32258076b9-Paper.pdf},
 publisher = {MIT Press},
 title = {Ensemble and Modular Approaches for Face Detection: A Comparison},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/eaa32c96f620053cf442ad32258076b9-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_ee8374ec,
 abstract = {In this paper, the technique of stacking, previously only used for supervised learning, is applied to unsupervised learning. Specifically, it is used for non-parametric multivariate density estimation, to combine finite mixture model and kernel density estimators. Experimental results on both simulated data and real world data sets clearly demonstrate that stacked density estimation outperforms other strategies such as choosing the single best model based on cross-validation, combining with uniform weights, and even the single best model chosen by cheating by looking at the data used for independent testing.},
 author = {Smyth, Padhraic and Wolpert, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ee8374ec4e4ad797d42350c904d73077-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ee8374ec4e4ad797d42350c904d73077-Metadata.json},
 openalex = {W2115980193},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ee8374ec4e4ad797d42350c904d73077-Paper.pdf},
 publisher = {MIT Press},
 title = {Stacked Density Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/ee8374ec4e4ad797d42350c904d73077-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_effc299a,
 abstract = {We describe the design, fabrication and test results of an analog CMOS VLSI neural network prototype chip intended for phase-based machine vision algorithms. The chip implements an image filtering operation similar to Gabor-filtering. Because a Gabor filter's output is complex valued, it can be used to define a phase at every pixel in an image. This phase can be used in robust algorithms for disparity estimation and binocular stereo vergence control in stereo vision and for image motion analysis. The chip reported here takes an input image and generates two outputs at every pixel corresponding to the real and imaginary parts of the output.},
 author = {Shi, Bertram and Hui, Kwok},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/effc299a1addb07e7089f9b269c31f2f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/effc299a1addb07e7089f9b269c31f2f-Metadata.json},
 openalex = {W2157369373},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/effc299a1addb07e7089f9b269c31f2f-Paper.pdf},
 publisher = {MIT Press},
 title = {An Analog VLSI Neural Network for Phase-based Machine Vision},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/effc299a1addb07e7089f9b269c31f2f-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_f016e59c,
 abstract = {Several effective methods for improving the performance of a single learning algorithm have been developed recently. The general approach is to create a set of learned models by repeatedly applying the algorithm to different versions of the training data, and then combine the learned models' predictions according to a prescribed voting scheme. Little work has been done in combining the predictions of a collection of models generated by many learning algorithms having different representation and/or search strategies. This paper describes a method which uses the strategies of stacking and correspondence analysis to model the relationship between the learning examples and the way in which they are classified by a collection of learned models. A nearest neighbor method is then applied within the resulting representation to classify previously unseen examples. The new algorithm consistently performs as well or better than other combining techniques on a suite of data sets.},
 author = {Merz, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f016e59c7ad8b1d72903bb1aa5720d53-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f016e59c7ad8b1d72903bb1aa5720d53-Metadata.json},
 openalex = {W2121095699},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f016e59c7ad8b1d72903bb1aa5720d53-Paper.pdf},
 publisher = {MIT Press},
 title = {Combining Classifiers Using Correspondence Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/f016e59c7ad8b1d72903bb1aa5720d53-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_f0dd4a99,
 abstract = {We have developed a neural network architecture that implements a theory of attention, learning, and trans-cortical communication based on adaptive synchronization of 5-15 Hz and 30-80 Hz oscillations between cortical areas. Here we present a specific higher order cortical model of attentional networks, rhythmic expectancy, and the interaction of higher-order and primary, cortical levels of processing. It accounts for the mismatch negativity of the auditory ERP and the results of psychological experiments of Jones showing that auditory stream segregation depends on the rhythmic structure of inputs. The timing mechanisms of the model allow us to explain how relative timing information such as the relative order of events between streams is lost when streams are formed. The model suggests how the theories of auditory perception and attention of Jones and Bregman may be reconciled.},
 author = {Baird, Bill},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f0dd4a99fba6075a9494772b58f95280-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f0dd4a99fba6075a9494772b58f95280-Metadata.json},
 openalex = {W2155273881},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f0dd4a99fba6075a9494772b58f95280-Paper.pdf},
 publisher = {MIT Press},
 title = {Synchronized Auditory and Cognitive 40 Hz Attentional Streams, and the Impact of Rhythmic Expectation on Auditory Scene Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/f0dd4a99fba6075a9494772b58f95280-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_f3bd5ad5,
 abstract = {We derive the correspondence between regularization operators used in Regularization Networks and Hilbert Schmidt Kernels appearing in Support Vector Machines. More specifically, we prove that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties. As a by-product we show that a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels.},
 author = {Smola, Alex and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f3bd5ad57c8389a8a1a541a76be463bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f3bd5ad57c8389a8a1a541a76be463bf-Metadata.json},
 openalex = {W2135998368},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f3bd5ad57c8389a8a1a541a76be463bf-Paper.pdf},
 publisher = {MIT Press},
 title = {From Regularization Operators to Support Vector Kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/f3bd5ad57c8389a8a1a541a76be463bf-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_f4a331b7,
 abstract = {We employ both master equation and order parameter approaches to analyze the asymptotic dynamics of on-line learning with different learning rate annealing schedules. We examine the relations between the results obtained by the two approaches and obtain new results on the optimal decay coefficients and their dependence on the number of hidden nodes in a two layer architecture.},
 author = {Leen, Todd and Schottky, Bernhard and Saad, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f4a331b7a22d1b237565d8813a34d8ac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f4a331b7a22d1b237565d8813a34d8ac-Metadata.json},
 openalex = {W2097340183},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f4a331b7a22d1b237565d8813a34d8ac-Paper.pdf},
 publisher = {MIT Press},
 title = {Two Approaches to Optimal Annealing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/f4a331b7a22d1b237565d8813a34d8ac-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_f52378e1,
 abstract = {Similarity based fault tolerant retrieval in neural associative memories (NAM) has not lead to wiedespread applications. A drawback of the efficient Willshaw model for sparse patterns [Ste61, WBLH69], is that the high asymptotic information capacity is of little practical use because of high cross talk noise arising in the retrieval for finite sizes. Here a new bidirectional iterative retrieval method for the Willshaw model is presented, called crosswise bidirectional (CB) retrieval, providing enhanced performance. We discuss its asymptotic capacity limit, analyze the first step, and compare it in experiments with the Willshaw model. Applying the very efficient CB memory model either in information retrieval systems or as a functional model for reciprocal cortico-cortical pathways requires more than robustness against random noise in the input: Our experiments show also the segmentation ability of CB-retrieval with addresses containing the superposition of pattens, provided even at high memory load.},
 author = {Sommer, Friedrich and Palm, G\"{u}nther},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f52378e14237225a6f6c7d802dc6abbd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f52378e14237225a6f6c7d802dc6abbd-Metadata.json},
 openalex = {W2108562031},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f52378e14237225a6f6c7d802dc6abbd-Paper.pdf},
 publisher = {MIT Press},
 title = {Bidirectional Retrieval from Associative Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/f52378e14237225a6f6c7d802dc6abbd-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_f69e505b,
 abstract = {One of the current challenges to understanding neural information processing in biological systems is to decipher the code carried by large populations of neurons acting in parallel. We present an algorithm for automated discovery of stochastic firing patterns in large ensembles of neurons. The algorithm, from the Helmholtz Machine family, attempts to predict the observed spike patterns in the data. The model consists of an observable layer which is directly activated by the input spike patterns, and hidden units that are activated through ascending connections from the input layer. The hidden unit activity can be propagated down to the observable layer to create a prediction of the data pattern that produced it. Hidden units are added incrementally and their weights are adjusted to improve the fit between the predictions and data, that is, to increase a bound on the probability of the data given the model. This greedy strategy is not globally optimal but is computationally tractable for large populations of neurons. We show benchmark data on artificially constructed spike trains and promising early results on neurophysiological data collected from our chronic multi-electrode cortical implant.},
 author = {de Sa, Virginia and DeCharms, R. and Merzenich, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f69e505b08403ad2298b9f262659929a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f69e505b08403ad2298b9f262659929a-Metadata.json},
 openalex = {W2118962631},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/f69e505b08403ad2298b9f262659929a-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Helmholtz Machines to Analyze Multi-channel Neuronal Recordings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/f69e505b08403ad2298b9f262659929a-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_fb508ef0,
 author = {Liu, Shih-Chii},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/fb508ef074ee78a0e58c68be06d8a2eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/fb508ef074ee78a0e58c68be06d8a2eb-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/fb508ef074ee78a0e58c68be06d8a2eb-Paper.pdf},
 publisher = {MIT Press},
 title = {Silicon Retina with Adaptive Filtering Properties},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/fb508ef074ee78a0e58c68be06d8a2eb-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_fe70c368,
 abstract = {Conditioning experiments probe the ways that animals make predictions about rewards and punishments and use those predictions to control their behavior. One standard model of conditioning paradigms which involve many conditioned stimuli suggests that individual predictions should be added together. Various key results show that this model fails in some circumstances, and motivate an alternative model, in which there is attentional selection between different available stimuli. The new model is a form of mixture of experts, has a close relationship with some other existing psychological suggestions, and is statistically well-founded.},
 author = {Dayan, Peter and Long, Theresa},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/fe70c36866add1572a8e2b96bfede7bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/fe70c36866add1572a8e2b96bfede7bf-Metadata.json},
 openalex = {W2155910333},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/fe70c36866add1572a8e2b96bfede7bf-Paper.pdf},
 publisher = {MIT Press},
 title = {Statistical Models of Conditioning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/fe70c36866add1572a8e2b96bfede7bf-Abstract.html},
 volume = {10},
 year = {1997}
}

@inproceedings{NIPS1997_ff49cc40,
 abstract = {The inverse of the Fisher information matrix is used in the natural gradient descent algorithm to train single-layer and multi-layer perceptrons. We have discovered a new scheme to represent the Fisher information matrix of a stochastic multi-layer perceptron. Based on this scheme, we have designed an algorithm to compute the natural gradient. When the input dimension n is much larger than the number of hidden neurons, the complexity of this algorithm is of order O(n). It is confirmed by simulations that the natural gradient descent learning rule is not only efficient but also robust.},
 author = {Yang, Howard and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ff49cc40a8890e6a60f40ff3026d2730-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ff49cc40a8890e6a60f40ff3026d2730-Metadata.json},
 openalex = {W2127366142},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1997/file/ff49cc40a8890e6a60f40ff3026d2730-Paper.pdf},
 publisher = {MIT Press},
 title = {The Efficiency and the Robustness of Natural Gradient Descent Learning Rule},
 url = {https://proceedings.neurips.cc/paper_files/paper/1997/hash/ff49cc40a8890e6a60f40ff3026d2730-Abstract.html},
 volume = {10},
 year = {1997}
}
