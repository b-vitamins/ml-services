@inproceedings{NIPS2003_01a06836,
 abstract = {Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle filtering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to filter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufficient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels.},
 author = {Kelly, Ryan and Lee, Tai Sing},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/01a0683665f38d8e5e567b3b15ca98bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/01a0683665f38d8e5e567b3b15ca98bf-Metadata.json},
 openalex = {W2137786693},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/01a0683665f38d8e5e567b3b15ca98bf-Paper.pdf},
 publisher = {MIT Press},
 title = {Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/01a0683665f38d8e5e567b3b15ca98bf-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_020bf2c4,
 abstract = {Belief propagation on cyclic graphs is an efficient algorithm for computing approximate marginal probability distributions over single nodes and neighboring nodes in the graph. In this paper we propose two new algorithms for approximating joint probabilities of arbitrary pairs of nodes and prove a number of desirable properties that these estimates fulfill. The first algorithm is a propagation algorithm which is shown to converge if belief propagation converges to a stable fixed point. The second algorithm is based on matrix inversion. Experiments compare a number of competing methods.},
 author = {Welling, Max and Teh, Yee},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/020bf2c45e7bb322f89a226bd2c5d41b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/020bf2c45e7bb322f89a226bd2c5d41b-Metadata.json},
 openalex = {W2154227688},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/020bf2c45e7bb322f89a226bd2c5d41b-Paper.pdf},
 publisher = {MIT Press},
 title = {Linear Response for Approximate Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/020bf2c45e7bb322f89a226bd2c5d41b-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_03cf8717,
 abstract = {In models that define probabilities via energies, maximum likelihood learning typically involves using Markov Chain Monte Carlo to sample from the model's distribution. If the Markov chain is started at the data distribution, learning often works well even if the chain is only run for a few time steps [3]. But if the data distribution contains modes separated by regions of very low density, brief MCMC will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another. We show how to improve brief MCMC by allowing long-range moves that are suggested by the data distribution. If the model is approximately correct, these long-range moves have a reasonable acceptance rate.},
 author = {Welling, Max and Mnih, Andriy and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/03cf87174debaccd689c90c34577b82f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/03cf87174debaccd689c90c34577b82f-Metadata.json},
 openalex = {W2148186262},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/03cf87174debaccd689c90c34577b82f-Paper.pdf},
 publisher = {MIT Press},
 title = {Wormholes Improve Contrastive Divergence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/03cf87174debaccd689c90c34577b82f-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_03e7ef47,
 abstract = {Pairwise coupling is a popular multi-class classification method that combines all comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods: voting and the method by Hastie and Tibshirani (1998)},
 author = {Wu, Ting-fan and Lin, Chih-jen and Weng, Ruby},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/03e7ef47cee6fa4ae7567394b99912b7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/03e7ef47cee6fa4ae7567394b99912b7-Metadata.json},
 openalex = {W1510526001},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/03e7ef47cee6fa4ae7567394b99912b7-Paper.pdf},
 publisher = {MIT Press},
 title = {Probability Estimates for Multi-class Classification by Pairwise Coupling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/03e7ef47cee6fa4ae7567394b99912b7-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_063e26c6,
 abstract = {We employ an efficient method using Bayesian and linear classifiers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information.},
 author = {Natschl\"{a}ger, Thomas and Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/063e26c670d07bb7c4d30e6fc69fe056-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/063e26c670d07bb7c4d30e6fc69fe056-Metadata.json},
 openalex = {W2160178706},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/063e26c670d07bb7c4d30e6fc69fe056-Paper.pdf},
 publisher = {MIT Press},
 title = {Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/063e26c670d07bb7c4d30e6fc69fe056-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_070dbb60,
 abstract = {Mutual Boosting is a method aimed at incorporating contextual information to augment object detection. When multiple detectors of objects and parts are trained in parallel using AdaBoost [1], object detectors might use the remaining intermediate detectors to enrich the weak learner set. This method generalizes the efficient features suggested by Viola and Jones [2] thus enabling information inference between parts and objects in a compositional hierarchy. In our experiments eye-, nose-, mouth- and face detectors are trained using the Mutual Boosting framework. Results show that the method outperforms applications overlooking contextual information. We suggest that achieving contextual integration is a step toward human-like detection capabilities.},
 author = {Fink, Michael and Perona, Pietro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/070dbb6024b5ef93784428afc71f2146-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/070dbb6024b5ef93784428afc71f2146-Metadata.json},
 openalex = {W2115135404},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/070dbb6024b5ef93784428afc71f2146-Paper.pdf},
 publisher = {MIT Press},
 title = {Mutual Boosting for Contextual Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/070dbb6024b5ef93784428afc71f2146-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_0747b9be,
 abstract = {In order to understand AdaBoost's dynamics, especially its ability to maximize margins, we derive an associated simplified nonlinear iterated map and analyze its behavior in low-dimensional cases. We find stable cycles for these cases, which can explicitly be used to solve for AdaBoost's output. By considering AdaBoost as a dynamical system, we are able to prove Ratsch and Warmuth's conjecture that AdaBoost may fail to converge to a maximal-margin combined classifier when given a 'non-optimal' weak learning algorithm. AdaBoost is known to be a coordinate descent method, but other known algorithms that explicitly aim to maximize the margin (such as AdaBoost* and arc-gv) are not. We consider a differentiable function for which coordinate ascent will yield a maximum margin solution. We then make a simple approximation to derive a new boosting algorithm whose updates are slightly more aggressive than those of arc-gv.},
 author = {Rudin, Cynthia and Daubechies, Ingrid and Schapire, Robert E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0747b9be4f90056c30eb5241f06bfe9b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0747b9be4f90056c30eb5241f06bfe9b-Metadata.json},
 openalex = {W2157705457},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0747b9be4f90056c30eb5241f06bfe9b-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Dynamics of Boosting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/0747b9be4f90056c30eb5241f06bfe9b-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_07a9d3fe,
 abstract = {We develop a protocol for optimizing dynamic behavior of a network of simple electronic components, such as a sensor network, an ad hoc network of mobile devices, or a network of communication switches. This protocol requires only local communication and simple computations which are distributed among devices. The protocol is scalable to large networks. As a motivating example, we discuss a problem involving optimization of power consumption, delay, and buffer overflow in a sensor network.

Our approach builds on policy gradient methods for optimization of Markov decision processes. The protocol can be viewed as an extension of policy gradient methods to a context involving a team of agents optimizing aggregate performance through asynchronous distributed communication and computation. We establish that the dynamics of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective.},
 author = {Moallemi, Ciamac C and Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/07a9d3fed4c5ea6b17e80258dee231fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/07a9d3fed4c5ea6b17e80258dee231fa-Metadata.json},
 openalex = {W2139546187},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/07a9d3fed4c5ea6b17e80258dee231fa-Paper.pdf},
 publisher = {MIT Press},
 title = {Distributed Optimization in Adaptive Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/07a9d3fed4c5ea6b17e80258dee231fa-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_0a65e195,
 abstract = {We present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs. Our goal is to pinpoint those features that are most correlated with crashes. This is accomplished by maximizing an appropriately defined utility function. It has analogies with intuitive debugging heuristics, and, as we demonstrate, is able to deal with various types of bugs that occur in real programs.},
 author = {Zheng, Alice and Jordan, Michael and Liblit, Ben and Aiken, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0a65e195cb51418279b6fa8d96847a60-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0a65e195cb51418279b6fa8d96847a60-Metadata.json},
 openalex = {W2155070368},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0a65e195cb51418279b6fa8d96847a60-Paper.pdf},
 publisher = {MIT Press},
 title = {Statistical Debugging of Sampled Programs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/0a65e195cb51418279b6fa8d96847a60-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_0abdc563,
 abstract = {Over the last years significant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classifiers such as SVM's. In this paper we suggest an alternative procedure to the Fisher kernel for systematically finding kernel functions that naturally handle variable length sequence data in multimedia domains. In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. We perform experiments on speaker identification/verification and image classification tasks and show that these new kernels have the best performance in speaker verification and mostly outperform the Fisher kernel based SVM's and the generative classifiers in speaker identification and image classification.},
 author = {Moreno, Pedro and Ho, Purdy and Vasconcelos, Nuno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0abdc563a06105aee3c6136871c9f4d1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0abdc563a06105aee3c6136871c9f4d1-Metadata.json},
 openalex = {W2098770944},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0abdc563a06105aee3c6136871c9f4d1-Paper.pdf},
 publisher = {MIT Press},
 title = {A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/0abdc563a06105aee3c6136871c9f4d1-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_0bf727e9,
 abstract = {We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval.},
 author = {Lavrenko, Victor and Manmatha, R. and Jeon, Jiwoon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0bf727e907c5fc9d5356f11e4c45d613-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0bf727e907c5fc9d5356f11e4c45d613-Metadata.json},
 openalex = {W2127411609},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0bf727e907c5fc9d5356f11e4c45d613-Paper.pdf},
 publisher = {MIT Press},
 title = {A Model for Learning the Semantics of Pictures},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/0bf727e907c5fc9d5356f11e4c45d613-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_0d736389,
 abstract = {Biochemical signal-transduction networks are the biological information-processing systems by which individual cells, from neurons to amoebae, perceive and respond to their chemical environments. We introduce a simplified model of a single biochemical relay and analyse its capacity as a communications channel. A diffusible ligand is released by a sending cell and received by binding to a transmembrane receptor protein on a receiving cell. This receptor-ligand interaction creates a nonlinear communications channel with non-Gaussian noise. We model this channel numerically and study its response to input signals of different frequencies in order to estimate its channel capacity. Stochastic effects introduced in both the diffusion process and the receptor-ligand interaction give the channel low-pass characteristics. We estimate the channel capacity using a water-filling formula adapted from the additive white-noise Gaussian channel.},
 author = {Thomas, Peter and Spencer, Donald and Hampton, Sierra and Park, Peter and Zurkus, Joseph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0d7363894acdee742caf7fe4e97c4d49-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0d7363894acdee742caf7fe4e97c4d49-Metadata.json},
 openalex = {W2168093113},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0d7363894acdee742caf7fe4e97c4d49-Paper.pdf},
 publisher = {MIT Press},
 title = {The Diffusion-Limited Biochemical Signal-Relay Channel},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/0d7363894acdee742caf7fe4e97c4d49-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_0fe47339,
 abstract = {Margin maximizing properties play an important role in the analysis of classification models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built. We formulate and prove a sufficient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss. We also generalize it to multi-class classification problems, and present margin maximizing multi-class versions of logistic regression and support vector machines.},
 author = {Rosset, Saharon and Zhu, Ji and Hastie, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0fe473396242072e84af286632d3f0ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0fe473396242072e84af286632d3f0ff-Metadata.json},
 openalex = {W2168885649},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/0fe473396242072e84af286632d3f0ff-Paper.pdf},
 publisher = {MIT Press},
 title = {Margin Maximizing Loss Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/0fe473396242072e84af286632d3f0ff-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_102f0bb6,
 abstract = {We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of pools of candidate states at each time. We then define an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efficiently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers.},
 author = {Neal, Radford and Beal, Matthew and Roweis, Sam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/102f0bb6efb3a6128a3c750dd16729be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/102f0bb6efb3a6128a3c750dd16729be-Metadata.json},
 openalex = {W2159323669},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
 publisher = {MIT Press},
 title = {Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_10c66082,
 abstract = {In the problem of probability the learner's goal is to output, given a training set and a new object, a suitable probability measure on the possible values of the new object's label. An on-line algorithm for probability is said to be well-calibrated if the probabilities it outputs agree with the observed frequencies. We give a natural non-asymptotic formalization of the notion of well-calibratedness, which we then study under the assumption of randomness (the object/label pairs are independent and identically distributed). It turns out that, although no probability algorithm is automatically well-calibrated in our sense, there exists a wide class of algorithms for multiprobability forecasting (such algorithms are allowed to output a set, ideally very narrow, of probability measures) which satisfy this property; we call the algorithms in this class probability machines. Our experimental results demonstrate that a 1-Nearest Neighbor Venn probability machine performs reasonably well on a standard benchmark data set, and one of our theoretical results asserts that a simple Venn probability machine asymptotically approaches the true conditional probabilities regardless, and without knowledge, of the true probability measure generating the examples.},
 author = {Vovk, Vladimir and Shafer, Glenn and Nouretdinov, Ilia},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/10c66082c124f8afe3df4886f5e516e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/10c66082c124f8afe3df4886f5e516e0-Metadata.json},
 openalex = {W2156656573},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/10c66082c124f8afe3df4886f5e516e0-Paper.pdf},
 publisher = {MIT Press},
 title = {Self-calibrating Probability Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/10c66082c124f8afe3df4886f5e516e0-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_123b7f02,
 abstract = {We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with different types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection.},
 author = {Bofill-i-petit, Adria and Murray, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/123b7f02433572a0a560e620311a469c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/123b7f02433572a0a560e620311a469c-Metadata.json},
 openalex = {W2101848872},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/123b7f02433572a0a560e620311a469c-Paper.pdf},
 publisher = {MIT Press},
 title = {Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/123b7f02433572a0a560e620311a469c-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_12ffb096,
 abstract = {Building an accurate protein classification system depends critically upon choosing a good representation of the input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data--examples with known 3D structures, organized into structural classes--whereas in practice, unlabeled data are far more plentiful.In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods and at the same time achieving far greater computational efficiency.Source code is available at www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot. The Spider matlab package is available at www.kyb.tuebingen.mpg.de/bs/people/spider.www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot.},
 author = {Weston, Jason and Zhou, Dengyong and Elisseeff, Andr\'{e} and Noble, William and Leslie, Christina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/12ffb0968f2f56e51a59a6beb37b2859-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/12ffb0968f2f56e51a59a6beb37b2859-Metadata.json},
 openalex = {W2136490963},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/12ffb0968f2f56e51a59a6beb37b2859-Paper.pdf},
 publisher = {MIT Press},
 title = {Semi-supervised protein classification using cluster kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/12ffb0968f2f56e51a59a6beb37b2859-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_1843e35d,
 abstract = {We interpret non-negative matrix factorization geometrically, as the problem of finding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that under certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sampling. For such databases there is a generative model in terms of 'parts' and NMF correctly identifies the 'parts'. We show that our theoretical results are predictive of the performance of published NMF code, by running the published algorithms on one of our synthetic image articulation databases.},
 author = {Donoho, David and Stodden, Victoria},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1843e35d41ccf6e63273495ba42df3c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1843e35d41ccf6e63273495ba42df3c1-Metadata.json},
 openalex = {W2140318696},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1843e35d41ccf6e63273495ba42df3c1-Paper.pdf},
 publisher = {MIT Press},
 title = {When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/1843e35d41ccf6e63273495ba42df3c1-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_186fb23a,
 abstract = {Significant plasticity in sensory cortical representations can be driven in mature animals either by behavioural tasks that pair sensory stimuli with reinforcement, or by electrophysiological experiments that pair sensory input with direct stimulation of neuromodulatory nuclei, but usually not by sensory stimuli presented alone. Biologically motivated theories of representational learning, however, have tended to focus on unsupervised mechanisms, which may play a significant role on evolutionary or developmental timescales, but which neglect this essential role of reinforcement in adult plasticity. By contrast, theoretical reinforcement learning has generally dealt with the acquisition of optimal policies for action in an uncertain world, rather than with the concurrent shaping of sensory representations. This paper develops a framework for representational learning which builds on the relative success of unsupervised generative-modelling accounts of cortical encodings to incorporate the effects of reinforcement in a biologically plausible way.},
 author = {Sahani, Maneesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/186fb23a33995d91ce3c2212189178c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/186fb23a33995d91ce3c2212189178c8-Metadata.json},
 openalex = {W2172114475},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/186fb23a33995d91ce3c2212189178c8-Paper.pdf},
 publisher = {MIT Press},
 title = {A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/186fb23a33995d91ce3c2212189178c8-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_1a68e5f4,
 abstract = {Online algorithms for classification often require vast amounts of memory and computation time when employed in conjunction with kernel functions. In this paper we describe and analyze a simple approach for an on-the-fly reduction of the number of past examples used for prediction. Experiments performed with real datasets show that using the proposed algorithmic approach with a single epoch is competitive with the support vector machine (SVM) although the latter, being a batch algorithm, accesses each training example multiple times.},
 author = {Crammer, Koby and Kandola, Jaz and Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1a68e5f4ade56ed1d4bf273e55510750-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1a68e5f4ade56ed1d4bf273e55510750-Metadata.json},
 openalex = {W2121423746},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1a68e5f4ade56ed1d4bf273e55510750-Paper.pdf},
 publisher = {MIT Press},
 title = {Online Classification on a Budget},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/1a68e5f4ade56ed1d4bf273e55510750-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_1bf0c592,
 abstract = {Significant progress in clustering has been achieved by algorithms that are based on pairwise affinities between the datapoints. In particular, spectral clustering methods have the advantage of being able to divide arbitrarily shaped clusters and are based on efficient eigenvector calculations. However, spectral methods lack a straightforward probabilistic interpretation which makes it difficult to automatically set parameters using training data.

In this paper we use the previously proposed typical cut framework for pairwise clustering. We show an equivalence between calculating the typical cut and inference in an undirected graphical model. We show that for clustering problems with hundreds of datapoints exact inference may still be possible. For more complicated datasets, we show that loopy belief propagation (BP) and generalized belief propagation (GBP) can give excellent results on challenging clustering problems. We also use graphical models to derive a learning algorithm for affinity matrices based on labeled data.},
 author = {Shental, Noam and Zomet, Assaf and Hertz, Tomer and Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1bf0c59238dd24a7f09a889483a50e8f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1bf0c59238dd24a7f09a889483a50e8f-Metadata.json},
 openalex = {W2135492819},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1bf0c59238dd24a7f09a889483a50e8f-Paper.pdf},
 publisher = {MIT Press},
 title = {Pairwise Clustering and Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/1bf0c59238dd24a7f09a889483a50e8f-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_1e0a8405,
 abstract = {Many real-world domains are relational in nature, consisting of a set of objects related to each other in complex ways. This paper focuses on predicting the existence and the type of links between entities in such domains. We apply the relational Markov network framework of Taskar et al. to define a joint probabilistic model over the entire link graph — entity attributes and links. The application of the RMN algorithm to this task requires the definition of probabilistic patterns over subgraph structures. We apply this method to two new relational datasets, one involving university webpages, and the other a social network. We show that the collective classification approach of RMNs, and the introduction of subgraph patterns over link labels, provide significant improvements in accuracy over flat classification, which attempts to predict each link in isolation.},
 author = {Taskar, Ben and Wong, Ming-fai and Abbeel, Pieter and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1e0a84051e6a4a7381473328f43c4884-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1e0a84051e6a4a7381473328f43c4884-Metadata.json},
 openalex = {W2127345773},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1e0a84051e6a4a7381473328f43c4884-Paper.pdf},
 publisher = {MIT Press},
 title = {Link Prediction in Relational Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/1e0a84051e6a4a7381473328f43c4884-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_1f34004e,
 abstract = {This paper compares the ability of human observers to detect target image curves with that of an ideal observer. The target curves are sampled from a generative model which specifies (probabilistically) the geometry and local intensity properties of the curve. The ideal observer performs Bayesian inference on the generative model using MAP estimation. Varying the probability model for the curve geometry enables us investigate whether human performance is best for target curves that obey specific shape statistics, in particular those observed on natural shapes. Experiments are performed with data on both rectangular and hexagonal lattices. Our results show that human observers' performance approaches that of the ideal observer and are, in general, closest to the ideal for conditions where the target curve tends to be straight or similar to natural statistics on curves. This suggests a bias of human observers towards straight curves and natural statistics.},
 author = {Fang, Fang and Kersten, Daniel and Schrater, Paul R and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1f34004ebcb05f9acda6016d5cc52d5e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1f34004ebcb05f9acda6016d5cc52d5e-Metadata.json},
 openalex = {W1507330005},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1f34004ebcb05f9acda6016d5cc52d5e-Paper.pdf},
 publisher = {MIT Press},
 title = {Human and Ideal Observers for Detecting Image Curves},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/1f34004ebcb05f9acda6016d5cc52d5e-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_1fb2a1c3,
 abstract = {Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with finite state spaces. In this work we show that ALP solutions are not limited only to MDPs with finite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors.},
 author = {Hauskrecht, Milos and Kveton, Branislav},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1fb2a1c37b18aa4611c3949d6148d0f8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1fb2a1c37b18aa4611c3949d6148d0f8-Metadata.json},
 openalex = {W2138429419},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/1fb2a1c37b18aa4611c3949d6148d0f8-Paper.pdf},
 publisher = {MIT Press},
 title = {Linear Program Approximations for Factored Continuous-State Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/1fb2a1c37b18aa4611c3949d6148d0f8-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_20c9f570,
 abstract = {Is there a way for an algorithm linked to an unknown body to infer by itself information about this body and the world it is in? Taking the case of space for example, is there a way for this algorithm to realize that its body is in a three dimensional world? Is it possible for this algorithm to discover how to move in a straight line? And more basically: do these questions make any sense at all given that the algorithm only has access to the very high-dimensional data consisting of its sensory inputs and motor outputs?

We demonstrate in this article how these questions can be given a positive answer. We show that it is possible to make an algorithm that, by analyzing the law that links its motor outputs to its sensory inputs, discovers information about the structure of the world regardless of the devices constituting the body it is linked to. We present results from simulations demonstrating a way to issue motor orders resulting in fundamental movements of the body as regards the structure of the physical world.},
 author = {Philipona, D. and O\textquotesingle regan, J.k. and Nadal, J.-p. and Coenen, Olivier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/20c9f5700da1088260df60fcc5df2b53-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/20c9f5700da1088260df60fcc5df2b53-Metadata.json},
 openalex = {W2130360653},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/20c9f5700da1088260df60fcc5df2b53-Paper.pdf},
 publisher = {MIT Press},
 title = {Perception of the Structure of the Physical World Using Unknown Multimodal Sensors and Effectors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/20c9f5700da1088260df60fcc5df2b53-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_217c0e01,
 abstract = {Label ranking is the task of inferring a total order over a predefined set of labels for each given instance. We present a general framework for batch learning of label ranking functions from supervised data. We assume that each instance in the training data is associated with a list of preferences over the label-set, however we do not assume that this list is either complete or consistent. This enables us to accommodate a variety of ranking problems. In contrast to the general form of the supervision, our goal is to learn a ranking function that induces a total order over the entire set of labels. Special cases of our setting are multilabel categorization and hierarchical classification. We present a general boosting-based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration. The applicability of our approach is demonstrated with a set of experiments on a large-scale text corpus.},
 author = {Dekel, Ofer and Singer, Yoram and Manning, Christopher D},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/217c0e01c1828e7279051f1b6675745d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/217c0e01c1828e7279051f1b6675745d-Metadata.json},
 openalex = {W2149166361},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/217c0e01c1828e7279051f1b6675745d-Paper.pdf},
 publisher = {MIT Press},
 title = {Log-Linear Models for Label Ranking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/217c0e01c1828e7279051f1b6675745d-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_23483314,
 abstract = {When clustering a dataset, the right number k of clusters to use is often not obvious, and choosing k automatically is a hard algorithmic problem. In this paper we present an improved algorithm for learning k while clustering. The G-means algorithm is based on a statistical test for the hypothesis that a subset of data follows a Gaussian distribution. G-means runs k-means with increasing k in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each k-means center are Gaussian. Two key advantages are that the hypothesis test does not limit the covariance of the data and does not compute a full covariance matrix. Additionally, G-means only requires one intuitive parameter, the standard statistical significance level α. We present results from experiments showing that the algorithm works well, and better than a recent method based on the BIC penalty for model complexity. In these experiments, we show that the BIC is ineffective as a scoring function, since it does not penalize strongly enough the model's complexity.},
 author = {Hamerly, Greg and Elkan, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/234833147b97bb6aed53a8f4f1c7a7d8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/234833147b97bb6aed53a8f4f1c7a7d8-Metadata.json},
 openalex = {W2150753219},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/234833147b97bb6aed53a8f4f1c7a7d8-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning the k in k-means},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/234833147b97bb6aed53a8f4f1c7a7d8-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_23af4b45,
 abstract = {Face detection is a canonical example of a rare event detection problem, in which target patterns occur with much lower frequency than non-targets. Out of millions of face-sized windows in an input image, for example, only a few will typically contain a face. Viola and Jones recently proposed a cascade architecture for face detection which successfully addresses the rare event nature of the task. A central part of their method is a feature selection algorithm based on AdaBoost. We present a novel cascade learning algorithm based on forward feature selection which is two orders of magnitude faster than the Viola-Jones approach and yields classifiers of equivalent quality. This faster method could be used for more demanding classification tasks, such as on-line learning.},
 author = {Wu, Jianxin and Rehg, James M and Mullin, Matthew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/23af4b45f1e166141a790d1a3126e77a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/23af4b45f1e166141a790d1a3126e77a-Metadata.json},
 openalex = {W2164302083},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/23af4b45f1e166141a790d1a3126e77a-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning a Rare Event Detection Cascade by Direct Feature Selection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/23af4b45f1e166141a790d1a3126e77a-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_24e27b86,
 abstract = {We propose a non-linear Canonical Correlation Analysis (CCA) method which works by coordinating or aligning mixtures of linear models. In the same way that CCA extends the idea of PCA, our work extends recent methods for non-linear dimensionality reduction to the case where multiple embeddings of the same underlying low dimensional coordinates are observed, each lying on a different high dimensional manifold. We also show that a special case of our method, when applied to only a single manifold, reduces to the Laplacian Eigenmaps algorithm. As with previous alignment schemes, once the mixture models have been estimated, all of the parameters of our model can be estimated in closed form without local optima in the learning. Experimental results illustrate the viability of the approach as a non-linear extension of CCA.},
 author = {Verbeek, Jakob and Roweis, Sam and Vlassis, Nikos},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/24e27b869b66e9e62724bd7725d5d9c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/24e27b869b66e9e62724bd7725d5d9c1-Metadata.json},
 openalex = {W2143618849},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/24e27b869b66e9e62724bd7725d5d9c1-Paper.pdf},
 publisher = {MIT Press},
 title = {Non-linear CCA and PCA by Alignment of Local Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/24e27b869b66e9e62724bd7725d5d9c1-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_250413d2,
 abstract = {We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Significant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of significant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proficiency. The results are encouraging: the model attains a level of performance considered to be intermediate for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children.},
 author = {Solan, Zach and Horn, David and Ruppin, Eytan and Edelman, Shimon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/250413d2982f1f83aa62a3a323cd2a87-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/250413d2982f1f83aa62a3a323cd2a87-Metadata.json},
 openalex = {W2134312227},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/250413d2982f1f83aa62a3a323cd2a87-Paper.pdf},
 publisher = {MIT Press},
 title = {Unsupervised Context Sensitive Language Acquisition from a Large Corpus},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/250413d2982f1f83aa62a3a323cd2a87-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_269d837a,
 abstract = {In this paper we present a generative latent variable model for rating-based collaborative filtering called the User Rating Profile model (URP). The generative process which underlies URP is designed to produce complete user rating profiles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each.},
 author = {Marlin, Benjamin M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Metadata.json},
 openalex = {W2151052953},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling User Rating Profiles For Collaborative Filtering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/269d837afada308dd4aeab28ca2d57e4-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_28b60a16,
 abstract = {We present a connectionist architecture that can learn a model of the relations between perceptions and actions and use this model for behavior planning. State representations are learned with a growing self-organizing layer which is directly coupled to a perception and a motor layer. Knowledge about possible state transitions is encoded in the lateral connectivity. Motor signals modulate this lateral connectivity and a dynamic field on the layer organizes a planning process. All mechanisms are local and adaptation is based on Hebbian ideas. The model is continuous in the action, perception, and time domain.},
 author = {Toussaint, Marc},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/28b60a16b55fd531047c0c958ce14b95-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/28b60a16b55fd531047c0c958ce14b95-Metadata.json},
 openalex = {W2951475770},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/28b60a16b55fd531047c0c958ce14b95-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning a world model and planning with a self-organizing, dynamic neural system},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/28b60a16b55fd531047c0c958ce14b95-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_2aaaddf2,
 abstract = {The bootstrap has become a popular method for exploring model (structure) uncertainty. Our experiments with artificial and real-world data demonstrate that the graphs learned from bootstrap samples can be severely biased towards too complex graphical models. Accounting for this bias is hence essential, e.g., when exploring model uncertainty. We find that this bias is intimately tied to (well-known) spurious dependences induced by the bootstrap. The leading-order bias-correction equals one half of Akaike's penalty for model complexity. We demonstrate the effect of this simple bias-correction in our experiments. We also relate this bias to the bias of the plug-in estimator for entropy, as well as to the difference between the expected test and training errors of a graphical model, which asymptotically equals Akaike's penalty (rather than one half).},
 author = {Steck, Harald and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2aaaddf27344ee54058548dc081c6541-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2aaaddf27344ee54058548dc081c6541-Metadata.json},
 openalex = {W2150661892},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2aaaddf27344ee54058548dc081c6541-Paper.pdf},
 publisher = {MIT Press},
 title = {Bias-Corrected Bootstrap and Model Uncertainty},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/2aaaddf27344ee54058548dc081c6541-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_2aedcba6,
 abstract = {According to a widely held view, neurons in lateral geniculate nucleus (LGN) operate on visual stimuli in a linear fashion. There is ample evidence, however, that LGN responses are not entirely linear. To account for nonlinearities we propose a model that synthesizes more than 30 years of research in the field. Model neurons have a linear receptive field, and a nonlinear, divisive suppressive field. The suppressive field computes local root-mean-square contrast. To test this model we recorded responses from LGN of anesthetized paralyzed cats. We estimate model parameters from a basic set of measurements and show that the model can accurately predict responses to novel stimuli. The model might serve as the new standard model of LGN responses. It specifies how visual processing in LGN involves both linear filtering and divisive gain control.},
 author = {Bonin, Vincent and Mante, Valerio and Carandini, Matteo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2aedcba61ca55ceb62d785c6b7f10a83-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2aedcba61ca55ceb62d785c6b7f10a83-Metadata.json},
 openalex = {W2153426880},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2aedcba61ca55ceb62d785c6b7f10a83-Paper.pdf},
 publisher = {MIT Press},
 title = {Nonlinear Processing in LGN Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/2aedcba61ca55ceb62d785c6b7f10a83-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_2c3ddf4b,
 abstract = {The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks. Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data. The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data. Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method.},
 author = {Zhou, Dengyong and Weston, Jason and Gretton, Arthur and Bousquet, Olivier and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2c3ddf4bf13852db711dd1901fb517fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2c3ddf4bf13852db711dd1901fb517fa-Metadata.json},
 openalex = {W2131791003},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2c3ddf4bf13852db711dd1901fb517fa-Paper.pdf},
 publisher = {MIT Press},
 title = {Ranking on Data Manifolds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/2c3ddf4bf13852db711dd1901fb517fa-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_2c6ae45a,
 abstract = {We compute approximate analytical bootstrap averages for support vector classification using a combination of the replica method of statistical physics and the TAP approach for approximate inference. We test our method on a few datasets and compare it with exact averages obtained by extensive Monte-Carlo sampling.},
 author = {Malzahn, D\"{o}rthe and Opper, Manfred},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2c6ae45a3e88aee548c0714fad7f8269-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2c6ae45a3e88aee548c0714fad7f8269-Metadata.json},
 openalex = {W2113983045},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2c6ae45a3e88aee548c0714fad7f8269-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximate Analytical Bootstrap Averages for Support Vector Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/2c6ae45a3e88aee548c0714fad7f8269-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_2dbf2163,
 abstract = {The purpose of this paper is to investigate infinity-sample properties of risk minimization based multi-category classification methods. These methods can be considered as natural extensions to binary large margin classification. We establish conditions that guarantee the infinity-sample consistency of classifiers obtained in the risk minimization framework. Examples are provided for two specific forms of the general formulation, which extend a number of known methods. Using these examples, we show that some risk minimization formulations can also be used to obtain conditional probability estimates for the underlying problem. Such conditional probability information will be useful for statistical inferencing tasks beyond classification.},
 author = {Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2dbf21633f03afcf882eaf10e4b5caca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2dbf21633f03afcf882eaf10e4b5caca-Metadata.json},
 openalex = {W2136380253},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2dbf21633f03afcf882eaf10e4b5caca-Paper.pdf},
 publisher = {MIT Press},
 title = {An Infinity-sample Theory for Multi-category Large Margin Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/2dbf21633f03afcf882eaf10e4b5caca-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_2f4fe03d,
 abstract = {The relative depth of objects causes small shifts in the left and right retinal positions of these objects, called binocular disparity. Here, we describe a neuromorphic implementation of a disparity selective complex cell using the binocular energy model, which has been proposed to model the response of disparity selective cells in the visual cortex. Our system consists of two silicon chips containing spiking neurons with monocular Gabor-type spatial receptive fields (RF) and circuits that combine the spike outputs to compute a disparity selective complex cell response. The disparity selectivity of the cell can be adjusted by both position and phase shifts between the monocular RF profiles, which are both used in biology. Our neuromorphic system performs better with phase encoding, because the relative responses of neurons tuned to different disparities by phase shifts are better matched than the responses of neurons tuned by position shifts.},
 author = {Shi, Bertram and Tsang, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2f4fe03d77724a7217006e5d16728874-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2f4fe03d77724a7217006e5d16728874-Metadata.json},
 openalex = {W2138134784},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neuromorphic Multi-chip Model of a Disparity Selective Complex Cell},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/2f4fe03d77724a7217006e5d16728874-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_300891a6,
 abstract = {Optimal solutions to Markov Decision Problems (MDPs) are very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of those probabilities is far from accurate. Hence, estimation errors are limiting factors in applying MDPs to real-world problems. We propose an algorithm for solving finite-state and finite-action MDPs, where the solution is guaranteed to be robust with respect to estimation errors on the state transition probabilities. Our algorithm involves a statistically accurate yet numerically efficient representation of uncertainty, via Kullback-Leibler divergence bounds. The worst-case complexity of the robust algorithm is the same as the original Bellman recursion. Hence, robustness can be added at practically no extra computing cost.},
 author = {Nilim, Arnab and Ghaoui, Laurent},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/300891a62162b960cf02ce3827bb363c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/300891a62162b960cf02ce3827bb363c-Metadata.json},
 openalex = {W2169186423},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/300891a62162b960cf02ce3827bb363c-Paper.pdf},
 publisher = {MIT Press},
 title = {Robustness in Markov Decision Problems with Uncertain Transition Matrices},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/300891a62162b960cf02ce3827bb363c-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_30aaf34d,
 abstract = {As part of an environmental observation and forecasting system, sensors deployed in the Columbia RIver Estuary (CORIE) gather information on physical dynamics and changes in estuary habitat. Of these, salinity sensors are particularly susceptible to bio-fouling, which gradually degrades sensor response and corrupts critical data. Automatic fault detectors have the capability to identify bio-fouling early and minimize data loss. Complicating the development of discriminatory classifiers is the scarcity of bio-fouling onset examples and the variability of the bio-fouling signature. To solve these problems, we take a novelty detection approach that incorporates a parameterized bio-fouling model. These detectors identify the occurrence of bio-fouling, and its onset time as reliably as human experts. Real-time detectors installed during the summer of 2001 produced no false alarms, yet detected all episodes of sensor degradation before the field staff scheduled these sensors for cleaning. From this initial deployment through February 2003, our bio-fouling detectors have essentially doubled the amount of useful data coming from the CORIE sensors.},
 author = {Archer, Cynthia and Leen, Todd and Baptista, Ant\'{o}nio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/30aaf34d6afd4b11cc3b3ac4704c7908-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/30aaf34d6afd4b11cc3b3ac4704c7908-Metadata.json},
 openalex = {W2131823313},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/30aaf34d6afd4b11cc3b3ac4704c7908-Paper.pdf},
 publisher = {MIT Press},
 title = {Parameterized Novelty Detectors for Environmental Sensor Monitoring},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/30aaf34d6afd4b11cc3b3ac4704c7908-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_31c97cbb,
 abstract = {Why are sensory modalities segregated the way they are? In this paper we show that sensory modalities are well designed for self-supervised cross-modal learning. Using the Minimizing-Disagreement algorithm on an unsupervised speech categorization task with (moving lips) and auditory (sound signal) inputs, we show that very informative auditory dimensions actually harm performance when moved to the side of the network. It is better to throw them away than to consider them part of the visual input. We explain this finding in terms of the statistical structure in sensory inputs.},
 author = {Sa, Virginia},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/31c97cbb941d3e92d0e6f9925e9bc4d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/31c97cbb941d3e92d0e6f9925e9bc4d7-Metadata.json},
 openalex = {W2139835298},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/31c97cbb941d3e92d0e6f9925e9bc4d7-Paper.pdf},
 publisher = {MIT Press},
 title = {Sensory Modality Segregation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/31c97cbb941d3e92d0e6f9925e9bc4d7-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_326a8c05,
 abstract = {We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matern stationary co-variance, in which the differentiability of the regression function is controlled by a parameter, freeing one from fixing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP fitting may allow for implementation of the method on larger datasets.},
 author = {Paciorek, Christopher and Schervish, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/326a8c055c0d04f5b06544665d8bb3ea-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/326a8c055c0d04f5b06544665d8bb3ea-Metadata.json},
 openalex = {W2153347097},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/326a8c055c0d04f5b06544665d8bb3ea-Paper.pdf},
 publisher = {MIT Press},
 title = {Nonstationary Covariance Functions for Gaussian Process Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/326a8c055c0d04f5b06544665d8bb3ea-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_33bb8372,
 abstract = {Inspired by events ranging from 9/11 to the collapse of the accounting firm Arthur Andersen, economists Kunreuther and Heal [5] recently introduced an interesting game-theoretic model for problems of interdependent security (IDS), in which a large number of players must make individual investment decisions related to security — whether physical, financial, medical, or some other type — but in which the ultimate safety of each participant may depend in a complex way on the actions of the entire population. A simple example is the choice of whether to install a fire sprinkler system in an individual condominium in a large building. While such a system might greatly reduce the chances of the owner’s property being destroyed by a fire originating within their own unit, it might do little or nothing to reduce the chances of damage caused by fires originating in other units (since sprinklers can usually only douse small fires early). If “enough” other unit owners have not made the investment in sprinklers, it may be not cost-effective for any individual to do so.},
 author = {Kearns, Michael and Ortiz, Luis E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/33bb83720ba9d2b6da87114380314af5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/33bb83720ba9d2b6da87114380314af5-Metadata.json},
 openalex = {W2149705024},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/33bb83720ba9d2b6da87114380314af5-Paper.pdf},
 publisher = {MIT Press},
 title = {Algorithms for Interdependent Security Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/33bb83720ba9d2b6da87114380314af5-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_3430095c,
 abstract = {The so-called constitute a methodology for choosing actions repeatedly, when the rewards depend both on the choice of action and on the unknown current state of the environment. An experts algorithm has access to a set of strategies (experts), each of which may recommend which action to choose. The algorithm learns how to combine the recommendations of individual experts so that, in the long run, for any fixed sequence of states of the environment, it does as well as the best expert would have done relative to the same sequence. This methodology may not be suitable for situations where the evolution of states of the environment depends on past chosen actions, as is usually the case, for example, in a repeated non-zero-sum game.

A new experts algorithm is presented and analyzed in the context of repeated games. It is shown that asymptotically, under certain conditions, it performs as well as the best available expert. This algorithm is quite different from previously proposed experts algorithms. It represents a shift from the paradigms of regret minimization and myopic optimization to consideration of the long-term effect of a player's actions on the opponent's actions or the environment. The importance of this shift is demonstrated by the fact that this algorithm is capable of inducing cooperation in the repeated Prisoner's Dilemma game, whereas previous experts algorithms converge to the suboptimal non-cooperative play.},
 author = {de Farias, Daniela and Megiddo, Nimrod},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3430095c577593aad3c39c701712bcfe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3430095c577593aad3c39c701712bcfe-Metadata.json},
 openalex = {W2112223472},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3430095c577593aad3c39c701712bcfe-Paper.pdf},
 publisher = {MIT Press},
 title = {How to Combine Expert (and Novice) Advice when Actions Impact the Environment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/3430095c577593aad3c39c701712bcfe-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_34766559,
 abstract = {We show that temporal logic and combinations of temporal logics and modal logics of knowledge can be effectively represented in artificial neural networks. We present a Translation Algorithm from temporal rules to neural networks, and show that the networks compute a fixed-point semantics of the rules. We also apply the translation to the muddy children puzzle, which has been used as a testbed for distributed multi-agent systems. We provide a complete solution to the puzzle with the use of simple neural networks, capable of reasoning about time and of knowledge acquisition through inductive learning.},
 author = {Garcez, Artur and Lamb, Luis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/347665597cbfaef834886adbb848011f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/347665597cbfaef834886adbb848011f-Metadata.json},
 openalex = {W2099997872},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/347665597cbfaef834886adbb848011f-Paper.pdf},
 publisher = {MIT Press},
 title = {Reasoning about Time and Knowledge in Neural Symbolic Learning Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/347665597cbfaef834886adbb848011f-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_3837a451,
 abstract = {We consider the policy search approach to reinforcement learning. We show that if a baseline distribution is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a finite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem.},
 author = {Bagnell, J. and Kakade, Sham M and Schneider, Jeff and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3837a451cd0abc5ce4069304c5442c87-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3837a451cd0abc5ce4069304c5442c87-Metadata.json},
 openalex = {W2165421048},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3837a451cd0abc5ce4069304c5442c87-Paper.pdf},
 publisher = {MIT Press},
 title = {Policy Search by Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/3837a451cd0abc5ce4069304c5442c87-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_3baa271b,
 abstract = {We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the compositionality of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the embodiment of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and confirmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors.},
 author = {Sugita, Yuuya and Tani, Jun},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3baa271bc35fe054c86928f7016e8ae6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3baa271bc35fe054c86928f7016e8ae6-Metadata.json},
 openalex = {W2167069442},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3baa271bc35fe054c86928f7016e8ae6-Paper.pdf},
 publisher = {MIT Press},
 title = {A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/3baa271bc35fe054c86928f7016e8ae6-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_3cf25597,
 abstract = {We present a new method for calculating approximate marginals for probability distributions defined by graphs with cycles, based on a Gaussian entropy bound combined with a semidefinite outer bound on the marginal polytope. This combination leads to a log-determinant maximization problem that can be solved by efficient interior point methods [8]. As with the Bethe approximation and its generalizations [12], the optimizing arguments of this problem can be taken as approximations to the exact marginals. In contrast to Bethe/Kikuchi approaches, our variational problem is strictly convex and so has a unique global optimum. An additional desirable feature is that the value of the optimal solution is guaranteed to provide an upper bound on the log partition function. In experimental trials, the performance of the log-determinant relaxation is comparable to or better than the sum-product algorithm, and by a substantial margin for certain problem classes. Finally, the zero-temperature limit of our log-determinant relaxation recovers a class of well-known semidefinite relaxations for integer programming [e.g., 3].},
 author = {Jordan, Michael and Wainwright, Martin J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3cf2559725a9fdfa602ec8c887440f32-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3cf2559725a9fdfa602ec8c887440f32-Metadata.json},
 openalex = {W2131536112},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3cf2559725a9fdfa602ec8c887440f32-Paper.pdf},
 publisher = {MIT Press},
 title = {Semidefinite Relaxations for Approximate Inference on Graphs with Cycles},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/3cf2559725a9fdfa602ec8c887440f32-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_3f53d719,
 abstract = {We present and empirically test a novel approach for categorizing 3-D free form object shapes represented by range data. In contrast to traditional surface-signature based systems that use alignment to match specific objects, we adapted the newly introduced symbolic-signature representation to classify deformable shapes [10]. Our approach constructs an abstract description of shape classes using an ensemble of classifiers that learn object class parts and their corresponding geometrical relationships from a set of numeric and symbolic descriptors. We used our classification engine in a series of large scale discrimination experiments on two well-defined classes that share many common distinctive features. The experimental results suggest that our method outperforms traditional numeric signature-based methodologies. 1},
 author = {Ruiz-correa, Salvador and Shapiro, Linda and Meila, Marina and Berson, Gabriel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3f53d7190148675e3cd472fc826828c5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3f53d7190148675e3cd472fc826828c5-Metadata.json},
 openalex = {W2096681591},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3f53d7190148675e3cd472fc826828c5-Paper.pdf},
 publisher = {MIT Press},
 title = {Discriminating Deformable Shape Classes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/3f53d7190148675e3cd472fc826828c5-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_3f998e71,
 abstract = {We derive the limiting form of the eigenvalue spectrum for sample co-variance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter α which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping α fixed. As α increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case.},
 author = {Hoyle, David and Rattray, Magnus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3f998e713a6e02287c374fd26835d87e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3f998e713a6e02287c374fd26835d87e-Metadata.json},
 openalex = {W2134848260},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/3f998e713a6e02287c374fd26835d87e-Paper.pdf},
 publisher = {MIT Press},
 title = {Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/3f998e713a6e02287c374fd26835d87e-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_40c48dab,
 abstract = {The design of cooperative multi-robot systems is a highly active research area in robotics. Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. We propose a new algorithm which joins together these two lines of research. For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball.},
 author = {Bererton, Curt and Gordon, Geoffrey J and Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/40c48dab939a482f04dcecde07e27de6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/40c48dab939a482f04dcecde07e27de6-Metadata.json},
 openalex = {W2103001076},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/40c48dab939a482f04dcecde07e27de6-Paper.pdf},
 publisher = {MIT Press},
 title = {Auction Mechanism Design for Multi-Robot Coordination},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/40c48dab939a482f04dcecde07e27de6-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_415e1af7,
 author = {Srivastava, Anuj and Mio, Washington and Liu, Xiuwen and Klassen, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/415e1af7ea95f89f4e375162b21ae38c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/415e1af7ea95f89f4e375162b21ae38c-Metadata.json},
 openalex = {W2134578337},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/415e1af7ea95f89f4e375162b21ae38c-Paper.pdf},
 publisher = {MIT Press},
 title = {Geometric Analysis of Constrained Curves},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/415e1af7ea95f89f4e375162b21ae38c-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_46515dcd,
 abstract = {We have designed and tested a single-chip analog VLSI sensor that detects imminent collisions by measuring radially expansive optic flow. The design of the chip is based on a model proposed to explain leg-extension behavior in flies during landing approaches. A new elementary motion detector (EMD) circuit was developed to measure optic flow. This EMD circuit models the bandpass nature of large monopolar cells (LMCs) immediately postsynaptic to photoreceptors in the fly visual system. A 16 × 16 array of 2-D motion detectors was fabricated on a 2.24 mm × 2.24 mm die in a standard 0.5-µm CMOS process. The chip consumes 140 µW of power from a 5 V supply. With the addition of wide-angle optics, the sensor is able to detect collisions around 500 ms before impact in complex, real-world scenes.},
 author = {Harrison, Reid},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/46515dcd99ea50dd0671bc6840830404-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/46515dcd99ea50dd0671bc6840830404-Metadata.json},
 openalex = {W2140722346},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/46515dcd99ea50dd0671bc6840830404-Paper.pdf},
 publisher = {MIT Press},
 title = {A Low-Power Analog VLSI Visual Collision Detector},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/46515dcd99ea50dd0671bc6840830404-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_466accba,
 abstract = {Consider a number of moving points, where each point is attached to a joint of the human body and projected onto an image plane. Johannson showed that humans can effortlessly detect and recognize the presence of other humans from such displays. This is true even when some of the body points are missing (e.g. because of occlusion) and unrelated clutter points are added to the display. We are interested in replicating this ability in a machine. To this end, we present a labelling and detection scheme in a probabilistic framework. Our method is based on representing the joint probability density of positions and velocities of body points with a graphical model, and using Loopy Belief Propagation to calculate a likely interpretation of the scene. Furthermore, we introduce a global variable representing the body's centroid. Experiments on one motion-captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical models, especially when very few parts are visible. The improvement is due both to the more general graph structure we use and, more significantly, to the introduction of the centroid variable.},
 author = {Fanti, Claudio and Polito, Marzia and Perona, Pietro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/466accbac9a66b805ba50e42ad715740-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/466accbac9a66b805ba50e42ad715740-Metadata.json},
 openalex = {W2101927187},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/466accbac9a66b805ba50e42ad715740-Paper.pdf},
 publisher = {MIT Press},
 title = {An Improved Scheme for Detection and Labelling in Johansson Displays},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/466accbac9a66b805ba50e42ad715740-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_49d4b2fa,
 abstract = {The standard 2-norm SVM is known for its good performance in two-class classification. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an efficient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM.},
 author = {Zhu, Ji and Rosset, Saharon and Tibshirani, Robert and Hastie, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/49d4b2faeb4b7b9e745775793141e2b2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/49d4b2faeb4b7b9e745775793141e2b2-Metadata.json},
 openalex = {W2130698119},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/49d4b2faeb4b7b9e745775793141e2b2-Paper.pdf},
 publisher = {MIT Press},
 title = {1-norm Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/49d4b2faeb4b7b9e745775793141e2b2-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_4a06d868,
 abstract = {A mobile robot acting in the world is faced with a large amount of sensory data and uncertainty in its action outcomes. Indeed, almost all interesting sequential decision-making domains involve large state spaces and large, stochastic action sets. We investigate a way to act intelligently as quickly as possible in domains where finding a complete policy would take a hopelessly long time. This approach, Relational Envelope-based Planning (REBP) tackles large, noisy problems along two axes. First, describing a domain as a relational MDP (instead of as an atomic or propositionally-factored MDP) allows problem structure and dynamics to be captured compactly with a small set of probabilistic, relational rules. Second, an envelope-based approach to planning lets an agent begin acting quickly within a restricted part of the full state space and to judiciously expand its envelope as resources permit.},
 author = {Gardiol, Natalia and Kaelbling, Leslie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4a06d868d044c50af0cf9bc82d2fc19f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4a06d868d044c50af0cf9bc82d2fc19f-Metadata.json},
 openalex = {W2144528208},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4a06d868d044c50af0cf9bc82d2fc19f-Paper.pdf},
 publisher = {MIT Press},
 title = {Envelope-based Planning in Relational MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/4a06d868d044c50af0cf9bc82d2fc19f-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_4a1590df,
 abstract = {In pattern classification tasks, errors are introduced because of differences between the true model and the one obtained via model estimation. Using likelihood-ratio based classification, it is possible to correct for this discrepancy by finding class-pair specific terms to adjust the likelihood ratio directly, and that can make class-pair preference relationships intransitive. In this work, we introduce new methodology that makes necessary corrections to the likelihood ratio, specifically those that are necessary to achieve perfect classification (but not perfect likelihood-ratio correction which can be overkill). The new corrections, while weaker than previously reported such adjustments, are analytically challenging since they involve discontinuous functions, therefore requiring several approximations. We test a number of these new schemes on an isolated-word speech recognition task as well as on the UCI machine learning data sets. Results show that by using the bias terms calculated in this new way, classification accuracy can substantially improve over both the baseline and over our previous results.},
 author = {Ji, Gang and Bilmes, Jeff A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4a1590df1d5968d41b855005bb8b67bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4a1590df1d5968d41b855005bb8b67bf-Metadata.json},
 openalex = {W2152051239},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4a1590df1d5968d41b855005bb8b67bf-Paper.pdf},
 publisher = {MIT Press},
 title = {Necessary Intransitive Likelihood-Ratio Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/4a1590df1d5968d41b855005bb8b67bf-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_4a8423d5,
 abstract = {To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a linear-time distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be 'explained' by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing low-complexity and intuitively interpretable representations.},
 author = {Girolami, Mark and Kab\'{a}n, Ata},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4a8423d5e91fda00bb7e46540e2b0cf1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4a8423d5e91fda00bb7e46540e2b0cf1-Metadata.json},
 openalex = {W2137805602},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4a8423d5e91fda00bb7e46540e2b0cf1-Paper.pdf},
 publisher = {MIT Press},
 title = {Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/4a8423d5e91fda00bb7e46540e2b0cf1-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_4c5bcfec,
 abstract = {We describe a new approximation algorithm for solving partially observable MDPs. Our bounded policy iteration approach searches through the space of bounded-size, stochastic finite state controllers, combining several advantages of gradient ascent (efficiency, search through restricted controller space) and policy iteration (less vulnerability to local optima).},
 author = {Poupart, Pascal and Boutilier, Craig},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4c5bcfec8584af0d967f1ab10179ca4b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4c5bcfec8584af0d967f1ab10179ca4b-Metadata.json},
 openalex = {W2144283793},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 publisher = {MIT Press},
 title = {Bounded Finite State Controllers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_4c8c76b3,
 abstract = {The decision functions constructed by support vector machines (SVM's) usually depend only on a subset of the training set—the so-called support vectors. We derive asymptotically sharp lower and upper bounds on the number of support vectors for several standard types of SVM's. In particular, we show for the Gaussian RBF kernel that the fraction of support vectors tends to twice the Bayes risk for the L1-SVM, to the probability of noise for the L2-SVM, and to 1 for the LS-SVM.},
 author = {Steinwart, Ingo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4c8c76b39d294759a9000cbda3a6571a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4c8c76b39d294759a9000cbda3a6571a-Metadata.json},
 openalex = {W2147079026},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4c8c76b39d294759a9000cbda3a6571a-Paper.pdf},
 publisher = {MIT Press},
 title = {Sparseness of Support Vector Machines---Some Asymptotically Sharp Bounds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/4c8c76b39d294759a9000cbda3a6571a-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_4dd9cec1,
 author = {Littlewort, G.C. and Bartlett, M.S. and Fasel, I.R. and Chenu, J. and Kanda, T. and Ishiguro, H. and Movellan, J.R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4dd9cec1c21bc54eecb53786a2c5fa09-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4dd9cec1c21bc54eecb53786a2c5fa09-Metadata.json},
 openalex = {W2132149409},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4dd9cec1c21bc54eecb53786a2c5fa09-Paper.pdf},
 publisher = {MIT Press},
 title = {Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/4dd9cec1c21bc54eecb53786a2c5fa09-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_4de75424,
 abstract = {We have constructed a second generation CPG chip capable of generating the necessary timing to control the leg of a walking machine. We demonstrate improvements over a previous chip by moving toward a significantly more versatile device. This includes a larger number of silicon neurons, more sophisticated neurons including voltage dependent charging and relative and absolute refractory periods, and enhanced programmability of neural networks. This chip builds on the basic results achieved on a previous chip and expands its versatility to get closer to a self-contained locomotion controller for walking robots.},
 author = {Tenore, Francesco and Etienne-Cummings, Ralph and Lewis, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4de754248c196c85ee4fbdcee89179bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4de754248c196c85ee4fbdcee89179bd-Metadata.json},
 openalex = {W2162978654},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf},
 publisher = {MIT Press},
 title = {Entrainment of Silicon Central Pattern Generators for Legged Locomotory Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/4de754248c196c85ee4fbdcee89179bd-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_4e0223a8,
 abstract = {This paper applies fast sparse multidimensional scaling (MDS) to a large graph of music similarity, with 267K vertices that represent artists, albums, and tracks; and 3.22M edges that represent similarity between those entities. Once vertices are assigned locations in a Euclidean space, the locations can be used to browse music and to generate playlists. MDS on very large sparse graphs can be effectively performed by a family of algorithms called Rectangular Dijsktra (RD) MDS algorithms. These RD algorithms operate on a dense rectangular slice of the distance matrix, created by calling Dijsktra a constant number of times. Two RD algorithms are compared: Landmark MDS, which uses the Nystrom approximation to perform MDS; and a new algorithm called Fast Sparse Embedding, which uses FastMap. These algorithms compare favorably to Laplacian Eigenmaps, both in terms of speed and embedding quality.},
 author = {Platt, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4e0223a87610176ef0d24ef6d2dcde3a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4e0223a87610176ef0d24ef6d2dcde3a-Metadata.json},
 openalex = {W2145713404},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4e0223a87610176ef0d24ef6d2dcde3a-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Embedding of Sparse Similarity Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/4e0223a87610176ef0d24ef6d2dcde3a-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_4ebd440d,
 abstract = {We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This unified view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any fixed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.},
 author = {Shalev-shwartz, Shai and Crammer, Koby and Dekel, Ofer and Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4ebd440d99504722d80de606ea8507da-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4ebd440d99504722d80de606ea8507da-Metadata.json},
 openalex = {W2160218441},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/4ebd440d99504722d80de606ea8507da-Paper.pdf},
 publisher = {MIT Press},
 title = {Online Passive-Aggressive Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/4ebd440d99504722d80de606ea8507da-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_50525975,
 abstract = {Abstract Neural networks enjoy widespread success in both research and industry and, with the advent of quantum technology, it is a crucial challenge to design quantum neural networks for fully quantum learning tasks. Here we propose a truly quantum analogue of classical neurons, which form quantum feedforward neural networks capable of universal quantum computation. We describe the efficient training of these networks using the fidelity as a cost function, providing both classical and efficient quantum implementations. Our method allows for fast optimisation with reduced memory requirements: the number of qudits required scales with only the width, allowing deep-network optimisation. We benchmark our proposal for the quantum task of learning an unknown unitary and find remarkable generalisation behaviour and a striking robustness to noisy training data.},
 author = {Ricks, Bob and Ventura, Dan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/505259756244493872b7709a8a01b536-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/505259756244493872b7709a8a01b536-Metadata.json},
 openalex = {W3004965358},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/505259756244493872b7709a8a01b536-Paper.pdf},
 publisher = {MIT Press},
 title = {Training deep quantum neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/505259756244493872b7709a8a01b536-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_52cf49fe,
 abstract = {Recently, relevance vector machines (RVM) have been fashioned from a sparse Bayesian learning (SBL) framework to perform supervised learning using a weight prior that encourages sparsity of representation. The methodology incorporates an additional set of hyperparameters governing the prior, one for each weight, and then adopts a specific approximation to the full marginalization over all weights and hyperparameters. Despite its empirical success however, no rigorous motivation for this particular approximation is currently available. To address this issue, we demonstrate that SBL can be recast as the application of a rigorous vari-ational approximation to the full model by expressing the prior in a dual form. This formulation obviates the necessity of assuming any hyperpriors and leads to natural, intuitive explanations of why sparsity is achieved in practice.},
 author = {Palmer, Jason and Rao, Bhaskar and Wipf, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/52cf49fea5ff66588408852f65cf8272-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/52cf49fea5ff66588408852f65cf8272-Metadata.json},
 openalex = {W2101730403},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/52cf49fea5ff66588408852f65cf8272-Paper.pdf},
 publisher = {MIT Press},
 title = {Perspectives on Sparse Bayesian Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/52cf49fea5ff66588408852f65cf8272-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_5314b967,
 abstract = {Speech dereverberation is desirable with a view to achieving, for example, robust speech recognition in the real world. However, it is still a challenging problem, especially when using a single microphone. Although blind equalization techniques have been exploited, they cannot deal with speech signals appropriately because their assumptions are not satisfied by speech signals. We propose a new dereverberation principle based on an inherent property of speech signals, namely quasi-periodicity. The present methods learn the dereverberation filter from a lot of speech data with no prior knowledge of the data, and can achieve high quality speech dereverberation especially when the reverberation time is long.},
 author = {Nakatani, Tomohiro and Miyoshi, Masato and Kinoshita, Keisuke},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5314b9674c86e3f9d1ba25ef9bb32895-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5314b9674c86e3f9d1ba25ef9bb32895-Metadata.json},
 openalex = {W2140365942},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5314b9674c86e3f9d1ba25ef9bb32895-Paper.pdf},
 publisher = {MIT Press},
 title = {One Microphone Blind Dereverberation Based on Quasi-periodicity of Speech Signals},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/5314b9674c86e3f9d1ba25ef9bb32895-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_53f0d7c5,
 abstract = {Learning from ambiguous training data is highly relevant in many applications. We present a new learning algorithm for classification problems where labels are associated with sets of pattern instead of individual patterns. This encompasses multiple instance learning as a special case. Our approach is based on a generalization of linear programming boosting and uses results from disjunctive programming to generate successively stronger linear relaxations of a discrete non-convex problem.},
 author = {Andrews, Stuart and Hofmann, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/53f0d7c537d99b3824f0f99d62ea2428-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/53f0d7c537d99b3824f0f99d62ea2428-Metadata.json},
 openalex = {W2165075008},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf},
 publisher = {MIT Press},
 title = {Multiple Instance Learning via Disjunctive Programming Boosting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_54b2b21a,
 abstract = {We present the software architecture of a robotic system for mapping abandoned mines. The software is capable of acquiring consistent 2D maps of large mines with many cycles, represented as Markov random fields. 3D C-space maps are acquired from local 3D range scans, which are used to identify navigable paths using A* search. Our system has been deployed in three abandoned mines, two of which inaccessible to people, where it has acquired maps of unprecedented detail and accuracy.},
 author = {Ferguson, David and Morris, Aaron and H\"{a}hnel, Dirk and Baker, Christopher and Omohundro, Zachary and Reverte, Carlos and Thayer, Scott and Whittaker, Charles and Whittaker, William and Burgard, Wolfram and Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/54b2b21af94108d83c2a909d5b0a6a50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/54b2b21af94108d83c2a909d5b0a6a50-Metadata.json},
 openalex = {W2154231454},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/54b2b21af94108d83c2a909d5b0a6a50-Paper.pdf},
 publisher = {MIT Press},
 title = {An Autonomous Robotic System for Mapping Abandoned Mines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/54b2b21af94108d83c2a909d5b0a6a50-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_55a988df,
 abstract = {We develop a framework based on Bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments. Traditional accounts of conditioning fit parameters within a fixed generative model of reinforcer delivery; uncertainty over the model structure is not considered. We apply the theory to explain the puzzling relationship between second-order conditioning and conditioned inhibition, two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes. According to the theory, second-order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations; conditioned inhibition results when a more complex model is justified by additional experience.},
 author = {Courville, Aaron C and Gordon, Geoffrey J and Touretzky, David and Daw, Nathaniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/55a988dfb00a914717b3000a3374694c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/55a988dfb00a914717b3000a3374694c-Metadata.json},
 openalex = {W2100327648},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/55a988dfb00a914717b3000a3374694c-Paper.pdf},
 publisher = {MIT Press},
 title = {Model Uncertainty in Classical Conditioning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/55a988dfb00a914717b3000a3374694c-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_565030e1,
 abstract = {Humans are able to detect blurring of visual images, but the mechanism by which they do so is not clear. A traditional view is that a blurred image looks unnatural because of the reduction in energy (either globally or locally) at high frequencies. In this paper, we propose that the disruption of local phase can provide an alternative explanation for blur perception. We show that precisely localized features such as step edges result in strong local phase coherence structures across scale and space in the complex wavelet transform domain, and blurring causes loss of such phase coherence. We propose a technique for coarse-to-fine phase prediction of wavelet coefficients, and observe that (1) such predictions are highly effective in natural images, (2) phase coherence increases with the strength of image features, and (3) blurring disrupts the phase coherence relationship in images. We thus lay the groundwork for a new theory of perceptual blur estimation, as well as a variety of algorithms for restoration and manipulation of photographic images.},
 author = {Wang, Zhou and Simoncelli, Eero},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/565030e1fce4e481f9823a7de3b8a047-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/565030e1fce4e481f9823a7de3b8a047-Metadata.json},
 openalex = {W2120038204},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/565030e1fce4e481f9823a7de3b8a047-Paper.pdf},
 publisher = {MIT Press},
 title = {Local Phase Coherence and the Perception of Blur},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/565030e1fce4e481f9823a7de3b8a047-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_57bafb2c,
 author = {liu, Ting and Moore, Andrew and Gray, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/57bafb2c2dfeefba931bb03a835b1fa9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/57bafb2c2dfeefba931bb03a835b1fa9-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/57bafb2c2dfeefba931bb03a835b1fa9-Paper.pdf},
 publisher = {MIT Press},
 title = {New Algorithms for Efficient High Dimensional Non-parametric Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/57bafb2c2dfeefba931bb03a835b1fa9-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_5dec7070,
 abstract = {Many classification algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classification. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a finite-dimensional base class.},
 author = {Bartlett, Peter and Jordan, Michael and Mcauliffe, Jon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5dec707028b05bcbd3a1db5640f842c5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5dec707028b05bcbd3a1db5640f842c5-Metadata.json},
 openalex = {W2111040334},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5dec707028b05bcbd3a1db5640f842c5-Paper.pdf},
 publisher = {MIT Press},
 title = {Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/5dec707028b05bcbd3a1db5640f842c5-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_5e6bd7a6,
 abstract = {We discuss an idea for collecting data in a relatively efficient manner. Our point of view is Bayesian and information-theoretic: on any given trial, we want to adaptively choose the input in such a way that the mutual information between the (unknown) state of the system and the (stochastic) output is maximal, given any prior information (including data collected on any previous trials). We prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to the more usual nonadaptive experimental design.},
 author = {Paninski, Liam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5e6bd7a6970cd4325e587f02667f7f73-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5e6bd7a6970cd4325e587f02667f7f73-Metadata.json},
 openalex = {W3165221460},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5e6bd7a6970cd4325e587f02667f7f73-Paper.pdf},
 publisher = {MIT Press},
 title = {Design of Experiments Via Information Theory},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/5e6bd7a6970cd4325e587f02667f7f73-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_5f146156,
 abstract = {Spectral methods for nonlinear dimensionality reduction (NLDR) impose a neighborhood graph on point data and compute eigenfunctions of a quadratic form generated from the graph. We introduce a more general and more robust formulation of NLDR based on the singular value decomposition (SVD). In this framework, most spectral NLDR principles can be recovered by taking a subset of the constraints in a quadratic form built from local nullspaces on the manifold. The minimax formulation also opens up an interesting class of methods in which the graph is decorated with information at the vertices, offering discrete or continuous maps, reduced computational complexity, and immunity to some solution instabilities of eigenfunction approaches. Apropos, we show almost all NLDR methods based on eigenvalue decompositions (EVD) have a solution instability that increases faster than problem size. This pathology can be observed (and corrected via the minimax formulation) in problems as small as N < 100 points.},
 author = {Brand, Matthew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5f14615696649541a025d3d0f8e0447f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5f14615696649541a025d3d0f8e0447f-Metadata.json},
 openalex = {W2293979602},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf},
 publisher = {MIT Press},
 title = {Minimax Embeddings},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/5f14615696649541a025d3d0f8e0447f-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_6048ff4e,
 abstract = {In pattern recognition, feature extraction techniques are widely employed to reduce the dimensionality of data and to enhance the discriminatory information. Principal component analysis (PCA) and linear discriminant analysis (LDA) are the two most popular linear dimensionality reduction methods. However, PCA is not very effective for the extraction of the most discriminant features, and LDA is not stable due to the small sample size problem. In this paper, we propose some new (linear and nonlinear) feature extractors based on maximum margin criterion (MMC). Geometrically, feature extractors based on MMC maximize the (average) margin between classes after dimensionality reduction. It is shown that MMC can represent class separability better than PCA. As a connection to LDA, we may also derive LDA from MMC by incorporating some constraints. By using some other constraints, we establish a new linear feature extractor that does not suffer from the small sample size problem, which is known to cause serious stability problems for LDA. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in the paper. Our extensive experiments demonstrate that the new feature extractors are effective, stable, and efficient.},
 author = {Li, Haifeng and Jiang, Tao and Zhang, Keshu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6048ff4e8cb07aa60b6777b6f7384d52-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6048ff4e8cb07aa60b6777b6f7384d52-Metadata.json},
 openalex = {W2105055468},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6048ff4e8cb07aa60b6777b6f7384d52-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient and Robust Feature Extraction by Maximum Margin Criterion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/6048ff4e8cb07aa60b6777b6f7384d52-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_61d77652,
 author = {Wang, Xuerui and Hutchinson, Rebecca and Mitchell, Tom M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/61d77652c97ef636343742fc3dcf3ba9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/61d77652c97ef636343742fc3dcf3ba9-Metadata.json},
 openalex = {W2127531787},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/61d77652c97ef636343742fc3dcf3ba9-Paper.pdf},
 publisher = {MIT Press},
 title = {Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/61d77652c97ef636343742fc3dcf3ba9-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_6547884c,
 abstract = {Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems.},
 author = {Pineau, Joelle and Gordon, Geoffrey J and Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6547884cea64550284728eb26b0947ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6547884cea64550284728eb26b0947ef-Metadata.json},
 openalex = {W2103918456},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6547884cea64550284728eb26b0947ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Applying Metric-Trees to Belief-Point POMDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/6547884cea64550284728eb26b0947ef-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_696b35cc,
 abstract = {Nonlinear filtering can solve very complex problems, but typically involve very time consuming calculations. Here we show that for filters that are constructed as a RBF network with Gaussian basis functions, a decomposition into linear filters exists, which can be computed efficiently in the frequency domain, yielding dramatic improvement in speed. We present an application of this idea to image processing. In electron micrograph images of photoreceptor terminals of the fruit fly, Drosophila, synaptic vesicles containing neurotransmitter should be detected and labeled automatically. We use hand labels, provided by human experts, to learn a RBF filter using Support Vector Regression with Gaussian kernels. We will show that the resulting nonlinear filter solves the task to a degree of accuracy, which is close to what can be achieved by human experts. This allows the very time consuming task of data evaluation to be done efficiently.},
 author = {Vollgraf, Roland and Scholz, Michael and Meinertzhagen, Ian and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/696b35cc35e710279b9c2dedc08e22d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/696b35cc35e710279b9c2dedc08e22d7-Metadata.json},
 openalex = {W2166453539},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/696b35cc35e710279b9c2dedc08e22d7-Paper.pdf},
 publisher = {MIT Press},
 title = {Nonlinear Filtering of Electron Micrographs by Means of Support Vector Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/696b35cc35e710279b9c2dedc08e22d7-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_6a4cbdae,
 abstract = {This article addresses the issues of colour classification and collision detection as they occur in the legged league robot soccer environment of RoboCup. We show how the method of one-class classification with support vector machines (SVMs) can be applied to solve these tasks satisfactorily using the limited hardware capacity of the prescribed Sony AIBO quadruped robots. The experimental evaluation shows an improvement over our previous methods of ellipse fitting for colour classification and the statistical approach used for collision detection.},
 author = {Quinlan, Michael and Chalup, Stephan and Middleton, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6a4cbdaedcbda0fa8ddc7ea32073c475-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6a4cbdaedcbda0fa8ddc7ea32073c475-Metadata.json},
 openalex = {W2121780856},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6a4cbdaedcbda0fa8ddc7ea32073c475-Paper.pdf},
 publisher = {MIT Press},
 title = {Application of SVMs for Colour Classification and Collision Detection with AIBO Robots},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/6a4cbdaedcbda0fa8ddc7ea32073c475-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_6b5754d7,
 abstract = {We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to significantly better performance than using a regular GP, or a GP with a fixed transformation.},
 author = {Snelson, Edward and Ghahramani, Zoubin and Rasmussen, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6b5754d737784b51ec5075c0dc437bf0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6b5754d737784b51ec5075c0dc437bf0-Metadata.json},
 openalex = {W2134477639},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6b5754d737784b51ec5075c0dc437bf0-Paper.pdf},
 publisher = {MIT Press},
 title = {Warped Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/6b5754d737784b51ec5075c0dc437bf0-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_6be93f7a,
 abstract = {This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a filtering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reflects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently.},
 author = {Carreras, Xavier and M\`{a}rquez, Llu\'{\i}s},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6be93f7a96fed60c477d30ae1de032fd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6be93f7a96fed60c477d30ae1de032fd-Metadata.json},
 openalex = {W2171775879},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6be93f7a96fed60c477d30ae1de032fd-Paper.pdf},
 publisher = {MIT Press},
 title = {Online Learning via Global Feedback for Phrase Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/6be93f7a96fed60c477d30ae1de032fd-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_6ee69d37,
 author = {Mizutani, Eiji and Demmel, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6ee69d3769e832ec77c9584e0b7ba112-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6ee69d3769e832ec77c9584e0b7ba112-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6ee69d3769e832ec77c9584e0b7ba112-Paper.pdf},
 publisher = {MIT Press},
 title = {Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter\textquotesingle s Implicit Sparse Hessian},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/6ee69d3769e832ec77c9584e0b7ba112-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_6ef80bb2,
 abstract = {The area under an ROC curve (AUC) is a criterion used in many applications to measure the quality of a classification algorithm. However, the objective function optimized in most of these algorithms is the error rate and not the AUC value. We give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate. Our results show that the average AUC is monotonically increasing as a function of the classification accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable. Thus, algorithms designed to minimize the error rate may not lead to the best possible AUC values. We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC. We report the results of our experiments with RankBoost in several datasets demonstrating the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC.},
 author = {Cortes, Corinna and Mohri, Mehryar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6ef80bb237adf4b6f77d0700e1255907-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6ef80bb237adf4b6f77d0700e1255907-Metadata.json},
 openalex = {W2120100126},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/6ef80bb237adf4b6f77d0700e1255907-Paper.pdf},
 publisher = {MIT Press},
 title = {AUC Optimization vs. Error Rate Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/6ef80bb237adf4b6f77d0700e1255907-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_70c445ee,
 abstract = {We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efficient computation of the optimal Bayesian classification function from the labeled examples. We test our approach on eight real-world datasets.},
 author = {Kemp, Charles and Griffiths, Thomas and Stromsten, Sean and Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/70c445ee64b1ed0583367a12a79a9ef2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/70c445ee64b1ed0583367a12a79a9ef2-Metadata.json},
 openalex = {W2121234459},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/70c445ee64b1ed0583367a12a79a9ef2-Paper.pdf},
 publisher = {MIT Press},
 title = {Semi-Supervised Learning with Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/70c445ee64b1ed0583367a12a79a9ef2-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_70fcb77e,
 abstract = {Loopy belief propagation (BP) has been successfully used in a number of difficult graphical models to find the most probable configuration of the hidden variables. In applications ranging from protein folding to image analysis one would like to find not just the best configuration but rather the top M. While this problem has been solved using the junction tree formalism, in many real world problems the clique size in the junction tree is prohibitively large. In this work we address the problem of finding the M best configurations when exact inference is impossible.

We start by developing a new exact inference algorithm for calculating the best configurations that uses only max-marginals. For approximate inference, we replace the max-marginals with the beliefs calculated using max-product BP and generalized BP. We show empirically that the algorithm can accurately and rapidly approximate the M best configurations in graphs with hundreds of variables.},
 author = {Yanover, Chen and Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/70fcb77e6349f4467edd7227baa73222-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/70fcb77e6349f4467edd7227baa73222-Metadata.json},
 openalex = {W2161508606},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/70fcb77e6349f4467edd7227baa73222-Paper.pdf},
 publisher = {MIT Press},
 title = {Finding the M Most Probable Configurations using Loopy Belief Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/70fcb77e6349f4467edd7227baa73222-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_72e6d323,
 abstract = {Predictive state representations (PSRs) use predictions of a set of tests to represent the state of controlled dynamical systems. One reason why this representation is exciting as an alternative to partially observable Markov decision processes (POMDPs) is that PSR models of dynamical systems may be much more compact than POMDP models. Empirical work on PSRs to date has focused on linear PSRs, which have not allowed for compression relative to POMDPs. We introduce a new notion of tests which allows us to define a new type of PSR that is nonlinear in general and allows for exponential compression in some deterministic dynamical systems. These new tests, called e-tests, are related to the tests used by Rivest and Schapire [1] in their work with the diversity representation, but our PSR avoids some of the pitfalls of their representation—in particular, its potential to be exponentially larger than the equivalent POMDP.},
 author = {Rudary, Matthew and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/72e6d3238361fe70f22fb0ac624a7072-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/72e6d3238361fe70f22fb0ac624a7072-Metadata.json},
 openalex = {W2159242091},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/72e6d3238361fe70f22fb0ac624a7072-Paper.pdf},
 publisher = {MIT Press},
 title = {A Nonlinear Predictive State Representation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/72e6d3238361fe70f22fb0ac624a7072-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_73f490f3,
 abstract = {We describe a neuromorphic chip that utilizes transistor heterogeneity, introduced by the fabrication process, to generate orientation maps similar to those imaged in vivo. Our model consists of a recurrent network of excitatory and inhibitory cells in parallel with a push-pull stage. Similar to a previous model the recurrent network displays hotspots of activity that give rise to visual feature maps. Unlike previous work, however, the map for orientation does not depend on the sign of contrast. Instead, sign-independent cells driven by both ON and OFF channels anchor the map, while push-pull interactions give rise to sign-preserving cells. These two groups of orientation-selective cells are similar to complex and simple cells observed in V1.},
 author = {Merolla, Paul and Boahen, Kwabena A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/73f490f3f868edbcd80b5d3f7cedc403-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/73f490f3f868edbcd80b5d3f7cedc403-Metadata.json},
 openalex = {W2132996777},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/73f490f3f868edbcd80b5d3f7cedc403-Paper.pdf},
 publisher = {MIT Press},
 title = {A Recurrent Model of Orientation Maps with Simple and Complex Cells},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/73f490f3f868edbcd80b5d3f7cedc403-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_777669af,
 abstract = {A major issue in evaluating speech enhancement and hearing compensation algorithms is to come up with a suitable metric that predicts intelligibility as judged by a human listener. Previous methods such as the widely used Speech Transmission Index (STI) fail to account for masking effects that arise from the highly nonlinear cochlear transfer function. We therefore propose a Neural Articulation Index (NAI) that estimates speech intelligibility from the instantaneous neural spike rate over time, produced when a signal is processed by an auditory neural model. By using a well developed model of the auditory periphery and detection theory we show that human perceptual discrimination closely matches the modeled distortion in the instantaneous spike rates of the auditory nerve. In highly rippled frequency transfer conditions the NAI's prediction error is 8% versus the STI's prediction error of 10.8%.},
 author = {Bondy, Jeff and Bruce, Ian and Becker, Suzanna and Haykin, Simon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/777669af68dbccabc30c3b6bcaa81825-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/777669af68dbccabc30c3b6bcaa81825-Metadata.json},
 openalex = {W2111725922},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/777669af68dbccabc30c3b6bcaa81825-Paper.pdf},
 publisher = {MIT Press},
 title = {Predicting Speech Intelligibility from a Population of Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/777669af68dbccabc30c3b6bcaa81825-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7827d1ec,
 abstract = {To understand the brain mechanisms involved in reward prediction on different time scales, we developed a Markov decision task that requires prediction of both immediate and future rewards, and analyzed subjects' brain activities using functional MRI. We estimated the time course of reward prediction and reward prediction error on different time scales from subjects' performance data, and used them as the explanatory variables for SPM analysis. We found topographic maps of different time scales in medial frontal cortex and striatum. The result suggests that different cortico-basal ganglia loops are specialized for reward prediction on different time scales.},
 author = {Tanaka, Saori and Doya, Kenji and Okada, Go and Ueda, Kazutaka and Okamoto, Yasumasa and Yamawaki, Shigeto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7827d1ec626c891d4b61a15c9dff296e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7827d1ec626c891d4b61a15c9dff296e-Metadata.json},
 openalex = {W2127690918},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7827d1ec626c891d4b61a15c9dff296e-Paper.pdf},
 publisher = {MIT Press},
 title = {Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction at Different Time Scales},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7827d1ec626c891d4b61a15c9dff296e-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7884a965,
 abstract = {We address in this paper the question of how the knowledge of the marginal distribution P(x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations.},
 author = {Bousquet, Olivier and Chapelle, Olivier and Hein, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7884a9652e94555c70f96b6be63be216-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7884a9652e94555c70f96b6be63be216-Metadata.json},
 openalex = {W2152859659},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7884a9652e94555c70f96b6be63be216-Paper.pdf},
 publisher = {MIT Press},
 title = {Measure Based Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7884a9652e94555c70f96b6be63be216-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_794288f2,
 abstract = {We argue that K–means and deterministic annealing algorithms for geometric clustering can be derived from the more general Information Bottleneck approach. If we cluster the identities of data points to preserve information about their location, the set of optimal solutions is massively degenerate. But if we treat the equations that define the optimal solution as an iterative algorithm, then a set of smooth initial conditions selects solutions with the desired geometrical properties. In addition to conceptual unification, we argue that this approach can be more efficient and robust than classic algorithms.},
 author = {Still, Susanne and Bialek, William and Bottou, L\'{e}on},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/794288f252f45d35735a13853e605939-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/794288f252f45d35735a13853e605939-Metadata.json},
 openalex = {W2115807451},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/794288f252f45d35735a13853e605939-Paper.pdf},
 publisher = {MIT Press},
 title = {Geometric Clustering Using the Information Bottleneck Method},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/794288f252f45d35735a13853e605939-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7989edad,
 abstract = {Rao–Blackwellization is an approximation technique for probabilistic inference that flexibly combines exact inference with sampling. It is useful in models where conditioning on some of the variables leaves a simpler inference problem that can be solved tractably. This paper presents Sample Propagation, an efficient implementation of Rao–Blackwellized approximate inference for a large class of models. Sample Propagation tightly integrates sampling with message passing in a junction tree, and is named for its simple, appealing structure: it walks the clusters of a junction tree, sampling some of the current cluster's variables and then passing a message to one of its neighbors. We discuss the application of Sample Propagation to conditional Gaussian inference problems such as switching linear dynamical systems.},
 author = {Paskin, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7989edad14ebcd3adfacc7344dc6b739-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7989edad14ebcd3adfacc7344dc6b739-Metadata.json},
 openalex = {W2293897496},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7989edad14ebcd3adfacc7344dc6b739-Paper.pdf},
 publisher = {MIT Press},
 title = {Sample Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7989edad14ebcd3adfacc7344dc6b739-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7993e112,
 abstract = {We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.},
 author = {Kuss, Malte and Rasmussen, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7993e11204b215b27694b6f139e34ce8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7993e11204b215b27694b6f139e34ce8-Metadata.json},
 openalex = {W2151268438},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7993e11204b215b27694b6f139e34ce8-Paper.pdf},
 publisher = {MIT Press},
 title = {Gaussian Processes in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7993e11204b215b27694b6f139e34ce8-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7a68443f,
 abstract = {A balanced network leads to contradictory constraints on memory models, as exemplified in previous work on accommodation of synfire chains. Here we show that these constraints can be overcome by introducing a 'shadow' inhibitory pattern for each excitatory pattern of the model. This is interpreted as a double-balance principle, whereby there exists both global balance between average excitatory and inhibitory currents and local balance between the currents carrying coherent activity at any given time frame. This principle can be applied to networks with Hebbian cell assemblies, leading to a high capacity of the associative memory. The number of possible patterns is limited by a combinatorial constraint that turns out to be P=0.06N within the specific model that we employ. This limit is reached by the Hebbian cell assembly network. To the best of our knowledge this is the first time that such high memory capacities are demonstrated in the asynchronous state of models of spiking neurons.},
 author = {Aviel, Yuval and Horn, David and Abeles, Moshe},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7a68443f5c80d181c42967cd71612af1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7a68443f5c80d181c42967cd71612af1-Metadata.json},
 openalex = {W2131842204},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7a68443f5c80d181c42967cd71612af1-Paper.pdf},
 publisher = {MIT Press},
 title = {The Doubly Balanced Network of Spiking Neurons: A Memory Model with High Capacity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7a68443f5c80d181c42967cd71612af1-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7b41bfa5,
 abstract = {We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts.},
 author = {Griffiths, Thomas and Jordan, Michael and Tenenbaum, Joshua and Blei, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7b41bfa5085806dfa24b8c9de0ce567f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7b41bfa5085806dfa24b8c9de0ce567f-Metadata.json},
 openalex = {W2132827946},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7b41bfa5085806dfa24b8c9de0ce567f-Paper.pdf},
 publisher = {MIT Press},
 title = {Hierarchical Topic Models and the Nested Chinese Restaurant Process},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7b41bfa5085806dfa24b8c9de0ce567f-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7b66b4fd,
 abstract = {Super-resolution aims to produce a high-resolution image from a set of one or more low-resolution images by recovering or inventing plausible high-frequency image content. Typical approaches try to reconstruct a high-resolution image using the sub-pixel displacements of several low-resolution images, usually regularized by a generic smoothness prior over the high-resolution image space. Other methods use training data to learn low-to-high-resolution matches, and have been highly successful even in the single-input-image case. Here we present a domain-specific image prior in the form of a p.d.f. based upon sampled images, and show that for certain types of super-resolution problems, this sample-based prior gives a significant improvement over other common multiple-image super-resolution techniques.},
 author = {Pickup, Lyndsey and Roberts, Stephen J and Zisserman, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7b66b4fd401a271a1c7224027ce111bc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7b66b4fd401a271a1c7224027ce111bc-Metadata.json},
 openalex = {W2123431518},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7b66b4fd401a271a1c7224027ce111bc-Paper.pdf},
 publisher = {MIT Press},
 title = {A Sampled Texture Prior for Image Super-Resolution},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7b66b4fd401a271a1c7224027ce111bc-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7cac11e2,
 abstract = {Spike timing plasticity (STDP) is a special form of synaptic plasticity where the relative timing of post- and presynaptic activity determines the change of the synaptic weight. On the postsynaptic side, active back-propagating spikes in dendrites seem to play a crucial role in the induction of spike timing dependent plasticity. We argue that postsynaptically the temporal change of the membrane potential determines the weight change. Coming from the presynaptic side induction of STDP is closely related to the activation of NMDA channels. Therefore, we will calculate analytically the change of the synaptic weight by correlating the derivative of the membrane potential with the activity of the NMDA channel. Thus, for this calculation we utilise biophysical variables of the physiological cell. The final result shows a weight change curve which conforms with measurements from biology. The positive part of the weight change curve is determined by the NMDA activation. The negative part of the weight change curve is determined by the membrane potential change. Therefore, the weight change curve should change its shape depending on the distance from the soma of the postsynaptic cell. We find temporally asymmetric weight change close to the soma and temporally symmetric weight change in the distal dendrite.},
 author = {Porr, Bernd and Saudargiene, Ausra and W\"{o}rg\"{o}tter, Florentin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7cac11e2f46ed46c339ec3d569853759-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7cac11e2f46ed46c339ec3d569853759-Metadata.json},
 openalex = {W2098817136},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7cac11e2f46ed46c339ec3d569853759-Paper.pdf},
 publisher = {MIT Press},
 title = {Analytical Solution of Spike-timing Dependent Plasticity Based on Synaptic Biophysics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7cac11e2f46ed46c339ec3d569853759-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7cc23420,
 abstract = {The Minimax Probability Machine Classification (MPMC) framework [Lanckriet et al., 2002] builds classifiers by minimizing the maximum probability of misclassification, and gives direct estimates of the probabilistic accuracy bound Ω. The only assumptions that MPMC makes is that good estimates of means and covariance matrices of the classes exist. However, as with Support Vector Machines, MPMC is computationally expensive and requires extensive cross validation experiments to choose kernels and kernel parameters that give good performance. In this paper we address the computational cost of MPMC by proposing an algorithm that constructs nonlinear sparse MPMC (SMPMC) models by incrementally adding basis functions (i.e. kernels) one at a time – greedily selecting the next one that maximizes the accuracy bound Ω. SMPMC automatically chooses both kernel parameters and feature weights without using computationally expensive cross validation. Therefore the SMPMC algorithm simultaneously addresses the problem of kernel selection and feature selection (i.e. feature weighting), based solely on maximizing the accuracy bound Ω. Experimental results indicate that we can obtain reliable bounds Ω, as well as test set accuracies that are comparable to state of the art classification algorithms.},
 author = {Strohmann, Thomas R. and Belitski, Andrei and Grudic, Gregory and DeCoste, Dennis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7cc234202e98d2722580858573fd0817-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7cc234202e98d2722580858573fd0817-Metadata.json},
 openalex = {W2156213105},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7cc234202e98d2722580858573fd0817-Paper.pdf},
 publisher = {MIT Press},
 title = {Sparse Greedy Minimax Probability Machine Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7cc234202e98d2722580858573fd0817-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7cf64379,
 abstract = {We explore approximate policy iteration, replacing the usual cost-function learning step with a learning step in policy space. We give policy-language biases that enable solution of very large relational Markov decision processes (MDPs) that no previous technique can solve. In particular, we induce high-quality domain-specific planners for classical planning domains (both deterministic and stochastic variants) by solving such domains as extremely large MDPs.},
 author = {Fern, Alan and Yoon, Sungwook and Givan, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7cf64379eb6f29a4d25c4b6a2df713e4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7cf64379eb6f29a4d25c4b6a2df713e4-Metadata.json},
 openalex = {W2128547596},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7cf64379eb6f29a4d25c4b6a2df713e4-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximate Policy Iteration with a Policy Language Bias},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7cf64379eb6f29a4d25c4b6a2df713e4-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7e05d6f8,
 abstract = {The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information about another - relevance - variable. The current work extends these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difficult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized regression matrix Σx|yΣx-1, which is also the basis obtained in canonical correlation analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the information-curve), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections.},
 author = {Chechik, Gal and Globerson, Amir and Tishby, Naftali and Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7e05d6f828574fbc975a896b25bb011e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7e05d6f828574fbc975a896b25bb011e-Metadata.json},
 openalex = {W2170503197},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7e05d6f828574fbc975a896b25bb011e-Paper.pdf},
 publisher = {MIT Press},
 title = {Information Bottleneck for Gaussian Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7e05d6f828574fbc975a896b25bb011e-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_7fd80429,
 abstract = {We present a novel method for approximate inference in Bayesian models and regularized risk functionals. It is based on the propagation of mean and variance derived from the Laplace approximation of conditional probabilities in factorizing distributions, much akin to Minka's Expectation Propagation. In the jointly normal case, it coincides with the latter and belief propagation, whereas in the general case, it provides an optimization strategy containing Support Vector chunking, the Bayes Committee Machine, and Gaussian Process chunking as special cases.},
 author = {Eskin, Eleazar and Smola, Alex and Vishwanathan, S.v.n.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7fd804295ef7f6a2822bf4c61f9dc4a8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7fd804295ef7f6a2822bf4c61f9dc4a8-Metadata.json},
 openalex = {W2293292859},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/7fd804295ef7f6a2822bf4c61f9dc4a8-Paper.pdf},
 publisher = {MIT Press},
 title = {Laplace Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/7fd804295ef7f6a2822bf4c61f9dc4a8-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_81b073de,
 abstract = {This paper is concerned with transductive learning. Although transduction appears to be an easier task than induction, there have not been many provably useful algorithms and bounds for transduction. We present explicit error bounds for transduction and derive a general technique for devising bounds within this setting. The technique is applied to derive error bounds for compression schemes such as (transductive) SVMs and for transduction algorithms based on clustering.},
 author = {Derbeko, Philip and El-Yaniv, Ran and Meir, Ron},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/81b073de9370ea873f548e31b8adc081-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/81b073de9370ea873f548e31b8adc081-Metadata.json},
 openalex = {W2110333788},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/81b073de9370ea873f548e31b8adc081-Paper.pdf},
 publisher = {MIT Press},
 title = {Error Bounds for Transductive Learning via Compression and Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/81b073de9370ea873f548e31b8adc081-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_82089746,
 abstract = {We discuss the integration of the expectation-maximization (EM) algorithm for maximum likelihood learning of Bayesian networks with belief propagation algorithms for approximate inference. Specifically we propose to combine the outer-loop step of convergent belief propagation algorithms with the M-step of the EM algorithm. This then yields an approximate EM algorithm that is essentially still double loop, with the important advantage of an inner loop that is guaranteed to converge. Simulations illustrate the merits of such an approach.},
 author = {Heskes, Tom and Zoeter, Onno and Wiegerinck, Wim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8208974663db80265e9bfe7b222dcb18-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8208974663db80265e9bfe7b222dcb18-Metadata.json},
 openalex = {W2123907277},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8208974663db80265e9bfe7b222dcb18-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximate Expectation Maximization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8208974663db80265e9bfe7b222dcb18-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_82ca5dd1,
 abstract = {Given an N×N grid of squares, where each square has a count and an underlying population, our goal is to find the square region with the highest density, and to calculate its significance by randomization. Any density measure D, dependent on the total count and total population of a region, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff's spatial scan statistic DK to find the most significant spatial disease cluster. A naive approach to finding the maximum density region requires O(N3) time, and is generally computationally infeasible. We present a novel algorithm which partitions the grid into overlapping regions, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufficiently dense regions, this method finds the maximum density region in optimal O(N2) time, in practice resulting in significant (10-200x) speedups.},
 author = {Neill, Daniel and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/82ca5dd156cc926b2992f73c2896f761-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/82ca5dd156cc926b2992f73c2896f761-Metadata.json},
 openalex = {W2147555723},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/82ca5dd156cc926b2992f73c2896f761-Paper.pdf},
 publisher = {MIT Press},
 title = {A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/82ca5dd156cc926b2992f73c2896f761-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_831caa1b,
 abstract = {Density estimation with Gaussian Mixture Models is a popular generative technique used also for clustering. We develop a framework to incorporate side information in the form of equivalence constraints into the model estimation procedure. Equivalence constraints are defined on pairs of data points, indicating whether the points arise from the same source (positive constraints) or from different sources (negative constraints). Such constraints can be gathered automatically in some learning problems, and are a natural form of supervision in others. For the estimation of model parameters we present a closed form EM procedure which handles positive constraints, and a Generalized EM procedure using a Markov net which handles negative constraints. Using publicly available data sets we demonstrate that such side information can lead to considerable improvement in clustering tasks, and that our algorithm is preferable to two other suggested methods using the same type of side information.},
 author = {Shental, Noam and Bar-hillel, Aharon and Hertz, Tomer and Weinshall, Daphna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/831caa1b600f852b7844499430ecac17-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/831caa1b600f852b7844499430ecac17-Metadata.json},
 openalex = {W2148687775},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/831caa1b600f852b7844499430ecac17-Paper.pdf},
 publisher = {MIT Press},
 title = {Computing Gaussian Mixture Models with EM Using Equivalence Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/831caa1b600f852b7844499430ecac17-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_83691715,
 abstract = {The 2-class transduction problem, as formulated by Vapnik [1], involves finding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test points. In this form, the problem has exponential computational complexity in the size of the working set. So far it has been attacked by means of integer programming techniques [2] that do not scale to reasonable problem sizes, or by local search procedures [3]. In this paper we present a relaxation of this task based on semi-definite programming (SDP), resulting in a convex optimization problem that has polynomial complexity in the size of the data set. The results are very encouraging for mid sized data sets, however the cost is still too high for large scale problems, due to the high dimensional search space. To this end, we restrict the feasible region by introducing an approximation based on solving an eigenproblem. With this approximation, the computational cost of the algorithm is such that problems with more than 1000 points can be treated.},
 author = {Bie, Tijl and Cristianini, Nello},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/83691715fdc5baf20ed0742b0b85785b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/83691715fdc5baf20ed0742b0b85785b-Metadata.json},
 openalex = {W2109501666},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/83691715fdc5baf20ed0742b0b85785b-Paper.pdf},
 publisher = {MIT Press},
 title = {Convex Methods for Transduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/83691715fdc5baf20ed0742b0b85785b-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_84b20b1f,
 abstract = {We propose a novel method of dimensionality reduction for supervised learning. Given a regression or classification problem in which we wish to predict a variable Y from an explanatory vector X, we treat the problem of dimensionality reduction as that of finding a low-dimensional subspace of X which retains the statistical relationship between X and Y. We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem, we characterize the notion of conditional independence using covariance operators on reproducing kernel Hilbert spaces; this allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y.},
 author = {Fukumizu, Kenji and Bach, Francis and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/84b20b1f5a0d103f5710bb67a043cd78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/84b20b1f5a0d103f5710bb67a043cd78-Metadata.json},
 openalex = {W2113722675},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/84b20b1f5a0d103f5710bb67a043cd78-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernel Dimensionality Reduction for Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/84b20b1f5a0d103f5710bb67a043cd78-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_87682805,
 abstract = {We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.},
 author = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas and Weston, Jason and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/87682805257e619d49b8e0dfdc14affa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/87682805257e619d49b8e0dfdc14affa-Metadata.json},
 openalex = {W2154455818},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/87682805257e619d49b8e0dfdc14affa-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning with Local and Global Consistency},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/87682805257e619d49b8e0dfdc14affa-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_878d5691,
 abstract = {In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.},
 author = {Taskar, Ben and Guestrin, Carlos and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/878d5691c824ee2aaf770f7d36c151d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/878d5691c824ee2aaf770f7d36c151d6-Metadata.json},
 openalex = {W2105644991},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/878d5691c824ee2aaf770f7d36c151d6-Paper.pdf},
 publisher = {MIT Press},
 title = {Max-Margin Markov Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/878d5691c824ee2aaf770f7d36c151d6-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_8a20a862,
 abstract = {The problem of Structure From Motion is a central problem in vision: given the 2D locations of certain points we wish to recover the camera motion and the 3D coordinates of the points. Under simplified camera models, the problem reduces to factorizing a measurement matrix into the product of two low rank matrices. Each element of the measurement matrix contains the position of a point in a particular image. When all elements are observed, the problem can be solved trivially using SVD, but in any realistic situation many elements of the matrix are missing and the ones that are observed have a different directional uncertainty. Under these conditions, most existing factorization algorithms fail while human perception is relatively unchanged.

In this paper we use the well known EM algorithm for factor analysis to perform factorization. This allows us to easily handle missing data and measurement uncertainty and more importantly allows us to place a prior on the temporal trajectory of the latent variables (the camera position). We show that incorporating this prior gives a significant improvement in performance in challenging image sequences.},
 author = {Gruber, Amit and Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8a20a8621978632d76c43dfd28b67767-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8a20a8621978632d76c43dfd28b67767-Metadata.json},
 openalex = {W2126290132},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 publisher = {MIT Press},
 title = {Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_8aec5142,
 abstract = {We propose a method for sequential Bayesian kernel regression. As is the case for the popular Relevance Vector Machine (RVM) [10, 11], the method automatically identifies the number and locations of the kernels. Our algorithm overcomes some of the computational difficulties related to batch methods for kernel regression. It is non-iterative, and requires only a single pass over the data. It is thus applicable to truly sequential data sets and batch data sets alike. The algorithm is based on a generalisation of Importance Sampling, which allows the design of intuitively simple and efficient proposal distributions for the model parameters. Comparative results on two standard data sets show our algorithm to compare favourably with existing batch estimation strategies.},
 author = {Vermaak, Jaco and Godsill, Simon and Doucet, Arnaud},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8aec51422b30d61bce078b27f0babeb1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8aec51422b30d61bce078b27f0babeb1-Metadata.json},
 openalex = {W2152156040},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8aec51422b30d61bce078b27f0babeb1-Paper.pdf},
 publisher = {MIT Press},
 title = {Sequential Bayesian Kernel Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8aec51422b30d61bce078b27f0babeb1-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_8b422406,
 abstract = {There exist many different generalization error bounds for classification. Each of these bounds contains an improvement over the others for certain situations. Our goal is to combine these different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classifiers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2]. This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classifiers, and such priors also arise in the PAC-bayesian setting.},
 author = {Audibert, Jean-yves and Bousquet, Olivier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8b4224068a41c5d37f5e2d54f3995089-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8b4224068a41c5d37f5e2d54f3995089-Metadata.json},
 openalex = {W2110276898},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8b4224068a41c5d37f5e2d54f3995089-Paper.pdf},
 publisher = {MIT Press},
 title = {PAC-Bayesian Generic Chaining},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8b4224068a41c5d37f5e2d54f3995089-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_8bdb5058,
 abstract = {What happens to the optimal interpretation of noisy data when there exists more than one equally plausible interpretation of the data? In a Bayesian model-learning framework the answer depends on the prior expectations of the dynamics of the model parameter that is to be inferred from the data. Local time constraints on the priors are insufficient to pick one interpretation over another. On the other hand, nonlocal time constraints, induced by a $1/f$ noise spectrum of the priors, is shown to permit learning of a specific model parameter even when there are infinitely many equally plausible interpretations of the data. This transition is inferred by a remarkable mapping of the model estimation problem to a dissipative physical system, allowing the use of powerful statistical mechanical methods to uncover the transition from indeterminate to determinate model learning.},
 author = {Atwal, Gurinder and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8bdb5058376143fa358981954e7626b8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8bdb5058376143fa358981954e7626b8-Metadata.json},
 openalex = {W2949638400},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8bdb5058376143fa358981954e7626b8-Paper.pdf},
 publisher = {MIT Press},
 title = {Ambiguous model learning made unambiguous with 1/f priors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8bdb5058376143fa358981954e7626b8-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_8c1b6fa9,
 abstract = {Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). However there are many situations where doing fast Fourier transforms without complete data would be desirable. In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coefficients. Furthermore efficient generalised belief propagation methods between clusters of four nodes enable the Fourier coefficients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points. This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. It is tested on generated data and for a Fourier analysis of a damaged audio signal.},
 author = {Storkey, Amos J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8c1b6fa97c4288a4514365198566c6fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8c1b6fa97c4288a4514365198566c6fa-Metadata.json},
 openalex = {W2117897450},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8c1b6fa97c4288a4514365198566c6fa-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8c1b6fa97c4288a4514365198566c6fa-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_8c9f32e0,
 abstract = {A novel algorithm for actively trading stocks is presented. While traditional expert advice and "universal" algorithms (as well as standard technical trading heuristics) attempt to predict winners or trends, our approach relies on predictable statistical relations between all pairs of stocks in the market. Our empirical results on historical markets provide strong evidence that this type of technical trading can "beat the market" and moreover, can beat the best stock in the market. In doing so we utilize a new idea for smoothing critical parameters in the context of expert learning.},
 author = {Borodin, Allan and El-Yaniv, Ran and Gogan, Vincent},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8c9f32e03aeb2e3000825c8c875c4edd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8c9f32e03aeb2e3000825c8c875c4edd-Metadata.json},
 openalex = {W2950932906},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8c9f32e03aeb2e3000825c8c875c4edd-Paper.pdf},
 publisher = {MIT Press},
 title = {Can We Learn to Beat the Best Stock},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8c9f32e03aeb2e3000825c8c875c4edd-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_8cbd005a,
 abstract = {Despite the popularity of connectionist models in cognitive science, their performance can often be difficult to evaluate. Inspired by the geometric approach to statistical model selection, we introduce a conceptually similar method to examine the global behavior of a connectionist model, by counting the number and types of response patterns it can simulate. The Markov Chain Monte Carlo-based algorithm that we constructed finds these patterns efficiently. We demonstrate the approach using two localist network models of speech perception.},
 author = {Kim, Woojae and Navarro, Daniel and Pitt, Mark and Myung, In},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8cbd005a556ccd4211ce43f309bc0eac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8cbd005a556ccd4211ce43f309bc0eac-Metadata.json},
 openalex = {W2145716531},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8cbd005a556ccd4211ce43f309bc0eac-Paper.pdf},
 publisher = {MIT Press},
 title = {An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8cbd005a556ccd4211ce43f309bc0eac-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_8d09e4b8,
 abstract = {We consider the question of predicting nonlinear time series. Kernel Dynamical Modeling (KDM), a new method based on kernels, is proposed as an extension to linear dynamical models. The kernel trick is used twice: first, to learn the parameters of the model, and second, to compute preimages of the time series predicted in the feature space by means of Support Vector Regression. Our model shows strong connection with the classic Kalman Filter model, with the kernel feature space as hidden state space. Kernel Dynamical Modeling is tested against two benchmark time series and achieves high quality predictions.},
 author = {Ralaivola, Liva and d\textquotesingle Alch\'{e}-Buc, Florence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8d09e4b85c783cbc30c9b8ae175f2d33-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8d09e4b85c783cbc30c9b8ae175f2d33-Metadata.json},
 openalex = {W2134327832},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8d09e4b85c783cbc30c9b8ae175f2d33-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamical Modeling with Kernels for Nonlinear Time Series Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8d09e4b85c783cbc30c9b8ae175f2d33-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_8db92642,
 abstract = {This paper presents an algorithm for learning the time-varying shape of a non-rigid 3D object from uncalibrated 2D tracking data. We model shape motion as a rigid component (rotation and translation) combined with a non-rigid deformation. Reconstruction is ill-posed if arbitrary deformations are allowed. We constrain the problem by assuming that the object shape at each time instant is drawn from a Gaussian distribution. Based on this assumption, the algorithm simultaneously estimates 3D shape and motion for each time frame, learns the parameters of the Gaussian, and robustly fills-in missing data points. We then extend the algorithm to model temporal smoothness in object shape, thus allowing it to handle severe cases of missing data.},
 author = {Torresani, Lorenzo and Hertzmann, Aaron and Bregler, Christoph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8db9264228dc48fbf47535e888c02ae0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8db9264228dc48fbf47535e888c02ae0-Metadata.json},
 openalex = {W2135085348},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/8db9264228dc48fbf47535e888c02ae0-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Non-Rigid 3D Shape from 2D Motion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/8db9264228dc48fbf47535e888c02ae0-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_92049deb,
 abstract = {In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classification of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classification problems using the graph min-cut algorithms. The performance of the model was verified on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments.},
 author = {Kumar, Sanjiv and Hebert, Martial},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/92049debbe566ca5782a3045cf300a3c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/92049debbe566ca5782a3045cf300a3c-Metadata.json},
 openalex = {W2096071754},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/92049debbe566ca5782a3045cf300a3c-Paper.pdf},
 publisher = {MIT Press},
 title = {Discriminative Fields for Modeling Spatial Dependencies in Natural Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/92049debbe566ca5782a3045cf300a3c-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_9407c826,
 abstract = {In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artificially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show an application to traffic analysis at a high-volume Web site.},
 author = {Felzenszwalb, Pedro and Huttenlocher, Daniel and Kleinberg, Jon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9407c826d8e3c07ad37cb2d13d1cb641-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9407c826d8e3c07ad37cb2d13d1cb641-Metadata.json},
 openalex = {W2144977619},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9407c826d8e3c07ad37cb2d13d1cb641-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/9407c826d8e3c07ad37cb2d13d1cb641-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_959ef477,
 abstract = {The barn owl is a nocturnal hunter, capable of capturing prey using auditory information alone [1]. The neural basis for this localization behavior is the existence of auditory neurons with spatial receptive fields [2]. We provide a mathematical description of the operations performed on auditory input signals by the barn owl that facilitate the creation of a representation of auditory space. To develop our model, we first formulate the sound localization problem solved by the barn owl as a statistical estimation problem. The implementation of the solution is constrained by the known neurobiology.},
 author = {Fischer, Brian and Anderson, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/959ef477884b6ac2241b19ee4fb776ae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/959ef477884b6ac2241b19ee4fb776ae-Metadata.json},
 openalex = {W2153452380},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/959ef477884b6ac2241b19ee4fb776ae-Paper.pdf},
 publisher = {MIT Press},
 title = {A Probabilistic Model of Auditory Space Representation in the Barn Owl},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/959ef477884b6ac2241b19ee4fb776ae-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_9657c1ff,
 abstract = {In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior's covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to 'twin kernel PCA' in which a mapping between feature spaces occurs.},
 author = {Lawrence, Neil},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9657c1fffd38824e5ab0472e022e577e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9657c1fffd38824e5ab0472e022e577e-Metadata.json},
 openalex = {W2169779569},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9657c1fffd38824e5ab0472e022e577e-Paper.pdf},
 publisher = {MIT Press},
 title = {Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/9657c1fffd38824e5ab0472e022e577e-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_97416ac0,
 abstract = {Approximation structure plays an important role in inference on loopy graphs. As a tractable structure, tree approximations have been utilized in the variational method of Ghahramani & Jordan (1997) and the sequential projection method of Frey et al. (2000). However, belief propagation represents each factor of the graph with a product of single-node messages. In this paper, belief propagation is extended to represent factors with tree approximations, by way of the expectation propagation framework. That is, each factor sends a message to all pairs of nodes in a tree structure. The result is more accurate inferences and more frequent convergence than ordinary belief propagation, at a lower cost than variational trees or double-loop algorithms.},
 author = {Qi, Yuan and Minka, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/97416ac0f58056947e2eb5d5d253d4f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/97416ac0f58056947e2eb5d5d253d4f2-Metadata.json},
 openalex = {W2141033994},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/97416ac0f58056947e2eb5d5d253d4f2-Paper.pdf},
 publisher = {MIT Press},
 title = {Tree-structured Approximations by Expectation Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/97416ac0f58056947e2eb5d5d253d4f2-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_9824f9c1,
 abstract = {We investigate improvements of AdaBoost that can exploit the fact that the weak hypotheses are one-sided, i.e. either all its positive (or negative) predictions are correct. In particular, for any set of m labeled examples consistent with a disjunction of k literals (which are one-sided in this case), AdaBoost constructs a consistent hypothesis by using O(k2 logm) iterations. On the other hand, a greedy set covering algorithm finds a consistent hypothesis of size O(k log m). Our primary question is whether there is a simple boosting algorithm that performs as well as the greedy set covering.

We first show that InfoBoost, a modification of AdaBoost proposed by Aslam for a different purpose, does perform as well as the greedy set covering algorithm. We then show that AdaBoost requires Ω(k2 log m) iterations for learning k-literal disjunctions. We achieve this with an adversary construction and as well as in simple experiments based on artificial data. Further we give a variant called SemiBoost that can handle the degenerate case when the given examples all have the same label. We conclude by showing that SemiBoost can be used to produce small conjunctions as well.},
 author = {Hatano, Kohei and Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9824f9c1543628a85bb51d2dd6fcf8a3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9824f9c1543628a85bb51d2dd6fcf8a3-Metadata.json},
 openalex = {W2124166527},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9824f9c1543628a85bb51d2dd6fcf8a3-Paper.pdf},
 publisher = {MIT Press},
 title = {Boosting versus Covering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/9824f9c1543628a85bb51d2dd6fcf8a3-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_999600eb,
 abstract = {We study how to learn to play a Pareto-optimal strict Nash equilibrium when there exist multiple equilibria and agents may have different preferences among the equilibria. We focus on repeated coordination games of non-identical interest where agents do not know the game structure up front and receive noisy payoffs. We design efficient near-optimal algorithms for both the perfect monitoring and the imperfect monitoring setting(where the agents only observe their own payoffs and the joint actions).},
 author = {Wang, Xiaofeng and Sandholm, Tuomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/999600eb275cc7196161261972daa59b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/999600eb275cc7196161261972daa59b-Metadata.json},
 openalex = {W2150170179},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/999600eb275cc7196161261972daa59b-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Near-Pareto-Optimal Conventions in Polynomial Time},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/999600eb275cc7196161261972daa59b-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_99f59c08,
 abstract = {Standard approaches to object detection focus on local patches of the image, and try to classify them as background or not. We propose to use the scene context (image as a whole) as an extra source of (global) information, to help resolve local ambiguities. We present a conditional random field for jointly solving the tasks of object detection and scene classification.},
 author = {Murphy, Kevin P and Torralba, Antonio and Freeman, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/99f59c0842e83c808dd1813b48a37c6a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/99f59c0842e83c808dd1813b48a37c6a-Metadata.json},
 openalex = {W2098355199},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/99f59c0842e83c808dd1813b48a37c6a-Paper.pdf},
 publisher = {MIT Press},
 title = {Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/99f59c0842e83c808dd1813b48a37c6a-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_9af76329,
 abstract = {The method of minimum discrimination information estimation is applied to the problem of estimating an <tex xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">n</tex> -dimensional discrete probability distribution in terms of lower order marginal distributions. The procedure provides a convergent iterative algorithm. The method yields regular best asymptotically normal (RBAN) estimates. The general procedure includes as a particular case that proposed by a method using dependence trees. An example is given.},
 author = {Beygelzimer, Alina and Rish, Irina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9af76329c78e28c977ab1bcd1c3fe9b8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9af76329c78e28c977ab1bcd1c3fe9b8-Metadata.json},
 openalex = {W2113700325},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9af76329c78e28c977ab1bcd1c3fe9b8-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximating discrete probability distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/9af76329c78e28c977ab1bcd1c3fe9b8-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_9ed93286,
 abstract = {We propose a functional mixture model for simultaneous clustering and alignment of sets of curves measured on a discrete time grid. The model is specifically tailored to gene expression time course data. Each functional cluster center is a nonlinear combination of solutions of a simple linear differential equation that describes the change of individual mRNA levels when the synthesis and decay rates are constant. The mixture of continuous time parametric functional forms allows one to (a) account for the heterogeneity in the observed profiles, (b) align the profiles in time by estimating real-valued time shifts, (c) capture the synthesis and decay of mRNA in the course of an experiment, and (d) regularize noisy profiles by enforcing smoothness in the mean curves. We derive an EM algorithm for estimating the parameters of the model, and apply the proposed approach to the set of cycling genes in yeast. The experiments show consistent improvement in predictive power and within cluster variance compared to regular Gaussian mixtures.},
 author = {Chudova, Darya and Hart, Christopher and Mjolsness, Eric and Smyth, Padhraic},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9ed9328611fe3f45b3cce8ffe386ee97-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9ed9328611fe3f45b3cce8ffe386ee97-Metadata.json},
 openalex = {W2132215799},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9ed9328611fe3f45b3cce8ffe386ee97-Paper.pdf},
 publisher = {MIT Press},
 title = {Gene Expression Clustering with Functional Mixture Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/9ed9328611fe3f45b3cce8ffe386ee97-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_9fb7b048,
 abstract = {We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented.},
 author = {Bottou, L\'{e}on and Cun, Yann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9fb7b048c96d44a0337f049e0a61ff06-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9fb7b048c96d44a0337f049e0a61ff06-Metadata.json},
 openalex = {W2102486516},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/9fb7b048c96d44a0337f049e0a61ff06-Paper.pdf},
 publisher = {MIT Press},
 title = {Large Scale Online Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/9fb7b048c96d44a0337f049e0a61ff06-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_a554f89d,
 abstract = {Computational mysteries surround the kernels relating the magnitude and sign of changes in efficacy as a function of the time difference between pre- and post-synaptic activity at a synapse. One important idea34 is that kernels result from filtering, ie an attempt by synapses to eliminate noise corrupting learning. This idea has hitherto been applied to trace learning rules; we apply it to experimentally-defined kernels, using it to reverse-engineer assumed signal statistics. We also extend it to consider the additional goal for filtering of weighting learning according to statistical surprise, as in the Z-score transform. This provides a fresh view of observed kernels and can lead to different, and more natural, signal statistics.},
 author = {Dayan, Peter and H\"{a}usser, Michael and London, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a554f89dd61cabd2ff833d3468e2008a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a554f89dd61cabd2ff833d3468e2008a-Metadata.json},
 openalex = {W2135865034},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a554f89dd61cabd2ff833d3468e2008a-Paper.pdf},
 publisher = {MIT Press},
 title = {Plasticity Kernels and Temporal Statistics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/a554f89dd61cabd2ff833d3468e2008a-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_a6d259bf,
 abstract = {A recent area of significant progress in speaker recognition is the use of high level features—idiolect, phonetic relations, prosody, discourse structure, etc. A speaker not only has a distinctive acoustic sound but uses language in a characteristic manner. Large corpora of speech data available in recent years allow experimentation with long term statistics of phone patterns, word patterns, etc. of an individual. We propose the use of support vector machines and term frequency analysis of phone sequences to model a given speaker. To this end, we explore techniques for text categorization applied to the problem. We derive a new kernel based upon a linearization of likelihood ratio scoring. We introduce a new phone-based SVM speaker recognition approach that halves the error rate of conventional phone-based approaches.},
 author = {Campbell, William and Campbell, Joseph and Reynolds, Douglas and Jones, Douglas and Leek, Timothy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a6d259bfbfa2062843ef543e21d7ec8e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a6d259bfbfa2062843ef543e21d7ec8e-Metadata.json},
 openalex = {W2153849757},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a6d259bfbfa2062843ef543e21d7ec8e-Paper.pdf},
 publisher = {MIT Press},
 title = {Phonetic Speaker Recognition with Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/a6d259bfbfa2062843ef543e21d7ec8e-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_a6ea8471,
 author = {Barber, David and Agakov, Felix},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a6ea8471c120fe8cc35a2954c9b9c595-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a6ea8471c120fe8cc35a2954c9b9c595-Metadata.json},
 openalex = {W2148989240},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf},
 publisher = {MIT Press},
 title = {Information Maximization in Noisy Channels : A Variational Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/a6ea8471c120fe8cc35a2954c9b9c595-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_a9365bd9,
 abstract = {A general linear response method for deriving improved estimates of correlations in the variational Bayes framework is presented. Three applications are given and it is discussed how to use linear response as a general principle for improving mean field approximations.},
 author = {Opper, Manfred and Winther, Ole},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a9365bd906e11324065c35be476beb0c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a9365bd906e11324065c35be476beb0c-Metadata.json},
 openalex = {W2139135413},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/a9365bd906e11324065c35be476beb0c-Paper.pdf},
 publisher = {MIT Press},
 title = {Variational Linear Response},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/a9365bd906e11324065c35be476beb0c-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_ab8aa05e,
 abstract = {The connectivity of the nervous system of the nematode Caenorhabditis elegans has been described completely, but the analysis of the neuronal basis of behavior in this system is just beginning. Here, we used an optimization algorithm to search for patterns of connectivity sufficient to compute the sensorimotor transformation underlying C. elegans chemotaxis, a simple form of spatial orientation behavior in which turning probability is modulated by the rate of change of chemical concentration. Optimization produced differentiator networks with inhibitory feedback among all neurons. Further analysis showed that feedback regulates the latency between sensory input and behavior. Common patterns of connectivity between the model and biological networks suggest new functions for previously identified connections in the C. elegans nervous system.},
 author = {Dunn, Nathan A. and Conery, John S. and Lockery, Shawn},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ab8aa05e782481f55fc1412a97e7ac34-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ab8aa05e782481f55fc1412a97e7ac34-Metadata.json},
 openalex = {W2150362841},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ab8aa05e782481f55fc1412a97e7ac34-Paper.pdf},
 publisher = {MIT Press},
 title = {Circuit Optimization Predicts Dynamic Networks for Chemosensory Orientation in Nematode C. elegans},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/ab8aa05e782481f55fc1412a97e7ac34-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_ac1ad983,
 abstract = {We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difficult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces.},
 author = {Weston, Jason and Sch\"{o}lkopf, Bernhard and Bakir, G\"{o}khan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Metadata.json},
 openalex = {W2132773116},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning to Find Pre-Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/ac1ad983e08ad3304a97e147f522747e-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_add21793,
 abstract = {In this paper, sparse representation (factorization) of a data matrix is first discussed. An overcomplete basis matrix is estimated by using the K-means method. We have proved that for the estimated overcom-plete basis matrix, the sparse solution (coefficient matrix) with minimum l1-norm is unique with probability of one, which can be obtained using a linear programming algorithm. The comparisons of the l1-norm solution and the l0-norm solution are also presented, which can be used in recoverability analysis of blind source separation (BSS). Next, we apply the sparse matrix factorization approach to BSS in the overcomplete case. Generally, if the sources are not sufficiently sparse, we perform blind separation in the time-frequency domain after preprocessing the observed data using the wavelet packets transformation. Third, an EEG experimental data analysis example is presented to illustrate the usefulness of the proposed approach and demonstrate its performance. Two almost independent components obtained by the sparse representation method are selected for phase synchronization analysis, and their periods of significant phase synchronization are found which are related to tasks. Finally, concluding remarks review the approach and state areas that require further study.},
 author = {Li, Yuanqing and Amari, Shun-ichi and Shishkin, Sergei and Cao, Jianting and Gu, Fanji and Cichocki, Andrzej},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/add217938e07bb1fd8796e0315b88c10-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/add217938e07bb1fd8796e0315b88c10-Metadata.json},
 openalex = {W2143450358},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/add217938e07bb1fd8796e0315b88c10-Paper.pdf},
 publisher = {MIT Press},
 title = {Sparse Representation and Its Applications in Blind Source Separation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/add217938e07bb1fd8796e0315b88c10-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_af5d5ef2,
 abstract = {When we learn a new motor skill, we have to contend with both the variability inherent in our sensors and the task. The sensory uncertainty can be reduced by using information about the distribution of previously experienced tasks. Here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback. We show that subjects internally represent both the distribution of the task as well as their sensory uncertainty. Moreover, they combine these two sources of information in a way that is qualitatively predicted by optimal Bayesian processing. We further analyze if the subjects can represent multimodal distributions such as mixtures of Gaussians. The results show that the CNS employs probabilistic models during sensorimotor learning even when the priors are multimodal.},
 author = {K\"{o}rding, Konrad and Wolpert, Daniel M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/af5d5ef24881f3c3049a7b9bfe74d58b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/af5d5ef24881f3c3049a7b9bfe74d58b-Metadata.json},
 openalex = {W2136373496},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/af5d5ef24881f3c3049a7b9bfe74d58b-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Inference in Human Sensorimotor Processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/af5d5ef24881f3c3049a7b9bfe74d58b-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_b06b5541,
 abstract = {We explore the phenomena of subjective randomness as a case study in understanding how people discover structure embedded in noise. We present a rational account of randomness perception based on the statistical problem of model selection: given a stimulus, inferring whether the process that generated it was random or regular. Inspired by the mathematical definition of randomness given by Kolmogorov complexity, we characterize regularity in terms of a hierarchy of automata that augment a finite controller with different forms of memory. We find that the regularities detected in binary sequences depend upon presentation format, and that the kinds of automata that can identify these regularities are informative about the cognitive processes engaged by different formats.},
 author = {Griffiths, Thomas and Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b06b5541a62ed438f956b662b4e1ec28-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b06b5541a62ed438f956b662b4e1ec28-Metadata.json},
 openalex = {W2170312178},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b06b5541a62ed438f956b662b4e1ec28-Paper.pdf},
 publisher = {MIT Press},
 title = {From Algorithmic to Subjective Randomness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/b06b5541a62ed438f956b662b4e1ec28-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_b06f50d1,
 abstract = {Even under perfect fixation the human eye is under steady motion (tremor, microsaccades, slow drift). The dynamic theory of vision [1, 2] states that eye-movements can improve hyperacuity. According to this theory, eye movements are thought to create variable spatial excitation patterns on the photoreceptor grid, which will allow for better spatiotemporal summation at later stages. We reexamine this theory using a realistic model of the vertebrate retina by comparing responses of a resting and a moving eye. The performance of simulated ganglion cells in a hyperacuity task is evaluated by ideal observer analysis. We find that in the central retina eye-micromovements have no effect on the performance. Here optical blurring limits vernier acuity. In the retinal periphery however, eye-micromovements clearly improve performance. Based on ROC analysis, our predictions are quantitatively testable in electro-physiological and psychophysical experiments.},
 author = {Hennig, Matthias and W\"{o}rg\"{o}tter, Florentin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Metadata.json},
 openalex = {W2159312462},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Paper.pdf},
 publisher = {MIT Press},
 title = {Eye Micro-movements Improve Stimulus Detection Beyond the Nyquist Limit in the Peripheral Retina},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/b06f50d1f89bd8b2a0fb771c1a69c2b0-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_b0904096,
 abstract = {This paper proposes neural mechanisms of transcranial magnetic stimulation (TMS). TMS can stimulate the brain non-invasively through a brief magnetic pulse delivered by a coil placed on the scalp, interfering with specific cortical functions with a high temporal resolution. Due to these advantages, TMS has been a popular experimental tool in various neuroscience fields. However, the neural mechanisms underlying TMS-induced interference are still unknown; a theoretical basis for TMS has not been developed. This paper provides computational evidence that inhibitory interactions in a neural population, not an isolated single neuron, play a critical role in yielding the neural interference induced by TMS.},
 author = {Miyawaki, Yoichi and Okada, Masato},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b090409688550f3cc93f4ed88ec6cafb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b090409688550f3cc93f4ed88ec6cafb-Metadata.json},
 openalex = {W2141768727},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b090409688550f3cc93f4ed88ec6cafb-Paper.pdf},
 publisher = {MIT Press},
 title = {Mechanism of Neural Interference by Transcranial Magnetic Stimulation: Network or Single Neuron?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/b090409688550f3cc93f4ed88ec6cafb-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_b1301141,
 abstract = {The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The first is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter є of their true probability. We compare both multi-scale samplers on a set of computational examples motivated by NBP, demonstrating significant improvements over existing methods.},
 author = {Ihler, Alexander and Sudderth, Erik and Freeman, William and Willsky, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b1301141feffabac455e1f90a7de2054-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b1301141feffabac455e1f90a7de2054-Metadata.json},
 openalex = {W2131297510},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b1301141feffabac455e1f90a7de2054-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient Multiscale Sampling from Products of Gaussian Mixtures},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/b1301141feffabac455e1f90a7de2054-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_b2397517,
 abstract = {We examine the use of hidden Markov and hidden semi-Markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features. An undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling. We show that the state durations implicit in a standard hidden Markov model are ill-suited to those of real ECG features, and we investigate the use of hidden semi-Markov models for improved state duration modelling.},
 author = {Hughes, Nicholas and Tarassenko, Lionel and Roberts, Stephen J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b23975176653284f1f7356ba5539cfcb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b23975176653284f1f7356ba5539cfcb-Metadata.json},
 openalex = {W2098790142},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b23975176653284f1f7356ba5539cfcb-Paper.pdf},
 publisher = {MIT Press},
 title = {Markov Models for Automated ECG Interval Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/b23975176653284f1f7356ba5539cfcb-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_b3f61131,
 abstract = {A mixed-signal image filtering VLSI has been developed aiming at real-time generation of edge-based image vectors for robust image recognition. A four-stage asynchronous median detection architecture based on analog digital mixed-signal circuits has been introduced to determine the threshold value of edge detection, the key processing parameter in vector generation. As a result, a fully seamless pipeline processing from threshold detection to edge feature map generation has been established. A prototype chip was designed in a 0.35-µm double-polysilicon three-metal-layer CMOS technology and the concept was verified by the fabricated chip. The chip generates a 64-dimension feature vector from a 64x64-pixel gray scale image every 80µsec. This is about 104 times faster than the software computation, making a real-time image recognition system feasible.},
 author = {Yagi, Masakazu and Yamasaki, Hideo and Shibata, Tadashi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b3f61131b6eceeb2b14835fa648a48ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b3f61131b6eceeb2b14835fa648a48ff-Metadata.json},
 openalex = {W2146112472},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b3f61131b6eceeb2b14835fa648a48ff-Paper.pdf},
 publisher = {MIT Press},
 title = {A Mixed-Signal VLSI for Real-Time Generation of Edge-Based Image Vectors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/b3f61131b6eceeb2b14835fa648a48ff-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_b427426b,
 abstract = {Autonomous helicopter flight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter flight. We first fit a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to fly a number of maneuvers taken from an RC helicopter competition.},
 author = {Kim, H. and Jordan, Michael and Sastry, Shankar and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b427426b8acd2c2e53827970f2c2f526-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b427426b8acd2c2e53827970f2c2f526-Metadata.json},
 openalex = {W2108734173},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b427426b8acd2c2e53827970f2c2f526-Paper.pdf},
 publisher = {MIT Press},
 title = {Autonomous Helicopter Flight via Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/b427426b8acd2c2e53827970f2c2f526-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_b4baaff0,
 abstract = {We attempt to understand visual classification in humans using both psychophysical and machine learning techniques. Frontal views of human faces were used for a gender classification task. Human subjects classified the faces and their gender judgment, reaction time and confidence rating were recorded. Several hyperplane learning algorithms were used on the same classification task using the Principal Components of the texture and shape representation of the faces. The classification performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. Our results suggest that human classification can be modeled by some hyperplane algorithms in the feature space we used. For classification, the brain needs more processing for stimuli close to that hyperplane than for those further away.},
 author = {Wichmann, Felix A. and Graf, Arnulf},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b4baaff0e2f11b5356193849021d641f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b4baaff0e2f11b5356193849021d641f-Metadata.json},
 openalex = {W2166739361},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b4baaff0e2f11b5356193849021d641f-Paper.pdf},
 publisher = {MIT Press},
 title = {Insights from Machine Learning Applied to Human Visual Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/b4baaff0e2f11b5356193849021d641f-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_b53477c2,
 abstract = {Although discriminatively trained classifiers are usually more accurate when labeled training data is abundant, previous work has shown that when training data is limited, generative classifiers can out-perform them. This paper describes a hybrid model in which a high-dimensional subset of the parameters are trained to maximize generative likelihood, and another, small, subset of parameters are discriminatively trained to maximize conditional likelihood. We give a sample complexity bound showing that in order to fit the discriminative parameters well, the number of training examples required depends only on the logarithm of the number of feature occurrences and feature set size. Experimental results show that hybrid models can provide lower test error and can produce better accuracy/coverage curves than either their purely generative or purely discriminative counterparts. We also discuss several advantages of hybrid models, and advocate further work in this area.},
 author = {Raina, Rajat and Shen, Yirong and McCallum, Andrew and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b53477c2821c1bf0da5d40e57b870d35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b53477c2821c1bf0da5d40e57b870d35-Metadata.json},
 openalex = {W2171849160},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b53477c2821c1bf0da5d40e57b870d35-Paper.pdf},
 publisher = {MIT Press},
 title = {Classification with Hybrid Generative/Discriminative Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/b53477c2821c1bf0da5d40e57b870d35-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_ba3e9b6a,
 abstract = {New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method. This makes them particularly suitable to the classification of microarray expression data, where the goal is to obtain accurate rules depending on few genes only. Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods. We report on preliminary experiments with five known DNA microarray datasets. These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks.},
 author = {Gentile, Claudio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ba3e9b6a519cfddc560b5d53210df1bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ba3e9b6a519cfddc560b5d53210df1bd-Metadata.json},
 openalex = {W2127692199},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ba3e9b6a519cfddc560b5d53210df1bd-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/ba3e9b6a519cfddc560b5d53210df1bd-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_bb03e43f,
 abstract = {A novel approach to combining clustering and feature selection is presented. It implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing the discriminative power of the used partitioning algorithm. On the technical side, we present an efficient optimization algorithm with guaranteed local convergence property. The only free parameter of this method is selected by a resampling-based stability analysis. Experiments with real-world datasets demonstrate that our method is able to infer both meaningful partitions and meaningful subsets of features.},
 author = {Roth, Volker and Lange, Tilman},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/bb03e43ffe34eeb242a2ee4a4f125e56-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/bb03e43ffe34eeb242a2ee4a4f125e56-Metadata.json},
 openalex = {W2153794775},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/bb03e43ffe34eeb242a2ee4a4f125e56-Paper.pdf},
 publisher = {MIT Press},
 title = {Feature Selection in Clustering Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/bb03e43ffe34eeb242a2ee4a4f125e56-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_bc7f6214,
 abstract = {We introduce an information theoretic method for nonparametric, nonlinear dimensionality reduction, based on the infinite cluster limit of rate distortion theory. By constraining the information available to manifold coordinates, a natural probabilistic map emerges that assigns original data to corresponding points on a lower dimensional manifold. With only the information-distortion trade off as a parameter, our method determines the shape of the manifold, its dimensionality, the probabilistic map and the prior that provide optimal description of the data.},
 author = {Chigirev, Denis and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/bc7f621451b4f5df308a8e098112185d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/bc7f621451b4f5df308a8e098112185d-Metadata.json},
 openalex = {W2134420283},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/bc7f621451b4f5df308a8e098112185d-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimal Manifold Representation of Data: An Information Theoretic Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/bc7f621451b4f5df308a8e098112185d-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_bd7db739,
 abstract = {Knowledge about local invariances with respect to given pattern transformations can greatly improve the accuracy of classification. Previous approaches are either based on regularisation or on the generation of virtual (transformed) examples. We develop a new framework for learning linear classifiers under known transformations based on semidefinite programming. We present a new learning algorithm— the Semidefinite Programming Machine (SDPM)—which is able to find a maximum margin hyperplane when the training examples are polynomial trajectories instead of single points. The solution is found to be sparse in dual variables and allows to identify those points on the trajectory with minimal real-valued output as virtual support vectors. Extensions to segments of trajectories, to more than one transformation parameter, and to learning with kernels are discussed. In experiments we use a Taylor expansion to locally approximate rotational invariance in pixel images from USPS and find improvements over known methods.},
 author = {Graepel, Thore and Herbrich, Ralf},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/bd7db7397f7d83052f829816ecc7f004-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/bd7db7397f7d83052f829816ecc7f004-Metadata.json},
 openalex = {W2101974476},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/bd7db7397f7d83052f829816ecc7f004-Paper.pdf},
 publisher = {MIT Press},
 title = {Invariant Pattern Recognition by Semi-Definite Programming Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/bd7db7397f7d83052f829816ecc7f004-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_be1df9a5,
 abstract = {This paper presents an energy normalization transform as a method to reduce system errors in the LF-ASD brain-computer interface. The energy normalization transform has two major benefits to the system performance. First, it can increase class separation between the active and idle EEG data. Second, it can desensitize the system to the signal amplitude variability. For four subjects in the study, the benefits resulted in the performance improvement of the LF-ASD in the range from 7.7% to 18.9%, while for the fifth subject, who had the highest non-normalized accuracy of 90.5%, the performance did not change notably with normalization.},
 author = {Zhou, Yu and Mason, Steven and Birch, Gary},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/be1df9a5d08724971f64a511e24fc904-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/be1df9a5d08724971f64a511e24fc904-Metadata.json},
 openalex = {W2166885413},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/be1df9a5d08724971f64a511e24fc904-Paper.pdf},
 publisher = {MIT Press},
 title = {Impact of an Energy Normalization Transform on the Performance of the LF-ASD Brain Computer Interface},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/be1df9a5d08724971f64a511e24fc904-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_c30fb4dc,
 abstract = {We describe a system that localizes a single dipole to reasonable accuracy from noisy magnetoencephalographic (MEG) measurements in real time. At its core is a multilayer perceptron (MLP) trained to map sensor signals and head position to dipole location. Including head position overcomes the previous need to retrain the MLP for each subject and session. The training dataset was generated by mapping randomly chosen dipoles and head positions through an analytic model and adding noise from real MEG recordings. After training, a localization took 0.7 ms with an average error of 0.90 cm. A few iterations of a Levenberg-Marquardt routine using the MLP's output as its initial guess took 15 ms and improved the accuracy to 0.53 cm, only slightly above the statistical limits on accuracy imposed by the noise. We applied these methods to localize single dipole sources from MEG components isolated by blind source separation and compared the estimated locations to those generated by standard manually-assisted commercial software.},
 author = {Jun, Sung and Pearlmutter, Barak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c30fb4dc55d801fc7473840b5b161dfa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c30fb4dc55d801fc7473840b5b161dfa-Metadata.json},
 openalex = {W2164225637},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c30fb4dc55d801fc7473840b5b161dfa-Paper.pdf},
 publisher = {MIT Press},
 title = {Subject-Independent Magnetoencephalographic Source Localization by a Multilayer Perceptron},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/c30fb4dc55d801fc7473840b5b161dfa-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_c3a690be,
 abstract = {One way of image denoising is to project a noisy image to the subspace of admissible images derived, for instance, by PCA. However, a major drawback of this method is that all pixels are updated by the projection, even when only a few pixels are corrupted by noise or occlusion. We propose a new method to identify the noisy pixels by l1-norm penalization and to update the identified pixels only. The identification and updating of noisy pixels are formulated as one linear program which can be efficiently solved. In particular, one can apply the upsilon trick to directly specify the fraction of pixels to be reconstructed. Moreover, we extend the linear program to be able to exploit prior knowledge that occlusions often appear in contiguous blocks (e.g., sunglasses on faces). The basic idea is to penalize boundary points and interior points of the occluded area differently. We are also able to show the upsilon property for this extended LP leading to a method which is easy to use. Experimental results demonstrate the power of our approach.},
 author = {Tsuda, Koji and R\"{a}tsch, Gunnar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c3a690be93aa602ee2dc0ccab5b7b67e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c3a690be93aa602ee2dc0ccab5b7b67e-Metadata.json},
 openalex = {W2112973517},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf},
 publisher = {MIT Press},
 title = {Image reconstruction by linear programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/c3a690be93aa602ee2dc0ccab5b7b67e-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_c5dc3e08,
 abstract = {Principal components (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for extreme components analysis (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efficient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets.},
 author = {Welling, Max and Williams, Christopher and Agakov, Felix},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c5dc3e08849bec07e33ca353de62ea04-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c5dc3e08849bec07e33ca353de62ea04-Metadata.json},
 openalex = {W2105354819},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c5dc3e08849bec07e33ca353de62ea04-Paper.pdf},
 publisher = {MIT Press},
 title = {Extreme Components Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/c5dc3e08849bec07e33ca353de62ea04-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_c65d7bd7,
 abstract = {We present a Bayesian approach to color constancy which utilizes a non-Gaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reflectance model when estimating the illuminants of a set of test images. This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base.},
 author = {Rosenberg, Charles and Ladsariya, Alok and Minka, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c65d7bd70fe3e5e3a2f3de681edc193d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c65d7bd70fe3e5e3a2f3de681edc193d-Metadata.json},
 openalex = {W2149875083},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Color Constancy with Non-Gaussian Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/c65d7bd70fe3e5e3a2f3de681edc193d-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_c8067ad1,
 abstract = {In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efficient algorithm that in part uses a linear system to model the world from a single agent's limited perspective, and takes advantage of Kalman filtering to allow an agent to construct a good training signal and learn an effective policy.},
 author = {Chang, Yu-han and Ho, Tracey and Kaelbling, Leslie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c8067ad1937f728f51288b3eb986afaa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c8067ad1937f728f51288b3eb986afaa-Metadata.json},
 openalex = {W2122763142},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf},
 publisher = {MIT Press},
 title = {All learning is Local: Multi-agent Learning in Global Reward Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/c8067ad1937f728f51288b3eb986afaa-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_ca43108d,
 abstract = {This paper describes a system that can annotate a video sequence with: a description of the appearance of each actor; when the actor is in view; and a representation of the actor's activity while in view. The system does not require a fixed background, and is automatic. The system works by (1) tracking people in 2D and then, using an annotated motion capture dataset, (2) synthesizing an annotated 3D motion sequence matching the 2D tracks. The 3D motion capture data is manually annotated off-line using a class structure that describes everyday motions and allows motion annotations to be composed — one may jump while running, for example. Descriptions computed from video of real motions show that the method is accurate.},
 author = {Ramanan, Deva and Forsyth, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ca43108ded5aabc7793d3f9b928cdd54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ca43108ded5aabc7793d3f9b928cdd54-Metadata.json},
 openalex = {W2134988438},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ca43108ded5aabc7793d3f9b928cdd54-Paper.pdf},
 publisher = {MIT Press},
 title = {Automatic Annotation of Everyday Movements},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/ca43108ded5aabc7793d3f9b928cdd54-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_cb2c2041,
 abstract = {At a cocktail party, a listener can selectively attend to a single voice and filter out other acoustical interferences. How to simulate this perceptual ability remains a great challenge. This paper describes a novel supervised learning approach to speech segregation, in which a target speech signal is separated from interfering sounds using spatial location cues: interaural time differences (ITD) and interaural intensity differences (IID). Motivated by the auditory masking effect, we employ the notion of an ideal time-frequency binary mask, which selects the target if it is stronger than the interference in a local time-frequency unit. Within a narrow frequency band, modifications to the relative strength of the target source with respect to the interference trigger systematic changes for estimated ITD and IID. For a given spatial configuration, this interaction produces characteristic clustering in the binaural feature space. Consequently, we perform pattern classification in order to estimate ideal binary masks. A systematic evaluation in terms of signal-to-noise ratio as well as automatic speech recognition performance shows that the resulting system produces masks very close to ideal binary ones. A quantitative comparison shows that our model yields significant improvement in performance over an existing approach. Furthermore, under certain conditions the model produces large speech intelligibility improvements with normal listeners.},
 author = {Roman, Nicoleta and Wang, Deliang and Brown, Guy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cb2c2041d9763d84d7d655e81178f444-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cb2c2041d9763d84d7d655e81178f444-Metadata.json},
 openalex = {W2111890897},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cb2c2041d9763d84d7d655e81178f444-Paper.pdf},
 publisher = {MIT Press},
 title = {A Classification-based Cocktail-party Processor},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/cb2c2041d9763d84d7d655e81178f444-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_cbf8710b,
 abstract = {We formulate linear dimensionality reduction as a semi-parametric estimation problem, enabling us to study its asymptotic behavior. We generalize the problem beyond additive Gaussian noise to (unknown) non-Gaussian additive noise, and to unbiased non-additive models.},
 author = {Srebro, Nathan and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cbf8710b43df3f2c1553e649403426df-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cbf8710b43df3f2c1553e649403426df-Metadata.json},
 openalex = {W2102248902},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cbf8710b43df3f2c1553e649403426df-Paper.pdf},
 publisher = {MIT Press},
 title = {Linear Dependent Dimensionality Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/cbf8710b43df3f2c1553e649403426df-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_cc099134,
 abstract = {Clustering aims at extracting hidden structure in dataset. While the problem of finding compact clusters has been widely studied in the literature, extracting arbitrarily formed elongated structures is considered a much harder problem. In this paper we present a novel clustering algorithm which tackles the problem by a two step procedure: first the data are transformed in such a way that elongated structures become compact ones. In a second step, these new objects are clustered by optimizing a compactness-based criterion. The advantages of the method over related approaches are threefold: (i) robustness properties of compactness-based criteria naturally transfer to the problem of extracting elongated structures, leading to a model which is highly robust against outlier objects; (ii) the transformed distances induce a Mercer kernel which allows us to formulate a polynomial approximation scheme to the generally NP-hard clustering problem; (iii) the new method does not contain free kernel parameters in contrast to methods like spectral clustering or mean-shift clustering.},
 author = {Fischer, Bernd and Roth, Volker and Buhmann, Joachim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cc0991344c3d760ae42259064406bae1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cc0991344c3d760ae42259064406bae1-Metadata.json},
 openalex = {W2157063016},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cc0991344c3d760ae42259064406bae1-Paper.pdf},
 publisher = {MIT Press},
 title = {Clustering with the Connectivity Kernel},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/cc0991344c3d760ae42259064406bae1-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_cd10c7f3,
 abstract = {The detection and pose estimation of people in images and video is made challenging by the variability of human appearance, the complexity of natural scenes, and the high dimensionality of articulated body models. To cope with these problems we represent the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions. We formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters (position and orientation). Because the limbs are described by 6-dimensional vectors encoding pose in 3-space, discretization is impractical and the random variables in our model must be continuous-valued. To approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle filter. This framework facilitates the automatic initialization of the body-model from low level cues and is robust to occlusion of body parts and scene clutter.},
 author = {Sigal, Leonid and Isard, Michael and Sigelman, Benjamin and Black, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cd10c7f376188a4a2ca3e8fea2c03aeb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cd10c7f376188a4a2ca3e8fea2c03aeb-Metadata.json},
 openalex = {W2117872873},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cd10c7f376188a4a2ca3e8fea2c03aeb-Paper.pdf},
 publisher = {MIT Press},
 title = {Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/cd10c7f376188a4a2ca3e8fea2c03aeb-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_cf059682,
 abstract = {Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data.},
 author = {Bengio, Yoshua and Paiement, Jean-fran\c{c}cois and Vincent, Pascal and Delalleau, Olivier and Roux, Nicolas and Ouimet, Marie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cf05968255451bdefe3c5bc64d550517-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cf05968255451bdefe3c5bc64d550517-Metadata.json},
 openalex = {W2153934661},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cf05968255451bdefe3c5bc64d550517-Paper.pdf},
 publisher = {MIT Press},
 title = {Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/cf05968255451bdefe3c5bc64d550517-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d04863f1,
 abstract = {Spectral clustering refers to a class of techniques which rely on the eigen-structure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm. Minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix. We develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors.},
 author = {Bach, Francis and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d04863f100d59b3eb688a11f95b0ae60-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d04863f100d59b3eb688a11f95b0ae60-Metadata.json},
 openalex = {W2141465109},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d04863f100d59b3eb688a11f95b0ae60-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Spectral Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d04863f100d59b3eb688a11f95b0ae60-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d0921d44,
 abstract = {We examine a cascade encoding model for neural response in which a linear filtering stage is followed by a noisy, leaky, integrate-and-fire spike generation mechanism. This model provides a biophysically more realistic alternative to models based on Poisson (memoryless) spike generation, and can effectively reproduce a variety of spiking behaviors seen in vivo. We describe the maximum likelihood estimator for the model parameters, given only extracellular spike train responses (not intracellular voltage data). Specifically, we prove that the log-likelihood function is concave and thus has an essentially unique global maximum that can be found using gradient ascent techniques. We develop an efficient algorithm for computing the maximum likelihood solution, demonstrate the effectiveness of the resulting estimator with numerical simulations, and discuss a method of testing the model's validity using time-rescaling and density evolution techniques.},
 author = {Paninski, Liam and Simoncelli, Eero and Pillow, Jonathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d0921d442ee91b896ad95059d13df618-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d0921d442ee91b896ad95059d13df618-Metadata.json},
 openalex = {W2116531552},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d0921d442ee91b896ad95059d13df618-Paper.pdf},
 publisher = {MIT Press},
 title = {Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Encoding Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d0921d442ee91b896ad95059d13df618-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d095a94d,
 abstract = {We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms.},
 author = {Eichhorn, Jan and Tolias, Andreas and Zien, Alexander and Kuss, Malte and Weston, Jason and Logothetis, Nikos and Sch\"{o}lkopf, Bernhard and Rasmussen, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d095a94d20dcaf7aa07301948549bede-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d095a94d20dcaf7aa07301948549bede-Metadata.json},
 openalex = {W2097353959},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d095a94d20dcaf7aa07301948549bede-Paper.pdf},
 publisher = {MIT Press},
 title = {Prediction on Spike Data Using Kernel Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d095a94d20dcaf7aa07301948549bede-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d16509f6,
 abstract = {Online mechanism design (MD) considers the problem of providing incentives to implement desired system-wide outcomes in systems with self-interested agents that arrive and depart dynamically. Agents can choose to misrepresent their arrival and departure times, in addition to information about their value for different outcomes. We consider the problem of maximizing the total long-term value of the system despite the self-interest of agents. The online MD problem induces a Markov Decision Process (MDP), which when solved can be used to implement optimal policies in a truth-revealing Bayesian-Nash equilibrium.},
 author = {Parkes, David C and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d16509f6eaca1022bd8f28d6bc582cae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d16509f6eaca1022bd8f28d6bc582cae-Metadata.json},
 openalex = {W2109516767},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d16509f6eaca1022bd8f28d6bc582cae-Paper.pdf},
 publisher = {MIT Press},
 title = {An MDP-Based Approach to Online Mechanism Design},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d16509f6eaca1022bd8f28d6bc582cae-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d20be76a,
 abstract = {Many techniques for complex speech processing such as denoising and deconvolution, time/frequency warping, multiple speaker separation, and multiple microphone analysis operate on sequences of short-time power spectra (spectrograms), a representation which is often well-suited to these tasks. However, a significant problem with algorithms that manipulate spectrograms is that the output spectrogram does not include a phase component, which is needed to create a time-domain signal that has good perceptual quality. Here we describe a generative model of time-domain speech signals and their spectrograms, and show how an efficient optimizer can be used to find the maximum a posteriori speech signal, given the spectrogram. In contrast to techniques that alternate between estimating the phase and a spectrally-consistent signal, our technique directly infers the speech signal, thus jointly optimizing the phase and a spectrally-consistent signal. We compare our technique with a standard method using signal-to-noise ratios, but we also provide audio files on the web for the purpose of demonstrating the improvement in perceptual quality that our technique offers.},
 author = {Achan, Kannan and Roweis, Sam and Frey, Brendan J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d20be76a86c0d71c75035fced631f874-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d20be76a86c0d71c75035fced631f874-Metadata.json},
 openalex = {W2138405935},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d20be76a86c0d71c75035fced631f874-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Inference of Speech Signals from Phaseless Spectrograms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d20be76a86c0d71c75035fced631f874-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d254c8a0,
 abstract = {This paper addresses the problem of untangling hidden graphs from a set of noisy detections of undirected edges. We present a model of the generation of the observed graph that includes degree-based structure priors on the hidden graphs. Exact inference in the model is intractable; we present an efficient approximate inference algorithm to compute edge appearance posteriors. We evaluate our model and algorithm on a biological graph inference problem.},
 author = {Morris, Quaid and Frey, Brendan J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d254c8a084d4545bd80577481aa03076-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d254c8a084d4545bd80577481aa03076-Metadata.json},
 openalex = {W2120295426},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d254c8a084d4545bd80577481aa03076-Paper.pdf},
 publisher = {MIT Press},
 title = {Denoising and Untangling Graphs Using Degree Priors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d254c8a084d4545bd80577481aa03076-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d3b1fb02,
 abstract = {This paper presents a method for learning a distance metric from relative comparison such as is closer to B than A is to C. Taking a Support Vector Machine (SVM) approach, we develop an algorithm that provides a flexible way of describing qualitative training data as a set of constraints. We show that such constraints lead to a convex quadratic programming problem that can be solved by adapting standard methods for SVM training. We empirically evaluate the performance and the modelling flexibility of the algorithm on a collection of text documents.},
 author = {Schultz, Matthew and Joachims, Thorsten},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d3b1fb02964aa64e257f9f26a31f72cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d3b1fb02964aa64e257f9f26a31f72cf-Metadata.json},
 openalex = {W2118393783},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning a Distance Metric from Relative Comparisons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d3b1fb02964aa64e257f9f26a31f72cf-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d69116f8,
 abstract = {Many problems in information processing involve some form of dimensionality reduction. In this paper, we introduce Locality Preserving Projections (LPP). These are linear projective maps that arise by solving a variational problem that optimally preserves the neighborhood structure of the data set. LPP should be seen as an alternative to Principal Component Analysis (PCA) – a classical linear technique that projects the data along the directions of maximal variance. When the high dimensional data lies on a low dimensional manifold embedded in the ambient space, the Locality Preserving Projections are obtained by finding the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the manifold. As a result, LPP shares many of the data representation properties of nonlinear techniques such as Laplacian Eigenmaps or Locally Linear Embedding. Yet LPP is linear and more crucially is defined everywhere in ambient space rather than just on the training data points. This is borne out by illustrative examples on some high dimensional data sets.},
 author = {He, Xiaofei and Niyogi, Partha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d69116f8b0140cdeb1f99a4d5096ffe4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d69116f8b0140cdeb1f99a4d5096ffe4-Metadata.json},
 openalex = {W2154872931},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d69116f8b0140cdeb1f99a4d5096ffe4-Paper.pdf},
 publisher = {MIT Press},
 title = {Locality Preserving Projections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d69116f8b0140cdeb1f99a4d5096ffe4-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d9fc0cdb,
 abstract = {Accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue sections. In this paper we present the first automated system for performing this decomposition. We compare the performance of our system with ground truth data and report favorable results.},
 author = {Rabinovich, Andrew and Agarwal, Sameer and Laris, Casey and Price, Jeffrey and Belongie, Serge},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d9fc0cdb67638d50f411432d0d41d0ba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d9fc0cdb67638d50f411432d0d41d0ba-Metadata.json},
 openalex = {W2100471550},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf},
 publisher = {MIT Press},
 title = {Unsupervised Color Decomposition Of Histologically Stained Tissue Samples},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d9fc0cdb67638d50f411432d0d41d0ba-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_d9ff90f4,
 abstract = {This paper presents VLSI circuits with continuous-valued probabilistic behaviour realized by injecting noise into each computing unit(neuron). Interconnecting the noisy neurons forms a Continuous Restricted Boltzmann Machine (CRBM), which has shown promising performance in modelling and classifying noisy biomedical data. The Minimising-Contrastive-Divergence learning algorithm for CRBM is also implemented in mixed-mode VLSI, to adapt the noisy neurons' parameters on-chip.},
 author = {Chen, Hsin and Fleury, Patrice and Murray, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d9ff90f4000eacd3a6c9cb27f78994cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d9ff90f4000eacd3a6c9cb27f78994cf-Metadata.json},
 openalex = {W2123914829},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/d9ff90f4000eacd3a6c9cb27f78994cf-Paper.pdf},
 publisher = {MIT Press},
 title = {Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/d9ff90f4000eacd3a6c9cb27f78994cf-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_da4902cb,
 abstract = {Recent eye tracking studies in natural tasks suggest that there is a tight link between eye movements and goal directed motor actions. However, most existing models of human eye movements provide a bottom up account that relates visual attention to attributes of the visual scene. The purpose of this paper is to introduce a new model of human eye movements that directly ties eye movements to the ongoing demands of behavior. The basic idea is that eye movements serve to reduce uncertainty about environmental variables that are task relevant. A value is assigned to an eye movement by estimating the expected cost of the uncertainty that will result if the movement is not made. If there are several candidate eye movements, the one with the highest expected value is chosen. The model is illustrated using a humanoid graphic figure that navigates on a sidewalk in a virtual urban environment. Simulations show our protocol is superior to a simple round robin scheduling mechanism.},
 author = {Sprague, Nathan and Ballard, Dana},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/da4902cb0bc38210839714ebdcf0efc3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/da4902cb0bc38210839714ebdcf0efc3-Metadata.json},
 openalex = {W2155844728},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/da4902cb0bc38210839714ebdcf0efc3-Paper.pdf},
 publisher = {MIT Press},
 title = {Eye Movements for Reward Maximization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/da4902cb0bc38210839714ebdcf0efc3-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_db60b95d,
 abstract = {When we model a higher order functions, such as learning and memory, we face a difficulty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle filter for estimating internal parameters and meta-parameters of a reinforcement learning model. We verified the effectiveness of the method using both artificial data and real animal behavioral data.},
 author = {Samejima, Kazuyuki and Doya, Kenji and Ueda, Yasumasa and Kimura, Minoru},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/db60b95decdeed944b4cd8685417cfdc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/db60b95decdeed944b4cd8685417cfdc-Metadata.json},
 openalex = {W2141597721},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/db60b95decdeed944b4cd8685417cfdc-Paper.pdf},
 publisher = {MIT Press},
 title = {Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/db60b95decdeed944b4cd8685417cfdc-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_dcf6070a,
 abstract = {We propose an unsupervised methodology using independent component analysis (ICA) to cluster genes from DNA microarray data. Based on an ICA mixture model of genomic expression patterns, linear and nonlinear ICA finds components that are specific to certain biological processes. Genes that exhibit significant up-regulation or down-regulation within each component are grouped into clusters. We test the statistical significance of enrichment of gene annotations within each cluster. ICA-based clustering outperformed other leading methods in constructing functionally coherent clusters on various datasets. This result supports our model of genomic expression data as composite effect of independent biological processes. Comparison of clustering performance among various ICA algorithms including a kernel-based nonlinear ICA algorithm shows that nonlinear ICA performed the best for small datasets and natural-gradient maximization-likelihood worked well for all the datasets.},
 author = {Lee, Su-in and Batzoglou, Serafim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/dcf6070a4ab7f3afbfd2809173e0824b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/dcf6070a4ab7f3afbfd2809173e0824b-Metadata.json},
 openalex = {W2131945985},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/dcf6070a4ab7f3afbfd2809173e0824b-Paper.pdf},
 publisher = {MIT Press},
 title = {ICA-based Clustering of Genes from Microarray Expression Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/dcf6070a4ab7f3afbfd2809173e0824b-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_ddeebdee,
 abstract = {We present an analysis of concentration-of-expectation phenomena in layered Bayesian networks that use generalized linear models as the local conditional probabilities. This framework encompasses a wide variety of probability distributions, including both discrete and continuous random variables. We utilize ideas from large deviation analysis and the delta method to devise and evaluate a class of approximate inference algorithms for layered Bayesian networks that have superior asymptotic error bounds and very fast computation time.},
 author = {Nguyen, XuanLong and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Metadata.json},
 openalex = {W2101813487},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Concentration of Expectation and Approximate Inference in Layered Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_de7092ba,
 abstract = {We propose an information-theoretic clustering approach that incorporates a pre-known partition of the data, aiming to identify common clusters that cut across the given partition. In the standard clustering setting the formation of clusters is guided by a single source of feature information. The newly utilized pre-partition factor introduces an additional bias that counterbalances the impact of the features whenever they become correlated with this known partition. The resulting algorithmic framework was applied successfully to synthetic data, as well as to identifying text-based cross-religion correspondences.},
 author = {Marx, Zvika and Dagan, Ido and Shamir, Eli},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/de7092ba6df4276921d27a3704c57998-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/de7092ba6df4276921d27a3704c57998-Metadata.json},
 openalex = {W2155922094},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/de7092ba6df4276921d27a3704c57998-Paper.pdf},
 publisher = {MIT Press},
 title = {Identifying Structure across Pre-partitioned Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/de7092ba6df4276921d27a3704c57998-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_e139c454,
 abstract = {Eigenvoice speaker adaptation has been shown to be effective when only a small amount of adaptation data is available. At the heart of the method is principal component analysis (PCA) employed to find the most important eigenvoices. In this paper, we postulate that nonlinear PCA, in particular kernel PCA, may be even more effective. One major challenge is to map the feature-space eigenvoices back to the observation space so that the state observation likelihoods can be computed during the estimation of eigenvoice weights and subsequent decoding. Our solution is to compute kernel PCA using composite kernels, and we will call our new method kernel eigenvoice speaker adaptation. On the TIDIGITS corpus, we found that compared with a speaker-independent model, our kernel eigenvoice adaptation method can reduce the word error rate by 28–33% while the standard eigenvoice approach can only match the performance of the speaker-independent model.},
 author = {Kwok, James and Mak, Brian and Ho, Simon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e139c454239bfde741e893edb46a06cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e139c454239bfde741e893edb46a06cc-Metadata.json},
 openalex = {W2120707558},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e139c454239bfde741e893edb46a06cc-Paper.pdf},
 publisher = {MIT Press},
 title = {Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/e139c454239bfde741e893edb46a06cc-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_e3ca0449,
 abstract = {Synapses are a critical element of biologically-realistic, spike-based neural computation, serving the role of communication, computation, and modification. Many different circuit implementations of synapse function exist with different computational goals in mind. In this paper we describe a new CMOS synapse design that separately controls quiescent leak current, synaptic gain, and time-constant of decay. This circuit implements part of a commonly-used kinetic model of synaptic conductance. We show a theoretical analysis and experimental data for prototypes fabricated in a commercially-available 1.5µm CMOS process.},
 author = {Shi, Z. and Horiuchi, Timothy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e3ca0449fa2ea7701a7ac53fb719c51a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e3ca0449fa2ea7701a7ac53fb719c51a-Metadata.json},
 openalex = {W2136811659},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e3ca0449fa2ea7701a7ac53fb719c51a-Paper.pdf},
 publisher = {MIT Press},
 title = {A Summating, Exponentially-Decaying CMOS Synapse for Spiking Neural Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/e3ca0449fa2ea7701a7ac53fb719c51a-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_e71e5cd1,
 abstract = {Recent multi-agent extensions of Q-Learning require knowledge of other agents' payoffs and Q-functions, and assume game-theoretic play at all times by all other agents. This paper proposes a fundamentally different approach, dubbed Learning, in which values of mixed strategies rather than base actions are learned, and in which other agents' strategies are estimated from observed actions via Bayesian inference. Hyper-Q may be effective against many different types of adaptive agents, even if they are persistently dynamic. Against certain broad categories of adaptation, it is argued that Hyper-Q may converge to exact optimal time-varying policies. In tests using Rock-Paper-Scissors, Hyper-Q learns to significantly exploit an Infinitesimal Gradient Ascent (IGA) player, as well as a Policy Hill Climber (PHC) player. Preliminary analysis of Hyper-Q against itself is also presented.},
 author = {Tesauro, Gerald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e71e5cd119bbc5797164fb0cd7fd94a4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e71e5cd119bbc5797164fb0cd7fd94a4-Metadata.json},
 openalex = {W2097498347},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e71e5cd119bbc5797164fb0cd7fd94a4-Paper.pdf},
 publisher = {MIT Press},
 title = {Extending Q-Learning to General Adaptive Multi-Agent Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/e71e5cd119bbc5797164fb0cd7fd94a4-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_e82c4b19,
 abstract = {Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms (in particular, their proposed algorithm). In...},
 author = {Bengio, Yoshua and Grandvalet, Yves},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e82c4b19b8151ddc25d4d93baf7b908f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e82c4b19b8151ddc25d4d93baf7b908f-Metadata.json},
 openalex = {W2998678080},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e82c4b19b8151ddc25d4d93baf7b908f-Paper.pdf},
 publisher = {MIT Press},
 title = {No Unbiased Estimator of the Variance of K-Fold Cross-Validation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/e82c4b19b8151ddc25d4d93baf7b908f-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_e92d74cc,
 abstract = {In this article, we present a novel approach to solving the localization problem in cellular networks. The goal is to estimate a mobile user's position, based on measurements of the signal strengths received from network base stations. Our solution works by building Gaussian process models for the distribution of signal strengths, as obtained in a series of calibration measurements. In the localization stage, the user's position can be estimated by maximizing the likelihood of received signal strengths with respect to the position. We investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network.},
 author = {Schwaighofer, Anton and Grigoras, Marian and Tresp, Volker and Hoffmann, Clemens},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e92d74ccacdc984afa0c517ad0d557a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e92d74ccacdc984afa0c517ad0d557a6-Metadata.json},
 openalex = {W2167387227},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e92d74ccacdc984afa0c517ad0d557a6-Paper.pdf},
 publisher = {MIT Press},
 title = {GPPS: A Gaussian Process Positioning System for Cellular Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/e92d74ccacdc984afa0c517ad0d557a6-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_e9412ee5,
 abstract = {Existing source location and recovery algorithms used in magnetoen-cephalographic imaging generally assume that the source activity at different brain locations is independent or that the correlation structure is known. However, electrophysiological recordings of local field potentials show strong correlations in aggregate activity over significant distances. Indeed, it seems very likely that stimulus-evoked activity would follow strongly correlated time-courses in different brain areas. Here, we present, and validate through simulations, a new approach to source reconstruction in which the correlation between sources is modelled and estimated explicitly by variational Bayesian methods, facilitating accurate recovery of source locations and the time-courses of their activation.},
 author = {Sahani, Maneesh and Nagarajan, Srikantan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e9412ee564384b987d086df32d4ce6b7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e9412ee564384b987d086df32d4ce6b7-Metadata.json},
 openalex = {W2102707976},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/e9412ee564384b987d086df32d4ce6b7-Paper.pdf},
 publisher = {MIT Press},
 title = {Reconstructing MEG Sources with Unknown Correlations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/e9412ee564384b987d086df32d4ce6b7-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_ea159dc9,
 abstract = {Brain-Computer Interfaces (BCI) are an interesting emerging technology that is driven by the motivation to develop an effective communication interface translating human intentions into a control signal for devices like computers or neuroprostheses. If this can be done bypassing the usual human output pathways like peripheral nerves and muscles it can ultimately become a valuable tool for paralyzed patients. Most activity in BCI research is devoted to finding suitable features and algorithms to increase information transfer rates (ITRs). The present paper studies the implications of using more classes, e.g., left vs. right hand vs. foot, for operating a BCI. We contribute by (1) a theoretical study showing under some mild assumptions that it is practically not useful to employ more than three or four classes, (2) two extensions of the common spatial pattern (CSP) algorithm, one interestingly based on simultaneous diagonalization, and (3) controlled EEG experiments that underline our theoretical findings and show excellent improved ITRs.},
 author = {Dornhege, Guido and Blankertz, Benjamin and Curio, Gabriel and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ea159dc9788ffac311592613b7f71fbb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ea159dc9788ffac311592613b7f71fbb-Metadata.json},
 openalex = {W2144917736},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ea159dc9788ffac311592613b7f71fbb-Paper.pdf},
 publisher = {MIT Press},
 title = {Increase Information Transfer Rates in BCI by CSP Extension to Multi-class},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/ea159dc9788ffac311592613b7f71fbb-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_ea204361,
 author = {Gruber, Aaron and Dayan, Peter and Gutkin, Boris and Solla, Sara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ea204361fe7f024b130143eb3e189a18-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ea204361fe7f024b130143eb3e189a18-Metadata.json},
 openalex = {W2152442860},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ea204361fe7f024b130143eb3e189a18-Paper.pdf},
 publisher = {MIT Press},
 title = {Dopamine Modulation in a Basal Ganglio-Cortical Network of Working Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/ea204361fe7f024b130143eb3e189a18-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_ea4eb493,
 abstract = {Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efficiently.},
 author = {Theocharous, Georgios and Kaelbling, Leslie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ea4eb49329550caaa1d2044105223721-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ea4eb49329550caaa1d2044105223721-Metadata.json},
 openalex = {W2115502112},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ea4eb49329550caaa1d2044105223721-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximate Planning in POMDPs with Macro-Actions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/ea4eb49329550caaa1d2044105223721-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_ee8fe909,
 author = {Likhachev, Maxim and Gordon, Geoffrey J and Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ee8fe9093fbbb687bef15a38facc44d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ee8fe9093fbbb687bef15a38facc44d2-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/ee8fe9093fbbb687bef15a38facc44d2-Paper.pdf},
 publisher = {MIT Press},
 title = {ARA\ast : Anytime A\ast with Provable Bounds on Sub-Optimality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/ee8fe9093fbbb687bef15a38facc44d2-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_f55cadb9,
 abstract = {This paper presents a novel graph-theoretic approach, named ratio contour, to extract perceptually salient boundaries from a set of noisy boundary fragments detected in real images. The boundary saliency is defined using the Gestalt laws of closure, proximity, and continuity. This paper first constructs an undirected graph with two different sets of edges: solid edges and dashed edges. The weights of solid and dashed edges measure the local saliency in and between boundary fragments, respectively. Then the most salient boundary is detected by searching for an optimal cycle in this graph with minimum average weight. The proposed approach guarantees the global optimality without introducing any biases related to region area or boundary length. We collect a variety of images for testing the proposed approach with encouraging results.},
 author = {Wang, Song and Kubota, Toshiro and Siskind, Jeffrey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f55cadb97eaff2ba1980e001b0bd9842-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f55cadb97eaff2ba1980e001b0bd9842-Metadata.json},
 openalex = {W2154587806},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f55cadb97eaff2ba1980e001b0bd9842-Paper.pdf},
 publisher = {MIT Press},
 title = {Salient Boundary Detection using Ratio Contour},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/f55cadb97eaff2ba1980e001b0bd9842-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_f708f064,
 abstract = {We present a modified version of the perceptron learning algorithm (PLA) which solves semidefinite programs (SDPs) in polynomial time. The algorithm is based on the following three observations: (i) Semidefinite programs are linear programs with infinitely many (linear) constraints; (ii) every linear program can be solved by a sequence of constraint satisfaction problems with linear constraints; (iii) in general, the perceptron learning algorithm solves a constraint satisfaction problem with linear constraints in finitely many updates. Combining the PLA with a probabilistic rescaling algorithm (which, on average, increases the size of the feasable region) results in a probabilistic algorithm for solving SDPs that runs in polynomial time. We present preliminary results which demonstrate that the algorithm works, but is not competitive with state-of-the-art interior point methods.},
 author = {Graepel, Thore and Herbrich, Ralf and Kharechko, Andriy and Shawe-taylor, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f708f064faaf32a43e4d3c784e6af9ea-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f708f064faaf32a43e4d3c784e6af9ea-Metadata.json},
 openalex = {W2121219281},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf},
 publisher = {MIT Press},
 title = {Semi-Definite Programming by Perceptron Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/f708f064faaf32a43e4d3c784e6af9ea-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_f7696a9b,
 abstract = {We describe a procedure which finds a hierarchical clustering by hill-climbing. The cost function we use is a hierarchical extension of the k-means cost; our local moves are tree restructurings and node reorderings. We show these can be accomplished efficiently, by exploiting special properties of squared Euclidean distances and by using techniques from scheduling algorithms.},
 author = {Kauchak, David and Dasgupta, Sanjoy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f7696a9b362ac5a51c3dc8f098b73923-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f7696a9b362ac5a51c3dc8f098b73923-Metadata.json},
 openalex = {W2124447006},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f7696a9b362ac5a51c3dc8f098b73923-Paper.pdf},
 publisher = {MIT Press},
 title = {An Iterative Improvement Procedure for Hierarchical Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/f7696a9b362ac5a51c3dc8f098b73923-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_f7ac67a9,
 abstract = {This paper devises a novel kernel function for structured natural language data. In the field of Natural Language Processing, feature extraction consists of the following two steps: (1) syntactically and semantically analyzing raw data, i.e., character strings, then representing the results as discrete structures, such as parse trees and dependency graphs with part-of-speech tags; (2) creating (possibly high-dimensional) numerical feature vectors from the discrete structures. The new kernels, called Hierarchical Directed Acyclic Graph (HDAG) kernels, directly accept DAGs whose nodes can contain DAGs. HDAG data structures are needed to fully reflect the syntactic and semantic structures that natural language data inherently have. In this paper, we define the kernel function and show how it permits efficient calculation. Experiments demonstrate that the proposed kernels are superior to existing kernel functions, e.g., sequence kernels, tree kernels, and bag-of-words kernels.},
 author = {Suzuki, Jun and Sasaki, Yutaka and Maeda, Eisaku},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f7ac67a9aa8d255282de7d11391e1b69-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f7ac67a9aa8d255282de7d11391e1b69-Metadata.json},
 openalex = {W2154315940},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f7ac67a9aa8d255282de7d11391e1b69-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernels for Structured Natural Language Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/f7ac67a9aa8d255282de7d11391e1b69-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_f7e2b2b7,
 abstract = {This paper reports on a family of computationally practical classifiers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classifiers are based on dyadic classification trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) fitting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classification rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n-1/2, the parametric rate. We are not aware of any other practical classifiers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed.},
 author = {Scott, Clayton and Nowak, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f7e2b2b75b04175610e5a00c1e221ebb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f7e2b2b75b04175610e5a00c1e221ebb-Metadata.json},
 openalex = {W2154408756},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f7e2b2b75b04175610e5a00c1e221ebb-Paper.pdf},
 publisher = {MIT Press},
 title = {Near-Minimax Optimal Classification with Dyadic Classification Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/f7e2b2b75b04175610e5a00c1e221ebb-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_f8363057,
 abstract = {One current explanation of the view independent representation of space by the place-cells of the hippocampus is that they arise out of the summation of view dependent Gaussians. This proposal assumes that visual representations show bounded invariance. Here we investigate whether a recently proposed visual encoding scheme called the temporal population code can provide such representations. Our analysis is based on the behavior of a simulated robot in a virtual environment containing specific visual cues. Our results show that the temporal population code provides a representational substrate that can naturally account for the formation of place fields.},
 author = {Wyss, Reto and Verschure, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f83630579d055dc5843ae693e7cdafe0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f83630579d055dc5843ae693e7cdafe0-Metadata.json},
 openalex = {W2166178051},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f83630579d055dc5843ae693e7cdafe0-Paper.pdf},
 publisher = {MIT Press},
 title = {Bounded Invariance and the Formation of Place Fields},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/f83630579d055dc5843ae693e7cdafe0-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_f8b932c7,
 abstract = {Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are sometimes used to overcome these difficulties. We analyze three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. Learning speed is defined as the rate of exponential decay in the learning curves. When the scalar parameter that controls the size of weight updates is chosen to maximize learning speed, node perturbation is slower than direct gradient descent by a factor equal to the number of output units; weight perturbation is slower still by an additional factor equal to the number of input units. Parallel perturbation allows faster learning than sequential perturbation, by a factor that does not depend on network size. We also characterize how uncertainty in quantities used in the stochastic updates affects the learning curves. This study suggests that in practice, weight perturbation may be slow for large networks, and node perturbation can have performance comparable to that of direct gradient descent when there are few output units. However, these statements depend on the specifics of the learning problem, such as the input distribution and the target function, and are not universally applicable.},
 author = {Werfel, Justin and Xie, Xiaohui and Seung, H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f8b932c70d0b2e6bf071729a4fa68dfc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f8b932c70d0b2e6bf071729a4fa68dfc-Metadata.json},
 openalex = {W2162160137},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/f8b932c70d0b2e6bf071729a4fa68dfc-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/f8b932c70d0b2e6bf071729a4fa68dfc-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_fc79250f,
 abstract = {In this paper we obtain convergence bounds for the concentration of Bayesian posterior distributions (around the true distribution) using a novel method that simplifies and enhances previous results. Based on the analysis, we also introduce a generalized family of Bayesian posteriors, and show that the convergence behavior of these generalized posteriors is completely determined by the local prior structure around the true distribution. This important and surprising robustness property does not hold for the standard Bayesian posterior in that it may not concentrate when there exist bad prior structures even at places far away from the true distribution.},
 author = {Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/fc79250f8c5b804390e8da280b4cf06e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/fc79250f8c5b804390e8da280b4cf06e-Metadata.json},
 openalex = {W2131935101},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/fc79250f8c5b804390e8da280b4cf06e-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Bounds for a Generalized Family of Bayesian Posterior Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/fc79250f8c5b804390e8da280b4cf06e-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_fec87a37,
 abstract = {Psychophysical studies suggest the existence of specialized detectors for component motion patterns (radial, circular, and spiral), that are consistent with the visual motion properties of cells in the dorsal medial superior temporal area (MSTd) of non-human primates. Here we use a biologically constrained model of visual motion processing in MSTd, in conjunction with psychophysical performance on two motion pattern tasks, to elucidate the computational mechanisms associated with the processing of wide-field motion patterns encountered during self-motion. In both tasks discrimination thresholds varied significantly with the type of motion pattern presented, suggesting perceptual correlates to the preferred motion bias reported in MSTd. Through the model we demonstrate that while independently responding motion pattern units are capable of encoding information relevant to the visual motion tasks, equivalent psychophysical performance can only be achieved using interconnected neural populations that systematically inhibit non-responsive units. These results suggest the cyclic trends in psychophysical performance may be mediated, in part, by recurrent connections within motion pattern responsive areas whose structure is a function of the similarity in preferred motion patterns and receptive field locations between units.},
 author = {Beardsley, Scott and Vaina, Lucia},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/fec87a37cdeec1c6ecf8181c0aa2d3bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/fec87a37cdeec1c6ecf8181c0aa2d3bf-Metadata.json},
 openalex = {W2104415113},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/fec87a37cdeec1c6ecf8181c0aa2d3bf-Paper.pdf},
 publisher = {MIT Press},
 title = {A Functional Architecture for Motion Pattern Processing in MSTd},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/fec87a37cdeec1c6ecf8181c0aa2d3bf-Abstract.html},
 volume = {16},
 year = {2003}
}

@inproceedings{NIPS2003_feecee9f,
 abstract = {We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts. On the basis of the performance bounds we provide the optimal a priori discretization for learning the parameter that governs the switching dynamics. We demonstrate the new algorithm in the context of wireless networks.},
 author = {Monteleoni, Claire and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2003/file/feecee9f1643651799ede2740927317a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2003/file/feecee9f1643651799ede2740927317a-Metadata.json},
 openalex = {W2120705090},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2003/file/feecee9f1643651799ede2740927317a-Paper.pdf},
 publisher = {MIT Press},
 title = {Online Learning of Non-stationary Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/hash/feecee9f1643651799ede2740927317a-Abstract.html},
 volume = {16},
 year = {2003}
}
