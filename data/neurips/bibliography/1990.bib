@inproceedings{NIPS1990_00411460,
 abstract = {Multi-layered neural networks have recently been proposed for nonlinear prediction and system modeling. Although proven successful for modeling time invariant nonlinear systems, the inability of neural networks to characterize temporal variability has so far been an obstacle in applying them to complicated non stationary signals, such as speech. In this paper we present a network architecture, called Hidden Control Neural Network (HCNN), for modeling signals generated by nonlinear dynamical systems with restricted time variability. The approach taken here is to allow the mapping that is implemented by a multi layered neural network to change with time as a function of an additional control input signal. This network is trained using an algorithm that is based on back-propagation and segmentation algorithms for estimating the unknown control together with the network's parameters. The HCNN approach was applied to several tasks including modeling of time-varying nonlinear systems and speaker-independent recognition of connected digits, yielding a word accuracy of 99.1%.},
 author = {Levin, Esther},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/00411460f7c92d2124a67ea0f4cb5f85-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/00411460f7c92d2124a67ea0f4cb5f85-Metadata.json},
 openalex = {W2156101286},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Modeling Time Varying Systems Using Hidden Control Neural Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_00ec53c4,
 abstract = {Genetic algorithms were used to select and create features and to select reference exemplar patterns for machine vision and speech pattern classification tasks. For a complex speech recognition task, genetic algorithms required no more computation time than traditional approaches to feature selection but reduced the number of input features required by a factor of five (from 153 to 33 features). On a difficult artificial machine-vision task, genetic algorithms were able to create new features (polynomial functions of the original features) which reduced classification error rates from 19% to almost 0%. Neural net and k nearest neighbor (KNN) classifiers were unable to provide such low error rates using only the original features. Genetic algorithms were also used to reduce the number of reference exemplar patterns for a KNN classifier. On a 338 training pattern vowel-recognition problem with 10 classes, genetic algorithms reduced the number of stored exemplars from 338 to 43 without significantly increasing classification error rate. In all applications, genetic algorithms were easy to apply and found good solutions in many fewer trials than would be required by exhaustive search. Run times were long, but not unreasonable. These results suggest that genetic algorithms are becoming practical for pattern classification problems as faster serial and parallel computers are developed.},
 author = {Chang, Eric and Lippmann, Richard P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Metadata.json},
 openalex = {W2129861716},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Using Genetic Algorithms to Improve Pattern Classification Performance},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/00ec53c4682d36f5c4359f4ae7bd7ba1-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_01386bd6,
 abstract = {For a simple linear case, a mathematical analysis of the training and generalization (validation) performance of networks trained by gradient descent on a Least Mean Square cost function is provided as a function of the learning parameters and of the statistics of the training data base. The analysis predicts that generalization error dynamics are very dependent on a priori initial weights. In particular, the generalization error might sometimes weave within a computable range during extended training. In some cases, the analysis provides bounds on the optimal number of training cycles for minimal validation error. For a speech labeling task, predicted weaving effects were qualitatively tested and observed by computer simulations in networks trained by the linear and non-linear back-propagation algorithm.},
 author = {Chauvin, Yves},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/01386bd6d8e091c2ab4c7c7de644d37b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/01386bd6d8e091c2ab4c7c7de644d37b-Metadata.json},
 openalex = {W2124539400},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generalization Dynamics in LMS Trained Linear Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_019d385e,
 abstract = {The dark-adapted visual system can count photons with a reliability limited by thermal noise in the rod photoreceptors — the processing circuitry between the rod cells and the brain is essentially noiseless and in fact may be close to optimal Here we design an optimal signal processor which estimates the time-varying light intensity at the retina based on the rod signals. We show that the first stage of optimal signal processing involves passing the rod cell output through a linear filter with characteristics determined entirely by the rod signal and noise spectra. This filter is very general; in fact it is the first stage in any visual signal processing task at low photon flux. We identify the output of this first-stage filter with the intracellular voltage response of the bipolar cell, the first anatomical stage in retinal signal processing. From recent data on tiger salamander photoreceptors we extract the relevant spectra and make parameter-free, quantitative predictions of the bipolar cell response to a dim, diffuse flash. Agreement with experiment is nearly perfect. As far as we know this is the first instance of a predictive theory of neural dynamics.},
 author = {Rieke, Fred and Owen, W. and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/019d385eb67632a7e958e23f24bd07d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/019d385eb67632a7e958e23f24bd07d7-Metadata.json},
 openalex = {W2115636295},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Filtering in the Salamander Retina},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/019d385eb67632a7e958e23f24bd07d7-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_04025959,
 abstract = {This paper studies dynamical aspects of neural systems with delayed negative feedback modelled by nonlinear delay-differential equations. These systems undergo a Hopf bifurcation from a stable fixed point to a stable limit cycle oscillation as certain parameters are varied. It is shown that their frequency of oscillation is robust to parameter variations and noisy fluctuations, a property that makes these systems good candidates for pacemakers. The onset of oscillation is postponed by both additive and parametric noise in the sense that the state variable spends more time near the fixed point than it would in the absence of noise. This is also the case when noise affects the delayed variable, i.e. when the system has a faulty memory. Finally, it is shown that a distribution of delays (rather than a fixed delay) also stabilizes the fixed point solution.},
 author = {Longtin, Andr\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/04025959b191f8f9de3f924f0940515f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/04025959b191f8f9de3f924f0940515f-Metadata.json},
 openalex = {W2150477930},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/04025959b191f8f9de3f924f0940515f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Oscillation onset in neural delayed feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/04025959b191f8f9de3f924f0940515f-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_05049e90,
 abstract = {Feedback connections are required so that the teacher signal on the output neurons can modify weights during supervised learning. Relaxation methods are needed for learning static patterns with full-time feedback connections. Feedback network learning techniques have not achieved wide popularity because of the still greater computational efficiency of back-propagation. We show by simulation that relaxation networks of the kind we are implementing in VLSI are capable of learning large problems just like back-propagation networks. A microchip incorporates deterministic mean-field theory learning as well as stochastic Boltzmann learning. A multiple-chip electronic system implementing these networks will make high-speed parallel learning in them feasible in the future.},
 author = {Alspector, Joshua and Allen, Robert and Jayakumar, Anthony and Zeppenfeld, Torsten and Meir, Ronny},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/05049e90fa4f5039a8cadc6acbb4b2cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/05049e90fa4f5039a8cadc6acbb4b2cc-Metadata.json},
 openalex = {W2161064497},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/05049e90fa4f5039a8cadc6acbb4b2cc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Relaxation Networks for Large Supervised Learning Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/05049e90fa4f5039a8cadc6acbb4b2cc-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_0584ce56,
 abstract = {We present a generic neural network architecture capable of controlling non-linear plants. The network is composed of dynamic, parallel, linear maps gated by non-linear switches. Using a recurrent form of the back-propagation algorithm, control is achieved by optimizing the control gains and task-adapted switch parameters. A mean quadratic cost function computed across a nominal plant trajectory is minimized along with performance constraint penalties. The approach is demonstrated for a control task consisting of landing a commercial aircraft in difficult wind conditions. We show that the network yields excellent performance while remaining within acceptable damping response constraints.},
 author = {Schley, Charles and Chauvin, Yves and Henkle, Van and Golden, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0584ce565c824b7b7f50282d9a19945b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0584ce565c824b7b7f50282d9a19945b-Metadata.json},
 openalex = {W2099512582},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0584ce565c824b7b7f50282d9a19945b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Networks Structured for Control Application to Aircraft Landing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/0584ce565c824b7b7f50282d9a19945b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_06eb61b8,
 abstract = {We describe a closed-form technique for mapping the output of a trained backpropagation network into input activity space. The mapping is an inverse mapping in the sense that, when the image of the mapping in input activity space is propagated forward through the normal network dynamics, it reproduces the output used to generate that image. When more than one such inverse mappings exist, our inverse mapping is special in that it has no projection onto the nullspace of the activation flow operator for the entire network. An important by-product of our calculation, when more than one inverse mappings exist, is an orthogonal basis set of a significant portion of the activation flow operator nullspace. This basis set can be used to obtain an alternate inverse mapping that is optimized for a particular real-world application.},
 author = {Rossen, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/06eb61b839a0cefee4967c67ccb099dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/06eb61b839a0cefee4967c67ccb099dc-Metadata.json},
 openalex = {W2158533106},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/06eb61b839a0cefee4967c67ccb099dc-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Closed-Form Inversion of Backpropagation Networks: Theory and Optimization Issues},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/06eb61b839a0cefee4967c67ccb099dc-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_0bb4aec1,
 abstract = {We study the evolution of the generalization ability of a simple linear perceptron with N inputs which learns to imitate a perceptron. The system is trained on p = αN binary example inputs and the generalization ability measured by testing for agreement with the teacher on all 2N possible binary input patterns. The dynamics may be solved analytically and exhibits a phase transition from imperfect to perfect generalization at α = 1. Except at this point the generalization ability approaches its asymptotic value exponentially, with critical slowing down near the transition; the relaxation time is ∞ (1 - √α)-2. Right at the critical point, the approach to perfect generalization follows a power law ∞ t-1/2. In the presence of noise, the generalization ability is degraded by an amount ∞ (√α - 1)-1 just above α = 1.},
 author = {Krogh, Anders and Hertz, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0bb4aec1710521c12ee76289d9440817-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0bb4aec1710521c12ee76289d9440817-Metadata.json},
 openalex = {W2117674238},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dynamics of Generalization in Linear Perceptrons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/0bb4aec1710521c12ee76289d9440817-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_0c74b7f7,
 abstract = {A network was trained by back propagation to map locative expressions of the form noun-preposition-noun to a semantic representation, as in Cosic and Munro (1988). The network's performance was analyzed over several simulations with training sets in both English and German. Translation of prepositions was attempted by presenting a locative expression to a network trained in one language to generate a semantic representation; the semantic representation was then presented to the network trained in the other language to generate the appropriate preposition.},
 author = {Munro, Paul and Tabasko, Mary},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Metadata.json},
 openalex = {W2164597770},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Translating Locative Prepositions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_0d0fd7c6,
 abstract = {A network based on splines is described. It automatically adapts the number of units, unit parameters, and the architecture of the network for each application.},
 author = {Friedman, Jerome},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0d0fd7c6e093f7b804fa0150b875b868-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0d0fd7c6e093f7b804fa0150b875b868-Metadata.json},
 openalex = {W2151843700},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive Spline Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/0d0fd7c6e093f7b804fa0150b875b868-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_0deb1c54,
 abstract = {We have designed and tested a one-dimensional 64 pixel, analog CMOS VLSI chip which localizes intensity edges in real-time. This device exploits on-chip photoreceptors and the natural filtering properties of resistive networks to implement a scheme similar to and motivated by the Difference of Gaussians (DOG) operator proposed by Marr and Hildreth (1980). Our chip computes the zero-crossings associated with the difference of two exponential weighting functions. If the derivative across this zero-crossing is above a threshold, an edge is reported. Simulations indicate that this technique will extend well to two dimensions.},
 author = {Bair, Wyeth and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0deb1c54814305ca9ad266f53bc82511-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0deb1c54814305ca9ad266f53bc82511-Metadata.json},
 openalex = {W2102764687},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Analog VLSI Chip for Finding Edges from Zero-crossings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/0deb1c54814305ca9ad266f53bc82511-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_1068c6e4,
 abstract = {In this paper, after some introductory remarks into the classification problem as considered in various research communities, and some discussions concerning some of the reasons for ascertaining the performances of the three chosen algorithms, viz., CART (Classification and Regression Tree), C4.5 (one of the more recent versions of a popular induction tree technique known as ID3), and a multi-layer perceptron (MLP), it is proposed to compare the performances of these algorithms under two criteria: classification and generalisation. It is found that, in general, the MLP has better classification and generalisation accuracies compared with the other two algorithms.},
 author = {Tsoi, A. C. and Pearson, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1068c6e4c8051cfd4e9ea8072e3189e2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1068c6e4c8051cfd4e9ea8072e3189e2-Metadata.json},
 openalex = {W2147032546},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Comparison of three classification techniques: CART, C4.5 and Multi-Layer Perceptrons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/1068c6e4c8051cfd4e9ea8072e3189e2-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_11b9842e,
 abstract = {The goal has been to construct a supervised artificial neural network that learns incrementally an unknown mapping. As a result a network consisting of a combination of ART2 and backpropagation is proposed and is called an network. The ART2 network is used to build and focus a supervised backpropagation network. The ART2/BP network has the advantage of being able to dynamically expand itself in response to input patterns containing new information. Simulation results show that the ART2/BP network outperforms a classical maximum likelihood method for the estimation of a discrete dynamic and nonlinear transfer function.},
 author = {S\o rheim, Einar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/11b9842e0a271ff252c1903e7132cd68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/11b9842e0a271ff252c1903e7132cd68-Metadata.json},
 openalex = {W2150301521},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/11b9842e0a271ff252c1903e7132cd68-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {ART2/BP architecture for adaptive estimation of dynamic processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/11b9842e0a271ff252c1903e7132cd68-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_138bb069,
 abstract = {We are exploring the significance of biological complexity for neuronal computation. Here we demonstrate that Hebbian synapses in realistically-modeled hippocampal pyramidal cells may give rise to two novel forms of self-organization in response to structured synaptic input. First, on the basis of the electrotonic relationships between synaptic contacts, a cell may become tuned to a small subset of its input space. Second, the same mechanisms may produce clusters of potentiated synapses across the space of the dendrites. The latter type of self-organization may be functionally significant in the presence of nonlinear dendritic conductances.},
 author = {Brown, Thomas and Mainen, Zachary and Zador, Anthony and Claiborne, Brenda},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/138bb0696595b338afbab333c555292a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/138bb0696595b338afbab333c555292a-Metadata.json},
 openalex = {W2097571885},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/138bb0696595b338afbab333c555292a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Self-Organization of Hebbian Synapses on Hippocampal Neurons.},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/138bb0696595b338afbab333c555292a-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_13f9896d,
 abstract = {We show analytically how the stability of two-dimensional lateral inhibition neural networks depends on the local connection topology. For various network topologies, we calculate the critical time delay for the onset of oscillation in continuous-time networks and present analytic phase diagrams characterizing the dynamics of discrete-time networks.},
 author = {Marcus, C.M and Waugh, F. and Westervelt, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/13f9896df61279c928f19721878fac41-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/13f9896df61279c928f19721878fac41-Metadata.json},
 openalex = {W2155052902},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/13f9896df61279c928f19721878fac41-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connection Topology and Dynamics in Lateral Inhibition Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/13f9896df61279c928f19721878fac41-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_142949df,
 abstract = {Inspired by a visual motion detection model for the rabbit retina and by a computational architecture used for early audition in the barn owl, we have designed a chip that employs a correlation model to report the one-dimensional field motion of a scene in real time. Using subthreshold analog VLSI techniques, we have fabricated and successfully tested a 8000 transistor chip using a standard MOSIS process.},
 author = {Horiuchi, Tim and Lazzaro, John and Moore, Andrew and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/142949df56ea8ae0be8b5306971900a4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/142949df56ea8ae0be8b5306971900a4-Metadata.json},
 openalex = {W2099832509},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/142949df56ea8ae0be8b5306971900a4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Delay-Line Based Motion Detection Chip},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/142949df56ea8ae0be8b5306971900a4-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_1543843a,
 abstract = {This paper explores the effect of initial weight selection on feed-forward networks learning simple functions with the back-propagation technique. We first demonstrate, through the use of Monte Carlo techniques, that the magnitude of the initial condition vector (in weight space) is a very significant parameter in convergence time variability. In order to further understand this result, additional deterministic experiments were performed. The results of these experiments demonstrate the extreme sensitivity of back propagation to initial weight configuration.},
 author = {Kolen, John and Pollack, Jordan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1543843a4723ed2ab08e18053ae6dc5b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1543843a4723ed2ab08e18053ae6dc5b-Metadata.json},
 openalex = {W2148520070},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Back Propagation is Sensitive to Initial Conditions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_158f3069,
 abstract = {Although color TV is an established technology, there are a number of longstanding problems for which neural networks may be suited. Impulse noise is such a problem, and a modular neural network approach is presented in this paper. The training and analysis was done on conventional computers, while real-time simulations were performed on a massively parallel computer called the Princeton Engine. The network approach was compared to a conventional alternative, a median filter. Real-time simulations and quantitative analysis demonstrated the technical superiority of the neural system. Ongoing work is investigating the complexity and cost of implementing this system in hardware.},
 author = {Pearson, John and Spence, Clay D. and Sverdlove, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/158f3069a435b314a80bdcb024f8e422-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/158f3069a435b314a80bdcb024f8e422-Metadata.json},
 openalex = {W2134190269},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/158f3069a435b314a80bdcb024f8e422-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Applications of Neural Networks in Video Signal Processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/158f3069a435b314a80bdcb024f8e422-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_1651cf0d,
 abstract = {We describe in this paper a network that performs grouping of image contours. The input to the net are fragments of image contours, and the output is the partitioning of the fragments into groups, together with a saliency measure for each group. The grouping is based on a measure of overall length and curvature. The network decomposes the overall optimization problem into independent optimal pairing problems performed at each node. The resulting computation maps into a uniform locally connected network of simple computing elements.},
 author = {Shashua, Amnon and Ullman, Shimon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1651cf0d2f737d7adeab84d339dbabd3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1651cf0d2f737d7adeab84d339dbabd3-Metadata.json},
 openalex = {W2145800791},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/1651cf0d2f737d7adeab84d339dbabd3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Grouping Contours by Iterated Pairing Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/1651cf0d2f737d7adeab84d339dbabd3-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_17d63b16,
 abstract = {The neocognitron is a neural network for pattern recognition and feature extraction. An analog CCD parallel processing architecture developed at Lincoln Laboratory is particularly well suited to the computational requirements of shared-weight networks such as the neocognitron, and implementation of the neocognitron using the CCD architecture was simulated. A modification to the neocognitron training procedure, which improves network performance under the limited arithmetic precision that would be imposed by the CCD architecture, is presented.},
 author = {Chuang, Michael and Chiang, Alice},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/17d63b1625c816c22647a73e1482372b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/17d63b1625c816c22647a73e1482372b-Metadata.json},
 openalex = {W2130727723},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/17d63b1625c816c22647a73e1482372b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Simulation of the Neocognitron on a CCD Parallel Processing Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/17d63b1625c816c22647a73e1482372b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_18997733,
 abstract = {We have devised a scheme to reduce the complexity of dynamical systems belonging to a class that includes most biophysically realistic neural models. The reduction is based on transformations of variables and perturbation expansions and it preserves a high level of fidelity to the original system. The techniques are illustrated by reductions of the Hodgkin-Huxley system and an augmented Hodgkin-Huxley system.},
 author = {Kepler, Thomas and Abbott, L. and Marder, Eve},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/18997733ec258a9fcaf239cc55d53363-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/18997733ec258a9fcaf239cc55d53363-Metadata.json},
 openalex = {W2125913197},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Order Reduction for Dynamical Systems Describing the Behavior of Complex Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/18997733ec258a9fcaf239cc55d53363-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_18d80423,
 abstract = {We present and compare learning rate schedules for stochastic gradient descent, a general algorithm which includes LMS, on-line backpropagation and k-means clustering as special cases. We introduce search-then-converge type schedules which outperform the classical constant and running average (1/t) schedules both in speed of convergence and quality of solution.},
 author = {Darken, Christian and Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/18d8042386b79e2c279fd162df0205c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/18d8042386b79e2c279fd162df0205c8-Metadata.json},
 openalex = {W2155677642},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/18d8042386b79e2c279fd162df0205c8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Note on Learning Rate Schedules for Stochastic Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/18d8042386b79e2c279fd162df0205c8-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_2421fcb1,
 abstract = {We consider different types of single-hidden-layer feedforward nets: with or without direct input to output connections, and using either threshold or sigmoidal activation functions. The main results show that direct connections in threshold nets double the recognition but not the interpolation power, while using sigmoids rather than thresholds allows (at least) doubling both. Various results are also given on VC dimension and other measures of recognition capabilities.},
 author = {Sontag, Eduardo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/2421fcb1263b9530df88f7f002e78ea5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/2421fcb1263b9530df88f7f002e78ea5-Metadata.json},
 openalex = {W2116594222},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/2421fcb1263b9530df88f7f002e78ea5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Remarks on Interpolation and Recognition Using Neural Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/2421fcb1263b9530df88f7f002e78ea5-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_248e8443,
 abstract = {The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses the problem of training artificial neural networks in real time to perform difficult perception tasks. ALVINN is a back-propagation network that uses inputs from a video camera and an imaging laser rangefinder to drive the CMU Navlab, a modified Chevy van. This paper describes training techniques which allow ALVINN to learn in under 5 minutes to autonomously control the Navlab by watching a human driver's response to new situations. Using these techniques, ALVINN has been trained to drive in a variety of circumstances including single-lane paved and unpaved roads, multilane lined and unlined roads, and obstacle-ridden on- and off-road environments, at speeds of up to 20 miles per hour.},
 author = {Pomerleau, Dean},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/248e844336797ec98478f85e7626de4a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/248e844336797ec98478f85e7626de4a-Metadata.json},
 openalex = {W2117109997},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/248e844336797ec98478f85e7626de4a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Rapidly Adapting Artificial Neural Networks for Autonomous Navigation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/248e844336797ec98478f85e7626de4a-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_24b16fed,
 abstract = {A neural network architecture was designed for locating word boundaries and identifying words from phoneme sequences. This architecture was tested in three sets of studies. First, a highly redundant corpus with a restricted vocabulary was generated and the network was trained with a limited number of phonemic variations for the words in the corpus. Tests of network performance on a transfer set yielded a very low error rate. In a second study, a network was trained to identify words from expert transcriptions of speech. On a transfer test, error rate for correct simultaneous identification of words and word boundaries was 18%. The third study used the output of a phoneme classifier as the input to the word and word boundary identification network. The error rate on a transfer test set was 49% for this task. Overall, these studies provide a first step at identifying words in connected discourse with a neural network.},
 author = {Allen, Robert and Kamm, Candace},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/24b16fede9a67c9251d3e7c7161c83ac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/24b16fede9a67c9251d3e7c7161c83ac-Metadata.json},
 openalex = {W2109812833},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/24b16fede9a67c9251d3e7c7161c83ac-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Recurrent Neural Network for Word Identification from Continuous Phoneme Strings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/24b16fede9a67c9251d3e7c7161c83ac-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_25b2822c,
 abstract = {The development of learning algorithms is generally based upon the minimization of an energy function. It is a fundamental requirement to compute the gradient of this energy function with respect to the various parameters of the neural architecture, e.g., synaptic weights, neural gain, etc. In principle, this requires solving a system of nonlinear equations for each parameter of the model, which is computationally very expensive. A new methodology for neural learning of time-dependent nonlinear mappings is presented. It exploits the concept of adjoint operators to enable a fast global computation of the network's response to perturbations in all the systems parameters. The importance of the time boundary conditions of the adjoint functions is discussed. An algorithm is presented in which the adjoint sensitivity equations are solved simultaneously (i.e., forward in time) along with the nonlinear dynamics of the neural networks. This methodology makes real-time applications and hardware implementation of temporal learning feasible.},
 author = {Toomarian, N. and Barhen, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/25b2822c2f5a3230abfadd476e8b04c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/25b2822c2f5a3230abfadd476e8b04c9-Metadata.json},
 openalex = {W2170605924},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/25b2822c2f5a3230abfadd476e8b04c9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adjoint-Functions and Temporal Learning Algorithms in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/25b2822c2f5a3230abfadd476e8b04c9-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_26e359e8,
 abstract = {A higher order recurrent neural network architecture learns to recognize and generate languages after being trained on categorized exemplars. Studying these networks from the perspective of dynamical systems yields two interesting discoveries: First, a longitudinal examination of the learning process illustrates a new form of mechanical inference: Induction by phase transition. A small weight adjustment causes a bifurcation in the limit behavior of the network. This phase transition corresponds to the onset of the network's capacity for generalizing to arbitrary-length strings. Second, a study of the automata resulting from the acquisition of previously published languages indicates that while the architecture is NOT guaranteed to find a minimal finite automata consistent with the given exemplars, which is an NP-Hard problem, the architecture does appear capable of generating nonregular languages by exploiting fractal and chaotic dynamics. I end the paper with a hypothesis relating linguistic generative capacity to the behavioral regimes of non-linear dynamical systems.},
 author = {Pollack, Jordan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/26e359e83860db1d11b6acca57d8ea88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/26e359e83860db1d11b6acca57d8ea88-Metadata.json},
 openalex = {W2146367896},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/26e359e83860db1d11b6acca57d8ea88-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Language Induction by Phase Transition in Dynamical Recognizers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/26e359e83860db1d11b6acca57d8ea88-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_28f0b864,
 abstract = {We present a unified framework for a number of different ways of failing to generalize properly. During learning, sources of random information contaminate the network, effectively augmenting the training data with random information. The complexity of the function computed is therefore increased, and generalization is degraded. We analyze replicated networks, in which a number of identical networks are independently trained on the same data and their results averaged. We conclude that replication almost always results in a decrease in the expected complexity of the network, and that replication therefore increases expected generalization. Simulations confirming the effect are also presented.},
 author = {Pearlmutter, Barak and Rosenfeld, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/28f0b864598a1291557bed248a998d4e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/28f0b864598a1291557bed248a998d4e-Metadata.json},
 openalex = {W2139763837},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/28f0b864598a1291557bed248a998d4e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Chaitin-Kolmogorov Complexity and Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/28f0b864598a1291557bed248a998d4e-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_2f2b2656,
 abstract = {A large number of VLSI implementations of neural network models have been reported. The diversity of these implementations is noteworthy. This paper attempts to put a group of representative VLSI implementations in perspective by comparing and contrasting them. Design trade-offs are discussed and some suggestions for the direction of future implementation efforts are made.},
 author = {Holler, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/2f2b265625d76a6704b08093c652fd79-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/2f2b265625d76a6704b08093c652fd79-Metadata.json},
 openalex = {W2101501703},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {VLSI Implementations of Learning and Memory Systems: A Review},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_310dcbbf,
 abstract = {A neural network model of motion segmentation by visual cortex is described. The model clarifies how preprocessing of motion signals by a Motion Oriented Contrast Filter (MOC Filter) is joined to long-range cooperative motion mechanisms in a motion Cooperative Competitive Loop (CC Loop) to control phenomena such as as induced motion, motion capture, and motion aftereffects. The total model system is a motion Boundary Contour System (BCS) that is computed in parallel with a static BCS before both systems cooperate to generate a boundary representation for three dimensional visual form perception. The present investigations clarify how the static BCS can be modified for use in motion segmentation problems, notably for analyzing how ambiguous local movements (the aperture problem) on a complex moving shape are suppressed and actively reorganized into a coherent global motion signal.},
 author = {Mingolla, Ennio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/310dcbbf4cce62f762a2aaa148d556bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/310dcbbf4cce62f762a2aaa148d556bd-Metadata.json},
 openalex = {W2105799179},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/310dcbbf4cce62f762a2aaa148d556bd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Dynamics of Motion Segmentation and Grouping},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/310dcbbf4cce62f762a2aaa148d556bd-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_32072254,
 abstract = {A novel unsupervised neural network for dimensionality reduction which seeks directions emphasizing multimodality is presented, and its connection to exploratory projection pursuit methods is discussed. This leads to a new statistical insight to the synaptic modification equations governing learning in Bienenstock, Cooper, and Munro (BCM) neurons (1982).

The importance of a dimensionality reduction principle based solely on distinguishing features, is demonstrated using a linguistically motivated phoneme recognition experiment, and compared with feature extraction using back-propagation network.},
 author = {Intrator, Nathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/320722549d1751cf3f247855f937b982-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/320722549d1751cf3f247855f937b982-Metadata.json},
 openalex = {W2152255582},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/320722549d1751cf3f247855f937b982-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Exploratory Feature Extraction in Speech Signals},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/320722549d1751cf3f247855f937b982-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_34ed066d,
 abstract = {While the network loading problem for 2-layer threshold nets is NP-hard when learning from examples alone (as with backpropagation), (Baum, 91) has now proved that a learner can employ queries to evade the hidden unit credit assignment problem and PAC-load nets with up to four hidden units in polynomial time. Empirical tests show that the method can also learn far more complicated functions such as randomly generated networks with 200 hidden units. The algorithm easily approximates Wieland's 2-spirals function using a single layer of 50 hidden units, and requires only 30 minutes of CPU time to learn 200-bit parity to 99.7% accuracy.},
 author = {Baum, Eric and Lang, Kevin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/34ed066df378efacc9b924ec161e7639-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/34ed066df378efacc9b924ec161e7639-Metadata.json},
 openalex = {W2159435100},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/34ed066df378efacc9b924ec161e7639-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Constructing Hidden Units using Examples and Queries},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/34ed066df378efacc9b924ec161e7639-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_352fe25d,
 abstract = {Analog neural networks with feedback can be used to implement K- (Winner-Take-All (KWTA) networks. In turn, KWTA networks can be used as decoders of a class of nonlinear error-correcting codes. By interconnecting such KWTA networks, we can construct decoders capable of decoding more powerful codes. We consider several families of interconnected KWTA networks, analyze their performance in terms of coding theory metrics, and consider the feasibility of embedding such networks in VLSI technologies.},
 author = {Erlanson, Ruth and Abu-Mostafa, Yaser},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/352fe25daf686bdb4edca223c921acea-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/352fe25daf686bdb4edca223c921acea-Metadata.json},
 openalex = {W2144984933},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/352fe25daf686bdb4edca223c921acea-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analog Neural Networks as Decoders},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/352fe25daf686bdb4edca223c921acea-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_357a6fdf,
 abstract = {We present an algorithm based on reinforcement and state recurrence learning techniques to solve control scheduling problems. In particular, we have devised a simple learning scheme called in which the weights of the associative search element are reinforced, either positively or negatively, such that the system is forced to move towards the desired setpoint in the shortest possible trajectory. To improve the learning rate, a variable reinforcement scheme is employed: negative reinforcement values are varied depending on whether the failure occurs in handicapped or normal mode of operation. Furthermore, to realize a simulated annealing scheme for accelerated learning, if the system visits the same failed state successively, the negative reinforcement value is increased. In examples studied, these learning schemes have demonstrated high learning rates, and therefore may prove useful for in-situ learning.},
 author = {Guha, Aloke},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/357a6fdf7642bf815a88822c447d9dc4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/357a6fdf7642bf815a88822c447d9dc4-Metadata.json},
 openalex = {W2161160133},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/357a6fdf7642bf815a88822c447d9dc4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Reinforcement Learning Variant for Control Scheduling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/357a6fdf7642bf815a88822c447d9dc4-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_371bce7d,
 abstract = {The problem of color clustering is defined and shown to be a problem of assigning a large number (hundreds of thousands) of 3-vectors to a small number (256) of clusters. Finding those clusters in such a way that they best represent a full color image using only 256 distinct colors is a burdensome computational problem. In this paper, the problem is solved using classical techniques -- k-means clustering, vector quantization (which turns out to be the same thing in this application), competitive learning, and Kohonen self-organizing feature maps. Quality of the result is judged subjectively by how much the pseudo-color result resembles the true color image, by RMS quantization error, and by run time. The Kohonen map provides the best solution.},
 author = {Snyder, Wesley and Nissman, Daniel and Van den Bout, David and Bilbro, Griff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/371bce7dc83817b7893bcdeed13799b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/371bce7dc83817b7893bcdeed13799b5-Metadata.json},
 openalex = {W2151890348},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Kohonen Networks and Clustering: Comparative Performance in Color Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/371bce7dc83817b7893bcdeed13799b5-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_37bc2f75,
 abstract = {We describe a CMOS neural net chip with a reconfigurable network architecture. It contains 32,768 binary, programmable connections arranged in 256 'building block' neurons. Several 'building blocks' can be connected to form long neurons with up to 1024 binary connections or to form neurons with analog connections. Single- or multi-layer networks can be implemented with this chip. We have integrated this chip into a board system together with a digital signal processor and fast memory. This system is currently in use for image processing applications in which the chip extracts features such as edges and corners from binary and gray-level images.},
 author = {Graf, H. P. and Janow, R. and Henderson, D. and Lee, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/37bc2f75bf1bcfe8450a1a41c200364c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/37bc2f75bf1bcfe8450a1a41c200364c-Metadata.json},
 openalex = {W2116521523},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Reconfigurable Neural Net Chip with 32K Connections},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_39461a19,
 abstract = {We develop a new feedforward neural network representation of Lipschitz functions from [0, ρ]n into [0, 1] based on the level sets of the function. We show that nρL/2er + 1/√2er + (1+n/√2)(ρL/4er)n is an upper bound on the number of nodes needed to represent f to within uniform error er, where L is the Lipschitz constant. We also show that the number of bits needed to represent the weights in the network in order to achieve this approximation is given by O(n2ρL/√2 4ner (ρL/er)n). We compare this bound with the e-entropy of the functional class under consideration.},
 author = {Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/39461a19e9eddfb385ea76b26521ea48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/39461a19e9eddfb385ea76b26521ea48-Metadata.json},
 openalex = {W2127278788},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {e-Entropy and the Complexity of Feedforward Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/39461a19e9eddfb385ea76b26521ea48-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_3ad7c2eb,
 abstract = {Learning an input-output mapping from a set of examples can be regarded as synthesizing an approximation of a multi-dimensional function. From this point of view, this form of learning is closely related to regularization theory, and we have previously shown (Poggio and Girosi, 1990a, 1990b) the equivalence between reglilarization and a class of three-layer networks that we call regularization networks. In this note, we extend the theory by introducing ways of dealing with two aspects of learning: learning in presence of unreliable examples or outliers, and learning from positive and negative examples.},
 author = {Girosi, Federico and Poggio, Tomaso and Caprile, Bruno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3ad7c2ebb96fcba7cda0cf54a2e802f5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3ad7c2ebb96fcba7cda0cf54a2e802f5-Metadata.json},
 openalex = {W2097629057},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3ad7c2ebb96fcba7cda0cf54a2e802f5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Extensions of a Theory of Networks for Approximation and Learning: Outliers and Negative Examples},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/3ad7c2ebb96fcba7cda0cf54a2e802f5-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_3c7781a3,
 abstract = {The main point of this paper is that stochastic neural networks have a mathematical structure that corresponds quite closely with that of quantum field theory. Neural network Liouvillians and Lagrangians can be derived, just as can spin Hamiltonians and Lagrangians in QFT. It remains to show the efficacy of such a description.},
 author = {Cowan, J.D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Metadata.json},
 openalex = {W2296615631},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3c7781a36bcd6cf08c11a970fbe0e2a6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Stochastic Neurodynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/3c7781a36bcd6cf08c11a970fbe0e2a6-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_3dd48ab3,
 abstract = {In this paper, we will describe several extensions to our earlier work, utilizing a segment-based approach. We will formulate our segmental framework and report our study on the use of multi-layer perceptrons for detection and classification of phonemes. We will also examine the outputs of the network, and compare the network performance with other classifiers. Our investigation is performed within a set of experiments that attempts to recognize 38 vowels and consonants in American English independent of speaker. When evaluated on the TIMIT database, our system achieves an accuracy of 56%.},
 author = {Leung, Hong and Glass, James and Phillips, Michael and Zue, Victor W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Metadata.json},
 openalex = {W2162550210},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Phonetic Classification and Recognition Using the Multi-Layer Perceptron},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/3dd48ab31d016ffcbf3314df2b3cb9ce-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_3fe94a00,
 abstract = {We propose a new parallel-hierarchical neural network model to enable motor learning for simultaneous control of both trajectory and force, by integrating Hogan's control method and our previous neural network control model using a feedback-error-learning scheme. Furthermore, two hierarchical control laws which apply to the model, are derived by using the Moore-Penrose pseudo-inverse matrix. One is related to the minimum muscle-tension-change trajectory and the other is related to the minimum motor-command-change trajectory. The human arm is redundant at the dynamics level since joint torque is generated by agonist and antagonist muscles. Therefore, acquisition of the inverse model is an ill-posed problem. However, the combination of these control laws and feedback-error-learning resolve the ill-posed problem. Finally, the efficiency of the parallel-hierarchical neural network model is shown by learning experiments using an artificial muscle arm and computer simulations.},
 author = {Katayama, Masazumi and Kawato, Mitsuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3fe94a002317b5f9259f82690aeea4cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3fe94a002317b5f9259f82690aeea4cd-Metadata.json},
 openalex = {W2099440200},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Trajectory and Force Control of an Artificial Muscle Arm by Parallel-hierarchical Neural Network Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/3fe94a002317b5f9259f82690aeea4cd-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_40008b9a,
 abstract = {Local variable selection has proven to be a powerful technique for approximating functions in high-dimensional spaces. It is used in several statistical methods, including CART, ID3, C4, MARS, and others (see the bibliography for references to these algorithms). In this paper I present a tree-structured network which is a generalization of these techniques. The network provides a framework for understanding the behavior of such algorithms and for modifying them to suit particular applications.},
 author = {Sanger, Terence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/40008b9a5380fcacce3976bf7c08af5b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/40008b9a5380fcacce3976bf7c08af5b-Metadata.json},
 openalex = {W2147512992},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Basis-Function Trees as a Generalization of Local Variable Selection Methods for Function Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_41ae36ec,
 abstract = {Despite its successes, Rumelhart and McClelland's (1986) well-known approach to the learning of morphophonemic rules suffers from two deficiencies: (1) It performs the artificial task of associating forms with forms rather than perception or production. (2) It is not constrained in ways that humans learners are. This paper describes a model which addresses both objections. Using a simple recurrent architecture which takes both forms and meanings as inputs, the model learns to generate verbs in one or another tense, given arbitrary meanings, and to recognize the tenses of verbs. Furthermore, it fails to learn reversal processes unknown in human language.},
 author = {Gasser, Michael and Lee, Chan-Do},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/41ae36ecb9b3eee609d05b90c14222fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/41ae36ecb9b3eee609d05b90c14222fb-Metadata.json},
 openalex = {W2150539551},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Short-Term Memory Architecture for the Learning of Morphophonemic Rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/41ae36ecb9b3eee609d05b90c14222fb-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_41f1f191,
 author = {Gerstner, Wulfram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/41f1f19176d383480afa65d325c06ed0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/41f1f19176d383480afa65d325c06ed0-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/41f1f19176d383480afa65d325c06ed0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Associative Memory in a Network of \textasciigrave Biological\textquotesingle Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/41f1f19176d383480afa65d325c06ed0-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_42e7aaa8,
 abstract = {We introduce oriented non-radial basis function networks (ONRBF) as a generalization of Radial Basis Function networks (RBF)- wherein the Euclidean distance metric in the exponent of the Gaussian is replaced by a more general polynomial. This permits the definition of more general regions and in particular- hyper-ellipses with orientations. In the case of hyper-surface estimation this scheme requires a smaller number of hidden units and alleviates the curse of dimensionality associated kernel type approximators. In the case of an image, the hidden units correspond to features in the image and the parameters associated with each unit correspond to the rotation, scaling and translation properties of that particular feature. In the context of the ONBF scheme, this means that an image can be represented by a small number of features. Since, transformation of an image by rotation, scaling and translation correspond to identical transformations of the individual features, the ONBF scheme can be used to considerable advantage for the purposes of image recognition and analysis.},
 author = {Saha, Avijit and Christian, Jim and Tang, Dun-Sung and Chuan-Lin, Wu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/42e7aaa88b48137a16a1acd04ed91125-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/42e7aaa88b48137a16a1acd04ed91125-Metadata.json},
 openalex = {W2150494089},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/42e7aaa88b48137a16a1acd04ed91125-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Oriented Non-Radial Basis Functions for Image Coding and Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/42e7aaa88b48137a16a1acd04ed91125-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_432aca3a,
 abstract = {We compare the performance of the modular architecture, composed of competing expert networks, suggested by Jacobs, Jordan, Nowlan and Hinton (1991) to the performance of a single back-propagation network on a complex, but low-dimensional, vowel recognition task. Simulations reveal that this system is capable of uncovering interesting decompositions in a complex task. The type of decomposition is strongly influenced by the nature of the input to the gating network that decides which expert to use for each case. The modular architecture also exhibits consistently better generalization on many variations of the task.},
 author = {Nowlan, Steven and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Metadata.json},
 openalex = {W2133081131},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Evaluation of Adaptive Mixtures of Competing Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_49182f81,
 abstract = {Automatic recognition of spoken letters is one of the most challenging tasks in the field of computer speech recognition. The difficulty of the task is due to the acoustic similarity of many of the letters. Accurate recognition requires the system to perform <i>fine phonetic distinctions,</i> such as B vs. D, B vs. P, D vs. T, T vs. G, C vs. Z, V vs. Z, M vs. N and J vs. K. The ability to perform fine phonetic distinctions---to discriminate among the minimal sound units of the language---is a fundamental unsolved problem in computer speech recognition.},
 author = {Fanty, Mark and Cole, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/49182f81e6a13cf5eaa496d51fea6406-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/49182f81e6a13cf5eaa496d51fea6406-Metadata.json},
 openalex = {W1966297316},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Spoken letter recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/49182f81e6a13cf5eaa496d51fea6406-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_4f4adcbf,
 abstract = {A high speed implementation of the CMAC neural network was designed using dedicated CMOS logic. This technology was then used to implement two general purpose CMAC associative memory boards for the VME bus. Each board implements up to 8 independent CMAC networks with a total of one million adjustable weights. Each CMAC network can be configured to have from 1 to 512 integer inputs and from 1 to 8 integer outputs. Response times for typical CMAC networks are well below 1 millisecond, making the networks sufficiently fast for most robot control problems, and many pattern recognition and signal processing problems.},
 author = {Miller, W. and Box, Brian and Whitney, Erich and Glynn, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/4f4adcbf8c6f66dcfc8a3282ac2bf10a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/4f4adcbf8c6f66dcfc8a3282ac2bf10a-Metadata.json},
 openalex = {W2117592133},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/4f4adcbf8c6f66dcfc8a3282ac2bf10a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Design and Implementation of a High Speed CMAC Neural Network Using Programmable CMOS Logic Cell Arrays},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/4f4adcbf8c6f66dcfc8a3282ac2bf10a-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_4f6ffe13,
 abstract = {Fully recurrent (asymmetrical) networks can be thought of as dynamic systems. The dynamics can be shaped to perform content addressable memories, recognize sequences, or generate trajectories. Unfortunately several problems can arise: First, the convergence in the state space is not guaranteed. Second, the learned fixed points or trajectories are not necessarily stable. Finally, there might exist spurious fixed points and/or spurious attracting trajectories that do not correspond to any patterns. In this paper, we introduce a new energy function that presents solutions to all of these problems. We present an efficient gradient descent algorithm which directly acts on the stability of the fixed points and trajectories and on the size and shape of the corresponding basin and valley of attraction. The results are illustrated by the simulation of a small content addressable memory.},
 author = {Simard, Patrice and Raysz, Jean and Victorri, Bernard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/4f6ffe13a5d75b2d6a3923922b3922e5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/4f6ffe13a5d75b2d6a3923922b3922e5-Metadata.json},
 openalex = {W2158181064},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/4f6ffe13a5d75b2d6a3923922b3922e5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Shaping the State Space Landscape in Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/4f6ffe13a5d75b2d6a3923922b3922e5-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_52720e00,
 abstract = {The dimensionality of a set of 160 face images of 10 male and 10 female subjects is reduced from 4096 to 40 via an autoencoder network. The extracted features do not correspond to the features used in previous face recognition systems (Kanade, 1973), such as ratios of distances between facial elements. Rather, they are whole-face features we call holons. The holons are given to 1 and 2 layer back propagation networks that are trained to classify the input features for identity, feigned emotional state and gender. The automatically extracted holons provide a sufficient basis for all of the gender discriminations, 99% of the identity discriminations and several of the emotion discriminations among the training set. Network and human judgements of the emotions are compared, and it is found that the networks tend to confuse more distant emotions than humans do.},
 author = {Cottrell, Garrison and Metcalfe, Janet},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/52720e003547c70561bf5e03b95aa99f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/52720e003547c70561bf5e03b95aa99f-Metadata.json},
 openalex = {W2108496475},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/52720e003547c70561bf5e03b95aa99f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {EMPATH: Face, Emotion, and Gender Recognition Using Holons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/52720e003547c70561bf5e03b95aa99f-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_5737c6ec,
 abstract = {The development of projections from the retinas to the cortex is mathematically analyzed according to the previously proposed thermodynamic formulation of the self-organization of neural networks. Three types of submodality included in the visual afferent pathways are assumed in two models: model (A), in which the ocularity and retinotopy are considered separately, and model (B), in which on-center/off-center pathways are considered in addition to ocularity and retinotopy. Model (A) shows striped ocular dominance spatial patterns and, in ocular dominance histograms, reveals a dip in the binocular bin. Model (B) displays spatially modulated irregular patterns and shows single-peak behavior in the histograms. When we compare the simulated results with the observed results, it is evident that the ocular dominance spatial patterns and histograms for models (A) and (B) agree very closely with those seen in monkeys and cats.},
 author = {Tanaka, Shigeru},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Metadata.json},
 openalex = {W2110178351},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Interaction Among Ocularity, Retinotopy and On-center/Off-center Pathways During Development},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_577bcc91,
 abstract = {Coherent oscillatory activity in large networks of biological or artificial neural units may be a useful mechanism for coding information pertaining to a single perceptual object or for detailing regularities within a data set. We consider the dynamics of a large array of simple coupled oscillators under a variety of connection schemes. Of particular interest is the rapid and robust phase-locking that results from a sparse scheme where each oscillator is strongly coupled to a tiny, randomly selected, subset of its neighbors.},
 author = {Niebur, Ernst and Kammen, Daniel and Koch, Christof and Ruderman, Daniel and Schuster, Heinz},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Metadata.json},
 openalex = {W2119650558},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Phase-coupling in Two-Dimensional Networks of Interacting Oscillators},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/577bcc914f9e55d5e4e4f82f9f00e7d4-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_58238e9a,
 abstract = {We have produced a VLSI circuit capable of learning to approximate arbitrary smooth of a single variable using a technique closely related to splines. The circuit effectively has 512 knots space on a uniform grid and has full support for learning. The circuit also can be used to approximate multi-variable functions as sum of splines.},
 author = {Schwartz, Daniel and Samalam, Vijay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/58238e9ae2dd305d79c2ebc8c1883422-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/58238e9ae2dd305d79c2ebc8c1883422-Metadata.json},
 openalex = {W2168000613},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Analog VLSI Splining Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/58238e9ae2dd305d79c2ebc8c1883422-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_5a4b25aa,
 abstract = {The three problems that concern us are identifying a natural domain of pattern classification applications of feed forward neural networks, selecting an appropriate feedforward network architecture, and assessing the tradeoff between network complexity, training set size, and statistical reliability as measured by the probability of incorrect classification. We close with some suggestions, for improving the bounds that come from Vapnik-Chervonenkis theory, that can narrow, but not close, the chasm between theory and practice.},
 author = {Fine, Terrence L.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5a4b25aaed25c2ee1b74de72dc03c14e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5a4b25aaed25c2ee1b74de72dc03c14e-Metadata.json},
 openalex = {W2118872834},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Designing Linear Threshold Based Neural Network Pattern Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_5b8add2a,
 abstract = {Existing computational models of structure-from-motion are all based on a quantitative analysis of variations in optical flow or feature point correspondences within the interiors of single objects. We present an alternative approach effective for objects rotating in depth. The method involves a set of qualitative constraints on shape and motion based on patterns of flow at surface boundaries. These constraints are used in the development of a simple approximation technique for recovering surface shape. The approach is based on large-magnitude effects that are likely to be easily extracted, even from noisy data. In addition, it explains a variety of phenomena described in the literature on human vision that cannot be accounted for by any existing computational model of structure-from-motion.},
 author = {Weinshall, Daphna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5b8add2a5d98b1a652ea7fd72d942dac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5b8add2a5d98b1a652ea7fd72d942dac-Metadata.json},
 openalex = {W2012920349},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5b8add2a5d98b1a652ea7fd72d942dac-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Qualitative constraints for structure-from-motion},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/5b8add2a5d98b1a652ea7fd72d942dac-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_5ef698cd,
 abstract = {Previous work (M.I. Sereno, 1989; cf. M.E. Sereno, 1987) showed that a feedforward network with area V1-like input-layer units and a Hebb rule can develop area MT-like second layer units that solve the aperture problem for pattern motion. The present study extends this earlier work to more complex motions. Saito et al. (1986) showed that neurons with large receptive fields in macaque visual area MST are sensitive to different senses of rotation and dilation, irrespective of the receptive field location of the movement singularity. A network with an MT-like second layer was trained and tested on combinations of rotating, dilating, and translating patterns. Third-layer units learn to detect specific senses of rotation or dilation in a position-independent fashion, despite having position-dependent direction selectivity within their receptive fields.},
 author = {Sereno, Martin and Sereno, Margaret},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5ef698cd9fe650923ea331c15af3b160-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5ef698cd9fe650923ea331c15af3b160-Metadata.json},
 openalex = {W2155427687},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning to See Rotation and Dilation with a Hebb Rule},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/5ef698cd9fe650923ea331c15af3b160-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_66368270,
 abstract = {Seven different pattern classifiers were implemented on a serial computer and compared using artificial and speech recognition tasks. Two neural network (radial basis function and high order polynomial GMDH network) and five conventional classifiers (Gaussian mixture, linear tree, K nearest neighbor, KD-tree, and condensed K nearest neighbor) were evaluated. Classifiers were chosen to be representative of different approaches to pattern classification and to complement and extend those evaluated in a previous study (Lee and Lippmann, 1989). This and the previous study both demonstrate that classification error rates can be equivalent across different classifiers when they are powerful enough to form minimum error decision regions, when they are properly tuned, and when sufficient training data is available. Practical characteristics such as training time, classification time, and memory requirements, however, can differ by orders of magnitude. These results suggest that the selection of a classifier for a particular task should be guided not so much by small differences in error rate, but by practical considerations concerning memory usage, computational resources, ease of implementation, and restrictions on training and classification times.},
 author = {Ng, Kenney and Lippmann, Richard P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/66368270ffd51418ec58bd793f2d9b1b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/66368270ffd51418ec58bd793f2d9b1b-Metadata.json},
 openalex = {W2108137523},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Comparative Study of the Practical Characteristics of Neural Network and Conventional Pattern Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_66808e32,
 abstract = {We describe a real time robot navigation system based on three VLSI neural network modules. These are a resistive grid for path planning, a nearest-neighbour classifier for localization using range data from a time-of-flight infra-red sensor and a sensory-motor associative network for dynamic obstacle avoidance.},
 author = {Tarassenko, Lionel and Brownlow, Michael and Marshall, Gillian and Tombs, Jan and Murray, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/66808e327dc79d135ba18e051673d906-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/66808e327dc79d135ba18e051673d906-Metadata.json},
 openalex = {W2134615954},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/66808e327dc79d135ba18e051673d906-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Real-time autonomous robot navigation using VLSI neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/66808e327dc79d135ba18e051673d906-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_6855456e,
 abstract = {A simple architecture and algorithm for analytically guaranteed associative memory storage of analog patterns, continuous sequences, and chaotic attractors in the same network is described. A matrix inversion determines network weights, given prototype patterns to be stored. There are N units of capacity in an N node network with 3N2 weights. It costs one unit per static attractor, two per Fourier component of each sequence, and four per chaotic attractor. There are no spurious attractors, and there is a Liapunov function in a special coordinate system which governs the approach of transient states to stored trajectories. Unsupervised or supervised incremental learning algorithms for pattern classification, such as competitive learning or bootstrap Widrow-Hoff can easily be implemented. The architecture can be folded into a recurrent network with higher order weights that can be used as a model of cortex that stores oscillatory and chaotic attractors by a Hebb rule. Hierarchical sensory-motor control networks may be constructed of interconnected cortical patches of these network modules. Network performance is being investigated by application to the problem of real time handwritten digit recognition.},
 author = {Baird, Bill and Eeckman, Frank},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6855456e2fe46a9d49d3d3af4f57443d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6855456e2fe46a9d49d3d3af4f57443d-Metadata.json},
 openalex = {W2164055039},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6855456e2fe46a9d49d3d3af4f57443d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {CAM Storage of Analog Patterns and Continuous Sequences with 3N2 Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/6855456e2fe46a9d49d3d3af4f57443d-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_6883966f,
 abstract = {An important feature of radial basis function neural networks is the existence of a fast, linear learning algorithm in a network capable of representing complex nonlinear mappings. Satisfactory generalization in these networks requires that the network mapping be sufficiently smooth. We show that a modification to the error functional allows smoothing to be introduced explicitly without significantly affecting the speed of training. A simple example is used to demonstrate the resulting improvement in the generalization properties of the network.},
 author = {Botros, Sherif and Atkeson, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6883966fd8f918a4aa29be29d2c386fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6883966fd8f918a4aa29be29d2c386fb-Metadata.json},
 openalex = {W1980290744},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6883966fd8f918a4aa29be29d2c386fb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Improving the Generalization Properties of Radial Basis Function Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/6883966fd8f918a4aa29be29d2c386fb-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_69cb3ea3,
 abstract = {This work extends computational learning theory to situations in which concepts vary over time, e.g., system identification of a time-varying plant. We have extended formal definitions of concepts and learning to provide a framework in which an algorithm can track a concept as it evolves over time. Given this framework and focusing on memory-based algorithms, we have derived some PAC-style sample complexity results that determine, for example, when tracking is feasible. We have also used a similar framework and focused on incremental tracking algorithms for which we have derived some bounds on the mistake or error rates for some specific concept classes.},
 author = {Kuh, Anthony and Petsche, Thomas and Rivest, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/69cb3ea317a32c4e6143e665fdb20b14-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/69cb3ea317a32c4e6143e665fdb20b14-Metadata.json},
 openalex = {W2137143853},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/69cb3ea317a32c4e6143e665fdb20b14-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Time-varying Concepts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/69cb3ea317a32c4e6143e665fdb20b14-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_6c524f9d,
 abstract = {We present a new neural network model for processing of temporal patterns. This model, the gamma neural model, is as general as a convolution delay model with arbitrary weight kernels w(t). We show that the gamma model can be formulated as a (partially prewired) additive model. A temporal hebbian learning rule is derived and we establish links to related existing models for temporal processing.},
 author = {de Vries, Bert and Pr\'{\i}ncipe, Jos\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6c524f9d5d7027454a783c841250ba71-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6c524f9d5d7027454a783c841250ba71-Metadata.json},
 openalex = {W2168934702},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Theory for Neural Networks with Time Delays},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/6c524f9d5d7027454a783c841250ba71-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_6da37dd3,
 abstract = {We have been studying the performance of a bottlenosed dolphin on a delayed matching-to-sample task to gain insight into the processes and mechanisms that the animal uses during echolocation. The dolphin recognizes targets by emitting natural sonar signals and listening to the echoes that return. This paper describes a novel neural network architecture, called an integrator gateway network, that we have developed to account for this performance. The integrator gateway network combines information from multiple echoes to classify targets with about 90% accuracy. In contrast, a standard backpropagation network performed with only about 63% accuracy.},
 author = {Roitblat, Herbert and Moore, Patrick and Nachtigall, Paul and Penner, Ralph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Metadata.json},
 openalex = {W2105070220},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Natural Dolphin Echo Recognition Using an Integrator Gateway Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_6ecbdd6e,
 abstract = {We are focusing on the development of a highly compact neural net weight function based on the use of EEPROM devices. These devices have already proven useful for analog weight storage, but existing designs rely on the use of conventional voltage multiplication as the weight function, requiring additional transistors per synapse. A parasitic capacitance between the floating gate and the drain of the EEPROM structure leads to an unusual I-V characteristic which can be used to advantage in designing a compact synapse. This novel behavior is well characterized by a model we have developed. A single-device circuit results in a 1-quadrant synapse function which is nonlinear, though monotonic. A simple extension employing 2 EEPROMs results in a 2 quadrant function which is much more linear. This approach offers the potential for more than a ten-fold increase in the density of neural net implementations.},
 author = {Kramer, A. and Sin, C. and Chu, R. and Ko, P.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6ecbdd6ec859d284dc13885a37ce8d81-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6ecbdd6ec859d284dc13885a37ce8d81-Metadata.json},
 openalex = {W2131462597},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Compact EEPROM-based Weight Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/6ecbdd6ec859d284dc13885a37ce8d81-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_6faa8040,
 abstract = {Using an unsupervised learning procedure, a network is trained on an ensemble of images of the same two-dimensional object at different positions, orientations and sizes. Each half of the network sees one fragment of the object, and tries to produce as output a set of 4 parameters that have high mutual information with the 4 parameters output by the other half of the network. Given the ensemble of training patterns, the 4 parameters on which the two halves of the network can agree are the position, orientation, and size of the whole object, or some recoding of them. After training, the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree. If two competing networks are trained on an unlabelled mixture of images of two objects, they cluster the training cases on the basis of the objects' shapes, independently of the position, orientation, and size.},
 author = {Zemel, Richard and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6faa8040da20ef399b63a72d0e4ab575-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6faa8040da20ef399b63a72d0e4ab575-Metadata.json},
 openalex = {W2142150270},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Discovering Viewpoint-Invariant Relationships That Characterize Objects},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/6faa8040da20ef399b63a72d0e4ab575-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_70c639df,
 abstract = {This work addresses three problems with reinforcement learning and adaptive neuro-control: 1. Non-Markovian interfaces between learner and environment. 2. On-line learning based on system realization. 3. Vector-valued adaptive critics. An algorithm is described which is based on system realization and on two interacting fully recurrent continually running networks which may learn in parallel. Problems with parallel learning are attacked by 'adaptive randomness'. It is also described how interacting model/controller systems can be combined with vector-valued 'adaptive critics' (previous critics have been scalar).},
 author = {Schmidhuber, J\"{u}rgen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/70c639df5e30bdee440e4cdf599fec2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/70c639df5e30bdee440e4cdf599fec2b-Metadata.json},
 openalex = {W2115121720},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/70c639df5e30bdee440e4cdf599fec2b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Reinforcement Learning in Markovian and Non-Markovian Environments},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/70c639df5e30bdee440e4cdf599fec2b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_75887499,
 abstract = {The learning time of a simple neural network model is obtained through an analytic computation of the eigenvalue spectrum for the Hessian matrix, which describes the second order properties of the cost function in the space of coupling coefficients. The form of the eigenvalue distribution suggests new techniques for accelerating the learning process, and provides a theoretical justification for the choice of centered versus biased state variables.},
 author = {LeCun, Yann and Kanter, Ido and Solla, Sara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/758874998f5bd0c393da094e1967a72b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/758874998f5bd0c393da094e1967a72b-Metadata.json},
 openalex = {W2121193622},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/758874998f5bd0c393da094e1967a72b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Second Order Properties of Error Surfaces: Learning Time and Generalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/758874998f5bd0c393da094e1967a72b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_75fc093c,
 abstract = {We describe a recurrent connectionist network, called CONCERT, that uses a set of melodies written in a given style to compose new melodies in that style. CONCERT is an extension of a traditional algorithmic composition technique in which transition tables specify the probability of the next note as a function of previous context. A central ingredient of CONCERT is the use of a psychologically-grounded representation of pitch.},
 author = {Mozer, Michael C and Soukup, Todd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/75fc093c0ee742f6dddaa13fff98f104-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/75fc093c0ee742f6dddaa13fff98f104-Metadata.json},
 openalex = {W2156452926},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connectionist Music Composition Based on Melodic and Stylistic Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/75fc093c0ee742f6dddaa13fff98f104-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_7eacb532,
 abstract = {(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques. (2) We present a method for computing the first two moments of the probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (e.g. the ensemble formalism) and practical techniques (e.g. back-prop). Our results also shed new light on and generalize the well-known softmax scheme.},
 author = {Denker, John and LeCun, Yann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/7eacb532570ff6858afd2723755ff790-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/7eacb532570ff6858afd2723755ff790-Metadata.json},
 openalex = {W2127538960},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/7eacb532570ff6858afd2723755ff790-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Transforming Neural-Net Output Levels to Probability Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/7eacb532570ff6858afd2723755ff790-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_816b112c,
 abstract = {We describe a series of careful numerical experiments which measure the average generalization capability of neural networks trained on a variety of simple functions. These experiments are designed to test whether average generalization performance can surpass the worst-case bounds obtained from formal learning theory using the Vapnik-Chervonenkis dimension (Blumer et al., 1989). We indeed find that, in some cases, the average generalization is significantly better than the VC bound: the approach to perfect performance is exponential in the number of examples m, rather than the 1/m result of the bound. In other cases, we do find the 1/m behavior of the VC bound, and in these cases, the numerical prefactor is closely related to prefactor contained in the bound.},
 author = {Cohn, David and Tesauro, Gerald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/816b112c6105b3ebd537828a39af4818-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/816b112c6105b3ebd537828a39af4818-Metadata.json},
 openalex = {W2165701400},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/816b112c6105b3ebd537828a39af4818-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Can neural networks do better than the Vapnik-Chervonenkis bounds?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/816b112c6105b3ebd537828a39af4818-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_819f46e5,
 abstract = {Competitive learning is an unsupervised algorithm that classifies input patterns into mutually exclusive clusters. In a neural net framework, each cluster is represented by a processing unit that competes with others in a winner-take-all pool for an input pattern. I present a simple extension to the algorithm that allows it to construct discrete, distributed representations. Discrete representations are useful because they are relatively easy to analyze and their information content can readily be measured. Distributed representations are useful because they explicitly encode similarity. The basic idea is to apply competitive learning iteratively to an input pattern, and after each stage to subtract from the input pattern the component that was captured in the representation at that stage. This component is simply the weight vector of the winning unit of the competitive pool. The subtraction procedure forces competitive pools at different stages to encode different aspects of the input. The algorithm is essentially the same as a traditional data compression technique known as multistep vector quantization, although the neural net perspective suggests potentially powerful extensions to that approach.},
 author = {Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/819f46e52c25763a55cc642422644317-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/819f46e52c25763a55cc642422644317-Metadata.json},
 openalex = {W2095712978},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/819f46e52c25763a55cc642422644317-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Discovering Discrete Distributed Representations with Iterative Competitive Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/819f46e52c25763a55cc642422644317-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_82cec960,
 author = {Hayashi, Yoichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/82cec96096d4281b7c95cd7e74623496-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/82cec96096d4281b7c95cd7e74623496-Metadata.json},
 openalex = {W3146590433},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A neural expert system with automated extraction of fuzzy if-then rules and its application to medical diagnosis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/82cec96096d4281b7c95cd7e74623496-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_89f0fd5c,
 abstract = {We have created a radial basis function network that allocates a new computational unit whenever an unusual pattern is presented to the network. The network learns by allocating new units and adjusting the parameters of existing units. If the network performs poorly on a presented pattern, then a new unit is allocated which memorizes the response to the presented pattern. If the network performs well on a presented pattern, then the network parameters are updated using standard LMS gradient descent. For predicting the Mackey Glass chaotic time series, our network learns much faster than do those using back-propagation and uses a comparable number of synapses.},
 author = {Platt, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Metadata.json},
 openalex = {W2112735360},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/89f0fd5c927d466d6ec9a21b9ac34ffa-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Leaning by Combining Memorization and Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/89f0fd5c927d466d6ec9a21b9ac34ffa-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_8bf1211f,
 abstract = {The model-based neural vision system presented here determines the position and identity of three-dimensional objects. Two stereo images of a scene are described in terms of shape primitives (line segments derived from edges in the scenes) and their relational structure. A recurrent neural matching network solves the correspondence problem by assigning corresponding line segments in right and left stereo images. A 3-D relational scene description it then generated and matched by a second neural network against models in a model base. The quality of the solutions and the convergence speed were both improved by using mean field approximations.},
 author = {Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8bf1211fd4b7b94528899de0a43b9fb3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8bf1211fd4b7b94528899de0a43b9fb3-Metadata.json},
 openalex = {W2136033320},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Neural Network Approach for Three-Dimensional Object Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/8bf1211fd4b7b94528899de0a43b9fb3-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_8cb22bdd,
 abstract = {Harmonic grammar (Legendre, et al., 1990) is a connectionist theory of linguistic well-formedness based on the assumption that the well-formedness of a sentence can be measured by the harmony (negative energy) of the corresponding connectionist state. Assuming a lower-level connectionist network that obeys a few general connectionist principles but is otherwise unspecified, we construct a higher-level network with an equivalent harmony function that captures the most linguistically relevant global aspects of the lower level network. In this paper, we extend the tensor product representation (Smolensky 1990) to fully recursive representations of recursively structured objects like sentences in the lower-level network. We show theoretically and with an example the power of the new technique for parallel distributed structure processing.},
 author = {Legendre, Geraldine and Miyata, Yoshiro and Smolensky, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Metadata.json},
 openalex = {W1530693036},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Distributed Recursive Structure Processing.},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/8cb22bdd0b7ba1ab13d742e22eed8da2-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_8d3bba74,
 abstract = {The self-organization of recurrent feature-discovery networks is studied from the perspective of dynamical systems. Bifurcation theory reveals parameter regimes in which multiple equilibria or limit cycles coexist with the equilibrium at which the networks perform principal component analysis.},
 author = {Leen, Todd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8d3bba7425e7c98c50f52ca1b52d3735-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8d3bba7425e7c98c50f52ca1b52d3735-Metadata.json},
 openalex = {W2106438572},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Dynamics of Learning in Recurrent Feature-Discovery Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/8d3bba7425e7c98c50f52ca1b52d3735-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_8d7d8ee0,
 abstract = {Barto, Sutton and Watkins [2] introduced a grid task as a didactic example of temporal difference planning and asynchronous dynamical programming. This paper considers the effects of changing the coding of the input stimulus, and demonstrates that the self-supervised learning of a particular form of hidden unit representation improves performance.},
 author = {Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8d7d8ee069cb0cbbf816bbb65d56947e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8d7d8ee069cb0cbbf816bbb65d56947e-Metadata.json},
 openalex = {W2129120309},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8d7d8ee069cb0cbbf816bbb65d56947e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Navigating through Temporal Difference},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/8d7d8ee069cb0cbbf816bbb65d56947e-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_8dd48d6a,
 abstract = {Neural networks usually work adequately on small problems but can run into trouble when they are scaled up to problems involving large amounts of input data. Circuit Complexity and Neural Networks addresses the important question of how well neural networks scale - that is, how fast the computation time and number of neurons grow as the problem size increases. It surveys recent research in circuit complexity (a robust branch of theoretical computer science) and applies this work to a theoretical understanding of the problem of scalability. Most research in neural networks focuses on learning, yet it is important to understand the physical limitations of the network before the resources needed to solve a certain problem can be calculated. One of the aims of this book is to compare the complexity of neural networks and the complexity of conventional computers, looking at the computational ability and resources (neurons and time) that are a necessary part of the foundations of neural network learning. Circuit Complexity and Neural Networks contains a significant amount of background material on conventional complexity theory that will enable neural network scientists to learn about how complexity theory applies to their discipline, and allow complexity theorists to see how their discipline applies to neural networks.},
 author = {Roychowdhury, V. P. and Siu, K. Y. and Orlitsky, A. and Kailath, T.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8dd48d6a2e2cad213179a3992c0be53c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8dd48d6a2e2cad213179a3992c0be53c-Metadata.json},
 openalex = {W4230719014},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Circuit Complexity and Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_8e98d81f,
 abstract = {This work presents an Attractor Neural Network (ANN) model of Recall and Recognition. It is shown that an ANN model can qualitatively account for a wide range of experimental psychological data pertaining to the these two main aspects of memory access. Certain psychological phenomena are accounted for, including the effects of list-length, word-frequency, presentation time, context shift, and aging. Thereafter, the probabilities of successful Recall and Recognition are estimated, in order to possibly enable further quantitative examination of the model.},
 author = {Ruppin, Eytan and Yeshurun, Yehezkel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8e98d81f8217304975ccb23337bb5761-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8e98d81f8217304975ccb23337bb5761-Metadata.json},
 openalex = {W2156715913},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8e98d81f8217304975ccb23337bb5761-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {An Attractor Neural Network Model of Recall and Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/8e98d81f8217304975ccb23337bb5761-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_8efb100a,
 abstract = {We describe the application of a hybrid symbolic/connectionist machine learning algorithm to the task of recognizing important genetic sequences. The symbolic portion of the KBANN system utilizes inference rules that provide a roughly-correct method for recognizing a class of DNA sequences known as eukaryotic splice-junctions. We then map this into a neural network and provide training examples. Using the samples, the neural network's learning algorithm adjusts the domain theory so that it properly classifies these DNA sequences. Our procedure constitutes a general method for incorporating preexisting knowledge into artificial neural networks. We present an experiment in molecular genetics that demonstrates the value of doing so.},
 author = {Noordewier, Michiel and Towell, Geoffrey and Shavlik, Jude},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8efb100a295c0c690931222ff4467bb8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8efb100a295c0c690931222ff4467bb8-Metadata.json},
 openalex = {W2114720696},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8efb100a295c0c690931222ff4467bb8-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Training Knowledge-Based Neural Networks to Recognize Genes in DNA Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/8efb100a295c0c690931222ff4467bb8-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_8fe0093b,
 abstract = {ALCOVE is a connectionist model of human category learning that fits a broad spectrum of human learning data. Its architecture is based on well-established psychological theory, and is related to networks using radial basis functions. From the perspective of cognitive psychology, ALCOVE can be construed as a combination of exemplar-based representation and error-driven learning. From the perspective of connectionism, it can be seen as incorporating constraints into back-propagation networks appropriate for modelling human learning.},
 author = {Kruschke, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8fe0093bb30d6f8c31474bd0764e6ac0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8fe0093bb30d6f8c31474bd0764e6ac0-Metadata.json},
 openalex = {W2097642122},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/8fe0093bb30d6f8c31474bd0764e6ac0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {ALCOVE: A Connectionist Model of Human Category Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/8fe0093bb30d6f8c31474bd0764e6ac0-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_94f6d7e0,
 abstract = {Multi-layer perceptrons are often slow to learn nonlinear functions with complex local structure due to the global nature of their function approximations. It is shown that standard multi-layer perceptrons are actually a special case of a more general network formulation that incorporates B-splines into the node computations. This allows novel spline network architectures to be developed that can combine the generalization capabilities and scaling properties of global multi-layer feedforward networks with the computational efficiency and learning speed of local computational paradigms. Simulation results are presented for the well known spiral problem of Weiland and of Lang and Witbrock to show the effectiveness of the Spline Net approach.},
 author = {Lane, Stephen and Flax, Marshall and Handelman, David and Gelfand, Jack},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/94f6d7e04a4d452035300f18b984988c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/94f6d7e04a4d452035300f18b984988c-Metadata.json},
 openalex = {W2144856549},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Multi-Layer Perceptrons with B-Spline Receptive Field Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/94f6d7e04a4d452035300f18b984988c-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_950a4152,
 abstract = {A new class of data structures called bumptrees is described. These structures are useful for efficiently implementing a number of neural network related operations. An empirical comparison with radial basis functions is presented on a robot arm mapping learning task. Applications to density estimation, classification, and constraint representation and learning are also outlined.},
 author = {Omohundro, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/950a4152c2b4aa3ad78bdd6b366cc179-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/950a4152c2b4aa3ad78bdd6b366cc179-Metadata.json},
 openalex = {W2153353990},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/950a4152c2b4aa3ad78bdd6b366cc179-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Bumptrees for Efficient Function, Constraint and Classification Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/950a4152c2b4aa3ad78bdd6b366cc179-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_9be40cee,
 abstract = {We present a new connectionist planning method [TML90]. By interaction with an unknown environment, a world model is progressively constructed using gradient descent. For deriving optimal actions with respect to future reinforcement, planning is applied in two steps: an experience network proposes a plan which is subsequently optimized by gradient descent with a chain of world models, so that an optimal reinforcement may be obtained when it is actually run. The appropriateness of this method is demonstrated by a robotics application and a pole balancing task.},
 author = {Thrun, Sebastian and M\"{o}ller, Knut and Linden, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9be40cee5b0eee1462c82c6964087ff9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9be40cee5b0eee1462c82c6964087ff9-Metadata.json},
 openalex = {W2157445228},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9be40cee5b0eee1462c82c6964087ff9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Planning with an Adaptive World Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/9be40cee5b0eee1462c82c6964087ff9-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_9de6d14f,
 abstract = {Simple classical spin models well-known to physicists as the ANNNI and Heisenberg XY Models, in which long-range interactions occur in a pattern given by the Mexican Hat operator, can generate many of the structural properties characteristic of the ocular dominance columns and iso-orientation patches seen in cat and primate visual cortex.},
 author = {Cowan, J.D. and Friedman, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9de6d14fff9806d4bcd1ef555be766cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9de6d14fff9806d4bcd1ef555be766cd-Metadata.json},
 openalex = {W2172110837},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9de6d14fff9806d4bcd1ef555be766cd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Simple Spin Models for the Development of Ocular Dominance Columns and Iso-Orientation Patches},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/9de6d14fff9806d4bcd1ef555be766cd-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_9dfcd5e5,
 abstract = {We demonstrate a multiscale adaptive network model of motion computation in primate area MT. The model consists of two stages: (1) local velocities are measured across multiple spatio-temporal channels, and (2) the optical flow field is computed by a network of direction-selective neurons at multiple spatial resolutions. This model embeds the computational efficiency of Multigrid algorithms within a parallel network as well as adaptively computes the most reliable estimate of the flow field across different spatial scales. Our model neurons show the same nonclassical receptive field properties as Allman's type I MT neurons. Since local velocities are measured across multiple channels, various channels often provide conflicting measurements to the network. We have incorporated a veto scheme for conflict resolution. This mechanism provides a novel explanation for the spatial frequency dependency of the psychophysical phenomenon called Motion Capture.},
 author = {Wang, H. and Mathur, Bimal and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9dfcd5e558dfa04aaf37f137a1d9d3e5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9dfcd5e558dfa04aaf37f137a1d9d3e5-Metadata.json},
 openalex = {W2152874884},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9dfcd5e558dfa04aaf37f137a1d9d3e5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Multiscale Adaptive Network Model of Motion Computation in Primates},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/9dfcd5e558dfa04aaf37f137a1d9d3e5-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_9fd81843,
 abstract = {Spherical Units can be used to construct dynamic reconfigurable consequential regions, the geometric bases for Shepard's (1987) theory of stimulus generalization in animals and humans. We derive from Shepard's (1987) generalization theory a particular multi-layer network with dynamic (centers and radii) spherical regions which possesses a specific mass function (Cauchy). This learning model generalizes the configural-cue network model (Gluck & Bower 1988): (1) configural cues can be learned and do not require pre-wiring the power-set of cues, (2) Consequential regions are continuous rather than discrete and (3) Competition amoungst receptive fields is shown to be increased by the global extent of a particular mass function (Cauchy). We compare other common mass functions (Gaussian; used in models of Moody & Darken; 1989, Krushke, 1990) or just standard backpropogation networks with hyperplane/logistic hidden units showing that neither fare as well as models of human generalization and learning.},
 author = {Hanson, Stephen and Gluck, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9fd81843ad7f202f26c1a174c7357585-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9fd81843ad7f202f26c1a174c7357585-Metadata.json},
 openalex = {W2163211671},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/9fd81843ad7f202f26c1a174c7357585-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Spherical Units as Dynamic Consequential Regions: Implications for Attention, Competition and Categorization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/9fd81843ad7f202f26c1a174c7357585-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_a01a0380,
 abstract = {This paper is a summary of SPRINT project aims and results. The project focus on the use of neuro-computing techniques to tackle various problems that remain unsolved in speech recognition. First results concern the use of feedforward nets for phonetic units classification, isolated word recognition, and speaker adaptation.},
 author = {Choukri, Khalid},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a01a0380ca3c61428c26a231f0e49a09-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a01a0380ca3c61428c26a231f0e49a09-Metadata.json},
 openalex = {W2123354195},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Speech Recognition Using Connectionist Approaches},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/a01a0380ca3c61428c26a231f0e49a09-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_a02ffd91,
 abstract = {Light adaptation (LA) allows cone vision to remain functional between twilight and the brightest time of day even though, at anyone time, their intensity-response (I-R) characteristic is limited to 3 log units of the stimulating light. One mechanism underlying LA, was localized in the outer segment of an isolated cone (1,2). We found that by adding annular illumination, an I-R characteristic of a cone can be shifted along the intensity domain. Neural network involving feedback synapse from horizontal cells to cones is involved to be in register with ambient light level of the periphery. An equivalent electrical circuit with three different transmembrane channels leakage, photocurrent and feedback was used to model static behavior of a cone. SPICE simulation showed that interactions between feedback synapse and the light sensitive conductance in the outer segment can shift the IR curves along the intensity domain, provided that phototransduction mechanism is not saturated during maximally hyperpolarized light response.},
 author = {Skrzypek, Josef},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a02ffd91ece5e7efeb46db8f10a74059-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a02ffd91ece5e7efeb46db8f10a74059-Metadata.json},
 openalex = {W2143014720},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Feedback Synapse to Cone and Light Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/a02ffd91ece5e7efeb46db8f10a74059-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_a49e9411,
 abstract = {For lack of alternative models, search and decision processes have provided the dominant paradigm for human memory access using two or more cues, despite evidence against search as an access process (Humphreys, Wiles & Bain, 1990). We present an alternative process to search, based on calculating the intersection of sets of targets activated by two or more cues. Two methods of computing the intersection are presented, one using information about the possible targets, the other constraining the cue-target strengths in the memory matrix. Analysis using orthogonal vectors to represent the cues and targets demonstrates the competence of both processes, and simulations using sparse distributed representations demonstrate the performance of the latter process for tasks involving 2 and 3 cues.},
 author = {Wiles, Janet and Humphreys, Michael and Bain, John and Dennis, Simon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a49e9411d64ff53eccfdd09ad10a15b3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a49e9411d64ff53eccfdd09ad10a15b3-Metadata.json},
 openalex = {W2162372691},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Direct memory access using two cues: Finding the intersection of sets in a connectionist model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_a666587a,
 abstract = {Neural network simulations of the dragonfly neurocontrol system have been developed to understand how this insect uses complex, aerodynamics. The simulation networks account for the ganglionic spatial distribution of cells as well as the physiologic operating range and the stochastic cellular firing history of each neuron. In addition the motor neuron firing patterns, flight command sequences, were utilized. Simulation training was targeted against both the cellular and motor neuron firing patterns. The trained networks accurately resynthesized the intraganglionic cellular firing patterns. These in turn controlled the motor neuron firing patterns that drive wing musculature during flight. Such networks provide both neurobiological analysis tools and first generation controls for the use of unsteady aerodynamics.},
 author = {Faller, William and Luttges, Marvin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a666587afda6e89aec274a3657558a27-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a666587afda6e89aec274a3657558a27-Metadata.json},
 openalex = {W2165664753},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a666587afda6e89aec274a3657558a27-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Flight Control in the Dragonfly: A Neurobiological Simulation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/a666587afda6e89aec274a3657558a27-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_a8c88a00,
 abstract = {We introduce a framework for training architectures composed of several modules. This framework, which uses a statistical formulation of learning systems, provides a unique formalism for describing many classical connectionist algorithms as well as complex systems where several algorithms interact. It allows to design hybrid systems which combine the advantages of connectionist algorithms as well as other learning algorithms.},
 author = {Bottou, L\'{e}on and Gallinari, Patrick},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a8c88a0055f636e4a163a5e3d16adab7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a8c88a0055f636e4a163a5e3d16adab7-Metadata.json},
 openalex = {W2137440383},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/a8c88a0055f636e4a163a5e3d16adab7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Framework for the Cooperation of Learning Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/a8c88a0055f636e4a163a5e3d16adab7-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_aa942ab2,
 abstract = {The authors present a large vocabulary, continuous speech recognition system based on linked predictive neural networks (LPNNs). The system uses neural networks as predictors of speech frames, yielding distortion measures which can be used by the one-stage DTW algorithm to perform continuous speech recognition. The system currently achieves 95%, 58%, and 39% word accuracy on tasks with perplexity 7, 111, and 402, respectively, outperforming several simple HMMs that have been tested. It was also found that the accuracy and speed of the LPNN can be slightly improved by the judicious use of hidden control inputs. The strengths and weaknesses of the predictive approach are discussed.< <ETX xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">&gt;</ETX>},
 author = {Tebelskis, Joe and Waibel, Alex and Petek, Bojan and Schmidbauer, Otto},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Metadata.json},
 openalex = {W2088943892},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Continuous speech recognition using linked predictive neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/aa942ab2bfa6ebda4840e7360ce6e7ef-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_ad13a2a0,
 abstract = {An Artificial Neural Network (ANN) is trained to recognize a buy/sell (long/short) pattern for a particular commodity future contract. The Back-Propagation of errors algorithm was used to encode the relationship between the Long/Short desired output and 18 fundamental variables plus 6 (or 18) technical variables into the ANN. Trained on one year of past data the ANN is able to predict long/short market positions for 9 months in the future that would have made $10,301 profit on an investment of less than $1000.},
 author = {Collard, Joseph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ad13a2a07ca4b7642959dc0c4c740ab6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ad13a2a07ca4b7642959dc0c4c740ab6-Metadata.json},
 openalex = {W2118694425},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ad13a2a07ca4b7642959dc0c4c740ab6-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A B-P ANN Commodity Trader},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/ad13a2a07ca4b7642959dc0c4c740ab6-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_ad972f10,
 abstract = {We study the representation of static patterns and temporal sequences in neural networks with signal delays and a stochastic parallel dynamics. For a wide class of delay distributions, the asymptotic network behavior can be described by a generalized Gibbs distribution, generated by a novel Lyapunov functional for the determination dynamics. We extend techniques of equilibrium statistical mechanics so as to deal with time-dependent phenomena, derive analytic results for both retrieval quality and storage capacity, and compare them with numerical simulations.},
 author = {Herz, Andreas and Li, Zhaoping and van Hemmen, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ad972f10e0800b49d76fed33a21f6698-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ad972f10e0800b49d76fed33a21f6698-Metadata.json},
 openalex = {W2170830619},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Statistical mechanics of temporal association in neural networks with transmission delays},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_b2eb7349,
 abstract = {Combining neuropharmacological experiments with computational modeling, we have shown that cholinergic modulation may enhance associative memory function in piriform (olfactory) cortex. We have shown that the acetylcholine analogue carbachol selectively suppresses synaptic transmission between cells within piriform cortex, while leaving input connections unaffected. When tested in a computational model of piriform cortex, this selective suppression, applied during learning, enhances associative memory performance.},
 author = {Hasselmo, Michael and Anderson, Brooke and Bower, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b2eb7349035754953b57a32e2841bda5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b2eb7349035754953b57a32e2841bda5-Metadata.json},
 openalex = {W2166695970},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b2eb7349035754953b57a32e2841bda5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Cholinergic Modulation May Enhance Cortical Associative Memory Function},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/b2eb7349035754953b57a32e2841bda5-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_b3967a0e,
 abstract = {Diagnosis of faults in complex, real-time control systems is a complicated task that has resisted solution by traditional methods. We have shown that neural networks can be successfully employed to diagnose faults in digitally controlled powertrain systems. This paper discusses the means we use to develop the appropriate databases for training and testing in order to select the optimum network architectures and to provide reasonable estimates of the classification accuracy of these networks on new samples of data. Recent work applying neural nets to adaptive control of an active suspension system is presented.},
 author = {Marko, Kenneth},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b3967a0e938dc2a6340e258630febd5a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b3967a0e938dc2a6340e258630febd5a-Metadata.json},
 openalex = {W2165762781},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Application to Diagnostics and Control of Vehicle Control Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/b3967a0e938dc2a6340e258630febd5a-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_b6f0479a,
 abstract = {Learning can increase the rate of evolution of a population of biological organisms (the Baldwin effect). Our simulations show that in a population of artificial neural networks solving a pattern recognition problem, no learning or too much learning leads to slow evolution of the genes whereas an intermediate amount is optimal. Moreover, for a given total number of training presentations, fastest evoution occurs if different individuals within each generation receive different numbers of presentations, rather than equal numbers. Because genetic algorithms (GAs) help avoid local minima in energy functions, our hybrid learning-GA systems can be applied successfully to complex, high-dimensional pattern recognition problems.},
 author = {Keesing, Ron and Stork, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b6f0479ae87d244975439c6124592772-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b6f0479ae87d244975439c6124592772-Metadata.json},
 openalex = {W2166563835},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b6f0479ae87d244975439c6124592772-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Evolution and Learning in Neural Networks: The Number and Distribution of Learning Trials Affect the Rate of Evolution},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/b6f0479ae87d244975439c6124592772-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_b7b16ecf,
 abstract = {We present a new way to derive dissipative, optimizing dynamics from the Lagrangian formulation of mechanics. It can be used to obtain both standard and novel neural net dynamics for optimization problems. To demonstrate this we derive standard descent dynamics as well as nonstandard variants that introduce a computational attention mechanism.},
 author = {Mjolsness, Eric and Miranker, Willard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b7b16ecf8ca53723593894116071700c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b7b16ecf8ca53723593894116071700c-Metadata.json},
 openalex = {W2162199500},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b7b16ecf8ca53723593894116071700c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Lagrangian Approach to Fixed Points},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/b7b16ecf8ca53723593894116071700c-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_b83aac23,
 abstract = {A feedforward layered network implements a mapping required to control an unknown stochastic nonlinear dynamical system. Training is based on a novel approach that combines stochastic approximation ideas with backpropagation. The method is applied to control admission into a queueing system operating in a time-varying environment.},
 author = {Milito, Rodolfo and Guyon, Isabelle and Solla, Sara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b83aac23b9528732c23cc7352950e880-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b83aac23b9528732c23cc7352950e880-Metadata.json},
 openalex = {W2140035663},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b83aac23b9528732c23cc7352950e880-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Neural Network Implementation of Admission Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/b83aac23b9528732c23cc7352950e880-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_b9228e09,
 abstract = {We consider feed-forward neural networks with one non-linear hidden layer and linear output units. The transfer function in the hidden layer are either bell-shaped or sigmoid. In the bell-shaped case, we show how Bernstein polynomials on one hand and the theory of the heat equation on the other are relevant for understanding the properties of the corresponding networks. In particular, these techniques yield simple proofs of universal approximation properties, i.e. of the fact that any reasonable function can be approximated to any degree of precision by a linear combination of bell-shaped functions. In addition, in this framework the problem of learning is equivalent to the problem of reversing the time course of a diffusion process. The results obtained in the bell-shaped case can then be applied to the case of sigmoid transfer functions in the hidden layer, yielding similar universality results. A conjecture related to the problem of generalization is briefly examined.},
 author = {Baldi, Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b9228e0962a78b84f3d5d92f4faa000b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b9228e0962a78b84f3d5d92f4faa000b-Metadata.json},
 openalex = {W2131862904},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/b9228e0962a78b84f3d5d92f4faa000b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Computing with Arrays of Bell-Shaped and Sigmoid Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/b9228e0962a78b84f3d5d92f4faa000b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_bac9162b,
 abstract = {A massively parallel, all-digital, stochastic architecture - TInMANN - is described which performs competitive and Kohonen types of learning. A VLSI design is shown for a TInMANN neuron which fits within a small, inexpensive MOSIS TinyChip frame, yet which can be used to build larger networks of several hundred neurons. The neuron operates at a speed of 15 MHz which allows the network to process 290,000 training examples per second. Use of level sensitive scan logic provides the chip with 100% fault coverage, permitting very reliable neural systems to be built.},
 author = {Melton, Matt and Phan, Tan and Reeves, Doug and Van den Bout, Dave},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bac9162b47c56fc8a4d2a519803d51b3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bac9162b47c56fc8a4d2a519803d51b3-Metadata.json},
 openalex = {W2164853310},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {VLSI Implementation of TInMANN},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/bac9162b47c56fc8a4d2a519803d51b3-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_bbcbff5c,
 abstract = {Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30×30 were compressed using a 900×40×900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans.},
 author = {Golomb, B.A. and Lawrence, D.T. and Sejnowski, T.J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bbcbff5c1f1ded46c25d28119a85c6c2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bbcbff5c1f1ded46c25d28119a85c6c2-Metadata.json},
 openalex = {W2123687672},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bbcbff5c1f1ded46c25d28119a85c6c2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {SEXNET: A Neural Network Identifies Sex From Human Faces},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/bbcbff5c1f1ded46c25d28119a85c6c2-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_bbf94b34,
 abstract = {Spoken language is one of the most natural, efficient, flexible, and economical means of communication among humans. As computers play an ever increasing role in our lives, it is important that we address the issue of providing a graceful human-machine interface through spoken language. In this paper, we will describe our recent efforts in moving beyond the scope of speech recognition into the realm of spoken-language understanding. Specifically, we report on the development of an urban navigation and exploration system called VOYAGER, an application which we have used as a basis for performing research in spoken-language understanding.},
 author = {Zue, Victor and Glass, James and Goodine, David and Hirschman, Lynette and Leung, Hong and Phillips, Michael and Polifroni, Joseph and Seneff, Stephanie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bbf94b34eb32268ada57a3be5062fe7d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bbf94b34eb32268ada57a3be5062fe7d-Metadata.json},
 openalex = {W2107333876},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {From Speech Recognition to Spoken Language Understanding: The Development of the MIT SUMMIT and VOYAGER Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/bbf94b34eb32268ada57a3be5062fe7d-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_bc6dc48b,
 abstract = {Inspired by the information theoretic idea of minimum description length, we add a term to the back propagation cost function that penalizes network complexity. We give the details of the procedure, called weight-elimination, describe its dynamics, and clarify the meaning of the parameters involved. From a Bayesian perspective, the complexity term can be usefully interpreted as an assumption about prior distribution of the weights. We use this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates.},
 author = {Weigend, Andreas and Rumelhart, David and Huberman, Bernardo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bc6dc48b743dc5d013b1abaebd2faed2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bc6dc48b743dc5d013b1abaebd2faed2-Metadata.json},
 openalex = {W2164359548},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bc6dc48b743dc5d013b1abaebd2faed2-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Generalization by Weight-Elimination with Application to Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_bca82e41,
 abstract = {Three-dimensional structures of protein backbones have been predicted using neural networks. A feed forward neural network was trained on a class of functionally, but not structurally, homologous proteins, using backpropagation learning. The network generated tertiary structure information in the form of binary distance constraints for the C(alpha) atoms in the protein backbone. The binary distance between two C(alpha) atoms was 0 if the distance between them was less than a certain threshold distance, and 1 otherwise. The distance constraints predicted by the trained neural network were utilized to generate a folded conformation of the protein backbone, using a steepest descent minimization approach.},
 author = {Fredholm, Henrik and Bohr, Henrik and Bohr, Jakob and Brunak, S\o ren and Cotterill, Rodney and Lautrup, Benny and Petersen, Steffen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bca82e41ee7b0833588399b1fcd177c7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bca82e41ee7b0833588399b1fcd177c7-Metadata.json},
 openalex = {W2053536934},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/bca82e41ee7b0833588399b1fcd177c7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A novel approach to prediction of the 3‐dimensional structures of protein backbones by neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/bca82e41ee7b0833588399b1fcd177c7-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_beed1360,
 abstract = {The Adaptive Solutions CNAPS architecture chip is a general purpose neurocomputer chip. It has 64 processors, each with 4 K bytes of local memory, running at 25 megahertz. It is capable of implementing most current neural network algorithms with on chip learning. This paper discusses the implementation of the Back Propagation algorithm on an array of these chips and shows performance figures from a clock accurate hardware simulator. An eight chip configuration on one board can update 2.3 billion connections per second in learning mode and process 9.6 billion connections per second in feed forward mode.},
 author = {McCartor, Hal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/beed13602b9b0e6ecb5b568ff5058f07-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/beed13602b9b0e6ecb5b568ff5058f07-Metadata.json},
 openalex = {W2112553863},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Back Propagation Implementation on the Adaptive Solutions CNAPS Neurocomputer Chip},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/beed13602b9b0e6ecb5b568ff5058f07-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_c042f4db,
 abstract = {If patterns are drawn from an n-dimensional feature space according to a probability distribution that obeys a weak smoothness criterion, we show that the probability that a random input pattern is misclassified by a nearest-neighbor classifier using M random reference patterns asymptotically satisfies PM(error) - P∞(error) + a/M2/n, for sufficiently large values of M. Here, P∞(error) denotes the probability of error in the infinite sample limit, and is at most twice the error of a Bayes classifier. Although the value of the coefficient a depends upon the underlying probability distributions, the exponent of M is largely distribution free. We thus obtain a concise relation between a classifier's ability to generalize from a finite reference sample and the dimensionality of the feature space, as well as an analytic validation of Bellman's well known curse of dimensionality.},
 author = {Snapp, Robert and Psaltis, Demetri and Venkatesh, Santosh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c042f4db68f23406c6cecf84a7ebb0fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c042f4db68f23406c6cecf84a7ebb0fe-Metadata.json},
 openalex = {W2106123800},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Asymptotic slowing down of the nearest-neighbor classifier},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/c042f4db68f23406c6cecf84a7ebb0fe-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_c058f544,
 abstract = {Previous work has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for Hidden Markov Models (HMMs). The advantages of a speech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. This paper presents results on the speaker-dependent portion of DARPA's English language Resource Management database. Results support the previously reported utility of MLP probability estimation for continuous speech recognition. An additional approach we are pursuing is to use MLPs as nonlinear predictors for autoregressive HMMs. While this is shown to be more compatible with the HMM formalism, it still suffers from several limitations. This approach is generalized to take account of time correlation between successive observations, without any restrictive assumptions about the driving noise.},
 author = {Bourlard, Herv\'{e} and Morgan, Nelson and Wooters, Chuck},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c058f544c737782deacefa532d9add4c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c058f544c737782deacefa532d9add4c-Metadata.json},
 openalex = {W2165397722},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c058f544c737782deacefa532d9add4c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connectionist Approaches to the Use of Markov Models for Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/c058f544c737782deacefa532d9add4c-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_c3e878e2,
 abstract = {A second-order architecture is presented here for translation, rotation and scale invariant processing of 2-D images mapped to n input units. This new architecture has a complexity of O(n) weights as opposed to the O(n3) weights usually required for a third-order, rotation invariant architecture. The reduction in complexity is due to the use of discrete frequency information. Simulations show favorable comparisons to other neural network architectures.},
 author = {Goggin, Shelly and Johnson, Kristina and Gustafson, Karl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c3e878e27f52e2a57ace4d9a76fd9acf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c3e878e27f52e2a57ace4d9a76fd9acf-Metadata.json},
 openalex = {W2100285521},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c3e878e27f52e2a57ace4d9a76fd9acf-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Second-Order Translation, Rotation and Scale Invariant Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/c3e878e27f52e2a57ace4d9a76fd9acf-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_c5ff2543,
 abstract = {Signal processing capabilities of biological neurons are investigated. Temporally coded signals in neurons can be multiplexed to increase the transmission capacity. Multiplexing of signal is suggested in bi-threshold neurons with high-threshold and low-threshold for switching firing modes. To extract the signal embedded in the interspike-intervals of firing, the encoded signal are demultiplexed and multiplexed by a network of neurons with delayed-line circuitry for signal processing. The temporally coded input signal is transformed spatially by mapping the firing intervals topographically to the output of the network, thus decoding the specific firing interspike-intervals. The network also provides a band-pass filtering capability where the variability of the timing of the original signal can be decoded.},
 author = {Tam, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c5ff2543b53f4cc0ad3819a36752467b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c5ff2543b53f4cc0ad3819a36752467b-Metadata.json},
 openalex = {W2133210059},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Signal Processing by Multiplexing and Demultiplexing in Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/c5ff2543b53f4cc0ad3819a36752467b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_c86a7ee3,
 abstract = {The authors demonstrate how artificial neural networks (ANNs) can be applied to characterizing seismic sources using high-frequency regional seismic data. The authors take the novel approach of using ANNs as a research tool for obtaining seismic source information, specifically depth of focus for earthquakes and ripple-fire characteristics for economic blasts, rather than as just a feature classifier between earthquake and explosion populations. The authors suggest that ANNs have potential applications to seismic event characterization and identification, beyond just a feature classifier. The Lg spectral matrix provides a hidden discriminant in that it appears to be sensitive to depth of focus effects, which can be recognized by the ANN. ANNs can also even recognize ripple-fire patterns not used in training. The results of this study indicate that an ANN should be evaluated as part of an operational seismic event identification system.< <ETX xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">&gt;</ETX>},
 author = {Perry, John and Baumgardt, Douglas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c86a7ee3d8ef0b551ed58e354a836f2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c86a7ee3d8ef0b551ed58e354a836f2b-Metadata.json},
 openalex = {W1854422967},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Lg depth estimation and ripple fire characterization using artificial neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/c86a7ee3d8ef0b551ed58e354a836f2b-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_caf1a3df,
 abstract = {This paper examines a class of neuron based learning systems for dynamic control that rely on adaptive range coding of sensor inputs. Sensors are assumed to provide binary coded range vectors that coarsely describe the system state. These vectors are input to neuron-like processing elements. Output decisions generated by these neurons in turn affect the system state, subsequently producing new inputs. Reinforcement signals from the environment are received at various intervals and evaluated. The weights as well as the range boundaries determining the output decisions are then altered with the goal of maximizing future reinforcement from the environment. Preliminary experiments show the promise of adapting neural receptive fields when learning dynamical control. The observed performance with this method exceeds that of earlier approaches.},
 author = {Rosen, Bruce and Goodwin, James and Vidal, Jacques},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/caf1a3dfb505ffed0d024130f58c5cfa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/caf1a3dfb505ffed0d024130f58c5cfa-Metadata.json},
 openalex = {W2167525967},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/caf1a3dfb505ffed0d024130f58c5cfa-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Adaptive Range Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/caf1a3dfb505ffed0d024130f58c5cfa-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_cd00692c,
 abstract = {A particular form of neural network is described, which has terminals for acoustic patterns, class labels and speaker parameters. A method of training this network to tune the speaker parameters to a particular speaker is outlined, based on a trick for converting a supervised network to an unsupervised mode. We describe experiments using this approach in isolated word recognition based on whole-word hidden Markov models. The results indicate an improvement over speaker-independent performance and, for unlabelled data, a performance close to that achieved on labelled data.},
 author = {Bridle, John and Cox, Stephen J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/cd00692c3bfe59267d5ecfac5310286c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/cd00692c3bfe59267d5ecfac5310286c-Metadata.json},
 openalex = {W2156692643},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/cd00692c3bfe59267d5ecfac5310286c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {RecNorm: Simultaneous Normalisation and Classification applied to Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/cd00692c3bfe59267d5ecfac5310286c-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_cf004fdc,
 abstract = {In salamander retina, the response of On-Off ganglion cells to a central flash is reduced by movement in the receptive field surround. Through computer simulation of a 2-D model which takes into account their anatomical and physiological properties, we show that interactions between four neuron types (two bipolar and two amacrine) may be responsible for the generation and lateral conductance of this change sensitive inhibition. The model shows that the four neuron circuit can account for previously observed movement sensitive reductions in ganglion cell sensitivity and allows visualization and prediction of the spatio-temporal pattern of activity in change sensitive retinal cells.},
 author = {Teeters, Jeffrey and Eeckman, Frank and Werblin, Frank},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Metadata.json},
 openalex = {W2124753757},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A four neuron circuit accounts for change sensitive inhibition in salamander retina},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/cf004fdc76fa1a4f25f62e0eb5261ca3-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_d1f255a3,
 abstract = {We introduce a method for the efficient design of a Boltzmann machine (or a Hopfield net) that computes an arbitrary given Boolean function f. This method is based on an efficient simulation of acyclic circuits with threshold gates by Boltzmann machines. As a consequence we can show that various concrete Boolean functions f that are relevant for classification problems can be computed by scalable Boltzmann machines that are guaranteed to converge to their global maximum configuration with high probability after constantly many steps.},
 author = {Gupta, Ajay and Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d1f255a373a3cef72e03aa9d980c7eca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d1f255a373a3cef72e03aa9d980c7eca-Metadata.json},
 openalex = {W2116794004},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Method for the Efficient Design of Boltzmann Machines for Classiffication Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/d1f255a373a3cef72e03aa9d980c7eca-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_d296c101,
 abstract = {We identify the three principle factors affecting the performance of learning by networks with localized units: unit noise, sample density, and the structure of the target function. We then analyze the effect of unit receptive field parameters on these factors and use this analysis to propose a new learning algorithm which dynamically alters receptive field properties during learning.},
 author = {Mel, Bartlett and Omohundro, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d296c101daa88a51f6ca8cfc1ac79b50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d296c101daa88a51f6ca8cfc1ac79b50-Metadata.json},
 openalex = {W2129003764},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {How Receptive Field Parameters Affect Neural Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/d296c101daa88a51f6ca8cfc1ac79b50-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_d34ab169,
 abstract = {We have used a neural network to compute corrections for images written by electron beams to eliminate the proximity effects caused by electron scattering. Iterative methods are effective, but require prohibitively computation time. We have instead trained a neural network to perform equivalent corrections, resulting in a significant speed-up. We have examined hardware implementations using both analog and digital electronic networks. Both had an acceptably small error of 0.5% compared to the iterative results. Additionally, we verified that the neural network correctly generalized the solution of the problem to include patterns not contained in its training set. We have experimentally verified this approach on a Cambridge Instruments EBMF 10.5 exposure system.},
 author = {Frye, Robert and Cummings, Kevin and Rietman, Edward},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d34ab169b70c9dcd35e62896010cd9ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d34ab169b70c9dcd35e62896010cd9ff-Metadata.json},
 openalex = {W2128971222},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d34ab169b70c9dcd35e62896010cd9ff-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Proximity Effect Corrections in Electron Beam Lithography Using a Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/d34ab169b70c9dcd35e62896010cd9ff-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_d709f38e,
 abstract = {In a previous paper (Touretzky & Wheeler, 1990a) we showed how adding a clustering operation to a connectionist phonology model produced a parallel processing account of certain iterative phenomena. In this paper we show how the addition of a second structuring primitive, syllabification, greatly increases the power of the model. We present examples from a non-Indo-European language that appear to require rule ordering to at least a depth of four. By adding syllabification circuitry to structure the model's perception of the input string, we are able to handle these examples with only two derivational steps. We conclude that in phonology, derivation can be largely replaced by structuring.},
 author = {Touretzky, David and Wheeler, Deirdre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d709f38ef758b5066ef31b18039b8ce5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d709f38ef758b5066ef31b18039b8ce5-Metadata.json},
 openalex = {W2156332683},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d709f38ef758b5066ef31b18039b8ce5-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Exploiting Syllable Structure in a Connectionist Phonology Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/d709f38ef758b5066ef31b18039b8ce5-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_d9fc5b73,
 abstract = {This is a summary of results with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned forward model of the world. We describe and show results for two Dyna architectures, Dyna-AHC and Dyna-Q. Using a navigation task, results are shown for a simple Dyna-AHC system which simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. We show that Dyna-Q architectures (based on Watkins's Q-learning) are easy to adapt for use in changing environments.},
 author = {Sutton, Richard S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d9fc5b73a8d78fad3d6dffe419384e70-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d9fc5b73a8d78fad3d6dffe419384e70-Metadata.json},
 openalex = {W2114384389},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/d9fc5b73a8d78fad3d6dffe419384e70-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_dc912a25,
 abstract = {A novel learning control architecture is used for navigation. A sophisticated test-bed is used to simulate a cylindrical robot with a sonar belt in a planar environment. The task is short-range homing in the presence of obstacles. The robot receives no global information and assumes no comprehensive world model. Instead the robot receives only sensory information which is inherently limited. A connectionist architecture is presented which incorporates a large amount of a priori knowledge in the form of hard-wired networks, architectural constraints, and initial weights. Instead of hard-wiring static potential fields from object models, my architecture learns sensor-based potential fields, automatically adjusting them to avoid local minima and to produce efficient homing trajectories. It does this without object models using only sensory information. This research demonstrates the use of a large modular architecture on a difficult task.},
 author = {Bachrach, Jonathan R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/dc912a253d1e9ba40e2c597ed2376640-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/dc912a253d1e9ba40e2c597ed2376640-Metadata.json},
 openalex = {W2141411709},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Connectionist Learning Control Architecture for Navigation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/dc912a253d1e9ba40e2c597ed2376640-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_ddb30680,
 abstract = {Given some training data how should we choose a particular network classifier from a family of networks of different complexities? In this paper we discuss how the application of stochastic complexity theory to classifier design problems can provide some insights into this problem. In particular we introduce the notion of admissible models whereby the complexity of models under consideration is affected by (among other factors) the class entropy, the amount of training data, and our prior belief. In particular we discuss the implications of these results with respect to neural architectures and demonstrate the approach on real data from a medical diagnosis task.},
 author = {Smyth, Padhraic},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ddb30680a691d157187ee1cf9e896d03-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ddb30680a691d157187ee1cf9e896d03-Metadata.json},
 openalex = {W2113825526},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {On Stochastic Complexity and Admissible Models for Neural Network Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/ddb30680a691d157187ee1cf9e896d03-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_e0c64119,
 abstract = {We show that a simple spin system biased at its critical point can encode spatial characteristics of external signals, such as the dimensions of objects in the visual field, in the temporal correlation functions of individual spins. Qualitative arguments suggest that regularly firing neurons should be described by a planar spin of unit length, and such XY models exhibit critical dynamics over a broad range of parameters. We show how to extract these spins from spike trains and then measure the interaction Hamiltonian using simulations of small dusters of cells. Static correlations among spike trains obtained from simulations of large arrays of cells are in agreement with the predictions from these Hamiltonians, and dynamic correlations display the predicted encoding of spatial information. We suggest that this novel representation of object dimensions in temporal correlations may be relevant to recent experiments on oscillatory neural firing in the visual cortex.},
 author = {Kruglyak, Leonid and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e0c641195b27425bb056ac56f8953d24-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e0c641195b27425bb056ac56f8953d24-Metadata.json},
 openalex = {W2144201170},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Analog Computation at a Critical Point: A Novel Function for Neuronal Oscillations?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_e46de7e1,
 abstract = {Neural network algorithms have proven useful for recognition of individual, segmented characters. However, their recognition accuracy has been limited by the accuracy of the underlying segmentation algorithm. Conventional, rule-based segmentation algorithms encounter difficulty if the characters are touching, broken, or noisy. The problem in these situations is that often one cannot properly segment a character until it is recognized yet one cannot properly recognize a character until it is segmented. We present here a neural network algorithm that simultaneously segments and recognizes in an integrated system. This algorithm has several novel features: it uses a supervised learning algorithm (backpropagation), but is able to take position-independent information as targets and self-organize the activities of the units in a competitive fashion to infer the positional information. We demonstrate this ability with overlapping handprinted numerals.},
 author = {Keeler, James and Rumelhart, David and Leow, Wee},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Metadata.json},
 openalex = {W2115240329},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Integrated Segmentation and Recognition of Hand-Printed Numerals},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_e7b24b11,
 abstract = {The Neural Prediction Model is the speech recognition model based on pattern prediction by multilayer perceptrons. Its effectiveness was confirmed by the speaker-independent digit recognition experiments. This paper presents an improvement in the model and its application to large vocabulary speech recognition, based on subword units. The improvement involves an introduction of backward prediction, which further improves the prediction accuracy of the original model with only forward prediction. In application of the model to speaker-dependent large vocabulary speech recognition, the demi-syllable unit is used as a subword recognition unit. Experimental results indicated a 95.2% recognition accuracy for a 5000 word test set and the effectiveness was confirmed for the proposed model improvement and the demi-syllable subword units.},
 author = {Iso, Ken-ichi and Watanabe, Takao},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Metadata.json},
 openalex = {W2394543670},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Speech recognition using demi-syllable neural prediction model.},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_eddea82a,
 abstract = {A three-layered neural network model was used to explore the organization of the vestibulo-ocular reflex (VOR). The dynamic model was trained using recurrent back-propagation to produce compensatory, long duration eye muscle motoneuron outputs in response to short duration vestibular afferent head velocity inputs. The network learned to produce this response prolongation, known as velocity storage, by developing complex, lateral inhibitory interactions among the interneurons. These had the low baseline, long time constant, rectified and skewed responses that are characteristic of real VOR interneurons. The model suggests that all of these features are interrelated and result from lateral inhibition.},
 author = {Anastasio, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/eddea82ad2755b24c4e168c5fc2ebd40-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/eddea82ad2755b24c4e168c5fc2ebd40-Metadata.json},
 openalex = {W2145875875},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Recurrent Neural Network Model of Velocity Storage in the Vestibulo-Ocular Reflex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_eed5af6a,
 abstract = {Robustness is a commonly bruited property of neural networks; in particular, a folk theorem in neural computation asserts that neural networks--in contexts with large interconnectivity--continue to function efficiently, albeit with some degradation, in the presence of component damage or loss. A second folk theorem in such contexts asserts that dense interconnectivity between neural elements is a sine qua non for the efficient usage of resources. These premises are formally examined in this communication in a setting that invokes the notion of the devil in the network as an agent that produces sparsity by snipping connections.},
 author = {Biswas, Sanjay and Venkatesh, Santosh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/eed5af6add95a9a6f1252739b1ad8c24-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/eed5af6add95a9a6f1252739b1ad8c24-Metadata.json},
 openalex = {W2114801581},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/eed5af6add95a9a6f1252739b1ad8c24-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Devil and the Network: What Sparsity Implies to Robustness and Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/eed5af6add95a9a6f1252739b1ad8c24-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_ef0d3930,
 abstract = {Feature selective cells in the primary visual cortex of several species are organized in hierarchical topographic maps of stimulus features like position in visual space, orientation ocular dominance. In order to understand and describe their structure and their development, we investigate a self-organizing neural network model based on the feature map algorithm. The model explains map formation as a dimension-reducing mapping from a high-dimensional feature space onto a two-dimensional lattice, such that similarity between features (or feature combinations) is translated into spatial proximity between the corresponding feature selective cells. The model is able to reproduce several aspects of the structure of cortical maps in the visual cortex.},
 author = {Obermayer, Klaus and Ritter, Helge and Schulten, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ef0d3930a7b6c95bd2b32ed45989c61f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ef0d3930a7b6c95bd2b32ed45989c61f-Metadata.json},
 openalex = {W2136757720},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Development and Spatial Structure of Cortical Feature Maps: A Model Study},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/ef0d3930a7b6c95bd2b32ed45989c61f-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_efe93778,
 abstract = {Empirically, generalization between a training and a test falls off in close approximation to an exponential decay function of distance between the two stimuli in the stimulus obtained by multidimensional scaling. Mathematically, this result is derivable from the assumption that an individual takes the training to belong to a region that includes that but is otherwise of unknown location, size, and shape in the space (Shepard, 1987). As the individual gains additional information about the consequential region--by finding other stimuli to be consequential or not--the theory predicts the shape of the generalization function to change toward the function relating actual probability of the consequence to location in the space. This paper describes a natural connectionist implementation of the theory, and illustrates how implications of the theory for generalization, discrimination, and classification learning can be explored by connectionist simulation.},
 author = {Shepard, Roger and Kannappan, Sheila},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/efe937780e95574250dabe07151bdc23-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/efe937780e95574250dabe07151bdc23-Metadata.json},
 openalex = {W2160913205},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/efe937780e95574250dabe07151bdc23-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Connectionist Implementation of a Theory of Generalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/efe937780e95574250dabe07151bdc23-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_f2fc9902,
 abstract = {In this paper we show that discrete affine wavelet transforms can provide a tool for the analysis and synthesis of standard feedforward neural networks. It is shown that wavelet frames for L2(IR) can be constructed based upon sigmoids. The spatia-spectral localization property of wavelets can be exploited in defining the topology and determining the weights of a feedforward network. Training a network constructed using the synthesis procedure described here involves minimization of a convex cost functional and therefore avoids pitfalls inherent in standard backpropagation algorithms. Extension of these methods to L2(IRN) is also discussed.},
 author = {Pati, Y. and Krishnaprasad, P.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f2fc990265c712c49d51a18a32b39f0c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f2fc990265c712c49d51a18a32b39f0c-Metadata.json},
 openalex = {W2141556666},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Discrete Affine Wavelet Transforms For Anaylsis And Synthesis Of Feedfoward Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_f4f6dce2,
 abstract = {It is shown that the LVQ (learning vector quantization) learning algorithm converges to locally asymptotic stable equilibria of an ordinary differential equation. It is demonstrated that the learning algorithm performs stochastic approximation. Convergence of the Voronoi vectors is guaranteed under the appropriate conditions on the underlying statistics of the classification problem. The authors also present a modification to the learning algorithm which, it is argued, results in convergence of the LVQ for a larger set of initial conditions. Finally, it is shown that the LVQ is a general histogram classifier and that its risk converges to the Bayesian optimal risk as the appropriate parameters go to infinity with the number of past observations.< <ETX xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">&gt;</ETX>},
 author = {Baras, John and LaVigna, Anthony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f4f6dce2f3a0f9dada0c2b5b66452017-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f4f6dce2f3a0f9dada0c2b5b66452017-Metadata.json},
 openalex = {W2136637968},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f4f6dce2f3a0f9dada0c2b5b66452017-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Convergence of a neural network classifier},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/f4f6dce2f3a0f9dada0c2b5b66452017-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_f61d6947,
 abstract = {We formulate the problem of optimizing the sampling of natural images using an array of linear filters. Optimization of information capacity is constrained by the noise levels of the individual channels and by a penalty for the construction of long-range interconnections in the array. At low signal-to-noise ratios the optimal filter characteristics correspond to bound states of a Schrodinger equation in which the signal spectrum plays the role of the potential. The resulting optimal filters are remarkably similar to those observed in the mammalian visual cortex and the retinal ganglion cells of lower vertebrates. The observed scale invariance of natural images plays an essential role in this construction.},
 author = {Bialek, William and Ruderman, Daniel and Zee, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f61d6947467ccd3aa5af24db320235dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f61d6947467ccd3aa5af24db320235dd-Metadata.json},
 openalex = {W2167511288},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Sampling of Natural Images: A Design Principle for the Visual System},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/f61d6947467ccd3aa5af24db320235dd-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_f73b76ce,
 abstract = {The performance of seven minimization algorithms are compared on five neural network problems. These include a variable-step-size algorithm, conjugate gradient, and several methods with explicit analytic or numerical approximations to the Hessian.},
 author = {Rohwer, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f73b76ce8949fe29bf2a537cfa420e8f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f73b76ce8949fe29bf2a537cfa420e8f-Metadata.json},
 openalex = {W2170667501},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f73b76ce8949fe29bf2a537cfa420e8f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Time Trials on Second-Order and Variable-Learning-Rate Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/f73b76ce8949fe29bf2a537cfa420e8f-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_f74909ac,
 abstract = {We describe a multi-network, or modular, connectionist architecture that captures that fact that many tasks have structure at a level of granularity intermediate to that assumed by local and global function approximation schemes. The main innovation of the architecture is that it combines associative and competitive learning in order to learn task decompositions. A task decomposition is discovered by forcing the networks comprising the architecture to compete to learn the training patterns. As a result of the competition, different networks learn different training patterns and, thus, learn to partition the input space. The performance of the architecture on a what and where vision task and on a multi-payload robotics task are presented.},
 author = {Jacobs, Robert and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f74909ace68e51891440e4da0b65a70c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f74909ace68e51891440e4da0b65a70c-Metadata.json},
 openalex = {W2135995262},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f74909ace68e51891440e4da0b65a70c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A competitive modular connectionist architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/f74909ace68e51891440e4da0b65a70c-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_f85454e8,
 abstract = {In response to a puff of wind, the American cockroach turns away and runs. The circuit underlying the initial turn of this escape response consists of three populations of individually identifiable nerve cells and appears to employ distributed representations in its operation. We have reconstructed several neuronal and behavioral properties of this system using simplified neural network models and the backpropagation learning algorithm constrained by known structural characteristics of the circuitry. In order to test and refine the model, we have also compared the model's responses to various lesions with the insect's responses to similar lesions.},
 author = {Beer, R.D. and Kacmarcik, G. and Ritzmann, R.E. and Chiel, H.J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f85454e8279be180185cac7d243c5eb3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f85454e8279be180185cac7d243c5eb3-Metadata.json},
 openalex = {W2128671594},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A Model of Distributed Sensorimotor Control in the Cockroach Escape Turn},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/f85454e8279be180185cac7d243c5eb3-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_f8c1f23d,
 abstract = {A system for color correction has been designed, built, and tested successfully; the essential components are three custom chips built using subthreshold analog CMOS VLSI. The system, based on Land's Retinex theory of color constancy, produces colors similar in many respects to those produced by the visual system. Resistive grids implemented in analog VLSI perform the smoothing operation central to the algorithm at video rates. With the electronic system, the strengths and weaknesses of the algorithm are explored.},
 author = {Moore, Andrew and Allman, John and Fox, Geoffrey and Goodman, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Metadata.json},
 openalex = {W2096674049},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {A VLSI Neural Network for Color Constancy},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_f9b902fc,
 abstract = {This paper presents a neural network (NN) approach to the problem of stereopsis. The correspondence problem (finding the correct matches between the pixels of the epipolar lines of the stereo pair from amongst all the possible matches) is posed as a non-iterative many-to-one mapping. A two-layer feed forward NN architecture is developed to learn and code this nonlinear and complex mapping using the back-propagation learning rule and a training set. The important aspect of this technique is that none of the typical constraints such as uniqueness and continuity are explicitly imposed. All the applicable constraints are learned and internally coded by the NN enabling it to be more flexible and more accurate than the existing methods. The approach is successfully tested on several random-dot stereograms. It is shown that the net can generalize its learned mapping to cases outside its training set. Advantages over the Marr-Poggio Algorithm are discussed and it is shown that the NN performance is superior.},
 author = {Khotanzad, Alireza and Lee, Ying-Wung},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f9b902fc3289af4dd08de5d1de54f68f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f9b902fc3289af4dd08de5d1de54f68f-Metadata.json},
 openalex = {W2171241193},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/f9b902fc3289af4dd08de5d1de54f68f-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Stereopsis by a Neural Network Which Learns the Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/f9b902fc3289af4dd08de5d1de54f68f-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_faa9afea,
 abstract = {In this work we describe a new method that adjusts time-delays and the widths of time-windows in artificial neural networks automatically. The input of the units are weighted by a gaussian input-window over time which allows the learning rules for the delays and widths to be derived in the same way as it is used for the weights. Our results on a phoneme classification task compare well with results obtained with the TDNN by Waibel et al., which was manually optimized for the same task.},
 author = {Bodenhausen, Ulrich and Waibel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/faa9afea49ef2ff029a833cccc778fd0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/faa9afea49ef2ff029a833cccc778fd0-Metadata.json},
 openalex = {W2139232801},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Tempo 2 Algorithm: Adjusting Time-Delays By Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/faa9afea49ef2ff029a833cccc778fd0-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_fb7b9ffa,
 abstract = {We apply the theory of Tishby, Levin, and Solla (TLS) to two problems. First we analyze an elementary problem for which we find the predictions consistent with conventional statistical results. Second we numerically examine the more realistic problem of training a competitive net to learn a probability density from samples. We find TLS useful for predicting average training behavior.},
 author = {Bilbro, Griff and van den Bout, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Metadata.json},
 openalex = {W2153977182},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Learning Theory and Experiments with Competitive Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/fb7b9ffa5462084c5f4e7e85a093e6d7-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_fccb60fb,
 abstract = {We describe a computational model of the development and regeneration of specific eye-brain circuits. The model comprises a self-organizing map-forming network which uses local Hebb rules, constrained by (genetically determined) molecular markers. Various simulations of the development and regeneration of eye-brain maps in fish and frogs are described, in particular successful simulations of experiments by Schmidt-Cicerone-Easter; Meyer; and Yoon.},
 author = {Cowan, Jack and Friedman, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fccb60fb512d13df5083790d64c4d5dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fccb60fb512d13df5083790d64c4d5dd-Metadata.json},
 openalex = {W2151136761},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fccb60fb512d13df5083790d64c4d5dd-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Further Studies of a Model for the Development and Regeneration of Eye-Brain Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/fccb60fb512d13df5083790d64c4d5dd-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_fe73f687,
 abstract = {Recurrent Cascade-Correlation (RCC) is a recurrent version of the Cascade-Correlation learning architecture of Fahlman and Lebiere [Fahlman, 1990]. RCC can learn from examples to map a sequence of inputs into a desired sequence of outputs. New hidden units with recurrent connections are added to the network as needed during training. In effect, the network builds up a finite-state machine tailored specifically for the current problem. RCC retains the advantages of Cascade-Correlation: fast learning, good generalization, automatic construction of a near-minimal multi-layered network, and incremental training.},
 author = {Fahlman, Scott},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fe73f687e5bc5280214e0486b273a5f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fe73f687e5bc5280214e0486b273a5f9-Metadata.json},
 openalex = {W2129831132},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/fe73f687e5bc5280214e0486b273a5f9-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {The Recurrent Cascade-Correlation Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/fe73f687e5bc5280214e0486b273a5f9-Abstract.html},
 volume = {3},
 year = {1990}
}

@inproceedings{NIPS1990_ffd52f3c,
 abstract = {We develop a sequential adaptation algorithm for radial basis function (RBF) neural networks of Gaussian nodes, based on the method of successive F-Projections. This method makes use of each observation efficiently in that the network mapping function so obtained is consistent with that information and is also optimal in the least L2-norm sense. The RBF network with the F-Projections adaptation algorithm was used for predicting a chaotic time-series. We compare its performance to an adaptation scheme based on the method of stochastic approximation, and show that the F-Projections algorithm converges to the underlying model much faster.},
 author = {Kadirkamanathan, V. and Niranjan, M. and Fallside, F.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ffd52f3c7e12435a724a8f30fddadd9c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ffd52f3c7e12435a724a8f30fddadd9c-Metadata.json},
 openalex = {W2123243460},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1990/file/ffd52f3c7e12435a724a8f30fddadd9c-Paper.pdf},
 publisher = {Morgan-Kaufmann},
 title = {Sequential Adaptation of Radial Basis Function Neural Networks and its Application to Time-series Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/hash/ffd52f3c7e12435a724a8f30fddadd9c-Abstract.html},
 volume = {3},
 year = {1990}
}
