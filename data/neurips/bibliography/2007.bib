@inproceedings{NIPS2007_0084ae4b,
 abstract = {The problem of obtaining the maximum a posteriori estimate of a general discrete random field (i.e. a random field defined using a finite and discrete set of labels) is known to be NP-hard. However, due to its central importance in many applications, several approximate algorithms have been proposed in the literature. In this paper, we present an analysis of three such algorithms based on convex relaxations: (i) LP-S: the linear programming (LP) relaxation proposed by Schlesinger [20] for a special case and independently in [4, 12, 23] for the general case; (ii) QP-RL: the quadratic programming (QP) relaxation by Ravikumar and Lafferty [18]; and (iii) SOCP-MS: the second order cone programming (SOCP) relaxation first proposed by Muramatsu and Suzuki [16] for two label problems and later extended in [14] for a general label set.

We show that the SOCP-MS and the QP-RL relaxations are equivalent. Furthermore, we prove that despite the flexibility in the form of the constraints/objective function offered by QP and SOCP, the LP-S relaxation strictly dominates (i.e. provides a better approximation than) QP-RL and SOCP-MS. We generalize these results by defining a large class of SOCP (and equivalent QP) relaxations which is dominated by the LP-S relaxation. Based on these results we propose some novel SOCP relaxations which strictly dominate the previous approaches.},
 author = {Mudigonda, Pawan and Kolmogorov, Vladimir and Torr, Philip},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0084ae4bc24c0795d1e6a4f58444d39b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0084ae4bc24c0795d1e6a4f58444d39b-Metadata.json},
 openalex = {W2097525406},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0084ae4bc24c0795d1e6a4f58444d39b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0084ae4bc24c0795d1e6a4f58444d39b-Supplemental.zip},
 title = {An Analysis of Convex Relaxations for MAP Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/0084ae4bc24c0795d1e6a4f58444d39b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_013a006f,
 abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
 author = {Rahimi, Ali and Recht, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Metadata.json},
 openalex = {W2144902422},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_0336dcba,
 abstract = {AbstractAs an alternative to variable selection or shrinkage in high-dimensional regression, we propose to randomly compress the predictors prior to analysis. This dramatically reduces storage and computational bottlenecks, performing well when the predictors can be projected to a low-dimensional linear subspace with minimal loss of information about the response. As opposed to existing Bayesian dimensionality reduction approaches, the exact posterior distribution conditional on the compressed data is available analytically, speeding up computation by many orders of magnitude while also bypassing robustness issues due to convergence and mixing problems with MCMC. Model averaging is used to reduce sensitivity to the random projection matrix, while accommodating uncertainty in the subspace dimension. Strong theoretical support is provided for the approach by showing near parametric convergence rates for the predictive density in the large p small n asymptotic paradigm. Practical performance relative to competitors is illustrated in simulations and real data applications.KeywordsCompressed sensingData compressionDimensionality reductionLarge p, small nRandom projectionSparsitySufficient dimension reduction. Additional informationNotes on contributorsRajarshi GuhaniyogiRajarshi Guhaniyogi is Assistant Professor, Department of Applied Mathematics & Statistics, Baskin School of Engineering, SOE2, University of California Santa Cruz, 1156 High St., Santa Cruz, CA 95064 (E-mail: rg124@stat.duke.edu). David B. Dunson is Professor, Department of Statistical Science, 219A Old Chemistry Building, Box 90251, Duke University, Durham, NC 27708-0251 (E-mail: dunson@stat.duke.edu).David B. DunsonRajarshi Guhaniyogi is Assistant Professor, Department of Applied Mathematics & Statistics, Baskin School of Engineering, SOE2, University of California Santa Cruz, 1156 High St., Santa Cruz, CA 95064 (E-mail: rg124@stat.duke.edu). David B. Dunson is Professor, Department of Statistical Science, 219A Old Chemistry Building, Box 90251, Duke University, Durham, NC 27708-0251 (E-mail: dunson@stat.duke.edu).},
 author = {Zhou, Shuheng and Wasserman, Larry and Lafferty, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0336dcbab05b9d5ad24f4333c7658a0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0336dcbab05b9d5ad24f4333c7658a0e-Metadata.json},
 openalex = {W1999351024},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Compressed Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_03afdbd6,
 abstract = {Simulated annealing is a popular method for approaching the solution of a global optimization problem. Existing results on its performance apply to discrete combinatorial optimization where the optimization variables can assume only a finite set of possible values. We introduce a new general formulation of simulated annealing which allows one to guarantee finite-time performance in the optimization of functions of continuous variables. The results hold universally for any optimization problem on a bounded domain and establish a connection between simulated annealing and up-to-date theory of convergence of Markov chain Monte Carlo methods on continuous domains. This work is inspired by the concept of finite-time learning with known accuracy and confidence developed in statistical learning theory.},
 author = {Lecchini-visintini, Andrea and Lygeros, John and Maciejowski, Jan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/03afdbd66e7929b125f8597834fa83a4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/03afdbd66e7929b125f8597834fa83a4-Metadata.json},
 openalex = {W2116048126},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/03afdbd66e7929b125f8597834fa83a4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/03afdbd66e7929b125f8597834fa83a4-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_061412e4,
 abstract = {It is becoming increasingly important to learn from a partially-observed random matrix and predict its missing elements. We assume that the entire matrix is a single sample drawn from a matrix-variate t distribution and suggest a matrix-variate t model (MVTM) to predict those missing elements. We show that MVTM generalizes a range of known probabilistic models, and automatically performs model selection to encourage sparse predictive models. Due to the non-conjugacy of its prior, it is difficult to make predictions by computing the mode or mean of the posterior distribution. We suggest an optimization method that sequentially minimizes a convex upper-bound of the log-likelihood, which is very efficient and scalable. The experiments on a toy data and EachMovie dataset show a good predictive accuracy of the model.},
 author = {Zhu, Shenghuo and Yu, Kai and Gong, Yihong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/061412e4a03c02f9902576ec55ebbe77-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/061412e4a03c02f9902576ec55ebbe77-Metadata.json},
 openalex = {W2114744608},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Predictive Matrix-Variate t Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/061412e4a03c02f9902576ec55ebbe77-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_062ddb6c,
 abstract = {Variational methods are frequently used to approximate or bound the partition or likelihood function of a Markov random field. Methods based on mean field theory are guaranteed to provide lower bounds, whereas certain types of convex relaxations provide upper bounds. In general, loopy belief propagation (BP) provides often accurate approximations, but not bounds. We prove that for a class of attractive binary models, the so-called Bethe approximation associated with any fixed point of loopy BP always lower bounds the true likelihood. Empirically, this bound is much tighter than the naive mean field bound, and requires no further work than running BP. We establish these lower bounds using a loop series expansion due to Chertkov and Chernyak, which we show can be derived as a consequence of the tree reparameterization characterization of BP fixed points.},
 author = {Willsky, Alan and Sudderth, Erik and Wainwright, Martin J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/062ddb6c727310e76b6200b7c71f63b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/062ddb6c727310e76b6200b7c71f63b5-Metadata.json},
 openalex = {W2101478306},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Loop Series and Bethe Variational Bounds in Attractive Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/062ddb6c727310e76b6200b7c71f63b5-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_069d3bb0,
 abstract = {Recently, we have introduced a novel approach to dynamic programming and reinforcement learning that is based on maintaining explicit representations of stationary distributions instead of value functions. In this paper, we investigate the convergence properties of these dual algorithms both theoretically and empirically, and show how they can be scaled up by incorporating function approximation.},
 author = {Wang, Tao and Bowling, Michael and Schuurmans, Dale and Lizotte, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/069d3bb002acd8d7dd095917f9efe4cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/069d3bb002acd8d7dd095917f9efe4cb-Metadata.json},
 openalex = {W2158191646},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/069d3bb002acd8d7dd095917f9efe4cb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Stable Dual Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/069d3bb002acd8d7dd095917f9efe4cb-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_072b030b,
 abstract = {We study boosting in the filtering setting, where the booster draws examples from an oracle instead of using a fixed training set and so may train efficiently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the first proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the filtering setting. Our proofs demonstrate the algorithm's strong theoretical properties for both classification and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overfitting than batch boosters in conditional probability estimation and proves competitive in classification.},
 author = {Bradley, Joseph K and Schapire, Robert E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/072b030ba126b2f4b2374f342be9ed44-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/072b030ba126b2f4b2374f342be9ed44-Metadata.json},
 openalex = {W2154436219},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/072b030ba126b2f4b2374f342be9ed44-Supplemental.zip},
 title = {FilterBoost: Regression and Classification on Large Datasets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_073b00ab,
 abstract = {Content-based image suggestion (CBIS) targets the recommendation of products based on user preferences on the visual content of images. In this paper, we motivate both feature selection and model order identification as two key issues for a successful CBIS. We propose a generative model in which the visual features and users are clustered into separate classes. We identify the number of both user and image classes with the simultaneous selection of relevant visual features using the message length approach. The goal is to ensure an accurate prediction of ratings for multidimensional non-Gaussian and continuous image descriptors. Experiments on a collected data have demonstrated the merits of our approach.},
 author = {Boutemedjet, Sabri and Ziou, Djemel and Bouguila, Nizar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/073b00ab99487b74b63c9a6d2b962ddc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/073b00ab99487b74b63c9a6d2b962ddc-Metadata.json},
 openalex = {W2159760567},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/073b00ab99487b74b63c9a6d2b962ddc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/073b00ab99487b74b63c9a6d2b962ddc-Supplemental.zip},
 title = {Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/073b00ab99487b74b63c9a6d2b962ddc-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_0768281a,
 abstract = {We present the first truly polynomial algorithm for PAC-learning the structure of bounded-treewidth junction trees - an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efficient exact inference. For a constant treewidth, our algorithm has polynomial time and sample complexity. If a junction tree with sufficiently strong intra-clique dependencies exists, we provide strong theoretical guarantees in terms of KL divergence of the result from the true distribution. We also present a lazy extension of our approach that leads to very significant speed ups in practice, and demonstrate the viability of our method empirically, on several real world datasets. One of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of variables with only polynomially many mutual information computations on fixed-size subsets of variables, if the underlying distribution can be approximated by a bounded-treewidth junction tree.},
 author = {Chechetka, Anton and Guestrin, Carlos},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0768281a05da9f27df178b5c39a51263-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0768281a05da9f27df178b5c39a51263-Metadata.json},
 openalex = {W2125984043},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0768281a05da9f27df178b5c39a51263-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Principled Learning of Thin Junction Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/0768281a05da9f27df178b5c39a51263-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_08d98638,
 abstract = {Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold'em with as many as 1012 states, two orders of magnitude larger than previous methods.},
 author = {Zinkevich, Martin and Johanson, Michael and Bowling, Michael and Piccione, Carmelo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Metadata.json},
 openalex = {W2103315867},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Supplemental.zip},
 title = {Regret Minimization in Games with Incomplete Information},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/08d98638c6fcd194a4b1e6992063e944-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_08fe2621,
 abstract = {We argue that in many circumstances, human observers evaluate sensory evidence simultaneously under multiple hypotheses regarding the physical process that has generated the sensory information. In such situations, inference can be optimal if an observer combines the evaluation results under each hypothesis according to the probability that the associated hypothesis is correct. However, a number of experimental results reveal suboptimal behavior and may be explained by assuming that once an observer has committed to a particular hypothesis, subsequent evaluation is based on that hypothesis alone. That is, observers sacrifice optimality in order to ensure self-consistency. We formulate this behavior using a conditional Bayesian observer model, and demonstrate that it can account for psychophysical data from a recently reported perceptual experiment in which strong biases in perceptual estimates arise as a consequence of a preceding decision. Not only does the model provide quantitative predictions of subjective responses in variants of the original experiment, but it also appears to be consistent with human responses to cognitive dissonance.},
 author = {Stocker, Alan A and Simoncelli, Eero},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/08fe2621d8e716b02ec0da35256a998d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/08fe2621d8e716b02ec0da35256a998d-Metadata.json},
 openalex = {W2098378005},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/08fe2621d8e716b02ec0da35256a998d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Bayesian Model of Conditioned Perception.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/08fe2621d8e716b02ec0da35256a998d-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_0bb4aec1,
 abstract = {We address the problem of adaptive sensor control in dynamic resource-constrained sensor networks. We focus on a meteorological sensing network comprising radars that can perform sector scanning rather than always scanning 360 degrees. We compare three sector scanning strategies. The sit-and-spin strategy always scans 360 degrees. The limited lookahead strategy additionally uses the expected environmental state K decision epochs in the future, as predicted from Kalman filters, in its decision-making. The full lookahead strategy uses all expected future states by casting the problem as a Markov decision process and using reinforcement learning to estimate the optimal scan strategy. We show that the main benefits of using a lookahead strategy are when there are multiple meteorological phenomena in the environment, and when the maximum radius of any phenomenon is sufficiently smaller than the radius of the radars. We also show that there is a trade-off between the average quality with which a phenomenon is scanned and the number of decision epochs before which a phenomenon is rescanned.},
 author = {Manfredi, Victoria and Kurose, Jim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0bb4aec1710521c12ee76289d9440817-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0bb4aec1710521c12ee76289d9440817-Metadata.json},
 openalex = {W2117139144},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Scan Strategies for Meteorological Radars},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/0bb4aec1710521c12ee76289d9440817-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_0d3180d6,
 abstract = {This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation-estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.},
 author = {Bottou, L\'{e}on and Bousquet, Olivier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Metadata.json},
 openalex = {W2113651538},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Tradeoffs of Large-Scale Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/0d3180d672e08b4c5312dcdafdf6ef36-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_0d352b4d,
 abstract = {Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. However, the processes that make such temporal judgements possible are unknown. A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. Such processes need not be specific to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process.},
 author = {Ahrens, Misha and Sahani, Maneesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d352b4d3a317e3eae221199fdb49651-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d352b4d3a317e3eae221199fdb49651-Metadata.json},
 openalex = {W2166678142},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d352b4d3a317e3eae221199fdb49651-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Inferring Elapsed Time from Stochastic Neural Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/0d352b4d3a317e3eae221199fdb49651-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_0d7de1ac,
 abstract = {Can we leverage learning techniques to build a fast nearest-neighbor (ANN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures.},
 author = {Cayton, Lawrence and Dasgupta, Sanjoy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d7de1aca9299fe63f3e0041f02638a3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d7de1aca9299fe63f3e0041f02638a3-Metadata.json},
 openalex = {W2155982529},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d7de1aca9299fe63f3e0041f02638a3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A learning framework for nearest neighbor search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/0d7de1aca9299fe63f3e0041f02638a3-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_0f840be9,
 abstract = {Learning in real-world domains often requires to deal with continuous state and action spaces. Although many solutions have been proposed to apply Reinforcement Learning algorithms to continuous state problems, the same techniques can be hardly extended to continuous action spaces, where, besides the computation of a good approximation of the value function, a fast method for the identification of the highest-valued action is needed. In this paper, we propose a novel actor-critic approach in which the policy of the actor is estimated through sequential Monte Carlo methods. The importance sampling step is performed on the basis of the values learned by the critic, while the resampling step modifies the actor's policy. The proposed approach has been empirically compared to other learning algorithms into several domains; in this paper, we report results obtained in a control problem consisting of steering a boat across a river.},
 author = {Lazaric, Alessandro and Restelli, Marcello and Bonarini, Andrea},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0f840be9b8db4d3fbd5ba2ce59211f55-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0f840be9b8db4d3fbd5ba2ce59211f55-Metadata.json},
 openalex = {W2109102709},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/0f840be9b8db4d3fbd5ba2ce59211f55-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_1385974e,
 abstract = {In this paper, we study the ensemble clustering problem, where the input is in the form of multiple clustering solutions. The goal of ensemble clustering algorithms is to aggregate the solutions into one solution that maximizes the agreement in the input ensemble. We obtain several new results for this problem. Specifically, we show that the notion of agreement under such circumstances can be better captured using a 2D string encoding rather than a voting strategy, which is common among existing approaches. Our optimization proceeds by first constructing a non-linear objective function which is then transformed into a 0-1 Semidefinite program (SDP) using novel convexification techniques. This model can be subsequently relaxed to a polynomial time solvable SDP. In addition to the theoretical contributions, our experimental results on standard machine learning and synthetic datasets show that this approach leads to improvements not only in terms of the proposed agreement measure but also the existing agreement measures based on voting strategies. In addition, we identify several new application scenarios for this problem. These include combining multiple image segmentations and generating tissue maps from multiple-channel Diffusion Tensor brain images to identify the underlying structure of the brain.},
 author = {Singh, Vikas and Mukherjee, Lopamudra and Peng, Jiming and Xu, Jinhui},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1385974ed5904a438616ff7bdb3f7439-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1385974ed5904a438616ff7bdb3f7439-Metadata.json},
 openalex = {W2080795979},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1385974ed5904a438616ff7bdb3f7439-Supplemental.zip},
 title = {Ensemble clustering using semidefinite programming with applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/1385974ed5904a438616ff7bdb3f7439-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_138bb069,
 abstract = {Planning in partially observable environments remains a challenging problem, despite significant recent advances in offline approximation techniques. A few online methods have also been proposed recently, and proven to be remarkably scalable, but without the theoretical guarantees of their offline counterparts. Thus it seems natural to try to unify offline and online techniques, preserving the theoretical properties of the former, and exploiting the scalability of the latter. In this paper, we provide theoretical guarantees on an anytime algorithm for POMDPs which aims to reduce the error made by approximate offline value iteration algorithms through the use of an efficient online searching procedure. The algorithm uses search heuristics based on an error analysis of lookahead search, to guide the online search towards reachable beliefs with the most potential to reduce error. We provide a general theorem showing that these search heuristics are admissible, and lead to complete and ε-optimal algorithms. This is, to the best of our knowledge, the strongest theoretical result available for online POMDP solution methods. We also provide empirical evidence showing that our approach is also practical, and can find (provably) near-optimal solutions in reasonable time.},
 author = {Ross, Stephane and Pineau, Joelle and Chaib-draa, Brahim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/138bb0696595b338afbab333c555292a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/138bb0696595b338afbab333c555292a-Metadata.json},
 openalex = {W2096976789},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/138bb0696595b338afbab333c555292a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Theoretical Analysis of Heuristic Search Methods for Online POMDPs.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/138bb0696595b338afbab333c555292a-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_13f9896d,
 abstract = {Stability is a desirable characteristic for linear dynamical systems, but it is often ignored by algorithms that learn these systems from data. We propose a novel method for learning stable linear dynamical systems: we formulate an approximation of the problem as a convex program, start with a solution to a relaxed version of the program, and incrementally add constraints to improve stability. Rather than continuing to generate constraints until we reach a feasible solution, we test stability at each step; because the convex program is only an approximation of the desired problem, this early stopping rule can yield a higher-quality solution. We apply our algorithm to the task of learning dynamic textures from image sequences as well as to modeling biosurveillance drug-sales data. The constraint generation approach leads to noticeable improvement in the quality of simulated sequences. We compare our method to those of Lacy and Bernstein [1, 2], with positive results in terms of accuracy, quality of simulated sequences, and efficiency.},
 author = {Boots, Byron and Gordon, Geoffrey J and Siddiqi, Sajid},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/13f9896df61279c928f19721878fac41-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/13f9896df61279c928f19721878fac41-Metadata.json},
 openalex = {W2113592754},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/13f9896df61279c928f19721878fac41-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Constraint Generation Approach to Learning Stable Linear Dynamical Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/13f9896df61279c928f19721878fac41-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_149e9677,
 abstract = {Independent component analysis (ICA) is a powerful method to decouple signals. Most of the algorithms performing ICA do not consider the temporal correlations of the signal, but only higher moments of its amplitude distribution. Moreover, they require some preprocessing of the data (whitening) so as to remove second order correlations. In this paper, we are interested in understanding the neural mechanism responsible for solving ICA. We present an online learning rule that exploits delayed correlations in the input. This rule performs ICA by detecting joint variations in the firing rates of pre- and postsynaptic neurons, similar to a local rate-based Hebbian learning rule.},
 author = {Clopath, Claudia and Longtin, Andr\'{e} and Gerstner, Wulfram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/149e9677a5989fd342ae44213df68868-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/149e9677a5989fd342ae44213df68868-Metadata.json},
 openalex = {W2133236557},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/149e9677a5989fd342ae44213df68868-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/149e9677a5989fd342ae44213df68868-Supplemental.zip},
 title = {An online Hebbian learning rule that performs independent component analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/149e9677a5989fd342ae44213df68868-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_1595af64,
 abstract = {Natural sounds are structured on many time-scales. A typical segment of speech, for example, contains features that span four orders of magnitude: Sentences (~ 1 s); phonemes (~ 10-1 s); glottal pulses (~ 10-2 s); and formants (≤ 10-3 s). The auditory system uses information from each of these time-scales to solve complicated tasks such as auditory scene analysis [1]. One route toward understanding how auditory processing accomplishes this analysis is to build neuroscience-inspired algorithms which solve similar tasks and to compare the properties of these algorithms with properties of auditory processing. There is however a discord: Current machine-audition algorithms largely concentrate on the shorter time-scale structures in sounds, and the longer structures are ignored. The reason for this is two-fold. Firstly, it is a difficult technical problem to construct an algorithm that utilises both sorts of information. Secondly, it is computationally demanding to simultaneously process data both at high resolution (to extract short temporal information) and for long duration (to extract long temporal information). The contribution of this work is to develop a new statistical model for natural sounds that captures structure across a wide range of time-scales, and to provide efficient learning and inference algorithms. We demonstrate the success of this approach on a missing data task.},
 author = {Turner, Richard and Sahani, Maneesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1595af6435015c77a7149e92a551338e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1595af6435015c77a7149e92a551338e-Metadata.json},
 openalex = {W2147019955},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1595af6435015c77a7149e92a551338e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Modeling Natural Sounds with Modulation Cascade Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/1595af6435015c77a7149e92a551338e-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_19b65066,
 abstract = {We present a new and efficient semi-supervised training method for parameter estimation and feature selection in conditional random fields (CRFs). In real-world applications such as activity recognition, unlabeled sensor traces are relatively easy to obtain whereas labeled examples are expensive and tedious to collect. Furthermore, the ability to automatically select a small subset of discriminatory features from a large pool can be advantageous in terms of computational speed as well as accuracy. In this paper, we introduce the semi-supervised virtual evidence boosting (sVEB) algorithm for training CRFs - a semi-supervised extension to the recently developed virtual evidence boosting (VEB) method for feature selection and parameter learning. The objective function of sVEB combines the unlabeled conditional entropy with labeled conditional pseudo-likelihood. It reduces the overall system cost as well as the human labeling cost required during training, which are both important considerations in building real-world inference systems. Experiments on synthetic data and real activity traces collected from wearable sensors, illustrate that sVEB benefits from both the use of unlabeled data and automatic feature selection, and outperforms other semi-supervised approaches.},
 author = {Mahdaviani, Maryam and Choudhury, Tanzeem},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/19b650660b253761af189682e03501dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/19b650660b253761af189682e03501dd-Metadata.json},
 openalex = {W2165324305},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/19b650660b253761af189682e03501dd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/19b650660b253761af189682e03501dd-Supplemental.zip},
 title = {Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/19b650660b253761af189682e03501dd-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_1be3bc32,
 abstract = {We investigate quantile regression based on the pinball loss and the ∊-insensitive loss. For the pinball loss a condition on the data-generating distribution P is given that ensures that the conditional quantiles are approximated with respect to ‖ · ‖1. This result is then used to derive an oracle inequality for an SVM based on the pinball loss. Moreover, we show that SVMs based on the ∊-insensitive loss estimate the conditional median only under certain conditions on P.},
 author = {Christmann, Andreas and Steinwart, Ingo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1be3bc32e6564055d5ca3e5a354acbef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1be3bc32e6564055d5ca3e5a354acbef-Metadata.json},
 openalex = {W2172024582},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {How SVMs can estimate quantiles and the median},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/1be3bc32e6564055d5ca3e5a354acbef-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_1e6e0a04,
 abstract = {We propose a novel method for linear dimensionality reduction of manifold modeled data. First, we show that with a small number M of random projections of sample points in ℝN belonging to an unknown K-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number of random projections required is linear in K and logarithmic in N, meaning that K < M ≪ N. To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to significant potential savings in data acquisition, storage and transmission costs.},
 author = {Hegde, Chinmay and Wakin, Michael and Baraniuk, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1e6e0a04d20f50967c64dac2d639a577-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1e6e0a04d20f50967c64dac2d639a577-Metadata.json},
 openalex = {W2107179775},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1e6e0a04d20f50967c64dac2d639a577-Supplemental.zip},
 title = {Random Projections for Manifold Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/1e6e0a04d20f50967c64dac2d639a577-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_1f4477ba,
 abstract = {Recent experimental studies have focused on the specialization of different neural structures for different types of instrumental behavior. Recent theoretical work has provided normative accounts for why there should be more than one control system, and how the output of different controllers can be integrated. Two particlar controllers have been identified, one associated with a forward model and the prefrontal cortex and a second associated with computationally simpler, habitual, actor-critic methods and part of the striatum. We argue here for the normative appropriateness of an additional, but so far marginalized control system, associated with episodic memory, and involving the hippocampus and medial temporal cortices. We analyze in depth a class of simple environments to show that episodic control should be useful in a range of cases characterized by complexity and inferential noise, and most particularly at the very early stages of learning, long before habitization has set in. We interpret data on the transfer of control from the hippocampus to the striatum in the light of this hypothesis.},
 author = {Lengyel, M\'{a}t\'{e} and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1f4477bad7af3616c1f933a02bfabe4e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1f4477bad7af3616c1f933a02bfabe4e-Metadata.json},
 openalex = {W2112707476},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1f4477bad7af3616c1f933a02bfabe4e-Supplemental.zip},
 title = {Hippocampal Contributions to Control: The Third Way},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_20b02dc9,
 abstract = {In this paper we formulate a novel AND/OR graph representation capable of describing the different configurations of deformable articulated objects such as horses. The representation makes use of the summarization principle so that lower level nodes in the graph only pass on summary statistics to the higher level nodes. The probability distributions are invariant to position, orientation, and scale. We develop a novel inference algorithm that combined a bottom-up process for proposing configurations for horses together with a top-down process for refining and validating these proposals. The strategy of surround suppression is applied to ensure that the inference time is polynomial in the size of input data. The algorithm was applied to the tasks of detecting, segmenting and parsing horses. We demonstrate that the algorithm is fast and comparable with the state of the art approaches.},
 author = {Chen, Yuanhao and Zhu, Long and Lin, Chenxi and Zhang, Hongjiang and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/20b02dc95171540bc52912baf3aa709d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/20b02dc95171540bc52912baf3aa709d-Metadata.json},
 openalex = {W2132630629},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/20b02dc95171540bc52912baf3aa709d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Rapid Inference on a Novel AND/OR graph for Object Detection, Segmentation and Parsing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/20b02dc95171540bc52912baf3aa709d-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_20b5e1cf,
 abstract = {Incorporating invariances into a learning algorithm is a common problem in machine learning. We provide a convex formulation which can deal with arbitrary loss functions and arbitrary losses. In addition, it is a drop-in replacement for most optimization algorithms for kernels, including solvers of the SVMStruct family. The advantage of our setting is that it relies on column generation instead of modifying the underlying optimization problem directly.},
 author = {Teo, Choon and Globerson, Amir and Roweis, Sam and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/20b5e1cf8694af7a3c1ba4a87f073021-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/20b5e1cf8694af7a3c1ba4a87f073021-Metadata.json},
 openalex = {W2142992973},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/20b5e1cf8694af7a3c1ba4a87f073021-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Convex Learning with Invariances},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/20b5e1cf8694af7a3c1ba4a87f073021-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_2291d2ec,
 abstract = {We describe a novel noisy-logical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variables. The distribution is represented in terms of noisy-or's and noisy-and-not's of causal features which are conjunctions of the binary inputs. The standard noisy-or and noisy-and-not models, used in causal reasoning and artificial intelligence, are special cases of the noisy-logical distribution. We prove that the noisy-logical distribution is complete in the sense that it can represent all conditional distributions provided a sufficient number of causal factors are used. We illustrate the noisy-logical distribution by showing that it can account for new experimental findings on how humans perform causal reasoning in complex contexts. We speculate on the use of the noisy-logical distribution for causal reasoning and artificial intelligence.},
 author = {Yuille, Alan L and Lu, Hongjing},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Metadata.json},
 openalex = {W2146048365},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Noisy-Logical Distribution and its Application to Causal Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_22fb0cee,
 abstract = {We present a novel linear clustering framework (DIFFRAC) which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem. The large convex optimization problem is solved through a sequence of lower dimensional singular value decompositions. This framework has several attractive properties: (1) although apparently similar to K-means, it exhibits superior clustering performance than K-means, in particular in terms of robustness to noise. (2) It can be readily extended to non linear clustering if the discriminative cost function is based on positive definite kernels, and can then be seen as an alternative to spectral clustering. (3) Prior information on the partition is easily incorporated, leading to state-of-the-art performance for semi-supervised learning, for clustering or classification. We present empirical evaluations of our algorithms on synthetic and real medium-scale datasets.},
 author = {Bach, Francis and Harchaoui, Za\"{\i}d},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/22fb0cee7e1f3bde58293de743871417-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/22fb0cee7e1f3bde58293de743871417-Metadata.json},
 openalex = {W2108282816},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {DIFFRAC: a discriminative and flexible framework for clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/22fb0cee7e1f3bde58293de743871417-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_26337353,
 abstract = {We present a globally convergent method for regularized risk minimization problems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the unified framework we present tight convergence bounds, which show that our algorithm converges in O(1/∊) steps to ∊ precision for general convex problems and in O(log(1/∊)) steps for continuously differentiable problems. We demonstrate in experiments the performance of our approach.},
 author = {Le, Quoc and Smola, Alex and Vishwanathan, S.v.n.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/26337353b7962f533d78c762373b3318-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/26337353b7962f533d78c762373b3318-Metadata.json},
 openalex = {W2105636360},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/26337353b7962f533d78c762373b3318-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bundle Methods for Machine Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/26337353b7962f533d78c762373b3318-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_2823f479,
 abstract = {Bayesian model averaging, model selection and their approximations such as BIC are generally statistically consistent, but sometimes achieve slower rates of convergence than other methods such as AIC and leave-one-out cross-validation. On the other hand, these other methods can be inconsistent. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods. Based on this analysis we define the switch-distribution, a modification of the Bayesian model averaging distribution. We prove that in many situations model selection and prediction based on the switch-distribution is both consistent and achieves optimal convergence rates, thereby resolving the AIC-BIC dilemma. The method is practical; we give an efficient algorithm.},
 author = {Erven, Tim and Rooij, Steven and Gr\"{u}nwald, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2823f4797102ce1a1aec05359cc16dd9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2823f4797102ce1a1aec05359cc16dd9-Metadata.json},
 openalex = {W2115319432},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2823f4797102ce1a1aec05359cc16dd9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Catching Up Faster in Bayesian Model Selection and Model Averaging},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_2838023a,
 abstract = {Rare category detection is an open challenge for active learning, especially in the de-novo case (no labeled examples), but of significant practical importance for data mining - e.g. detecting new financial transaction fraud patterns, where normal legitimate transactions dominate. This paper develops a new method for detecting an instance of each minority class via an unsupervised local-density-differential sampling strategy. Essentially a variable-scale nearest neighbor process is used to optimize the probability of sampling tightly-grouped minority classes, subject to a local smoothness assumption of the majority class. Results on both synthetic and real data sets are very positive, detecting each minority class with only a fraction of the actively sampled points required by random sampling and by Pelleg's Interleave method, the prior best technique in the sparse literature on this topic.},
 author = {He, Jingrui and Carbonell, Jaime},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2838023a778dfaecdc212708f721b788-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2838023a778dfaecdc212708f721b788-Metadata.json},
 openalex = {W2149263498},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2838023a778dfaecdc212708f721b788-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Nearest-Neighbor-Based Active Learning for Rare Category Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/2838023a778dfaecdc212708f721b788-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_289dff07,
 abstract = {Stimulus selectivity of sensory neurons is often characterized by estimating their receptive field properties such as orientation selectivity. Receptive fields are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we find a concise description for the processing of a whole population of neurons analogous to the receptive field for single neurons? Here, we present a generalization of the linear receptive field which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive fields span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive fields from multi-dimensional neural measurements such as those obtained from dynamic imaging methods.},
 author = {Zeck, Guenther and Bethge, Matthias and Macke, Jakob H},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/289dff07669d7a23de0ef88d2f7129e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/289dff07669d7a23de0ef88d2f7129e7-Metadata.json},
 openalex = {W2103081851},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/289dff07669d7a23de0ef88d2f7129e7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Receptive Fields without Spike-Triggering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/289dff07669d7a23de0ef88d2f7129e7-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_2ab56412,
 abstract = {We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmussenand Ghahramani [2]. The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional benefit over the latter method lies in our ability to incorporate knowledge of the noise domain to influence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more confident predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. [3].},
 author = {Naish-guzman, Andrew and Holden, Sean},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2ab56412b1163ee131e1246da0955bd1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2ab56412b1163ee131e1246da0955bd1-Metadata.json},
 openalex = {W2167613337},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Robust Regression with Twinned Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/2ab56412b1163ee131e1246da0955bd1-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_2b323d6e,
 abstract = {We give a new class of outer bounds on the marginal polytope, and propose a cutting-plane algorithm for efficiently optimizing over these constraints. When combined with a concave upper bound on the entropy, this gives a new variational inference algorithm for probabilistic inference in discrete Markov Random Fields (MRFs). Valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope. As a result, we obtain tighter upper bounds on the log-partition function. We also show empirically that the approximations of the marginals are significantly more accurate when using the tighter outer bounds. Finally, we demonstrate the advantage of the new constraints for finding the MAP assignment in protein structure prediction.},
 author = {Sontag, David and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2b323d6eb28422cef49b266557dd31ad-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2b323d6eb28422cef49b266557dd31ad-Metadata.json},
 openalex = {W2150675045},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2b323d6eb28422cef49b266557dd31ad-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {New Outer Bounds on the Marginal Polytope},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/2b323d6eb28422cef49b266557dd31ad-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_2bcab9d9,
 abstract = {Point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, owing in part to the fact that they do not take into account multiple stages of processing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more flexible for fitting neural data. More importantly, it allows us to estimate connectivity patterns among neurons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model's performance using a simulated example network consisting of two coupled neurons.},
 author = {Pillow, Jonathan and Latham, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2bcab9d935d219641434683dd9d18a03-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2bcab9d935d219641434683dd9d18a03-Metadata.json},
 openalex = {W2147530857},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Neural characterization in partially observed populations of spiking neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/2bcab9d935d219641434683dd9d18a03-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_2ca65f58,
 abstract = {We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman's coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over others, and demonstrate our approach in document clustering and phylolinguistics.},
 author = {Teh, Yee and Daume III, Hal and Roy, Daniel M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Metadata.json},
 openalex = {W2950400502},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Agglomerative Clustering with Coalescents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_2dea61ee,
 abstract = {We investigate the problem of learning a widely-used latent-variable model - the Latent Dirichlet Allocation (LDA) or topic model - using distributed computation, where each of P processors only sees 1/P of the total data set. We propose two distributed inference schemes that are motivated from different perspectives. The first scheme uses local Gibbs sampling on each processor with periodic updates—it is simple to implement and can be viewed as an approximation to a single processor implementation of Gibbs sampling. The second scheme relies on a hierarchical Bayesian extension of the standard LDA model to directly account for the fact that data are distributed across P processors—it has a theoretical guarantee of convergence but is more complex to implement than the approximate method. Using five real-world text corpora we show that distributed learning works very well for LDA models, i.e., perplexity and precision-recall scores for distributed learning are indistinguishable from those obtained with single-processor learning. Our extensive experimental results include large-scale distributed computation on 1000 virtual processors; and speedup experiments of learning topics in a 100-million word corpus using 16 processors.},
 author = {Newman, David and Smyth, Padhraic and Welling, Max and Asuncion, Arthur},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2dea61eed4bceec564a00115c4d21334-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2dea61eed4bceec564a00115c4d21334-Metadata.json},
 openalex = {W2153029569},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2dea61eed4bceec564a00115c4d21334-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Inference for Latent Dirichlet Allocation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/2dea61eed4bceec564a00115c4d21334-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_2f2b2656,
 author = {Sharpee, Tatyana},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2f2b265625d76a6704b08093c652fd79-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2f2b265625d76a6704b08093c652fd79-Metadata.json},
 openalex = {W2126324770},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Better than least squares: comparison of objective functions for estimating linear-nonlinear models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_32b30a25,
 abstract = {In many structured prediction problems, the highest-scoring labeling is hard to compute exactly, leading to the use of approximate inference methods. However, when inference is used in a learning algorithm, a good approximation of the score may not be sufficient. We show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees. There are two reasons for this. First, approximate methods can effectively reduce the expressivity of an underlying model by making it impossible to choose parameters that reliably give good predictions. Second, approximations can respond to parameter changes in such a way that standard learning algorithms are misled. In contrast, we give two positive results in the form of learning bounds for the use of LP-relaxed inference in structured perceptron and empirical risk minimization settings. We argue that without understanding combinations of inference and learning, such as these, that are appropriately compatible, learning performance under approximate inference cannot be guaranteed.},
 author = {Kulesza, Alex and Pereira, Fernando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/32b30a250abd6331e03a2a1f16466346-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/32b30a250abd6331e03a2a1f16466346-Metadata.json},
 openalex = {W2159992248},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/32b30a250abd6331e03a2a1f16466346-Supplemental.zip},
 title = {Structured Learning with Approximate Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/32b30a250abd6331e03a2a1f16466346-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_33e8075e,
 abstract = {In this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem. The concordance index (CI), which quantifies the quality of rankings, is the standard performance measure for model assessment in survival analysis. In contrast, the standard approach to learning the popular proportional hazard (PH) model is based on Cox's partial likelihood. We devise two bounds on CI-one of which emerges directly from the properties of PH models-and optimize them directly. Our experimental results suggest that all three methods perform about equally well, with our new approach giving slightly better results. We also explain why a method designed to maximize the Cox's partial likelihood also ends up (approximately) maximizing the CI.},
 author = {Steck, Harald and Krishnapuram, Balaji and Dehing-oberije, Cary and Lambin, Philippe and Raykar, Vikas C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/33e8075e9970de0cfea955afd4644bb2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/33e8075e9970de0cfea955afd4644bb2-Metadata.json},
 openalex = {W2125782079},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On Ranking in Survival Analysis: Bounds on the Concordance Index},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_3435c378,
 abstract = {It is known that determinining whether a DEC-POMDP, namely, a cooperative partially observable stochastic game (POSG), has a cooperative strategy with positive expected reward is complete for NEXP. It was not known until now how cooperation affected that complexity. We show that, for competitive POSGs, the complexity of determining whether one team has a positive-expected-reward strategy is complete for NEXPNP.},
 author = {Goldsmith, Judy and Mundhenk, Martin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3435c378bb76d4357324dd7e69f3cd18-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3435c378bb76d4357324dd7e69f3cd18-Metadata.json},
 openalex = {W2168476921},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3435c378bb76d4357324dd7e69f3cd18-Supplemental.zip},
 title = {Competition Adds Complexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/3435c378bb76d4357324dd7e69f3cd18-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_37693cfc,
 abstract = {We present a simple new criterion for classiﬁcation, based on principles from lossy data compression. The criterion assigns a test sample to the class that uses the minimum number of additional bits to code the test sample, subject to an allowable distortion. We demonstrate the asymptotic optimality of this criterion for Gaussian distributions and analyze its relationships to classical classiﬁers. The theoretical results clarify the connections between our approach and popular classiﬁers such as MAP, RDA, k-NN, and SVM, as well as unsupervised methods based on lossy coding. Our formulation induces severa lgood effects on the resulting classiﬁer. First, minimizing the lossy coding length induces a regularization effect which stabilizes the (implicit) density estimate in a small sample setting. Second, compression provides a uniform means of handling classes of varying dimension. The new criterion and its kernel and local versions perform competitively on synthetic examples, as well as on real imagery data such as handwritten digits and face images. On these problems, the performance of our simple classiﬁer approaches the best reported results, without using domain-speciﬁc information. All MATLAB code and classiﬁcation results are publicly available for peer evaluation at http://perception.csl.uiuc.edu/coding/home.htm.},
 author = {Wright, John and Tao, Yangyu and Lin, Zhouchen and Ma, Yi and Shum, Heung-yeung},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/37693cfc748049e45d87b8c7d8b9aacd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/37693cfc748049e45d87b8c7d8b9aacd-Metadata.json},
 openalex = {W2946502179},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Classification via Minimum Incremental Coding Length (MICL)},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/37693cfc748049e45d87b8c7d8b9aacd-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_3a077244,
 abstract = {We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not depend on the choice of kernel in the limit of infinite data, for a wide class of kernels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments.},
 author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a0772443a0739141292a5429b952fe6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a0772443a0739141292a5429b952fe6-Metadata.json},
 openalex = {W2100600008},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a0772443a0739141292a5429b952fe6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a0772443a0739141292a5429b952fe6-Supplemental.zip},
 title = {Kernel Measures of Conditional Dependence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/3a0772443a0739141292a5429b952fe6-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_3a15c7d0,
 author = {Hoffman, Matthew and Doucet, Arnaud and Freitas, Nando and Jasra, Ajay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a15c7d0bbe60300a39f76f8a5ba6896-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a15c7d0bbe60300a39f76f8a5ba6896-Metadata.json},
 openalex = {W2136751544},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Policy Learning with Trans-Dimensional MCMC},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_3a30be93,
 abstract = {We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(λ), however it lacks the parameter α that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(λ) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins' Q(λ) and Sarsa(λ) and find that it again offers superior performance without a learning rate parameter.},
 author = {Hutter, Marcus and Legg, Shane},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a30be93eb45566a90f4e95ee72a089a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a30be93eb45566a90f4e95ee72a089a-Metadata.json},
 openalex = {W2100563667},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a30be93eb45566a90f4e95ee72a089a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a30be93eb45566a90f4e95ee72a089a-Supplemental.zip},
 title = {Temporal Difference Updating without a Learning Rate},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/3a30be93eb45566a90f4e95ee72a089a-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_3b3dbaf6,
 abstract = {Bayesian Reinforcement Learning has generated substantial interest recently, as it provides an elegant solution to the exploration-exploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). Our goal is to extend these ideas to the more general Partially Observable MDP (POMDP) framework, where the state is a hidden variable. To address this problem, we introduce a new mathematical model, the Bayes-Adaptive POMDP. This new model allows us to (1) improve knowledge of the POMDP domain through interaction with the environment, and (2) plan optimal sequences of actions which can tradeoff between improving the model, identifying the state, and gathering reward. We show how the model can be finitely approximated while preserving the value function. We describe approximations for belief tracking and planning in this model. Empirical results on two domains show that the model estimate and agent's return improve over time, as the agent learns better model estimates.},
 author = {Ross, Stephane and Chaib-draa, Brahim and Pineau, Joelle},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3b3dbaf68507998acd6a5a5254ab2d76-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3b3dbaf68507998acd6a5a5254ab2d76-Metadata.json},
 openalex = {W2144794447},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayes-Adaptive POMDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/3b3dbaf68507998acd6a5a5254ab2d76-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_3e89ebdb,
 abstract = {We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efficient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with significant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).},
 author = {Hern\'{a}ndez-lobato, Jos\'{e} and Dijkstra, Tjeerd and Heskes, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3e89ebdb49f712c7d90d1b39e348bbbf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3e89ebdb49f712c7d90d1b39e348bbbf-Metadata.json},
 openalex = {W2106108430},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3e89ebdb49f712c7d90d1b39e348bbbf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/3e89ebdb49f712c7d90d1b39e348bbbf-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_3fe94a00,
 abstract = {Clustering is often formulated as the maximum likelihood estimation of a mixture model that explains the data. The EM algorithm widely used to solve the resulting optimization problem is inherently a gradient-descent method and is sensitive to initialization. The resulting solution is a local optimum in the neighborhood of the initial guess. This sensitivity to initialization presents a significant challenge in clustering large data sets into many clusters. In this paper, we present a different approach to approximate mixture fitting for clustering. We introduce an exemplar-based likelihood function that approximates the exact likelihood. This formulation leads to a convex minimization problem and an efficient algorithm with guaranteed convergence to the globally optimal solution. The resulting clustering can be thought of as a probabilistic mapping of the data points to the set of exemplars that minimizes the average distance and the information-theoretic cost of mapping. We present experimental results illustrating the performance of our algorithm and its comparison with the conventional approach to mixture model clustering.},
 author = {Lashkari, Danial and Golland, Polina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3fe94a002317b5f9259f82690aeea4cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3fe94a002317b5f9259f82690aeea4cd-Metadata.json},
 openalex = {W2168036472},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Convex Clustering with Exemplar-Based Models.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/3fe94a002317b5f9259f82690aeea4cd-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_42e77b63,
 abstract = {Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization.},
 author = {Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Wortman, Jennifer},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Metadata.json},
 openalex = {W2110091014},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Supplemental.zip},
 title = {Learning Bounds for Domain Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/42e77b63637ab381e8be5f8318cc28a2-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_42e7aaa8,
 abstract = {We present a new class of models for high-dimensional nonparametric regression and classification called sparse additive models (SpAM). Our methods combine ideas from sparse linear modeling and additive nonparametric regression. We derive a method for fitting the models that is effective even when the number of covariates is larger than the sample size. A statistical analysis of the properties of SpAM is given together with empirical results on synthetic and real data, showing that SpAM can be effective in fitting sparse nonparametric models in high dimensional data.},
 author = {Liu, Han and Wasserman, Larry and Lafferty, John and Ravikumar, Pradeep},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/42e7aaa88b48137a16a1acd04ed91125-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/42e7aaa88b48137a16a1acd04ed91125-Metadata.json},
 openalex = {W2135327149},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/42e7aaa88b48137a16a1acd04ed91125-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {SpAM: Sparse Additive Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/42e7aaa88b48137a16a1acd04ed91125-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_46ba9f2a,
 abstract = {Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically influence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain confidence intervals which makes it possible to assess the statistical significance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical significance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those filters from a spike-triggered covariance analysis that are most informative about the neural response.},
 author = {Gerwinn, Sebastian and Bethge, Matthias and Macke, Jakob H and Seeger, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/46ba9f2a6976570b0353203ec4474217-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/46ba9f2a6976570b0353203ec4474217-Metadata.json},
 openalex = {W2122197371},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/46ba9f2a6976570b0353203ec4474217-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Inference for Spiking Neuron Models with a Sparsity Prior},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/46ba9f2a6976570b0353203ec4474217-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_4b025079,
 abstract = {In online handwriting recognition the trajectory of the pen is recorded during writing. Although the trajectory provides a compact and complete representation of the written output, it is hard to transcribe directly, because each letter is spread over many pen locations. Most recognition systems therefore employ sophisticated preprocessing techniques to put the inputs into a more localised form. However these techniques require considerable human effort, and are specific to particular languages and alphabets. This paper describes a system capable of directly transcribing raw online handwriting data. The system consists of an advanced recurrent neural network with an output layer designed for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases.},
 author = {Graves, Alex and Liwicki, Marcus and Bunke, Horst and Schmidhuber, J\"{u}rgen and Fern\'{a}ndez, Santiago},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b0250793549726d5c1ea3906726ebfe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b0250793549726d5c1ea3906726ebfe-Metadata.json},
 openalex = {W2167898728},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_4b04a686,
 abstract = {We present Epoch-Greedy, an algorithm for multi-armed bandits with observable side information. Epoch-Greedy has the following properties: No knowledge of a time horizon $T$ is necessary. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. The regret scales as $O(T^{2/3} S^{1/3})$ or better (sometimes, much better). Here $S$ is the complexity term in a sample complexity bound for standard supervised learning.},
 author = {Langford, John and Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b04a686b0ad13dce35fa99fa4161c65-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b04a686b0ad13dce35fa99fa4161c65-Metadata.json},
 openalex = {W2119850747},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b04a686b0ad13dce35fa99fa4161c65-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/4b04a686b0ad13dce35fa99fa4161c65-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_4b6538a4,
 abstract = {We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel.},
 author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Metadata.json},
 openalex = {W2162724919},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/4b6538a44a1dfdc2b83477cd76dee98e-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_4c56ff4c,
 abstract = {This paper introduces kernels on attributed pointsets, which are sets of vectors embedded in an euclidean space. The embedding gives the notion of neighborhood, which is used to define positive semidefinite kernels on pointsets. Two novel kernels on neighborhoods are proposed, one evaluating the attribute similarity and the other evaluating shape similarity. Shape similarity function is motivated from spectral graph matching techniques. The kernels are tested on three real life applications: face recognition, photo album tagging, and shot annotation in video sequences, with encouraging results.},
 author = {Parsana, Mehul and Bhattacharya, Sourangshu and Bhattacharya, Chiru and Ramakrishnan, K.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4c56ff4ce4aaf9573aa5dff913df997a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4c56ff4ce4aaf9573aa5dff913df997a-Metadata.json},
 openalex = {W2101150485},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Kernels on Attributed Pointsets with Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/4c56ff4ce4aaf9573aa5dff913df997a-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_4ca82782,
 abstract = {We propose to investigate test statistics for testing homogeneity in reproducing kernel Hilbert spaces. Asymptotic null distributions under null hypothesis are derived, and consistency against fixed and local alternatives is assessed. Finally, experimental evidence of the performance of the proposed approach on both artificial data and a speaker verification task is provided.},
 author = {Eric, Moulines and Bach, Francis and Harchaoui, Za\"{\i}d},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ca82782c5372a547c104929f03fe7a9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ca82782c5372a547c104929f03fe7a9-Metadata.json},
 openalex = {W2951907984},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Testing for Homogeneity with Kernel Fisher Discriminant Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/4ca82782c5372a547c104929f03fe7a9-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_4daa3db3,
 abstract = {Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or deep, structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Specifically, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge filters, similar to the Gabor functions known to model V1 cell receptive fields. Further, the second layer in our model encodes correlations of the first layer responses in the data. Specifically, it picks up both colinear (contour) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex corner features matches well with the results from the Ito & Komatsu's study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features.},
 author = {Lee, Honglak and Ekanadham, Chaitanya and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4daa3db355ef2b0e64b472968cb70f0d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4daa3db355ef2b0e64b472968cb70f0d-Metadata.json},
 openalex = {W2133257461},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sparse deep belief net model for visual area V2},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/4daa3db355ef2b0e64b472968cb70f0d-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_4ea06fbc,
 abstract = {Traditional analysis methods for single-trial classification of electro-encephalography (EEG) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classification, e.g. event related potentials; and second order methods, in which the feature of interest is the power of the signal, e.g. event related (de)synchronization. The procedure for deciding which paradigm to use is ad hoc and is typically driven by knowledge of the underlying neurophysiology. Here we propose a principled method, based on a bilinear model, in which the algorithm simultaneously learns the best first and second order spatial and temporal features for classification of EEG. The method is demonstrated on simulated data as well as on EEG taken from a benchmark data used to test classification algorithms for brain computer interfaces.},
 author = {Christoforou, Christoforos and Sajda, Paul and Parra, Lucas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ea06fbc83cdd0a06020c35d50e1e89a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ea06fbc83cdd0a06020c35d50e1e89a-Metadata.json},
 openalex = {W2108315404},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Second Order Bilinear Discriminant Analysis for single trial EEG analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/4ea06fbc83cdd0a06020c35d50e1e89a-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_4ea83d95,
 abstract = {We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semidefinite relaxation that yields global training by eliminating local minima.},
 author = {Guo, Yuhong and Schuurmans, Dale},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ea83d951990d8bf07a68ec3e50f9156-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ea83d951990d8bf07a68ec3e50f9156-Metadata.json},
 openalex = {W2104952380},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ea83d951990d8bf07a68ec3e50f9156-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Convex Relaxations of Latent Variable Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/4ea83d951990d8bf07a68ec3e50f9156-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_500e75a0,
 abstract = {We summarize the implementation of an analog VLSI chip hosting a network of 32 integrate-and-fire (IF) neurons with spike-frequency adaptation and 2,048 Hebbian plastic bistable spike-driven stochastic synapses endowed with a self-regulating mechanism which stops unnecessary synaptic changes. The synaptic matrix can be flexibly configured and provides both recurrent and AER-based connectivity with external, AER compliant devices. We demonstrate the ability of the network to efficiently classify overlapping patterns, thanks to the self-regulating mechanism.},
 author = {Giulioni, Massimiliano and Pannunzi, Mario and Badoni, Davide and Dante, Vittorio and Giudice, Paolo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/500e75a036dc2d7d2fec5da1b71d36cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/500e75a036dc2d7d2fec5da1b71d36cc-Metadata.json},
 openalex = {W2147982749},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/500e75a036dc2d7d2fec5da1b71d36cc-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_51ef186e,
 abstract = {The classical hypothesis, that bottom-up saliency is a center-surround process, is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense. The combined hypothesis is denoted as discriminant center-surround saliency, and the corresponding optimal saliency architecture is derived. This architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classification problem that opposes stimuli at center and surround, at that location. It is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency, including non-linear properties beyond the reach of previous saliency models. Furthermore, it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities (such as color, orientation and motion), and provides optimal solutions for many other saliency problems of interest for computer vision. Optimal solutions, under this hypothesis, are derived for a number of the former (including static natural images, dense motion fields, and even dynamic textures), and applied to a number of the latter (the prediction of human eye fixations, motion-based saliency in the presence of ego-motion, and motion-based saliency in the presence of highly dynamic backgrounds). In result, discriminant saliency is shown to predict eye fixations better than previous models, and produces background subtraction algorithms that outperform the state-of-the-art in computer vision.},
 author = {Gao, Dashan and Mahadevan, Vijay and Vasconcelos, Nuno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/51ef186e18dc00c2d31982567235c559-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/51ef186e18dc00c2d31982567235c559-Metadata.json},
 openalex = {W2130637418},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/51ef186e18dc00c2d31982567235c559-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/51ef186e18dc00c2d31982567235c559-Supplemental.zip},
 title = {The discriminant center-surround hypothesis for bottom-up saliency},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/51ef186e18dc00c2d31982567235c559-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_53c3bce6,
 abstract = {Semi-supervised methods use unlabeled data in addition to labeled data to construct predictors. While existing semi-supervised methods have shown some promising empirical performance, their development has been based largely based on heuristics. In this paper we study semi-supervised learning from the viewpoint of minimax theory. Our first result shows that some common methods based on regularization using graph Laplacians do not lead to faster minimax rates of convergence. Thus, the estimators that use the unlabeled data do not have smaller risk than the estimators that use only labeled data. We then develop several new approaches that provably lead to improved performance. The statistical tools of minimax analysis are thus used to offer some new perspective on the problem of semi-supervised learning.},
 author = {Wasserman, Larry and Lafferty, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/53c3bce66e43be4f209556518c2fcb54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/53c3bce66e43be4f209556518c2fcb54-Metadata.json},
 openalex = {W2110316582},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Statistical Analysis of Semi-Supervised Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_54a367d6,
 abstract = {We consider apprenticeship learning—learning from expert demonstrations—in the setting of large, complex domains. Past work in apprenticeship learning requires that the expert demonstrate complete trajectories through the domain. However, in many problems even an expert has difficulty controlling the system, which makes this approach infeasible. For example, consider the task of teaching a quadruped robot to navigate over extreme terrain; demonstrating an optimal policy (i.e., an optimal set of foot locations over the entire terrain) is a highly non-trivial task, even for an expert. In this paper we propose a method for hierarchical apprenticeship learning, which allows the algorithm to accept isolated advice at different hierarchical levels of the control task. This type of advice is often feasible for experts to give, even if the expert is unable to demonstrate complete trajectories. This allows us to extend the apprenticeship learning paradigm to much larger, more challenging domains. In particular, in this paper we apply the hierarchical apprenticeship learning algorithm to the task of quadruped locomotion over extreme terrain, and achieve, to the best of our knowledge, results superior to any previously published work.},
 author = {Kolter, J. and Abbeel, Pieter and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/54a367d629152b720749e187b3eaa11b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/54a367d629152b720749e187b3eaa11b-Metadata.json},
 openalex = {W2105947986},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/54a367d629152b720749e187b3eaa11b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/54a367d629152b720749e187b3eaa11b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_55a7cf9c,
 abstract = {Maximum variance unfolding (MVU) is an effective heuristic for dimensionality reduction. It produces a low-dimensional representation of the data by maximizing the variance of their embeddings while preserving the local distances of the original data. We show that MVU also optimizes a statistical dependence measure which aims to retain the identity of individual observations under the distance-preserving constraints. This general view allows us to design colored variants of MVU, which produce low-dimensional representations for a given task, e.g. subject to class labels or other side information.},
 author = {Song, Le and Gretton, Arthur and Borgwardt, Karsten and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Metadata.json},
 openalex = {W2120481923},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Supplemental.zip},
 title = {Colored Maximum Variance Unfolding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/55a7cf9c71f1c9c495413f934dd1a158-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_57370345,
 abstract = {We consider the estimation problem in Gaussian graphical models with arbitrary structure. We analyze the Embedded Trees algorithm, which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph. Our analysis is based on the recently developed walk-sum interpretation of Gaussian estimation. We show that non-stationary iterations of the Embedded Trees algorithm using any sequence of subgraphs converge in walk-summable models. Based on walk-sum calculations, we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error. These adaptive procedures provide a significant speedup in convergence over stationary iterative methods, and also appear to converge in a larger class of models.},
 author = {Chandrasekaran, Venkat and Willsky, Alan and Johnson, Jason},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5737034557ef5b8c02c0e46513b98f90-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5737034557ef5b8c02c0e46513b98f90-Metadata.json},
 openalex = {W2138074492},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5737034557ef5b8c02c0e46513b98f90-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/5737034557ef5b8c02c0e46513b98f90-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_5e6d27a7,
 author = {Isbell, Charles and Holmes, Michael and Gray, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5e6d27a7a8a8330df4b53240737ccc85-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5e6d27a7a8a8330df4b53240737ccc85-Metadata.json},
 openalex = {W2142802760},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5e6d27a7a8a8330df4b53240737ccc85-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Ultrafast Monte Carlo for Statistical Summations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/5e6d27a7a8a8330df4b53240737ccc85-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_5ef698cd,
 abstract = {Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscientific and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train's underlying firing rate. Current techniques to find time-varying firing rates require ad hoc choices of parameters, offer no confidence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of firing rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators.},
 author = {Cunningham, John P and Yu, Byron M and Shenoy, Krishna V and Sahani, Maneesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5ef698cd9fe650923ea331c15af3b160-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5ef698cd9fe650923ea331c15af3b160-Metadata.json},
 openalex = {W2131351008},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/5ef698cd9fe650923ea331c15af3b160-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_5f93f983,
 abstract = {Reliably recovering 3D human pose from monocular video requires models that bias the estimates towards typical human poses and motions. We construct priors for people tracking using the Laplacian Eigenmaps Latent Variable Model (LELVM). LELVM is a recently introduced probabilistic dimensionality reduction model that combines the advantages of latent variable models—a multimodal probability density for latent and observed variables, and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction—with those of spectral manifold learning methods—no local optima, ability to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension. LELVM is computationally efficient, simple to learn from sparse training data, and compatible with standard probabilistic trackers such as particle filters. We analyze the performance of a LELVM-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that LELVM not only provides sufficient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements, but also compares favorably with alternative trackers based on PCA or GPLVM priors.},
 author = {Lu, Zhengdong and Sminchisescu, Cristian and Carreira-Perpi\~{n}\'{a}n, Miguel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5f93f983524def3dca464469d2cf9f3e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5f93f983524def3dca464469d2cf9f3e-Metadata.json},
 openalex = {W2118502328},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {People Tracking with the Laplacian Eigenmaps Latent Variable Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/5f93f983524def3dca464469d2cf9f3e-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_60815949,
 abstract = {Assessing similarity between features is a key step in object recognition and scene categorization tasks. We argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not. Intuitively one would expect that similarities between features could arise from any distribution. In this paper, we will derive the contrary, and report the theoretical result that Lp-norms -a class of commonly applied distance metrics- from one feature vector to other vectors are Weibull-distributed if the feature values are correlated and non-identically distributed. Besides these assumptions being realistic for images, we experimentally show them to hold for various popular feature extraction algorithms, for a diverse range of images. This fundamental insight opens new directions in the assessment of feature similarity, with projected improvements in object and scene recognition algorithms.},
 author = {Burghouts, Gertjan and Smeulders, Arnold and Geusebroek, Jan-mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6081594975a764c8e3a691fa2b3a321d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6081594975a764c8e3a691fa2b3a321d-Metadata.json},
 openalex = {W2166430258},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6081594975a764c8e3a691fa2b3a321d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Distribution Family of Similarity Distances},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/6081594975a764c8e3a691fa2b3a321d-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_621461af,
 abstract = {Current computational models of bottom-up and top-down components of attention are predictive of eye movements across a range of stimuli and of simple, fixed visual tasks (such as visual search for a target among distractors). However, to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environments and tasks, such as driving a vehicle through traffic. Here, we develop a hybrid computational/behavioral framework, combining simple models for bottom-up salience and top-down relevance, and looking for changes in the predictive power of these components at different critical event times during 4.7 hours (500,000 video frames) of observers playing car racing and flight combat video games. This approach is motivated by our observation that the predictive strengths of the salience and relevance models exhibit reliable temporal signatures during critical event windows in the task sequence—for example, when the game player directly engages an enemy plane in a flight combat game, the predictive strength of the salience model increases significantly, while that of the relevance model decreases significantly. Our new framework combines these temporal signatures to implement several event detectors. Critically, we find that an event detector based on fused behavioral and stimulus information (in the form of the model's predictive strength) is much stronger than detectors based on behavioral information alone (eye position) or image information alone (model prediction maps). This approach to event detection, based on eye tracking combined with computational models applied to the visual input, may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from EEG or fMRI recordings.},
 author = {Peters, Robert and Itti, Laurent},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/621461af90cadfdaf0e8d4cc25129f91-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/621461af90cadfdaf0e8d4cc25129f91-Metadata.json},
 openalex = {W2109186376},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Congruence between model and human attention reveals unique signatures of critical visual events},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/621461af90cadfdaf0e8d4cc25129f91-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_66368270,
 abstract = {In this paper we investigate multi-task learning in the context of Gaussian Processes (GP). We propose a model that learns a shared covariance function on input-dependent features and a free-form covariance matrix over tasks. This allows for good flexibility when modelling inter-task dependencies while avoiding the need for large amounts of data for training. We show that under the assumption of noise-free observations and a block design, predictions for a given task only depend on its target values and therefore a cancellation of inter-task transfer occurs. We evaluate the benefits of our model on two practical applications: a compiler performance prediction problem and an exam score prediction task. Additionally, we make use of GP approximations and properties of our model in order to provide scalability to large data sets.},
 author = {Bonilla, Edwin V and Chai, Kian and Williams, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Metadata.json},
 openalex = {W2119595900},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Supplemental.zip},
 title = {Multi-task Gaussian Process Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_67f7fb87,
 abstract = {When we have several related tasks, solving them simultaneously is shown to be more effective than solving them individually. This approach is called multi-task learning (MTL) and has been studied extensively. Existing approaches to MTL often treat all the tasks as uniformly related to each other and the relatedness of the tasks is controlled globally. For this reason, the existing methods can lead to undesired solutions when some tasks are not highly related to each other, and some pairs of related tasks can have significantly different solutions. In this paper, we propose a novel MTL algorithm that can overcome these problems. Our method makes use of a task network, which describes the relation structure among tasks. This allows us to deal with intricate relation structures in a systematic way. Furthermore, we control the relatedness of the tasks locally, so all pairs of related tasks are guaranteed to have similar solutions. We apply the above idea to support vector machines (SVMs) and show that the optimization problem can be cast as a second order cone program, which is convex and can be solved efficiently. The usefulness of our approach is demonstrated through simulations with protein super-family classification and ordinal regression problems.},
 author = {Kato, Tsuyoshi and Kashima, Hisashi and Sugiyama, Masashi and Asai, Kiyoshi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/67f7fb873eaf29526a11a9b7ac33bfac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/67f7fb873eaf29526a11a9b7ac33bfac-Metadata.json},
 openalex = {W2153086202},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multi-Task Learning via Conic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_6883966f,
 abstract = {We present four new reinforcement learning algorithms based on actor-critic and natural-gradient ideas, and provide their convergence proofs. Actor-critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor-critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients, and they extend prior empirical studies of natural actor-critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms.},
 author = {Bhatnagar, Shalabh and Ghavamzadeh, Mohammad and Lee, Mark and Sutton, Richard S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6883966fd8f918a4aa29be29d2c386fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6883966fd8f918a4aa29be29d2c386fb-Metadata.json},
 openalex = {W2136602922},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6883966fd8f918a4aa29be29d2c386fb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Incremental Natural Actor-Critic Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/6883966fd8f918a4aa29be29d2c386fb-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_69421f03,
 abstract = {We investigate a family of inference problems on Markov models, where many sample paths are drawn from a Markov chain and partial information is revealed to an observer who attempts to reconstruct the sample paths. We present algorithms and hardness results for several variants of this problem which arise by revealing different information to the observer and imposing different requirements for the reconstruction of sample paths. Our algorithms are analogous to the classical Viterbi algorithm for Hidden Markov Models, which finds the single most probable sample path given a sequence of observations. Our work is motivated by an important application in ecology: inferring bird migration paths from a large database of observations.},
 author = {Elmohamed, M.a. and Kozen, Dexter and Sheldon, Daniel R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/69421f032498c97020180038fddb8e24-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/69421f032498c97020180038fddb8e24-Metadata.json},
 openalex = {W2151409566},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/69421f032498c97020180038fddb8e24-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Collective Inference on Markov Models for Modeling Bird Migration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/69421f032498c97020180038fddb8e24-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_6974ce5a,
 abstract = {Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject's intent. An elegant approach to improve the accuracy of BCIs consists in a verification procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment confirms the previously reported presence of a new kind of ErrP. These Interaction exhibit a first sharp negative peak followed by a positive peak and a second broader negative peak (~290, ~350 and ~470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classifier embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject's intent while trying to mentally drive the cursor of 73.1%. These results show that it's possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the brain-computer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex.},
 author = {Ferrez, Pierre and Mill\'{a}n, Jos\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6974ce5ac660610b44d9b9fed0ff9548-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6974ce5ac660610b44d9b9fed0ff9548-Metadata.json},
 openalex = {W2142194392},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6974ce5ac660610b44d9b9fed0ff9548-Supplemental.zip},
 title = {EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/6974ce5ac660610b44d9b9fed0ff9548-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_6aab1270,
 abstract = {Brain-Computer Interfaces can suffer from a large variance of the subject conditions within and across sessions. For example vigilance fluctuations in the individual, variable task involvement, workload etc. alter the characteristics of EEG signals and thus challenge a stable BCI operation. In the present work we aim to define features based on a variant of the common spatial patterns (CSP) algorithm that are constructed invariant with respect to such nonstationarities. We enforce invariance properties by adding terms to the denominator of a Rayleigh coefficient representation of CSP such as disturbance covariance matrices from fluctuations in visual processing. In this manner physiological prior knowledge can be used to shape the classification engine for BCI. As a proof of concept we present a BCI classifier that is robust to changes in the level of parietal α-activity. In other words, the EEG decoding still works when there are lapses in vigilance.},
 author = {Blankertz, Benjamin and Kawanabe, Motoaki and Tomioka, Ryota and Hohlefeld, Friederike and M\"{u}ller, Klaus-Robert and Nikulin, Vadim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6aab1270668d8cac7cef2566a1c5f569-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6aab1270668d8cac7cef2566a1c5f569-Metadata.json},
 openalex = {W2142183266},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/6aab1270668d8cac7cef2566a1c5f569-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_6da37dd3,
 abstract = {We present a probability distribution over non-negative integer valued matrices with possibly an infinite number of columns. We also derive a stochastic process that reproduces this distribution over equivalence classes. This model can play the role of the prior in nonparametric Bayesian learning scenarios where multiple latent features are associated with the observed data and each feature can have multiple appearances or occurrences within each data point. Such data arise naturally when learning visual object recognition systems from unlabelled images. Together with the nonparametric prior we consider a likelihood model that explains the visual appearance and location of local image patches. Inference with this model is carried out using a Markov chain Monte Carlo algorithm.},
 author = {Titsias, Michalis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Metadata.json},
 openalex = {W2148215528},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Infinite Gamma-Poisson Feature Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_6dfe08ed,
 author = {Li, Ping and Hastie, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6dfe08eda761bd321f8a9b239f6f4ec3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6dfe08eda761bd321f8a9b239f6f4ec3-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6dfe08eda761bd321f8a9b239f6f4ec3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Near-Optimal Estimator For Dimension Reduction in l\_\textbackslash alpha (0<\textbackslash alpha\textbackslash leq 2) Using Stable Random Projections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/6dfe08eda761bd321f8a9b239f6f4ec3-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_6e2713a6,
 abstract = {We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difficult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle filter and smoother to the task, and discuss some of the practical approaches used to tackle the difficulties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study.},
 author = {Murray, Lawrence and Storkey, Amos J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6e2713a6efee97bacb63e52c54f0ada0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6e2713a6efee97bacb63e52c54f0ada0-Metadata.json},
 openalex = {W2158304243},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Continuous Time Particle Filtering for fMRI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_6e7b33fd,
 abstract = {Adaptation to other initially unknown agents often requires computing an effective counter-strategy. In the Bayesian paradigm, one must find a good counter-strategy to the inferred posterior of the other agents' behavior. In the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents' expected behavior. In this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms. The strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed. The technique involves solving a modified game, and therefore can make use of recently developed algorithms for solving very large extensive games. We demonstrate the effectiveness of the technique in two-player Texas Hold'em. We show that the computed poker strategies are substantially more robust than best response counter-strategies, while still exploiting a suspected tendency. We also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses.},
 author = {Johanson, Michael and Zinkevich, Martin and Bowling, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6e7b33fdea3adc80ebd648fffb665bb8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6e7b33fdea3adc80ebd648fffb665bb8-Metadata.json},
 openalex = {W2153816030},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6e7b33fdea3adc80ebd648fffb665bb8-Supplemental.zip},
 title = {Computing Robust Counter-Strategies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/6e7b33fdea3adc80ebd648fffb665bb8-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_6faa8040,
 abstract = {We combine three threads of research on approximate dynamic programming: sparse random sampling of states, value function and policy approximation using local models, and using local trajectory optimizers to globally optimize a policy and associated value function. Our focus is on finding steady-state policies for deterministic time-invariant discrete time control problems with continuous states and actions often found in robotics. In this paper, we describe our approach and provide initial results on several simulated robotics problems.},
 author = {Atkeson, Chris and Stephens, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6faa8040da20ef399b63a72d0e4ab575-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6faa8040da20ef399b63a72d0e4ab575-Metadata.json},
 openalex = {W2130094217},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6faa8040da20ef399b63a72d0e4ab575-Supplemental.zip},
 title = {Random Sampling of States in Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/6faa8040da20ef399b63a72d0e4ab575-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_708f3cf8,
 abstract = {Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models try to predict such voluntary eye and attentional shifts. Although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties. We here demonstrate that a combined model of face detection and low-level saliency significantly outperforms a low-level model in predicting locations humans fixate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. Observers, even when not instructed to look for anything particular, fixate on a face with a probability of over 80% within their first two fixations; furthermore, they exhibit more similar scan-paths when faces are present. Remarkably, our model's predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses.},
 author = {Cerf, Moran and Harel, Jonathan and Einhaeuser, Wolfgang and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/708f3cf8100d5e71834b1db77dfa15d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/708f3cf8100d5e71834b1db77dfa15d6-Metadata.json},
 openalex = {W2169561119},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/708f3cf8100d5e71834b1db77dfa15d6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/708f3cf8100d5e71834b1db77dfa15d6-Supplemental.zip},
 title = {Predicting human gaze using low-level saliency combined with face detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/708f3cf8100d5e71834b1db77dfa15d6-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_7143d7fb,
 abstract = {We present a new local approximation algorithm for computing MAP and log-partition function for arbitrary exponential family distribution represented by a finite-valued pair-wise Markov random field (MRF), say G. Our algorithm is based on decomposing G into appropriately chosen small components; computing estimates locally in each of these components and then producing a good global solution. We prove that the algorithm can provide approximate solution within arbitrary accuracy when G excludes some finite sized graph as its minor and G has bounded degree: all Planar graphs with bounded degree are examples of such graphs. The running time of the algorithm is Θ(n) (n is the number of nodes in G), with constant dependent on accuracy, degree of graph and size of the graph that is excluded as a minor (constant for Planar graphs).

Our algorithm for minor-excluded graphs uses the decomposition scheme of Klein, Plotkin and Rao (1993). In general, our algorithm works with any decomposition scheme and provides quantifiable approximation guarantee that depends on the decomposition scheme.},
 author = {Jung, Kyomin and Shah, Devavrat},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7143d7fbadfa4693b9eec507d9d37443-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7143d7fbadfa4693b9eec507d9d37443-Metadata.json},
 openalex = {W2108553206},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7143d7fbadfa4693b9eec507d9d37443-Supplemental.zip},
 title = {Local Algorithms for Approximate Inference in Minor-Excluded Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/7143d7fbadfa4693b9eec507d9d37443-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_72da7fd6,
 abstract = {We develop and analyze an algorithm for nonparametric estimation of divergence functionals and the density ratio of two probability distributions. Our method is based on a variational characterization of f-divergences, which turns the estimation into a penalized convex risk minimization problem. We present a derivation of our kernel-based estimation algorithm and an analysis of convergence rates for the estimator. Our simulation results demonstrate the convergence behavior of the method, which compares favorably with existing methods in the literature.},
 author = {Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Metadata.json},
 openalex = {W2169269897},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Supplemental.zip},
 title = {Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/72da7fd6d1302c0a159f6436d01e9eb0-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_731c83db,
 abstract = {We utilize the ensemble of trees framework, a tractable mixture over super-exponential number of tree-structured distributions [1], to develop a new model for multivariate density estimation. The model is based on a construction of tree-structured copulas - multivariate distributions with uniform on [0, 1] marginals. By averaging over all possible tree structures, the new model can approximate distributions with complex variable dependencies. We propose an EM algorithm to estimate the parameters for these tree-averaged models for both the real-valued and the categorical case. Based on the tree-averaged framework, we propose a new model for joint precipitation amounts data on networks of rain stations.},
 author = {Kirshner, Sergey},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/731c83db8d2ff01bdc000083fd3c3740-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/731c83db8d2ff01bdc000083fd3c3740-Metadata.json},
 openalex = {W2164319128},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning with Tree-Averaged Densities and Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/731c83db8d2ff01bdc000083fd3c3740-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_735b90b4,
 abstract = {Markov jump processes play an important role in a large number of application domains. However, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference. We propose a mean field approximation to perform posterior inference and parameter estimation. The approximation allows a practical solution to the inference problem, while still retaining a good degree of accuracy. We illustrate our approach on two biologically motivated systems.},
 author = {Opper, Manfred and Sanguinetti, Guido},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/735b90b4568125ed6c3f678819b6e058-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/735b90b4568125ed6c3f678819b6e058-Metadata.json},
 openalex = {W2156404185},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/735b90b4568125ed6c3f678819b6e058-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Variational inference for Markov jump processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/735b90b4568125ed6c3f678819b6e058-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_73e5080f,
 abstract = {The expectation maximization (EM) algorithm is a widely used maximum likelihood estimation procedure for statistical models when the values of some of the variables in the model are not observed. Very often, however, our aim is primarily to find a model that assigns values to the latent variables that have intended meaning for our data and maximizing expected likelihood only sometimes accomplishes this. Unfortunately, it is typically difficult to add even simple a-priori information about latent variables in graphical models without making the models overly complex or intractable. In this paper, we present an efficient, principled way to inject rich constraints on the posteriors of latent variables into the EM algorithm. Our method can be used to learn tractable graphical models that satisfy additional, otherwise intractable constraints. Focusing on clustering and the alignment problem for statistical machine translation, we show that simple, intuitive posterior constraints can greatly improve the performance over standard baselines and be competitive with more complex, intractable models.},
 author = {Ganchev, Kuzman and Taskar, Ben and Gama, Jo\~{a}o},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/73e5080f0f3804cb9cf470a8ce895dac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/73e5080f0f3804cb9cf470a8ce895dac-Metadata.json},
 openalex = {W2115511080},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/73e5080f0f3804cb9cf470a8ce895dac-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Expectation Maximization and Posterior Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/73e5080f0f3804cb9cf470a8ce895dac-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_74db120f,
 abstract = {Machine learning techniques are increasingly being used to produce a wide-range of classifiers for complex real-world applications that involve nonuniform testing costs and misclassification costs. As the complexity of these applications grows, the management of resources during the learning and classification processes becomes a challenging task. In this work we introduce ACT (Anytime Cost-sensitive Trees), a novel framework for operating in such environments. ACT is an anytime algorithm that allows trading computation time for lower classification costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques ACT approximates for each candidate split the cost of the subtree under it and favors the one with a minimal cost. Due to its stochastic nature ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare the performance of ACT to that of the state of the art cost-sensitive tree learners. The results show that for most domains ACT produces trees of significantly lower costs. ACT is also shown to exhibit good anytime behavior with diminishing returns.},
 author = {Esmeir, Saher and Markovitch, Shaul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/74db120f0a8e5646ef5a30154e9f6deb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/74db120f0a8e5646ef5a30154e9f6deb-Metadata.json},
 openalex = {W2140441384},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/74db120f0a8e5646ef5a30154e9f6deb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Anytime Induction of Cost-sensitive Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/74db120f0a8e5646ef5a30154e9f6deb-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_75fc093c,
 abstract = {We present a new analysis for the combination of binary classifiers. Our analysis makes use of the Neyman-Pearson lemma as a theoretical basis to analyze combinations of classifiers. We give a method for finding the optimal decision rule for a combination of classifiers and prove that it has the optimal ROC curve. We show how our method generalizes and improves previous work on combining classifiers and generating ROC curves.},
 author = {Barreno, Marco and Cardenas, Alvaro and Tygar, J. D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/75fc093c0ee742f6dddaa13fff98f104-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/75fc093c0ee742f6dddaa13fff98f104-Metadata.json},
 openalex = {W2109431881},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Optimal ROC Curve for a Combination of Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/75fc093c0ee742f6dddaa13fff98f104-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_766ebcd5,
 abstract = {This article discusses a latent variable model for inference and prediction of symmetric relational data. The model, based on the idea of the eigenvalue decomposition, represents the relationship between two nodes as the weighted inner-product of node-specific vectors of latent characteristics. This generalizes other popular latent variable models, such as latent class and distance models: It is shown mathematically that any latent class or distance model has a representation as an eigenmodel, but not vice-versa. The practical implications of this are examined in the context of three real datasets, for which the eigenmodel has as good or better out-of-sample predictive performance than the other two models.},
 author = {Hoff, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/766ebcd59621e305170616ba3d3dac32-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/766ebcd59621e305170616ba3d3dac32-Metadata.json},
 openalex = {W2159203990},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Modeling homophily and stochastic equivalence in symmetric relational data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/766ebcd59621e305170616ba3d3dac32-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_7cce53cf,
 abstract = {Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding model based on a Student-t prior. Having validated our methods on toy data, we find that natural images are indeed best modelled by extremely sparse distributions; although for the Student-t prior, the associated optimal basis size is only modestly over-complete.},
 author = {Berkes, Pietro and Turner, Richard and Sahani, Maneesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7cce53cf90577442771720a370c3c723-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7cce53cf90577442771720a370c3c723-Metadata.json},
 openalex = {W2170092788},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7cce53cf90577442771720a370c3c723-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On Sparsity and Overcompleteness in Image Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/7cce53cf90577442771720a370c3c723-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_7ce3284b,
 abstract = {We present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree. This framework combines the advantages of the classical comparative method with the robustness of corpus-based probabilistic models. We use this framework to explore the consequences of two different schemes for defining probabilistic models of phonological change, evaluating these schemes by reconstructing ancient word forms of Romance languages. The result is an efficient inference procedure for automatically inferring ancient word forms from modern languages, which can be generalized to support inferences about linguistic phylogenies.},
 author = {Bouchard-c\^{o}t\'{e}, Alexandre and Liang, Percy S and Klein, Dan and Griffiths, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7ce3284b743aefde80ffd9aec500e085-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7ce3284b743aefde80ffd9aec500e085-Metadata.json},
 openalex = {W2112042807},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7ce3284b743aefde80ffd9aec500e085-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Probabilistic Approach to Language Change},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/7ce3284b743aefde80ffd9aec500e085-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_7fa732b5,
 abstract = {We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a fixed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes, but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topology-extraction approaches and show how having the two-dimensional topology can be exploited.},
 author = {Roux, Nicolas and Bengio, Yoshua and Lamblin, Pascal and Joliveau, Marc and K\'{e}gl, Bal\'{a}zs},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7fa732b517cbed14a48843d74526c11a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7fa732b517cbed14a48843d74526c11a-Metadata.json},
 openalex = {W2126760242},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7fa732b517cbed14a48843d74526c11a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning the 2-D Topology of Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/7fa732b517cbed14a48843d74526c11a-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8065d07d,
 abstract = {We present a novel Bayesian model for semi-supervised part-of-speech tagging. Our model extends the Latent Dirichlet Allocation model and incorporates the intuition that words' distributions over tags, p(t|w), are sparse. In addition we introduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words. Our model outperforms the best previously proposed model for this task on a standard dataset.},
 author = {Toutanova, Kristina and Johnson, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8065d07da4a77621450aa84fee5656d9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8065d07da4a77621450aa84fee5656d9-Metadata.json},
 openalex = {W2144508908},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8065d07da4a77621450aa84fee5656d9-Supplemental.zip},
 title = {A Bayesian LDA-based model for semi-supervised part-of-speech tagging},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_812b4ba2,
 abstract = {Over the past few years, the notion of stability in data clustering has received growing attention as a cluster validation criterion in a sample-based framework. However, recent work has shown that as the sample size increases, any clustering model will usually become asymptotically stable. This led to the conclusion that stability is lacking as a theoretical and practical tool. The discrepancy between this conclusion and the success of stability in practice has remained an open question, which we attempt to address. Our theoretical approach is that stability, as used by cluster validation algorithms, is similar in certain respects to measures of generalization in a model-selection framework. In such cases, the model chosen governs the convergence rate of generalization bounds. By arguing that these rates are more important than the sample size, we are led to the prediction that stability-based cluster validation algorithms should not degrade with increasing sample size, despite the asymptotic universal stability. This prediction is substantiated by a theoretical analysis as well as some empirical results. We conclude that stability remains a meaningful cluster validation criterion over finite samples.},
 author = {Shamir, Ohad and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Metadata.json},
 openalex = {W2138980613},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Supplemental.zip},
 title = {Cluster Stability for Finite Samples},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_818f4654,
 abstract = {Diffusion processes are a family of continuous-time continuous-state stochastic processes that are in general only partially observed. The joint estimation of the forcing parameters and the system noise (volatility) in these dynamical systems is a crucial, but non-trivial task, especially when the system is nonlinear and multi-modal. We propose a variational treatment of diffusion processes, which allows us to compute type II maximum likelihood estimates of the parameters by simple gradient techniques and which is computationally less demanding than most MCMC approaches. We also show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy.},
 author = {Archambeau, C\'{e}dric and Opper, Manfred and Shen, Yuan and Cornford, Dan and Shawe-taylor, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/818f4654ed39a1c147d1e51a00ffb4cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/818f4654ed39a1c147d1e51a00ffb4cb-Metadata.json},
 openalex = {W2103747245},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/818f4654ed39a1c147d1e51a00ffb4cb-Supplemental.zip},
 title = {Variational Inference for Diffusion Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/818f4654ed39a1c147d1e51a00ffb4cb-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_81e74d67,
 abstract = {We introduce a functional representation of time series which allows forecasts to be performed over an unspecified horizon with progressively-revealed information sets. By virtue of using Gaussian processes, a complete covariance matrix between forecasts at several time-steps is available. This information is put to use in an application to actively trade price spreads between commodity futures contracts. The approach delivers impressive out-of-sample risk-adjusted returns after transaction costs on a portfolio of 30 spreads.},
 author = {Chapados, Nicolas and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/81e74d678581a3bb7a720b019f4f1a93-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/81e74d678581a3bb7a720b019f4f1a93-Metadata.json},
 openalex = {W2152205111},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/81e74d678581a3bb7a720b019f4f1a93-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/81e74d678581a3bb7a720b019f4f1a93-Supplemental.zip},
 title = {Augmented Functional Time Series Representation and Forecasting with Gaussian Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/81e74d678581a3bb7a720b019f4f1a93-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_83fa5a43,
 abstract = {An important problem in many fields is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and lack an explicit provision to control the expressiveness of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.},
 author = {Shashanka, Madhusudana and Raj, Bhiksha and Smaragdis, Paris},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/83fa5a432ae55c253d0e60dbfa716723-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/83fa5a432ae55c253d0e60dbfa716723-Metadata.json},
 openalex = {W2154587273},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/83fa5a432ae55c253d0e60dbfa716723-Supplemental.zip},
 title = {Sparse Overcomplete Latent Variable Decomposition of Counts Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_84117275,
 abstract = {Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance profile of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output.},
 author = {Williams, Ben and Toussaint, Marc and Storkey, Amos J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/84117275be999ff55a987b9381e01f96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/84117275be999ff55a987b9381e01f96-Metadata.json},
 openalex = {W2149377129},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/84117275be999ff55a987b9381e01f96-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/84117275be999ff55a987b9381e01f96-Supplemental.zip},
 title = {Modelling motion primitives and their timing in biologically executed movements},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/84117275be999ff55a987b9381e01f96-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_846c260d,
 abstract = {We describe an analog-VLSI neural network for face recognition based on subspace methods. The system uses a dimensionality-reduction network whose coefficients can be either programmed or learned on-chip to perform PCA, or programmed to perform LDA. A second network with user-programmed coefficients performs classification with Manhattan distances. The system uses on-chip compensation techniques to reduce the effects of device mismatch. Using the ORL database with 12x12-pixel images, our circuit achieves up to 85% classification performance (98% of an equivalent software implementation).},
 author = {Carvajal, Gonzalo and Valenzuela, Waldo and Figueroa, Miguel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/846c260d715e5b854ffad5f70a516c88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/846c260d715e5b854ffad5f70a516c88-Metadata.json},
 openalex = {W2155867089},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/846c260d715e5b854ffad5f70a516c88-Supplemental.zip},
 title = {Subspace-Based Face Recognition in Analog VLSI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_851ddf50,
 abstract = {In problems where input features have varying amounts of noise, using distinct regularization hyperparameters for different features provides an effective means of managing model complexity. While regularizers for neural networks and support vector machines often rely on multiple hyperparameters, regularizers for structured prediction models (used in tasks such as sequence labeling or parsing) typically rely only on a single shared hyperparameter for all features. In this paper, we consider the problem of choosing regularization hyperparameters for log-linear models, a class of structured prediction probabilistic models which includes conditional random fields (CRFs). Using an implicit differentiation trick, we derive an efficient gradient-based method for learning Gaussian regularization priors with multiple hyperparameters. In both simulations and the real-world task of computational RNA secondary structure prediction, we find that multiple hyperparameter learning can provide a significant boost in accuracy compared to using only a single regularization hyperparameter.},
 author = {Foo, Chuan-sheng and B., Chuong and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/851ddf5058cf22df63d3344ad89919cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/851ddf5058cf22df63d3344ad89919cf-Metadata.json},
 openalex = {W2158915909},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/851ddf5058cf22df63d3344ad89919cf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient multiple hyperparameter learning for log-linear models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/851ddf5058cf22df63d3344ad89919cf-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8597a6cf,
 abstract = {Dynamic Bayesian networks are structured representations of stochastic processes. Despite their structure, exact inference in DBNs is generally intractable. One approach to approximate inference involves grouping the variables in the process into smaller factors and keeping independent beliefs over these factors. In this paper we present several techniques for decomposing a dynamic Bayesian network automatically to enable factored inference. We examine a number of features of a DBN that capture different types of dependencies that will cause error in factored inference. An empirical comparison shows that the most useful of these is a heuristic that estimates the mutual information introduced between factors by one step of belief propagation. In addition to features computed over entire factors, for efficiency we explored scores computed over pairs of variables. We present search methods that use these features, pairwise and not, to find a factorization, and we compare their results on several datasets. Automatic factorization extends the applicability of factored inference to large, complex models that are undesirable to factor by hand. Moreover, tests on real DBNs show that automatic factorization can achieve significantly lower error in some cases.},
 author = {Frogner, Charlie and Pfeffer, Avi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8597a6cfa74defcbde3047c891d78f90-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8597a6cfa74defcbde3047c891d78f90-Metadata.json},
 openalex = {W2159432773},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Discovering Weakly-Interacting Factors in a Complex Stochastic Process},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8597a6cfa74defcbde3047c891d78f90-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_85d8ce59,
 abstract = {The notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds. A key advantage of these bounds is that they are designed for specific learning algorithms, exploiting their particular properties. But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (i.i.d.). In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence, which is clear in system diagnosis or time series prediction problems. This paper studies the scenario where the observations are drawn from a stationary mixing sequence, which implies a dependence between observations that weaken over time. It proves novel stability-based generalization bounds that hold even with this more general setting. These bounds strictly generalize the bounds given in the i.i.d. case. It also illustrates their application in the case of several general classes of learning algorithms, including Support Vector Regression and Kernel Ridge Regression.},
 author = {Mohri, Mehryar and Rostamizadeh, Afshin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/85d8ce590ad8981ca2c8286f79f59954-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/85d8ce590ad8981ca2c8286f79f59954-Metadata.json},
 openalex = {W2129247014},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Stability Bounds for Non-i.i.d. Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_872488f8,
 abstract = {We propose a model that leverages the millions of clicks received by web search engines to predict document relevance. This allows the comparison of ranking functions when clicks are available but complete relevance judgments are not. After an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged. These predictions can be used to evaluate the performance of a search engine, using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain (DCG), so comparisons can be made across time and datasets. This contrasts with previous methods which can provide only pair-wise relevance judgments between results shown for the same query. When no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time. While our experiments are on sponsored search results, which is the financial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well. Furthermore, we give an algorithm to guide the selection of additional documents to judge to improve confidence.},
 author = {Carterette, Ben and Jones, Rosie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/872488f88d1b2db54d55bc8bba2fad1b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/872488f88d1b2db54d55bc8bba2fad1b-Metadata.json},
 openalex = {W2156160882},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/872488f88d1b2db54d55bc8bba2fad1b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8757150d,
 author = {Sumer, Ozgur and Acar, Umut and Ihler, Alexander and Mettu, Ramgopal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8757150decbd89b0f5442ca3db4d0e0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8757150decbd89b0f5442ca3db4d0e0e-Metadata.json},
 openalex = {W2158814117},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8757150decbd89b0f5442ca3db4d0e0e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8757150decbd89b0f5442ca3db4d0e0e-Supplemental.zip},
 title = {Efficient Bayesian Inference for Dynamically Changing Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8757150decbd89b0f5442ca3db4d0e0e-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_89d4402d,
 abstract = {Many formal models of cognition implicitly use subjective probability distributions to capture the assumptions of human learners. Most applications of these models determine these distributions indirectly. We propose a method for directly determining the assumptions of human learners by sampling from subjective probability distributions. Using a correspondence between a model of human choice and Markov chain Monte Carlo (MCMC), we describe a method for sampling from the distributions over objects that people associate with different categories. In our task, subjects choose whether to accept or reject a proposed change to an object. The task is constructed so that these decisions follow an MCMC acceptance rule, defining a Markov chain for which the stationary distribution is the category distribution. We test this procedure for both artificial categories acquired in the laboratory, and natural categories acquired from experience.},
 author = {Sanborn, Adam and Griffiths, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/89d4402dc03d3b7318bbac10203034ab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/89d4402dc03d3b7318bbac10203034ab-Metadata.json},
 openalex = {W2105889829},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/89d4402dc03d3b7318bbac10203034ab-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Markov Chain Monte Carlo with People},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/89d4402dc03d3b7318bbac10203034ab-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8a1e808b,
 abstract = {The peak location in a population of phase-tuned neurons has been shown to be a more reliable estimator for disparity than the peak location in a population of position-tuned neurons. Unfortunately, the disparity range covered by a phase-tuned population is limited by phase wraparound. Thus, a single population cannot cover the large range of disparities encountered in natural scenes unless the scale of the receptive fields is chosen to be very large, which results in very low resolution depth estimates. Here we describe a biologically plausible measure of the confidence that the stimulus disparity is inside the range covered by a population of phase-tuned neurons. Based upon this confidence measure, we propose an algorithm for disparity estimation that uses many populations of high-resolution phase-tuned neurons that are biased to different disparity ranges via position shifts between the left and right eye receptive fields. The population with the highest confidence is used to estimate the stimulus disparity. We show that this algorithm outperforms a previously proposed coarse-to-fine algorithm for disparity estimation, which uses disparity estimates from coarse scales to select the populations used at finer scales and can effectively detect occlusions.},
 author = {Tsang, Eric and Shi, Bertram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8a1e808b55fde9455cb3d8857ed88389-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8a1e808b55fde9455cb3d8857ed88389-Metadata.json},
 openalex = {W2157164951},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Estimating disparity with confidence from energy neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8a1e808b55fde9455cb3d8857ed88389-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8b6dd7db,
 abstract = {Functional Magnetic Resonance Imaging (fMRI) provides dynamical access into the complex functioning of the human brain, detailing the hemodynamic activity of thousands of voxels during hundreds of sequential time points. One approach towards illuminating the connection between fMRI and cognitive function is through decoding; how do the time series of voxel activities combine to provide information about internal and external experience? Here we seek models of fMRI decoding which are balanced between the simplicity of their interpretation and the effectiveness of their prediction. We use signals from a subject immersed in virtual reality to compare global and local methods of prediction applying both linear and nonlinear techniques of dimensionality reduction. We find that the prediction of complex stimuli is remarkably low-dimensional, saturating with less than 100 features. In particular, we build effective models based on the decorrelated components of cognitive activity in the classically-defined Brodmann areas. For some of the stimuli, the top predictive areas were surprisingly transparent, including Wernicke's area for verbal instructions, visual cortex for facial and body features, and visual-temporal regions for velocity. Direct sensory experience resulted in the most robust predictions, with the highest correlation ($c \sim 0.8$) between the predicted and experienced time series of verbal instructions. Techniques based on non-linear dimensionality reduction (Laplacian eigenmaps) performed similarly. The interpretability and relative simplicity of our approach provides a conceptual basis upon which to build more sophisticated techniques for fMRI decoding and offers a window into cognitive function during dynamic, natural experience.},
 author = {Meyer, Francois and Stephens, Greg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8b6dd7db9af49e67306feb59a8bdc52c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8b6dd7db9af49e67306feb59a8bdc52c-Metadata.json},
 openalex = {W2949112253},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Locality and low-dimensions in the prediction of natural experience from fMRI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8b6dd7db9af49e67306feb59a8bdc52c-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8c7bbbba,
 abstract = {Fair discriminative pedestrian finders are now available. In fact, these pedestrian finders make most errors on pedestrians in configurations that are uncommon in the training data, for example, mounting a bicycle. This is undesirable. However, the human configuration can itself be estimated discriminatively using structure learning. We demonstrate a pedestrian finder which first finds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset. We then present features (local histogram of oriented gradient and local PCA of gradient) based on that configuration to an SVM classifier. We show, using the INRIA Person dataset, that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder.},
 author = {Tran, Duan and Forsyth, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8c7bbbba95c1025975e548cee86dfadc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8c7bbbba95c1025975e548cee86dfadc-Metadata.json},
 openalex = {W2104679563},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8c7bbbba95c1025975e548cee86dfadc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Configuration Estimates Improve Pedestrian Finding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8c7bbbba95c1025975e548cee86dfadc-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8d317bdc,
 abstract = {We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly, this general framework enables us to use a standard regression base learner such as single regression tree for fitting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show significant improvements of our proposed methods over some existing methods.},
 author = {Zheng, Zhaohui and Zha, Hongyuan and Zhang, Tong and Chapelle, Olivier and Chen, Keke and Sun, Gordon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8d317bdcf4aafcfc22149d77babee96d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8d317bdcf4aafcfc22149d77babee96d-Metadata.json},
 openalex = {W2100235073},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8d317bdcf4aafcfc22149d77babee96d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A General Boosting Method and its Application to Learning Ranking Functions for Web Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8d317bdcf4aafcfc22149d77babee96d-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8d6dc35e,
 abstract = {We present a novel message passing algorithm for approximating the MAP problem in graphical models. The algorithm is similar in structure to max-product but unlike max-product it always converges, and can be proven to find the exact MAP solution in various settings. The algorithm is derived via block coordinate descent in a dual of the LP relaxation of MAP, but does not require any tunable parameters such as step size or tree weights. We also describe a generalization of the method to cluster based potentials. The new method is tested on synthetic and real-world problems, and compares favorably with previous approaches.},
 author = {Globerson, Amir and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8d6dc35e506fc23349dd10ee68dabb64-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8d6dc35e506fc23349dd10ee68dabb64-Metadata.json},
 openalex = {W2149474573},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8d6dc35e506fc23349dd10ee68dabb64-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8e6b42f1,
 abstract = {This paper describes a new model for human visual classification that enables the recovery of image features that explain human subjects' performance on different visual classification tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classifier operating on raw image pixels. Instead, it represents classification as the combination of multiple feature detectors. This approach extracts more information about human visual classification than previous methods and provides a foundation for further exploration.},
 author = {Ross, Michael and Cohen, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8e6b42f1644ecb1327dc03ab345e618b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8e6b42f1644ecb1327dc03ab345e618b-Metadata.json},
 openalex = {W2140287249},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {GRIFT: A graphical model for inferring visual classification features from human data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8e6b42f1644ecb1327dc03ab345e618b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8f1d4362,
 abstract = {We describe a neurobiologically plausible model to implement dynamic routing using the concept of neuronal communication through neuronal coherence. The model has a three-tier architecture: a raw input tier, a routing control tier, and an invariant output tier. The correct mapping between input and output tiers is realized by an appropriate alignment of the phases of their respective background oscillations by the routing control units. We present an example architecture, implemented on a neuromorphic chip, that is able to achieve circular-shift invariance. A simple extension to our model can accomplish circular-shift dynamic routing with only O(N) connections, compared to O(N2) connections required by traditional models.},
 author = {Sridharan, Devarajan and Percival, Brian and Arthur, John and Boahen, Kwabena A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8f1d43620bc6bb580df6e80b0dc05c48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8f1d43620bc6bb580df6e80b0dc05c48-Metadata.json},
 openalex = {W2131033938},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {An in-silico Neural Model of Dynamic Routing through Neuronal Coherence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8f855179,
 abstract = {We present an agnostic active learning algorithm for any hypothesis class of bounded VC dimension under arbitrary data distributions. Most previous work on active learning either makes strong distributional assumptions, or else is computationally prohibitive. Our algorithm extends the simple scheme of Cohn, Atlas, and Ladner [1] to the agnostic setting, using reductions to supervised learning that harness generalization bounds in a simple but subtle manner. We provide a fall-back guarantee that bounds the algorithm's label complexity by the agnostic PAC sample complexity. Our analysis yields asymptotic label complexity improvements for certain hypothesis classes and distributions. We also demonstrate improvements experimentally.},
 author = {Dasgupta, Sanjoy and Hsu, Daniel J and Monteleoni, Claire},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8f85517967795eeef66c225f7883bdcb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8f85517967795eeef66c225f7883bdcb-Metadata.json},
 openalex = {W2117756453},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A general agnostic active learning algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8f85517967795eeef66c225f7883bdcb-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_8fe0093b,
 abstract = {We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed [1]. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis (PCA) with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals X that are related or are not related to some additional target signal YT. In a biological interpretation, this target signal YT (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals.},
 author = {Buesing, Lars and Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8fe0093bb30d6f8c31474bd0764e6ac0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8fe0093bb30d6f8c31474bd0764e6ac0-Metadata.json},
 openalex = {W2154233093},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8fe0093bb30d6f8c31474bd0764e6ac0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8fe0093bb30d6f8c31474bd0764e6ac0-Supplemental.zip},
 title = {Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/8fe0093bb30d6f8c31474bd0764e6ac0-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_912d2b1c,
 abstract = {When predicting class labels for objects within a relational database, it is often helpful to consider a model for relationships: this allows for information between class labels to be shared and to improve prediction performance. However, there are different ways by which objects can be related within a relational database. One traditional way corresponds to a Markov network structure: each existing relation is represented by an undirected edge. This encodes that, conditioned on input features, each object label is independent of other object labels given its neighbors in the graph. However, there is no reason why Markov networks should be the only representation of choice for symmetric dependence structures. Here we discuss the case when relationships are postulated to exist due to hidden common causes. We discuss how the resulting graphical model differs from Markov networks, and how it describes different types of real-world relational processes. A Bayesian nonparametric classification model is built upon this graphical representation and evaluated with several empirical studies.},
 author = {Silva, Ricardo and Chu, Wei and Ghahramani, Zoubin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/912d2b1c7b2826caf99687388d2e8f7c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/912d2b1c7b2826caf99687388d2e8f7c-Metadata.json},
 openalex = {W2118159011},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Hidden Common Cause Relations in Relational Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/912d2b1c7b2826caf99687388d2e8f7c-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9232fe81,
 abstract = {We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images.},
 author = {Osindero, Simon and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9232fe81225bcaef853ae32870a2b0fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9232fe81225bcaef853ae32870a2b0fe-Metadata.json},
 openalex = {W2134653808},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Modeling image patches with a directed hierarchy of Markov random fields},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9232fe81225bcaef853ae32870a2b0fe-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_94c7bb58,
 abstract = {We present an efficient generalization of the sparse pseudo-input Gaussian process (SPGP) model developed by Snelson and Ghahramani [1], applying it to binary classification problems. By taking advantage of the SPGP prior covariance structure, we derive a numerically stable algorithm with O(NM2) training complexity—asymptotically the same as related sparse methods such as the informative vector machine [2], but which more faithfully represents the posterior. We present experimental results for several benchmark problems showing that in many cases this allows an exceptional degree of sparsity without compromising accuracy. Following [1], we locate pseudo-inputs by gradient ascent on the marginal likelihood, but exhibit occasions when this is likely to fail, for which we suggest alternative solutions.},
 author = {Naish-guzman, Andrew and Holden, Sean},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/94c7bb58efc3b337800875b5d382a072-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/94c7bb58efc3b337800875b5d382a072-Metadata.json},
 openalex = {W2148006136},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/94c7bb58efc3b337800875b5d382a072-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/94c7bb58efc3b337800875b5d382a072-Supplemental.zip},
 title = {The Generalized FITC Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/94c7bb58efc3b337800875b5d382a072-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9778d5d2,
 abstract = {We describe a new algorithm, Relaxed Survey Propagation (RSP), for finding MAP configurations in Markov random fields. We compare its performance with state-of-the-art algorithms including the max-product belief propagation, its sequential tree-reweighted variant, residual (sum-product) belief propagation, and tree-structured expectation propagation. We show that it outperforms all approaches for Ising models with mixed couplings, as well as on a web person disambiguation task formulated as a supervised clustering problem.},
 author = {Chieu, Hai and Lee, Wee and Teh, Yee},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9778d5d219c5080b9a6a17bef029331c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9778d5d219c5080b9a6a17bef029331c-Metadata.json},
 openalex = {W2151213817},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Cooled and Relaxed Survey Propagation for MRFs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9778d5d219c5080b9a6a17bef029331c-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9872ed9f,
 abstract = {Learning the common structure shared by a set of supervised tasks is an important practical and theoretical problem. Knowledge of this structure may lead to better generalization performance on the tasks and may also facilitate learning new tasks. We propose a framework for solving this problem, which is based on regularization with spectral functions of matrices. This class of regularization problems exhibits appealing computational properties and can be optimized efficiently by an alternating minimization algorithm. In addition, we provide a necessary and sufficient condition for convexity of the regularizer. We analyze concrete examples of the framework, which are equivalent to regularization with Lp matrix norms. Experiments on two real data sets indicate that the algorithm scales well with the number of tasks and improves on state of the art statistical performance.},
 author = {Argyriou, Andreas and Pontil, Massimiliano and Ying, Yiming and Micchelli, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9872ed9fc22fc182d371c3e9ed316094-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9872ed9fc22fc182d371c3e9ed316094-Metadata.json},
 openalex = {W2170563643},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Spectral Regularization Framework for Multi-Task Structure Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9872ed9fc22fc182d371c3e9ed316094-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9908279e,
 abstract = {This paper proposes constraint propagation relaxation (CPR), a probabilistic approach to classical constraint propagation that provides another view on the whole parametric family of survey propagation algorithms SP(ρ). More importantly, the approach elucidates the implicit, but fundamental assumptions underlying SP(ρ), thus shedding some light on its effectiveness and leading to applications beyond k-SAT.},
 author = {Ortiz, Luis E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9908279ebbf1f9b250ba689db6a0222b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9908279ebbf1f9b250ba689db6a0222b-Metadata.json},
 openalex = {W2111421282},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {CPR for CSPs: A Probabilistic Relaxation of Constraint Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9908279ebbf1f9b250ba689db6a0222b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9b698eb3,
 abstract = {Reward-modulated spike-timing-dependent plasticity (STDP) has recently emerged as a candidate for a learning rule that could explain how local learning rules at single synapses support behaviorally relevant adaptive changes in complex networks of spiking neurons. However the potential and limitations of this learning rule could so far only be tested through computer simulations. This article provides tools for an analytic treatment of reward-modulated STDP, which allow us to predict under which conditions reward-modulated STDP will be able to achieve a desired learning effect. In particular, we can produce in this way a theoretical explanation and a computer model for a fundamental experimental finding on biofeedback in monkeys (reported in [1]).},
 author = {Pecevski, Dejan and Maass, Wolfgang and Legenstein, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9b698eb3105bd82528f23d0c92dedfc0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9b698eb3105bd82528f23d0c92dedfc0-Metadata.json},
 openalex = {W2107931919},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9b698eb3105bd82528f23d0c92dedfc0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9b698eb3105bd82528f23d0c92dedfc0-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9c01802d,
 abstract = {Automatic relevance determination (ARD) and the closely-related sparse Bayesian learning (SBL) framework are effective tools for pruning large numbers of irrelevant features leading to a sparse explanatory subset. However, popular update rules used for ARD are either difficult to extend to more general problems of interest or are characterized by non-ideal convergence properties. Moreover, it remains unclear exactly how ARD relates to more traditional MAP estimation-based methods for learning sparse representations (e.g., the Lasso). This paper furnishes an alternative means of expressing the ARD cost function using auxiliary functions that naturally addresses both of these issues. First, the proposed reformulation of ARD can naturally be optimized by solving a series of re-weighted l1 problems. The result is an efficient, extensible algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a local minimum (or saddle point). Secondly, the analysis reveals that ARD is exactly equivalent to performing standard MAP estimation in weight space using a particular feature- and noise-dependent, non-factorial weight prior. We then demonstrate that this implicit prior maintains several desirable advantages over conventional priors with respect to feature selection. Overall these results suggest alternative cost functions and update procedures for selecting features and promoting sparse solutions in a variety of general situations. In particular, the methodology readily extends to handle problems such as non-negative sparse coding and covariance component estimation.},
 author = {Wipf, David and Nagarajan, Srikantan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Metadata.json},
 openalex = {W2171980229},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A New View of Automatic Relevance Determination},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9c01802ddb981e6bcfbec0f0516b8e35-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9c82c714,
 abstract = {Most models of decision-making in neuroscience assume an infinite horizon, which yields an optimal solution that integrates evidence up to a fixed decision threshold; however, under most experimental as well as naturalistic behavioral settings, the decision has to be made before some finite deadline, which is often experienced as a stochastic quantity, either due to variable external constraints or internal timing uncertainty. In this work, we formulate this problem as sequential hypothesis testing under a stochastic horizon. We use dynamic programming tools to show that, for a large class of deadline distributions, the Bayes-optimal solution requires integrating evidence up to a threshold that declines monotonically over time. We use numerical simulations to illustrate the optimal policy in the special cases of a fixed deadline and one that is drawn from a gamma distribution.},
 author = {Frazier, Peter and Yu, Angela J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9c82c7143c102b71c593d98d96093fde-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9c82c7143c102b71c593d98d96093fde-Metadata.json},
 openalex = {W2117166535},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9c82c7143c102b71c593d98d96093fde-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sequential Hypothesis Testing under Stochastic Deadlines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9c82c7143c102b71c593d98d96093fde-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9cc138f8,
 abstract = {We demonstrate that log-linear grammars with latent variables can be practically trained using discriminative methods. Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient-based procedure. We compare L1 and L2 regularization and show that L1 regularization is superior, requiring fewer iterations to converge, and yielding sparser solutions. On full-scale treebank parsing experiments, the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non-latent baselines.},
 author = {Petrov, Slav and Klein, Dan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9cc138f8dc04cbf16240daa92d8d50e2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9cc138f8dc04cbf16240daa92d8d50e2-Metadata.json},
 openalex = {W2168194229},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9cc138f8dc04cbf16240daa92d8d50e2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Discriminative Log-Linear Grammars with Latent Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9cc138f8dc04cbf16240daa92d8d50e2-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9dcb88e0,
 abstract = {We present a novel paradigm for statistical machine translation (SMT), based on a joint modeling of word alignment and the topical aspects underlying bilingual document-pairs, via a hidden Markov Bilingual Topic AdMixture (HM-BiTAM). In this paradigm, parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-flow, to ensure coherence of topical context in the alignment of mapping words between languages, likelihood-based training of topic-dependent translational lexicons, as well as in the inference of topic representations in each language. The learned HM-BiTAM can not only display topic patterns like methods such as LDA [1], but now for bilingual corpora; it also offers a principled way of inferring optimal translation using document context. Our method integrates the conventional model of HMM — a key component for most of the state-of-the-art SMT systems, with the recently proposed BiTAM model [10]; we report an extensive empirical analysis (in many ways complementary to the description-oriented [10]) of our method in three aspects: bilingual topic representation, word alignment, and translation.},
 author = {Zhao, Bing and Xing, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9dcb88e0137649590b755372b040afad-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9dcb88e0137649590b755372b040afad-Metadata.json},
 openalex = {W2116229791},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9dcb88e0137649590b755372b040afad-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9dcb88e0137649590b755372b040afad-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9f396fe4,
 abstract = {We present an algorithm called Optimistic Linear Programming (OLP) for learning to optimize average reward in an irreducible but otherwise unknown Markov decision process (MDP). OLP uses its experience so far to estimate the MDP. It chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates, a computation that corresponds to solving linear programs. We show that the total expected reward obtained by OLP up to time T is within C(P) log T of the reward obtained by the optimal policy, where C(P) is an explicit, MDP-dependent constant. OLP is closely related to an algorithm proposed by Burnetas and Katehakis with four key differences: OLP is simpler, it does not require knowledge of the supports of transition probabilities, the proof of the regret bound is simpler, but our regret bound is a constant factor larger than the regret of their algorithm. OLP is also similar in flavor to an algorithm recently proposed by Auer and Ortner. But OLP is simpler and its regret bound has a better dependence on the size of the MDP.},
 author = {Tewari, Ambuj and Bartlett, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f396fe44e7c05c16873b05ec425cbad-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f396fe44e7c05c16873b05ec425cbad-Metadata.json},
 openalex = {W2145049547},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f396fe44e7c05c16873b05ec425cbad-Supplemental.zip},
 title = {Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9f396fe44e7c05c16873b05ec425cbad-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9f53d83e,
 abstract = {We extend the Bayesian skill rating system TrueSkill to infer entire time series of skills of players by smoothing through time instead of filtering. The skill of each participating player, say, every year is represented by a latent skill variable which is affected by the relevant game outcomes that year, and coupled with the skill variables of the previous and subsequent year. Inference in the resulting factor graph is carried out by approximate message passing (EP) along the time series of skills. As before the system tracks the uncertainty about player skills, explicitly models draws, can deal with any number of competing entities and can infer individual skills from team results. We extend the system to estimate player-specific draw margins. Based on these models we present an analysis of the skill curves of important players in the history of chess over the past 150 years. Results include plots of players' lifetime skill development as well as the ability to compare the skills of different players across time. Our results indicate that the overall playing strength has increased over the past 150 years, and that modelling a player's ability to force a draw provides significantly better predictive power.},
 author = {Dangauthier, Pierre and Herbrich, Ralf and Minka, Tom and Graepel, Thore},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f53d83ec0691550f7d2507d57f4f5a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f53d83ec0691550f7d2507d57f4f5a2-Metadata.json},
 openalex = {W2132726007},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {TrueSkill Through Time: Revisiting the History of Chess},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9f53d83ec0691550f7d2507d57f4f5a2-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9f61408e,
 abstract = {Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efficient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.},
 author = {Roux, Nicolas and Manzagol, Pierre-antoine and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f61408e3afb633e50cdf1b20de6f466-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f61408e3afb633e50cdf1b20de6f466-Metadata.json},
 openalex = {W2164273299},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f61408e3afb633e50cdf1b20de6f466-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Topmoumoute Online Natural Gradient Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9f61408e3afb633e50cdf1b20de6f466-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_9fc3d715,
 abstract = {We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data.},
 author = {Freund, Yoav and Dasgupta, Sanjoy and Kabra, Mayank and Verma, Nakul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9fc3d7152ba9336a670e36d0ed79bc43-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9fc3d7152ba9336a670e36d0ed79bc43-Metadata.json},
 openalex = {W2165888865},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning the structure of manifolds using random projections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/9fc3d7152ba9336a670e36d0ed79bc43-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a0e2a2c5,
 abstract = {A discriminative method is proposed for learning monotonic transformations of the training data while jointly estimating a large-margin classifier. In many domains such as document classification, image histogram classification and gene microarray experiments, fixed monotonic transformations can be useful as a preprocessing step. However, most classifiers only explore these transformations through manual trial and error or via prior domain knowledge. The proposed method learns monotonic transformations automatically while training a large-margin classifier without any prior knowledge of the domain. A monotonic piecewise linear function is learned which transforms data for subsequent processing by a linear hyperplane classifier. Two algorithmic implementations of the method are formalized. The first solves a convergent alternating sequence of quadratic and linear programs until it obtains a locally optimal solution. An improved algorithm is then derived using a convex semidefinite relaxation that overcomes initialization issues in the greedy optimization problem. The effectiveness of these learned transformations on synthetic problems, text data and image data is demonstrated.},
 author = {Howard, Andrew and Jebara, Tony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a0e2a2c563d57df27213ede1ac4ac780-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a0e2a2c563d57df27213ede1ac4ac780-Metadata.json},
 openalex = {W2112353934},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a0e2a2c563d57df27213ede1ac4ac780-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning Monotonic Transformations for Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a0e2a2c563d57df27213ede1ac4ac780-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a1140a3d,
 abstract = {Estimation of three-dimensional articulated human pose and motion from images is a central problem in computer vision. Much of the previous work has been limited by the use of crude generative models of humans represented as articulated collections of simple parts such as cylinders. Automatic initialization of such models has proved difficult and most approaches assume that the size and shape of the body parts are known a priori. In this paper we propose a method for automatically recovering a detailed parametric model of non-rigid body shape and pose from monocular imagery. Specifically, we represent the body using a parameterized triangulated mesh model that is learned from a database of human range scans. We demonstrate a discriminative method to directly recover the model parameters from monocular images using a conditional mixture of kernel regressors. This predicted pose and shape are used to initialize a generative model for more detailed pose and shape estimation. The resulting approach allows fully automatic pose and shape recovery from monocular and multi-camera imagery. Experimental results show that our method is capable of robustly recovering articulated pose, shape and biometric measurements (e.g. height, weight, etc.) in both calibrated and uncalibrated camera environments.},
 author = {Sigal, Leonid and Balan, Alexandru and Black, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a1140a3d0df1c81e24ae954d935e8926-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a1140a3d0df1c81e24ae954d935e8926-Metadata.json},
 openalex = {W2103025041},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Combined discriminative and generative articulated pose and non-rigid shape estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a1519de5,
 abstract = {We present a framework for active learning in the multiple-instance (MI) setting. In an MI learning problem, instances are naturally organized into bags and it is the bags, instead of individual instances, that are labeled for training. MI learners assume that every instance in a bag labeled negative is actually negative, whereas at least one instance in a bag labeled positive is actually positive. We consider the particular case in which an MI learner is allowed to selectively query unlabeled instances from positive bags. This approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible, but expensive, to acquire instance labels. We describe a method for learning from labels at mixed levels of granularity, and introduce two active query selection strategies motivated by the MI setting. Our experiments show that learning from instance labels can significantly improve performance of a basic MI learning algorithm in two multiple-instance domains: content-based image retrieval and text classification.},
 author = {Settles, Burr and Craven, Mark and Ray, Soumya},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a1519de5b5d44b31a01de013b9b51a80-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a1519de5b5d44b31a01de013b9b51a80-Metadata.json},
 openalex = {W2128678390},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multiple-Instance Active Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a1519de5b5d44b31a01de013b9b51a80-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a34bacf8,
 abstract = {Scene recognition has been widely studied to understand visual information from the level of objects and their relationships. Toward scene recognition, many methods have been proposed. They, however, encounter difficulty to improve the accuracy, mainly due to two limitations: 1) lack of analysis of intrinsic relationships across different scales, say, the initial input and its down-sampled versions and 2) existence of redundant features. This paper develops a semi-supervised learning mechanism to reduce the above two limitations. To address the first limitation, we propose a multitask model to integrate scene images of different resolutions. For the second limitation, we build a model of sparse feature selection-based manifold regularization (SFSMR) to select the optimal information and preserve the underlying manifold structure of data. SFSMR coordinates the advantages of sparse feature selection and manifold regulation. Finally, we link the multitask model and SFSMR, and propose the semi-supervised learning method to reduce the two limitations. Experimental results report the improvements of the accuracy in scene recognition.},
 author = {Liu, Qiuhua and Liao, Xuejun and Carin, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a34bacf839b923770b2c360eefa26748-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a34bacf839b923770b2c360eefa26748-Metadata.json},
 openalex = {W1977617632},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a34bacf839b923770b2c360eefa26748-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Semi-Supervised Multitask Learning for Scene Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a34bacf839b923770b2c360eefa26748-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a4f23670,
 abstract = {A non-linear dynamic system is called contracting if initial conditions are forgotten exponentially fast, so that all trajectories converge to a single trajectory. We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. Specifically, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. This specific type of network is believed to play a major role in shaping cortical responses and selecting the relevant signal among distractors and noise. In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses.},
 author = {Neftci, Emre and Chicca, Elisabetta and Indiveri, Giacomo and Slotine, Jean-jeacques and Douglas, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a4f23670e1833f3fdb077ca70bbd5d66-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a4f23670e1833f3fdb077ca70bbd5d66-Metadata.json},
 openalex = {W2106164734},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a4f23670e1833f3fdb077ca70bbd5d66-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a4f23670e1833f3fdb077ca70bbd5d66-Supplemental.zip},
 title = {Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a4f23670e1833f3fdb077ca70bbd5d66-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a532400e,
 abstract = {Large repositories of source code create new challenges and opportunities for statistical machine learning. Here we first develop Sourcerer, an infrastructure for the automated crawling, parsing, and database storage of open source software. Sourcerer allows us to gather Internet-scale source code. For instance, in one experiment, we gather 4,632 java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, SLOC, and lexical containment distributions. We then develop and apply unsupervised author-topic, probabilistic models to automatically discover the topics embedded in the code and extract topic-word and author-topic distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the AUC metric to 0.84- roughly 10-30% better than previous approaches based on text alone. Supplementary material may be found at: http://sourcerer.ics.uci.edu/nips2007/nips07.html.},
 author = {Linstead, Erik and Rigor, Paul and Bajracharya, Sushil and Lopes, Cristina and Baldi, Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a532400ed62e772b9dc0b86f46e583ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a532400ed62e772b9dc0b86f46e583ff-Metadata.json},
 openalex = {W2141102925},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a532400ed62e772b9dc0b86f46e583ff-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Mining Internet-Scale Software Repositories},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a532400ed62e772b9dc0b86f46e583ff-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a5771bce,
 abstract = {Sound localization by barn owls is commonly modeled as a matching procedure where localization cues derived from auditory inputs are compared to stored templates. While the matching models can explain properties of neural responses, no model explains how the owl resolves spatial ambiguity in the localization cues to produce accurate localization for sources near the center of gaze. Here, I examine two models for the barn owl's sound localization behavior. First, I consider a maximum likelihood estimator in order to further evaluate the cue matching model. Second, I consider a maximum a posteriori estimator to test whether a Bayesian model with a prior that emphasizes directions near the center of gaze can reproduce the owl's localization behavior. I show that the maximum likelihood estimator can not reproduce the owl's behavior, while the maximum a posteriori estimator is able to match the behavior. This result suggests that the standard cue matching model will not be sufficient to explain sound localization behavior in the barn owl. The Bayesian model provides a new framework for analyzing sound localization in the barn owl and leads to predictions about the owl's localization behavior.},
 author = {Fischer, Brian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a5771bce93e200c36f7cd9dfd0e5deaa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a5771bce93e200c36f7cd9dfd0e5deaa-Metadata.json},
 openalex = {W2133610747},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a5771bce93e200c36f7cd9dfd0e5deaa-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a5771bce93e200c36f7cd9dfd0e5deaa-Supplemental.zip},
 title = {Optimal models of sound localization by barn owls},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a5771bce93e200c36f7cd9dfd0e5deaa-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a5cdd4aa,
 abstract = {We present a theoretical study on the discriminative clustering framework, recently proposed for simultaneous subspace selection via linear discriminant analysis (LDA) and clustering. Empirical results have shown its favorable performance in comparison with several other popular clustering algorithms. However, the inherent relationship between subspace selection and clustering in this framework is not well understood, due to the iterative nature of the algorithm. We show in this paper that this iterative subspace selection and clustering is equivalent to kernel K-means with a specific kernel Gram matrix. This provides significant and new insights into the nature of this subspace selection procedure. Based on this equivalence relationship, we propose the Discriminative K-means (DisKmeans) algorithm for simultaneous LDA subspace selection and clustering, as well as an automatic parameter estimation procedure. We also present the nonlinear extension of DisKmeans using kernels. We show that the learning of the kernel matrix over a convex set of pre-specified kernel matrices can be incorporated into the clustering formulation. The connection between DisKmeans and several other clustering algorithms is also analyzed. The presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets.},
 author = {Ye, Jieping and Zhao, Zheng and Wu, Mingrui},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Metadata.json},
 openalex = {W2132771435},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Discriminative K-means for Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a5cdd4aa0048b187f7182f1b9ce7a6a7-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a8abb4bb,
 abstract = {In bioinformatics it is often desirable to combine data from various measurement sources and thus structured feature vectors are to be analyzed that possess different intrinsic blocking characteristics (e.g., different patterns of missing values, observation noise levels, effective intrinsic dimensionalities). We propose a new machine learning tool, heterogeneous component analysis (HCA), for feature extraction in order to better understand the factors that underlie such complex structured heterogeneous data. HCA is a linear block-wise sparse Bayesian PCA based not only on a probabilistic model with block-wise residual variance terms but also on a Bayesian treatment of a block-wise sparse factor-loading matrix. We study various algorithms that implement our HCA concept extracting sparse heterogeneous structure by obtaining common components for the blocks and specific components within each block. Simulations on toy and bioinformatics data underline the usefulness of the proposed structured matrix factorization concept.},
 author = {Oba, Shigeyuki and Kawanabe, Motoaki and M\"{u}ller, Klaus-Robert and Ishii, Shin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a8abb4bb284b5b27aa7cb790dc20f80b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a8abb4bb284b5b27aa7cb790dc20f80b-Metadata.json},
 openalex = {W2147384530},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a8abb4bb284b5b27aa7cb790dc20f80b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Heterogeneous Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a8abb4bb284b5b27aa7cb790dc20f80b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a8e864d0,
 abstract = {We study a pattern classification algorithm which has recently been proposed by Vapnik and coworkers. It builds on a new inductive principle which assumes that in addition to positive and negative data, a third class of data is available, termed the Universum. We assay the behavior of the algorithm by establishing links with Fisher discriminant analysis and oriented PCA, as well as with an SVM in a projected subspace (or, equivalently, with a data-dependent reduced kernel). We also provide experimental results.},
 author = {Chapelle, Olivier and Agarwal, Alekh and Sinz, Fabian and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a8e864d04c95572d1aece099af852d0a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a8e864d04c95572d1aece099af852d0a-Metadata.json},
 openalex = {W2171394671},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a8e864d04c95572d1aece099af852d0a-Supplemental.zip},
 title = {An Analysis of Inference with the Universum},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a8e864d04c95572d1aece099af852d0a-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a9a1d531,
 abstract = {Many agent-environment interactions can be framed as dynamical systems in which agents take actions and receive observations. These dynamical systems are diverse, representing such things as a biped walking, a stock price changing over time, the trajectory of a missile, or the shifting fish population in a lake. 
Often, interacting successfully with the environment requires the use of a model, which allows the agent to predict something about the future by summarizing the past. Two of the basic problems in modeling partially observable dynamical systems are selecting a representation of state and selecting a mechanism for maintaining that state. This thesis explores both problems from a learning perspective: we are interested in learning a predictive model directly from the data that arises as an agent interacts with its environment. 
This thesis develops models for dynamical systems which represent state as a set of statistics about the short-term future, as opposed to treating state as a latent, unobservable quantity. In other words, the agent summarizes the past into predictions about the short-term future, which allow the agent to make further predictions about the infinite future. Because all parameters in the model are defined using only observable quantities, the learning algorithms for such models are often straightforward and have attractive theoretical properties. We examine in depth the case where state is represented as the parameters of an exponential family distribution over a short-term window of future observations. We unify a number of different existing models under this umbrella, and predict and analyze new models derived from the generalization. 
One goal of this research is to push models with predictively defined state towards real-world applications. We contribute models and companion learning algorithms for domains with partial observability, continuous observations, structured observations, high-dimensional observations, and/or continuous actions. Our models successfully capture standard POMDPs and benchmark nonlinear timeseries problems with performance comparable to state-of-the-art models. They also allow us to perform well on novel domains which are larger than those captured by other models with predictively defined state, including traffic prediction problems and domains analogous to autonomous mobile robots with camera sensors.},
 author = {Wingate, David and Baveja, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a1d5317a33ae8cef33961c34144f84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a1d5317a33ae8cef33961c34144f84-Metadata.json},
 openalex = {W2171627535},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Exponential Family Predictive Representations of State},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_a9a6653e,
 abstract = {This paper studies boosting algorithms that make a single pass over a set of base classifiers.

We first analyze a one-pass algorithm in the setting of boosting with diverse base classifiers. Our guarantee is the same as the best proved for any boosting algorithm, but our one-pass algorithm is much faster than previous approaches.

We next exhibit a random source of examples for which a variant of Ad-aBoost that skips poor base classifiers can outperform the standard AdaBoost algorithm, which uses every base classifier, by an exponential factor.

Experiments with Reuters and synthetic data show that one-pass boosting can substantially improve on the accuracy of Naive Bayes, and that picky boosting can sometimes lead to a further improvement in accuracy.},
 author = {Barutcuoglu, Zafer and Long, Phil and Servedio, Rocco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a6653e48976138166de32772b1bf40-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a6653e48976138166de32772b1bf40-Metadata.json},
 openalex = {W2099011549},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a6653e48976138166de32772b1bf40-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a6653e48976138166de32772b1bf40-Supplemental.zip},
 title = {One-Pass Boosting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/a9a6653e48976138166de32772b1bf40-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_aa68c75c,
 abstract = {Semi-supervised learning, i.e. learning from both labeled and unlabeled data has received significant attention in the machine learning literature in recent years. Still our understanding of the theoretical foundations of the usefulness of unlabeled data remains somewhat limited. The simplest and the best understood situation is when the data is described by an identifiable mixture model, and where each class comes from a pure component. This natural setup and its implications ware analyzed in [11, 5]. One important result was that in certain regimes, labeled data becomes exponentially more valuable than unlabeled data.

However, in most realistic situations, one would not expect that the data comes from a parametric mixture distribution with identifiable components. There have been recent efforts to analyze the non-parametric situation, for example, cluster and manifold assumptions have been suggested as a basis for analysis. Still, a satisfactory and fairly complete theoretical understanding of the nonparametric problem, similar to that in [11, 5] has not yet been developed.

In this paper we investigate an intermediate situation, when the data comes from a probability distribution, which can be modeled, but not perfectly, by an identifiable mixture distribution. This seems applicable to many situation, when, for example, a mixture of Gaussians is used to model the data. the contribution of this paper is an analysis of the role of labeled and unlabeled data depending on the amount of imperfection in the model.},
 author = {Sinha, Kaushik and Belkin, Mikhail},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/aa68c75c4a77c87f97fb686b2f068676-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/aa68c75c4a77c87f97fb686b2f068676-Metadata.json},
 openalex = {W2122515188},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Value of Labeled and Unlabeled Examples when the Model is Imperfect},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/aa68c75c4a77c87f97fb686b2f068676-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_ac1dd209,
 abstract = {A new algorithm for on-line learning linear-threshold functions is proposed which efficiently combines second-order statistics about the data with the logarithmic behavior of multiplicative/dual-norm algorithms. An initial theoretical analysis is provided suggesting that our algorithm might be viewed as a standard Perceptron algorithm operating on a transformed sequence of examples with improved margin properties. We also report on experiments carried out on datasets from diverse domains, with the goal of comparing to known Perceptron algorithms (first-order, second-order, additive, multiplicative). Our learning procedure seems to generalize quite well, and converges faster than the corresponding multiplicative baseline algorithms.},
 author = {Gentile, Claudio and Vitale, Fabio and Brotto, Cristian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Metadata.json},
 openalex = {W2157512055},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On higher-order perceptron algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_afd48367,
 abstract = {We study the rates of growth of the regret in online convex optimization. First, we show that a simple extension of the algorithm of Hazan et al eliminates the need for a priori knowledge of the lower bound on the second derivatives of the observed functions. We then provide an algorithm, Adaptive Online Gradient Descent, which interpolates between the results of Zinkevich for linear functions and of Hazan et al for strongly convex functions, achieving intermediate rates between √T and log T. Furthermore, we show strong optimality of the algorithm. Finally, we provide an extension of our results to general norms.},
 author = {Hazan, Elad and Rakhlin, Alexander and Bartlett, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/afd4836712c5e77550897e25711e1d96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/afd4836712c5e77550897e25711e1d96-Metadata.json},
 openalex = {W2139759436},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/afd4836712c5e77550897e25711e1d96-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Online Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/afd4836712c5e77550897e25711e1d96-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_aff16212,
 abstract = {Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use approximate Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 104 possible faults from 105 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a mean-field variational approximation and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours.},
 author = {Kiciman, Emre and Maltz, David and Platt, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/aff1621254f7c1be92f64550478c56e6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/aff1621254f7c1be92f64550478c56e6-Metadata.json},
 openalex = {W2130527038},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fast Variational Inference for Large-scale Internet Diagnosis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/aff1621254f7c1be92f64550478c56e6-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_b0ab42fc,
 abstract = {Much of human knowledge is organized into sophisticated systems that are often called intuitive theories. We propose that intuitive theories are mentally represented in a logical language, and that the subjective complexity of a theory is determined by the length of its representation in this language. This complexity measure helps to explain how theories are learned from relational data, and how they support inductive inferences about unobserved relations. We describe two experiments that test our approach, and show that it provides a better account of human learning and reasoning than an approach developed by Goodman [1].},
 author = {Kemp, Charles and Goodman, Noah and Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b0ab42fcb7133122b38521d13da7120b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b0ab42fcb7133122b38521d13da7120b-Metadata.json},
 openalex = {W2169382901},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b0ab42fcb7133122b38521d13da7120b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning and using relational theories},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/b0ab42fcb7133122b38521d13da7120b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_b0b183c2,
 abstract = {We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite— consider, for example, symbols being possible words appearing in English text.},
 author = {Mochihashi, Daichi and Sumita, Eiichiro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b0b183c207f46f0cca7dc63b2604f5cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b0b183c207f46f0cca7dc63b2604f5cc-Metadata.json},
 openalex = {W2158190429},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b0b183c207f46f0cca7dc63b2604f5cc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Infinite Hidden Markov Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/b0b183c207f46f0cca7dc63b2604f5cc-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_b3967a0e,
 abstract = {Semantic memory refers to our knowledge of facts and relationships between concepts. A successful semantic memory depends on inferring relationships between items that are not explicitly taught. Recent mathematical modeling of episodic memory argues that episodic recall relies on retrieval of a gradually-changing representation of temporal context. We show that retrieved context enables the development of a global memory space that reflects relationships between all items that have been previously learned. When newly-learned information is integrated into this structure, it is placed in some relationship to all other items, even if that relationship has not been explicitly learned. We demonstrate this effect for global semantic structures shaped topologically as a ring, and as a two-dimensional sheet. We also examined the utility of this learning algorithm for learning a more realistic semantic space by training it on a large pool of synonym pairs. Retrieved context enabled the model to "infer" relationships between synonym pairs that had not yet been presented.},
 author = {Rao, Vinayak and Howard, Marc},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b3967a0e938dc2a6340e258630febd5a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b3967a0e938dc2a6340e258630febd5a-Metadata.json},
 openalex = {W2137185953},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Retrieved context and the discovery of semantic structure.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/b3967a0e938dc2a6340e258630febd5a-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_b6a1085a,
 abstract = {We propose an active learning algorithm that learns a continuous valuation model from discrete preferences. The algorithm automatically decides what items are best presented to an individual in order to find the item that they value highly in as few trials as possible, and exploits quirks of human psychology to minimize time and cognitive burden. To do this, our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface, which would be needlessly expensive. The problem is particularly difficult because the space of choices is infinite. We demonstrate the effectiveness of the new algorithm compared to related active learning methods. We also embed the algorithm within a decision making tool for assisting digital artists in rendering materials. The tool finds the best parameters while minimizing the number of queries.},
 author = {Eric, Brochu and Freitas, Nando and Ghosh, Abhijeet},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b6a1085a27ab7bff7550f8a3bd017df8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b6a1085a27ab7bff7550f8a3bd017df8-Metadata.json},
 openalex = {W2161735965},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Active Preference Learning with Discrete Choice Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/b6a1085a27ab7bff7550f8a3bd017df8-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_b73ce398,
 abstract = {The peristimulus time histogram (PSTH) and its more continuous cousin, the spike density function (SDF) are staples in the analytic toolkit of neurophysiologists. The former is usually obtained by binning spike trains, whereas the standard method for the latter is smoothing with a Gaussian kernel. Selection of a bin width or a kernel size is often done in an relatively arbitrary fashion, even though there have been recent attempts to remedy this situation [1, 2]. We develop an exact Bayesian, generative model approach to estimating PSTHs and demonstate its superiority to competing methods. Further advantages of our scheme include automatic complexity control and error bars on its predictions.},
 author = {Endres, Dominik and Oram, Mike and Schindelin, Johannes and Foldiak, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b73ce398c39f506af761d2277d853a92-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b73ce398c39f506af761d2277d853a92-Metadata.json},
 openalex = {W2157568856},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b73ce398c39f506af761d2277d853a92-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b73ce398c39f506af761d2277d853a92-Supplemental.zip},
 title = {Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/b73ce398c39f506af761d2277d853a92-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_b7bb35b9,
 abstract = {We provide a provably efficient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting. Specifically, we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (possibly kernalized) linearly parameterized dynamics. This result builds on Kearns and Singh's work that provides a provably efficient algorithm for finite state MDPs. Our approach is not restricted to the linear setting, and is applicable to other classes of continuous MDPs.},
 author = {Strehl, Alexander and Littman, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Metadata.json},
 openalex = {W2134807560},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Online Linear Regression and Its Application to Model-Based Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/b7bb35b9c6ca2aee2df08cf09d7016c2-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_b83aac23,
 abstract = {In transfer learning we aim to solve new problems using fewer examples using information gained from solving related problems. Transfer learning has been successful in practice, and extensive PAC analysis of these methods has been developed. However it is not yet clear how to define relatedness between tasks. This is considered as a major problem as it is conceptually troubling and it makes it unclear how much information to transfer and when and how to transfer it. In this paper we propose to measure the amount of information one task contains about another using conditional Kolmogorov complexity between the tasks. We show how existing theory neatly solves the problem of measuring relatedness and transferring the 'right' amount of information in sequential transfer learning in a Bayesian setting. The theory also suggests that, in a very formal and precise sense, no other reasonable transfer method can do much better than our Kolmogorov Complexity theoretic transfer method, and that sequential transfer is always justified. We also develop a practical approximation to the method and use it to transfer information between 8 arbitrarily chosen databases from the UCI ML repository.},
 author = {Mahmud, M. and Ray, Sylvian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b83aac23b9528732c23cc7352950e880-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b83aac23b9528732c23cc7352950e880-Metadata.json},
 openalex = {W2165347833},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b83aac23b9528732c23cc7352950e880-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/b83aac23b9528732c23cc7352950e880-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_b86e8d03,
 abstract = {We cast the ranking problem as (1) multiple classification (Mc) (2) multiple ordinal classification, which lead to computationally tractable learning algorithms for relevance ranking in Web search. We consider the DCG criterion (discounted cumulative gain), a standard quality measure in information retrieval. Our approach is motivated by the fact that perfect classifications result in perfect DCG scores and the DCG errors are bounded by classification errors. We propose using the Expected Relevance to convert class probabilities into ranking scores. The class probabilities are learned using a gradient boosting tree algorithm. Evaluations on large-scale datasets show that our approach can improve LambdaRank [5] and the regressions-based ranker [6], in terms of the (normalized) DCG scores. An efficient implementation of the boosting tree algorithm is also presented.},
 author = {Li, Ping and Wu, Qiang and Burges, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b86e8d03fe992d1b0e19656875ee557c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b86e8d03fe992d1b0e19656875ee557c-Metadata.json},
 openalex = {W2120391124},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {McRank: Learning to Rank Using Multiple Classification and Gradient Boosting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_ba2fd310,
 abstract = {This paper investigates the application of randomized algorithms for large scale SVM learning. The key contribution of the paper is to show that, by using ideas random projections, the minimal number of support vectors required to solve almost separable classification problems, such that the solution obtained is near optimal with a very high probability, is given by O(log n); if on removal of properly chosen O(log n) points the data becomes linearly separable then it is called almost separable. The second contribution is a sampling based algorithm, motivated from randomized algorithms, which solves a SVM problem by considering subsets of the dataset which are greater in size than the number of support vectors for the problem. These two ideas are combined to obtain an algorithm for SVM classification problems which performs the learning by considering only O(log n) points at a time. Experiments done on synthetic and real life datasets show that the algorithm does scale up state of the art SVM solvers in terms of memory required and execution time without loss in accuracy. It is to be noted that the algorithm presented here nicely complements existing large scale SVM learning approaches as it can be used to scale up any SVM solver.},
 author = {Kumar, Krishnan and Bhattacharya, Chiru and Hariharan, Ramesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ba2fd310dcaa8781a9a652a31baf3c68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ba2fd310dcaa8781a9a652a31baf3c68-Metadata.json},
 openalex = {W2108430336},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Randomized Algorithm for Large Scale Support Vector Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/ba2fd310dcaa8781a9a652a31baf3c68-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_be83ab3e,
 abstract = {A situation where training and test samples follow different input distributions is called covariate shift. Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent—weighted variants according to the ratio of test and training input densities are consistent. Therefore, accurately estimating the density ratio, called the importance, is one of the key issues in covariate shift adaptation. A naive approach to this task is to first estimate training and test input densities separately and then estimate the importance by taking the ratio of the estimated densities. However, this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional cases. In this paper, we propose a direct importance estimation method that does not involve density estimation. Our method is equipped with a natural cross validation procedure and hence tuning parameters such as the kernel width can be objectively optimized. Simulations illustrate the usefulness of our approach.},
 author = {Sugiyama, Masashi and Nakajima, Shinichi and Kashima, Hisashi and Buenau, Paul and Kawanabe, Motoaki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/be83ab3ecd0db773eb2dc1b0a17836a1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/be83ab3ecd0db773eb2dc1b0a17836a1-Metadata.json},
 openalex = {W2103851188},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_bf62768c,
 abstract = {In the online linear optimization problem, a learner must choose, in each round, a decision from a set D ⊂ ℝn in order to minimize an (unknown and changing) linear cost function. We present sharp rates of convergence (with respect to additive regret) for both the full information setting (where the cost function is revealed at the end of each round) and the bandit setting (where only the scalar cost incurred is revealed). In particular, this paper is concerned with the price of bandit information, by which we mean the ratio of the best achievable regret in the bandit setting to that in the full-information setting. For the full information case, the upper bound on the regret is O*( √nT), where n is the ambient dimension and T is the time horizon. For the bandit case, we present an algorithm which achieves O*(n3/2 √T) regret — all previous (nontrivial) bounds here were O(poly(n)T2/3) or worse. It is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case — in stark contrast to the K-arm bandit setting, where the gap in the dependence on K is exponential (√TK vs. √T log K). We also present lower bounds showing that this gap is at least √n, which we conjecture to be the correct order. The bandit algorithm we present can be implemented efficiently in special cases of particular interest, such as path planning and Markov Decision Problems.},
 author = {Dani, Varsha and Kakade, Sham M and Hayes, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/bf62768ca46b6c3b5bea9515d1a1fc45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/bf62768ca46b6c3b5bea9515d1a1fc45-Metadata.json},
 openalex = {W2120745256},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Price of Bandit Information for Online Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/bf62768ca46b6c3b5bea9515d1a1fc45-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c06d06da,
 abstract = {Many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose significant precision in this transformation. Manifold Sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both generated and natural data-sets. Manifold Sculpting is also able to benefit from both prior dimensionality reduction efforts.},
 author = {Gashler, Michael and Ventura, Dan and Martinez, Tony},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c06d06da9666a219db15cf575aff2824-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c06d06da9666a219db15cf575aff2824-Metadata.json},
 openalex = {W2150711601},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c06d06da9666a219db15cf575aff2824-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c06d06da9666a219db15cf575aff2824-Supplemental.zip},
 title = {Iterative Non-linear Dimensionality Reduction with Manifold Sculpting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c06d06da9666a219db15cf575aff2824-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c0c7c76d,
 abstract = {We propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our algorithm simultaneously computes support vectors and a proxy kernel matrix used in forming the loss. This can be interpreted as a penalized kernel learning problem where indefinite kernel matrices are treated as noisy observations of a true Mercer kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the projected gradient or analytic center cutting plane methods. We compare the performance of our technique with other methods on several standard data sets.},
 author = {Luss, Ronny and D\textquotesingle aspremont, Alexandre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Metadata.json},
 openalex = {W2015855658},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Support vector machine classification with indefinite kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c0c7c76d30bd3dcaefc96f40275bdc0a-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c0e190d8,
 abstract = {This paper considers kernels invariant to translation, rotation and dilation. We show that no non-trivial positive definite (p.d.) kernels exist which are radial and dilation invariant, only conditionally positive definite (c.p.d.) ones. Accordingly, we discuss the c.p.d. case and provide some novel analysis, including an elementary derivation of a c.p.d. representer theorem. On the practical side, we give a support vector machine (s.v.m.) algorithm for arbitrary c.p.d. kernels. For the thin-plate kernel this leads to a classifier with only one parameter (the amount of regularisation), which we demonstrate to be as effective as an s.v.m. with the Gaussian kernel, even though the Gaussian involves a second parameter (the length scale).},
 author = {Walder, Christian and Chapelle, Olivier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c0e190d8267e36708f955d7ab048990d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c0e190d8267e36708f955d7ab048990d-Metadata.json},
 openalex = {W2101924749},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c0e190d8267e36708f955d7ab048990d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c0e190d8267e36708f955d7ab048990d-Supplemental.zip},
 title = {Learning with Transformation Invariant Kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c0e190d8267e36708f955d7ab048990d-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c203d8a1,
 abstract = {The present work aims to model the correspondence between facial motion and speech. The face and sound are modelled separately, with phonemes being the link between both. We propose a sequential model and evaluate its suitability for the generation of the facial animation from a sequence of phonemes, which we obtain from speech. We evaluate the results both by computing the error between generated sequences and real video, as well as with a rigorous double-blind test with human subjects. Experiments show that our model compares favourably to other existing methods and that the sequences generated are comparable to real video sequences.},
 author = {Englebienne, Gwenn and Cootes, Tim and Rattray, Magnus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c203d8a151612acf12457e4d67635a95-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c203d8a151612acf12457e4d67635a95-Metadata.json},
 openalex = {W2140199909},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c203d8a151612acf12457e4d67635a95-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A probabilistic model for generating realistic lip movements from speech},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c203d8a151612acf12457e4d67635a95-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c2aee861,
 abstract = {Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of Web2.0 recommender systems, allowing users to generate playlists based on use-dependent terms such as chill or jogging that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 files. Using a set of boosted classifiers, we map audio features onto social tags collected from the Web. The resulting automatic tags (or autotags) furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the cold-start problem common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system.},
 author = {Eck, Douglas and Lamere, Paul and Bertin-mahieux, Thierry and Green, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c2aee86157b4a40b78132f1e71a9e6f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c2aee86157b4a40b78132f1e71a9e6f1-Metadata.json},
 openalex = {W2162124943},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c2aee86157b4a40b78132f1e71a9e6f1-Supplemental.zip},
 title = {Automatic Generation of Social Tags for Music Recommendation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c2aee86157b4a40b78132f1e71a9e6f1-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c3992e9a,
 abstract = {We propose a compact, low power VLSI network of spiking neurons which can learn to classify complex patterns of mean firing rates on-line and in real-time. The network of integrate-and-fire neurons is connected by bistable synapses that can change their weight using a local spike-based plasticity mechanism. Learning is supervised by a teacher which provides an extra input to the output neurons during training. The synaptic weights are updated only if the current generated by the plastic synapses does not match the output desired by the teacher (as in the perceptron learning rule). We present experimental results that demonstrate how this VLSI network is able to robustly classify uncorrelated linearly separable spatial patterns of mean firing rates.},
 author = {Mitra, Srinjoy and Indiveri, Giacomo and Fusi, Stefano},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c3992e9a68c5ae12bd18488bc579b30d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c3992e9a68c5ae12bd18488bc579b30d-Metadata.json},
 openalex = {W2126960735},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning to classify complex patterns using a VLSI network of spiking neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c399862d,
 abstract = {We consider the problem of Support Vector Machine transduction, which involves a combinatorial problem with exponential computational complexity in the number of unlabeled examples. Although several studies are devoted to Transductive SVM, they suffer either from the high computation complexity or from the solutions of local optimum. To address this problem, we propose solving Transductive SVM via a convex relaxation, which converts the NP-hard problem to a semi-definite programming. Compared with the other SDP relaxation for Transductive SVM, the proposed algorithm is computationally more efficient with the number of free parameters reduced from O(n2) to O(n) where n is the number of examples. Empirical study with several benchmark data sets shows the promising performance of the proposed algorithm in comparison with other state-of-the-art implementations of Transductive SVM.},
 author = {Xu, Zenglin and Jin, Rong and Zhu, Jianke and King, Irwin and Lyu, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c399862d3b9d6b76c8436e924a68c45b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c399862d3b9d6b76c8436e924a68c45b-Metadata.json},
 openalex = {W2105632579},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Convex Relaxation for Transductive Support Vector Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c4015b7f,
 abstract = {We investigate the use of message-passing algorithms for the problem of finding the max-weight independent set (MWIS) in a graph. First, we study the performance of loopy max-product belief propagation. We show that, if it converges, the quality of the estimate is closely related to the tightness of an LP relaxation of the MWIS problem. We use this relationship to obtain sufficient conditions for correctness of the estimate. We then develop a modification of max-product - one that converges to an optimal solution of the dual of the MWIS problem. We also develop a simple iterative algorithm for estimating the max-weight independent set from this dual solution. We show that the MWIS estimate obtained using these two algorithms in conjunction is correct when the graph is bipartite and the MWIS is unique. Finally, we show that any problem of MAP estimation for probability distributions over finite domains can be reduced to an MWIS problem. We believe this reduction will yield new insights and algorithms for MAP estimation.},
 author = {Sanghavi, Sujay and Shah, Devavrat and Willsky, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c4015b7f368e6b4871809f49debe0579-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c4015b7f368e6b4871809f49debe0579-Metadata.json},
 openalex = {W2099515224},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c4015b7f368e6b4871809f49debe0579-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Message Passing for Max-weight Independent Set},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c4015b7f368e6b4871809f49debe0579-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c5ff2543,
 abstract = {We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efficiently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of independent misclassification noise, given access to a noise-tolerant weak ranker.},
 author = {Long, Phil and Servedio, Rocco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c5ff2543b53f4cc0ad3819a36752467b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c5ff2543b53f4cc0ad3819a36752467b-Metadata.json},
 openalex = {W2117707191},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c5ff2543b53f4cc0ad3819a36752467b-Supplemental.zip},
 title = {Boosting the Area under the ROC Curve},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c5ff2543b53f4cc0ad3819a36752467b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c60d060b,
 abstract = {Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured.},
 author = {Ranzato, Marc\textquotesingle aurelio and Boureau, Y-lan and Cun, Yann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c60d060b946d6dd6145dcbad5c4ccf6f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c60d060b946d6dd6145dcbad5c4ccf6f-Metadata.json},
 openalex = {W2108665656},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sparse Feature Learning for Deep Belief Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c60d060b946d6dd6145dcbad5c4ccf6f-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c6bff625,
 abstract = {The control of high-dimensional, continuous, non-linear dynamical systems is a key problem in reinforcement learning and control. Local, trajectory-based methods, using techniques such as Differential Dynamic Programming (DDP), are not directly subject to the curse of dimensionality, but generate only local controllers. In this paper, we introduce Receding Horizon DDP (RH-DDP), an extension to the classic DDP algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories. We demonstrate the effectiveness of our approach on a series of high-dimensional problems using a simulated multi-link swimming robot. These experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing with problems of (at least) 24 state and 9 action dimensions.},
 author = {Tassa, Yuval and Erez, Tom and Smart, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c6bff625bdb0393992c9d4db0c6bbe45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c6bff625bdb0393992c9d4db0c6bbe45-Metadata.json},
 openalex = {W2109944946},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c6bff625bdb0393992c9d4db0c6bbe45-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c6bff625bdb0393992c9d4db0c6bbe45-Supplemental.zip},
 title = {Receding Horizon Differential Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c6bff625bdb0393992c9d4db0c6bbe45-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c6e19e83,
 abstract = {This paper1 explores the use of a Maximal Average Margin (MAM) optimality principle for the design of learning algorithms. It is shown that the application of this risk minimization principle results in a class of (computationally) simple learning machines similar to the classical Parzen window classifier. A direct relation with the Rademacher complexities is established, as such facilitating analysis and providing a notion of certainty of prediction. This analysis is related to Support Vector Machines by means of a margin transformation. The power of the MAM principle is illustrated further by application to ordinal regression tasks, resulting in an O(n) algorithm able to process large datasets in reasonable time.},
 author = {Pelckmans, Kristiaan and Suykens, Johan and Moor, Bart},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Metadata.json},
 openalex = {W2097383369},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Risk Minimization Principle for a Class of Parzen Estimators},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c6e19e830859f2cb9f7c8f8cacb8d2a6-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_c8fbbc86,
 abstract = {Electrical power management in large-scale IT systems such as commercial data-centers is an application area of rapidly growing interest from both an economic and ecological perspective, with billions of dollars and millions of metric tons of CO2 emissions at stake annually. Businesses want to save power without sacrificing performance. This paper presents a reinforcement learning approach to simultaneous online management of both performance and power consumption. We apply RL in a realistic laboratory testbed using a Blade cluster and dynamically varying HTTP workload running on a commercial web applications middleware platform. We embed a CPU frequency controller in the Blade servers' firmware, and we train policies for this controller using a multi-criteria reward signal depending on both application performance and CPU power consumption. Our testbed scenario posed a number of challenges to successful use of RL, including multiple disparate reward functions, limited decision sampling rates, and pathologies arising when using multiple sensor readings as state variables. We describe innovative practical solutions to these challenges, and demonstrate clear performance improvements over both hand-designed policies as well as obvious cookbook RL implementations.},
 author = {Tesauro, Gerald and Das, Rajarshi and Chan, Hoi and Kephart, Jeffrey and Levine, David and Rawson, Freeman and Lefurgy, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Metadata.json},
 openalex = {W2116001481},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/c8fbbc86abe8bd6a5eb6a3b4d0411301-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_ca3ec598,
 abstract = {We study the problem of an apprentice learning to behave in an environment with an unknown reward function by observing the behavior of an expert. We follow on the work of Abbeel and Ng [1] who considered a framework in which the true reward function is assumed to be a linear combination of a set of known and observable features. We give a new algorithm that, like theirs, is guaranteed to learn a policy that is nearly as good as the expert's, given enough examples. However, unlike their algorithm, we show that ours may produce a policy that is substantially better than the expert's. Moreover, our algorithm is computationally faster, is easier to implement, and can be applied even in the absence of an expert. The method is based on a game-theoretic view of the problem, which leads naturally to a direct application of the multiplicative-weights algorithm of Freund and Schapire [2] for playing repeated matrix games. In addition to our formal presentation and analysis of the new algorithm, we sketch how the method can be applied when the transition function itself is unknown, and we provide an experimental demonstration of the algorithm on a toy video-game environment.},
 author = {Syed, Umar and Schapire, Robert E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ca3ec598002d2e7662e2ef4bdd58278b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ca3ec598002d2e7662e2ef4bdd58278b-Metadata.json},
 openalex = {W2113023245},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ca3ec598002d2e7662e2ef4bdd58278b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ca3ec598002d2e7662e2ef4bdd58278b-Supplemental.zip},
 title = {A Game-Theoretic Approach to Apprenticeship Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/ca3ec598002d2e7662e2ef4bdd58278b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_cb70ab37,
 abstract = {Conditional Random Fields (CRFs) are an effective tool for a variety of different data segmentation and labeling tasks including visual scene interpretation, which seeks to partition images into their constituent semantic-level regions and assign appropriate class labels to each region. For accurate labeling it is important to capture the global context of the image as well as local information. We introduce a CRF based scene labeling model that incorporates both local features and features aggregated over the whole image or large sections of it. Secondly, traditional CRF learning requires fully labeled datasets which can be costly and troublesome to produce. We introduce a method for learning CRFs from datasets with many unlabeled nodes by marginalizing out the unknown labels so that the log-likelihood of the known ones can be maximized by gradient ascent. Loopy Belief Propagation is used to approximate the marginals needed for the gradient and log-likelihood calculations and the Bethe free-energy approximation to the log-likelihood is monitored to control the step size. Our experimental results show that effective models can be learned from fragmentary labelings and that incorporating top-down aggregate features significantly improves the segmentations. The resulting segmentations are compared to the state-of-the-art on three different image datasets.},
 author = {Triggs, Bill and Verbeek, Jakob},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cb70ab375662576bd1ac5aaf16b3fca4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cb70ab375662576bd1ac5aaf16b3fca4-Metadata.json},
 openalex = {W2114930007},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Scene Segmentation with CRFs Learned from Partially Labeled Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_cbcb58ac,
 abstract = {A novel approach to measure the interdependence of two time series is proposed, referred to as stochastic event (SES); it quantifies the alignment of two point processes by means of the following parameters: time delay, variance of the timing jitter, fraction of spurious events, and average similarity of events. SES may be applied to generic one-dimensional and multi-dimensional point processes, however, the paper mainly focusses on point processes in time-frequency domain. The average event similarity is in that case described by two parameters: the average frequency offset between events in the time-frequency plane, and the variance of the frequency offset (frequency jitter); SES then consists of five parameters in total. Those parameters quantify the synchrony of oscillatory events, and hence, they provide an alternative to existing synchrony measures that quantify amplitude or phase synchrony. The pairwise alignment of point processes is cast as a statistical inference problem, which is solved by applying the max-product algorithm on a graphical model. The SES parameters are determined from the resulting pairwise alignment by maximum a posteriori (MAP) estimation. The proposed interdependence measure is applied to the problem of detecting anomalies in EEG synchrony of Mild Cognitive Impairment (MCI) patients; the results indicate that SES significantly improves the sensitivity of EEG in detecting MCI.},
 author = {Dauwels, Justin and Vialatte, Fran\c{c}ois and Rutkowski, Tomasz and Cichocki, Andrzej},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cbcb58ac2e496207586df2854b17995f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cbcb58ac2e496207586df2854b17995f-Metadata.json},
 openalex = {W2135115045},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cbcb58ac2e496207586df2854b17995f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Measuring Neural Synchrony by Message Passing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/cbcb58ac2e496207586df2854b17995f-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_ccc0aa1b,
 abstract = {Active learning sequentially selects unlabeled instances to label with the goal of reducing the effort needed to learn a good classifier. Most previous studies in active learning have focused on selecting one unlabeled instance to label at a time while retraining in each iteration. Recently a few batch mode active learning approaches have been proposed that select a set of most informative unlabeled instances in each iteration under the guidance of heuristic scores. In this paper, we propose a discriminative batch mode active learning approach that formulates the instance selection task as a continuous optimization problem over auxiliary instance selection variables. The optimization is formulated to maximize the discriminative classification performance of the target classifier, while also taking the unlabeled data into account. Although the objective is not convex, we can manipulate a quasi-Newton method to obtain a good local solution. Our empirical studies on UCI datasets show that the proposed active learning is more effective than current state-of-the art batch mode active learning algorithms.},
 author = {Guo, Yuhong and Schuurmans, Dale},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ccc0aa1b81bf81e16c676ddb977c5881-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ccc0aa1b81bf81e16c676ddb977c5881-Metadata.json},
 openalex = {W2137795521},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Discriminative Batch Mode Active Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/ccc0aa1b81bf81e16c676ddb977c5881-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_cf004fdc,
 abstract = {Bayesian models of multisensory perception traditionally address the problem of estimating an underlying variable that is assumed to be the cause of the two sensory signals. The brain, however, has to solve a more general problem: it also has to establish which signals come from the same source and should be integrated, and which ones do not and should be segregated. In the last couple of years, a few models have been proposed to solve this problem in a Bayesian fashion. One of these has the strength that it formalizes the causal structure of sensory signals. We first compare these models on a formal level. Furthermore, we conduct a psychophysics experiment to test human performance in an auditory-visual spatial localization task in which integration is not mandatory. We find that the causal Bayesian inference model accounts for the data better than other models.},
 author = {Beierholm, Ulrik and Shams, Ladan and J., Wei and Koerding, Konrad},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Metadata.json},
 openalex = {W2134972763},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Supplemental.zip},
 title = {Comparing Bayesian models for multisensory cue combination without mandatory integration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/cf004fdc76fa1a4f25f62e0eb5261ca3-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_cfa0860e,
 abstract = {Point-based algorithms have been surprisingly successful in computing approximately optimal solutions for partially observable Markov decision processes (POMDPs) in high dimensional belief spaces. In this work, we seek to understand the belief-space properties that allow some POMDP problems to be approximated efficiently and thus help to explain the point-based algorithms' success often observed in the experiments. We show that an approximately optimal POMDP solution can be computed in time polynomial in the covering number of a reachable belief space, which is the subset of the belief space reachable from a given belief point. We also show that under the weaker condition of having a small covering number for an optimal reachable space, which is the subset of the belief space reachable under an optimal policy, computing an approximately optimal solution is NP-hard. However, given a suitable set of points that cover an optimal reachable space well, an approximate solution can be computed in polynomial time. The covering number highlights several interesting properties that reduce the complexity of POMDP planning in practice, e.g., fully observed state variables, beliefs with sparse support, smooth beliefs, and circulant state-transition matrices.},
 author = {Lee, Wee and Rong, Nan and Hsu, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cfa0860e83a4c3a763a7e62d825349f7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cfa0860e83a4c3a763a7e62d825349f7-Metadata.json},
 openalex = {W2152706713},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cfa0860e83a4c3a763a7e62d825349f7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {What makes some POMDP problems easy to approximate},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/cfa0860e83a4c3a763a7e62d825349f7-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_cfbce4c1,
 abstract = {We present a novel boosting algorithm, called SoftBoost, designed for sets of binary labeled examples that are not necessarily separable by convex combinations of base hypotheses. Our algorithm achieves robustness by capping the distributions on the examples. Our update of the distribution is motivated by minimizing a relative entropy subject to the capping constraints and constraints on the edges of the obtained base hypotheses. The capping constraints imply a soft margin in the dual optimization problem. Our algorithm produces a convex combination of hypotheses whose soft margin is within δ of its maximum. We employ relative entropy projection methods to prove an O(ln N/δ2) iteration bound for our algorithm, where N is number of examples.

We compare our algorithm with other approaches including LPBoost, Brown-Boost, and SmoothBoost. We show that there exist cases where the number of iterations required by LPBoost grows linearly in N instead of the logarithmic growth for SoftBoost. In simulation studies we show that our algorithm converges about as fast as LPBoost, faster than BrownBoost, and much faster than SmoothBoost. In a benchmark comparison we illustrate the competitiveness of our approach.},
 author = {R\"{a}tsch, Gunnar and Warmuth, Manfred K. K and Glocer, Karen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cfbce4c1d7c425baf21d6b6f2babe6be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cfbce4c1d7c425baf21d6b6f2babe6be-Metadata.json},
 openalex = {W2097738668},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Boosting Algorithms for Maximizing the Soft Margin},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/cfbce4c1d7c425baf21d6b6f2babe6be-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d045c59a,
 abstract = {This paper aims to model relational data on edges of networks. We describe appropriate Gaussian Processes (GPs) for directed, undirected, and bipartite networks. The inter-dependencies of edges can be effectively modeled by adapting the GP hyper-parameters. The framework suggests an intimate connection between link prediction and transfer learning, which were traditionally two separate research topics. We develop an efficient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity.},
 author = {Yu, Kai and Chu, Wei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d045c59a90d7587d8d671b5f5aec4e7c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d045c59a90d7587d8d671b5f5aec4e7c-Metadata.json},
 openalex = {W2155459494},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Gaussian Process Models for Link Analysis and Transfer Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d045c59a90d7587d8d671b5f5aec4e7c-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d296c101,
 abstract = {Maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data—the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible.},
 author = {Bethge, Matthias and Berens, Philipp},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d296c101daa88a51f6ca8cfc1ac79b50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d296c101daa88a51f6ca8cfc1ac79b50-Metadata.json},
 openalex = {W2148936397},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d296c101daa88a51f6ca8cfc1ac79b50-Supplemental.zip},
 title = {Near-Maximum Entropy Models for Binary Neural Representations of Natural Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d296c101daa88a51f6ca8cfc1ac79b50-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d516b136,
 abstract = {We provide provably privacy-preserving versions of belief propagation, Gibbs sampling, and other local algorithms — distributed multiparty protocols in which each party or vertex learns only its final local value, and absolutely nothing else.},
 author = {Kearns, Michael and Tan, Jinsong and Wortman, Jennifer},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d516b13671a4179d9b7b458a6ebdeb92-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d516b13671a4179d9b7b458a6ebdeb92-Metadata.json},
 openalex = {W2156605731},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d516b13671a4179d9b7b458a6ebdeb92-Supplemental.zip},
 title = {Privacy-Preserving Belief Propagation and Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d54ce9de,
 abstract = {Co-training (or more generally, co-regularization) has been a popular algorithm for semi-supervised learning in data with two feature representations (or views), but the fundamental assumptions underlying this type of models are still unclear. In this paper we propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clarifies the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classifiers. The resulting approach is convex and avoids local-maxima problems, and it can also automatically estimate how much each view should be trusted to accommodate noisy or unreliable views. The Bayesian co-training approach can also elegantly handle data samples with missing views, that is, some of the views are not available for some data points at learning time. This is further extended to an active sensing framework, in which the missing (sample, view) pairs are actively acquired to improve learning performance. The strength of active sensing model is that one actively sensed (sample, view) pair would improve the joint multi-view classification on all the samples. Experiments on toy data and several real world data sets illustrate the benefits of this approach.},
 author = {Yu, Shipeng and Krishnapuram, Balaji and Steck, Harald and Rao, R. and Rosales, R\'{o}mer},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d54ce9de9df77c579775a7b6b1a4bdc0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d54ce9de9df77c579775a7b6b1a4bdc0-Metadata.json},
 openalex = {W53987483},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d54ce9de9df77c579775a7b6b1a4bdc0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Co-Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d54ce9de9df77c579775a7b6b1a4bdc0-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d56b9fc4,
 abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
 author = {Mcauliffe, Jon and Blei, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Metadata.json},
 openalex = {W2098062695},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Supervised Topic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d56b9fc4b0f1be8871f5e1c40c0067e7-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d5cfead9,
 abstract = {Although kernel measures of independence have been widely applied in machine learning (notably in kernel ICA), there is as yet no method to determine whether they have detected statistically significant dependence. We provide a novel test of the independence hypothesis for one particular kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). The resulting test costs O(m2), where m is the sample size. We demonstrate that this test outperforms established contingency table and functional correlation-based tests, and that this advantage is greater for multivariate data. Finally, we show the HSIC test also applies to text (and to structured data more generally), for which no other independence test presently exists.},
 author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon and Song, Le and Sch\"{o}lkopf, Bernhard and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Metadata.json},
 openalex = {W2112552549},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Kernel Statistical Test of Independence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d5cfead94f5350c12c322b5b664544c1-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d6c651dd,
 abstract = {Many tasks in speech processing involve classification of long term characteristics of a speech segment such as language, speaker, dialect, or topic. A natural technique for determining these characteristics is to first convert the input speech into a sequence of tokens such as words, phones, etc. From these tokens, we can then look for distinctive sequences, keywords, that characterize the speech. In many applications, a set of distinctive keywords may not be known a priori. In this case, an automatic method of building up keywords from short context units such as phones is desirable. We propose a method for the construction of keywords based upon Support Vector Machines. We cast the problem of keyword selection as a feature selection problem for n-grams of phones. We propose an alternating filter-wrapper method that builds successively longer keywords. Application of this method to language recognition and topic recognition tasks shows that the technique produces interesting and significant qualitative and quantitative results.},
 author = {Richardson, Fred and Campbell, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d6c651ddcd97183b2e40bc464231c962-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d6c651ddcd97183b2e40bc464231c962-Metadata.json},
 openalex = {W2164922523},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d6c651ddcd97183b2e40bc464231c962-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Discriminative Keyword Selection Using Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d6c651ddcd97183b2e40bc464231c962-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d7322ed7,
 abstract = {Many existing approaches to collaborative filtering can neither handle very large datasets nor easily deal with users who have very few ratings. In this paper we present the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced Netflix dataset. We further extend the PMF model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically. Finally, we introduce a constrained version of the PMF model that is based on the assumption that users who have rated similar sets of movies are likely to have similar preferences. The resulting model is able to generalize considerably better for users with very few ratings. When the predictions of multiple PMF models are linearly combined with the predictions of Restricted Boltzmann Machines models, we achieve an error rate of 0.8861, that is nearly 7% better than the score of Netflix's own system.},
 author = {Mnih, Andriy and Salakhutdinov, Russ R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Metadata.json},
 openalex = {W2137245235},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Probabilistic Matrix Factorization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d7322ed717dedf1eb4e6e52a37ea7bcd-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d8700cbd,
 abstract = {A method is proposed for semiparametric estimation where parametric and non-parametric criteria are exploited in density estimation and unsupervised learning. This is accomplished by making sampling assumptions on a dataset that smoothly interpolate between the extreme of independently distributed (or id) sample data (as in nonparametric kernel density estimators) to the extreme of independent identically distributed (or iid) sample data. This article makes independent similarly distributed (or isd) sampling assumptions and interpolates between these two using a scalar parameter. The parameter controls a Bhattacharyya affinity penalty between pairs of distributions on samples. Surprisingly, the isd method maintains certain consistency and unimodality properties akin to maximum likelihood estimation. The proposed isd scheme is an alternative for handling nonstationarity in data without making drastic hidden variable assumptions which often make estimation difficult and laden with local optima. Experiments in density estimation on a variety of datasets confirm the value of isd over iid estimation, id estimation and mixture modeling.},
 author = {Jebara, Tony and Song, Yingbo and Thadani, Kapil},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d8700cbd38cc9f30cecb34f0c195b137-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d8700cbd38cc9f30cecb34f0c195b137-Metadata.json},
 openalex = {W2165471219},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d8700cbd38cc9f30cecb34f0c195b137-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Density Estimation under Independent Similarly Distributed Sampling Assumptions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d8700cbd38cc9f30cecb34f0c195b137-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_d9896106,
 abstract = {Permutations are ubiquitous in many real world problems, such as voting, rankings and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities, and typical compact representations such as graphical models cannot efficiently capture the mutual exclusivity constraints associated with permutations. In this paper, we use the low-frequency terms of a Fourier decomposition to represent such distributions compactly. We present Kronecker conditioning, a general and efficient approach for maintaining these distributions directly in the Fourier domain. Low order Fourier-based approximations can lead to functions that do not correspond to valid distributions. To address this problem, we present an efficient quadratic program defined directly in the Fourier domain to project the approximation onto a relaxed form of the marginal polytope. We demonstrate the effectiveness of our approach on a real camera-based multi-people tracking setting.},
 author = {Huang, Jonathan and Guestrin, Carlos and Guibas, Leonidas J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Metadata.json},
 openalex = {W2143756966},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Supplemental.zip},
 title = {Efficient Inference for Distributions on Permutations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/d9896106ca98d3d05b8cbdf4fd8b13a1-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_da0d1111,
 abstract = {We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by some policy. We study a variant of fitted Q-iteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous analysis of this algorithm, proving what we believe is the first finite-time bound for value-function based algorithms for continuous state and action problems.},
 author = {Antos, Andr\'{a}s and Szepesv\'{a}ri, Csaba and Munos, R\'{e}mi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/da0d1111d2dc5d489242e60ebcbaf988-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/da0d1111d2dc5d489242e60ebcbaf988-Metadata.json},
 openalex = {W2151416233},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fitted Q-iteration in continuous action-space MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/da0d1111d2dc5d489242e60ebcbaf988-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_da8ce53c,
 abstract = {Speech dereverberation remains an open problem after more than three decades of research. The most challenging step in speech dereverberation is blind channel identification (BCI). Although many BCI approaches have been developed, their performance is still far from satisfactory for practical applications. The main difficulty in BCI lies in finding an appropriate acoustic model, which not only can effectively resolve solution degeneracies due to the lack of knowledge of the source, but also robustly models real acoustic environments. This paper proposes a sparse acoustic room impulse response (RIR) model for BCI, that is, an acoustic RIR can be modeled by a sparse FIR filter. Under this model, we show how to formulate the BCI of a single-input multiple-output (SIMO) system into a l1-norm regularized least squares (LS) problem, which is convex and can be solved efficiently with guaranteed global convergence. The sparseness of solutions is controlled by l1-norm regularization parameters. We propose a sparse learning scheme that infers the optimal l1-norm regularization parameters directly from microphone observations under a Bayesian framework. Our results show that the proposed approach is effective and robust, and it yields source estimates in real acoustic environments with high fidelity to anechoic chamber measurements.},
 author = {Lin, Yuanqing and Chen, Jingdong and Kim, Youngmoo and Lee, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/da8ce53cf0240070ce6c69c48cd588ee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/da8ce53cf0240070ce6c69c48cd588ee-Metadata.json},
 openalex = {W2113489929},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/da8ce53cf0240070ce6c69c48cd588ee-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Blind channel identification for speech dereverberation using l1-norm sparse learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/da8ce53cf0240070ce6c69c48cd588ee-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_db8e1af0,
 author = {Ghebreab, Sennay and Smeulders, Arnold and Adriaans, Pieter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/db8e1af0cb3aca1ae2d0018624204529-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/db8e1af0cb3aca1ae2d0018624204529-Metadata.json},
 openalex = {W2161829094},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/db8e1af0cb3aca1ae2d0018624204529-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/db8e1af0cb3aca1ae2d0018624204529-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_dbe272ba,
 abstract = {The learning of probabilistic models with many hidden variables and non-decomposable dependencies is an important and challenging problem. In contrast to traditional approaches based on approximate inference in a single intractable model, our approach is to train a set of tractable submodels by encouraging them to agree on the hidden variables. This allows us to capture non-decomposable aspects of the data while still maintaining tractability. We propose an objective function for our approach, derive EM-style algorithms for parameter estimation, and demonstrate their effectiveness on three challenging real-world learning tasks.},
 author = {Liang, Percy S and Klein, Dan and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dbe272bab69f8e13f14b405e038deb64-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dbe272bab69f8e13f14b405e038deb64-Metadata.json},
 openalex = {W2163403121},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Agreement-Based Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/dbe272bab69f8e13f14b405e038deb64-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_dc82d632,
 abstract = {We extend position and phase-shift tuning, concepts already well established in the disparity energy neuron literature, to motion energy neurons. We show that Reichardt-like detectors can be considered examples of position tuning, and that motion energy filters whose complex valued spatio-temporal receptive fields are space-time separable can be considered examples of phase tuning. By combining these two types of detectors, we obtain an architecture for constructing motion energy neurons whose center frequencies can be adjusted by both phase and position shifts. Similar to recently described neurons in the primary visual cortex, these new motion energy neurons exhibit tuning that is between purely space-time separable and purely speed tuned. We propose a functional role for this intermediate level of tuning by demonstrating that comparisons between pairs of these motion energy neurons can reliably discriminate between inputs whose velocities lie above or below a given reference velocity.},
 author = {Lam, Yiu and Shi, Bertram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dc82d632c9fcecb0778afbc7924494a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dc82d632c9fcecb0778afbc7924494a6-Metadata.json},
 openalex = {W2121524207},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Extending position/phase-shift tuning to motion energy neurons improves velocity discrimination},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/dc82d632c9fcecb0778afbc7924494a6-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_dd8eb9f2,
 abstract = {For infants, early word learning is a chicken-and-egg problem. One way to learn a word is to observe that it co-occurs with a particular referent across different situations. Another way is to use the social context of an utterance to infer the intended referent of a word. Here we present a Bayesian model of cross-situational word learning, and an extension of this model that also learns which social cues are relevant to determining reference. We test our model on a small corpus of mother-infant interaction and find it performs better than competing models. Finally, we show that our model accounts for experimental phenomena including mutual exclusivity, fast-mapping, and generalization from social cues.},
 author = {Goodman, Noah and Tenenbaum, Joshua and Black, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dd8eb9f23fbd362da0e3f4e70b878c16-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dd8eb9f23fbd362da0e3f4e70b878c16-Metadata.json},
 openalex = {W2127438782},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dd8eb9f23fbd362da0e3f4e70b878c16-Supplemental.zip},
 title = {A Bayesian Framework for Cross-Situational Word-Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/dd8eb9f23fbd362da0e3f4e70b878c16-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_ddb30680,
 author = {Zhu, Kaihua and Wang, Hao and Bai, Hongjie and Li, Jian and Qiu, Zhihuan and Cui, Hang and Chang, Edward},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ddb30680a691d157187ee1cf9e896d03-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ddb30680a691d157187ee1cf9e896d03-Metadata.json},
 openalex = {W2099262739},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {PSVM: Parallelizing Support Vector Machines on Distributed Computers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/ddb30680a691d157187ee1cf9e896d03-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_e0741335,
 abstract = {Current object recognition systems can only recognize a limited number of object categories; scaling up to many categories is the next challenge. We seek to build a system to recognize and localize many different object categories in complex scenes. We achieve this through a simple approach: by matching the input image, in an appropriate representation, to images in a large training set of labeled images. Due to regularities in object identities across similar scenes, the retrieved matches provide hypotheses for object identities and locations. We build a probabilistic model to transfer the labels from the retrieval set to the input image. We demonstrate the effectiveness of this approach and study algorithm component contributions using held-out test sets from the LabelMe database.},
 author = {Russell, Bryan and Torralba, Antonio and Liu, Ce and Fergus, Rob and Freeman, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e07413354875be01a996dc560274708e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e07413354875be01a996dc560274708e-Metadata.json},
 openalex = {W2157035885},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e07413354875be01a996dc560274708e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Object Recognition by Scene Alignment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/e07413354875be01a996dc560274708e-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_e44fea3b,
 abstract = {It is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate Bayesian statistical calculations in order to continuously estimate the environmental state, integrate information from multiple sensory modalities, form predictions and choose actions. What is less clear is how these putative computations are implemented by cortical neural networks. An additional level of complexity is introduced because these networks observe the through spike trains received from primary sensory afferents, rather than directly. A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible world states. Much of this work, however, uses various approximations, which severely restrict the domain of applicability of these implementations. Here we make use of rigorous mathematical results from the theory of continuous time point process filtering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantifying the compatibility of a given network with its environment.},
 author = {Bobrowski, Omer and Meir, Ron and Shoham, Shy and Eldar, Yonina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e44fea3bec53bcea3b7513ccef5857ac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e44fea3bec53bcea3b7513ccef5857ac-Metadata.json},
 openalex = {W2126705763},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e44fea3bec53bcea3b7513ccef5857ac-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A neural network implementing optimal state estimation based on dynamic spike train decoding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/e44fea3bec53bcea3b7513ccef5857ac-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_e4bb4c51,
 abstract = {We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modeled by players using no regret algorithms, which guarantee that their payoff in the long run is close to the maximum they could hope to achieve by consistently deviating from the algorithm's suggested action.

We show that for a given set of deviations over the strategy set of a player, it is possible to efficiently approximate fixed points of a given deviation if and only if there exist efficient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium.},
 author = {Hazan, Elad and Kale, Satyen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e4bb4c5173c2ce17fd8fcd40041c068f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e4bb4c5173c2ce17fd8fcd40041c068f-Metadata.json},
 openalex = {W2152767111},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/e4bb4c5173c2ce17fd8fcd40041c068f-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_e5841df2,
 abstract = {We propose a new approach for dealing with the estimation of the location of change-points in one-dimensional piecewise constant signals observed in white noise. Our approach consists in reframing this task in a variable selection context. We use a penalized least-squares criterion with a e1-type penalty for this purpose. We prove some theoretical results on the estimated change-points and on the underlying piecewise constant estimated function. Then, we explain how to implement this method in practice by combining the LAR algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data.},
 author = {Levy-leduc, C\'{e}line and Harchaoui, Za\"{\i}d},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e5841df2166dd424a57127423d276bbe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e5841df2166dd424a57127423d276bbe-Metadata.json},
 openalex = {W2149247444},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e5841df2166dd424a57127423d276bbe-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Catching Change-points with Lasso},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/e5841df2166dd424a57127423d276bbe-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_e8258e51,
 abstract = {Rosetta is one of the leading algorithms for protein structure prediction today. It is a Monte Carlo energy minimization method requiring many random restarts to find structures with low energy. In this paper we present a resampling technique for structure prediction of small alpha/beta proteins using Rosetta. From an initial round of Rosetta sampling, we learn properties of the energy landscape that guide a subsequent round of sampling toward lower-energy structures. Rather than attempt to fit the full energy landscape, we use feature selection methods—both L1-regularized linear regression and decision trees—to identify structural features that give rise to low energy. We then enrich these structural features in the second sampling round. Results are presented across a benchmark set of nine small alpha/beta proteins demonstrating that our methods seldom impair, and frequently improve, Rosetta's performance.},
 author = {Blum, Ben and Baker, David and Jordan, Michael and Bradley, Philip and Das, Rhiju and Kim, David E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e8258e5140317ff36c7f8225a3bf9590-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e8258e5140317ff36c7f8225a3bf9590-Metadata.json},
 openalex = {W2134232723},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e8258e5140317ff36c7f8225a3bf9590-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Feature Selection Methods for Improving Protein Structure Prediction with Rosetta},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/e8258e5140317ff36c7f8225a3bf9590-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_eae27d77,
 abstract = {In many applications, one has to actively select among a set of expensive observations before making an informed decision. Often, we want to select observations which perform well when evaluated with an objective function chosen by an adversary. Examples include minimizing the maximum posterior variance in Gaussian Process regression, robust experimental design, and sensor placement for outbreak detection. In this paper, we present the Submodular Saturation algorithm, a simple and efficient algorithm with strong theoretical approximation guarantees for the case where the possible objective functions exhibit submodularity, an intuitive diminishing returns property. Moreover, we prove that better approximation algorithms do not exist unless NP-complete problems admit efficient algorithms. We evaluate our algorithm on several real-world problems. For Gaussian Process regression, our algorithm compares favorably with state-of-the-art heuristics described in the geostatistics literature, while being simpler, faster and providing theoretical guarantees. For robust experimental design, our algorithm performs favorably compared to SDP-based algorithms.},
 author = {Krause, Andreas and Mcmahan, Brendan and Guestrin, Carlos and Gupta, Anupam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/eae27d77ca20db309e056e3d2dcd7d69-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/eae27d77ca20db309e056e3d2dcd7d69-Metadata.json},
 openalex = {W2119186364},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Selecting Observations against Adversarial Objectives},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/eae27d77ca20db309e056e3d2dcd7d69-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_ec895663,
 abstract = {In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring into topics, has been widely applied in the computer vision field. However, many of these applications have difficulty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a bag-of-words. It is also critical to properly design words and when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structures among visual that are essential for solving many vision problems. The spatial information is not encoded in the values of visual but in the design of documents. Instead of knowing the partition of into documents a priori, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be flexibly added as a prior, grouping visual which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA.},
 author = {Wang, Xiaogang and Grimson, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ec8956637a99787bd197eacd77acce5e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ec8956637a99787bd197eacd77acce5e-Metadata.json},
 openalex = {W2149035855},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ec8956637a99787bd197eacd77acce5e-Supplemental.zip},
 title = {Spatial Latent Dirichlet Allocation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/ec8956637a99787bd197eacd77acce5e-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_ed265bc9,
 abstract = {We present a probabilistic generative model of visual attributes, together with an efficient learning algorithm. Attributes are visual qualities of objects, such as 'red', 'striped', or 'spotted'. The model sees attributes as patterns of image segments, repeatedly sharing some characteristic properties. These can be any combination of appearance, shape, or the layout of segments within the pattern. Moreover, attributes with general appearance are taken into account, such as the pattern of alternation of any two colors which is characteristic for stripes. To enable learning from unsegmented training images, the model is learnt discriminatively, by optimizing a likelihood ratio.

As demonstrated in the experimental evaluation, our model can learn in a weakly supervised setting and encompasses a broad range of attributes. We show that attributes can be learnt starting from a text query to Google image search, and can then be used to recognize the attribute and determine its spatial extent in novel real-world images.},
 author = {Ferrari, Vittorio and Zisserman, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ed265bc903a5a097f61d3ec064d96d2e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ed265bc903a5a097f61d3ec064d96d2e-Metadata.json},
 openalex = {W2125560515},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning Visual Attributes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_eefc9e10,
 abstract = {A wide variety of Dirichlet-multinomial 'topic' models have found interesting applications in recent years. While Gibbs sampling remains an important method of inference in such models, variational techniques have certain advantages such as easy assessment of convergence, easy optimization without the need to maintain detailed balance, a bound on the marginal likelihood, and side-stepping of issues with topic-identifiability. The most accurate variational technique thus far, namely collapsed variational latent Dirichlet allocation, did not deal with model selection nor did it include inference for hyperparameters. We address both issues by generalizing the technique, obtaining the first variational algorithm to deal with the hierarchical Dirichlet process and to deal with hyperparameters of Dirichlet variables. Experiments show a significant improvement in accuracy.},
 author = {Teh, Yee and Kurihara, Kenichi and Welling, Max},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Metadata.json},
 openalex = {W2157906684},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Collapsed Variational Inference for HDP},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/eefc9e10ebdc4a2333b42b2dbb8f27b6-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_ef575e88,
 abstract = {We consider the learning task consisting in predicting as well as the best function in a finite reference set G up to the smallest possible additive term. If R(g) denotes the generalization error of a prediction function g, under reasonable assumptions on the loss function (typically satisfied by the least square loss when the output is bounded), it is known that the progressive mixture rule ĝ satisfies

ER(ĝ) ≤ ming∈G R(g) + Cst log|G|/n, (1)

where n denotes the size of the training set, and E denotes the expectation w.r.t. the training set distribution. This work shows that, surprisingly, for appropriate reference sets G, the deviation convergence rate of the progressive mixture rule is no better than Cst/√n: it fails to achieve the expected Cst/n. We also provide an algorithm which does not suffer from this drawback, and which is optimal in both deviation and expectation convergence rates.},
 author = {Audibert, Jean-yves},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ef575e8837d065a1683c022d2077d342-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ef575e8837d065a1683c022d2077d342-Metadata.json},
 openalex = {W2154637324},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ef575e8837d065a1683c022d2077d342-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Progressive mixture rules are deviation suboptimal},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/ef575e8837d065a1683c022d2077d342-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_efe93778,
 abstract = {People perform a remarkable range of tasks that require search of the visual environment for a target item among distractors. The Guided Search model (Wolfe, 1994, 2007), or GS, is perhaps the best developed psychological account of human visual search. To prioritize search, GS assigns saliency to locations in the visual field. Saliency is a linear combination of activations from retinotopic maps representing primitive visual features. GS includes heuristics for setting the gain coefficient associated with each map. Variants of GS have formalized the notion of optimization as a principle of attentional control (e.g., Baldwin & Mozer, 2006; Cave, 1999; Navalpakkam & Itti, 2006; Rao et al., 2002), but every GS-like model must be 'dumbed down' to match human data, e.g., by corrupting the saliency map with noise and by imposing arbitrary restrictions on gain modulation. We propose a principled probabilistic formulation of GS, called Experience-Guided Search (EGS), based on a generative model of the environment that makes three claims: (1) Feature detectors produce Poisson spike trains whose rates are conditioned on feature type and whether the feature belongs to a target or distractor; (2) the environment and/or task is nonstationary and can change over a sequence of trials; and (3) a prior specifies that features are more likely to be present for target than for distractors. Through experience, EGS infers latent environment variables that determine the gains for guiding search. Control is thus cast as probabilistic inference, not optimization. We show that EGS can replicate a range of human data from visual search, including data that GS does not address.},
 author = {Baldwin, David and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/efe937780e95574250dabe07151bdc23-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/efe937780e95574250dabe07151bdc23-Metadata.json},
 openalex = {W2139682177},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/efe937780e95574250dabe07151bdc23-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Experience-Guided Search: A Theory of Attentional Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/efe937780e95574250dabe07151bdc23-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_f29c21d4,
 abstract = {Hierarchical penalization is a generic framework for incorporating prior information in the fitting of statistical models, when the explicative variables are organized in a hierarchical structure. The penalizer is a convex functional that performs soft selection at the group level, and shrinks variables within each group. This favors solutions with few leading terms in the final combination. The framework, originally derived for taking prior knowledge into account, is shown to be useful in linear regression, when several parameters are used to model the influence of one feature, or in kernel regression, for learning multiple kernels.},
 author = {Szafranski, Marie and Grandvalet, Yves and Morizet-mahoudeaux, Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f29c21d4897f78948b91f03172341b7b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f29c21d4897f78948b91f03172341b7b-Metadata.json},
 openalex = {W2293576742},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Hierarchical Penalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/f29c21d4897f78948b91f03172341b7b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_f57a2f55,
 abstract = {Loopy belief propagation has been employed in a wide variety of applications with great empirical success, but it comes with few theoretical guarantees. In this paper we investigate the use of the max-product form of belief propagation for weighted matching problems on general graphs. We show that max-product converges to the correct answer if the linear programming (LP) relaxation of the weighted matching problem is tight and does not converge if the LP relaxation is loose. This provides an exact characterization of max-product performance and reveals connections to the widely used optimization technique of LP relaxation. In addition, we demonstrate that max-product is effective in solving practical weighted matching problems in a distributed fashion by applying it to the problem of self-organization in sensor networks.},
 author = {Sanghavi, Sujay and Malioutov, Dmitry and Willsky, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f57a2f557b098c43f11ab969efe1504b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f57a2f557b098c43f11ab969efe1504b-Metadata.json},
 openalex = {W2099091355},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f57a2f557b098c43f11ab969efe1504b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Linear programming analysis of loopy belief propagation for weighted matching},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/f57a2f557b098c43f11ab969efe1504b-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_f61d6947,
 abstract = {It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefficients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive fields. However, the resulting sparse coefficients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. Here, we propose a model that attempts to capture the dependencies among the basis function coefficients by including a pairwise coupling term in the prior over the coefficient activity states. When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1 in terms of a prior over natural images.},
 author = {Garrigues, Pierre and Olshausen, Bruno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f61d6947467ccd3aa5af24db320235dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f61d6947467ccd3aa5af24db320235dd-Metadata.json},
 openalex = {W2172172255},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f61d6947467ccd3aa5af24db320235dd-Supplemental.zip},
 title = {Learning Horizontal Connections in a Sparse Coding Model of Natural Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/f61d6947467ccd3aa5af24db320235dd-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_f76a89f0,
 author = {Weimer, Markus and Karatzoglou, Alexandros and Le, Quoc and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f76a89f0cb91bc419542ce9fa43902dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f76a89f0cb91bc419542ce9fa43902dc-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f76a89f0cb91bc419542ce9fa43902dc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f76a89f0cb91bc419542ce9fa43902dc-Supplemental.zip},
 title = {COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/f76a89f0cb91bc419542ce9fa43902dc-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_f899139d,
 author = {Welling, Max and Porteous, Ian and Bart, Evgeniy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f899139df5e1059396431415e770c6dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f899139df5e1059396431415e770c6dd-Metadata.json},
 openalex = {W2170789131},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f899139df5e1059396431415e770c6dd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Infinite State Bayes-Nets for Structured Domains},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/f899139df5e1059396431415e770c6dd-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_fa7cdfad,
 abstract = {Semi-supervised inductive learning concerns how to learn a decision rule from a data set containing both labeled and unlabeled data. Several boosting algorithms have been extended to semi-supervised learning with various strategies. To our knowledge, however, none of them takes local smoothness constraints among data into account during ensemble learning. In this paper, we introduce a local smoothness regularizer to semi-supervised boosting algorithms based on the universal optimization framework of margin cost functionals. Our regularizer is applicable to existing semi-supervised boosting algorithms to improve their generalization and speed up their training. Comparative results on synthetic, benchmark and real world tasks demonstrate the effectiveness of our local smoothness regularizer. We discuss relevant issues and relate our regularizer to previous work.},
 author = {Chen, Ke and Wang, Shihai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Metadata.json},
 openalex = {W2130844314},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Regularized Boost for Semi-Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_faa9afea,
 abstract = {Clustering is often formulated as a discrete optimization problem. The objective is to find, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the finite data set has been sampled from some underlying space, the goal is not to find the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal. As an alternative, we suggest the paradigm of neighbor clustering. Instead of selecting the best out of all partitions of the sample, it only considers partitions in some restricted function class. Using tools from statistical learning theory we prove that nearest neighbor clustering is statistically consistent. Moreover, its worst case complexity is polynomial by construction, and it can be implemented with small average case complexity using branch and bound.},
 author = {Luxburg, Ulrike and Jegelka, Stefanie and Kaufmann, Michael and Bubeck, S\'{e}bastien},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/faa9afea49ef2ff029a833cccc778fd0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/faa9afea49ef2ff029a833cccc778fd0-Metadata.json},
 openalex = {W2162818082},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Consistent Minimization of Clustering Objective Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/faa9afea49ef2ff029a833cccc778fd0-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_fba9d881,
 abstract = {The core tenet of Bayesian modeling is that subjects represent beliefs as distributions over possible hypotheses. Such models have fruitfully been applied to the study of learning in the context of animal conditioning experiments (and analogously designed human learning tasks), where they explain phenomena such as retrospective revaluation that seem to demonstrate that subjects entertain multiple hypotheses simultaneously. However, a recent quantitative analysis of individual subject records by Gallistel and colleagues cast doubt on a very broad family of conditioning models by showing that all of the key features the models capture about even simple learning curves are artifacts of averaging over subjects. Rather than smooth learning curves (which Bayesian models interpret as revealing the gradual tradeoff from prior to posterior as data accumulate), subjects acquire suddenly, and their predictions continue to fluctuate abruptly. These data demand revisiting the model of the individual versus the ensemble, and also raise the worry that more sophisticated behaviors thought to support Bayesian models might also emerge artifactually from averaging over the simpler behavior of individuals. We suggest that the suddenness of changes in subjects’ beliefs (as expressed in conditioned behavior) can be modeled by assuming they are conducting inference using sequential Monte Carlo sampling with a small number of samples — one, in our simulations. Ensemble behavior resembles exact Bayesian models since, as in particle filters, it averages over many samples. Further, the model is capable of exhibiting sophisticated behaviors like retrospective revaluation at the ensemble level, even given minimally sophisticated individuals that do not track uncertainty from trial to trial. These results point to the need for more sophisticated experimental analysis to test Bayesian models, and refocus theorizing on the individual, while at the same time clarifying why the ensemble may be of interest.},
 author = {Courville, Aaron C and Daw, Nathaniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fba9d88164f3e2d9109ee770223212a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fba9d88164f3e2d9109ee770223212a0-Metadata.json},
 openalex = {W2119421677},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The rat as particle filter},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/fba9d88164f3e2d9109ee770223212a0-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_fe8c15fe,
 abstract = {Statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive computationally efﬁcient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. A bias-variance analysis and an experimental study demonstrate the applicability of the proposed method.},
 author = {Lebanon, Guy and Mao, Yi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fe8c15fed5f808006ce95eddb7366e35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fe8c15fed5f808006ce95eddb7366e35-Metadata.json},
 openalex = {W2305586808},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Non-Parametric Modeling of Partially Ranked Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/fe8c15fed5f808006ce95eddb7366e35-Abstract.html},
 volume = {20},
 year = {2007}
}

@inproceedings{NIPS2007_ffeabd22,
 abstract = {Cascade detectors have been shown to operate extremely rapidly, with high accuracy, and have important applications such as face detection. Driven by this success, cascade learning has been an area of active research in recent years. Nevertheless, there are still challenging technical problems during the training process of cascade detectors. In particular, determining the optimal target detection rate for each stage of the cascade remains an unsolved issue. In this paper, we propose the multiple instance pruning (MIP) algorithm for soft cascades. This algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset. The algorithm is based on two key insights: i) examples that are destined to be rejected by the complete classifier can be safely pruned early; ii) face detection is a multiple instance learning problem. The MIP process is fully automatic and requires no assumptions of probability distributions, statistical independence, or ad hoc intermediate rejection targets. Experimental results on the MIT+CMU dataset demonstrate significant performance advantages.},
 author = {Zhang, Cha and Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ffeabd223de0d4eacb9a3e6e53e5448d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ffeabd223de0d4eacb9a3e6e53e5448d-Metadata.json},
 openalex = {W2135502357},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multiple-Instance Pruning For Learning Efficient Cascade Detectors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/ffeabd223de0d4eacb9a3e6e53e5448d-Abstract.html},
 volume = {20},
 year = {2007}
}
