@inproceedings{NIPS1996_0188e8b8,
 abstract = {This paper investigates the stationary points of a Hebb learning rule with a sigmoid nonlinearity in it. We show mathematically that when the input has a low information content, as measured by the input's variance, this learning rule suppresses learning, that is, forces the weight vector to converge to the zero vector. When the information content exceeds a certain value, the rule will automatically begin to learn a feature in the input. Our analysis suggests that under certain conditions it is the first principal component that is learned. The weight vector length remains bounded, provided the variance of the input is finite. Simulations confirm the theoretical results derived.},
 author = {Peper, Ferdinand and Noda, Hideki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0188e8b8b014829e2fa0f430f0a95961-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0188e8b8b014829e2fa0f430f0a95961-Metadata.json},
 openalex = {W2125377350},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0188e8b8b014829e2fa0f430f0a95961-Paper.pdf},
 publisher = {MIT Press},
 title = {Hebb Learning of Features based on their Information Content},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/0188e8b8b014829e2fa0f430f0a95961-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_018b59ce,
 abstract = {Given a multidimensional data set and a model of its density, we consider how to define the optimal interpolation between two points. This is done by assigning a cost to each path through space, based on two competing goals-one to interpolate through regions of high density, the other to minimize arc length. From this path functional, we derive the Euler-Lagrange equations for extremal motion; given two points, the desired interpolation is found by solving a boundary value problem. We show that this interpolation can be done efficiently, in high dimensions, for Gaussian, Dirichlet, and mixture models.},
 author = {Saul, Lawrence and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/018b59ce1fd616d874afad0f44ba338d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/018b59ce1fd616d874afad0f44ba338d-Metadata.json},
 openalex = {W2123911885},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf},
 publisher = {MIT Press},
 title = {A Variational Principle for Model-based Morphing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/018b59ce1fd616d874afad0f44ba338d-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_01e9565c,
 abstract = {A self-organizing architecture is developed for image region classification. The system consists of a preprocessor that utilizes multiscale filtering, competition, cooperation, and diffusion to compute a vector of image boundary and surface properties, notably texture and brightness properties. This vector inputs to a system that incrementally learns noisy multidimensional mappings and their probabilities. The architecture is applied to difficult real-world image classification problems, including classification of synthetic aperture radar and natural texture images, and outperforms a recent state-of-the-art system at classifying natural textures.},
 author = {Grossberg, Stephen and Williamson, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/01e9565cecc4e989123f9620c1d09c09-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/01e9565cecc4e989123f9620c1d09c09-Metadata.json},
 openalex = {W2153683700},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/01e9565cecc4e989123f9620c1d09c09-Paper.pdf},
 publisher = {MIT Press},
 title = {ARTEX: A Self-organizing Architecture for Classifying Image Regions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_0829424f,
 abstract = {The power of sampling methods in Bayesian reconstruction of noisy signals is well known. The extension of sampling to temporal problems is discussed. Efficacy of sampling over time is demonstrated with visual tracking.},
 author = {Blake, Andrew and Isard, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0829424ffa0d3a2547b6c9622c77de03-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0829424ffa0d3a2547b6c9622c77de03-Metadata.json},
 openalex = {W2110049245},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0829424ffa0d3a2547b6c9622c77de03-Paper.pdf},
 publisher = {MIT Press},
 title = {The CONDENSATION Algorithm - Conditional Density Propagation and Applications to Visual Tracking},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/0829424ffa0d3a2547b6c9622c77de03-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_0ce2ffd2,
 abstract = {Detection of the periodicity of amplitude modulation is a major step in the determination of the pitch of a Sound. In this article we will present a silicon model that uses synchronicity of spiking neurons to extract the fundamental frequency of a Sound. It is based on the observation that the so called 'Choppers' in the mammalian Cochlear Nucleus synchronize well for certain rates of amplitude modulation, depending on the cell's intrinsic chopping frequency. Our silicon model uses three different circuits, i.e., an artificial cochlea, an Inner Hair Cell circuit, and a spiking neuron circuit.},
 author = {van Schaik, Andr\'{e} and Fragni\`{e}re, Eric and Vittoz, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0ce2ffd21fc958d9ef0ee9ba5336e357-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0ce2ffd21fc958d9ef0ee9ba5336e357-Metadata.json},
 openalex = {W2144860194},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0ce2ffd21fc958d9ef0ee9ba5336e357-Paper.pdf},
 publisher = {MIT Press},
 title = {A Silicon Model of Amplitude Modulation Detection in the Auditory Brainstem},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/0ce2ffd21fc958d9ef0ee9ba5336e357-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_0e095e05,
 abstract = {An adaptive on-line algorithm extending the learning of learning idea is proposed and theoretically motivated. Relying only on gradient flow information it can be applied to learning continuous functions or distributions, even when no explicit loss function is given and the Hessian is not available. Its efficiency is demonstrated for a non-stationary blind separation task of acoustic signals.},
 author = {Murata, Noboru and M\"{u}ller, Klaus-Robert and Ziehe, Andreas and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0e095e054ee94774d6a496099eb1cf6a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0e095e054ee94774d6a496099eb1cf6a-Metadata.json},
 openalex = {W2131859749},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0e095e054ee94774d6a496099eb1cf6a-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive On-line Learning in Changing Environments},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/0e095e054ee94774d6a496099eb1cf6a-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_0e9fa1f3,
 abstract = {I describe an exploration criterion that attempts to minimize the error of a learner by minimizing its estimated squared bias. I describe experiments with locally-weighted regression on two simple kinematics problems, and observe that this bias-only approach outperforms the more common variance-only exploration approach, even in the presence of noise.},
 author = {Cohn, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0e9fa1f3e9e66792401a6972d477dcc3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0e9fa1f3e9e66792401a6972d477dcc3-Metadata.json},
 openalex = {W2150568880},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf},
 publisher = {MIT Press},
 title = {Minimizing Statistical Bias with Queries.},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/0e9fa1f3e9e66792401a6972d477dcc3-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_144a3f71,
 abstract = {We describe the implementation of a hidden Markov model state decoding system, a component for a wordspotting speech recognition system. The key specification for this state decoder design is microwatt power dissipation; this requirement led to a continuous-time, analog circuit implementation. We characterize the operation of a 10-word (81 state) state decoder test chip.},
 author = {Lazzaro, John and Wawrzynek, John and Lippmann, Richard P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/144a3f71a03ab7c4f46f9656608efdb2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/144a3f71a03ab7c4f46f9656608efdb2-Metadata.json},
 openalex = {W2142696641},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/144a3f71a03ab7c4f46f9656608efdb2-Paper.pdf},
 publisher = {MIT Press},
 title = {A Micropower Analog VLSI HMM State Decoder for Wordspotting},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/144a3f71a03ab7c4f46f9656608efdb2-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_147702db,
 abstract = {Prediction, estimation, and smoothing are fundamental to signal processing. To perform these interrelated tasks given noisy data, we form a time series model of the process that generates the data. Taking noise in the system explicitly into account, maximum-likelihood and Kalman frameworks are discussed which involve the dual process of estimating both the model parameters and the underlying state of the system. We review several established methods in the linear case, and propose several extensions utilizing dual Kalman filters (DKF) and forward-backward (FB) filters that are applicable to neural networks. Methods are compared on several simulations of noisy time series. We also include an example of nonlinear noise reduction in speech.},
 author = {Wan, Eric and Nelson, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/147702db07145348245dc5a2f2fe5683-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/147702db07145348245dc5a2f2fe5683-Metadata.json},
 openalex = {W2118538769},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/147702db07145348245dc5a2f2fe5683-Paper.pdf},
 publisher = {MIT Press},
 title = {Dual Kalman Filtering Methods for Nonlinear Prediction, Smoothing and Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/147702db07145348245dc5a2f2fe5683-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_160c8865,
 abstract = {We couple the tasks of source separation and density estimation by extracting the local geometrical structure of distributions obtained from mixtures of statistically independent sources. Our modifications of the self-organizing map (SOM) algorithm results in purely digital learning rules which perform non-parametric histogram density estimation. The non-parametric nature of the separation allows for source separation of non-linear mixtures. An anisotropic coupling is introduced into our SOM with the role of aligning the network locally with the independent component contours. This approach provides an exact verification condition for source separation with no prior on the source distributions.},
 author = {Lin, Juan and Cowan, Jack and Grier, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/160c88652d47d0be60bfbfed25111412-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/160c88652d47d0be60bfbfed25111412-Metadata.json},
 openalex = {W2149569475},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/160c88652d47d0be60bfbfed25111412-Paper.pdf},
 publisher = {MIT Press},
 title = {Source Separation and Density Estimation by Faithful Equivariant SOM},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/160c88652d47d0be60bfbfed25111412-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_17326d10,
 abstract = {Real-valued random hidden variables can be useful for modelling latent structure that explains correlations among observed variables. I propose a simple unit that adds zero-mean Gaussian noise to its input before passing it through a sigmoidal squashing function. Such units can produce a variety of useful behaviors, ranging from deterministic to binary stochastic to continuous stochastic. I show how slice sampling can be used for inference and learning in top-down networks of these units and demonstrate learning on two simple problems.},
 author = {Frey, Brendan J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/17326d10d511828f6b34fa6d751739e2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/17326d10d511828f6b34fa6d751739e2-Metadata.json},
 openalex = {W2105659727},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/17326d10d511828f6b34fa6d751739e2-Paper.pdf},
 publisher = {MIT Press},
 title = {Continuous Sigmoidal Belief Networks Trained using Slice Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/17326d10d511828f6b34fa6d751739e2-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_17fafe5f,
 abstract = {Images are ambiguous at each of many levels of a contextual hierarchy. Nevertheless, the high-level interpretation of most scenes is unambiguous, as evidenced by the superior performance of humans. This observation argues for global vision models, such as deformable templates. Unfortunately, such models are computationally intractable for unconstrained problems. We propose a compositional model in which primitives are recursively composed, subject to syntactic restrictions, to form tree-structured objects and object groupings. Ambiguity is propagated up the hierarchy in the form of multiple interpretations, which are later resolved by a Bayesian, equivalently minimum-description-Iength, cost functional.},
 author = {Bienenstock, Elie and Geman, Stuart and Potter, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Metadata.json},
 openalex = {W2165657641},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/17fafe5f6ce2f1904eb09d2e80a4cbf6-Paper.pdf},
 publisher = {MIT Press},
 title = {Compositionality, MDL Priors, and Object Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/17fafe5f6ce2f1904eb09d2e80a4cbf6-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_193002e6,
 abstract = {We present an algorithm which is expected to realise Bayes optimal predictions in large feed-forward networks. It is based on mean field methods developed within statistical mechanics of disordered systems. We give a derivation for the single layer perceptron and show that the algorithm also provides a leave-one-out cross-validation test of the predictions. Simulations show excellent agreement with theoretical results of statistical mechanics.},
 author = {Opper, Manfred and Winther, Ole},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/193002e668758ea9762904da1a22337c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/193002e668758ea9762904da1a22337c-Metadata.json},
 openalex = {W2151200696},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/193002e668758ea9762904da1a22337c-Paper.pdf},
 publisher = {MIT Press},
 title = {A Mean Field Algorithm for Bayes Learning in Large Feed-forward Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/193002e668758ea9762904da1a22337c-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_1c65cef3,
 abstract = {The classes in classification tasks often have a natural ordering, and the training and testing examples are often incomplete. We propose a nonlinear ordinal model for classification into ordered classes. Predictive, simulation-based approaches are used to learn from past and classify future incomplete examples. These techniques are illustrated by making prognoses for patients who have suffered severe head injuries.},
 author = {Mathieson, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1c65cef3dfd1e00c0b03923a1c591db4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1c65cef3dfd1e00c0b03923a1c591db4-Metadata.json},
 openalex = {W2165486401},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1c65cef3dfd1e00c0b03923a1c591db4-Paper.pdf},
 publisher = {MIT Press},
 title = {Ordered Classes and Incomplete Examples in Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/1c65cef3dfd1e00c0b03923a1c591db4-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_1cecc7a7,
 abstract = {In the present paper, we propose a method to unify information maximization and minimization in hidden units. The information maximization and minimization are performed on two different levels: collective and individual level. Thus, two kinds of information: collective and individual information are defined. By maximizing collective information and by minimizing individual information, simple networks can be generated in terms of the number of connections and the number of hidden units. Obtained networks are expected to give better generalization and improved interpretation of internal representations. This method was applied to the inference of the maximum onset principle of an artificial language. In this problem, it was shown that the individual information minimization is not contradictory to the collective information maximization. In addition, experimental results confirmed improved generalization performance, because over-training can significantly be suppressed.},
 author = {Kamimura, Ryotaro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1cecc7a77928ca8133fa24680a88d2f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1cecc7a77928ca8133fa24680a88d2f9-Metadata.json},
 openalex = {W2121723556},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
 publisher = {MIT Press},
 title = {Unification of Information Maximization and Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_1d72310e,
 abstract = {Artificial Neural Networks can be used to predict future returns of stocks in order to take financial decisions. Should one build a separate network for each stock or share the same network for all the stocks? In this paper we also explore other alternatives, in which some layers are shared and others are not shared. When the prediction of future returns for different stocks are viewed as different tasks, sharing some parameters across stocks is a form of multi-task learning. In a series of experiments with Canadian stocks, we obtain yearly returns that are more than 14% above various benchmarks.},
 author = {Ghosn, Joumana and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1d72310edc006dadf2190caad5802983-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1d72310edc006dadf2190caad5802983-Metadata.json},
 openalex = {W2144807460},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1d72310edc006dadf2190caad5802983-Paper.pdf},
 publisher = {MIT Press},
 title = {Multi-Task Learning for Stock Selection},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/1d72310edc006dadf2190caad5802983-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_1ee3dfcd,
 abstract = {Neuromodulation can change not only the mean firing rate of a neuron, but also its pattern of firing. Therefore, a reliable neural coding scheme, whether a rate coding or a spike time based coding, must be robust in a dynamic neuromodulatory environment. The common observation that cholinergic modulation leads to a reduction in spike frequency adaptation implies a modification of spike timing, which would make a neural code based on precise spike timing difficult to maintain. In this paper, the effects of cholinergic modulation were studied to test the hypothesis that precise spike timing can serve as a reliable neural code. Using the whole cell patch-clamp technique in rat neocortical slice preparation and compartmental modeling techniques, we show that cholinergic modulation, surprisingly, preserved spike timing in response to a fluctuating inputs that resembles in vivo conditions. This result suggests that in vivo spike timing may be much more resistant to changes in neuromodulator concentrations than previous physiological studies have implied.},
 author = {Tang, Akaysha and Bartels, Andreas and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1ee3dfcd8a0645a25a35977997223d22-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1ee3dfcd8a0645a25a35977997223d22-Metadata.json},
 openalex = {W2151037517},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/1ee3dfcd8a0645a25a35977997223d22-Paper.pdf},
 publisher = {MIT Press},
 title = {Cholinergic Modulation Preserves Spike Timing Under Physiologically Realistic Fluctuating Input},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/1ee3dfcd8a0645a25a35977997223d22-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_242c100d,
 author = {Singh, Satinder and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/242c100dc94f871b6d7215b868a875f8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/242c100dc94f871b6d7215b868a875f8-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/242c100dc94f871b6d7215b868a875f8-Paper.pdf},
 publisher = {MIT Press},
 title = {Analytical Mean Squared Error Curves in Temporal Difference Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/242c100dc94f871b6d7215b868a875f8-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_25df35de,
 abstract = {A new method to calculate the full training process of a neural network is introduced. No sophisticated methods like the replica trick are used. The results are directly related to the actual number of training steps. Some results are presented here, like the maximal learning rate, an exact description of early stopping, and the necessary number of training steps. Further problems can be addressed with this approach.},
 author = {B\"{o}s, Siegfried and Opper, Manfred},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/25df35de87aa441b88f22a6c2a830a17-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/25df35de87aa441b88f22a6c2a830a17-Metadata.json},
 openalex = {W2166193626},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/25df35de87aa441b88f22a6c2a830a17-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamics of Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/25df35de87aa441b88f22a6c2a830a17-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_26588e93,
 abstract = {This paper discusses a fairly general adaptation algorithm which augments a standard neural network to increase its recognition accuracy for a specific user. The basis for the algorithm is that the output of a neural network is characteristic of the input, even when the output is incorrect. We exploit this characteristic output by using an Output Adaptation Module (OAM) which maps this output into the correct user-dependent confidence vector. The OAM is a simplified Resource Allocating Network which constructs radial basis functions on-line. We applied the OAM to construct a writer-adaptive character recognition system for on-line handprinted characters. The OAM decreases the word error rate on a test set by an average of 45%, while creating only 3 to 25 basis functions for each writer in the test set.},
 author = {Platt, John and Matic, Nada},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/26588e932c7ccfa1df309280702fe1b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/26588e932c7ccfa1df309280702fe1b5-Metadata.json},
 openalex = {W2118980686},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/26588e932c7ccfa1df309280702fe1b5-Paper.pdf},
 publisher = {MIT Press},
 title = {A Constructive RBF Network for Writer Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/26588e932c7ccfa1df309280702fe1b5-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_2715518c,
 abstract = {Results of a study of the worst case learning curves for a particular class of probability distribution on input space to MLP with hard threshold hidden units are presented. It is shown in particular, that in the thermodynamic limit for scaling by the number of connections to the first hidden layer, although the true learning curve behaves as ≈ α-1 for α ≈ 1, its VC-dimension based bound is trivial (≡ 1) and its VC-entropy bound is trivial for α ≤ 6.2. It is also shown that bounds following the true learning curve can be derived from a formalism based on the density of error patterns.},
 author = {Kowalczyk, Adam and Ferr\'{a}, Herman},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2715518c875999308842e3455eda2fe3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2715518c875999308842e3455eda2fe3-Metadata.json},
 openalex = {W2106870773},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2715518c875999308842e3455eda2fe3-Paper.pdf},
 publisher = {MIT Press},
 title = {MLP Can Provably Generalize Much Better than VC-bounds Indicate},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/2715518c875999308842e3455eda2fe3-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_2812e5cf,
 abstract = {In 1990 Poggio and Edelman proposed a view-based model of object recognition that accounts for several psychophysical properties of certain recognition tasks. The model predicted the existence of view-tuned and view-invariant units, that were later found by Logothetis et al. (Logothetis et al., 1995) in IT cortex of monkeys trained with views of specific paperclip objects. The model, however, does not specify the inputs to the view-tuned units and their internal organization. In this paper we propose a model of these view-tuned units that is consistent with physiological data from single cell responses.},
 author = {Bricolo, Emanuela and Poggio, Tomaso and Logothetis, Nikos K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2812e5cf6d8f21d69c91dddeefb792a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2812e5cf6d8f21d69c91dddeefb792a7-Metadata.json},
 openalex = {W2162200008},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2812e5cf6d8f21d69c91dddeefb792a7-Paper.pdf},
 publisher = {MIT Press},
 title = {3D Object Recognition: A Model of View-Tuned Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/2812e5cf6d8f21d69c91dddeefb792a7-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_285ab944,
 abstract = {In this paper we propose a method for learning Bayesian belief networks from data. The method uses artificial neural networks as probability estimators, thus avoiding the need for making prior assumptions on the nature of the probability distributions governing the relationships among the participating variables. This new method has the potential for being applied to domains containing both discrete and continuous variables arbitrarily distributed. We compare the learning performance of this new method with the performance of the method proposed by Cooper and Herskovits in [7]. The experimental results show that, although the learning scheme based on the use of ANN estimators is slower, the learning accuracy of the two methods is comparable.},
 author = {Monti, Stefano and Cooper, Gregory},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/285ab9448d2751ee57ece7f762c39095-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/285ab9448d2751ee57ece7f762c39095-Metadata.json},
 openalex = {W2097296535},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/285ab9448d2751ee57ece7f762c39095-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Bayesian Belief Networks with Neural Network Estimators},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/285ab9448d2751ee57ece7f762c39095-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_286674e3,
 abstract = {In this paper we establish an extension of the method of approximating optimal discrete-time stopping problems by related limiting stopping problems for Poisson-type processes. This extension allows us to apply this method to a larger class of examples, such as those arising, for example, from point process convergence results in extreme value theory. Furthermore, we develop new classes of solutions of the differential equations which characterize optimal threshold functions. As a particular application, we give a fairly complete discussion of the approximative optimal stopping behavior of independent and identically distributed sequences with discount and observation costs.},
 author = {Tsitsiklis, John and Van Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/286674e3082feb7e5afb92777e48821f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/286674e3082feb7e5afb92777e48821f-Metadata.json},
 openalex = {W2060224423},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/286674e3082feb7e5afb92777e48821f-Paper.pdf},
 publisher = {MIT Press},
 title = {On approximative solutions of optimal stopping problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/286674e3082feb7e5afb92777e48821f-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_2a50e9c2,
 abstract = {We present an algorithm for identifying linear patterns on a two-dimensional lattice based on the concept of an orientation selective cell, a concept borrowed from neurobiology of vision. Constructing a multi-layered neural network with fixed architecture which implements orientation selectivity, we define output elements corresponding to different orientations, which allow us to make a selection decision. The algorithm takes into account the granularity of the lattice as well as the presence of noise and inefficiencies. The method is applied to a sample of data collected with the ZEUS detector at HERA in order to identify cosmic muons that leave a linear pattern of signals in the segmented calorimeter. A two dimensional representation of the relevant part of the detector is used. The algorithm performs very well. Given its architecture, this system becomes a good candidate for fast pattern recognition in parallel processing devices.},
 author = {Abramowicz, Halina and Horn, David and Naftaly, Ury and Sahar-Pikielny, Carmit},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2a50e9c2d6b89b95bcb416d6857f8b45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2a50e9c2d6b89b95bcb416d6857f8b45-Metadata.json},
 openalex = {W2133301185},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2a50e9c2d6b89b95bcb416d6857f8b45-Paper.pdf},
 publisher = {MIT Press},
 title = {An Orientation Selective Neural Network for Pattern Identification in Particle Detectors},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/2a50e9c2d6b89b95bcb416d6857f8b45-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_2ac2406e,
 author = {Tresp, Volker and Neuneier, Ralph and Zimmermann, Hans-Georg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2ac2406e835bd49c70469acae337d292-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2ac2406e835bd49c70469acae337d292-Metadata.json},
 openalex = {W411900653},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2ac2406e835bd49c70469acae337d292-Paper.pdf},
 publisher = {MIT Press},
 title = {Early brain damage},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/2ac2406e835bd49c70469acae337d292-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_2b6d65b9,
 abstract = {The following investigates the use of single-neuron learning algorithms to improve the performance of text-retrieval systems that accept natural-language queries. A retrieval process is explained that transforms the natural-language query into the query syntax of a real retrieval system: the initial query is expanded using statistical and learning techniques and is then used for document ranking and binary classification. The results of experiments suggest that Kivinen and Warmuth's Exponentiated Gradient Descent learning algorithm works significantly better than previous approaches.},
 author = {Papka, Ron and Callan, James and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2b6d65b9a9445c4271ab9076ead5605a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2b6d65b9a9445c4271ab9076ead5605a-Metadata.json},
 openalex = {W2097879972},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2b6d65b9a9445c4271ab9076ead5605a-Paper.pdf},
 publisher = {MIT Press},
 title = {Text-Based Information Retrieval Using Exponentiated Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/2b6d65b9a9445c4271ab9076ead5605a-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_2ba8698b,
 abstract = {We use the constant statistics constraint to calibrate an array of sensors that contains gain and offset variations. This algorithm has been mapped to analog hardware and designed and fabricated with a 2µm CMOS technology. Measured results from the chip show that the system achieves invariance to gain and offset variations of the input signal.},
 author = {Harris, John and Chiang, Yu-Ming},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2ba8698b79439589fdd2b0f7218d8b07-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2ba8698b79439589fdd2b0f7218d8b07-Metadata.json},
 openalex = {W2169231356},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2ba8698b79439589fdd2b0f7218d8b07-Paper.pdf},
 publisher = {MIT Press},
 title = {An Analog Implementation of the Constant Average Statistics Constraint For Sensor Calibration},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/2ba8698b79439589fdd2b0f7218d8b07-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_2c89109d,
 abstract = {We introduce a neurobiologically plausible model of contour integration from visual inputs of individual oriented edges. The model is composed of interacting excitatory neurons and inhibitory interneurons, receives visual inputs via oriented receptive fields (RFs) like those in V1. The RF centers are distributed in space. At each location, a finite number of cells tuned to orientations spanning 180° compose a model hypercolumn. Cortical interactions modify neural activities produced by visual inputs, selectively amplifying activities for edge elements belonging to smooth input contours. Elements within one contour produce synchronized neural activities. We show analytically and empirically that contour enhancement and neural synchrony increase with contour length, smoothness and closure, as observed experimentally. This model gives testable predictions, and in addition, introduces a feedback mechanism allowing higher visual centers to enhance, suppress, and segment contours.},
 author = {Li, Zhaoping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2c89109d42178de8a367c0228f169bf8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2c89109d42178de8a367c0228f169bf8-Metadata.json},
 openalex = {W2140115648},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2c89109d42178de8a367c0228f169bf8-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neural Model of Visual Contour Integration},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/2c89109d42178de8a367c0228f169bf8-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_2de5d166,
 abstract = {Unsupervised learning algorithms based on convex and conic encoders are proposed. The encoders find the closest convex or conic combination of basis vectors to the input. The learning algorithms produce basis vectors that minimize the reconstruction error of the encoders. The convex algorithm develops locally linear models of the input, while the conic algorithm discovers features. Both algorithms are used to model handwritten digits and compared with vector quantization and principal component analysis. The neural network implementations involve feedback connections that project a reconstruction back to the input layer.},
 author = {Lee, Daniel and Seung, H. Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2de5d16682c3c35007e4e92982f1a2ba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2de5d16682c3c35007e4e92982f1a2ba-Metadata.json},
 openalex = {W2148365208},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf},
 publisher = {MIT Press},
 title = {Unsupervised Learning by Convex and Conic Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_2df45244,
 abstract = {We employed a white-noise velocity signal to study the dynamics of the response of single neurons in the cortical area MT to visual motion. Responses were quantified using reverse correlation, optimal linear reconstruction filters, and reconstruction signal-to-noise ratio (SNR). The SNR and lower bound estimates of information rate were lower than we expected. Ninety percent of the information was transmitted below 18 Hz, and the highest lower bound on bit rate was 12 bits/s. A simulated opponent motion energy subunit with Poisson spike statistics was able to out-perform the MT neurons. The temporal integration window, measured from the reverse correlation half-width, ranged from 30-90 ms. The window was narrower when a stimulus moved faster, but did not change when temporal frequency was held constant.},
 author = {Bair, Wyeth and Cavanaugh, James and Movshon, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2df45244f09369e16ea3f9117ca45157-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2df45244f09369e16ea3f9117ca45157-Metadata.json},
 openalex = {W2154476188},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/2df45244f09369e16ea3f9117ca45157-Paper.pdf},
 publisher = {MIT Press},
 title = {Reconstructing Stimulus Velocity from Neuronal Responses in Area MT},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/2df45244f09369e16ea3f9117ca45157-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_310ce61c,
 abstract = {Dynamic Programming, Q-learning and other discrete Markov Decision Process solvers can be applied to continuous d-dimensional state-spaces by quantizing the state space into an array of boxes. This is often problematic above two dimensions: a coarse quantization can lead to poor policies, and fine quantization is too expensive. Possible solutions are variable-resolution discretization, or function approximation by neural nets. A third option, which has been little studied in the reinforcement learning literature, is interpolation on a coarse grid. In this paper we study interpolation techniques that can result in vast improvements in the online behavior of the resulting control systems: multilinear interpolation, and an interpolation algorithm based on an interesting regular triangulation of d-dimensional space. We adapt these interpolators under three reinforcement learning paradigms: (i) offline value iteration with a known model, (ii) Q-learning, and (iii) online value iteration with a previously unknown model learned from data. We describe empirical results, and the resulting implications for practical learning of continuous non-linear dynamic control.},
 author = {Davies, Scott},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/310ce61c90f3a46e340ee8257bc70e93-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/310ce61c90f3a46e340ee8257bc70e93-Metadata.json},
 openalex = {W2145756561},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf},
 publisher = {MIT Press},
 title = {Multidimensional Triangulation and Interpolation for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/310ce61c90f3a46e340ee8257bc70e93-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_3210ddbe,
 abstract = {We have explored two approaches to recognizing faces across changes in pose. First, we developed a representation of face images based on independent component analysis (ICA) and compared it to a principal component analysis (PCA) representation for face recognition. The ICA basis vectors for this data set were more spatially local than the PCA basis vectors and the ICA representation had greater invariance to changes in pose. Second, we present a model for the development of viewpoint invariant responses to faces from visual experience in a biological system. The temporal continuity of natural visual experience was incorporated into an attractor network model by Hebbian learning following a lowpass temporal filter on unit activities. When combined with the temporal filter, a basic Hebbian update rule became a generalization of Griniasty et al. (1993), which associates temporally proximal input patterns into basins of attraction. The system acquired representations of faces that were largely independent of pose.},
 author = {Bartlett, Marian and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3210ddbeaa16948a702b6049b8d9a202-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3210ddbeaa16948a702b6049b8d9a202-Metadata.json},
 openalex = {W2147221263},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3210ddbeaa16948a702b6049b8d9a202-Paper.pdf},
 publisher = {MIT Press},
 title = {Viewpoint Invariant Face Recognition using Independent Component Analysis and Attractor Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/3210ddbeaa16948a702b6049b8d9a202-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_33ceb07b,
 abstract = {We introduce a model for analog computation with discrete time in the presence of analog noise that is flexible enough to cover the most important concrete cases, such as noisy analog neural nets and networks of spiking neurons. This model subsumes the classical model for digital computation in the presence of noise. We show that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata, and we also prove a new type of upper bound for the VC-dimension of computational models with analog noise.},
 author = {Maass, Wolfgang and Orponen, Pekka},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/33ceb07bf4eeb3da587e268d663aba1a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/33ceb07bf4eeb3da587e268d663aba1a-Metadata.json},
 openalex = {W2065771184},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Effect of Analog Noise in Discrete-Time Analog Computations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_3546ab44,
 abstract = {To obtain classification systems with both good generalization performance and efficiency in space and time, we propose a learning method based on combinations of weak classifiers, where weak classifiers are linear classifiers (perceptrons) which can do a little better than making random guesses. A randomized algorithm is proposed to find the weak classifiers. They are then combined through a majority vote. As demonstrated through systematic experiments, the method developed is able to obtain combinations of weak classifiers with good generalization performance and a fast training time on a variety of test problems and real applications. Theoretical analysis on one of the test problems investigated in our experiments provides insights on when and why the proposed method works. In particular, when the strength of weak classifiers is properly chosen, combinations of weak classifiers can achieve a good generalization performance with polynomial space- and time-complexity.},
 author = {Ji, Chuanyi and Ma, Sheng},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3546ab441e56fa333f8b44b610d95691-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3546ab441e56fa333f8b44b610d95691-Metadata.json},
 openalex = {W2119103742},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3546ab441e56fa333f8b44b610d95691-Paper.pdf},
 publisher = {MIT Press},
 title = {Combinations of weak classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/3546ab441e56fa333f8b44b610d95691-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_3948ead6,
 abstract = {In cellular telephone systems, an important problem is to dynamically allocate the communication resource (channels) so as to maximize service in a stochastic caller environment. This problem is naturally formulated as a dynamic programming problem and we use a reinforcement learning (RL) method to find dynamic channel allocation policies that are better than previous heuristic solutions. The policies obtained perform well for a broad variety of call traffic patterns. We present results on a large cellular system with approximately 4949 states.},
 author = {Singh, Satinder and Bertsekas, Dimitri},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3948ead63a9f2944218de038d8934305-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3948ead63a9f2944218de038d8934305-Metadata.json},
 openalex = {W2169022337},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3948ead63a9f2944218de038d8934305-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/3948ead63a9f2944218de038d8934305-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_39e4973b,
 abstract = {The parameter space of neural networks has a Riemannian metric structure. The natural Riemannian gradient should be used instead of the conventional gradient, since the former denotes the true steepest descent direction of a loss function in the Riemannian space. The behavior of the stochastic gradient learning algorithm is much more effective if the natural gradient is used. The present paper studies the information-geometrical structure of perceptrons and other networks, and prove that the on-line learning method based on the natural gradient is asymptotically as efficient as the optimal batch algorithm. Adaptive modification of the learning constant is proposed and analyzed in terms of the Riemannian measure and is shown to be efficient. The natural gradient is finally applied to blind separation of mixtured independent signal sources.},
 author = {Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/39e4973ba3321b80f37d9b55f63ed8b8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/39e4973ba3321b80f37d9b55f63ed8b8-Metadata.json},
 openalex = {W2167729035},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Learning in Structured Parameter Spaces - Natural Riemannian Gradient},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/39e4973ba3321b80f37d9b55f63ed8b8-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_3a029f04,
 abstract = {The encoding of random time-varying stimuli in single spike trains of electrosensory neurons in the weakly electric fish Eigenmannia was investigated using methods of statistical signal processing. At the first stage of the electrosensory system, spike trains were found to encode faithfully the detailed time-course of random stimuli, while at the second stage neurons responded specifically to features in the temporal waveform of the stimulus. Therefore stimulus information is processed at the second stage of the electrosensory system by extracting temporal features from the faithfully preserved image of the environment sampled at the first stage.},
 author = {Gabbiani, Fabrizio and Metzner, Walter and Wessel, Ralf and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3a029f04d76d32e79367c4b3255dda4d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3a029f04d76d32e79367c4b3255dda4d-Metadata.json},
 openalex = {W2169634480},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf},
 publisher = {MIT Press},
 title = {Extraction of Temporal Features in the Electrosensory System of Weakly Electric Fish},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/3a029f04d76d32e79367c4b3255dda4d-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_3bf55bba,
 abstract = {A general feature of the cerebral cortex is its massive interconnectivity - it has been estimated anatomically [19] that cortical neurons receive upwards of 5,000 synapses, the majority of which originate from other nearby cortical neurons. Numerous experiments in primary visual cortex (V1) have revealed strongly nonlinear interactions between stimulus elements which activate classical and non-classical receptive field regions. Recurrent cortical connections likely contribute substantially to these effects. However, most theories of visual processing have either assumed a feedforward processing scheme [7], or have used recurrent interactions to account for isolated effects only [1, 16, 18]. Since nonlinear systems cannot in general be taken apart and analyzed in pieces, it is not clear what one learns by building a recurrent model that only accounts for one, or very few phenomena. Here we develop a relatively simple model of recurrent interactions in V1, that reflects major anatomical and physiological features of intracortical connectivity, and simultaneously accounts for a wide range of phenomena observed physiologically. All phenomena we address are strongly nonlinear, and cannot be explained by linear feedforward models.},
 author = {Todorov, Emanuel and Siapas, Athanassios and Somers, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3bf55bbad370a8fcad1d09b005e278c2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3bf55bbad370a8fcad1d09b005e278c2-Metadata.json},
 openalex = {W2133713135},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3bf55bbad370a8fcad1d09b005e278c2-Paper.pdf},
 publisher = {MIT Press},
 title = {A Model of Recurrent Interactions in Primary Visual Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/3bf55bbad370a8fcad1d09b005e278c2-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_3f67fd97,
 abstract = {A modification is described to the use of mean field approximations in the E step of EM algorithms for analysing data from latent structure models, as described by Ghahramani (1995), among others. The modification involves second-order Taylor approximations to expectations computed in the E step. The potential benefits of the method are illustrated using very simple latent profile models.},
 author = {Dunmur, A. and Titterington, D.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3f67fd97162d20e6fe27748b5b372509-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3f67fd97162d20e6fe27748b5b372509-Metadata.json},
 openalex = {W2172110455},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/3f67fd97162d20e6fe27748b5b372509-Paper.pdf},
 publisher = {MIT Press},
 title = {On a Modification to the Mean Field EM Algorithm in Factorial Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/3f67fd97162d20e6fe27748b5b372509-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_4079016d,
 abstract = {Two dimensional image motion detection neural networks have been implemented using a general purpose analog neural computer. The neural circuits perform spatiotemporal feature extraction based on the cortical motion detection model of Adelson and Bergen. The neural computer provides the neurons, synapses and synaptic time-constants required to realize the model in VLSI hardware. Results show that visual motion estimation can be implemented with simple sum-and-threshold neural hardware with temporal computational capabilities. The neural circuits compute general 20 visual motion in real-time.},
 author = {Etienne-Cummings, Ralph and Van der Spiegel, Jan and Takahashi, Naomi and Apsel, Alyssa and Mueller, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4079016d940210b4ae9ae7d41c4a2065-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4079016d940210b4ae9ae7d41c4a2065-Metadata.json},
 openalex = {W2153606892},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4079016d940210b4ae9ae7d41c4a2065-Paper.pdf},
 publisher = {MIT Press},
 title = {VLSI Implementation of Cortical Visual Motion Detection Using an Analog Neural Computer},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/4079016d940210b4ae9ae7d41c4a2065-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_4122cb13,
 abstract = {In general, procedures for determining Bayes-optimal adaptive controls for Markov decision processes (MDP's) require a prohibitive amount of computation-the optimal learning problem is intractable. This paper proposes an approximate approach in which bandit processes are used to model, in a certain sense, a given MDP. Bandit processes constitute an important subclass of MDP's, and have optimal learning strategies (defined in terms of Gittins indices) that can be computed relatively efficiently. Thus, one scheme for achieving approximately-optimal learning for general MDP's proceeds by taking actions suggested by strategies that are optimal with respect to local bandit models.},
 author = {Duff, Michael and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4122cb13c7a474c1976c9706ae36521d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4122cb13c7a474c1976c9706ae36521d-Metadata.json},
 openalex = {W2116686327},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4122cb13c7a474c1976c9706ae36521d-Paper.pdf},
 publisher = {MIT Press},
 title = {Local Bandit Approximation for Optimal Learning Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/4122cb13c7a474c1976c9706ae36521d-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_43cca4b3,
 abstract = {This paper describes a new technique for object recognition based on learning appearance models. The image is decomposed into local regions which are described by a new texture representation called Generalized Second Moments that are derived from the output of multiscale, multiorientation filter banks. Class-characteristic local texture features and their global composition is learned by a hierarchical mixture of experts architecture (Jordan & Jacobs). The technique is applied to a vehicle database consisting of 5 general car categories (Sedan, Van with back-doors, Van without back-doors, old Sedan, and Volkswagen Bug). This is a difficult problem with considerable in-class variation. The new technique has a 6.5% misclassification rate, compared to eigen-images which give 17.4% misclassification rate, and nearest neighbors which give 15.7% misclassification rate.},
 author = {Bregler, Christoph and Malik, Jitendra},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/43cca4b3de2097b9558efefd0ecc3588-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/43cca4b3de2097b9558efefd0ecc3588-Metadata.json},
 openalex = {W2167050501},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/43cca4b3de2097b9558efefd0ecc3588-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Appearance Based Models: Mixtures of Second Moment Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/43cca4b3de2097b9558efefd0ecc3588-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_4476b929,
 abstract = {A central theme of computational vision research has been the realization that reliable estimation of local scene properties requires propagating measurements across the image. Many authors have therefore suggested solving vision problems using architectures of locally connected units updating their activity in parallel. Unfortunately, the convergence of traditional relaxation methods on such architectures has proven to be excruciatingly slow and in general they do not guarantee that the stable point will be a global minimum.

In this paper we show that an architecture in which Bayesian Beliefs about image properties are propagated between neighboring units yields convergence times which are several orders of magnitude faster than traditional methods and avoids local minima. In particular our architecture is non-iterative in the sense of Marr [5]: at every time step, the local estimates at a given location are optimal given the information which has already been propagated to that location. We illustrate the algorithm's performance on real images and compare it to several existing methods.},
 author = {Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Metadata.json},
 openalex = {W2159938933},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf},
 publisher = {MIT Press},
 title = {Interpreting Images by Propagating Bayesian Beliefs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/4476b929e30dd0c4e8bdbcc82c6ba23a-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_459a4ddc,
 abstract = {Although TD-Gammon is one of the major successes in machine learning, it has not led to similar impressive breakthroughs in temporal difference learning for other applications or even other games. We were able to replicate some of the success of TD-Gammon, developing a competitive evaluation function on a 4000 parameter feed-forward neural network, without using back-propagation, reinforcement or temporal difference learning methods. Instead we apply simple hill-climbing in a relative fitness environment. These results and further analysis suggest that the surprising success of Tesauro's program had more to do with the co-evolutionary structure of the learning task and the dynamics of the backgammon game itself.},
 author = {Pollack, Jordan and Blair, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/459a4ddcb586f24efd9395aa7662bc7c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/459a4ddcb586f24efd9395aa7662bc7c-Metadata.json},
 openalex = {W2112836703},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf},
 publisher = {MIT Press},
 title = {Why did TD-Gammon Work?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/459a4ddcb586f24efd9395aa7662bc7c-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_4671aeaf,
 abstract = {Dimension-reducing feature extraction neural network techniques which also preserve neighbourhood relationships in data have traditionally been the exclusive domain of Kohonen self organising maps. Recently, we introduced a novel dimension-reducing feature extraction process, which is also topographic, based upon a Radial Basis Function architecture. It has been observed that the generalisation performance of the system is broadly insensitive to model order complexity and other smoothing factors such as the kernel widths, contrary to intuition derived from supervised neural network models. In this paper we provide an effective demonstration of this property and give a theoretical justification for the apparent 'self-regularising' behaviour of the 'NEUROSCALE' architecture.},
 author = {Lowe, David and Tipping, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4671aeaf49c792689533b00664a5c3ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4671aeaf49c792689533b00664a5c3ef-Metadata.json},
 openalex = {W2140130790},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4671aeaf49c792689533b00664a5c3ef-Paper.pdf},
 publisher = {MIT Press},
 title = {NeuroScale: Novel Topographic Feature Extraction using RBF Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/4671aeaf49c792689533b00664a5c3ef-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_491442df,
 abstract = {We consider the problem of prediction of stationary time series, using the architecture known as mixtures of experts (MEM). Here we suggest a mixture which blends several autoregressive models. This study focuses on some theoretical foundations of the prediction problem in this context. More precisely, it is demonstrated that this model is a universal approximator, with respect to learning the unknown prediction function. This statement is strengthened as upper bounds on the mean squared error are established. Based on these results it is possible to compare the MEM to other families of models (e.g., neural networks and state dependent models). It is shown that a degenerate version of the MEM is in fact equivalent to a neural network, and the number of experts in the architecture plays a similar role to the number of hidden units in the latter model.},
 author = {Zeevi, Assaf and Meir, Ron and Adler, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/491442df5f88c6aa018e86dac21d3606-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/491442df5f88c6aa018e86dac21d3606-Metadata.json},
 openalex = {W2168020547},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/491442df5f88c6aa018e86dac21d3606-Paper.pdf},
 publisher = {MIT Press},
 title = {Time Series Prediction using Mixtures of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/491442df5f88c6aa018e86dac21d3606-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_4c22bd44,
 abstract = {In many optimization problems, the structure of solutions reflects complex relationships between the different input parameters. For example, experience may tell us that certain parameters are closely related and should not be explored independently. Similarly, experience may establish that a subset of parameters must take on particular values. Any search of the cost landscape should take advantage of these relationships. We present MIMIC, a framework in which we analyze the global structure of the optimization landscape. A novel and efficient algorithm for the estimation of this structure is derived. We use knowledge of this structure to guide a randomized search through the solution space and, in turn, to refine our estimate ofthe structure. Our technique obtains significant speed gains over other randomized optimization procedures.},
 author = {De Bonet, Jeremy and Isbell, Charles and Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4c22bd444899d3b6047a10b20a2f26db-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4c22bd444899d3b6047a10b20a2f26db-Metadata.json},
 openalex = {W2166603077},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4c22bd444899d3b6047a10b20a2f26db-Paper.pdf},
 publisher = {MIT Press},
 title = {MIMIC: Finding Optima by Estimating Probability Densities},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/4c22bd444899d3b6047a10b20a2f26db-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_4d2e7bd3,
 abstract = {We train recurrent networks to control chemotaxis in a computer model of the nematode C. elegans. The model presented is based closely on the body mechanics, behavioral analyses, neuroanatomy and neurophysiology of C. elegans, each imposing constraints relevant for information processing. Simulated worms moving autonomously in simulated chemical environments display a variety of chemotaxis strategies similar to those of biological worms.},
 author = {Ferr\'{e}e, Thomas and Marcotte, Ben and Lockery, Shawn},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4d2e7bd33c475784381a64e43e50922f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4d2e7bd33c475784381a64e43e50922f-Metadata.json},
 openalex = {W2122030211},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4d2e7bd33c475784381a64e43e50922f-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Network Models of Chemotaxis in the Nematode Caenorhabditis Elegans},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/4d2e7bd33c475784381a64e43e50922f-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_4e4e53aa,
 abstract = {The Self-Organizing Map (SOM) algorithm has been extensively studied and has been applied with considerable success to a wide variety of problems. However, the algorithm is derived from heuristic ideas and this leads to a number of significant limitations. In this paper, we consider the problem of modelling the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. We introduce a novel form of latent variable model, which we call the GTM algorithm (for Generative Topographic Map), which allows general non-linear transformations from latent space to data space, and which is trained using the EM (expectation-maximization) algorithm. Our approach overcomes the limitations of the SOM, while introducing no significant disadvantages. We demonstrate the performance of the GTM algorithm on simulated data from flow diagnostics for a multi-phase oil pipeline.},
 author = {Bishop, Christopher and Svens\'{e}n, Markus and Williams, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4e4e53aa080247bc31d0eb4e7aeb07a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4e4e53aa080247bc31d0eb4e7aeb07a0-Metadata.json},
 openalex = {W1726174625},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4e4e53aa080247bc31d0eb4e7aeb07a0-Paper.pdf},
 publisher = {MIT Press},
 title = {GTM: A principled alternative to the Self-Organizing Map},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/4e4e53aa080247bc31d0eb4e7aeb07a0-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_4f284803,
 abstract = {The Support Vector (SV) method was recently proposed for estimating regressions, constructing multidimensional splines, and solving linear operator equations [Vapnik, 1995]. In this presentation we report results of applying the SV method to these problems.},
 author = {Vapnik, Vladimir and Golowich, Steven and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4f284803bd0966cc24fa8683a34afc6e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4f284803bd0966cc24fa8683a34afc6e-Metadata.json},
 openalex = {W2123737232},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/4f284803bd0966cc24fa8683a34afc6e-Paper.pdf},
 publisher = {MIT Press},
 title = {Support Vector Method for Function Approximation, Regression Estimation and Signal Processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/4f284803bd0966cc24fa8683a34afc6e-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_50905d7b,
 abstract = {Smoothing regularizers for radial basis functions have been studied extensively, but no general smoothing regularizers for projective basis junctions (PBFs), such as the widely-used sigmoidal PBFs, have heretofore been proposed. We derive new classes of algebraically-simple mth-order smoothing regularizers for networks of the form f(W, x) = Σi=1N Ujg [xT vj + Vjo] + uo, with general projective basis functions g[ċ]. These regularizers are: RG(W,m) = Σi=1N Uj2||vj||2m-1 Global Form RL(W,m) = Σi=1N uj2||vj||2m Local Form These regularizers bound the corresponding mth-order smoothing integral S(W,m) = ∫dD xω(x) ||∂m f(W,x)/∂xm||2, where W denotes all the network weights {uj, uo, vj, vo}, and Ω(x) is a weighting function on the D-dimensional input space. The global and local cases are distinguished by different choices of Ω(x).

The simple algebraic forms R(W, m) enable the direct enforcement of smoothness without the need for costly Monte-Carlo integrations of S(W, m). The new regularizers are shown to yield better generalization errors than weight decay when the implicit assumptions in the latter are wrong. Unlike weight decay, the new regularizers distinguish between the roles of the input and output weights and capture the interactions between them.},
 author = {Moody, John and R\"{o}gnvaldsson, Thorsteinn},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/50905d7b2216bfeccb5b41016357176b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/50905d7b2216bfeccb5b41016357176b-Metadata.json},
 openalex = {W2125416644},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf},
 publisher = {MIT Press},
 title = {Smoothing Regularizers for Projective Basis Function Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/50905d7b2216bfeccb5b41016357176b-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_52292e0c,
 abstract = {We propose a neuromorphic architecture for real-time processing of acoustic transients in analog VLSI. We show how judicious normalization of a time-frequency signal allows an elegant and robust implementation of a correlation algorithm. The algorithm uses binary multiplexing instead of analog-analog multiplication. This removes the need for analog storage and analog-multiplication. Simulations show that the resulting algorithm has the same out-of-sample classification performance (∼93% correct) as a baseline template-matching algorithm.},
 author = {Pineda, Fernando and Cauwenberghs, Gert and Edwards, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/52292e0c763fd027c6eba6b8f494d2eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/52292e0c763fd027c6eba6b8f494d2eb-Metadata.json},
 openalex = {W2117603471},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf},
 publisher = {MIT Press},
 title = {Bangs, Clicks, Snaps, Thuds and Whacks: An Architecture for Acoustic Transient Processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/52292e0c763fd027c6eba6b8f494d2eb-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_535ab766,
 abstract = {It has been suggested that long-range intrinsic connections in striate cortex may play a role in contour extraction (Gilbert et al., 1996). A number of recent physiological and psychophysical studies have examined the possible role of long range connections in the modulation of contrast detection thresholds (Polat and Sagi, 1993, 1994; Kapadia et al., 1995; Kovacs and Julesz, 1994) and various pre-attentive detection tasks (Kovacs and Julesz, 1993; Field et al., 1993). We have developed a network architecture based on the anatomical connectivity of striate cortex, as well as the temporal dynamics of neuronal processing, that is able to reproduce the observed experimental results. The network has been tested on real images and has applications in terms of identifying salient contours in automatic image processing systems.},
 author = {Yen, Shih-Cheng and Finkel, Leif},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/535ab76633d94208236a2e829ea6d888-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/535ab76633d94208236a2e829ea6d888-Metadata.json},
 openalex = {W2154140092},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/535ab76633d94208236a2e829ea6d888-Paper.pdf},
 publisher = {MIT Press},
 title = {Salient Contour Extraction by Temporal Binding in a Cortically-based Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/535ab76633d94208236a2e829ea6d888-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_5680522b,
 abstract = {Probability models can be used to predict outcomes and compensate for missing data, but even a perfect model cannot be used to make decisions unless the utility of the outcomes, or preferences between them, are also provided. This arises in many real-world problems, such as medical diagnosis, where the cost of the test as well as the expected improvement in the outcome must be considered. Relatively little work has been done on learning the utilities of outcomes for optimal decision making. In this paper, we show how temporal-difference reinforcement learning (TD(λ)) can be used to determine decision theoretic utilities within the context of a mixture model and apply this new approach to a problem in medical diagnosis. TD(λ) learning of utilities reduces the number of tests that have to be done to achieve the same level of performance compared with the probability model alone, which results in significant cost savings and increased efficiency.},
 author = {Stensmo, Magnus and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/5680522b8e2bb01943234bce7bf84534-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/5680522b8e2bb01943234bce7bf84534-Metadata.json},
 openalex = {W2165119135},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Decision Theoretic Utilities through Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_5caf41d6,
 abstract = {In this paper we propose a model for the lateral connectivity of orientation-selective cells in the visual cortex based on information-theoretic considerations. We study the properties of the input signal to the visual cortex and find new statistical structures which have not been processed in the retino-geniculate pathway. Applying the idea that the system optimizes the representation of incoming signals, we derive the lateral connectivity that will achieve this for a set of local orientation-selective patches, as well as the complete spatial structure of a layer of such patches. We compare the results with various physiological measurements.},
 author = {Dimitrov, Alexander and Cowan, Jack},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/5caf41d62364d5b41a893adc1a9dd5d4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/5caf41d62364d5b41a893adc1a9dd5d4-Metadata.json},
 openalex = {W2151465996},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/5caf41d62364d5b41a893adc1a9dd5d4-Paper.pdf},
 publisher = {MIT Press},
 title = {Spatial Decorrelation in Orientation Tuned Cortical Cells},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/5caf41d62364d5b41a893adc1a9dd5d4-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_5eac43ac,
 abstract = {This paper shows how the prices of option contracts traded in financial markets can be tracked sequentially by means of the Extended Kalman Filter algorithm. I consider call and put option pairs with identical strike price and time of maturity as a two output nonlinear system. The Black-Scholes approach popular in Finance literature and the Radial Basis Functions neural network are used in modelling the nonlinear system generating these observations. I show how both these systems may be identified recursively using the EKF algorithm. I present results of simulations on some FTSE 100 Index options data and discuss the implications of viewing the pricing problem in this sequential manner.},
 author = {Niranjan, Mahesan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/5eac43aceba42c8757b54003a58277b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/5eac43aceba42c8757b54003a58277b5-Metadata.json},
 openalex = {W2158352727},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/5eac43aceba42c8757b54003a58277b5-Paper.pdf},
 publisher = {MIT Press},
 title = {Sequential Tracking in Pricing Financial Options using Model Based and Neural Network Approaches},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/5eac43aceba42c8757b54003a58277b5-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_68a83eeb,
 abstract = {It is shown that conventional computers can be exponentially faster than planar Hopfield networks: although there are planar Hopfield networks that take exponential time to converge, a stable state of an arbitrary planar Hopfield network can be found by a conventional computer in polynomial time. The theory of PLS-completeness gives strong evidence that such a separation is unlikely for nonplanar Hopfield networks, and it is demonstrated that this is also the case for several restricted classes of nonplanar Hopfield networks, including those who interconnection graphs are the class of bipartite graphs, graphs of degree 3, the dual of the knight's graph, the 8-neighbor mesh, the hypercube, the butterfly, the cube-connected cycles, and the shuffle-exchange graph.},
 author = {Parberry, Ian and Tseng, Hung-Li},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/68a83eeb494a308fe5295da69428a507-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/68a83eeb494a308fe5295da69428a507-Metadata.json},
 openalex = {W2115241551},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/68a83eeb494a308fe5295da69428a507-Paper.pdf},
 publisher = {MIT Press},
 title = {Are Hopfield Networks Faster than Conventional Computers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/68a83eeb494a308fe5295da69428a507-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_68d13cf2,
 abstract = {Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator’s actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD’s performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.},
 author = {Schaal, Stefan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/68d13cf26c4b4f4f932e3eff990093ba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/68d13cf26c4b4f4f932e3eff990093ba-Metadata.json},
 openalex = {W2788862220},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/68d13cf26c4b4f4f932e3eff990093ba-Paper.pdf},
 publisher = {MIT Press},
 title = {Deep Q-learning From Demonstrations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/68d13cf26c4b4f4f932e3eff990093ba-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_6a61d423,
 abstract = {This paper discusses a probabilistic model-based approach to clustering sequences, using hidden Markov models (HMMs). The problem can be framed as a generalization of the standard mixture model approach to clustering in feature space. Two primary issues are addressed. First, a novel parameter initialization procedure is proposed, and second, the more difficult problem of determining the number of clusters K, from the data, is investigated. Experimental results indicate that the proposed techniques are useful for revealing hidden cluster structure in data sets of sequences.},
 author = {Smyth, Padhraic},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6a61d423d02a1c56250dc23ae7ff12f3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6a61d423d02a1c56250dc23ae7ff12f3-Metadata.json},
 openalex = {W2132875213},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf},
 publisher = {MIT Press},
 title = {Clustering Sequences with Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/6a61d423d02a1c56250dc23ae7ff12f3-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_6c14da10,
 abstract = {In supervised learning there is usually a clear distinction between inputs and outputs - inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead. This result is surprising since a feature used as an output is not used during testing.},
 author = {Caruana, Rich and de Sa, Virginia},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6c14da109e294d1e8155be8aa4b1ce8e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6c14da109e294d1e8155be8aa4b1ce8e-Metadata.json},
 openalex = {W2117482391},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf},
 publisher = {MIT Press},
 title = {Promoting Poor Features to Supervisors: Some Inputs Work Better as Outputs},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/6c14da109e294d1e8155be8aa4b1ce8e-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_6c8dba7d,
 abstract = {We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is intractable for exact calculations, thus we utilize variational approximations. We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly and the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence. We present simulation results for artificial data and the Bach chorales.},
 author = {Jordan, Michael and Ghahramani, Zoubin and Saul, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6c8dba7d0df1c4a79dd07646be9a26c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6c8dba7d0df1c4a79dd07646be9a26c8-Metadata.json},
 openalex = {W2145211911},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6c8dba7d0df1c4a79dd07646be9a26c8-Paper.pdf},
 publisher = {MIT Press},
 title = {Hidden Markov Decision Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/6c8dba7d0df1c4a79dd07646be9a26c8-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_6eb6e75f,
 abstract = {We compare the generalization performance of three distinct representation schemes for facial emotions using a single classification strategy (neural network). The face images presented to the classifiers are represented as: full face projections of the dataset onto their eigenvectors (eigenfaces); a similar projection constrained to eye and mouth areas (eigenfeatures); and finally a projection of the eye and mouth areas onto the eigenvectors obtained from 32×32 random image patches from the dataset. The latter system achieves 86% generalization on novel face images (individuals the networks were not trained on) drawn from a database in which human subjects consistently identify a single emotion for the face.},
 author = {Padgett, Curtis and Cottrell, Garrison},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6eb6e75fddec0218351dc5c0c8464104-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6eb6e75fddec0218351dc5c0c8464104-Metadata.json},
 openalex = {W2116836390},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/6eb6e75fddec0218351dc5c0c8464104-Paper.pdf},
 publisher = {MIT Press},
 title = {Representing Face Images for Emotion Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/6eb6e75fddec0218351dc5c0c8464104-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_70222949,
 abstract = {We seek to analyze and manipulate two factors, which we call style and content, underlying a set of observations. We fit training data with bilinear models which explicitly represent the two-factor structure. These models can adapt easily during testing to new styles or content, allowing us to solve three general tasks: extrapolation of a new style to unobserved content; classification of content observed in a new style; and translation of new content observed in a new style. For classification, we embed bilinear models in a probabilistic framework, Separable Mixture Models (SMMs), which generalizes earlier work on factorial mixture models [7, 3]. Significant performance improvement on a benchmark speech dataset shows the benefits of our approach.},
 author = {Tenenbaum, Joshua and Freeman, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/70222949cc0db89ab32c9969754d4758-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/70222949cc0db89ab32c9969754d4758-Metadata.json},
 openalex = {W2151750630},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/70222949cc0db89ab32c9969754d4758-Paper.pdf},
 publisher = {MIT Press},
 title = {Separating Style and Content},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/70222949cc0db89ab32c9969754d4758-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_71f6278d,
 abstract = {This paper describes how the early visual process of contour organisation can be realised using the EM algorithm. The underlying computational representation is based on fine spline coverings. According to our EM approach the adjustment of spline parameters draws on an iterative weighted least-squares fitting process. The expectation step of our EM procedure computes the likelihood of the data using a mixture model defined over the set of spline coverings. These splines are limited in their spatial extent using Gaussian windowing functions. The maximisation of the likelihood leads to a set of linear equations in the spline parameters which solve the weighted least squares problem. We evaluate the technique on the localisation of road structures in aerial infra-red images.},
 author = {Leite, Jos\'{e} and Hancock, Edwin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/71f6278d140af599e06ad9bf1ba03cb0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/71f6278d140af599e06ad9bf1ba03cb0-Metadata.json},
 openalex = {W2141760759},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf},
 publisher = {MIT Press},
 title = {Contour Organisation with the EM Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_7501e5d4,
 abstract = {When combining a set of learned models to form an improved estimator, the issue of redundancy or multicollinearity in the set of models must be addressed. A progression of existing approaches and their limitations with respect to the redundancy is discussed. A new approach, PCR*, based on principal components regression is proposed to address these limitations. An evaluation of the new approach on a collection of domains reveals that: 1) PCR* was the most robust combination method as the redundancy of the learned models increased, 2) redundancy could be handled without eliminating any of the learned models, and 3) the principal components of the learned models provided a continuum of regularized weights from which PCR* could choose.},
 author = {Merz, Christopher and Pazzani, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7501e5d4da87ac39d782741cd794002d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7501e5d4da87ac39d782741cd794002d-Metadata.json},
 openalex = {W2101693728},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7501e5d4da87ac39d782741cd794002d-Paper.pdf},
 publisher = {MIT Press},
 title = {Combining Neural Network Regression Estimates with Regularized Linear Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/7501e5d4da87ac39d782741cd794002d-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_76cf99d3,
 abstract = {When triangulating a belief network we aim to obtain a junction tree of minimum state space. According to (Rose, 1970), searching for the optimal triangulation can be cast as a search over all the permutations of the graph's vertices. Our approach is to embed the discrete set of permutations in a convex continuous domain D. By suitably extending the cost function over D and solving the continous nonlinear optimization task we hope to obtain a good triangulation with respect to the aformentioned cost. This paper presents two ways of embedding the triangulation problem into continuous domain and shows that they perform well compared to the best known heuristic.},
 author = {Meila, Marina and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/76cf99d3614e23eabab16fb27e944bf9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/76cf99d3614e23eabab16fb27e944bf9-Metadata.json},
 openalex = {W2160557065},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf},
 publisher = {MIT Press},
 title = {Triangulation by Continuous Embedding},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/76cf99d3614e23eabab16fb27e944bf9-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_7810ccd4,
 abstract = {The techniques of Bayesian inference have been applied with great success to many problems in neural computing including evaluation of regression functions, determination of error bars on predictions, and the treatment of hyper-parameters. However, the problem of model comparison is a much more challenging one for which current techniques have significant limitations. In this paper we show how an extended form of Markov chain Monte Carlo, called chaining, is able to provide effective estimates of the relative probabilities of different models. We present results from the robot arm problem and compare them with the corresponding results obtained using the standard Gaussian approximation framework.},
 author = {Barber, David and Bishop, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7810ccd41bf26faaa2c4e1f20db70a71-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7810ccd41bf26faaa2c4e1f20db70a71-Metadata.json},
 openalex = {W2096725679},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7810ccd41bf26faaa2c4e1f20db70a71-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Model Comparison by Monte Carlo Chaining},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/7810ccd41bf26faaa2c4e1f20db70a71-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_7940ab47,
 abstract = {We propose a new method to compute prediction intervals. Especially for small data sets the width of a prediction interval does not only depend on the variance of the target distribution, but also on the accuracy of our estimator of the mean of the target, i.e., on the width of the confidence interval. The confidence interval follows from the variation in an ensemble of neural networks, each of them trained and stopped on bootstrap replicates of the original data set. A second improvement is the use of the residuals on validation patterns instead of on training patterns for estimation of the variance of the target distribution. As illustrated on a synthetic example, our method is better than existing methods with regard to extrapolation and interpolation in data regimes with a limited amount of data, and yields prediction intervals which actual confidence levels are closer to the desired confidence levels.},
 author = {Heskes, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7940ab47468396569a906f75ff3f20ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7940ab47468396569a906f75ff3f20ef-Metadata.json},
 openalex = {W2115299933},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7940ab47468396569a906f75ff3f20ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Practical Confidence and Prediction Intervals},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/7940ab47468396569a906f75ff3f20ef-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_7bb06076,
 abstract = {A classifier is called consistent with respect to a given set of class-labeled points if it correctly classifies the set. We consider classifiers defined by unions of local separators and propose algorithms for consistent classifier reduction. The expected complexities of the proposed algorithms are derived along with the expected classifier sizes. In particular, the proposed approach yields a consistent reduction of the nearest neighbor classifier, which performs firm classification, assigning each new object to a class, regardless of the data structure. The proposed reduction method suggests a notion of soft classification, allowing for indecision with respect to objects which are insufficiently or ambiguously supported by the data. The performances of the proposed classifiers in predicting stock behavior are compared to that achieved by the nearest neighbor method.},
 author = {Baram, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7bb060764a818184ebb1cc0d43d382aa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7bb060764a818184ebb1cc0d43d382aa-Metadata.json},
 openalex = {W2145887795},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf},
 publisher = {MIT Press},
 title = {Consistent Classification, Firm and Soft},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/7bb060764a818184ebb1cc0d43d382aa-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_7bccfde7,
 abstract = {We present a connectionist method for representing images that explicitly addresses their hierarchical nature. It blends data from neuroscience about whole-object viewpoint sensitive cells in inferotemporal cortex and attentional basis-field modulation in V4 with ideas about hierarchical descriptions based on microfeatures. The resulting model makes critical use of bottom-up and top-down pathways for analysis and synthesis. We illustrate the model with a simple example of representing information about faces.},
 author = {Riesenhuber, Maximilian and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7bccfde7714a1ebadf06c5f4cea752c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7bccfde7714a1ebadf06c5f4cea752c1-Metadata.json},
 openalex = {W2161251272},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Models for Part-Whole Hierarchies},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/7bccfde7714a1ebadf06c5f4cea752c1-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_7c82fab8,
 abstract = {Multilayer architectures such as those used in Bayesian belief networks and Helmholtz machines provide a powerful framework for representing and learning higher order statistical relations among inputs. Because exact probability calculations with these models are often intractable, there is much interest in finding approximate algorithms. We present an algorithm that efficiently discovers higher order structure using EM and Gibbs sampling. The model can be interpreted as a stochastic recurrent network in which ambiguity in lower-level states is resolved through feedback from higher levels. We demonstrate the performance of the algorithm on benchmark problems.},
 author = {Lewicki, Michael and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7c82fab8c8f89124e2ce92984e04fb40-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7c82fab8c8f89124e2ce92984e04fb40-Metadata.json},
 openalex = {W2129759359},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7c82fab8c8f89124e2ce92984e04fb40-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Unsupervised Learning of Higher Order Structure},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/7c82fab8c8f89124e2ce92984e04fb40-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_7d771e0e,
 abstract = {A linear architectural model of cortical simple cells is presented. The model evidences how mutual inhibition, occurring through synaptic coupling functions asymmetrically distributed in space, can be a possible basis for a wide variety of spatio-temporal simple cell response properties, including direction selectivity and velocity tuning. While spatial asymmetries are included explicitly in the structure of the inhibitory interconnections, temporal asymmetries originate from the specific mutual inhibition scheme considered. Extensive simulations supporting the model are reported.},
 author = {Sabatini, Silvio and Solari, Fabio and Bisio, Giacomo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7d771e0e8f3633ab54856925ecdefc5d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7d771e0e8f3633ab54856925ecdefc5d-Metadata.json},
 openalex = {W2124822809},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7d771e0e8f3633ab54856925ecdefc5d-Paper.pdf},
 publisher = {MIT Press},
 title = {An Architectural Mechanism for Direction-tuned Cortical Simple Cells: The Role of Mutual Inhibition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/7d771e0e8f3633ab54856925ecdefc5d-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_7e7e69ea,
 abstract = {Biophysical modeling studies have previously shown that cortical pyramidal cells driven by strong NMDA-type synaptic currents and/or containing dendritic voltage-dependent Ca++ or Na+ channels, respond more strongly when synapses are activated in several spatially clustered groups of optimal size-in comparison to the same number of synapses activated diffusely about the dendritic arbor [8]. The nonlinear intradendritic interactions giving rise to this cluster sensitivity property are akin to a layer of virtual nonlinear hidden units in the dendrites, with implications for the cellular basis of learning and memory [7, 6], and for certain classes of nonlinear sensory processing [8]. In the present study, we show that a single neuron, with access only to excitatory inputs from unoriented ON- and OFF-center cells in the LGN, exhibits the principal nonlinear response properties of a cell in primary visual cortex, namely orientation tuning coupled with translation invariance and contrast insensitivity. We conjecture that this type of intradendritic processing could explain how complex cell responses can persist in the absence of oriented simple cell input [13].},
 author = {Mel, Bartlett and Ruderman, Daniel and Archie, Kevin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7e7e69ea3384874304911625ac34321c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7e7e69ea3384874304911625ac34321c-Metadata.json},
 openalex = {W2131303571},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7e7e69ea3384874304911625ac34321c-Paper.pdf},
 publisher = {MIT Press},
 title = {Complex-Cell Responses Derived from Center-Surround Inputs: The Surprising Power of Intradendritic Computation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/7e7e69ea3384874304911625ac34321c-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_7eb3c8be,
 abstract = {We present new algorithms for parameter estimation of HMMs. By adapting a framework used for supervised learning, we construct iterative algorithms that maximize the likelihood of the observations while also attempting to stay close to the current estimated parameters. We use a bound on the relative entropy between the two HMMs as a distance measure between them. The result is new iterative training algorithms which are similar to the EM (Baum-Welch) algorithm for training HMMs. The proposed algorithms are composed of a step similar to the expectation step of Baum-Welch and a new update of the parameters which replaces the maximization (re-estimation) step. The algorithm takes only negligibly more time per iteration and an approximated version uses the same expectation step as Baum-Welch. We evaluate experimentally the new algorithms on synthetic and natural speech pronunciation data. For sparse models, i.e. models with relatively small number of non-zero parameters, the proposed algorithms require significantly fewer iterations.},
 author = {Singer, Yoram and Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7eb3c8be3d411e8ebfab08eba5f49632-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7eb3c8be3d411e8ebfab08eba5f49632-Metadata.json},
 openalex = {W2164715647},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf},
 publisher = {MIT Press},
 title = {Training Algorithms for Hidden Markov Models using Entropy Based Distance Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/7eb3c8be3d411e8ebfab08eba5f49632-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_81dc9bdb,
 abstract = {To reduce the computational complexity of classification systems using tangent distance, Hastie et al. (HSS) developed an algorithm to devise rich models for representing large subsets of the data which computes automatically the best associated tangent subspace. Schwenk & Milgram proposed a discriminant modular classification system (Diabolo) based on several autoassociative multilayer perceptrons which use tangent distance as error reconstruction measure.

We propose a gradient based constructive learning algorithm for building a tangent subspace model with discriminant capabilities which combines several of the the advantages of both HSS and Diabolo: devised tangent models hold discriminant capabilities, space requirements are improved with respect to HSS since our algorithm is discriminant and thus it needs fewer prototype models, dimension of the tangent subspace is determined automatically by the constructive algorithm, and our algorithm is able to learn new transformations.},
 author = {Sona, Diego and Sperduti, Alessandro and Starita, Antonina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/81dc9bdb52d04dc20036dbd8313ed055-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/81dc9bdb52d04dc20036dbd8313ed055-Metadata.json},
 openalex = {W2141871293},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/81dc9bdb52d04dc20036dbd8313ed055-Paper.pdf},
 publisher = {MIT Press},
 title = {A Constructive Learning Algorithm for Discriminant Tangent Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/81dc9bdb52d04dc20036dbd8313ed055-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_81e5f81d,
 abstract = {We have combined an artificial neural network (ANN) character classifier with context-driven search over character segmentation, word segmentation, and word recognition hypotheses to provide robust recognition of hand-printed English text in new models of Apple Computer's Newton Message Pad. We present some innovations in the training and use of ANNs as character classifiers for word recognition, including normalized output error, frequency balancing, error emphasis, negative training, and stroke warping. A recurring theme of reducing a priori biases emerges and is discussed.},
 author = {Yaeger, Larry and Lyon, Richard and Webb, Brandyn},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/81e5f81db77c596492e6f1a5a792ed53-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/81e5f81db77c596492e6f1a5a792ed53-Metadata.json},
 openalex = {W2166469100},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/81e5f81db77c596492e6f1a5a792ed53-Paper.pdf},
 publisher = {MIT Press},
 title = {Effective Training of a Neural Network Character Classifier for Word Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/81e5f81db77c596492e6f1a5a792ed53-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_82c25591,
 abstract = {This article compares three penalty terms with respect to the efficiency of supervised learning, by using first- and second-order off-line learning algorithms and a first-order on-line algorithm. Our experiments showed that for a reasonably adequate penalty factor, the combination of the squared penalty term and the second-order learning algorithm drastically improves the convergence performance in comparison to the other combinations, at the same time bringing about excellent generalization performance. Moreover, in order to understand how differently each penalty term works, a function surface evaluation is described. Finally, we show how cross validation can be applied to find an optimal penalty factor.},
 author = {Saito, Kazumi and Nakano, Ryohei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/82c2559140b95ccda9c6ca4a8b981f1e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/82c2559140b95ccda9c6ca4a8b981f1e-Metadata.json},
 openalex = {W2138747680},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/82c2559140b95ccda9c6ca4a8b981f1e-Paper.pdf},
 publisher = {MIT Press},
 title = {Second-Order Learning Algorithm with Squared Penalty Term},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/82c2559140b95ccda9c6ca4a8b981f1e-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_838e8afb,
 abstract = {High frequency foreign exchange data can be decomposed into three components: the inventory effect component, the surprise information (news) component and the regular information component. The presence of the inventory effect and news can make analysis of trends due to the diffusion of information (regular information component) difficult.

We propose a neural-net-based, independent component analysis to separate high frequency foreign exchange data into these three components. Our empirical results show that our proposed multi-effect decomposition can reveal the intrinsic price behavior.},
 author = {Wu, Lizhong and Moody, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/838e8afb1ca34354ac209f53d90c3a43-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/838e8afb1ca34354ac209f53d90c3a43-Metadata.json},
 openalex = {W2130489569},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/838e8afb1ca34354ac209f53d90c3a43-Paper.pdf},
 publisher = {MIT Press},
 title = {Multi-effect Decompositions for Financial Data Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/838e8afb1ca34354ac209f53d90c3a43-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_83f97f48,
 abstract = {We consider the microscopic equations for learning problems in neural networks. The aligning fields of an example are obtained from the cavity fields, which are the fields if that example were absent in the learning process. In a rough energy landscape, we assume that the density of the local minima obey an exponential distribution, yielding macroscopic properties agreeing with the first step replica symmetry breaking solution. Iterating the microscopic equations provide a learning algorithm, which results in a higher stability than conventional algorithms.},
 author = {Wong, K. Y. Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/83f97f4825290be4cb794ec6a234595f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/83f97f4825290be4cb794ec6a234595f-Metadata.json},
 openalex = {W2124077701},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/83f97f4825290be4cb794ec6a234595f-Paper.pdf},
 publisher = {MIT Press},
 title = {Microscopic Equations in Rough Energy Landscape for Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/83f97f4825290be4cb794ec6a234595f-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_84438b7a,
 abstract = {Humans use visual as well as auditory speech signals to recognize spoken words. A variety of systems have been investigated for performing this task. The main purpose of this research was to systematically compare the performance of a range of dynamic visual features on a speechreading task. We have found that normalization of images to eliminate variation due to translation, scale, and planar rotation yielded substantial improvements in generalization performance regardless of the visual representation used. In addition, the dynamic information in the difference between successive frames yielded better performance than optical-flow based approaches, and compression by local low-pass filtering worked surprisingly better than global principal components analysis (PCA). These results are examined and possible explanations are explored.},
 author = {Gray, Michael and Movellan, Javier and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/84438b7aae55a0638073ef798e50b4ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/84438b7aae55a0638073ef798e50b4ef-Metadata.json},
 openalex = {W2118258383},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamic Features for Visual Speechreading: A Systematic Comparison},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/84438b7aae55a0638073ef798e50b4ef-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_884ce4bb,
 abstract = {The convergence properties of the gradient descent algorithm in the case of the linear perceptron may be obtained from the response function. We derive a general expression for the response function and apply it to the case of data with simple input correlations. It is found that correlations severely may slow down learning. This explains the success of PCA as a method for reducing training time. Motivated by this finding we furthermore propose to transform the input data by removing the mean across input variables as well as examples to decrease correlations. Numerical findings for a medical classification problem are in fine agreement with the theoretical results.},
 author = {Halkj\ae r, S\o ren and Winther, Ole},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/884ce4bb65d328ecb03c598409e2b168-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/884ce4bb65d328ecb03c598409e2b168-Metadata.json},
 openalex = {W2098688960},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/884ce4bb65d328ecb03c598409e2b168-Paper.pdf},
 publisher = {MIT Press},
 title = {The Effect of Correlated Input Data on the Dynamics of Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/884ce4bb65d328ecb03c598409e2b168-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_8e2cfdc2,
 abstract = {A biologically motivated model of cortical self-organization is proposed. Context is combined with bottom-up information via a maximum likelihood cost function. Clusters of one or more units are modulated by a common contextual gating Signal; they thereby organize themselves into mutually supportive predictors of abstract contextual features. The model was tested in its ability to discover viewpoint-invariant classes on a set of real image sequences of centered, gradually rotating faces. It performed considerably better than supervised back-propagation at generalizing to novel views from a small number of training examples.},
 author = {Becker, Suzanna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/8e2cfdc275761edc592f73a076197c33-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/8e2cfdc275761edc592f73a076197c33-Metadata.json},
 openalex = {W2127954488},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/8e2cfdc275761edc592f73a076197c33-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Temporally Persistent Hierarchical Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/8e2cfdc275761edc592f73a076197c33-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_93fb9d4b,
 abstract = {Model learning combined with dynamic programming has been shown to be effective for learning control of continuous state dynamic systems. The simplest method assumes the learned model is correct and applies dynamic programming to it, but many approximators provide uncertainty estimates on the fit. How can they be exploited? This paper addresses the case where the system must be prevented from having catastrophic failures during learning. We propose a new algorithm adapted from the dual control literature and use Bayesian locally weighted regression models with dynamic programming. A common reinforcement learning assumption is that aggressive exploration should be encouraged. This paper addresses the converse case in which the system has to reign in exploration. The algorithm is illustrated on a 4 dimensional simulated control problem.},
 author = {Schneider, Jeff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/93fb9d4b16aa750c7475b6d601c35c2c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/93fb9d4b16aa750c7475b6d601c35c2c-Metadata.json},
 openalex = {W2139769245},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/93fb9d4b16aa750c7475b6d601c35c2c-Paper.pdf},
 publisher = {MIT Press},
 title = {Exploiting Model Uncertainty Estimates for Safe Dynamic Control Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/93fb9d4b16aa750c7475b6d601c35c2c-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_944bdd96,
 abstract = {The mortality related to cervical cancer can be substantially reduced through early detection and treatment. However, current detection techniques, such as Pap smear and colposcopy, fail to achieve a concurrently high sensitivity and specificity. In vivo fluorescence spectroscopy is a technique which quickly, noninvasively and quantitatively probes the biochemical and morphological changes that occur in pre-cancerous tissue. RBF ensemble algorithms based on such spectra provide automated, and near realtime implementation of pre-cancer detection in the hands of nonexperts. The results are more reliable, direct and accurate than those achieved by either human experts or multivariate statistical algorithms.},
 author = {Tumer, Kagan and Ramanujam, Nirmala and Richards-Kortum, Rebecca and Ghosh, Joydeep},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/944bdd9636749a0801c39b6e449dbedc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/944bdd9636749a0801c39b6e449dbedc-Metadata.json},
 openalex = {W2118353561},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/944bdd9636749a0801c39b6e449dbedc-Paper.pdf},
 publisher = {MIT Press},
 title = {Spectroscopic Detection of Cervical Pre-Cancer through Radial Basis Function Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/944bdd9636749a0801c39b6e449dbedc-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_97af4fb3,
 abstract = {We apply the method of complexity regularization to derive estimation bounds for nonlinear function estimation using a single hidden layer radial basis function network. Our approach differs from previous complexity regularization neural-network function learning schemes in that we operate with random covering numbers and l/sub 1/ metric entropy, making it possible to consider much broader families of activation functions, namely functions of bounded variation. Some constraints previously imposed on the network parameters are also eliminated this way. The network is trained by means of complexity regularization involving empirical risk minimization. Bounds on the expected risk in terms of the sample size are obtained for a large class of loss functions. Rates of convergence to the optimal loss are also derived.},
 author = {Krzyzak, Adam and Linder, Tam\'{a}s},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/97af4fb322bb5c8973ade16764156bed-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/97af4fb322bb5c8973ade16764156bed-Metadata.json},
 openalex = {W2122515683},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/97af4fb322bb5c8973ade16764156bed-Paper.pdf},
 publisher = {MIT Press},
 title = {Radial basis function networks and complexity regularization in function learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/97af4fb322bb5c8973ade16764156bed-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_99566564,
 abstract = {We propose a novel approach to automatically growing and pruning Hierarchical Mixtures of Experts. The constructive algorithm proposed here enables large hierarchies consisting of several hundred experts to be trained effectively. We show that HME's trained by our automatic growing procedure yield better generalization performance than traditional static and balanced hierarchies. Evaluation of the algorithm is performed (1) on vowel classification and (2) within a hybrid version of the JANUS [9] speech recognition system using a subset of the Switchboard large-vocabulary speaker-independent continuous speech recognition database.},
 author = {Fritsch, J\"{u}rgen and Finke, Michael and Waibel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/995665640dc319973d3173a74a03860c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/995665640dc319973d3173a74a03860c-Metadata.json},
 openalex = {W2097138116},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/995665640dc319973d3173a74a03860c-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptively Growing Hierarchical Mixtures of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/995665640dc319973d3173a74a03860c-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_996009f2,
 abstract = {We present a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. In the Monte-Carlo simulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. The action maximizing the measured expected reward is then taken, resulting in an improved policy. Our algorithm is easily parallelizable and has been implemented on the IBM SP1 and SP2 parallel-RISC supercomputers.

We have obtained promising initial results in applying this algorithm to the domain of backgammon. Results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network. In each case, the Monte-Carlo algorithm gives a substantial reduction, by as much as a factor of 5 or more, in the error rate of the base players. The algorithm is also potentially useful in many other adaptive control applications in which it is possible to simulate the environment.},
 author = {Tesauro, Gerald and Galperin, Gregory},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/996009f2374006606f4c0b0fda878af1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/996009f2374006606f4c0b0fda878af1-Metadata.json},
 openalex = {W2135997697},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/996009f2374006606f4c0b0fda878af1-Paper.pdf},
 publisher = {MIT Press},
 title = {On-line Policy Improvement using Monte-Carlo Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/996009f2374006606f4c0b0fda878af1-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_9996535e,
 abstract = {We present a new method to tackle the problem of separating mixtures of real sources which have been convolved and time-delayed under real world conditions. To this end, we learn two sets of parameters to unmix the mixtures and to estimate the true density function. The solutions are discussed for feedback and feedforward architectures. Since the quality of separation depends on the modeling of the underlying density we propose different methods to closer approximate the density function using some contest. The proposed density estimation achieves separation of a wider class of sources. Furthermore, we employ the FIR polynomial matrix techniques in the frequency domain to invert a true-phase mixing system. The significance of the new method is demonstrated with the successful separation of two speakers and separation of music and speech recorded with two microphones in a reverberating room.},
 author = {Lee, Te-Won and Bell, Anthony and Lambert, Russell},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/9996535e07258a7bbfd8b132435c5962-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/9996535e07258a7bbfd8b132435c5962-Metadata.json},
 openalex = {W2121054335},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/9996535e07258a7bbfd8b132435c5962-Paper.pdf},
 publisher = {MIT Press},
 title = {A contextual blind separation of delayed and convolved sources},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/9996535e07258a7bbfd8b132435c5962-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_9a3d4583,
 abstract = {This paper presents a new approach to speech recognition with hybrid HMM/ANN technology. While the standard approach to hybrid HMM/ANN systems is based on the use of neural networks as posterior probability estimators, the new approach is based on the use of mutual information neural networks trained with a special learning algorithm in order to maximize the mutual information between the input classes of the network and its resulting sequence of firing output neurons during training. It is shown in this paper that such a neural network is an optimal neural vector quantizer for a discrete hidden Markov model system trained on Maximum Likelihood principles. One of the main advantages of this approach is the fact, that such neural networks can be easily combined with HMM's of any complexity with context-dependent capabilities. It is shown that the resulting hybrid system achieves very high recognition rates, which are now already on the same level as the best conventional HMM systems with continuous parameters, and the capabilities of the mutual information neural networks are not yet entirely exploited.},
 author = {Rigoll, Gerhard and Neukirchen, Christoph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/9a3d458322d70046f63dfd8b0153ece4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/9a3d458322d70046f63dfd8b0153ece4-Metadata.json},
 openalex = {W2146915677},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/9a3d458322d70046f63dfd8b0153ece4-Paper.pdf},
 publisher = {MIT Press},
 title = {A New Approach to Hybrid HMM/ANN Speech Recognition using Mutual Information Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/9a3d458322d70046f63dfd8b0153ece4-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_9adeb82f,
 abstract = {The separation of generalization error into two types, bias and variance (Geman, Bienenstock, Doursat, 1992), leads to the notion of error reduction by averaging over a of classifiers (Perrone, 1993). Committee performance decreases with both the average error of the constituent classifiers and increases with the degree to which the misclassifications are correlated across the committee. Here, a method for reducing correlations is introduced, that uses a winner-take-all procedure similar to competitive learning to drive the individual networks to different minima in weight space with respect to the training set, such that correlations in generalization performance will be reduced, thereby reducing committee error.},
 author = {Munro, Paul and Parmanto, Bambang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Metadata.json},
 openalex = {W2168040522},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Paper.pdf},
 publisher = {MIT Press},
 title = {Competition Among Networks Improves Committee Performance},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/9adeb82fffb5444e81fa0ce8ad8afe7a-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a0161022,
 abstract = {Local disparity information is often sparse and noisy, which creates two conflicting demands when estimating disparity in an image region: the need to spatially average to get an accurate estimate, and the problem of not averaging over discontinuities. We have developed a network model of disparity estimation based on disparity-selective neurons, such as those found in the early stages of processing in visual cortex. The model can accurately estimate multiple disparities in a region, which may be caused by transparency or occlusion, in real images and random-dot stereograms. The use of a selection mechanism to selectively integrate reliable local disparity estimates results in superior performance compared to standard back-propagation and cross-correlation approaches. In addition, the representations learned with this selection mechanism are consistent with recent neurophysiological results of von der Heydt, Zhou, Friedman, and Poggio [8] for cells in cortical visual area V2. Combining multi-scale biologically-plausible image processing with the power of the mixture-of-experts learning algorithm represents a promising approach that yields both high performance and new insights into visual system function.},
 author = {Gray, Michael and Pouget, Alexandre and Zemel, Richard and Nowlan, Steven and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a01610228fe998f515a72dd730294d87-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a01610228fe998f515a72dd730294d87-Metadata.json},
 openalex = {W2127525521},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a01610228fe998f515a72dd730294d87-Paper.pdf},
 publisher = {MIT Press},
 title = {Selective Integration: A Model for Disparity Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a01610228fe998f515a72dd730294d87-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a0833c8a,
 abstract = {The Neurothermostat is an adaptive controller that regulates indoor air temperature in a residence by switching a furnace on or off. The task is framed as an optimal control problem in which both comfort and energy costs are considered as part of the control objective. Because the consequences of control decisions are delayed in time, the Neurothermostat must anticipate heating demands with predictive models of occupancy patterns and the thermal response of the house and furnace. Occupancy pattern prediction is achieved by a hybrid neural net/look-up table. The Neurothermostat searches, at each discrete time step, for a decision sequence that minimizes the expected cost over a fixed planning horizon. The first decision in this sequence is taken, and this process repeats. Simulations of the Neurothermostat were conducted using artificial occupancy data in which regularity was systematically varied, as well as occupancy data from an actual residence. The Neurothermostat is compared against three conventional policies, and achieves reliably lower costs. This result is robust to the relative weighting of comfort and energy costs and the degree of variability in the occupancy patterns.},
 author = {Mozer, Michael C and Vidmar, Lucky and Dodier, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a0833c8a1817526ac555f8d67727caf6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a0833c8a1817526ac555f8d67727caf6-Metadata.json},
 openalex = {W2147051918},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a0833c8a1817526ac555f8d67727caf6-Paper.pdf},
 publisher = {MIT Press},
 title = {The Neurothermostat: Predictive Optimal Control of Residential Heating Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a0833c8a1817526ac555f8d67727caf6-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a0872cc5,
 abstract = {This paper describes a new framework for relational graph matching. The starting point is a recently reported Bayesian consistency measure which gauges structural differences using Hamming distance. The main contributions of the work are threefold. Firstly, we demonstrate how the discrete components of the cost function can be softened. The second contribution is to show how the softened cost function can be used to locate matches using continuous non-linear optimisation. Finally, we show how the resulting graph matching algorithm relates to the standard quadratic assignment problem.},
 author = {Finch, Andrew and Wilson, Richard and Hancock, Edwin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a0872cc5b5ca4cc25076f3d868e1bdf8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a0872cc5b5ca4cc25076f3d868e1bdf8-Metadata.json},
 openalex = {W2151321874},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a0872cc5b5ca4cc25076f3d868e1bdf8-Paper.pdf},
 publisher = {MIT Press},
 title = {Softening Discrete Relaxation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a0872cc5b5ca4cc25076f3d868e1bdf8-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a284df11,
 abstract = {Epidemiological data is traditionally analyzed with very simple techniques. Flexible models, such as neural networks, have the potential to discover unanticipated features in the data. However, to be useful, flexible models must have effective control on overfitting. This paper reports on a comparative study of the predictive quality of neural networks and other flexible models applied to real and artificial epidemiological data. The results suggest that there are no major unanticipated complex features in the real data, and also demonstrate that MacKay's [1995] Bayesian neural network methodology provides effective control on overfitting while retaining the ability to discover complex features in the artificial data.},
 author = {Plate, Tony and Band, Pierre and Bert, Joel and Grace, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a284df1155ec3e67286080500df36a9a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a284df1155ec3e67286080500df36a9a-Metadata.json},
 openalex = {W2144380031},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a284df1155ec3e67286080500df36a9a-Paper.pdf},
 publisher = {MIT Press},
 title = {A Comparison between Neural Networks and other Statistical Techniques for Modeling the Relationship between Tobacco and Alcohol and Cancer},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a284df1155ec3e67286080500df36a9a-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a42a596f,
 abstract = {We study a mistake-driven variant of an on-line Bayesian learning algorithm (similar to one studied by Cesa-Bianchi, Helmbold, and Panizza [CHP96]). This variant only updates its state (learns) on trials in which it makes a mistake. The algorithm makes binary classifications using a linear-threshold classifier and runs in time linear in the number of attributes seen by the learner. We have been able to show, theoretically and in simulations, that this algorithm performs well under assumptions quite different from those embodied in the prior of the original Bayesian algorithm. It can handle situations that we do not know how to handle in linear time with Bayesian algorithms. We expect our techniques to be useful in deriving and analyzing other apobayesian algorithms.},
 author = {Littlestone, Nick and Mesterharm, Chris},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a42a596fc71e17828440030074d15e74-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a42a596fc71e17828440030074d15e74-Metadata.json},
 openalex = {W2162953025},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a42a596fc71e17828440030074d15e74-Paper.pdf},
 publisher = {MIT Press},
 title = {An Apobayesian Relative of Winnow},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a42a596fc71e17828440030074d15e74-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a4d2f0d2,
 abstract = {Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of.},
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a4d2f0d23dcc84ce983ff9157f8b7f88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a4d2f0d23dcc84ce983ff9157f8b7f88-Metadata.json},
 openalex = {W2136939460},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf},
 publisher = {MIT Press},
 title = {LSTM can Solve Hard Long Time Lag Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a4d2f0d23dcc84ce983ff9157f8b7f88-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a51fb975,
 abstract = {A globally convergent homotopy method is defined that is capable of sequentially producing large numbers of stationary points of the multi-layer perceptron mean-squared error surface. Using this algorithm large subsets of the stationary points of two test problems are found. It is shown empirically that the MLP neural network appears to have an extreme ratio of saddle points compared to local minima, and that even small neural network problems have extremely large numbers of solutions.},
 author = {Coetzee, Frans and Stonick, Virginia},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a51fb975227d6640e4fe47854476d133-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a51fb975227d6640e4fe47854476d133-Metadata.json},
 openalex = {W2152703617},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a51fb975227d6640e4fe47854476d133-Paper.pdf},
 publisher = {MIT Press},
 title = {488 Solutions to the XOR Problem},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a51fb975227d6640e4fe47854476d133-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a58149d3,
 abstract = {We address statistical classifier design given a mixed training set consisting of a small labelled feature set and a (generally larger) set of unlabelled features. This situation arises, e.g., for medical images, where although training features may be plentiful, expensive expertise is required to extract their class labels. We propose a classifier structure and learning algorithm that make effective use of unlabelled data to improve performance. The learning is based on maximization of the total data likelihood, i.e. over both the labelled and unlabelled data subsets. Two distinct EM learning algorithms are proposed, differing in the EM formalism applied for unlabelled data. The classifier, based on a joint probability model for features and labels, is a mixture of experts structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training. The scope of application for the new method is greatly extended by the observation that test data, or any new data to classify, is in fact additional, unlabelled data - thus, a combined learning/classification operation - much akin to what is done in image segmentation - can be invoked whenever there is new data to classify. Experiments with data sets from the UC Irvine database demonstrate that the new learning algorithms and structure achieve substantial performance gains over alternative approaches.},
 author = {Miller, David J and Uyar, Hasan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a58149d355f02887dfbe55ebb2b64ba3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a58149d355f02887dfbe55ebb2b64ba3-Metadata.json},
 openalex = {W2137054688},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a58149d355f02887dfbe55ebb2b64ba3-Paper.pdf},
 publisher = {MIT Press},
 title = {A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a58149d355f02887dfbe55ebb2b64ba3-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a7d8ae45,
 abstract = {We study generalization capability of the mixture of experts learning from examples generated by another network with the same architecture. When the number of examples is smaller than a critical value, the network shows a symmetric phase where the role of the experts is not specialized. Upon crossing the critical point, the system undergoes a continuous phase transition to a symmetry breaking phase where the gating network partitions the input space effectively and each expert is assigned to an appropriate subspace. We also find that the mixture of experts with multiple level of hierarchy shows multiple phase transitions.},
 author = {Kang, Kukjin and Oh, Jong-Hoon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a7d8ae4569120b5bec12e7b6e9648b86-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a7d8ae4569120b5bec12e7b6e9648b86-Metadata.json},
 openalex = {W2146067011},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a7d8ae4569120b5bec12e7b6e9648b86-Paper.pdf},
 publisher = {MIT Press},
 title = {Statistical Mechanics of the Mixture of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_a9078e86,
 abstract = {Predictions of lifetimes of dynamically allocated objects can be used to improve time and space efficiency of dynamic memory management in computer programs. Barrett and Zorn [1993] used a simple lifetime predictor and demonstrated this improvement on a variety of computer programs. In this paper, we use decision trees to do lifetime prediction on the same programs and show significantly better prediction. Our method also has the advantage that during training we can use a large number of features and let the decision tree automatically choose the relevant subset.},
 author = {Cohn, David and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a9078e8653368c9c291ae2f8b74012e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a9078e8653368c9c291ae2f8b74012e7-Metadata.json},
 openalex = {W2168396813},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/a9078e8653368c9c291ae2f8b74012e7-Paper.pdf},
 publisher = {MIT Press},
 title = {Predicting Lifetimes in Dynamically Allocated Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/a9078e8653368c9c291ae2f8b74012e7-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_ab1a4d0d,
 abstract = {Closed-loop control relies on sensory feedback that is usually assumed to be free. But if sensing incurs a cost, it may be cost-effective to take sequences of actions in open-loop mode. We describe a reinforcement learning algorithm that learns to combine open-loop and closed-loop control when sensing incurs a cost. Although we assume reliable sensors, use of open-loop control means that actions must sometimes be taken when the current state of the controlled system is uncertain. This is a special case of the hidden-state problem in reinforcement learning, and to cope, our algorithm relies on short-term memory. The main result of the paper is a rule that significantly limits exploration of possible memory states by pruning memory states for which the estimated value of information is greater than its cost. We prove that this rule allows convergence to an optimal policy.},
 author = {Hansen, Eric and Barto, Andrew and Zilberstein, Shlomo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/ab1a4d0dd4d48a2ba1077c4494791306-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/ab1a4d0dd4d48a2ba1077c4494791306-Metadata.json},
 openalex = {W2162137442},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/ab1a4d0dd4d48a2ba1077c4494791306-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning for Mixed Open-loop and Closed-loop Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/ab1a4d0dd4d48a2ba1077c4494791306-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_ae5e3ce4,
 abstract = {For neural networks with a wide class of weight-priors, it can be shown that in the limit of an infinite number of hidden units the prior over functions tends to a Gaussian process. In this paper analytic forms are derived for the covariance function of the Gaussian processes corresponding to networks with sigmoidal and Gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units, and shows that, somewhat paradoxically, it may be easier to compute with infinite networks than finite ones.},
 author = {Williams, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/ae5e3ce40e0404a45ecacaaf05e5f735-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/ae5e3ce40e0404a45ecacaaf05e5f735-Metadata.json},
 openalex = {W2154087390},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf},
 publisher = {MIT Press},
 title = {Computing with Infinite Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_b20bb95a,
 abstract = {In most treatments of the regression problem it is assumed that the distribution of target data can be described by a deterministic function of the inputs, together with additive Gaussian noise having constant variance. The use of maximum likelihood to train such models then corresponds to the minimization of a sum-of-squares error function. In many applications a more realistic model would allow the noise variance itself to depend on the input variables. However, the use of maximum likelihood to train such models would give highly biased results. In this paper we show how a Bayesian treatment can allow for an input-dependent variance while overcoming the bias of maximum likelihood.},
 author = {Bishop, Christopher and Quazaz, Cazhaow},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b20bb95ab626d93fd976af958fbc61ba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b20bb95ab626d93fd976af958fbc61ba-Metadata.json},
 openalex = {W2099954285},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf},
 publisher = {MIT Press},
 title = {Regression with Input-Dependent Noise: A Bayesian Treatment},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/b20bb95ab626d93fd976af958fbc61ba-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_b24d516b,
 abstract = {Given unlimited computational resources, it is best to use a criterion of minimal expected generalisation error to select a model and determine its parameters. However, it may be worthwhile to sacrifice some generalisation performance for higher learning speed. A method for quantifying sub-optimality is set out here, so that this choice can be made intelligently. Furthermore, the method is applicable to a broad class of models, including the ultra-fast memory-based methods such as RAMnets. This brings the added benefit of providing, for the first time, the means to analyse the generalisation properties of such models in a Bayesian framework.},
 author = {Rohwer, Richard and Morciniec, Michal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b24d516bb65a5a58079f0f3526c87c57-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b24d516bb65a5a58079f0f3526c87c57-Metadata.json},
 openalex = {W2155242866},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b24d516bb65a5a58079f0f3526c87c57-Paper.pdf},
 publisher = {MIT Press},
 title = {The Generalisation Cost of RAMnets},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/b24d516bb65a5a58079f0f3526c87c57-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_b3ba8f1b,
 abstract = {We study the number of hidden layers required by a multilayer neural network with threshold units to compute a function f from Rd to {0, 1}. In dimension d = 2, Gibson characterized the functions computable with just one hidden layer, under the assumption that there is no intersection and that f is only defined on a compact set. We consider the restriction of f to the neighborhood of a multiple intersection point or of infinity, and give necessary and sufficient conditions for it to be locally computable with one hidden layer. We show that adding these conditions to Gibson's assumptions is not sufficient to ensure global computability with one hidden layer, by exhibiting a new non-local configuration, the critical cycle, which implies that f is not computable with one hidden layer.},
 author = {Brightwell, Graham and Kenyon, Claire and Paugam-Moisy, H\'{e}l\`{e}ne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b3ba8f1bee1238a2f37603d90b58898d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b3ba8f1bee1238a2f37603d90b58898d-Metadata.json},
 openalex = {W2147491311},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf},
 publisher = {MIT Press},
 title = {Multilayer Neural Networks: One or Two Hidden Layers?},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/b3ba8f1bee1238a2f37603d90b58898d-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_b495ce63,
 abstract = {Support Vector Learning Machines (SVM) are finding application in pattern recognition, regression estimation, and operator inversion for ill-posed problems. Against this very general backdrop, any methods for improving the generalization performance, or for improving the speed in test phase, of SVMs are of increasing interest. In this paper we combine two such techniques on a pattern recognition problem. The method for improving generalization performance (the support method) does so by incorporating known invariances of the problem. This method achieves a drop in the error rate on 10,000 NIST test digit images of 1.4% to 1.0%. The method for improving the speed (the method) does so by approximating the support vector decision surface. We apply this method to achieve a factor of fifty speedup in test phase over the virtual support vector machine. The combined approach yields a machine which is both 22 times faster than the original machine, and which has better generalization performance, achieving 1.1 % error. The virtual support vector method is applicable to any SVM problem with known invariances. The reduced set method is applicable to any support vector machine.},
 author = {Burges, Christopher J. C. and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b495ce63ede0f4efc9eec62cb947c162-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b495ce63ede0f4efc9eec62cb947c162-Metadata.json},
 openalex = {W2104867159},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf},
 publisher = {MIT Press},
 title = {Improving the Accuracy and Speed of Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/b495ce63ede0f4efc9eec62cb947c162-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_b51a15f3,
 abstract = {This paper presents a method that decides which combinations of traffic can be accepted on a packet data link, so that quality of service (QoS) constraints can be met. The method uses samples of QoS results at different load conditions to build a neural network decision function. Previous similar approaches to the problem have a significant bias. This bias is likely to occur in any real system and results in accepting loads that miss QoS targets by orders of magnitude. Preprocessing the data to either remove the bias or provide a confidence level, the method was applied to sources based on difficult-to-analyze ethernet data traces. With this data, the method produces an accurate access control function that dramatically outperforms analytic alternatives. Interestingly, the results depend on throwing away more than 99% of the data.},
 author = {Brown, Timothy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b51a15f382ac914391a58850ab343b00-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b51a15f382ac914391a58850ab343b00-Metadata.json},
 openalex = {W2102288158},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b51a15f382ac914391a58850ab343b00-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive Access Control Applied to Ethernet Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/b51a15f382ac914391a58850ab343b00-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_b571ecea,
 abstract = {We have designed, fabricated, and tested an adaptive Winner-Take-All (WTA) circuit based upon the classic WTA of Lazzaro, et al [1]. We have added a time dimension (adaptation) to this circuit to make the input derivative an important factor in winner selection. To accomplish this, we have modified the classic WTA circuit by adding floating gate transistors which slowly null their inputs over time. We present a simplified analysis and experimental data of this adaptive WTA fabricated in a standard CMOS 2µm process.},
 author = {Kruger, W. and Hasler, Paul and Minch, Bradley and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b571ecea16a9824023ee1af16897a582-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b571ecea16a9824023ee1af16897a582-Metadata.json},
 openalex = {W2105630422},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/b571ecea16a9824023ee1af16897a582-Paper.pdf},
 publisher = {MIT Press},
 title = {An Adaptive WTA using Floating Gate Technology},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/b571ecea16a9824023ee1af16897a582-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_bb04af0f,
 abstract = {This work investigates the representational and inductive capabilities of time-delay neural networks (TDNNs) in general, and of two subclasses of TDNN, those with delays only on the inputs (IDNN), and those which include delays on hidden units (HDNN). Both architectures are capable of representing the same class of languages, the definite memory machine (DMM) languages, but the delays on the hidden units in the HDNN helps it outperform the IDNN on problems composed of repeated features over short time windows.},
 author = {Clouse, Daniel and Giles, C. and Horne, Bill and Cottrell, Garrison},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/bb04af0f7ecaee4aae62035497da1387-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/bb04af0f7ecaee4aae62035497da1387-Metadata.json},
 openalex = {W2120358166},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/bb04af0f7ecaee4aae62035497da1387-Paper.pdf},
 publisher = {MIT Press},
 title = {Representation and Induction of Finite State Machines using Time-Delay Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/bb04af0f7ecaee4aae62035497da1387-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_bb7946e7,
 abstract = {We present a general encoding-decoding framework for interpreting the activity of a population of units. A standard population code interpretation method, the Poisson model, starts from a description as to how a single value of an underlying quantity can generate the activities of each unit in the population. In casting it in the encoding-decoding framework, we find that this model is too restrictive to describe fully the activities of units in population codes in higher processing areas, such as the medial temporal area. Under a more powerful model, the population activity can convey information not only about a single value of some quantity but also about its whole distribution, including its variance, and perhaps even the certainty the system has in the actual presence in the world of the entity generating this quantity. We propose a novel method for forming such probabilistic interpretations of population codes and compare it to the existing method.},
 author = {Zemel, Richard and Dayan, Peter and Pouget, Alexandre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/bb7946e7d85c81a9e69fee1cea4a087c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/bb7946e7d85c81a9e69fee1cea4a087c-Metadata.json},
 openalex = {W1985162039},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Interpretation of Population Codes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/bb7946e7d85c81a9e69fee1cea4a087c-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_c0a271bc,
 abstract = {A one-dimensional visual tracking chip has been implemented using neuromorphic, analog VLSI techniques to model selective visual attention in the control of saccadic and smooth pursuit eye movements. The chip incorporates focal-plane processing to compute image saliency and a winner-take-all circuit to select a feature for tracking. The target position and direction of motion are reported as the target moves across the array. We demonstrate its functionality in a closed-loop system which performs saccadic and smooth pursuit tracking movements using a one-dimensional mechanical eye.},
 author = {Horiuchi, Timothy and Morris, Tonia and Koch, Christof and DeWeerth, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c0a271bc0ecb776a094786474322cb82-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c0a271bc0ecb776a094786474322cb82-Metadata.json},
 openalex = {W2149010506},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c0a271bc0ecb776a094786474322cb82-Paper.pdf},
 publisher = {MIT Press},
 title = {Analog VLSI Circuits for Attention-Based, Visual Tracking},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/c0a271bc0ecb776a094786474322cb82-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_c1e39d91,
 abstract = {We analyse online learning from finite training sets at non-infinitesimal learning rates η. By an extension of statistical mechanics methods, we obtain exact results for the time-dependent generalization error of a linear network with a large number of weights N. We find, for example, that for small training sets of size p ≈ N, larger learning rates can be used without compromising asymptotic generalization performance or convergence speed. Encouragingly, for optimal settings of η (and, less importantly, weight decay λ) at given final learning time, the generalization performance of online learning is essentially as good as that of offline learning.},
 author = {Sollich, Peter and Barber, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c1e39d912d21c91dce811d6da9929ae8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c1e39d912d21c91dce811d6da9929ae8-Metadata.json},
 openalex = {W2109428726},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c1e39d912d21c91dce811d6da9929ae8-Paper.pdf},
 publisher = {MIT Press},
 title = {Online Learning from Finite Training Sets: An Analytical Case Study},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/c1e39d912d21c91dce811d6da9929ae8-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_c44e5038,
 abstract = {We study the spatiotemporal correlation in natural time-varying images and explore the hypothesis that the visual system is concerned with the optimal coding of visual representation through spatiotemporal decorrelation of the input signal. Based on the measured spatiotemporal power spectrum, the transform needed to decorrelate input signal is derived analytically and then compared with the actual processing observed in psychophysical experiments.},
 author = {Dong, Dawei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c44e503833b64e9f27197a484f4257c0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c44e503833b64e9f27197a484f4257c0-Metadata.json},
 openalex = {W2123734708},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c44e503833b64e9f27197a484f4257c0-Paper.pdf},
 publisher = {MIT Press},
 title = {Spatiotemporal Coupling and Scaling of Natural Images and Human Visual Sensitivities},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/c44e503833b64e9f27197a484f4257c0-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_c4851e8e,
 abstract = {We present an algorithm for fast stochastic gradient descent that uses a nonlinear adaptive momentum scheme to optimize the late time convergence rate. The algorithm makes effective use of curvature information, requires only O(n) storage and computation, and delivers convergence rates close to the theoretical optimum. We demonstrate the technique on linear and large nonlinear backprop networks.},
 author = {Orr, Genevieve and Leen, Todd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c4851e8e264415c4094e4e85b0baa7cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c4851e8e264415c4094e4e85b0baa7cc-Metadata.json},
 openalex = {W2123462950},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c4851e8e264415c4094e4e85b0baa7cc-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Curvature Information for Fast Stochastic Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/c4851e8e264415c4094e4e85b0baa7cc-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_c54e7837,
 abstract = {We introduce arc-lh, a new algorithm for improvement of ANN classifier performance, which measures the importance of patterns by aggregated network output errors. On several artificial benchmark problems, this algorithm compares favorably with other resample and combine techniques.},
 author = {Leisch, Friedrich and Hornik, Kurt},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c54e7837e0cd0ced286cb5995327d1ab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c54e7837e0cd0ced286cb5995327d1ab-Metadata.json},
 openalex = {W2170407887},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c54e7837e0cd0ced286cb5995327d1ab-Paper.pdf},
 publisher = {MIT Press},
 title = {ARC-LH: A New Adaptive Resampling Algorithm for Improving ANN Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/c54e7837e0cd0ced286cb5995327d1ab-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_c70daf24,
 abstract = {We describe the notion of and suggest that this provides a framework for comparing different classes of regression models, including neural networks and both parametric and non-parametric statistical techniques. Unfortunately, standard techniques break down when faced with models, such as neural networks, in which there is more than one layer of adjustable parameters. We propose an algorithm which overcomes this limitation, estimating the equivalent kernels for neural network models using a data perturbation approach. Experimental results indicate that the networks do not use the maximum possible number of degrees of freedom, that these can be controlled using regularisation techniques and that the equivalent kernels learnt by the network vary both in size and in shape in different regions of the input space.},
 author = {Burgess, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c70daf247944fe3add32218f914c75a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c70daf247944fe3add32218f914c75a6-Metadata.json},
 openalex = {W2118527271},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c70daf247944fe3add32218f914c75a6-Paper.pdf},
 publisher = {MIT Press},
 title = {Estimating Equivalent Kernels for Neural Networks: A Data Perturbation Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/c70daf247944fe3add32218f914c75a6-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_c850371f,
 abstract = {A hint is any piece of side information about the target function to be learned. We consider the monotonicity hint, which states that the function to be learned is monotonic in some or all of the input variables. The application of monotonicity hints is demonstrated on two real-world problems- a credit card application task, and a problem in medical diagnosis. A measure of the monotonicity error of a candidate function is defined and an objective function for the enforcement of monotonicity is derived from Bayesian principles. We report experimental results which show that using monotonicity hints leads to a statistically significant improvement in performance on both problems.},
 author = {Sill, Joseph and Abu-Mostafa, Yaser},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c850371fda6892fbfd1c5a5b457e5777-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c850371fda6892fbfd1c5a5b457e5777-Metadata.json},
 openalex = {W2293457282},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf},
 publisher = {MIT Press},
 title = {Monotonicity Hints},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/c850371fda6892fbfd1c5a5b457e5777-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_c8ba76c2,
 abstract = {The softassign quadratic assignment algorithm has recently emerged as an effective strategy for a variety of optimization problems in pattern recognition and combinatorial optimization. While the effectiveness of the algorithm was demonstrated in thousands of simulations, there was no known proof of convergence. Here, we provide a proof of convergence for the most general form of the algorithm.},
 author = {Rangarajan, Anand and Yuille, Alan L and Gold, Steven and Mjolsness, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c8ba76c279269b1c6bc8a07e38e78fa4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c8ba76c279269b1c6bc8a07e38e78fa4-Metadata.json},
 openalex = {W2127078870},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/c8ba76c279269b1c6bc8a07e38e78fa4-Paper.pdf},
 publisher = {MIT Press},
 title = {A Convergence Proof for the Softassign Quadratic Assignment Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/c8ba76c279269b1c6bc8a07e38e78fa4-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_cd758e8f,
 abstract = {Stochastic (on-line) learning can be faster than batch learning. However, at late times, the learning rate must be annealed to remove the noise present in the stochastic weight updates. In this annealing phase, the convergence rate (in mean square) is at best proportional to 1/τ where τ is the number of input presentations. An alternative is to increase the batch size to remove the noise. In this paper we explore convergence for LMS using 1) small but fixed batch sizes and 2) an adaptive batch size. We show that the best adaptive batch schedule is exponential and has a rate of convergence which is the same as for annealing, Le., at best proportional to 1/τ.},
 author = {Orr, Genevieve},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/cd758e8f59dfdf06a852adad277986ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/cd758e8f59dfdf06a852adad277986ca-Metadata.json},
 openalex = {W2104054119},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/cd758e8f59dfdf06a852adad277986ca-Paper.pdf},
 publisher = {MIT Press},
 title = {Removing Noise in On-Line Search using Adaptive Batch Sizes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/cd758e8f59dfdf06a852adad277986ca-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_d282ef26,
 abstract = {This article presents a new result about the size of a multilayer neural network computing real outputs for exact learning of a finite set of real samples. The architecture of the network is feedforward, with one hidden layer and several outputs. Starting from a fixed training set, we consider the network as a function of its weights. We derive, for a wide family of transfer functions, a lower and an upper bound on the number of hidden units for exact learning, given the size of the dataset and the dimensions of the input and output spaces.},
 author = {Elisseeff, Andr\'{e} and Paugam-Moisy, H\'{e}l\`{e}ne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d282ef263719ab842e05382dc235f69e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d282ef263719ab842e05382dc235f69e-Metadata.json},
 openalex = {W2096632339},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d282ef263719ab842e05382dc235f69e-Paper.pdf},
 publisher = {MIT Press},
 title = {Size of Multilayer Networks for Exact Learning: Analytic Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/d282ef263719ab842e05382dc235f69e-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_d3890178,
 abstract = {A new regression technique based on Vapnik's concept of support vectors is introduced. We compare support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space. On the basis of these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space.},
 author = {Drucker, Harris and Burges, Christopher J. C. and Kaufman, Linda and Smola, Alex and Vapnik, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d38901788c533e8286cb6400b40b386d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d38901788c533e8286cb6400b40b386d-Metadata.json},
 openalex = {W2137226992},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d38901788c533e8286cb6400b40b386d-Paper.pdf},
 publisher = {MIT Press},
 title = {Support Vector Regression Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/d38901788c533e8286cb6400b40b386d-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_d759175d,
 abstract = {Binocular rivalry is the alternating percept that can result when the two eyes see different scenes. Recent psychophysical evidence supports an account for one component of binocular rivalry similar to that for other bistable percepts. We test the hypothesis that alternation can be generated by competition between top-down cortical explanations for the inputs, rather than by direct competition between the inputs. Recent neurophysiological evidence shows that some binocular neurons are modulated with the changing percept; others are not, even if they are selective between the stimuli presented to the eyes. We extend our model to a hierarchy to address these effects.},
 author = {Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d759175de8ea5b1d9a2660e45554894f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d759175de8ea5b1d9a2660e45554894f-Metadata.json},
 openalex = {W2142843614},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Hierarchical Model of Visual Rivalry},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/d759175de8ea5b1d9a2660e45554894f-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_d94e18a8,
 abstract = {The major problem that has prevented practical application of analog neuro-LSIs has been poor accuracy due to fluctuating analog device characteristics inherent in each device as a result of manufacturing. This paper proposes a dynamic control architecture that allows analog silicon neural networks to compensate for the fluctuating device characteristics and adapt to a change in input DC level. We have applied this architecture to compensate for input offset voltages of an analog CMOS WTA (Winner-Take-All) chip that we have fabricated. Experimental data show the effectiveness of the architecture.},
 author = {Iizuka, Kunihiko and Miyamoto, Masayuki and Matsui, Hirofumi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d94e18a8adb4cc0f623f7a83b1ac75b4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d94e18a8adb4cc0f623f7a83b1ac75b4-Metadata.json},
 openalex = {W2095785485},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/d94e18a8adb4cc0f623f7a83b1ac75b4-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamically Adaptable CMOS Winner-Take-All Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/d94e18a8adb4cc0f623f7a83b1ac75b4-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_da11e8cd,
 abstract = {This paper investigates a number of ensemble methods for improving the performance of phoneme classification for use in a speech recognition system. Two ensemble methods are described; boosting and mixtures of experts, both in isolation and in combination. Results are presented on two speech recognition databases: an isolated word database and a large vocabulary continuous speech database. These results show that principled ensemble methods such as boosting and mixtures provide superior performance to more naive ensemble methods such as averaging.},
 author = {Waterhouse, Steve and Cook, Gary},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/da11e8cd1811acb79ccf0fd62cd58f86-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/da11e8cd1811acb79ccf0fd62cd58f86-Metadata.json},
 openalex = {W2150037445},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/da11e8cd1811acb79ccf0fd62cd58f86-Paper.pdf},
 publisher = {MIT Press},
 title = {Ensemble Methods for Phoneme Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/da11e8cd1811acb79ccf0fd62cd58f86-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_dabd8d2c,
 abstract = {In the square linear blind source separation problem, one must find a linear unmixing operator which can detangle the result xi(t) of mixing n unknown independent sources si(t) through an unknown n × n mixing matrix A(t) of causal linear filters: xi = Σj aij * sj. We cast the problem as one of maximum likelihood density estimation, and in that framework introduce an algorithm that searches for independent components using both temporal and spatial cues. We call the resulting algorithm Contextual ICA, after the (Bell and Sejnowski 1995) Infomax algorithm, which we show to be a special case of cICA. Because cICA can make use of the temporal structure of its input, it is able separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms.},
 author = {Pearlmutter, Barak and Parra, Lucas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dabd8d2ce74e782c65a973ef76fd540b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dabd8d2ce74e782c65a973ef76fd540b-Metadata.json},
 openalex = {W2125941613},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf},
 publisher = {MIT Press},
 title = {Maximum Likelihood Blind Source Separation: A Context-Sensitive Generalization of ICA},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/dabd8d2ce74e782c65a973ef76fd540b-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_dc4c44f6,
 abstract = {In order to process incoming sounds efficiently, it is advantageous for the auditory system to be adapted to the statistical structure of natural auditory scenes. As a first step in investigating the relation between the system and its inputs, we study low-order statistical properties in several sound ensembles using a filter bank analysis. Focusing on the amplitude and phase in different frequency bands, we find simple parametric descriptions for their distribution and power spectrum that are valid for very different types of sounds. In particular, the amplitude distribution has an exponential tail and its power spectrum exhibits a modified power-law behavior, which is manifested by self-similarity and long-range temporal correlations. Furthermore, the statistics for different bands within a given ensemble are virtually identical, suggesting translation invariance along the cochlear axis. These results show that natural sounds are highly redundant, and have possible implications to the neural code used by the auditory system.},
 author = {Attias, Hagai and Schreiner, Christoph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dc4c44f624d600aa568390f1f1104aa0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dc4c44f624d600aa568390f1f1104aa0-Metadata.json},
 openalex = {W2131757759},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf},
 publisher = {MIT Press},
 title = {Temporal Low-Order Statistics of Natural Sounds},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/dc4c44f624d600aa568390f1f1104aa0-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_dc87c137,
 abstract = {The limitations of using self-organizing maps (SOM) for either clustering/vector quantization (VQ) or multidimensional scaling (MDS) are being discussed by reviewing recent empirical findings and the relevant theory. SOM's remaining ability of doing both VQ and MDS at the same time is challenged by a new combined technique of online K-means clustering plus Sammon mapping of the cluster centroids. SOM are shown to perform significantly worse in terms of quantization error, in recovering the structure of the clusters and in preserving the topology in a comprehensive empirical study using a series of multivariate normal clustering problems.},
 author = {Flexer, Arthur},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dc87c13749315c7217cdc4ac692e704c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dc87c13749315c7217cdc4ac692e704c-Metadata.json},
 openalex = {W2109950258},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dc87c13749315c7217cdc4ac692e704c-Paper.pdf},
 publisher = {MIT Press},
 title = {Limitations of Self-organizing Maps for Vector Quantization and Multidimensional Scaling},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/dc87c13749315c7217cdc4ac692e704c-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_dc960c46,
 abstract = {Many popular learning rules are formulated in terms of continuous, analog inputs and outputs. Biological systems, however, use action potentials, which are digital-amplitude events that encode analog information in the inter-event interval. Action-potential representations are now being used to advantage in neuromorphic VLSI systems as well. We report on a simple learning rule, based on the Riccati equation described by Kohonen [1], modified for action-potential neuronal outputs. We demonstrate this learning rule in an analog VLSI chip that uses volatile capacitive storage for synaptic weights. We show that our time-dependent learning rule is sufficient to achieve approximate weight normalization and can detect temporal correlations in spike trains.},
 author = {H\"{a}fliger, Philipp and Mahowald, Misha and Watts, Lloyd},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dc960c46c38bd16e953d97cdeefdbc68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dc960c46c38bd16e953d97cdeefdbc68-Metadata.json},
 openalex = {W2108043710},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf},
 publisher = {MIT Press},
 title = {A Spike Based Learning Neuron in Analog VLSI},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/dc960c46c38bd16e953d97cdeefdbc68-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_dfd7468a,
 abstract = {Neural one-unit learning rules for the problem of Independent Component Analysis (ICA) and blind source separation are introduced. In these new algorithms, every ICA neuron develops into a separator that finds one of the independent components. The learning rules use very simple constrained Hebbian/anti-Hebbian learning in which decorrelating feedback may be added. To speed up the convergence of these stochastic gradient descent rules, a novel computationally efficient fixed-point algorithm is introduced.},
 author = {Hyv\"{a}rinen, Aapo and Oja, Erkki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dfd7468ac613286cdbb40872c8ef3b06-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dfd7468ac613286cdbb40872c8ef3b06-Metadata.json},
 openalex = {W2108373649},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/dfd7468ac613286cdbb40872c8ef3b06-Paper.pdf},
 publisher = {MIT Press},
 title = {One-unit Learning Rules for Independent Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/dfd7468ac613286cdbb40872c8ef3b06-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_e0040614,
 author = {Tsitsiklis, John and Van Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e00406144c1e7e35240afed70f34166a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e00406144c1e7e35240afed70f34166a-Metadata.json},
 openalex = {W2137766593},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},
 publisher = {MIT Press},
 title = {Analysis of Temporal-Diffference Learning with Function Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/e00406144c1e7e35240afed70f34166a-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_e034fb6b,
 abstract = {The algorithm described in this article is based on the OBS algorithm by Hassibi, Stork and Wolff ([1] and [2]). The main disadvantage of OBS is its high complexity. OBS needs to calculate the inverse Hessian to delete only one weight (thus needing much time to prune a big net). A better algorithm should use this matrix to remove more than only one weight, because calculating the inverse Hessian takes the most time in the OBS algorithm.

The algorithm, called Unit-OBS, described in this article is a method to overcome this disadvantage. This algorithm only needs to calculate the inverse Hessian once to remove one whole unit thus drastically reducing the time to prune big nets.

A further advantage of Unit-OBS is that it can be used to do a feature extraction on the input data. This can be helpful on the understanding of unknown problems.},
 author = {Stahlberger, Achim and Riedmiller, Martin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e034fb6b66aacc1d48f445ddfb08da98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e034fb6b66aacc1d48f445ddfb08da98-Metadata.json},
 openalex = {W2098016524},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Network Pruning and Feature Extraction by using the Unit-OBS Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/e034fb6b66aacc1d48f445ddfb08da98-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_e1d5be1c,
 abstract = {We study the effect of noise and regularization in an on-line gradient-descent learning scenario for a general two-layer student network with an arbitrary number of hidden units. Training examples are randomly drawn input vectors labeled by a two-layer teacher network with an arbitrary number of hidden units; the examples are corrupted by Gaussian noise affecting either the output or the model itself. We examine the effect of both types of noise and that of weight-decay regularization on the dynamical evolution of the order parameters and the generalization error in various phases of the learning process.},
 author = {Saad, David and Solla, Sara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e1d5be1c7f2f456670de3d53c7b54f4a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e1d5be1c7f2f456670de3d53c7b54f4a-Metadata.json},
 openalex = {W2155568373},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning with Noise and Regularizers in Multilayer Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/e1d5be1c7f2f456670de3d53c7b54f4a-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_e3251075,
 abstract = {The learning properties of a universal approximator, a normalized committee machine with adjustable biases, are studied for on-line back-propagation learning. Within a statistical mechanics framework, numerical studies show that this model has features which do not exist in previously studied two-layer network models without adjustable biases, e.g., attractive suboptimal symmetric phases even for realizable cases and noiseless data.},
 author = {West, Ansgar and Saad, David and Nabney, Ian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e3251075554389fe91d17a794861d47b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e3251075554389fe91d17a794861d47b-Metadata.json},
 openalex = {W2100553405},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e3251075554389fe91d17a794861d47b-Paper.pdf},
 publisher = {MIT Press},
 title = {The Learning Dynamcis of a Universal Approximator},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/e3251075554389fe91d17a794861d47b-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_e53a0a29,
 abstract = {The full Bayesian method for applying neural networks to a prediction problem is to set up the prior/hyperprior structure for the net and then perform the necessary integrals. However, these integrals are not tractable analytically, and Markov Chain Monte Carlo (MCMC) methods are slow, especially if the parameter space is high-dimensional. Using Gaussian processes we can approximate the weight space integral analytically, so that only a small number of hyperparameters need be integrated over by MCMC methods. We have applied this idea to classification problems, obtaining excellent results on the real-world problems investigated so far.},
 author = {Barber, David and Williams, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e53a0a2978c28872a4505bdb51db06dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e53a0a2978c28872a4505bdb51db06dc-Metadata.json},
 openalex = {W2118460912},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf},
 publisher = {MIT Press},
 title = {Gaussian Processes for Bayesian Classification via Hybrid Monte Carlo},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_e6d8545d,
 abstract = {The genetic algorithm (GA) is a heuristic search procedure based on mechanisms abstracted from population genetics. In a previous paper [Baluja & Caruana, 1995], we showed that much simpler algorithms, such as hillclimbing and Population-Based Incremental Learning (PBIL), perform comparably to GAs on an optimization problem custom designed to benefit from the GA's operators. This paper extends these results in two directions. First, in a large-scale empirical comparison of problems that have been reported in GA literature, we show that on many problems, simpler algorithms can perform significantly better than GAs. Second, we describe when crossover is useful, and show how it can be incorporated into PBIL.},
 author = {Baluja, Shumeet},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e6d8545daa42d5ced125a4bf747b3688-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e6d8545daa42d5ced125a4bf747b3688-Metadata.json},
 openalex = {W2147610016},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf},
 publisher = {MIT Press},
 title = {Genetic Algorithms and Explicit Search Statistics},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_e702e51d,
 abstract = {This paper develops arguments for a family of temporal log-linear models to represent spatio-temporal correlations among the spiking events in a group of neurons. The models can represent not just pairwise correlations but also correlations of higher order. Methods are discussed for inferring the existence or absence of correlations and estimating their strength.

A frequentist and a Bayesian approach to correlation detection are compared. The frequentist method is based on G2 statistic with estimates obtained via the Max-Ent principle. In the Bayesian approach a Markov Chain Monte Carlo Model Composition (MC3) algorithm is applied to search over connectivity structures and Laplace's method is used to approximate their posterior probability. Performance of the methods was tested on synthetic data. The methods were applied to experimental data obtained by the fourth author by means of measurements carried out on behaving Rhesus monkeys at the Hadassah Medical School of the Hebrew University. As conjectured, neural connectivity structures need not be neither hierarchical nor decomposable.},
 author = {Martignon, Laura and Laskey, Kathryn and Deco, Gustavo and Vaadia, Eilon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e702e51da2c0f5be4dd354bb3e295d37-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e702e51da2c0f5be4dd354bb3e295d37-Metadata.json},
 openalex = {W2141772409},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Exact Patterns of Quasi-synchronization among Spiking Neurons from Data on Multi-unit Recordings},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/e702e51da2c0f5be4dd354bb3e295d37-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_eb86d510,
 abstract = {A new reinforcement learning architecture for nonlinear control is proposed. A direct feedback controller, or the actor, is trained by a value-gradient based controller, or the tutor. This architecture enables both efficient use of the value function and simple computation for real-time implementation. Good performance was verified in multi-dimensional nonlinear control tasks using Gaussian softmax networks.},
 author = {Doya, Kenji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/eb86d510361fc23b59f18c1bc9802cc6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/eb86d510361fc23b59f18c1bc9802cc6-Metadata.json},
 openalex = {W2098607794},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient Nonlinear Control with Actor-Tutor Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/eb86d510361fc23b59f18c1bc9802cc6-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_f0969691,
 abstract = {We present a mixture of experts (ME) approach to interpolate sparse, spatially correlated earth-science data. Kriging is an interpolation method which uses a global covariation model estimated from the data to take account of the spatial dependence in the data. Based on the close relationship between kriging and the radial basis function (RBF) network (Wan & Bone, 1996), we use a mixture of generalized RBF networks to partition the input space into statistically correlated regions and learn the local covariation model of the data in each region. Applying the ME approach to simulated and real-world data, we show that it is able to achieve good partitioning of the input space, learn the local covariation models and improve generalization.},
 author = {Wan, Ernest and Bone, Don},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f09696910bdd874a99cd74c8f05b5c44-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f09696910bdd874a99cd74c8f05b5c44-Metadata.json},
 openalex = {W2116357479},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f09696910bdd874a99cd74c8f05b5c44-Paper.pdf},
 publisher = {MIT Press},
 title = {Interpolating Earth-science Data using RBF Networks and Mixtures of Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/f09696910bdd874a99cd74c8f05b5c44-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_f29b38f1,
 abstract = {Coarse codes are widely used throughout the brain to encode sensory and motor variables. Methods designed to interpret these codes, such as population vector analysis, are either inefficient, i.e., the variance of the estimate is much larger than the smallest possible variance, or biologically implausible, like maximum likelihood. Moreover, these methods attempt to compute a scalar or vector estimate of the encoded variable. Neurons are faced with a similar estimation problem. They must read out the responses of the presynaptic neurons, but, by contrast, they typically encode the variable with a further population code rather than as a scalar. We show how a non-linear recurrent network can be used to perform these estimation in an optimal way while keeping the estimate in a coarse code format. This work suggests that lateral connections in the cortex may be involved in cleaning up uncorrelated noise among neurons representing similar variables.},
 author = {Pouget, Alexandre and Zhang, Kechen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f29b38f160f87ae86df31cee1982066f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f29b38f160f87ae86df31cee1982066f-Metadata.json},
 openalex = {W2146661091},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f29b38f160f87ae86df31cee1982066f-Paper.pdf},
 publisher = {MIT Press},
 title = {Statistically Efficient Estimations Using Cortical Lateral Connections},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/f29b38f160f87ae86df31cee1982066f-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_f3f1b7fc,
 abstract = {We develop a recursive node-elimination formalism for efficiently approximating large probabilistic networks. No constraints are set on the network topologies. Yet the formalism can be straightforwardly integrated with exact methods whenever they are/become applicable. The approximations we use are controlled: they maintain consistently upper and lower bounds on the desired quantities at all times. We show that Boltzmann machines, sigmoid belief networks, or any combination (i.e., chain graphs) can be handled within the same framework. The accuracy of the methods is verified experimentally.},
 author = {Jaakkola, Tommi and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f3f1b7fc5a8779a9e618e1f23a7b7860-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f3f1b7fc5a8779a9e618e1f23a7b7860-Metadata.json},
 openalex = {W2133231442},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f3f1b7fc5a8779a9e618e1f23a7b7860-Paper.pdf},
 publisher = {MIT Press},
 title = {Recursive Algorithms for Approximating Probabilities in Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/f3f1b7fc5a8779a9e618e1f23a7b7860-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_f4573fc7,
 abstract = {The problem of assigning m points in the n-dimensional real space Rn to k clusters is formulated as that of determining k centers in Rn such that the sum of distances of each point to the nearest center is minimized. If a polyhedral distance is used, the problem can be formulated as that of minimizing a piecewise-linear concave function on a polyhedral set which is shown to be equivalent to a bilinear program: minimizing a bilinear function on a polyhedral set. A fast finite k-Median Algorithm consisting of solving few linear programs in closed form leads to a stationary point of the bilinear program. Computational testing on a number of real-world databases was carried out. On the Wisconsin Diagnostic Breast Cancer (WDBC) database, k-Median training set correctness was comparable to that of the k-Mean Algorithm, however its testing set correctness was better. Additionally, on the Wisconsin Prognostic Breast Cancer (WPBC) database, distinct and clinically important survival curves were extracted by the k-Median Algorithm, whereas the k-Mean Algorithm failed to obtain such distinct survival curves for the same database.},
 author = {Bradley, Paul and Mangasarian, Olvi and Street, W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f4573fc71c731d5c362f0d7860945b88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f4573fc71c731d5c362f0d7860945b88-Metadata.json},
 openalex = {W2105535594},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f4573fc71c731d5c362f0d7860945b88-Paper.pdf},
 publisher = {MIT Press},
 title = {Clustering via Concave Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/f4573fc71c731d5c362f0d7860945b88-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_f4733064,
 abstract = {We compare different methods to combine predictions from neural networks trained on different bootstrap samples of a regression problem. One of these methods, introduced in [6] and which we here call balancing, is based on the analysis of the ensemble generalization error into an ambiguity term and a term incorporating generalization performances of individual networks. We show how to estimate these individual errors from the residuals on validation patterns. Weighting factors for the different networks follow from a quadratic programming problem. On a real-world problem concerning the prediction of sales figures and on the well-known Boston housing data set, balancing clearly outperforms other recently proposed alternatives as bagging [1] and bumping [8].},
 author = {Heskes, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f47330643ae134ca204bf6b2481fec47-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f47330643ae134ca204bf6b2481fec47-Metadata.json},
 openalex = {W2096193168},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf},
 publisher = {MIT Press},
 title = {Balancing Between Bagging and Bumping},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/f47330643ae134ca204bf6b2481fec47-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_f7cade80,
 abstract = {The paper is developed in two parts where we discuss a new approach to self-organization in a single-layer linear feed-forward network. First, two novel algorithms for self-organization are derived from a two-layer linear hetero-associative network performing a one-of-m classification, and trained with the constrained least-mean-squared classification error criterion. Second, two adaptive algorithms are derived from these self-organizing procedures to compute the principal generalized eigenvectors of two correlation matrices from two sequences of random vectors. These novel adaptive algorithms can be implemented in a single-layer linear feed-forward network. We give a rigorous convergence analysis of the adaptive algorithms by using stochastic approximation theory. As an example, we consider a problem of online signal detection in digital mobile communications.},
 author = {Chatterjee, Chanchal and Roychowdhury, Vwani},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f7cade80b7cc92b991cf4d2806d6bd78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f7cade80b7cc92b991cf4d2806d6bd78-Metadata.json},
 openalex = {W2167894557},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf},
 publisher = {MIT Press},
 title = {Self-Organizing and Adaptive Algorithms for Generalized Eigen-Decomposition},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/f7cade80b7cc92b991cf4d2806d6bd78-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_f91e24df,
 abstract = {Reinforcement learning methods for discrete and semi-Markov decision problems such as Real-Time Dynamic Programming can be generalized for Controlled Diffusion Processes. The optimal control problem reduces to a boundary value problem for a fully nonlinear second-order elliptic differential equation of Hamilton-Jacobi-Bellman (HJB-) type. Numerical analysis provides multigrid methods for this kind of equation. In the case of Learning Control, however, the systems of equations on the various grid-levels are obtained using observed information (transitions and local cost). To ensure consistency, special attention needs to be directed toward the type of time and space discretization during the observation. An algorithm for multi-grid observation is proposed. The multi-grid algorithm is demonstrated on a simple queuing problem.},
 author = {Pareigis, Stephan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f91e24dfe80012e2a7984afa4480a6d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f91e24dfe80012e2a7984afa4480a6d6-Metadata.json},
 openalex = {W2128777402},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f91e24dfe80012e2a7984afa4480a6d6-Paper.pdf},
 publisher = {MIT Press},
 title = {Multi-Grid Methods for Reinforcement Learning in Controlled Diffusion Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/f91e24dfe80012e2a7984afa4480a6d6-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_f93882cb,
 abstract = {We exhibit a novel way of simulating sigmoidal neural nets by networks of noisy spiking neurons in temporal coding. Furthermore it is shown that networks of noisy spiking neurons with temporal coding have a strictly larger computational power than sigmoidal neural nets with the same number of units.},
 author = {Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f93882cbd8fc7fb794c1011d63be6fb6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f93882cbd8fc7fb794c1011d63be6fb6-Metadata.json},
 openalex = {W2169074879},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f93882cbd8fc7fb794c1011d63be6fb6-Paper.pdf},
 publisher = {MIT Press},
 title = {Noisy Spiking Neurons with Temporal Coding have more Computational Power than Sigmoidal Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/f93882cbd8fc7fb794c1011d63be6fb6-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_f9be311e,
 author = {Bell, Anthony and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f9be311e65d81a9ad8150a60844bb94c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f9be311e65d81a9ad8150a60844bb94c-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf},
 publisher = {MIT Press},
 title = {Edges are the \textquotesingle Independent Components\textquotesingle of Natural Scenes.},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_fb2fcd53,
 abstract = {This paper shows that if a large neural network is used for a pattern classification problem, and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. More specifically, consider an l-layer feed-forward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A. The misclassification probability converges to an error estimate (that is closely related to squared error on the training set) at rate O((cA)l(l+1)/2 √(log n)/m) ignoring log factors, where m is the number of training patterns, n is the input dimension, and c is a constant. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training.},
 author = {Bartlett, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fb2fcd534b0ff3bbed73cc51df620323-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fb2fcd534b0ff3bbed73cc51df620323-Metadata.json},
 openalex = {W2128882956},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fb2fcd534b0ff3bbed73cc51df620323-Paper.pdf},
 publisher = {MIT Press},
 title = {For Valid Generalization the Size of the Weights is More Important than the Size of the Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/fb2fcd534b0ff3bbed73cc51df620323-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_fb60d411,
 abstract = {Time series prediction is one of the major applications of neural networks. After a short introduction into the basic theoretical foundations we argue that the iterated prediction of a dynamical system may be interpreted as a model of the system dynamics. By means of RBF neural networks we describe a modeling approach and extend it to be able to model instationary systems. As a practical test for the capabilities of the method we investigate the modeling of musical and speech signals and demonstrate that the model may be used for synthesis of musical and speech signals.},
 author = {R\"{o}bel, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Metadata.json},
 openalex = {W2161335528},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Network Modeling of Speech and Music Signals},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_fd5c905b,
 abstract = {We have investigated the possibility that rapid processing in the visual system could be achieved by using the order of firing in different neurones as a code, rather than more conventional firing rate schemes. Using SPIKENET, a neural net simulator based on integrate-and-fire neurones and in which neurones in the input layer function as analog-to-delay converters, we have modeled the initial stages of visual processing. Initial results are extremely promising. Even with activity in retinal output cells limited to one spike per neuron per image (effectively ruling out any form of rate coding), sophisticated processing based on asynchronous activation was nonetheless possible.},
 author = {Thorpe, Simon and Gautrais, Jacques},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Metadata.json},
 openalex = {W2155789201},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Paper.pdf},
 publisher = {MIT Press},
 title = {Rapid Visual Processing using Spike Asynchrony},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/fd5c905bcd8c3348ad1b35d7231ee2b1-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_fe2d0103,
 abstract = {A simple mathematical model for the large-scale circuitry of primary visual cortex is introduced. It is shown that a basic cortical architecture of recurrent local excitation and lateral inhibition can account quantitatively for such properties as orientation tuning. The model can also account for such local effects as cross-orientation suppression. It is also shown that nonlocal state-dependent coupling between similar orientation patches, when added to the model, can satisfactorily reproduce such effects as non-local iso--orientation suppression, and non-local cross-orientation enhancement. Following this an account is given of perceptual phenomena involving object segmentation, such as popout, and the direct and indirect tilt illusions.},
 author = {Mundel, Trevor and Dimitrov, Alexander and Cowan, Jack},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fe2d010308a6b3799a3d9c728ee74244-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fe2d010308a6b3799a3d9c728ee74244-Metadata.json},
 openalex = {W2127746315},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf},
 publisher = {MIT Press},
 title = {Visual Cortex Circuitry and Orientation Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/fe2d010308a6b3799a3d9c728ee74244-Abstract.html},
 volume = {9},
 year = {1996}
}

@inproceedings{NIPS1996_fe51510c,
 abstract = {Recently Sillito and coworkers (Nature 378, pp. 492, 1995) demonstrated that stimulation beyond the classical receptive field (cRF) can not only modulate, but radically change a neuron's response to oriented stimuli. They revealed that patch-suppressed cells when stimulated with contrasting orientations inside and outside their cRF can strongly respond to stimuli oriented orthogonal to their nominal preferred orientation. Here we analyze the emergence of such complex response patterns in a simple model of primary visual cortex. We show that the observed sensitivity for orientation contrast can be explained by a delicate interplay between local isotropic interactions and patchy long-range connectivity between distant iso-orientation domains. In particular we demonstrate that the observed properties might arise without specific connections between sites with cross-oriented cRFs.},
 author = {Pawelzik, Klaus and Ernst, Udo and Wolf, Fred and Geisel, Theo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fe51510c80bfd6e5d78a164cd5b1f688-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fe51510c80bfd6e5d78a164cd5b1f688-Metadata.json},
 openalex = {W2109983170},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/1996/file/fe51510c80bfd6e5d78a164cd5b1f688-Paper.pdf},
 publisher = {MIT Press},
 title = {Orientation Contrast Sensitivity from Long-range Interactions in Visual Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/fe51510c80bfd6e5d78a164cd5b1f688-Abstract.html},
 volume = {9},
 year = {1996}
}
