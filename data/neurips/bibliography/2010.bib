@inproceedings{NIPS2010_00411460,
 abstract = {We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.},
 author = {Beygelzimer, Alina and Hsu, Daniel J and Langford, John and Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00411460f7c92d2124a67ea0f4cb5f85-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00411460f7c92d2124a67ea0f4cb5f85-Metadata.json},
 openalex = {W2951528191},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00411460f7c92d2124a67ea0f4cb5f85-Supplemental.zip},
 title = {Agnostic Active Learning Without Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_00e26af6,
 abstract = {We consider multi-task learning in the setting of multiple linear regression, and where some relevant features could be shared across the tasks. Recent research has studied the use of l1/lq norm block-regularizations with q > 1 for such block-sparse structured problems, establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations. However, these papers also caution that the performance of such block-regularized methods are very dependent on the extent to which the features are shared across tasks. Indeed they show [8] that if the extent of overlap is less than a threshold, or even if parameter values in the shared features are highly uneven, then block l1/lq regularization could actually perform worse than simple separate elementwise l1 regularization. Since these caveats depend on the unknown true parameters, we might not know when and which method to apply. Even otherwise, we are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks, but their values have to as well.

Here, we ask the question: can we leverage parameter overlap when it exists, but not pay a penalty when it does not? Indeed, this falls under a more general question of whether we can model such dirty data which may not fall into a single neat structural bracket (all block-sparse, or all low-rank and so on). With the explosion of such dirty high-dimensional data in modern settings, it is vital to develop tools - dirty models - to perform biased statistical estimation tailored to such data. Here, we take a first step, focusing on developing a dirty model for the multiple regression problem. Our method uses a very simple idea: we estimate a superposition of two sets of parameters and regularize them differently. We show both theoretically and empirically, our method strictly and noticeably outperforms both l1 or l1/lq methods, under high-dimensional scaling and over the entire range of possible overlaps (except at boundary cases, where we match the best method).},
 author = {Jalali, Ali and Sanghavi, Sujay and Ruan, Chao and Ravikumar, Pradeep},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Metadata.json},
 openalex = {W2166721725},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/00e26af6ac3b1c1c49d7c3d79c60d000-Supplemental.zip},
 title = {A Dirty Model for Multi-task Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/00e26af6ac3b1c1c49d7c3d79c60d000-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_01386bd6,
 abstract = {We consider the problem of learning a local metric in order to enhance the performance of nearest neighbor classification. Conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models. We focus on the bias in the information-theoretic error arising from finite sampling effects, and find an appropriate local metric that maximally reduces the bias based upon knowledge from generative models. As a byproduct, the asymptotic theoretical analysis in this work relates metric learning to dimensionality reduction from a novel perspective, which was not understood from previous discriminative approaches. Empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models such as a Gaussian.},
 author = {Noh, Yung-kyun and Zhang, Byoung-tak and Lee, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01386bd6d8e091c2ab4c7c7de644d37b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01386bd6d8e091c2ab4c7c7de644d37b-Metadata.json},
 openalex = {W2586258914},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Generative Local Metric Learning for Nearest Neighbor Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_01882513,
 abstract = {Robust regression and classification are often thought to require non-convex functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a function more robust to outliers is to truncate values that exceed a maximum threshold. We demonstrate that a relaxation of this form of loss clipping can be made globally solvable and applicable to any standard while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard functions and demonstrate improved robustness in regression and classification problems.},
 author = {Yang, Min and Xu, Linli and White, Martha and Schuurmans, Dale and Yu, Yao-liang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01882513d5fa7c329e940dda99b12147-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01882513d5fa7c329e940dda99b12147-Metadata.json},
 openalex = {W2151976295},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01882513d5fa7c329e940dda99b12147-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01882513d5fa7c329e940dda99b12147-Supplemental.zip},
 title = {Relaxed Clipping: A Global Training Method for Robust Regression and Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/01882513d5fa7c329e940dda99b12147-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0188e8b8,
 abstract = {How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reflect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to fill the gaps in noise correlations matrices using an iterative application of the Wishart distribution over positive definitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternative-forced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations.},
 author = {Wohrer, Adrien and Romo, Ranulfo and Machens, Christian K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0188e8b8b014829e2fa0f430f0a95961-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0188e8b8b014829e2fa0f430f0a95961-Metadata.json},
 openalex = {W2171201546},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0188e8b8b014829e2fa0f430f0a95961-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Linear readout from a neural population with partial correlation data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0188e8b8b014829e2fa0f430f0a95961-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_019d385e,
 abstract = {The paper develops a connection between traditional perceptron algorithms and recently introduced herding algorithms. It is shown that both algorithms can be viewed as an application of the perceptron cycling theorem. This connection strengthens some herding results and suggests new (supervised) herding algorithms that, like CRFs or discriminative RBMs, make predictions by conditioning on the input attributes. We develop and investigate variants of conditional herding, and show that conditional herding leads to practical algorithms that perform better than or on par with related classifiers such as the voted perceptron and the discriminative RBM.},
 author = {Gelfand, Andrew and Chen, Yutian and Maaten, Laurens and Welling, Max},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/019d385eb67632a7e958e23f24bd07d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/019d385eb67632a7e958e23f24bd07d7-Metadata.json},
 openalex = {W2146540262},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On Herding and the Perceptron Cycling Theorem},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/019d385eb67632a7e958e23f24bd07d7-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_01f78be6,
 abstract = {Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose convolution neural networks (Tiled CNNs), which use a regular tiled pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs' advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.},
 author = {Ngiam, Jiquan and Chen, Zhenghao and Chia, Daniel and Koh, Pang and Le, Quoc and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01f78be6f7cad02658508fe4616098a9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01f78be6f7cad02658508fe4616098a9-Metadata.json},
 openalex = {W2147860648},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/01f78be6f7cad02658508fe4616098a9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Tiled convolutional neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/01f78be6f7cad02658508fe4616098a9-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_03c6b069,
 abstract = {A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efficient methods for each partitioning subproblem through an equivalent representation as a network flow problem, and prove that this sequence of partitions converges to the global solution. These network flow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm's favorable computational properties are demonstrated through simulated examples as large as 2 x 105 variables and 107 constraints.},
 author = {Luss, Ronny and Rosset, Saharon and Shahar, Moni},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/03c6b06952c750899bb03d998e631860-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/03c6b06952c750899bb03d998e631860-Metadata.json},
 openalex = {W2112728248},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/03c6b06952c750899bb03d998e631860-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Decomposing Isotonic Regression for Efficiently Solving Large Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/03c6b06952c750899bb03d998e631860-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_05049e90,
 abstract = {In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classifier. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2) of norm constraints on combination coefficients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method significantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches.},
 author = {Gai, Kun and Chen, Guangyun and Zhang, Chang-shui},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/05049e90fa4f5039a8cadc6acbb4b2cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/05049e90fa4f5039a8cadc6acbb4b2cc-Metadata.json},
 openalex = {W2165417234},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/05049e90fa4f5039a8cadc6acbb4b2cc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/05049e90fa4f5039a8cadc6acbb4b2cc-Supplemental.zip},
 title = {Learning Kernels with Radiuses of Minimum Enclosing Balls},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/05049e90fa4f5039a8cadc6acbb4b2cc-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_06138bc5,
 abstract = {Multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a tree-structure of classifiers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster.},
 author = {Bengio, Samy and Weston, Jason and Grangier, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/06138bc5af6023646ede0e1f7c1eac75-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/06138bc5af6023646ede0e1f7c1eac75-Metadata.json},
 openalex = {W2155144632},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/06138bc5af6023646ede0e1f7c1eac75-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Label Embedding Trees for Large Multi-Class Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_062ddb6c,
 abstract = {This paper proposes a principled extension of the traditional single-layer flat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.},
 author = {Lin, Yuanqing and Zhang, Tong and Zhu, Shenghuo and Yu, Kai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/062ddb6c727310e76b6200b7c71f63b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/062ddb6c727310e76b6200b7c71f63b5-Metadata.json},
 openalex = {W2165337236},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Deep Coding Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/062ddb6c727310e76b6200b7c71f63b5-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_06409663,
 abstract = {We pose transductive classification as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspecified, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modified fixed-point continuation method that is guaranteed to find the global optimum.},
 author = {Goldberg, Andrew and Recht, Ben and Xu, Junming and Nowak, Robert and Zhu, Jerry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/06409663226af2f3114485aa4e0a23b4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/06409663226af2f3114485aa4e0a23b4-Metadata.json},
 openalex = {W2160569988},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/06409663226af2f3114485aa4e0a23b4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Transduction with Matrix Completion: Three Birds with One Stone},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/06409663226af2f3114485aa4e0a23b4-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_072b030b,
 abstract = {Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n.},
 author = {Foygel, Rina and Drton, Mathias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Metadata.json},
 openalex = {W4294560492},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/072b030ba126b2f4b2374f342be9ed44-Supplemental.zip},
 title = {Extended Bayesian Information Criteria for Gaussian Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_076a0c97,
 abstract = {There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning significantly improves the performance of the state-of-the-art.},
 author = {Gupta, Abhinav and Hebert, Martial and Kanade, Takeo and Blei, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Metadata.json},
 openalex = {W2145567954},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/076a0c97d09cf1a0ec3e19c7f2529f2b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_07871915,
 abstract = {We study several classes of interactive assistants from the points of view of decision theory and computational complexity. We first introduce a special class of POMDPs called hidden-goal MDPs (HGMDPs), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection in finite horizon HGMDPs is PSPACE-complete even in domains with deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), where the assistant's action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise. We show classes of HAMDPs that are complete for PSPACE and NP along with a polynomial time class. Furthermore, we show that for general HAMDPs a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution. A variation of this policy is also shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution.},
 author = {Fern, Alan and Tadepalli, Prasad},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/07871915a8107172b3b5dc15a6574ad3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/07871915a8107172b3b5dc15a6574ad3-Metadata.json},
 openalex = {W2406740259},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A computational decision theory for interactive assistants},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/07871915a8107172b3b5dc15a6574ad3-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_08740852,
 abstract = {Multi-task learning (MTL) improves the prediction performance on multiple, different but related, learning problems through shared parameters or representations. One of the most prominent multi-task learning algorithms is an extension to support vector machines (svm) by Evgeniou et al. [15]. Although very elegant, multi-task svm is inherently restricted by the fact that support vector machines require each class to be addressed explicitly with its own weight vector which, in a multi-task setting, requires the different learning tasks to share the same set of classes. This paper proposes an alternative formulation for multi-task learning by extending the recently published large margin nearest neighbor (1mnn) algorithm to the MTL paradigm. Instead of relying on separating hyperplanes, its decision function is based on the nearest neighbor rule which inherently extends to many classes and becomes a natural fit for multi-task learning. We evaluate the resulting multi-task 1mnn on real-world insurance data and speech classification problems and show that it consistently outperforms single-task kNN under several metrics and state-of-the-art MTL classifiers.},
 author = {Parameswaran, Shibin and Weinberger, Kilian Q},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/087408522c31eeb1f982bc0eaf81d35f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/087408522c31eeb1f982bc0eaf81d35f-Metadata.json},
 openalex = {W2110994494},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/087408522c31eeb1f982bc0eaf81d35f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Large Margin Multi-Task Metric Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/087408522c31eeb1f982bc0eaf81d35f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_08b255a5,
 abstract = {Event Abstract Back to Event Evaluating neuronal codes for inference using Fisher information Ralf Haefner1* and Matthias Bethge1 1 Max Planck Institute for Biological Cybernetics, NWG Bethge, Germany Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively acts as noise. Here we explore the impact of such stimulus-induced response variability for two model cases: a) binocular disparity inference and b) orientation discrimination. We characterize the response distribution for the energy model in response to random dot stereograms and to displays of oriented random dots.For the case of bincular disparity processing we find the response distribution to be very different from the Poisson-like noise usually assumed. We compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We find that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions. Figure 1: A: Disparity tuning curves for the model using position disparity (even) and phase disparity (odd) in blue and red, respectively. B: Black: Fisher information contained in the monocular inputs. Blue: Fisher information left after combining inputs from left and right eye according to position disparity model. Red: Fisher information after combining inputs using phase disparity model. Note that the black and red curves diverge at zero disparity. C: Fisher information for the final model output/neuronal response. Same color code as before. Solid lines correspond to complex, dashed lines to simple cells. D: Same as C but with added Gaussian noise in the monocular inputs. Figure 1 Keywords: computational neuroscience Conference: Bernstein Conference on Computational Neuroscience, Berlin, Germany, 27 Sep - 1 Oct, 2010. Presentation Type: Presentation Topic: Bernstein Conference on Computational Neuroscience Citation: Haefner R and Bethge M (2010). Evaluating neuronal codes for inference using Fisher information. Front. Comput. Neurosci. Conference Abstract: Bernstein Conference on Computational Neuroscience. doi: 10.3389/conf.fncom.2010.51.00069 Copyright: The abstracts in this collection have not been subject to any Frontiers peer review or checks, and are not endorsed by Frontiers. They are made available through the Frontiers publishing platform as a service to conference organizers and presenters. The copyright in the individual abstracts is owned by the author of each abstract or his/her employer unless otherwise stated. Each abstract, as well as the collection of abstracts, are published under a Creative Commons CC-BY 4.0 (attribution) licence (https://creativecommons.org/licenses/by/4.0/) and may thus be reproduced, translated, adapted and be the subject of derivative works provided the authors and Frontiers are attributed. For Frontiers’ terms and conditions please see https://www.frontiersin.org/legal/terms-and-conditions. Received: 15 Sep 2010; Published Online: 23 Sep 2010. * Correspondence: Dr. Ralf Haefner, Max Planck Institute for Biological Cybernetics, NWG Bethge, Tübingen, Germany, ralf.haefner@gmail.com Login Required This action requires you to be registered with Frontiers and logged in. To register or login click here. Abstract Info Abstract The Authors in Frontiers Ralf Haefner Matthias Bethge Google Ralf Haefner Matthias Bethge Google Scholar Ralf Haefner Matthias Bethge PubMed Ralf Haefner Matthias Bethge Related Article in Frontiers Google Scholar PubMed Abstract Close Back to top Javascript is disabled. Please enable Javascript in your browser settings in order to see all the content on this page.},
 author = {Ralf, Haefner and Bethge, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/08b255a5d42b89b0585260b6f2360bdd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/08b255a5d42b89b0585260b6f2360bdd-Metadata.json},
 openalex = {W2144442831},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Evaluating neuronal codes for inference using Fisher information},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/08b255a5d42b89b0585260b6f2360bdd-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_08d98638,
 abstract = {Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under affine constraints (ARMP) and show that SVP recovers the minimum rank solution for affine constraints that satisfy a restricted isometry property (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of low-rank matrix completion, for which the defining affine constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of [5, 18, 14], for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is significantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem.},
 author = {Jain, Prateek and Meka, Raghu and Dhillon, Inderjit},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Metadata.json},
 openalex = {W1573301526},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Supplemental.zip},
 title = {Guaranteed Rank Minimization via Singular Value Projection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/08d98638c6fcd194a4b1e6992063e944-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_091d584f,
 abstract = {In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.},
 author = {Hasselt, Hado},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Metadata.json},
 openalex = {W2115211925},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Supplemental.zip},
 title = {Double Q-learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_093f65e0,
 abstract = {Consider a convex relaxation fˆ of a pseudo-Boolean function f. We say that the relaxation is totally half-integral if fˆ(x) is a polyhedral function with half-integral extreme points x, and this property is preserved after adding an arbitrary combination of constraints of the form xi=xj, xi=1−xj, and xi=γ where γ∈{0,1,12} is a constant. A well-known example is the roof duality relaxation for quadratic pseudo-Boolean functions f. We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-Boolean functions. Our contributions are as follows. First, we provide a complete characterization of totally half-integral relaxations fˆ by establishing a one-to-one correspondence with bisubmodular functions. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality. On the conceptual level, our results show that bisubmodular functions provide a natural generalization of the roof duality approach to higher-order terms. This can be viewed as a non-submodular analogue of the fact that submodular functions generalize the s-t minimum cut problem with non-negative weights to higher-order terms.},
 author = {Kolmogorov, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/093f65e080a295f8076b1c5722a46aa2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/093f65e080a295f8076b1c5722a46aa2-Metadata.json},
 openalex = {W2076534077},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Generalized roof duality and bisubmodular functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_09c6c378,
 author = {Nie, Feiping and Huang, Heng and Cai, Xiao and Ding, Chris},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/09c6c3783b4a70054da74f2538ed47c6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/09c6c3783b4a70054da74f2538ed47c6-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient and Robust Feature Selection via Joint \mathscr{l}2,1-Norms Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0a0a0c8a,
 abstract = {We study repeated zero-sum games against an adversary on a budget. Given that an adversary has some constraint on the sequence of actions that he plays, we consider what ought to be the player's best mixed strategy with knowledge of this budget. We show that, for a general class of normal-form games, the min-imax strategy is indeed efficiently computable and relies on a random playout technique. We give three diverse applications of this new algorithmic template: a cost-sensitive Hedge setting, a particular problem in Metrical Task Systems, and the design of combinatorial prediction markets.},
 author = {Abernethy, Jacob D and Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Metadata.json},
 openalex = {W2166694847},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Repeated Games against Budgeted Adversaries},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0a0a0c8aaa00ade50f74a3f0ca981ed7-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0aa1883c,
 abstract = {We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary firing rates using a switching state space model (SSSM). This algorithm enables us to detect state transitions on the basis of not only the discontinuous changes of mean firing rates but also discontinuous changes in temporal profiles of firing rates, e.g., temporal correlation. We construct a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events. Synthetic data analysis reveals that our algorithm has the high performance for estimating state transitions, the number of neural states, and nonstationary firing rates compared to previous methods. We also analyze neural data that were recorded from the medial temporal area. The statistically detected neural states probably coincide with transient and sustained states that have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition on the basis of discontinuous changes in the temporal correlation of firing rates, which transitions previous methods cannot detect. This result suggests that our algorithm is advantageous in real-data analysis.},
 author = {Takiyama, Ken and Okada, Masato},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0aa1883c6411f7873cb83dacb17b0afc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0aa1883c6411f7873cb83dacb17b0afc-Metadata.json},
 openalex = {W2155199533},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0aa1883c6411f7873cb83dacb17b0afc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Switching state space model for simultaneously estimating state transitions and nonstationary firing rates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0aa1883c6411f7873cb83dacb17b0afc-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0c74b7f7,
 abstract = {Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an information-theoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-first languages are particularly disfavored because they are highly non-optimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work.},
 author = {Maurits, Luke and Navarro, Dan and Perfors, Amy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Metadata.json},
 openalex = {W2099293539},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Supplemental.zip},
 title = {Why are some word orders more common than others? A uniform information density account},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0d0871f0,
 abstract = {The commute distance between two vertices in a graph is the expected time it takes a random walk to travel from the first to the second vertex and back. We study the behavior of the commute distance as the size of the underlying graph increases. We prove that the commute distance converges to an expression that does not take into account the structure of the graph at all and that is completely meaningless as a distance function on the graph. Consequently, the use of the raw commute distance for machine learning purposes is strongly discouraged for large graphs and in high dimensions. As an alternative we introduce the amplified commute distance that corrects for the undesired large sample effects.},
 author = {Luxburg, Ulrike and Radl, Agnes and Hein, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0871f0806eae32d30983b62252da50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0871f0806eae32d30983b62252da50-Metadata.json},
 openalex = {W2163413889},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0871f0806eae32d30983b62252da50-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0871f0806eae32d30983b62252da50-Supplemental.zip},
 title = {Getting lost in space: Large sample analysis of the resistance distance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0d0871f0806eae32d30983b62252da50-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0d0fd7c6,
 abstract = {As increasing amounts of sensitive personal information finds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classifier using classifiers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classifier. We also present a detailed theoretical analysis containing a proof of differential privacy of the perturbed aggregate classifier and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset.},
 author = {Pathak, Manas and Rane, Shantanu and Raj, Bhiksha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0fd7c6e093f7b804fa0150b875b868-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0fd7c6e093f7b804fa0150b875b868-Metadata.json},
 openalex = {W2110287632},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0d0fd7c6e093f7b804fa0150b875b868-Supplemental.zip},
 title = {Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0d0fd7c6e093f7b804fa0150b875b868-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0e9fa1f3,
 abstract = {We propose an approach to multiple-instance learning that reformulates the problem as a convex optimization on the likelihood ratio between the positive and the negative class for each training instance. This is casted as joint estimation of both a likelihood ratio predictor and the target (likelihood ratio variable) for instances. Theoretically, we prove a quantitative relationship between the risk estimated under the 0-1 classification loss, and under a loss function for likelihood ratio. It is shown that likelihood ratio estimation is generally a good surrogate for the 0-1 loss, and separates positive and negative instances well. The likelihood ratio estimates provide a ranking of instances within a bag and are used as input features to learn a linear classifier on bags of instances. Instance-level classification is achieved from the bag-level predictions and the individual likelihood ratios. Experiments on synthetic and real datasets demonstrate the competitiveness of the approach.},
 author = {Li, Fuxin and Sminchisescu, Cristian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0e9fa1f3e9e66792401a6972d477dcc3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0e9fa1f3e9e66792401a6972d477dcc3-Metadata.json},
 openalex = {W2113295315},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0e9fa1f3e9e66792401a6972d477dcc3-Supplemental.zip},
 title = {Convex Multiple-Instance Learning by Estimating Likelihood Ratio},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0e9fa1f3e9e66792401a6972d477dcc3-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0f2c9a93,
 abstract = {Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of orthogonal recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance.},
 author = {Ganguli, Surya and Sompolinsky, Haim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0f2c9a93eea6f38fabb3acb1c31488c6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0f2c9a93eea6f38fabb3acb1c31488c6-Metadata.json},
 openalex = {W2131037016},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0f2c9a93eea6f38fabb3acb1c31488c6-Supplemental.zip},
 title = {Short-term memory in neuronal networks through dynamical compressed sensing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0f2c9a93eea6f38fabb3acb1c31488c6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0f9cafd0,
 abstract = {Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (e.g. the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different schools of thought amongst the annotators, and can group together images belonging to separate categories.},
 author = {Welinder, Peter and Branson, Steve and Perona, Pietro and Belongie, Serge},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0f9cafd014db7a619ddb4276af0d692c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0f9cafd014db7a619ddb4276af0d692c-Metadata.json},
 openalex = {W2149273804},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0f9cafd014db7a619ddb4276af0d692c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Multidimensional Wisdom of Crowds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0f9cafd014db7a619ddb4276af0d692c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_0ff39bbb,
 abstract = {The problem of optimal and automatic design of a detector cascade is considered. A novel mathematical model is introduced for a cascaded detector. This model is analytically tractable, leads to recursive computation, and accounts for both classification and complexity. A boosting algorithm, FCBoost, is proposed for fully automated cascade design. It exploits the new cascade model, minimizes a Lagrangian cost that accounts for both classification risk and complexity. It searches the space of cascade configurations to automatically determine the optimal number of stages and their predictors, and is compatible with bootstrapping of negative examples and cost sensitive learning. Experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems.},
 author = {Vasconcelos, Nuno and Saberian, Mohammad},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0ff39bbbf981ac0151d340c9aa40e63e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0ff39bbbf981ac0151d340c9aa40e63e-Metadata.json},
 openalex = {W2161841481},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Boosting Classifier Cascades},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/0ff39bbbf981ac0151d340c9aa40e63e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_115f8950,
 abstract = {Estimating 3D pose from monocular images is a highly ambiguous problem. Physical constraints can be exploited to restrict the space of feasible configurations. In this paper we propose an approach to constraining the prediction of a discriminative predictor. We first show that the mean prediction of a Gaussian process implicitly satisfies linear constraints if those constraints are satisfied by the training examples. We then show how, by performing a change of variables, a GP can be forced to satisfy quadratic constraints. As evidenced by the experiments, our method outperforms state-of-the-art approaches on the tasks of rigid and non-rigid pose estimation.},
 author = {Salzmann, Mathieu and Urtasun, Raquel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/115f89503138416a242f40fb7d7f338e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/115f89503138416a242f40fb7d7f338e-Metadata.json},
 openalex = {W2170045362},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/115f89503138416a242f40fb7d7f338e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/115f89503138416a242f40fb7d7f338e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_11b921ef,
 abstract = {When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modifications in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein's matching law. Loewenstein & Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synap-tic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufficiently strong so that the fluctuations in input from individual sensory neurons influence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.},
 author = {Katahira, Kentaro and Okanoya, Kazuo and Okada, Masato},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/11b921ef080f7736089c757404650e40-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/11b921ef080f7736089c757404650e40-Metadata.json},
 openalex = {W2163795947},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/11b921ef080f7736089c757404650e40-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/11b921ef080f7736089c757404650e40-Supplemental.zip},
 title = {Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/11b921ef080f7736089c757404650e40-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_13f3cf8c,
 abstract = {The reinforcement learning community has explored many approaches to obtaining value estimates and models to guide decision making; these approaches, however, do not usually provide a measure of confidence in the estimate. Accurate estimates of an agent's confidence are useful for many applications, such as biasing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning. Computing confidence intervals on reinforcement learning value estimates, however, is challenging because data generated by the agent-environment interaction rarely satisfies traditional assumptions. Samples of value-estimates are dependent, likely non-normally distributed and often limited, particularly in early learning when confidence estimates are pivotal. In this work, we investigate how to compute robust confidences for value estimates in continuous Markov decision processes. We illustrate how to use bootstrapping to compute confidence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions. We demonstrate the applicability of our confidence estimation algorithms with experiments on exploration, parameter estimation and tracking.},
 author = {White, Martha and White, Adam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/13f3cf8c531952d72e5847c4183e6910-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/13f3cf8c531952d72e5847c4183e6910-Metadata.json},
 openalex = {W2105114545},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/13f3cf8c531952d72e5847c4183e6910-Supplemental.zip},
 title = {Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/13f3cf8c531952d72e5847c4183e6910-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_140f6969,
 author = {Li, Li-jia and Su, Hao and Fei-fei, Li and Xing, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/140f6969d5213fd0ece03148e62e461e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/140f6969d5213fd0ece03148e62e461e-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Object Bank: A High-Level Image Representation for Scene Classification \&amp; Semantic Feature Sparsification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_15d4e891,
 abstract = {This paper presents an approach to the visual recognition of human actions using only single images as input. The task is easy for humans but difficult for current approaches to object recognition, because instances of different actions may be similar in terms of body pose, and often require detailed examination of relations between participating objects and body parts in order to be recognized. The proposed approach applies a two-stage interpretation procedure to each training and test image. The first stage produces accurate detection of the relevant body parts of the actor, forming a prior for the local evidence needed to be considered for identifying the action. The second stage extracts features that are anchored to the detected body parts, and uses these features and their feature-to-part relations in order to recognize the action. The body anchored priors we propose apply to a large range of human actions. These priors allow focusing on the relevant regions and relations, thereby significantly simplifying the learning process and increasing recognition performance.},
 author = {Karlinsky, Leonid and Dinerstein, Michael and Ullman, Shimon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/15d4e891d784977cacbfcbb00c48f133-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/15d4e891d784977cacbfcbb00c48f133-Metadata.json},
 openalex = {W2146236261},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/15d4e891d784977cacbfcbb00c48f133-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/15d4e891d784977cacbfcbb00c48f133-Supplemental.zip},
 title = {Using body-anchored priors for identifying actions in single images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/15d4e891d784977cacbfcbb00c48f133-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_168908dd,
 abstract = {Recent work has demonstrated that when artificial agents are limited in their ability to achieve their goals, the agent designer can benefit by making the agent's goals different from the designer's. This gives rise to the optimization problem of designing the artificial agent's goals—in the RL framework, designing the agent's reward function. Existing attempts at solving this optimal reward problem do not leverage experience gained online during the agent's lifetime nor do they take advantage of knowledge about the agent's structure. In this work, we develop a gradient ascent approach with formal convergence guarantees for approximately solving the optimal reward problem online during an agent's lifetime. We show that our method generalizes a standard policy gradient approach, and we demonstrate its ability to improve reward functions in agents with various forms of limitations.},
 author = {Sorg, Jonathan and Lewis, Richard L and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/168908dd3227b8358eababa07fcaf091-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/168908dd3227b8358eababa07fcaf091-Metadata.json},
 openalex = {W2163602945},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/168908dd3227b8358eababa07fcaf091-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Reward Design via Online Gradient Ascent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/168908dd3227b8358eababa07fcaf091-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_182be0c5,
 abstract = {Steinwart was the first to prove universal consistency of support vector machine classification. His proof analyzed the 'standard' support vector machine classifier, which is restricted to binary classification problems. In contrast, recent analysis has resulted in the common belief that several extensions of SVM classification to more than two classes are inconsistent.

Countering this belief, we prove the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart's techniques to the multi-class case.},
 author = {Glasmachers, Tobias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/182be0c5cdcd5072bb1864cdee4d3d6e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/182be0c5cdcd5072bb1864cdee4d3d6e-Metadata.json},
 openalex = {W2110127684},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Universal Consistency of Multi-Class Support Vector Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/182be0c5cdcd5072bb1864cdee4d3d6e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_18997733,
 abstract = {We focus on microarray data where experiments monitor gene expression in different tissues and where each experiment is equipped with an additional response variable such as a cancer type. Although the number of measured genes is in the thousands, it is assumed that only a few marker components of gene subsets determine the type of a tissue. Here we present a new method for finding such groups of genes by directly incorporating the response variables into the grouping process, yielding a supervised clustering algorithm for genes.An empirical study on eight publicly available microarray datasets shows that our algorithm identifies gene clusters with excellent predictive potential, often superior to classification with state-of-the-art methods based on single genes. Permutation tests and bootstrapping provide evidence that the output is reasonably stable and more than a noise artifact.In contrast to other methods such as hierarchical clustering, our algorithm identifies several gene clusters whose expression levels clearly distinguish the different tissue types. The identification of such gene clusters is potentially useful for medical diagnostics and may at the same time reveal insights into functional genomics.},
 author = {Awasthi, Pranjal and Zadeh, Reza},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/18997733ec258a9fcaf239cc55d53363-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/18997733ec258a9fcaf239cc55d53363-Metadata.json},
 openalex = {W2141025089},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Supervised clustering of genes.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/18997733ec258a9fcaf239cc55d53363-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_19b65066,
 abstract = {How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of fluctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses.},
 author = {Rajan, Kanaka and Abbott, L and Sompolinsky, Haim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/19b650660b253761af189682e03501dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/19b650660b253761af189682e03501dd-Metadata.json},
 openalex = {W2138618236},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/19b650660b253761af189682e03501dd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/19b650660b253761af189682e03501dd-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_19f3cd30,
 abstract = {We consider Markov decision processes where the values of the parameters are uncertain. This uncertainty is described by a sequence of nested sets (that is, each set contains the previous one), each of which corresponds to a probabilistic guarantee for a different confidence level. Consequently, a set of admissible probability distributions of the unknown parameters is specified. This formulation models the case where the decision maker is aware of and wants to exploit some (yet imprecise) a priori information of the distribution of parameters, and it arises naturally in practice where methods for estimating the confidence region of parameters abound. We propose a decision criterion based on distributional robustness: the optimal strategy maximizes the expected total reward under the most adversarial admissible probability distributions. We show that finding the optimal distributionally robust strategy can be reduced to the standard robust MDP where parameters are known to belong to a single uncertainty set; hence, it can be computed in polynomial time under mild technical conditions.},
 author = {Xu, Huan and Mannor, Shie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/19f3cd308f1455b3fa09a282e0d496f4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/19f3cd308f1455b3fa09a282e0d496f4-Metadata.json},
 openalex = {W2165428239},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Distributionally Robust Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/19f3cd308f1455b3fa09a282e0d496f4-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_1aa48fc4,
 abstract = {Probabilistic grammars are generative statistical models that are useful for compositional and sequential structures. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of the parameters of a fixed probabilistic grammar using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the un-supervised setting.},
 author = {Smith, Noah A and Cohen, Shay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Metadata.json},
 openalex = {W2096672602},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Supplemental.zip},
 title = {Empirical Risk Minimization with Approximations of Probabilistic Grammars},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/1aa48fc4880bb0c9b8a3bf979d3b917e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_1c1d4df5,
 abstract = {Computing a maximum a posteriori (MAP) assignment in graphical models is a crucial inference problem for many practical applications. Several provably convergent approaches have been successfully developed using linear programming (LP) relaxation of the MAP problem. We present an alternative approach, which transforms the MAP problem into that of inference in a mixture of simple Bayes nets. We then derive the Expectation Maximization (EM) algorithm for this mixture that also monotonically increases a lower bound on the MAP assignment until convergence. The update equations for the EM algorithm are remarkably simple, both conceptually and computationally, and can be implemented using a graph-based message passing paradigm similar to max-product computation. Experiments on the real-world protein design dataset show that EM's convergence rate is significantly higher than the previous LP relaxation based approach MPLP. EM also achieves a solution quality within 95% of optimal for most instances.},
 author = {Kumar, Akshat and Zilberstein, Shlomo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1c1d4df596d01da60385f0bb17a4a9e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1c1d4df596d01da60385f0bb17a4a9e0-Metadata.json},
 openalex = {W2112026143},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {MAP Estimation for Graphical Models by Likelihood Maximization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/1c1d4df596d01da60385f0bb17a4a9e0-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_1cc3633c,
 abstract = {We consider the problem of identifying an activation pattern in a complex, large-scale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of high-dimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size.},
 author = {Sharpnack, James and Singh, Aarti},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1cc3633c579a90cfdd895e64021e2163-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1cc3633c579a90cfdd895e64021e2163-Metadata.json},
 openalex = {W2161073512},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1cc3633c579a90cfdd895e64021e2163-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Identifying graph-structured activation patterns in networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/1cc3633c579a90cfdd895e64021e2163-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_1d72310e,
 abstract = {Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information-object and feature absolute size-can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a 2.1D local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata fields specifying camera intrinstics. We develop an efficient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata.},
 author = {Fritz, Mario and Saenko, Kate and Darrell, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1d72310edc006dadf2190caad5802983-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1d72310edc006dadf2190caad5802983-Metadata.json},
 openalex = {W2158522123},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1d72310edc006dadf2190caad5802983-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Size Matters: Metric Visual Search Constraints from Monocular Metadata},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/1d72310edc006dadf2190caad5802983-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_1e6e0a04,
 abstract = {We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise-free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near-optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2, a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the first competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non-uniform cost and their noise is correlated. We also propose EffECXtive, a particularly fast approximation of EC2, and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty.},
 author = {Golovin, Daniel and Krause, Andreas and Ray, Debajyoti},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1e6e0a04d20f50967c64dac2d639a577-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1e6e0a04d20f50967c64dac2d639a577-Metadata.json},
 openalex = {W2143203060},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Near-Optimal Bayesian Active Learning with Noisy Observations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/1e6e0a04d20f50967c64dac2d639a577-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_1f4477ba,
 abstract = {Experts (human or computer) are often required to assess the probability of uncertain events. When a collection of experts independently assess events that are structurally interrelated, the resulting assessment may violate fundamental laws of probability. Such an assessment is termed incoherent. In this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence.},
 author = {Jones, Peter and Saligrama, Venkatesh and Mitter, Sanjoy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1f4477bad7af3616c1f933a02bfabe4e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1f4477bad7af3616c1f933a02bfabe4e-Metadata.json},
 openalex = {W2130104355},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Probabilistic Belief Revision with Structural Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_1f50893f,
 abstract = {We present a novel probabilistic model for distributions over sets of structures— for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of structured probabilistic models, like Markov random fields and context free grammars, with determinantal point processes, which arise in quantum physics as models of particles with repulsive interactions. We extend the determinantal point process model to handle an exponentially-sized set of particles (structures) via a natural factorization of the model into parts. We show how this factorization leads to tractable algorithms for exact inference, including computing marginals, computing conditional probabilities, and sampling. Our algorithms exploit a novel polynomially-sized dual representation of determinantal point processes, and use message passing over a special semiring to compute relevant quantities. We illustrate the advantages of the model on tracking and articulated pose estimation problems.},
 author = {Kulesza, Alex and Taskar, Ben},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1f50893f80d6830d62765ffad7721742-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1f50893f80d6830d62765ffad7721742-Metadata.json},
 openalex = {W2154090186},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1f50893f80d6830d62765ffad7721742-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/1f50893f80d6830d62765ffad7721742-Supplemental.zip},
 title = {Structured Determinantal Point Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/1f50893f80d6830d62765ffad7721742-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_208e43f0,
 abstract = {Computing1 two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest 6 bits (where b ≥ 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data. Our analysis shows that b-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance.},
 author = {Li, Ping and Konig, Arnd and Gui, Wenhao},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/208e43f0e45c4c78cafadb83d2888cb6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/208e43f0e45c4c78cafadb83d2888cb6-Metadata.json},
 openalex = {W2126887541},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {b-Bit Minwise Hashing for Estimating Three-Way Similarities},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/208e43f0e45c4c78cafadb83d2888cb6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_20f07591,
 abstract = {Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent flickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and find out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation.},
 author = {Yu, Stella},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/20f07591c6fcb220ffe637cda29bb3f6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/20f07591c6fcb220ffe637cda29bb3f6-Metadata.json},
 openalex = {W2144573618},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/20f07591c6fcb220ffe637cda29bb3f6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/20f07591c6fcb220ffe637cda29bb3f6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_242c100d,
 abstract = {Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efficient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). We introduce a simple approximate inference method for AR-HMMs, which we believe has applications in other problem domains. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes significantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes.},
 author = {Barber, Chris and Bockhorst, Joseph and Roebber, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/242c100dc94f871b6d7215b868a875f8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/242c100dc94f871b6d7215b868a875f8-Metadata.json},
 openalex = {W2099087188},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/242c100dc94f871b6d7215b868a875f8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/242c100dc94f871b6d7215b868a875f8-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_24896ee4,
 abstract = {We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology.},
 author = {Richard, Emile and Baskiotis, Nicolas and Evgeniou, Theodoros and Vayatis, Nicolas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/24896ee4c6526356cc127852413ea3b4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/24896ee4c6526356cc127852413ea3b4-Metadata.json},
 openalex = {W2158812535},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/24896ee4c6526356cc127852413ea3b4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Link Discovery using Graph Feature Tracking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/24896ee4c6526356cc127852413ea3b4-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_25b2822c,
 abstract = {We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-fire neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientific experiments, the focus lays on parameterizability and reproduction of the analytical model.},
 author = {Millner, Sebastian and Gr\"{u}bl, Andreas and Meier, Karlheinz and Schemmel, Johannes and Schwartz, Marc-olivier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/25b2822c2f5a3230abfadd476e8b04c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/25b2822c2f5a3230abfadd476e8b04c9-Metadata.json},
 openalex = {W2122872855},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/25b2822c2f5a3230abfadd476e8b04c9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/25b2822c2f5a3230abfadd476e8b04c9-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2723d092,
 abstract = {Gaussian graphical models are of great interest in statistical learning. Because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an $\ell_1$-regularization term. In this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem's special structure; in particular, the subproblems solved in each iteration have closed-form solutions. Moreover, our algorithm obtains an $\epsilon$-optimal solution in $O(1/\epsilon)$ iterations. Numerical experiments on both synthetic and real data from gene association networks show that a practical version of this algorithm outperforms other competitive algorithms.},
 author = {Scheinberg, Katya and Ma, Shiqian and Goldfarb, Donald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2723d092b63885e0d7c260cc007e8b9d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2723d092b63885e0d7c260cc007e8b9d-Metadata.json},
 openalex = {W2148974830},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2723d092b63885e0d7c260cc007e8b9d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2723d092b63885e0d7c260cc007e8b9d-Supplemental.zip},
 title = {Sparse Inverse Covariance Selection via Alternating Linearization Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2723d092b63885e0d7c260cc007e8b9d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_27ed0fb9,
 abstract = {We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a change-point detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further refined through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction.},
 author = {Konidaris, George and Kuindersma, Scott and Grupen, Roderic and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/27ed0fb950b856b06e1273989422e7d3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/27ed0fb950b856b06e1273989422e7d3-Metadata.json},
 openalex = {W2172131460},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/27ed0fb950b856b06e1273989422e7d3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/27ed0fb950b856b06e1273989422e7d3-Supplemental.zip},
 title = {Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/27ed0fb950b856b06e1273989422e7d3-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_286674e3,
 author = {Sayedi, Amin and Zadimoghaddam, Morteza and Blum, Avrim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/286674e3082feb7e5afb92777e48821f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/286674e3082feb7e5afb92777e48821f-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/286674e3082feb7e5afb92777e48821f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Trading off Mistakes and Don\textquotesingle t-Know Predictions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/286674e3082feb7e5afb92777e48821f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_287e03db,
 abstract = {A method for computing the rarity of latent fingerprints represented by minutiae is given. It allows determining the probability of finding a match for an evidence print in a database of n known prints. The probability of random correspondence between evidence and database is determined in three procedural steps. In the registration step the latent print is aligned by finding its core point; which is done using a procedure based on a machine learning approach based on Gaussian processes. In the evidence probability evaluation step a generative model based on Bayesian networks is used to determine the probability of the evidence; it takes into account both the dependency of each minutia on nearby minutiae and the confidence of their presence in the evidence. In the specific probability of random correspondence step the evidence probability is used to determine the probability of match among n for a given tolerance; the last evaluation is similar to the birthday correspondence probability for a specific birthday. The generative model is validated using a goodness-of-fit test evaluated with a standard database of fingerprints. The probability of random correspondence for several latent fingerprints are evaluated for varying numbers of minutiae.},
 author = {Su, Chang and Srihari, Sargur},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/287e03db1d99e0ec2edb90d079e142f3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/287e03db1d99e0ec2edb90d079e142f3-Metadata.json},
 openalex = {W2127565930},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/287e03db1d99e0ec2edb90d079e142f3-Supplemental.zip},
 title = {Evaluation of Rarity of Fingerprints in Forensics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/287e03db1d99e0ec2edb90d079e142f3-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_289dff07,
 abstract = {We combine random forest (RF) and conditional random field (CRF) into a new computational framework, called random forest random field (RF)2. Inference of (RF)2 uses the Swendsen-Wang cut algorithm, characterized by Metropolis-Hastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a non-parametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)2. (RF)2 is applied to a challenging task of multiclass object recognition and segmentation over a random field of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.},
 author = {Payet, Nadia and Todorovic, Sinisa},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/289dff07669d7a23de0ef88d2f7129e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/289dff07669d7a23de0ef88d2f7129e7-Metadata.json},
 openalex = {W2168189154},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/289dff07669d7a23de0ef88d2f7129e7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {(RF)^2 -- Random Forest Random Field},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/289dff07669d7a23de0ef88d2f7129e7-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_298923c8,
 abstract = {When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches for minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is hard to compute, and so is the projection operator that approximates it, we describe another second-order retraction that can be computed efficiently, with run time and memory complexity of O ((n + m)k) for a rank-k matrix of dimension m x n, given rank-one gradients. We use this algorithm, LORETA, to learn a matrix-form similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive- aggressive approach in a factorized model, and also improves over a full model trained over pre-selected features using the same memory requirements. LORETA also showed consistent improvement over standard methods in a large (1600 classes) multi-label image classification task.},
 author = {Shalit, Uri and Weinshall, Daphna and Chechik, Gal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/298923c8190045e91288b430794814c4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/298923c8190045e91288b430794814c4-Metadata.json},
 openalex = {W2160634266},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/298923c8190045e91288b430794814c4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/298923c8190045e91288b430794814c4-Supplemental.zip},
 title = {Online Learning in The Manifold of Low-Rank Matrices},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/298923c8190045e91288b430794814c4-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2a084e55,
 abstract = {We propose a new variational EM algorithm for fitting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is significantly faster than previous variational methods. We show that EM is significantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further benefit of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method.},
 author = {Khan, Mohammad Emtiyaz E and Bouchard, Guillaume and Murphy, Kevin P and Marlin, Benjamin M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Metadata.json},
 openalex = {W2095999217},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2a084e55c87b1ebcdaad1f62fdbbac8e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Variational bounds for mixed-data factor analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2a084e55c87b1ebcdaad1f62fdbbac8e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2a79ea27,
 abstract = {We present the Copula Bayesian Network model for representing multivariate continuous distributions, while taking advantage of the relative ease of estimating univariate distributions. Using a novel copula-based reparameterization of a conditional density, joined with a graph that encodes independencies, our model offers great flexibility in modeling high-dimensional densities, while maintaining control over the form of the univariate marginals. We demonstrate the advantage of our framework for generalization over standard Bayesian networks as well as tree structured copula models for varied real-life domains that are of substantially higher dimension than those typically considered in the copula literature.},
 author = {Elidan, Gal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2a79ea27c279e471f4d180b08d62b00a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2a79ea27c279e471f4d180b08d62b00a-Metadata.json},
 openalex = {W2131203126},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2a79ea27c279e471f4d180b08d62b00a-Supplemental.zip},
 title = {Copula Bayesian Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2a79ea27c279e471f4d180b08d62b00a-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2ac2406e,
 abstract = {We present a simple and effective approach to learning tractable conditional random fields with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efficient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to fixed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup.},
 author = {Chechetka, Anton and Guestrin, Carlos},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2ac2406e835bd49c70469acae337d292-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2ac2406e835bd49c70469acae337d292-Metadata.json},
 openalex = {W2144742221},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2ac2406e835bd49c70469acae337d292-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Evidence-Specific Structures for Rich Tractable CRFs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2ac2406e835bd49c70469acae337d292-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2b44928a,
 abstract = {We propose a discriminative model for recognizing group activities. Our model jointly captures the group activity, the individual person actions, and the interactions among them. Two new types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. Different from most of the previous latent structured models which assume a predefined structure for the hidden layer, e.g. a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. Our experimental results demonstrate that by inferring this contextual information together with adaptive structures, the proposed model can significantly improve activity recognition performance.},
 author = {Lan, Tian and Wang, Yang and Yang, Weilong and Mori, Greg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2b44928ae11fb9384c4cf38708677c48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2b44928ae11fb9384c4cf38708677c48-Metadata.json},
 openalex = {W2163415258},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2b44928ae11fb9384c4cf38708677c48-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Actions: Discriminative Models for Contextual Group Activities},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2b44928ae11fb9384c4cf38708677c48-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2b8a6159,
 abstract = {Several motor related Brain Computer Interfaces (BCIs) have been developed over the years that use activity decoded from the contralateral hemisphere to operate devices. Contralateral primary motor cortex is also the region most severely affected by hemispheric stroke. Recent studies have identified ipsilateral cortical activity in planning of motor movements and its potential implications for a stroke relevant BCI. The most fundamental functional loss after a hemispheric stroke is the loss of fine motor control of the hand. Thus, whether ipsilateral cortex encodes finger movements is critical to the potential feasibility of BCI approaches in the future. This study uses ipsilateral cortical signals from humans (using ECoG) to decode finger movements. We demonstrate, for the first time, successful finger movement detection using machine learning algorithms. Our results show high decoding accuracies in all cases which are always above chance. We also show that significant accuracies can be achieved with the use of only a fraction of all the features recorded and that these core features are consistent with previous physiological findings. The results of this study have substantial implications for advancing neuroprosthetic approaches to stroke populations not currently amenable to existing BCI techniques.},
 author = {Liu, Yuzong and Sharma, Mohit and Gaona, Charles and Breshears, Jonathan and Roland, Jarod and Freudenburg, Zachary and Leuthardt, Eric and Weinberger, Kilian Q},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2b8a61594b1f4c4db0902a8a395ced93-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2b8a61594b1f4c4db0902a8a395ced93-Metadata.json},
 openalex = {W2161164684},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Decoding Ipsilateral Finger Movements from ECoG Signals in Humans},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2b8a61594b1f4c4db0902a8a395ced93-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2bcab9d9,
 abstract = {We propose a general framework to online learning for classification problems with time-varying potential functions in the adversarial setting. This framework allows to design and prove relative mistake bounds for any generic loss function. The mistake bounds can be specialized for the hinge loss, allowing to recover and improve the bounds of known online classification algorithms. By optimizing the general bound we derive a new online classification algorithm, called NAROW, that hybridly uses adaptive- and fixed- second order information. We analyze the properties of the algorithm and illustrate its performance using synthetic dataset.},
 author = {Orabona, Francesco and Crammer, Koby},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2bcab9d935d219641434683dd9d18a03-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2bcab9d935d219641434683dd9d18a03-Metadata.json},
 openalex = {W2126561871},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {New Adaptive Algorithms for Online Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2bcab9d935d219641434683dd9d18a03-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2ca65f58,
 abstract = {Automatic speech recognition has gradually improved over the years, but the reliable recognition of unconstrained speech is still not within reach. In order to achieve a breakthrough, many research groups are now investigating new methodologies that have potential to outperform the Hidden Markov Model technology that is at the core of all present commercial systems. In this paper, it is shown that the recently introduced concept of Reservoir Computing might form the basis of such a methodology. In a limited amount of time, a reservoir system that can recognize the elementary sounds of continuous speech has been built. The system already achieves a state-of-the-art performance, and there is evidence that the margin for further improvements is still significant.},
 author = {Triefenbach, Fabian and Jalalvand, Azarakhsh and Schrauwen, Benjamin and Martens, Jean-pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Metadata.json},
 openalex = {W2171306489},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Phoneme Recognition with Large Hierarchical Reservoirs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2cbca448,
 abstract = {We present a novel method for multitask learning (MTL) based on manifold regularization: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common linear subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is fixed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efficient and easy to implement. We show the efficacy of our method on several datasets.},
 author = {Agarwal, Arvind and Gerber, Samuel and Daume, Hal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2cbca44843a864533ec05b321ae1f9d1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2cbca44843a864533ec05b321ae1f9d1-Metadata.json},
 openalex = {W2108873311},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2cbca44843a864533ec05b321ae1f9d1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning Multiple Tasks using Manifold Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2cbca44843a864533ec05b321ae1f9d1-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2d6cc4b2,
 abstract = {We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefficients. Each coefficient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efficient inference procedures for both the coefficients and the scale parameter. When the scale parameters of a group of coefficients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude fluctuations among coefficients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefficients follows a divisive normalization rule, and that this may be efficiently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model.},
 author = {Garrigues, Pierre and Olshausen, Bruno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Metadata.json},
 openalex = {W2144759647},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Supplemental.zip},
 title = {Group Sparse Coding with a Laplacian Scale Mixture Prior},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2d6cc4b2d139a53512fb8cbb3086ae2e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_2dace78f,
 abstract = {Recent experimental work has suggested that the neural firing rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron suffices to carry out such an approximation, given a suitable refractory response. Empirically, we find that the online approximation of signals with a sum of power-law kernels is beneficial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spike-trains by a receiving neuron allows for natural and transparent temporal signal filtering by tuning the weights of the decoding kernel.},
 author = {Rombouts, Jaldert and Bohte, Sander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2dace78f80bc92e6d7493423d729448e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2dace78f80bc92e6d7493423d729448e-Metadata.json},
 openalex = {W2154684016},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fractionally Predictive Spiking Neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/2dace78f80bc92e6d7493423d729448e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_301ad0e3,
 abstract = {A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.},
 author = {Liu, Han and Roeder, Kathryn and Wasserman, Larry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/301ad0e3bd5cb1627a2044908a42fdc2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/301ad0e3bd5cb1627a2044908a42fdc2-Metadata.json},
 openalex = {W2163702333},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/301ad0e3bd5cb1627a2044908a42fdc2-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_32b30a25,
 abstract = {The problem of learning to predict structured labels is of key importance in many applications. However, for general graph structure both learning and inference are intractable. Here we show that it is possible to circumvent this difficulty when the distribution of training examples is rich enough, via a method similar in spirit to pseudo-likelihood. We show that our new method achieves consistency, and illustrate empirically that it indeed approaches the performance of exact methods when sufficiently large training sets are used.},
 author = {Sontag, David and Meshi, Ofer and Globerson, Amir and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/32b30a250abd6331e03a2a1f16466346-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/32b30a250abd6331e03a2a1f16466346-Metadata.json},
 openalex = {W2098708428},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/32b30a250abd6331e03a2a1f16466346-Supplemental.zip},
 title = {More data means less inference: A pseudo-max approach to structured learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/32b30a250abd6331e03a2a1f16466346-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_33e8075e,
 abstract = {Lifted Inference algorithms for representations that combine first-order logic and graphical models have been the focus of much recent research. All lifted algorithms developed to date are based on the same underlying idea: take a standard probabilistic inference algorithm (e.g., variable elimination, belief propagation etc.) and improve its efficiency by exploiting repeated structure in the first-order model. In this paper, we propose an approach from the other side in that we use techniques from logic for probabilistic inference. In particular, we define a set of rules that look only at the logical representation to identify models for which exact efficient inference is possible. Our rules yield new tractable classes that could not be solved efficiently by any of the existing techniques.},
 author = {Jha, Abhay and Gogate, Vibhav and Meliou, Alexandra and Suciu, Dan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/33e8075e9970de0cfea955afd4644bb2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/33e8075e9970de0cfea955afd4644bb2-Metadata.json},
 openalex = {W2115836268},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Lifted Inference Seen from the Other Side : The Tractable Features},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_3473decc,
 abstract = {We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.},
 author = {Boots, Byron and Gordon, Geoffrey J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3473decccb0509fb264818a7512a8b9b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3473decccb0509fb264818a7512a8b9b-Metadata.json},
 openalex = {W2949683464},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3473decccb0509fb264818a7512a8b9b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Predictive State Temporal Difference Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/3473decccb0509fb264818a7512a8b9b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_35051070,
 abstract = {In system identification both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing filter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identification of the dendritic processing filter and reconstruct its kernel with arbitrary precision.},
 author = {Lazar, Aurel A and Slutskiy, Yevgeniy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35051070e572e47d2c26c241ab88307f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35051070e572e47d2c26c241ab88307f-Metadata.json},
 openalex = {W2114280454},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35051070e572e47d2c26c241ab88307f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Identifying Dendritic Processing.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/35051070e572e47d2c26c241ab88307f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_35cf8659,
 abstract = {Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U(θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U(θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds.},
 author = {Jie, Tang and Abbeel, Pieter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35cf8659cfcb13224cbd47863a34fc58-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35cf8659cfcb13224cbd47863a34fc58-Metadata.json},
 openalex = {W2162262334},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35cf8659cfcb13224cbd47863a34fc58-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/35cf8659cfcb13224cbd47863a34fc58-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_35f4a8d4,
 abstract = {Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difficult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We first embed each brain into a functional map that reflects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions.},
 author = {Langs, Georg and Tie, Yanmei and Rigolo, Laura and Golby, Alexandra and Golland, Polina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35f4a8d465e6e1edc05f3d8ab658c551-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35f4a8d465e6e1edc05f3d8ab658c551-Metadata.json},
 openalex = {W2159302141},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/35f4a8d465e6e1edc05f3d8ab658c551-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Functional Geometry Alignment and Localization of Brain Areas.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/35f4a8d465e6e1edc05f3d8ab658c551-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_3621f145,
 abstract = {The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be $\widetilde{O}(\log\frac{1}ε)$, contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is $\widetilde{O}(\frac{1}ε)$, where the order of $1/ε$ is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of $1/ε$ is related to the parameter in Tsybakov noise.},
 author = {Wang, Wei and Zhou, Zhi-Hua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3621f1454cacf995530ea53652ddf8fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3621f1454cacf995530ea53652ddf8fb-Metadata.json},
 openalex = {W2953223201},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3621f1454cacf995530ea53652ddf8fb-Supplemental.zip},
 title = {Multi-View Active Learning in the Non-Realizable Case},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_37a749d8,
 abstract = {We study the problem of segmenting specific white matter structures of interest from Diffusion Tensor (DT-MR) images of the human brain. This is an important requirement in many Neuroimaging studies: for instance, to evaluate whether a brain structure exhibits group level differences as a function of disease in a set of images. Typically, interactive expert guided segmentation has been the method of choice for such applications, but this is tedious for large datasets common today. To address this problem, we endow an image segmentation algorithm with "advice" encoding some global characteristics of the region(s) we want to extract. This is accomplished by constructing (using expert-segmented images) an epitome of a specific region - as a histogram over a bag of 'words' (e.g., suitable feature descriptors). Now, given such a representation, the problem reduces to segmenting a new brain image with additional constraints that enforce consistency between the segmented foreground and the pre-specified histogram over features. We present combinatorial approximation algorithms to incorporate such domain specific constraints for Markov Random Field (MRF) segmentation. Making use of recent results on image co-segmentation, we derive effective solution strategies for our problem. We provide an analysis of solution quality, and present promising experimental evidence showing that many structures of interest in Neuroscience can be extracted reliably from 3-D brain image volumes using our algorithm.},
 author = {Motwani, Kamiya and Adluru, Nagesh and Hinrichs, Chris and Alexander, Andrew and Singh, Vikas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/37a749d808e46495a8da1e5352d03cae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/37a749d808e46495a8da1e5352d03cae-Metadata.json},
 openalex = {W2138988757},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/37a749d808e46495a8da1e5352d03cae-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_39059724,
 abstract = {A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive field of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit.},
 author = {Druckmann, Shaul and Chklovskii, Dmitri},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/39059724f73a9969845dfe4146c5660e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/39059724f73a9969845dfe4146c5660e-Metadata.json},
 openalex = {W2134278657},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/39059724f73a9969845dfe4146c5660e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Over-complete representations on recurrent neural networks can support persistent percepts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/39059724f73a9969845dfe4146c5660e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_390e9825,
 abstract = {We consider bandit problems, motivated by applications in online advertising and news story selection, in which the learner must repeatedly select a slate, that is, a subset of size s from K possible actions, and then receives rewards for just the selected actions. The goal is to minimize the regret with respect to total reward of the best slate computed in hindsight. We consider unordered and ordered versions of the problem, and give efficient algorithms which have regret O(√T), where the constant depends on the specific nature of the problem. We also consider versions of the problem where we have access to a number of policies which make recommendations for slates in every round, and give algorithms with O(√T) regret for competing with the best such policy as well. We make use of the technique of relative entropy projections combined with the usual multiplicative weight update algorithm to obtain our algorithms.},
 author = {Kale, Satyen and Reyzin, Lev and Schapire, Robert E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/390e982518a50e280d8e2b535462ec1f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/390e982518a50e280d8e2b535462ec1f-Metadata.json},
 openalex = {W2122422466},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Non-Stochastic Bandit Slate Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/390e982518a50e280d8e2b535462ec1f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_39461a19,
 abstract = {Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an unbalanced prediction rule for scene classification. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efficiently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.},
 author = {Zhu, Jun and Li, Li-jia and Fei-fei, Li and Xing, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/39461a19e9eddfb385ea76b26521ea48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/39461a19e9eddfb385ea76b26521ea48-Metadata.json},
 openalex = {W2151800797},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/39461a19e9eddfb385ea76b26521ea48-Supplemental.zip},
 title = {Large Margin Learning of Upstream Scene Understanding Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/39461a19e9eddfb385ea76b26521ea48-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_3a029f04,
 abstract = {Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology.},
 author = {Alvarez, Mauricio and Peters, Jan and Lawrence, Neil and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3a029f04d76d32e79367c4b3255dda4d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3a029f04d76d32e79367c4b3255dda4d-Metadata.json},
 openalex = {W2162942763},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3a029f04d76d32e79367c4b3255dda4d-Supplemental.zip},
 title = {Switched Latent Force Models for Movement Segmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/3a029f04d76d32e79367c4b3255dda4d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_3cf166c6,
 abstract = {To understand the relationship between genomic variations among population and complex diseases, it is essential to detect eQTLs which are associated with phenotypic effects. However, detecting eQTLs remains a challenge due to complex underlying mechanisms and the very large number of genetic loci involved compared to the number of samples. Thus, to address the problem, it is desirable to take advantage of the structure of the data and prior information about genomic locations such as conservation scores and transcription factor binding sites.

In this paper, we propose a novel regularized regression approach for detecting eQTLs which takes into account related traits simultaneously while incorporating many regulatory features. We first present a Bayesian network for a multi-task learning problem that includes priors on SNPs, making it possible to estimate the significance of each covariate adaptively. Then we find the maximum a posteriori (MAP) estimation of regression coefficients and estimate weights of covariates jointly. This optimization procedure is efficient since it can be achieved by using a projected gradient descent and a coordinate descent procedure iteratively. Experimental results on simulated and real yeast datasets confirm that our model outperforms previous methods for finding eQTLs.},
 author = {Lee, Seunghak and Zhu, Jun and Xing, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3cf166c6b73f030b4f67eeaeba301103-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3cf166c6b73f030b4f67eeaeba301103-Metadata.json},
 openalex = {W2156200251},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3cf166c6b73f030b4f67eeaeba301103-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Multi-Task Lasso: with Application to eQTL Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/3cf166c6b73f030b4f67eeaeba301103-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_3dc4876f,
 abstract = {We present a model that describes the structure in the responses of different brain areas to a set of stimuli in terms of stimulus categories (clusters of stimuli) and functional units (clusters of voxels). We assume that voxels within a unit respond similarly to all stimuli from the same category, and design a nonparametric hierarchical model to capture inter-subject variability among the units. The model explicitly encodes the relationship between brain activations and fMRI time courses. A variational inference algorithm derived based on the model learns categories, units, and a set of unit-category activation probabilities from data. When applied to data from an fMRI study of object recognition, the method finds meaningful and consistent clusterings of stimuli into categories and voxels into units.},
 author = {Lashkari, Danial and Sridharan, Ramesh and Golland, Polina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3dc4876f3f08201c7c76cb71fa1da439-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3dc4876f3f08201c7c76cb71fa1da439-Metadata.json},
 openalex = {W2108515610},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3dc4876f3f08201c7c76cb71fa1da439-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3dc4876f3f08201c7c76cb71fa1da439-Supplemental.zip},
 title = {Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/3dc4876f3f08201c7c76cb71fa1da439-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_3def184a,
 abstract = {The Random Projection Tree (RPTREE) structures proposed in [1] are space partitioning data structures that automatically adapt to various notions of intrinsic dimensionality of data. We prove new results for both the RPTREE-MAX and the RPTREE-MEAN data structures. Our result for RPTREE-MAX gives a near-optimal bound on the number of levels required by this data structure to reduce the size of its cells by a factor s ≥ 2. We also prove a packing lemma for this data structure. Our final result shows that low-dimensional manifolds have bounded Local Covariance Dimension. As a consequence we show that RPTREE-MEAN adapts to manifold dimension as well.},
 author = {Dhesi, Aman and Kar, Purushottam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3def184ad8f4755ff269862ea77393dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3def184ad8f4755ff269862ea77393dd-Metadata.json},
 openalex = {W2962923175},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/3def184ad8f4755ff269862ea77393dd-Supplemental.zip},
 title = {Random Projection Trees Revisited},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/3def184ad8f4755ff269862ea77393dd-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_41ae36ec,
 abstract = {We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here defined by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efficient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives.},
 author = {Wang, Eric and Liu, Dehong and Silva, Jorge and Carin, Lawrence and Dunson, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/41ae36ecb9b3eee609d05b90c14222fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/41ae36ecb9b3eee609d05b90c14222fb-Metadata.json},
 openalex = {W2131684395},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Joint Analysis of Time-Evolving Binary Matrices and Associated Documents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/41ae36ecb9b3eee609d05b90c14222fb-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_42998cf3,
 abstract = {Is there a principled way to learn a probabilistic discriminative classifier from an unlabeled data set? We present a framework that simultaneously clusters the data and trains a discriminative classifier. We call it Regularized Information Maximization (RIM). RIM optimizes an intuitive information-theoretic objective function which balances class separation, class balance and classifier complexity. The approach can flexibly incorporate different likelihood functions, express prior assumptions about the relative size of different classes and incorporate partial labels for semi-supervised learning. In particular, we instantiate the framework to un-supervised, multi-class kernelized logistic regression. Our empirical evaluation indicates that RIM outperforms existing methods on several real data sets, and demonstrates that RIM is an effective model selection method.},
 author = {Krause, Andreas and Perona, Pietro and Gomes, Ryan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/42998cf32d552343bc8e460416382dca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/42998cf32d552343bc8e460416382dca-Metadata.json},
 openalex = {W2097482982},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/42998cf32d552343bc8e460416382dca-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Discriminative Clustering by Regularized Information Maximization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/42998cf32d552343bc8e460416382dca-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_42a0e188,
 abstract = {To localise the source of a sound, we use location-specific properties of the signals received at the two ears caused by the asymmetric filtering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism's lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from filtered sounds. We present a spiking neural network model of sound localisation based on extracting location-specific synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difficult task of distinguishing sounds coming from the front and back.},
 author = {Goodman, Dan and Brette, Romain},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/42a0e188f5033bc65bf8d78622277c4e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/42a0e188f5033bc65bf8d78622277c4e-Metadata.json},
 openalex = {W2134871056},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning to localise sounds with spiking neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/42a0e188f5033bc65bf8d78622277c4e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_432aca3a,
 abstract = {We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks. Our proposed model abstracts observed time-varying object-object relationships into relationships between object clusters. We extend the infinite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.},
 author = {Ishiguro, Katsuhiko and Iwata, Tomoharu and Ueda, Naonori and Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/432aca3a1e345e339f35a30c8f65edce-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/432aca3a1e345e339f35a30c8f65edce-Metadata.json},
 openalex = {W2101065161},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Dynamic Infinite Relational Model for Time-varying Relational Data Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_43baa676,
 abstract = {We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difficult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a finite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail.},
 author = {Urry, Matthew and Sollich, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/43baa6762fa81bb43b39c62553b2970d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/43baa6762fa81bb43b39c62553b2970d-Metadata.json},
 openalex = {W2165615334},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/43baa6762fa81bb43b39c62553b2970d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Exact learning curves for Gaussian process regression on large random graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/43baa6762fa81bb43b39c62553b2970d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_4476b929,
 abstract = {This paper describes a probabilistic framework for studying associations between multiple genotypes, biomarkers, and phenotypic traits in the presence of noise and unobserved confounders for large genetic studies. The framework builds on sparse linear methods developed for regression and modified here for inferring causal structures of richer networks with latent variables. The method is motivated by the use of genotypes as to infer causal associations between phenotypic biomarkers and outcomes, without making the common restrictive assumptions of instrumental variable methods. The method may be used for an effective screening of potentially interesting genotype-phenotype and biomarker-phenotype associations in genome-wide studies, which may have important implications for validating biomarkers as possible proxy endpoints for early-stage clinical trials. Where the biomarkers are gene transcripts, the method can be used for fine mapping of quantitative trait loci (QTLs) detected in genetic linkage studies. The method is applied for examining effects of gene transcript levels in the liver on plasma HDL cholesterol levels for a sample of sequenced mice from a heterogeneous stock, with ~ 105 genetic instruments and ~ 47 x 103 gene transcripts.},
 author = {Mckeigue, Paul and Krohn, Jon and Storkey, Amos J and Agakov, Felix},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Metadata.json},
 openalex = {W2136905437},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sparse Instrumental Variables (SPIV) for Genome-Wide Studies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/4476b929e30dd0c4e8bdbcc82c6ba23a-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_44c4c173,
 abstract = {In this paper, we propose an efficient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods.},
 author = {Miyamae, Atsushi and Nagata, Yuichi and Ono, Isao and Kobayashi, Shigenobu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/44c4c17332cace2124a1a836d9fc4b6f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/44c4c17332cace2124a1a836d9fc4b6f-Metadata.json},
 openalex = {W2141083508},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/44c4c17332cace2124a1a836d9fc4b6f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/44c4c17332cace2124a1a836d9fc4b6f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_4558dbb6,
 abstract = {The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT [16] and HOG [3], are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a unified and principled framework to turn pixel attributes (gradient, color, local binary pattern, etc.) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA) [23]. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classification benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.},
 author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Metadata.json},
 openalex = {W2103444992},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Kernel Descriptors for Visual Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/4558dbb6f6f8bb2e16d03b85bde76e2c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_459a4ddc,
 abstract = {Continuous Markov random fields are a general formalism to model joint probability distributions over events with continuous outcomes. We prove that marginal computation for constrained continuous MRFs is #P-hard in general and present a polynomial-time approximation scheme under mild assumptions on the structure of the random field. Moreover, we introduce a sampling algorithm to compute marginal distributions and develop novel techniques to increase its efficiency. Continuous MRFs are a general purpose probabilistic modeling tool and we demonstrate how they can be applied to statistical relational learning. On the problem of collective classification, we evaluate our algorithm and show that the standard deviation of marginals serves as a useful measure of confidence.},
 author = {Broecheler, Matthias and Getoor, Lise},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/459a4ddcb586f24efd9395aa7662bc7c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/459a4ddcb586f24efd9395aa7662bc7c-Metadata.json},
 openalex = {W2114817827},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/459a4ddcb586f24efd9395aa7662bc7c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_46922a08,
 abstract = {Bayesian approaches to preference elicitation (PE) are particularly attractive due to their ability to explicitly model uncertainty in users' latent utility functions. However, previous approaches to Bayesian PE have ignored the important problem of generalizing from previous users to an unseen user in order to reduce the elicitation burden on new users. In this paper, we address this deficiency by introducing a Gaussian Process (GP) prior over users' latent utility functions on the joint space of user and item features. We learn the hyper-parameters of this GP on a set of preferences of previous users and use it to aid in the elicitation process for a new user. This approach provides a flexible model of a multi-user utility function, facilitates an efficient value of information (VOI) heuristic query selection strategy, and provides a principled way to incorporate the elicitations of multiple users back into the model. We show the effectiveness of our method in comparison to previous work on a real dataset of user preferences over sushi types.},
 author = {Guo, Shengbo and Sanner, Scott and Bonilla, Edwin V},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/46922a0880a8f11f8f69cbb52b1396be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/46922a0880a8f11f8f69cbb52b1396be-Metadata.json},
 openalex = {W2150905308},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Gaussian Process Preference Elicitation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/46922a0880a8f11f8f69cbb52b1396be-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_46fc943e,
 abstract = {Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the correct requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements.},
 author = {Mukherjee, Indraneel and Schapire, Robert E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/46fc943ecd56441056a560ba37d0b9e8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/46fc943ecd56441056a560ba37d0b9e8-Metadata.json},
 openalex = {W2570929519},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/46fc943ecd56441056a560ba37d0b9e8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/46fc943ecd56441056a560ba37d0b9e8-Supplemental.zip},
 title = {A theory of multiclass boosting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/46fc943ecd56441056a560ba37d0b9e8-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_470e7a4f,
 abstract = {We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyper-plane query. Both use hashing to retrieve near points in sub-linear time. Our first method's preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods' tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points.},
 author = {Jain, Prateek and Vijayanarasimhan, Sudheendra and Grauman, Kristen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/470e7a4f017a5476afb7eeb3f8b96f9b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/470e7a4f017a5476afb7eeb3f8b96f9b-Metadata.json},
 openalex = {W2126458834},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/470e7a4f017a5476afb7eeb3f8b96f9b-Supplemental.zip},
 title = {Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/470e7a4f017a5476afb7eeb3f8b96f9b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_47a65822,
 abstract = {We consider the problem of apprenticeship learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generalizing the demonstration, based on the assumption that the expert is maximizing a utility function that is a linear combination of state-action features. Most IRL algorithms use a simple Monte Carlo estimation to approximate the expected feature counts under the expert's policy. In this paper, we show that the quality of the learned policies is highly sensitive to the error in estimating the feature counts. To reduce this error, we introduce a novel approach for bootstrapping the demonstration by assuming that: (i), the expert is (near-)optimal, and (ii), the dynamics of the system is known. Empirical results on gridworlds and car racing problems show that our approach is able to learn good policies from a small number of demonstrations.},
 author = {Boularias, Abdeslam and Chaib-draa, Brahim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/47a658229eb2368a99f1d032c8848542-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/47a658229eb2368a99f1d032c8848542-Metadata.json},
 openalex = {W2115318338},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/47a658229eb2368a99f1d032c8848542-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Bootstrapping Apprenticeship Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/47a658229eb2368a99f1d032c8848542-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_4a213d37,
 abstract = {This paper presents a co-regularization based approach to semi-supervised domain adaptation. Our proposed approach (EA++) builds on the notion of augmented space (introduced in EASYADAPT (EA) [1]) and harnesses unlabeled data in target domain to further assist the transfer of information from source to target. This semi-supervised approach to domain adaptation is extremely simple to implement and can be applied as a pre-processing step to any supervised learner. Our theoretical analysis (in terms of Rademacher complexity) of EA and EA++ show that the hypothesis class of EA++ has lower complexity (compared to EA) and hence results in tighter generalization bounds. Experimental results on sentiment analysis tasks reinforce our theoretical findings and demonstrate the efficacy of the proposed method when compared to EA as well as few other representative baseline approaches.},
 author = {Kumar, Abhishek and Saha, Avishek and Daume, Hal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4a213d37242bdcad8e7300e202e7caa4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4a213d37242bdcad8e7300e202e7caa4-Metadata.json},
 openalex = {W2158751697},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4a213d37242bdcad8e7300e202e7caa4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4a213d37242bdcad8e7300e202e7caa4-Supplemental.zip},
 title = {Co-regularization Based Semi-supervised Domain Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/4a213d37242bdcad8e7300e202e7caa4-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_4b0a59dd,
 abstract = {Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its \lova extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.},
 author = {Bach, Francis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4b0a59ddf11c58e7446c9df0da541a84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4b0a59ddf11c58e7446c9df0da541a84-Metadata.json},
 openalex = {W2953028742},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4b0a59ddf11c58e7446c9df0da541a84-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Structured sparsity-inducing norms through submodular functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/4b0a59ddf11c58e7446c9df0da541a84-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_4c27cea8,
 abstract = {We present a fast online solver for large scale parametric max-flow problems as they occur in portfolio optimization, inventory management, computer vision, and logistics. Our algorithm solves an integer linear program in an online fashion. It exploits total unimodularity of the constraint matrix and a Lagrangian relaxation to solve the problem as a convex online game. The algorithm generates approximate solutions of max-flow problems by performing stochastic gradient descent on a set of flows. We apply the algorithm to optimize tier arrangement of over 84 million web pages on a layered set of caches to serve an incoming query stream optimally.},
 author = {Leung, Gilbert and Quadrianto, Novi and Tsioutsiouliklis, Kostas and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4c27cea8526af8cfee3be5e183ac9605-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4c27cea8526af8cfee3be5e183ac9605-Metadata.json},
 openalex = {W2238306519},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4c27cea8526af8cfee3be5e183ac9605-Supplemental.zip},
 title = {Optimal Web-Scale Tiering as a Flow Problem},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/4c27cea8526af8cfee3be5e183ac9605-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_4d5b9953,
 abstract = {The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.},
 author = {Murray, Iain and Adams, Ryan P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4d5b995358e7798bc7e9d9db83c612a5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4d5b995358e7798bc7e9d9db83c612a5-Metadata.json},
 openalex = {W2949152134},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4d5b995358e7798bc7e9d9db83c612a5-Supplemental.zip},
 title = {Slice sampling covariance hyperparameters of latent Gaussian models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/4d5b995358e7798bc7e9d9db83c612a5-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_4da04049,
 abstract = {Dimensionality reduction is commonly used in the setting of multi-label supervised classification to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression, and show that this model provides a probabilistic interpretation of discriminative clustering methods with added benefits in terms of number of hyperparameters and optimization. While the expectation-maximization (EM) algorithm is commonly used to learn these probabilistic models, it usually leads to local maxima because it relies on a non-convex cost function. To avoid this problem, we introduce a local approximation of this cost function, which in turn leads to a quadratic non-convex optimization problem over a product of simplices. In order to maximize quadratic functions, we propose an efficient algorithm based on convex relaxations and low-rank representations of the data, capable of handling large-scale problems. Experiments on text document classification show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods.},
 author = {Joulin, Armand and Ponce, Jean and Bach, Francis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4da04049a062f5adfe81b67dd755cecc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4da04049a062f5adfe81b67dd755cecc-Metadata.json},
 openalex = {W2099987944},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4da04049a062f5adfe81b67dd755cecc-Supplemental.zip},
 title = {Efficient Optimization for Discriminative Latent Class Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/4da04049a062f5adfe81b67dd755cecc-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_4e0cb6fb,
 abstract = {During the last years support vector machines (SVMs) have been successfully applied in situations where the input space X is not necessarily a subset of ℝd. Examples include SVMs for the analysis of histograms or colored images, SVMs for text classification and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) H ⊂ Lp(PX) is dense, or if the SVM uses a universal kernel k. So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X ⊄ ℝd. We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of ℝd. We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing.},
 author = {Christmann, Andreas and Steinwart, Ingo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4e0cb6fb5fb446d1c92ede2ed8780188-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4e0cb6fb5fb446d1c92ede2ed8780188-Metadata.json},
 openalex = {W2107552638},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4e0cb6fb5fb446d1c92ede2ed8780188-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Universal Kernels on Non-Standard Input Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/4e0cb6fb5fb446d1c92ede2ed8780188-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_4e4b5fbb,
 abstract = {Dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved. In this paper, we first analyze the scatter measures used in the conventional linear discriminant analysis (LDA) model and note that the formulation is based on the average-case view. Based on this analysis, we then propose a new dimensionality reduction method called worst-case linear discriminant analysis (WLDA) by defining new between-class and within-class scatter measures. This new model adopts the worst-case view which arguably is more suitable for applications such as classification. When the number of training data points or the number of features is not very large, we relax the optimization problem involved and formulate it as a metric learning problem. Otherwise, we take a greedy approach by finding one direction of the transformation at a time. Moreover, we also analyze a special case of WLDA to show its relationship with conventional LDA. Experiments conducted on several benchmark datasets demonstrate the effectiveness of WLDA when compared with some related dimensionality reduction methods.},
 author = {Zhang, Yu and Yeung, Dit-Yan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Metadata.json},
 openalex = {W2145342134},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Worst-Case Linear Discriminant Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/4e4b5fbbbb602b6d35bea8460aa8f8e5-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_51d92be1,
 abstract = {In this paper, we propose a matrix-variate normal penalty with sparse inverse co-variances to couple multiple tasks. Learning multiple (parametric) models can be viewed as estimating a matrix of parameters, where rows and columns of the matrix correspond to tasks and features, respectively. Following the matrix-variate normal density, we design a penalty that decomposes the full covariance of matrix elements into the Kronecker product of row covariance and column covariance, which characterizes both task relatedness and feature representation. Several recently proposed methods are variants of the special cases of this formulation. To address the overfitting issue and select meaningful task and feature structures, we include sparse covariance selection into our matrix-normal regularization via l1 penalties on task and feature inverse covariances. We empirically study the proposed method and compare with related models in two real-world problems: detecting landmines in multiple fields and recognizing faces between different subjects. Experimental results show that the proposed framework provides an effective and flexible way to model various different structures of multiple tasks.},
 author = {Zhang, Yi and Schneider, Jeff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/51d92be1c60d1db1d2e5e7a07da55b26-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/51d92be1c60d1db1d2e5e7a07da55b26-Metadata.json},
 openalex = {W2145620688},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning Multiple Tasks with a Sparse Matrix-Normal Penalty},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/51d92be1c60d1db1d2e5e7a07da55b26-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_537d9b6c,
 abstract = {We consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of $\ell_\infty$-norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems.},
 author = {Mairal, Julien and Jenatton, Rodolphe and Bach, Francis and Obozinski, Guillaume R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/537d9b6c927223c796cac288cced29df-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/537d9b6c927223c796cac288cced29df-Metadata.json},
 openalex = {W2951862640},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/537d9b6c927223c796cac288cced29df-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Network Flow Algorithms for Structured Sparsity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/537d9b6c927223c796cac288cced29df-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_5487315b,
 abstract = {Active learning reduces the labeling cost by iteratively selecting the most valuable data to query their labels. It has attracted a lot of interests given the abundance of unlabeled data and the high cost of labeling. Most active learning approaches select either informative or representative unlabeled instances to query their labels, which could significantly limit their performance. Although several active learning algorithms were proposed to combine the two query selection criteria, they are usually ad hoc in finding unlabeled instances that are both informative and representative. We address this limitation by developing a principled approach, termed QUIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an unlabeled instance. Further, by incorporating the correlation among labels, we extend the QUIRE approach to multi-label learning by actively querying instance-label pairs. Extensive experimental results show that the proposed QUIRE approach outperforms several state-of-the-art active learning approaches in both single-label and multi-label learning.},
 author = {Huang, Sheng-jun and Jin, Rong and Zhou, Zhi-Hua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5487315b1286f907165907aa8fc96619-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5487315b1286f907165907aa8fc96619-Metadata.json},
 openalex = {W2012878613},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5487315b1286f907165907aa8fc96619-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5487315b1286f907165907aa8fc96619-Supplemental.zip},
 title = {Active Learning by Querying Informative and Representative Examples},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/5487315b1286f907165907aa8fc96619-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_550a141f,
 abstract = {Bayesian approaches to utility elicitation typically adopt (myopic) expected value of information (EVOI) as a natural criterion for selecting queries. However, EVOI-optimization is usually computationally prohibitive. In this paper, we examine EVOI optimization using choice queries, queries in which a user is ask to select her most preferred product from a set. We show that, under very general assumptions, the optimal choice query w.r.t. EVOI coincides with the optimal recommendation set, that is, a set maximizing the expected utility of the user selection. Since recommendation set optimization is a simpler, submodular problem, this can greatly reduce the complexity of both exact and approximate (greedy) computation of optimal choice queries. We also examine the case where user responses to choice queries are error-prone (using both constant and mixed multinomial logit noise models) and provide worst-case guarantees. Finally we present a local search technique for query optimization that works extremely well with large outcome spaces.},
 author = {Viappiani, Paolo and Boutilier, Craig},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/550a141f12de6341fba65b0ad0433500-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/550a141f12de6341fba65b0ad0433500-Metadata.json},
 openalex = {W2146412174},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/550a141f12de6341fba65b0ad0433500-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/550a141f12de6341fba65b0ad0433500-Supplemental.zip},
 title = {Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/550a141f12de6341fba65b0ad0433500-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_556f3919,
 abstract = {We address the problem of semi-supervised learning in an adversarial setting. Instead of assuming that labels are missing at random, we analyze a less favorable scenario where the label information can be missing partially and arbitrarily, which is motivated by several practical examples. We present nearly matching upper and lower generalization bounds for learning in this setting under reasonable assumptions about available label information. Motivated by the analysis, we formulate a convex optimization problem for parameter estimation, derive an efficient algorithm, and analyze its convergence. We provide experimental results on several standard data sets showing the robustness of our algorithm to the pattern of missing label information, outperforming several strong baselines.},
 author = {Syed, Umar and Taskar, Ben},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/556f391937dfd4398cbac35e050a2177-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/556f391937dfd4398cbac35e050a2177-Metadata.json},
 openalex = {W2152967272},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/556f391937dfd4398cbac35e050a2177-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/556f391937dfd4398cbac35e050a2177-Supplemental.zip},
 title = {Semi-Supervised Learning with Adversarially Missing Label Information},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/556f391937dfd4398cbac35e050a2177-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_5737c6ec,
 abstract = {We describe a log-bilinear model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables. Even though the latent variables can take on exponentially many possible combinations of values, we can efficiently compute the exact probability of each class by marginalizing over the latent variables. This makes it possible to get the exact gradient of the log likelihood. The bilinear score-functions are defined using a three-dimensional weight tensor, and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions. Experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with (kernel) SVMs, backpropagation, and deep belief nets.},
 author = {Memisevic, Roland and Zach, Christopher and Pollefeys, Marc and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Metadata.json},
 openalex = {W2135217348},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Gated Softmax Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_577ef115,
 author = {P\'{a}l, D\'{a}vid and P\'{o}czos, Barnab\'{a}s and Szepesv\'{a}ri, Csaba},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/577ef1154f3240ad5b9b413aa7346a1e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/577ef1154f3240ad5b9b413aa7346a1e-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/577ef1154f3240ad5b9b413aa7346a1e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Estimation of R\'{e}nyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/577ef1154f3240ad5b9b413aa7346a1e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_5807a685,
 abstract = {A long-standing open research problem is how to use information from different experiments, including background knowledge, to infer causal relations. Recent developments have shown ways to use multiple data sets, provided they originate from identical experiments. We present the MCI-algorithm as the first method that can infer provably valid causal relations in the large sample limit from different experiments. It is fast, reliable and produces very clear and easily interpretable output. It is based on a result that shows that constraint-based causal discovery is decomposable into a candidate pair identification and subsequent elimination step that can be applied separately from different models. We test the algorithm on a variety of synthetic input model sets to assess its behavior and the quality of the output. The method shows promising signs that it can be adapted to suit causal discovery in real-world application areas as well, including large databases.},
 author = {Claassen, Tom and Heskes, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5807a685d1a9ab3b599035bc566ce2b9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5807a685d1a9ab3b599035bc566ce2b9-Metadata.json},
 openalex = {W2170138516},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Causal discovery in multiple models from different experiments},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/5807a685d1a9ab3b599035bc566ce2b9-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_59c33016,
 abstract = {This paper presents an analysis of importance weighting for learning from finite samples and gives a series of theoretical and algorithmic results. We point out simple cases where importance weighting can fail, which suggests the need for an analysis of the properties of this technique. We then give both upper and lower bounds for generalization with bounded importance weights and, more significantly, give learning guarantees for the more common case of unbounded importance weights under the weak assumption that the second moment is bounded, a condition related to the Renyi divergence of the traning and test distributions. These results are based on a series of novel and general bounds we derive for unbounded loss functions, which are of independent interest. We use these bounds to guide the definition of an alternative reweighting algorithm and report the results of experiments demonstrating its benefits. Finally, we analyze the properties of normalized importance weights which are also commonly used.},
 author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/59c33016884a62116be975a9bb8257e3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/59c33016884a62116be975a9bb8257e3-Metadata.json},
 openalex = {W2111355007},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/59c33016884a62116be975a9bb8257e3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/59c33016884a62116be975a9bb8257e3-Supplemental.zip},
 title = {Learning Bounds for Importance Weighting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/59c33016884a62116be975a9bb8257e3-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_5c572eca,
 abstract = {We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classification algorithm to learn to imitate the expert's behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classifier has error rate e, the difference between the value of the apprentice's policy and the expert's policy is O(√e). Further, we prove that this difference is only O(e) when the expert's policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difficult to obtain.},
 author = {Syed, Umar and Schapire, Robert E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5c572eca050594c7bc3c36e7e8ab9550-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5c572eca050594c7bc3c36e7e8ab9550-Metadata.json},
 openalex = {W2103235543},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5c572eca050594c7bc3c36e7e8ab9550-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Reduction from Apprenticeship Learning to Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/5c572eca050594c7bc3c36e7e8ab9550-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_5dd9db5e,
 abstract = {Generalized Binary Search (GBS) is a well known greedy algorithm for identifying an unknown object while minimizing the number of yes or no questions posed about that object, and arises in problems such as active learning and active diagnosis. Here, we provide a coding-theoretic interpretation for GBS and show that GBS can be viewed as a top-down algorithm that greedily minimizes the expected number of queries required to identify an object. This interpretation is then used to extend GBS in two ways. First, we consider the case where the objects are partitioned into groups, and the objective is to identify only the group to which the object belongs. Then, we consider the case where the cost of identifying an object grows exponentially in the number of queries. In each case, we present an exact formula for the objective function involving Shannon or Renyi entropy, and develop a greedy algorithm for minimizing it.},
 author = {Bellala, Gowtham and Bhavnani, Suresh and Scott, Clayton},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Metadata.json},
 openalex = {W2104712439},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Supplemental.zip},
 title = {Extensions of Generalized Binary Search to Group Identification and Exponential Costs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_5f0f5e5f,
 abstract = {We present a new learning strategy for classification problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-specific subspace. In contrast, our method considers instances as sets of (feature, value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classification strategy for sets. Our proposal maps (feature, value) pairs into an embedding space and then non-linearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the final classification objective. This simple strategy allows great flexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets.},
 author = {Grangier, David and Melvin, Iain},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5f0f5e5f33945135b874349cfbed4fb9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5f0f5e5f33945135b874349cfbed4fb9-Metadata.json},
 openalex = {W2107103981},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/5f0f5e5f33945135b874349cfbed4fb9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Feature Set Embedding for Incomplete Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/5f0f5e5f33945135b874349cfbed4fb9-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_621461af,
 abstract = {For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are influenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random field with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments.},
 author = {Mozer, Michael C and Pashler, Harold and Wilder, Matthew and Lindsey, Robert V and Jones, Matt and Jones, Michael N},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/621461af90cadfdaf0e8d4cc25129f91-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/621461af90cadfdaf0e8d4cc25129f91-Metadata.json},
 openalex = {W2141489888},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Improving Human Judgments by Decontaminating Sequential Dependencies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/621461af90cadfdaf0e8d4cc25129f91-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_632cee94,
 abstract = {We consider least-squares regression using a randomly generated subspace GP ⊂ F of finite dimension P, where F is a function space of infinite dimension, e.g. L2([0, 1]d). GP is defined as the span of P random features that are linear combinations of the basis functions of F weighted by random Gaussian i.i.d. coefficients. In particular, we consider multi-resolution random combinations at all scales of a given mother function, such as a hat function or a wavelet. In this latter case, the resulting Gaussian objects are called scrambled wavelets and we show that they enable to approximate functions in Sobolev spaces Hs([0, l]d). As a result, given N data, the least-squares estimate ĝ built from P scrambled wavelets has excess risk ‖f* - ĝ‖2P = O(‖f*‖2Hs([0,1]d)(log N)/P + P(log N)/N) for target functions f* ∈ Hs ([0,1]d) of smoothness order s > d/2. An interesting aspect of the resulting bounds is that they do not depend on the distribution P from which the data are generated, which is important in a statistical regression setting considered here. Randomization enables to adapt to any possible distribution.

We conclude by describing an efficient numerical implementation using lazy expansions with numerical complexity O(2dN3/2 log N + N2), where d is the dimension of the input space.},
 author = {Maillard, Odalric and Munos, R\'{e}mi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/632cee946db83e7a52ce5e8d6f0fed35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/632cee946db83e7a52ce5e8d6f0fed35-Metadata.json},
 openalex = {W2119151302},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Scrambled Objects for Least-Squares Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/632cee946db83e7a52ce5e8d6f0fed35-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6395ebd0,
 abstract = {When working with network datasets, the theoretical framework of for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph in a signal processing context, this article provides a framework and empirical results that elucidate a detection theory for graph-valued data. Its focus is the of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph's so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets confirms the efficacy of this approach.},
 author = {Miller, Benjamin and Bliss, Nadya and Wolfe, Patrick},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6395ebd0f4b478145ecfbaf939454fa4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6395ebd0f4b478145ecfbaf939454fa4-Metadata.json},
 openalex = {W2162384258},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Subgraph Detection Using Eigenvector L1 Norms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6395ebd0f4b478145ecfbaf939454fa4-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_65cc2c82,
 abstract = {We address the question of how the approximation error/Bellman residual at each iteration of the Approximate Policy/Value Iteration algorithms influences the quality of the resulted policy. We quantify the performance loss as the Lp norm of the approximation error/Bellman residual at each iteration. Moreover, we show that the performance loss depends on the expectation of the squared Radon-Nikodym derivative of a certain distribution rather than its supremum - as opposed to what has been suggested by the previous results. Also our results indicate that the contribution of the approximation/Bellman error to the performance loss is more prominent in the later iterations of API/AVI, and the effect of an error term in the earlier iterations decays exponentially fast.},
 author = {Farahmand, Amir-massoud and Szepesv\'{a}ri, Csaba and Munos, R\'{e}mi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/65cc2c8205a05d7379fa3a6386f710e1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/65cc2c8205a05d7379fa3a6386f710e1-Metadata.json},
 openalex = {W2128812357},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/65cc2c8205a05d7379fa3a6386f710e1-Supplemental.zip},
 title = {Error propagation for approximate policy and value iteration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/65cc2c8205a05d7379fa3a6386f710e1-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_66368270,
 abstract = {This paper introduces the first set of PAC-Bayesian bounds for the batch reinforcement learning problem in finite state spaces. These bounds hold regardless of the correctness of the prior distribution. We demonstrate how such bounds can be used for model-selection in control problems where prior information is available either on the dynamics of the environment, or on the value of actions. Our empirical results confirm that PAC-Bayesian model-selection is able to leverage prior distributions when they are informative and, unlike standard Bayesian RL approaches, ignores them when they are misleading.},
 author = {Fard, M. and Pineau, Joelle},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/66368270ffd51418ec58bd793f2d9b1b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/66368270ffd51418ec58bd793f2d9b1b-Metadata.json},
 openalex = {W2099089474},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {PAC-Bayesian Model Selection for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_677e0972,
 abstract = {We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the glimpse at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images.},
 author = {Larochelle, Hugo and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/677e09724f0e2df9b6c000b75b5da10d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/677e09724f0e2df9b6c000b75b5da10d-Metadata.json},
 openalex = {W2141399712},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/677e09724f0e2df9b6c000b75b5da10d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/677e09724f0e2df9b6c000b75b5da10d-Supplemental.zip},
 title = {Learning to combine foveal glimpses with a third-order Boltzmann machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/677e09724f0e2df9b6c000b75b5da10d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_67d96d45,
 abstract = {We show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields significant gains on the highly non-uniformly sampled Netflix dataset.},
 author = {Srebro, Nathan and Salakhutdinov, Russ R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/67d96d458abdef21792e6d8e590244e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/67d96d458abdef21792e6d8e590244e7-Metadata.json},
 openalex = {W1863732927},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/67d96d458abdef21792e6d8e590244e7-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_67f7fb87,
 abstract = {We study worst-case bounds on the quality of any fixed point assignment of the max-product algorithm for Markov Random Fields (MRF). We start providing a bound independent of the MRF structure and parameters. Afterwards, we show how this bound can be improved for MRFs with specific structures such as bipartite graphs or grids. Our results provide interesting insight into the behavior of max-product. For example, we prove that max-product provides very good results (at least 90% optimal) on MRFs with large variable-disjoint cycles1.},
 author = {Vinyals, Meritxell and Cerquides, Jes\textbackslash \textquotesingle us and Farinelli, Alessandro and Rodr\'{\i}guez-aguilar, Juan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/67f7fb873eaf29526a11a9b7ac33bfac-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/67f7fb873eaf29526a11a9b7ac33bfac-Metadata.json},
 openalex = {W2140167763},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/67f7fb873eaf29526a11a9b7ac33bfac-Supplemental.zip},
 title = {Worst-case bounds on the quality of max-product fixed-points},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_68053af2,
 abstract = {Partially Observable Markov Decision Processes (POMDPs) model sequential decision-making problems under uncertainty and partial observability. Unfortunately, some problems cannot be modeled with state-dependent reward functions, e.g., problems whose objective explicitly implies reducing the uncertainty on the state. To that end, we introduce ρPOMDPs, an extension of POMDPs where the reward function ρ depends on the belief state. We show that, under the common assumption that ρ is convex, the value function is also convex, what makes it possible to (1) approximate ρ arbitrarily well with a piecewise linear and convex (PWLC) function, and (2) use state-of-the-art exact or approximate solving algorithms with limited changes.},
 author = {Araya, Mauricio and Buffet, Olivier and Thomas, Vincent and Charpillet, Fran\c{c}cois},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68053af2923e00204c3ca7c6a3150cf7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68053af2923e00204c3ca7c6a3150cf7-Metadata.json},
 openalex = {W2099495504},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68053af2923e00204c3ca7c6a3150cf7-Supplemental.zip},
 title = {A POMDP Extension with Belief-dependent Rewards},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_68a83eeb,
 abstract = {Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the definition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the infinite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects.},
 author = {M\o rup, Morten and Madsen, Kristoffer and Dogonowski, Anne-marie and Siebner, Hartwig and Hansen, Lars K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68a83eeb494a308fe5295da69428a507-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68a83eeb494a308fe5295da69428a507-Metadata.json},
 openalex = {W2168298321},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68a83eeb494a308fe5295da69428a507-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68a83eeb494a308fe5295da69428a507-Supplemental.zip},
 title = {Infinite Relational Modeling of Functional Connectivity in Resting State fMRI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/68a83eeb494a308fe5295da69428a507-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_68b1fbe7,
 abstract = {Determining whether someone is talking has applications in many areas such as speech recognition, speaker diarization, social robotics, facial expression recognition, and human computer interaction. One popular approach to this problem is audio-visual synchrony detection [10, 21, 12]. A candidate speaker is deemed to be talking if the visual signal around that speaker correlates with the auditory signal. Here we show that with the proper visual features (in this case movements of various facial muscle groups), a very accurate detector of speech can be created that does not use the audio signal at all. Further we show that this person independent visual-only detector can be used to train very accurate audio-based person dependent voice models. The voice model has the advantage of being able to identify when a particular person is speaking even when they are not visible to the camera (e.g. in the case of a mobile robot). Moreover, we show that a simple sensory fusion scheme between the auditory and visual models improves performance on the task of talking detection. The work here provides dramatic evidence about the efficacy of two very different approaches to multimodal speech detection on a challenging database.},
 author = {Movellan, Javier and Ruvolo, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68b1fbe7f16e4ae3024973f12f3cb313-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68b1fbe7f16e4ae3024973f12f3cb313-Metadata.json},
 openalex = {W2112043905},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/68b1fbe7f16e4ae3024973f12f3cb313-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {An Alternative to Low-level-Sychrony-Based Methods for Speech Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/68b1fbe7f16e4ae3024973f12f3cb313-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_692f93be,
 abstract = {Intelligent agents are often faced with the need to choose actions with uncertain consequences, and to modify those actions according to ongoing sensory processing and changing task demands. The requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology. We formalize inhibitory control as a rational decision-making problem, and apply to it to the classical stop-signal task. Using Bayesian inference and stochastic control tools, we show that the optimal policy systematically depends on various parameters of the problem, such as the relative costs of different action choices, the noise level of sensory inputs, and the dynamics of changing environmental demands. Our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task, suggesting that the brain implements statistically optimal, dynamically adaptive, and reward-sensitive decision-making in the context of inhibitory control problems.},
 author = {Shenoy, Pradeep and Yu, Angela J and Rao, Rajesh PN},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/692f93be8c7a41525c0baf2076aecfb4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/692f93be8c7a41525c0baf2076aecfb4-Metadata.json},
 openalex = {W2136112606},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A rational decision making framework for inhibitory control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/692f93be8c7a41525c0baf2076aecfb4-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_69421f03,
 abstract = {We present policy gradient results within the framework of linearly-solvable MDPs. For the first time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the first compatible function approximators and natural policy gradients for continuous-time stochastic systems.},
 author = {Todorov, Emanuel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/69421f032498c97020180038fddb8e24-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/69421f032498c97020180038fddb8e24-Metadata.json},
 openalex = {W2142044314},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/69421f032498c97020180038fddb8e24-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/69421f032498c97020180038fddb8e24-Supplemental.zip},
 title = {Policy gradients in linearly-solvable MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/69421f032498c97020180038fddb8e24-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6c524f9d,
 abstract = {Regularization technique has become a principled tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further give mathematically exact definition for a novel representation called sparse grouping representation (SGR), and prove a set of sufficient conditions for generating such group level sparsity. Under these sufficient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also provide some generalization bounds in a classification setting.},
 author = {Zhou, Hongbo and Cheng, Qiang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6c524f9d5d7027454a783c841250ba71-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6c524f9d5d7027454a783c841250ba71-Metadata.json},
 openalex = {W2159492401},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6c524f9d5d7027454a783c841250ba71-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6c8dba7d,
 abstract = {We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difficult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs.},
 author = {Chambers, America and Smyth, Padhraic and Steyvers, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6c8dba7d0df1c4a79dd07646be9a26c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6c8dba7d0df1c4a79dd07646be9a26c8-Metadata.json},
 openalex = {W2139031914},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6c8dba7d0df1c4a79dd07646be9a26c8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6c8dba7d0df1c4a79dd07646be9a26c8-Supplemental.zip},
 title = {Learning concept graphs from text with stick-breaking priors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6c8dba7d0df1c4a79dd07646be9a26c8-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6cdd60ea,
 abstract = {Remarkably easy implementation and guaranteed convergence has made the EM algorithm one of the most used algorithms for mixture modeling. On the downside, the E-step is linear in both the sample size and the number of mixture components, making it impractical for large-scale data. Based on the variational EM framework, we propose a fast alternative that uses component-specific data partitions to obtain a sub-linear E-step in sample size, while the algorithm still maintains provable convergence. Our approach builds on previous work, but is significantly faster and scales much better in the number of mixture components. We demonstrate this speedup by experiments on large-scale synthetic and real data.},
 author = {Thiesson, Bo and Wang, Chong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cdd60ea0045eb7a6ec44c54d29ed402-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cdd60ea0045eb7a6ec44c54d29ed402-Metadata.json},
 openalex = {W2121648248},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cdd60ea0045eb7a6ec44c54d29ed402-Supplemental.zip},
 title = {Fast Large-scale Mixture Modeling with Component-specific Data Partitions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6cdd60ea0045eb7a6ec44c54d29ed402-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6cfe0e61,
 abstract = {The sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression. We propose a number of improvements to the model and inference algorithm, including an enlarged range of hyperparameters, a memory-efficient representation, and inference algorithms operating on the new representation. Our derivations are based on precise definitions of the various processes that will also allow us to provide an elementary proof of the mysterious coagulation and fragmentation properties used in the original paper on the sequence memoizer by Wood et al. (2009). We present some experimental results supporting our improvements.},
 author = {Gasthaus, Jan and Teh, Yee},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cfe0e6127fa25df2a0ef2ae1067d915-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cfe0e6127fa25df2a0ef2ae1067d915-Metadata.json},
 openalex = {W2113430702},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6cfe0e6127fa25df2a0ef2ae1067d915-Supplemental.zip},
 title = {Improvements to the Sequence Memoizer},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6da37dd3,
 abstract = {A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated.

To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance.

The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a significant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results.},
 author = {Blaschko, Matthew and Vedaldi, Andrea and Zisserman, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Metadata.json},
 openalex = {W2137023887},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Simultaneous Object Detection and Ranking with Weak Supervision},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6e7d2da6,
 author = {Ranzato, Marc\textquotesingle aurelio and Mnih, Volodymyr and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6e7d2da6d3953058db75714ac400b584-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6e7d2da6d3953058db75714ac400b584-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Generating more realistic images using gated MRF\textquotesingle s},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6e7d2da6d3953058db75714ac400b584-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6ea2ef73,
 abstract = {Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions. We develop an algorithm, SLG, that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.},
 author = {Stobbe, Peter and Krause, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Metadata.json},
 openalex = {W2118544668},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6ea2ef7311b482724a9b7b0bc0dd85c6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Minimization of Decomposable Submodular Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6ea2ef7311b482724a9b7b0bc0dd85c6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6ecbdd6e,
 abstract = {This paper proposes a simple and efficient finite difference method for implicit differentiation of marginal inference results in discrete graphical models. Given an arbitrary loss function, defined on marginals, we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice, on slightly perturbed model parameters. This method can be used with approximate inference, with a loss function over approximate marginals. Convenient choices of loss functions make it practical to fit graphical models with hidden variables, high treewidth and/or model misspecification.},
 author = {Domke, Justin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6ecbdd6ec859d284dc13885a37ce8d81-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6ecbdd6ec859d284dc13885a37ce8d81-Metadata.json},
 openalex = {W2129116030},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Differentiation by Perturbation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6ecbdd6ec859d284dc13885a37ce8d81-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6f2268bd,
 abstract = {We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we find that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reflects properties of simple cell receptive fields that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the first large-scale application of such an approach.},
 author = {Puertas, Jose and Bornschein, Joerg and L\"{u}cke, J\"{o}rg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f2268bd1d3d3ebaabb04d6b5d099425-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f2268bd1d3d3ebaabb04d6b5d099425-Metadata.json},
 openalex = {W2128514094},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f2268bd1d3d3ebaabb04d6b5d099425-Supplemental.zip},
 title = {The Maximal Causes of Natural Scenes are Edge Filters},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6f2268bd1d3d3ebaabb04d6b5d099425-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6f3e29a3,
 abstract = {Score Matching is a recently-proposed criterion for training high-dimensional density models for which maximum likelihood training is intractable. It has been applied to learning natural image statistics but has so-far been limited to simple models due to the difficulty of differentiating the loss with respect to the model parameters. We show how this differentiation can be automated with an extended version of the double-backpropagation algorithm. In addition, we introduce a regularization term for the Score Matching loss that enables its use for a broader range of problem by suppressing instabilities that occur with finite training sample sizes and quantized input values. Results are reported for image denoising and super-resolution.},
 author = {Kingma, Durk P and Cun, Yann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Metadata.json},
 openalex = {W2156047073},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Supplemental.zip},
 title = {Regularized estimation of image statistics by Score Matching},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6f3e29a35278d71c7f65495871231324-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_6f3ef77a,
 abstract = {Cardiovascular disease is the leading cause of death globally, resulting in 17 million deaths each year. Despite the availability of various treatment options, existing techniques based upon conventional medical knowledge often fail to identify patients who might have benefited from more aggressive therapy. In this paper, we describe and evaluate a novel unsupervised machine learning approach for cardiac risk stratification. The key idea of our approach is to avoid specialized medical knowledge, and assess patient risk using symbolic mismatch, a new metric to assess similarity in long-term time-series activity. We hypothesize that high risk patients can be identified using symbolic mismatch, as individuals in a population with unusual long-term physiological activity. We describe related approaches that build on these ideas to provide improved medical decision making for patients who have recently suffered coronary attacks. We first describe how to compute the symbolic mismatch between pairs of long term electrocardiographic (ECG) signals. This algorithm maps the original signals into a symbolic domain, and provides a quantitative assessment of the difference between these symbolic representations of the original signals. We then show how this measure can be used with each of a one-class SVM, a nearest neighbor classifier, and hierarchical clustering to improve risk stratification. We evaluated our methods on a population of 686 cardiac patients with available long-term electrocardiographic data. In a univariate analysis, all of the methods provided a statistically significant association with the occurrence of a major adverse cardiac event in the next 90 days. In a multivariate analysis that incorporated the most widely used clinical risk variables, the nearest neighbor and hierarchical clustering approaches were able to statistically significantly distinguish patients with a roughly two-fold risk of suffering a major adverse cardiac event in the next 90 days.},
 author = {Syed, Zeeshan and Guttag, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3ef77ac0e3619e98159e9b6febf557-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3ef77ac0e3619e98159e9b6febf557-Metadata.json},
 openalex = {W2102584140},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/6f3ef77ac0e3619e98159e9b6febf557-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_704afe07,
 abstract = {Many time-series such as human movement data consist of a sequence of basic actions, e.g., forehands and backhands in tennis. Automatically extracting and characterizing such actions is an important problem for a variety of different applications. In this paper, we present a probabilistic segmentation approach in which an observed time-series is modeled as a concatenation of segments corresponding to different basic actions. Each segment is generated through a noisy transformation of one of a few hidden trajectories representing different types of movement, with possible time re-scaling. We analyze three different approximation methods for dealing with model intractability, and demonstrate how the proposed approach can successfully segment table tennis movements recorded using a robot arm as haptic input device.},
 author = {Chiappa, Silvia and Peters, Jan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/704afe073992cbe4813cae2f7715336f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/704afe073992cbe4813cae2f7715336f-Metadata.json},
 openalex = {W2165300526},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/704afe073992cbe4813cae2f7715336f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Movement extraction by detecting dynamics switches and repetitions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/704afe073992cbe4813cae2f7715336f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_705f2172,
 abstract = {Many problem domains including climatology and epidemiology require models that can capture both heavy-tailed statistics and local dependencies. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Existing algorithms for inference and learning in CDNs are limited to those with tree-structured (non-loopy) graphs. In this paper, we develop inference and learning algorithms for CDNs with arbitrary topology. Our approach to inference and learning relies on recursively decomposing the computation of mixed derivatives based on a junction trees over the cumulative distribution functions. We demonstrate that our systematic approach to utilizing the sparsity represented by the junction tree yields significant performance improvements over the general symbolic differentiation programs Mathematica and D*. Using two real-world datasets, we demonstrate that non-tree structured (loopy) CDNs are able to provide significantly better fits to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.},
 author = {Jojic, Nebojsa and Meek, Chris and Huang, Jim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/705f2172834666788607efbfca35afb3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/705f2172834666788607efbfca35afb3-Metadata.json},
 openalex = {W2125931899},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/705f2172834666788607efbfca35afb3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/705f2172834666788607efbfca35afb3-Supplemental.zip},
 title = {Exact inference and learning for cumulative distribution functions on loopy graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/705f2172834666788607efbfca35afb3-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7143d7fb,
 abstract = {In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call completely regular . Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.},
 author = {Vito, Ernesto and Rosasco, Lorenzo and Toigo, Alessandro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7143d7fbadfa4693b9eec507d9d37443-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7143d7fbadfa4693b9eec507d9d37443-Metadata.json},
 openalex = {W2107159587},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7143d7fbadfa4693b9eec507d9d37443-Supplemental.zip},
 title = {Spectral Regularization for Support Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7143d7fbadfa4693b9eec507d9d37443-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_71f6278d,
 abstract = {We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.},
 author = {Hoffman, Matthew and Bach, Francis and Blei, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Metadata.json},
 openalex = {W2165599843},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Supplemental.zip},
 title = {Online Learning for Latent Dirichlet Allocation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_73278a4a,
 abstract = {This paper discusses the topic of dimensionality reduction for $k$-means clustering. We prove that any set of $n$ points in $d$ dimensions (rows in a matrix $A \in \RR^{n \times d}$) can be projected into $t = Ω(k / \eps^2)$ dimensions, for any $\eps \in (0,1/3)$, in $O(n d \lceil \eps^{-2} k/ \log(d) \rceil )$ time, such that with constant probability the optimal $k$-partition of the point set is preserved within a factor of $2+\eps$. The projection is done by post-multiplying $A$ with a $d \times t$ random matrix $R$ having entries $+1/\sqrt{t}$ or $-1/\sqrt{t}$ with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.},
 author = {Boutsidis, Christos and Zouzias, Anastasios and Drineas, Petros},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/73278a4a86960eeb576a8fd4c9ec6997-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/73278a4a86960eeb576a8fd4c9ec6997-Metadata.json},
 openalex = {W2952682616},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Random Projections for $k$-means Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/73278a4a86960eeb576a8fd4c9ec6997-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_766d856e,
 abstract = {Communication between a speaker and hearer will be most efficient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efficient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other's perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We find evidence in support of all three predictions, and demonstrate in addition that efficient communication tends to break down when speakers and hearers are placed under time pressure.},
 author = {Xu, Yang and Kemp, Charles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/766d856ef1a6b02f93d894415e6bfa0e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/766d856ef1a6b02f93d894415e6bfa0e-Metadata.json},
 openalex = {W2102310846},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Inference and communication in the game of Password},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/766d856ef1a6b02f93d894415e6bfa0e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_76cf99d3,
 abstract = {We establish an excess risk bound of O(H R_n^2 + R_n \sqrt{H L*}) for empirical risk minimization with an H-smooth loss function and a hypothesis class with Rademacher complexity R_n, where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where R_n = \sqrt{R/n}, this translates to a learning rate of O(RH/n) in the separable (L*=0) case and O(RH/n + \sqrt{L^* RH/n}) more generally. We also provide similar guarantees for online and stochastic convex optimization with a smooth non-negative objective.},
 author = {Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Metadata.json},
 openalex = {W3013820469},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Supplemental.zip},
 title = {Smoothness, Low-Noise and Fast Rates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/76cf99d3614e23eabab16fb27e944bf9-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7810ccd4,
 abstract = {Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having device-level energy information can cause users to conserve significant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In this paper, we examine a large scale energy disaggregation task, and apply a novel extension of sparse coding to this problem. In particular, we develop a method, based upon structured prediction, for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance. We show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage.},
 author = {Kolter, J. and Batra, Siddharth and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7810ccd41bf26faaa2c4e1f20db70a71-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7810ccd41bf26faaa2c4e1f20db70a71-Metadata.json},
 openalex = {W2167985623},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7810ccd41bf26faaa2c4e1f20db70a71-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Energy Disaggregation via Discriminative Sparse Coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7810ccd41bf26faaa2c4e1f20db70a71-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7940ab47,
 abstract = {We present a novel algorithm, Random Conic Pursuit, that solves semidefinite programs (SDPs) via repeated optimization over randomly selected two-dimensional subcones of the PSD cone. This scheme is simple, easily implemented, applicable to very general SDPs, scalable, and theoretically interesting. Its advantages are realized at the expense of an ability to readily compute highly exact solutions, though useful approximate solutions are easily obtained. This property renders Random Conic Pursuit of particular interest for machine learning applications, in which the relevant SDPs are generally based upon random data and so exact minima are often not a priority. Indeed, we present empirical results to this effect for various SDPs encountered in machine learning; these experiments demonstrate the potential practical usefulness of Random Conic Pursuit. We also provide a preliminary analysis that yields insight into the theoretical properties and convergence of the algorithm.},
 author = {Kleiner, Ariel and Rahimi, Ali and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7940ab47468396569a906f75ff3f20ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7940ab47468396569a906f75ff3f20ef-Metadata.json},
 openalex = {W2146931605},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7940ab47468396569a906f75ff3f20ef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7940ab47468396569a906f75ff3f20ef-Supplemental.zip},
 title = {Random Conic Pursuit for Semidefinite Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7940ab47468396569a906f75ff3f20ef-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7a53928f,
 abstract = {We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.},
 author = {Sato, Issei and Kurihara, Kenichi and Nakagawa, Hiroshi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7a53928fa4dd31e82c6ef826f341daec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7a53928fa4dd31e82c6ef826f341daec-Metadata.json},
 openalex = {W2113601767},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7a53928fa4dd31e82c6ef826f341daec-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7a53928fa4dd31e82c6ef826f341daec-Supplemental.zip},
 title = {Deterministic Single-Pass Algorithm for LDA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7a53928fa4dd31e82c6ef826f341daec-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7b13b220,
 abstract = {Figure/ground assignment, in which the visual image is divided into nearer (figural) and farther (ground) surfaces, is an essential step in visual processing, but its underlying computational mechanisms are poorly understood. Figural assignment (often referred to as border ownership) can vary along a contour, suggesting a spatially distributed process whereby local and global cues are combined to yield local estimates of border ownership. In this paper we model figure/ground estimation in a Bayesian belief network, attempting to capture the propagation of border ownership across the image as local cues (contour curvature and T-junctions) interact with more global cues to yield a figure/ground assignment. Our network includes as a nonlocal factor skeletal (medial axis) structure, under the hypothesis that medial structure draws border ownership so that borders are owned by the skeletal hypothesis that best explains them. We also briefly present a psychophysical experiment in which we measured local border ownership along a contour at various distances from an inducing cue (a T-junction). Both the human subjects and the network show similar patterns of performance, converging rapidly to a similar pattern of spatial variation in border ownership along contours.},
 author = {Froyen, Vicky and Feldman, Jacob and Singh, Manish},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7b13b2203029ed80337f27127a9f1d28-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7b13b2203029ed80337f27127a9f1d28-Metadata.json},
 openalex = {W2113182542},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Bayesian Framework for Figure-Ground Interpretation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7b13b2203029ed80337f27127a9f1d28-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7bb06076,
 abstract = {We consider online learning in finite stochastic Markovian environments where in each time step a new reward function is chosen by an oblivious adversary. The goal of the learning agent is to compete with the best stationary policy in hindsight in terms of the total reward received. Specifically, in each time step the agent observes the current state and the reward associated with the last transition, however, the agent does not observe the rewards associated with other state-action pairs. The agent is assumed to know the transition probabilities. The state of the art result for this setting is an algorithm with an expected regret of O(T <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">2/3</sup> lnT). In this paper, assuming that stationary policies mix uniformly fast, we show that after T time steps, the expected regret of this algorithm (more precisely, a slightly modified version thereof) is O(T <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">1/2</sup> lnT), giving the first rigorously proven, essentially tight regret bound for the problem.},
 author = {Neu, Gergely and Antos, Andras and Gy\"{o}rgy, Andr\'{a}s and Szepesv\'{a}ri, Csaba},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7bb060764a818184ebb1cc0d43d382aa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7bb060764a818184ebb1cc0d43d382aa-Metadata.json},
 openalex = {W2000850397},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7bb060764a818184ebb1cc0d43d382aa-Supplemental.zip},
 title = {Online Markov Decision Processes Under Bandit Feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7bb060764a818184ebb1cc0d43d382aa-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7bcdf75a,
 abstract = {The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuous-time paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the log-domain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time.},
 author = {Wu, Yi-da and Lin, Shi-jie and Chen, Hsin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Metadata.json},
 openalex = {W2154437746},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7bcdf75ad237b8e02e301f4091fb6bc8-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7c9d0b1f,
 abstract = {Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Specifically, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities - similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed.},
 author = {Chevallier, Sylvain and Paugam-moisy, H\'{e}l\textbackslash \textasciigrave ene and Sebag, Michele},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Metadata.json},
 openalex = {W2136549954},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7cbbc409,
 abstract = {We define a data dependent permutation complexity for a hypothesis set H, which is similar to a Rademacher complexity or maximum discrepancy. The permutation complexity is based (like the maximum discrepancy) on dependent sampling. We prove a uniform bound on the generalization error, as well as a concentration result which means that the permutation estimate can be efficiently estimated.},
 author = {Magdon-Ismail, Malik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7cbbc409ec990f19c78c75bd1e06f215-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7cbbc409ec990f19c78c75bd1e06f215-Metadata.json},
 openalex = {W2161340880},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Permutation Complexity Bound on Out-Sample Error},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7cbbc409ec990f19c78c75bd1e06f215-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7cce53cf,
 abstract = {Many statistical M-estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizes We analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension d to grow with (and possibly exceed) the sample size n. This high-dimensional structure precludes the usual global assumptions— namely, strong convexity and smoothness conditions—that underlie classical optimization analysis. We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models. Under these conditions, our theory guarantees that Nesterov's first-order method [12] has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter θ* and the optimal solution ^θ. This globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates. Our analysis applies to a wide range of M-estimators and statistical models, including sparse linear regression using Lasso (l1-regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization. Overall, this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation.},
 author = {Agarwal, Alekh and Negahban, Sahand and Wainwright, Martin J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7cce53cf90577442771720a370c3c723-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7cce53cf90577442771720a370c3c723-Metadata.json},
 openalex = {W1874232560},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7cce53cf90577442771720a370c3c723-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fast global convergence rates of gradient methods for high-dimensional statistical recovery},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7cce53cf90577442771720a370c3c723-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7d04bbbe,
 abstract = {Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We find that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we find that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.},
 author = {Wong, K. and Wang, He and Wu, Si and Fung, Chi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Metadata.json},
 openalex = {W2126842889},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Supplemental.zip},
 title = {Attractor Dynamics with Synaptic Depression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7eabe3a1,
 abstract = {Deep networks can potentially express a learning problem more efficiently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers.},
 author = {Montavon, Gr\'{e}goire and M\"{u}ller, Klaus-Robert and Braun, Mikio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Metadata.json},
 openalex = {W2115781030},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Layer-wise analysis of deep networks with Gaussian kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7eabe3a1649ffa2b3ff8c02ebfd5659f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_7f5d04d1,
 abstract = {Modelling camera shake as a space-invariant convolution simplifies the problem of removing camera shake, but often insufficiently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera. In an effort to address these limitations, (i) we introduce a taxonomy of camera shakes, (ii) we build on a recently introduced framework for space-variant filtering by Hirsch et al. and a fast algorithm for single image blind deconvolution for space-invariant filters by Cho and Lee to construct a method for blind deconvolution in the case of space-variant blur, and (iii), we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time recording the space-variant point spread function corresponding to that blur. Finally, we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake, even without using additionally motion sensor information.},
 author = {Harmeling, Stefan and Michael, Hirsch and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7f5d04d189dfb634e6a85bb9d9adf21e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7f5d04d189dfb634e6a85bb9d9adf21e-Metadata.json},
 openalex = {W2151109506},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/7f5d04d189dfb634e6a85bb9d9adf21e-Supplemental.zip},
 title = {Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/7f5d04d189dfb634e6a85bb9d9adf21e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8065d07d,
 abstract = {Sodium entry during an action potential determines the energy efficiency of a neuron. The classic Hodgkin-Huxley model of action potential generation is notoriously inefficient in that regard with about 4 times more charges flowing through the membrane than the theoretical minimum required to achieve the observed depolarization. Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efficiency and that the dynamics of their voltage-gated channels is significantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. Here, we introduce a novel family of Hodgkin-Huxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. We speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to Purkinje cells, yielding a very economical framework to model a wide range of different central neurons. The present paper demonstrates the performances and discuss the properties of this new family of models.},
 author = {Singh, Anand and Jolivet, Renaud and Magistretti, Pierre and Weber, Bruno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8065d07da4a77621450aa84fee5656d9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8065d07da4a77621450aa84fee5656d9-Metadata.json},
 openalex = {W2136740393},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_818f4654,
 abstract = {Bayesian methods of matrix factorization (MF) have been actively explored recently as promising alternatives to classical singular value decomposition. In this paper, we show that, despite the fact that the optimization problem is non-convex, the global optimal solution of variational Bayesian (VB) MF can be computed analytically by solving a quartic equation. This is highly advantageous over a popular VBMF algorithm based on iterated conditional modes since it can only find a local optimal solution after iterations. We further show that the global optimal solution of empirical VBMF (hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments.},
 author = {Nakajima, Shinichi and Sugiyama, Masashi and Tomioka, Ryota},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/818f4654ed39a1c147d1e51a00ffb4cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/818f4654ed39a1c147d1e51a00ffb4cb-Metadata.json},
 openalex = {W2127292230},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Global Analytic Solution for Variational Bayesian Matrix Factorization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/818f4654ed39a1c147d1e51a00ffb4cb-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_819f46e5,
 abstract = {We present a new way of converting a reversible finite Markov chain into a non-reversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result confirms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC.},
 author = {Sun, Yi and Schmidhuber, J\"{u}rgen and Gomez, Faustino},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/819f46e52c25763a55cc642422644317-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/819f46e52c25763a55cc642422644317-Metadata.json},
 openalex = {W2102124467},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/819f46e52c25763a55cc642422644317-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/819f46e52c25763a55cc642422644317-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_81dc9bdb,
 abstract = {Recent work in reinforcement learning has emphasized the power of L1 regularization to perform feature selection and prevent overfitting. We propose formulating the L1 regularized linear fixed point problem as a linear complementarity problem (LCP). This formulation offers several advantages over the LARS-inspired formulation, LARS-TD. The LCP formulation allows the use of efficient off-the-shelf solvers, leads to a new uniqueness result, and can be initialized with starting points from similar problems (warm starts). We demonstrate that warm starts, as well as the efficiency of LCP solvers, can speed up policy iteration. Moreover, warm starts permit a form of modified policy iteration that canbeusedto approximate a greedy homotopy path, a generalization of the LARS-TD homotopy path that combines policy evaluation and optimization.},
 author = {Johns, Jeffrey and Painter-wakefield, Christopher and Parr, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/81dc9bdb52d04dc20036dbd8313ed055-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/81dc9bdb52d04dc20036dbd8313ed055-Metadata.json},
 openalex = {W2108430473},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/81dc9bdb52d04dc20036dbd8313ed055-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/81dc9bdb52d04dc20036dbd8313ed055-Supplemental.zip},
 title = {Linear Complementarity for Regularized Policy Evaluation and Improvement},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/81dc9bdb52d04dc20036dbd8313ed055-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_81e74d67,
 abstract = {Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this flexibility leads to the risk of overfitting and inflated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of five genes in yeast.},
 author = {Husmeier, Dirk and Dondelinger, Frank and Lebre, Sophie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/81e74d678581a3bb7a720b019f4f1a93-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/81e74d678581a3bb7a720b019f4f1a93-Metadata.json},
 openalex = {W2153337510},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/81e74d678581a3bb7a720b019f4f1a93-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/81e74d678581a3bb7a720b019f4f1a93-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_821fa74b,
 abstract = {Undirected graphical models encode in a graph $G$ the dependency structure of a random vector $Y$. In many applications, it is of interest to model $Y$ given another random vector $X$ as input. We refer to the problem of estimating the graph $G(x)$ of $Y$ conditioned on $X=x$ as ``graph-valued regression.'' In this paper, we propose a semiparametric method for estimating $G(x)$ that builds a tree on the $X$ space just as in CART (classification and regression trees), but at each leaf of the tree estimates a graph. We call the method ``Graph-optimized CART,'' or Go-CART. We study the theoretical properties of Go-CART using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data.},
 author = {Liu, Han and Chen, Xi and Wasserman, Larry and Lafferty, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/821fa74b50ba3f7cba1e6c53e8fa6845-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/821fa74b50ba3f7cba1e6c53e8fa6845-Metadata.json},
 openalex = {W2953340953},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/821fa74b50ba3f7cba1e6c53e8fa6845-Supplemental.zip},
 title = {Graph-Valued Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/821fa74b50ba3f7cba1e6c53e8fa6845-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_839ab468,
 abstract = {Recently, some variants of the l1 norm, particularly matrix norms such as the l1,2 and l1,∞ norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the l1,2 and l1,∞ norms by considering a family of l1,q norms for 1 < q < ∞ and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the l1,q norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization (EM) algorithms to learn all model parameters, including q, automatically. Experiments have been conducted on two cancer classification applications using microarray gene expression data.},
 author = {Zhang, Yu and Yeung, Dit-Yan and Xu, Qian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/839ab46820b524afda05122893c2fe8e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/839ab46820b524afda05122893c2fe8e-Metadata.json},
 openalex = {W2105180523},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/839ab46820b524afda05122893c2fe8e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Probabilistic Multi-Task Feature Selection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/839ab46820b524afda05122893c2fe8e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_846c260d,
 abstract = {Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efficient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a flexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology.},
 author = {Shojaie, Ali and Michailidis, George},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/846c260d715e5b854ffad5f70a516c88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/846c260d715e5b854ffad5f70a516c88-Metadata.json},
 openalex = {W2135386089},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Penalized Principal Component Regression on Graphs for Analysis of Subnetworks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_847cc55b,
 abstract = {Games of incomplete information, or Bayesian games, are an important game-theoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-specific utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve significantly on the state of the art.},
 author = {Jiang, Albert and Leyton-brown, Kevin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/847cc55b7032108eee6dd897f3bca8a5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/847cc55b7032108eee6dd897f3bca8a5-Metadata.json},
 openalex = {W2110830122},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/847cc55b7032108eee6dd897f3bca8a5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/847cc55b7032108eee6dd897f3bca8a5-Supplemental.zip},
 title = {Bayesian Action-Graph Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/847cc55b7032108eee6dd897f3bca8a5-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_884d247c,
 abstract = {We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients. This family subsumes the li norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.},
 author = {Morales, Jean and Micchelli, Charles and Pontil, Massimiliano},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/884d247c6f65a96a7da4d1105d584ddd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/884d247c6f65a96a7da4d1105d584ddd-Metadata.json},
 openalex = {W2150979613},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Family of Penalty Functions for Structured Sparsity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/884d247c6f65a96a7da4d1105d584ddd-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_892c91e0,
 abstract = {When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modifications. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We find that our model reproduces data from to recent experimental studies with a small number of biophysically in-terpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear filter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic firing rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modifications are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also find emphasis of specific baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models.},
 author = {Schmiedt, Joscha and Albers, Christian and Pawelzik, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/892c91e0a653ba19df81a90f89d99bcd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/892c91e0a653ba19df81a90f89d99bcd-Metadata.json},
 openalex = {W2141573658},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/892c91e0a653ba19df81a90f89d99bcd-Supplemental.zip},
 title = {Spike timing-dependent plasticity as dynamic filter},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/892c91e0a653ba19df81a90f89d99bcd-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8a0e1141,
 abstract = {Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the benefits of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive fields, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty.},
 author = {Gershman, Samuel and Wilson, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Metadata.json},
 openalex = {W2147341853},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The Neural Costs of Optimal Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8a0e1141fd37fa5b98d5bb769ba1a7cc-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8a1e808b,
 abstract = {The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of fitting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is independent of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume and curvature is unavoidable. Whether the known lower bound of O(k/e2 + log 1/δ/e2) for the sample complexity of Empirical Risk minimization on k-means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 [3]. Here e is the desired bound on the error and δ is a bound on the probability of failure. We improve the best currently known upper bound [14] of O(k2/e2 + log 1/δ/e2) to O(k/e2 (min (k, + log4 k/e/e2)) + log 1/δ/e2). Based on these results, we devise a simple algorithm for k-means and another that uses a family of convex programs to fit a piecewise linear curve of a specified length to high dimensional data, where the sample complexity is independent of the ambient dimension.},
 author = {Narayanan, Hariharan and Mitter, Sanjoy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8a1e808b55fde9455cb3d8857ed88389-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8a1e808b55fde9455cb3d8857ed88389-Metadata.json},
 openalex = {W2140793251},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sample Complexity of Testing the Manifold Hypothesis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8a1e808b55fde9455cb3d8857ed88389-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8c235f89,
 abstract = {The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justification for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classification tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems.},
 author = {Muralidharan, Kritika and Vasconcelos, Nuno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8c235f89a8143a28a1d6067e959dd858-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8c235f89a8143a28a1d6067e959dd858-Metadata.json},
 openalex = {W2150474396},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A biologically plausible network for the computation of orientation dominance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8c235f89a8143a28a1d6067e959dd858-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8c6744c9,
 abstract = {We deal with the problem of variable selection when variables must be selected group-wise, with possibly overlapping groups defined a priori. In particular we propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups of variables. While in [12] the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and projected Newton method in a reduced dual space, corresponding to the active groups. This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing for dimensionality reduction. The computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations.},
 author = {Mosci, Sofia and Villa, Silvia and Verri, Alessandro and Rosasco, Lorenzo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8c6744c9d42ec2cb9e8885b54ff744d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8c6744c9d42ec2cb9e8885b54ff744d0-Metadata.json},
 openalex = {W2126976013},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8c6744c9d42ec2cb9e8885b54ff744d0-Supplemental.zip},
 title = {A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8c6744c9d42ec2cb9e8885b54ff744d0-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8d317bdc,
 abstract = {Heavy-tailed distributions are often used to enhance the robustness of regression and classification methods to in output space. Often, however, we are confronted with outliers in input space, which are isolated observations in sparsely populated regions. We show that heavy-tailed stochastic processes (which we construct from Gaussian processes via a copula), can be used to improve robustness of regression and classification estimators to such by selectively shrinking them more strongly in sparse regions than in dense regions. We carry out a theoretical analysis to show that selective shrinkage occurs when the marginals of the heavy-tailed process have sufficiently heavy tails. The analysis is complemented by experiments on biological data which indicate significant improvements of estimates in sparse regions while producing competitive results in dense regions.},
 author = {Wauthier, Fabian L and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8d317bdcf4aafcfc22149d77babee96d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8d317bdcf4aafcfc22149d77babee96d-Metadata.json},
 openalex = {W2155746256},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8d317bdcf4aafcfc22149d77babee96d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Heavy-Tailed Process Priors for Selective Shrinkage},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8d317bdcf4aafcfc22149d77babee96d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8dd48d6a,
 abstract = {This paper is concerned with rank aggregation, which aims to combine multiple input rankings to get a better ranking. A popular approach to rank aggregation is based on probabilistic models on permutations, e.g., the Luce model and the Mallows model. However, these models have their limitations in either poor expressiveness or high computational complexity. To avoid these limitations, in this paper, we propose a new model, which is defined with a coset-permutation distance, and models the generation of a permutation as a stagewise process. We refer to the new model as coset-permutation distance based stagewise (CPS) model. The CPS model has rich expressiveness and can therefore be used in versatile applications, because many different permutation distances can be used to induce the coset-permutation distance. The complexity of the CPS model is low because of the stagewise decomposition of the permutation probability and the efficient computation of most coset-permutation distances. We apply the CPS model to supervised rank aggregation, derive the learning and inference algorithms, and empirically study their effectiveness and efficiency. Experiments on public datasets show that the derived algorithms based on the CPS model can achieve state-of-the-art ranking accuracy, and are much more efficient than previous algorithms.},
 author = {Qin, Tao and Geng, Xiubo and Liu, Tie-yan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8dd48d6a2e2cad213179a3992c0be53c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8dd48d6a2e2cad213179a3992c0be53c-Metadata.json},
 openalex = {W2115822807},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A New Probabilistic Model for Rank Aggregation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8df707a9,
 abstract = {We consider linear models for stochastic dynamics. To any such model can be associated a network (namely a directed graph) describing which degrees of freedom interact under the dynamics. We tackle the problem of learning such a network from observation of the system trajectory over a time interval $T$. We analyze the $\ell_1$-regularized least squares algorithm and, in the setting in which the underlying network is sparse, we prove performance guarantees that are \emph{uniform in the sampling rate} as long as this is sufficiently high. This result substantiates the notion of a well defined `time complexity' for the network inference problem.},
 author = {Pereira, Jos\'{e} and Ibrahimi, Morteza and Montanari, Andrea},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8df707a948fac1b4a0f97aa554886ec8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8df707a948fac1b4a0f97aa554886ec8-Metadata.json},
 openalex = {W2154104617},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8df707a948fac1b4a0f97aa554886ec8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8df707a948fac1b4a0f97aa554886ec8-Supplemental.zip},
 title = {Learning Networks of Stochastic Differential Equations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8df707a948fac1b4a0f97aa554886ec8-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8e6b42f1,
 abstract = {Learning from multi-view data is important in many applications, such as image classification and annotation. In this paper, we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views. Our approach is based on an undirected latent space Markov network that fulfills a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables. We provide efficient inference and parameter estimation methods for the latent sub-space model. Finally, we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classification, annotation and retrieval.},
 author = {Chen, Ning and Zhu, Jun and Xing, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8e6b42f1644ecb1327dc03ab345e618b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8e6b42f1644ecb1327dc03ab345e618b-Metadata.json},
 openalex = {W2123111158},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8e6b42f1644ecb1327dc03ab345e618b-Supplemental.zip},
 title = {Predictive Subspace Learning for Multi-view Data: a Large Margin Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8e6b42f1644ecb1327dc03ab345e618b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8edd7215,
 abstract = {To cope with concept drift, we placed a probability distribution over the location of the most-recent drift point. We used Bayesian model comparison to update this distribution from the predictions of models trained on blocks of consecutive observations and pruned potential drift points with low probability. We compare our approach to a non-probabilistic method for drift and a probabilistic method for change-point detection. In our experiments, our approach generally yielded improved accuracy and/or speed over these other methods.},
 author = {Bach, Stephen and Maloof, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8edd72158ccd2a879f79cb2538568fdc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8edd72158ccd2a879f79cb2538568fdc-Metadata.json},
 openalex = {W2140879111},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8edd72158ccd2a879f79cb2538568fdc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Bayesian Approach to Concept Drift},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8edd72158ccd2a879f79cb2538568fdc-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_8f121ce0,
 abstract = {We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classification and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets.},
 author = {Liu, Han and Chen, Xi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8f121ce07d74717e0b1f21d122e04521-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8f121ce07d74717e0b1f21d122e04521-Metadata.json},
 openalex = {W2106485032},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/8f121ce07d74717e0b1f21d122e04521-Supplemental.zip},
 title = {Multivariate Dyadic Regression Trees for Sparse Learning Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/8f121ce07d74717e0b1f21d122e04521-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_912d2b1c,
 abstract = {Hypothesis testing on point processes has several applications such as model fitting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean firing rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov-Smirnov and Cramer-von-Mises tests to the space of spike trains via stratification, and show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to find optimally matched point processes.},
 author = {Seth, Sohan and Il, Park and Brockmeier, Austin and Semework, Mulugeta and Choi, John and Francis, Joseph and Principe, Jose},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/912d2b1c7b2826caf99687388d2e8f7c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/912d2b1c7b2826caf99687388d2e8f7c-Metadata.json},
 openalex = {W2119915532},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/912d2b1c7b2826caf99687388d2e8f7c-Supplemental.zip},
 title = {A novel family of non-parametric cumulative based divergences for point processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/912d2b1c7b2826caf99687388d2e8f7c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_93fb9d4b,
 abstract = {For many structured prediction problems, complex models often require adopting approximate inference techniques such as variational methods or sampling, which generally provide no satisfactory accuracy guarantees. In this work, we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade. We focus in particular on problems with high-treewidth and large state-spaces, which occur in many computer vision tasks. Unlike other variational methods, our ensembles do not enforce agreement between sub-models, but filter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model. Our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel, convex loss function, yet requires only a linear increase in computation over learning or inference in a single tractable sub-model. We provide a generalization bound on the filtering loss of the ensemble as a theoretical justification of our approach, and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos. We find that our approach significantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem.},
 author = {Weiss, David and Sapp, Benjamin and Taskar, Ben},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/93fb9d4b16aa750c7475b6d601c35c2c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/93fb9d4b16aa750c7475b6d601c35c2c-Metadata.json},
 openalex = {W2130325067},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/93fb9d4b16aa750c7475b6d601c35c2c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/93fb9d4b16aa750c7475b6d601c35c2c-Supplemental.zip},
 title = {Sidestepping Intractable Inference with Structured Ensemble Cascades},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/93fb9d4b16aa750c7475b6d601c35c2c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_94f6d7e0,
 abstract = {Recent studies compare gene expression data across species to identify core and species specific genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes.},
 author = {Bar-joseph, Ziv and Le, Hai-son},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/94f6d7e04a4d452035300f18b984988c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/94f6d7e04a4d452035300f18b984988c-Metadata.json},
 openalex = {W2107454255},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/94f6d7e04a4d452035300f18b984988c-Supplemental.zip},
 title = {Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/94f6d7e04a4d452035300f18b984988c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_96da2f59,
 abstract = {We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efficient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets.},
 author = {Ding, Nan and Vishwanathan, S.v.n.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/96da2f590cd7246bbde0051047b0d6f7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/96da2f590cd7246bbde0051047b0d6f7-Metadata.json},
 openalex = {W2128385268},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/96da2f590cd7246bbde0051047b0d6f7-Supplemental.zip},
 title = {t-logistic regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/96da2f590cd7246bbde0051047b0d6f7-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_9778d5d2,
 abstract = {We tackle the problem of simultaneously detecting occlusions and estimating optical flow. We show that, under standard assumptions of Lambertian reflection and static illumination, the task can be posed as a convex minimization problem. Therefore, the solution, computed using efficient algorithms, is guaranteed to be globally optimal, for any number of independently moving objects, and any number of occlusion layers. We test the proposed algorithm on benchmark datasets, expanded to enable evaluation of occlusion detection performance.},
 author = {Ayvaci, Alper and Raptis, Michalis and Soatto, Stefano},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9778d5d219c5080b9a6a17bef029331c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9778d5d219c5080b9a6a17bef029331c-Metadata.json},
 openalex = {W2098299336},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Occlusion Detection and Motion Estimation with Convex Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/9778d5d219c5080b9a6a17bef029331c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_98dce83d,
 abstract = {Most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models. The time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes. In order to address this shortcoming, in recent years several authors have proposed to learn object classifiers from weakly-labeled Internet images, such as photos retrieved by keyword-based image search engines. While this strategy eliminates the need for human supervision, the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches, because of the noisy nature of the labels associated to Web data.

In this paper we investigate and compare methods that learn image classifiers by combining very few manually annotated examples (e.g., 1-10 images per class) and a large number of weakly-labeled Web photos retrieved using keyword-based image search. We cast this as a domain adaptation problem: given a few strongly-labeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled Web photos), learn classifiers yielding small generalization error on the target domain. Our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces significant recognition rate improvements over the best published results (e.g., 65% better when using 5 labeled training examples per class) and that our classifiers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets.},
 author = {Bergamo, Alessandro and Torresani, Lorenzo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/98dce83da57b0395e163467c9dae521b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/98dce83da57b0395e163467c9dae521b-Metadata.json},
 openalex = {W2107250100},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/98dce83da57b0395e163467c9dae521b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/98dce83da57b0395e163467c9dae521b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_99566564,
 abstract = {Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (e.g., the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g., features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program's behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition, we compare SPORE algorithms to state-of-the-art sparse regression algorithms, and show that SPORE methods, motivated by real applications, outperform the other methods in terms of both interpretability and prediction accuracy.},
 author = {Huang, Ling and Jia, Jinzhu and Yu, Bin and Chun, Byung-gon and Maniatis, Petros and Naik, Mayur},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/995665640dc319973d3173a74a03860c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/995665640dc319973d3173a74a03860c-Metadata.json},
 openalex = {W2155575073},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/995665640dc319973d3173a74a03860c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/995665640dc319973d3173a74a03860c-Supplemental.zip},
 title = {Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/995665640dc319973d3173a74a03860c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_996009f2,
 abstract = {When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classification in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human's ability to use a manifold in a semi-supervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary.},
 author = {Rogers, Tim and Kalish, Chuck and Harrison, Joseph and Zhu, Jerry and Gibson, Bryan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/996009f2374006606f4c0b0fda878af1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/996009f2374006606f4c0b0fda878af1-Metadata.json},
 openalex = {W2110300407},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/996009f2374006606f4c0b0fda878af1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Humans Learn Using Manifolds, Reluctantly},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/996009f2374006606f4c0b0fda878af1-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_9adeb82f,
 abstract = {Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a high-dimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks.},
 author = {Maron, Yariv and Lamar, Michael and Bienenstock, Elie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Metadata.json},
 openalex = {W2126341068},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9adeb82fffb5444e81fa0ce8ad8afe7a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Sphere Embedding: An Application to Part-of-Speech Induction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/9adeb82fffb5444e81fa0ce8ad8afe7a-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_9b04d152,
 abstract = {We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L_2 regularization: We introduce the \gamma-adapted-dimension, which is a simple function of the spectrum of a distribution's covariance matrix, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the \gamma-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. The bounds hold for a rich family of sub-Gaussian distributions.},
 author = {Sabato, Sivan and Srebro, Nathan and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9b04d152845ec0a378394003c96da594-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9b04d152845ec0a378394003c96da594-Metadata.json},
 openalex = {W2171229259},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9b04d152845ec0a378394003c96da594-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9b04d152845ec0a378394003c96da594-Supplemental.zip},
 title = {Tight Sample Complexity of Large-Margin Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/9b04d152845ec0a378394003c96da594-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_9c01802d,
 abstract = {A number of objective functions in clustering problems can be described with submodular functions. In this paper, we introduce the minimum average cost criterion, and show that the theory of intersecting submodular functions can be used for clustering with submodular objective functions. The proposed algorithm does not require the number of clusters in advance, and it will be determined by the property of a given set of data points. The minimum average cost clustering problem is parameterized with a real variable, and surprisingly, we show that all information about optimal clusterings for all parameters can be computed in polynomial time in total. Additionally, we evaluate the performance of the proposed algorithm through computational experiments.},
 author = {Nagano, Kiyohito and Kawahara, Yoshinobu and Iwata, Satoru},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9c01802ddb981e6bcfbec0f0516b8e35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9c01802ddb981e6bcfbec0f0516b8e35-Metadata.json},
 openalex = {W2153010866},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Minimum Average Cost Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/9c01802ddb981e6bcfbec0f0516b8e35-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_9cfdf10e,
 abstract = {We consider the online binary classification problem, where we are given m classifiers. At each stage, the classifiers map the input to the probability that the input belongs to the positive class. An online classification meta-algorithm is an algorithm that combines the outputs of the classifiers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classifiers. In this paper, we use sensitivity and specificity as the performance metrics of the meta-algorithm. In particular, our goal is to design an algorithm that satisfies the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold; and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classifiers that satisfies fp-rate constraint, in hindsight. We show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable. Hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it. In the case of two classifiers, we show that this algorithm takes a very simple form. To our best knowledge, this is the first algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting.},
 author = {Bernstein, Andrey and Mannor, Shie and Shimkin, Nahum},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Metadata.json},
 openalex = {W2113399228},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9cfdf10e8fc047a44b08ed031e1f0ed1-Supplemental.zip},
 title = {Online Classification with Specificity Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/9cfdf10e8fc047a44b08ed031e1f0ed1-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_9f60ab2b,
 abstract = {We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models.},
 author = {Lin, Dahua and Grimson, Eric and Fisher, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9f60ab2b55468f104055b16df8f69e81-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9f60ab2b55468f104055b16df8f69e81-Metadata.json},
 openalex = {W2105767795},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9f60ab2b55468f104055b16df8f69e81-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9f60ab2b55468f104055b16df8f69e81-Supplemental.zip},
 title = {Construction of Dependent Dirichlet Processes based on Poisson Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/9f60ab2b55468f104055b16df8f69e81-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_9fe8593a,
 abstract = {The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative filtering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable first-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas.},
 author = {Lee, Jason D and Recht, Ben and Srebro, Nathan and Tropp, Joel and Salakhutdinov, Russ R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9fe8593a8a330607d76796b35c64c600-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9fe8593a8a330607d76796b35c64c600-Metadata.json},
 openalex = {W2102138843},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/9fe8593a8a330607d76796b35c64c600-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Practical Large-Scale Optimization for Max-norm Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/9fe8593a8a330607d76796b35c64c600-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a0160709,
 abstract = {A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efficient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through fiber bottlenecks are able to form coherent response properties.},
 author = {Isely, Guy and Hillar, Christopher and Sommer, Fritz},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a0160709701140704575d499c997b6ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a0160709701140704575d499c997b6ca-Metadata.json},
 openalex = {W2147605902},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a0160709701140704575d499c997b6ca-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a0160709701140704575d499c997b6ca-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a0161022,
 abstract = {We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting filters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efficiency of the overall representation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efficient feed-forward encoder that predicts quasi-sparse features from the input. While patch-based training rarely produces anything but oriented edge detectors, we show that convolutional training produces highly diverse filters, including center-surround filters, corner detectors, cross detectors, and oriented grating detectors. We show that using these filters in multistage convolutional network architecture improves performance on a number of visual recognition and detection tasks.},
 author = {Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-lan and Gregor, Karol and Mathieu, Michael and Cun, Yann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a01610228fe998f515a72dd730294d87-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a01610228fe998f515a72dd730294d87-Metadata.json},
 openalex = {W2169488311},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a01610228fe998f515a72dd730294d87-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning Convolutional Feature Hierarchies for Visual Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a01610228fe998f515a72dd730294d87-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a01a0380,
 abstract = {While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes.},
 author = {Sun, Zhaonan and Ampornpunt, Nawanol and Varma, Manik and Vishwanathan, S.v.n.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a01a0380ca3c61428c26a231f0e49a09-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a01a0380ca3c61428c26a231f0e49a09-Metadata.json},
 openalex = {W2031823405},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Multiple kernel learning, conic duality, and the SMO algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a01a0380ca3c61428c26a231f0e49a09-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a0a080f4,
 abstract = {Given an ensemble of distinct, low-level segmentations of an image, our goal is to identify visually meaningful segments in the ensemble. Knowledge about any specific objects and surfaces present in the image is not available. The selection of image regions occupied by objects is formalized as the maximum-weight independent set (MWIS) problem. MWIS is the heaviest subset of mutually non-adjacent nodes of an attributed graph. We construct such a graph from all segments in the ensemble. Then, MWIS selects maximally distinctive segments that together partition the image. A new MWIS algorithm is presented. The algorithm seeks a solution directly in the discrete domain, instead of relaxing MWIS to a continuous problem, as common in previous work. It iteratively finds a candidate discrete solution of the Taylor series expansion of the original MWIS objective function around the previous solution. The algorithm is shown to converge to an optimum. Our empirical evaluation on the benchmark Berkeley segmentation dataset shows that the new algorithm eliminates the need for hand-picking optimal input parameters of the state-of-the-art segmenters, and outperforms their best, manually optimized results.},
 author = {Brendel, William and Todorovic, Sinisa},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a0a080f42e6f13b3a2df133f073095dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a0a080f42e6f13b3a2df133f073095dd-Metadata.json},
 openalex = {W2117242571},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Segmentation as Maximum-Weight Independent Set},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a0a080f42e6f13b3a2df133f073095dd-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a1d33d0d,
 abstract = {We cast the problem of identifying basic blocks of code in a binary executable as learning a mapping from a byte sequence to a segmentation of the sequence. In general, inference in segmentation models, such as semi-CRFs, can be cubic in the length of the sequence. By taking advantage of the structure of our problem, we derive a linear-time inference algorithm which makes our approach practical, given that even small programs are tens or hundreds of thousands bytes long. Furthermore, we introduce two loss functions which are appropriate for our problem and show how to use structural SVMs to optimize the learned mapping for these losses. Finally, we present experimental results that demonstrate the advantages of our method against a strong baseline.},
 author = {Karampatziakis, Nikos},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a1d33d0dfec820b41b54430b50e96b5c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a1d33d0dfec820b41b54430b50e96b5c-Metadata.json},
 openalex = {W2104912713},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a1d33d0dfec820b41b54430b50e96b5c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Static Analysis of Binary Executables Using Structural SVMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a1d33d0dfec820b41b54430b50e96b5c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a49e9411,
 abstract = {Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multi-view learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation.},
 author = {Jia, Yangqing and Salzmann, Mathieu and Darrell, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a49e9411d64ff53eccfdd09ad10a15b3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a49e9411d64ff53eccfdd09ad10a15b3-Metadata.json},
 openalex = {W2156872152},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Factorized Latent Spaces with Structured Sparsity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a50abba8,
 abstract = {The human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes. Previous work has assumed that two different mechanisms are involved in processing these two types of motion. In this paper, we propose a hierarchical model as a unified framework for modeling both short-range and long-range motion perception. Our model consists of two key components: a data likelihood that proposes multiple motion hypotheses using nonlinear matching, and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion field at multiple scales. We tested our model on two types of stimuli, random dot kinematograms and multiple-aperture stimuli, both commonly used in human vision research. We demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments.},
 author = {Wu, Shuang and He, Xuming and Lu, Hongjing and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a50abba8132a77191791390c3eb19fe7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a50abba8132a77191791390c3eb19fe7-Metadata.json},
 openalex = {W2140780313},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A unified model of short-range and long-range motion perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a50abba8132a77191791390c3eb19fe7-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a51fb975,
 abstract = {In order to study the properties of total visual input in humans, a single subject wore a camera for two weeks capturing, on average, an image every 20 seconds. The resulting new dataset contains a mix of indoor and outdoor scenes as well as numerous foreground objects. Our first goal is to create a visual summary of the subject's two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes, familiar faces or common actions. Direct application of existing algorithms, such as panoramic stitching (e.g., Photosynth) or appearance-based clustering models (e.g., the epitome), is impractical due to either the large dataset size or the dramatic variations in the lighting conditions. As a remedy to these problems, we introduce a novel image representation, the structural element (stel) epitome, and an associated efficient learning algorithm. In our model, each image or image patch is characterized by a hidden mapping T which, as in previous epitome models, defines a mapping between the image coordinates and the coordinates in the large all-I-have-seen epitome matrix. The limited epitome real-estate forces the mappings of different images to overlap which indicates image similarity. However, the image similarity no longer depends on direct pixel-to-pixel intensity/color/feature comparisons as in previous epitome models, but on spatial configuration of scene or object parts, as the model is based on the palette-invariant stel models. As a result, stel epitomes capture structure that is invariant to non-structural changes, such as illumination changes, that tend to uniformly affect pixels belonging to a single scene or object part.},
 author = {Jojic, Nebojsa and Perina, Alessandro and Murino, Vittorio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a51fb975227d6640e4fe47854476d133-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a51fb975227d6640e4fe47854476d133-Metadata.json},
 openalex = {W2152042401},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a51fb975227d6640e4fe47854476d133-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a51fb975227d6640e4fe47854476d133-Supplemental.zip},
 title = {Structural epitome: a way to summarize oneâ€™s visual experience},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a51fb975227d6640e4fe47854476d133-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a5e00132,
 abstract = {Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a flexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. One can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data.},
 author = {Ghahramani, Zoubin and Jordan, Michael and Adams, Ryan P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a5e00132373a7031000fd987a3c9f87b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a5e00132373a7031000fd987a3c9f87b-Metadata.json},
 openalex = {W2123094878},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a5e00132373a7031000fd987a3c9f87b-Supplemental.zip},
 title = {Tree-Structured Stick Breaking for Hierarchical Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a8e864d0,
 abstract = {We consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples. In particular, we study the least-squares temporal difference (LSTD) learning algorithm when a space of low dimension is generated with a random projection from a high-dimensional space. We provide a thorough theoretical analysis of the LSTD with random projections and derive performance bounds for the resulting algorithm. We also show how the error of LSTD with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration (LSPI) algorithm.},
 author = {Ghavamzadeh, Mohammad and Lazaric, Alessandro and Maillard, Odalric and Munos, R\'{e}mi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a8e864d04c95572d1aece099af852d0a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a8e864d04c95572d1aece099af852d0a-Metadata.json},
 openalex = {W2132301339},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {LSTD with Random Projections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a8e864d04c95572d1aece099af852d0a-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a8f15eda,
 abstract = {The goal of inverse reinforcement learning is to find a reward function for a Markov decision process, given example traces from its optimal policy. Current IRL techniques generally rely on user-supplied features that form a concise basis for the reward. We present an algorithm that instead constructs reward features from a large collection of component features, by building logical conjunctions of those component features that are relevant to the example policy. Given example traces, the algorithm returns a reward function as well as the constructed features. The reward function can be used to recover a full, deterministic, stationary policy, and the features can be used to transplant the reward function into any novel environment on which the component features are well defined.},
 author = {Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a8f15eda80c50adb0e71943adc8015cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a8f15eda80c50adb0e71943adc8015cf-Metadata.json},
 openalex = {W2162009473},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a8f15eda80c50adb0e71943adc8015cf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Feature Construction for Inverse Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a8f15eda80c50adb0e71943adc8015cf-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_a9b7ba70,
 abstract = {In the neural-network parameter space, an field is likely to be induced by singularities. In such a singularity region, first-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a flat region). Therefore, it may be confused with attractive local minima. Our analysis shows that the Hessian matrix of E tends to be indefinite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to confirm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efficient methods have been previously developed that avoided plateaus.},
 author = {Mizutani, Eiji and Dreyfus, Stuart},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a9b7ba70783b617e9998dc4dd82eb3c5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a9b7ba70783b617e9998dc4dd82eb3c5-Metadata.json},
 openalex = {W2155147726},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {An analysis on negative curvature induced by singularity in multi-layer neural-network learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/a9b7ba70783b617e9998dc4dd82eb3c5-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_aa169b49,
 abstract = {The standard strategy for efficient object detection consists of building a cascade composed of several binary classifiers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classifiers, and concentrates the computation on difficult parts of the image which cannot be trivially rejected.

We introduce a novel algorithm to construct jointly the classifiers of such a cascade, which interprets the response of a classifier as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set.

Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efficiency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines.},
 author = {Lefakis, Leonidas and Fleuret, Francois},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aa169b49b583a2b5af89203c2b78c67c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aa169b49b583a2b5af89203c2b78c67c-Metadata.json},
 openalex = {W2097735973},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Joint Cascade Optimization Using A Product Of Boosted Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/aa169b49b583a2b5af89203c2b78c67c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_abea47ba,
 abstract = {With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8].},
 author = {Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Metadata.json},
 openalex = {W2166706236},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Supplemental.zip},
 title = {Parallelized Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/abea47ba24142ed16b7d8fbf2c740e0d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_acf4b89d,
 abstract = {Although the Dirichlet distribution is widely used, the independence structure of its components limits its accuracy as a model. The proposed shadow Dirichlet distribution manipulates the support in order to model probability mass functions (pmfs) with dependencies or constraints that often arise in real world problems, such as regularized pmfs, monotonic pmfs, and pmfs with bounded variation. We describe some properties of this new class of distributions, provide maximum entropy constructions, give an expectation-maximization method for estimating the mean parameter, and illustrate with real data.},
 author = {Frigyik, Bela and Gupta, Maya and Chen, Yihua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Metadata.json},
 openalex = {W2114741189},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Shadow Dirichlet for Restricted Probability Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/acf4b89d3d503d8252c9c4ba75ddbf6d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_adc8ca1b,
 abstract = {We propose a new LP relaxation for obtaining the MAP assignment of a binary MRF with pairwise potentials. Our relaxation is derived from reducing the MAP assignment problem to an instance of a recently proposed Bipartite Multi-cut problem where the LP relaxation is guaranteed to provide an O(log k) approximation where k is the number of vertices adjacent to non-submodular edges in the MRF. We then propose a combinatorial algorithm to efficiently solve the LP and also provide a lower bound by concurrently solving its dual to within an e approximation. The algorithm is up to an order of magnitude faster and provides better MAP scores and bounds than the state of the art message passing algorithm of [1] that tightens the local marginal polytope with third-order marginal constraints.},
 author = {J. Reddi, Sashank and Sarawagi, Sunita and Vishwanathan, Sundar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/adc8ca1b15e20915c3ea6008fc2f52ed-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/adc8ca1b15e20915c3ea6008fc2f52ed-Metadata.json},
 openalex = {W2127162427},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/adc8ca1b15e20915c3ea6008fc2f52ed-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/adc8ca1b15e20915c3ea6008fc2f52ed-Supplemental.zip},
 title = {MAP estimation in Binary MRFs via Bipartite Multi-cuts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/adc8ca1b15e20915c3ea6008fc2f52ed-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_addfa9b7,
 abstract = {Arithmetic circuits (ACs) exploit context-specific independence and determinism to allow exact inference even in networks with high treewidth. In this paper, we introduce the first ever approximate inference methods using ACs, for domains where exact inference remains intractable. We propose and evaluate a variety of techniques based on exact compilation, forward sampling, AC structure learning, Markov network parameter learning, variational inference, and Gibbs sampling. In experiments on eight challenging real-world domains, we find that the methods based on sampling and learning work best: one such method (AC2-F) is faster and usually more accurate than loopy belief propagation, mean field, and Gibbs sampling; another (AC2-G) has a running time similar to Gibbs sampling but is consistently more accurate than all baselines.},
 author = {Lowd, Daniel and Domingos, Pedro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/addfa9b7e234254d26e9c7f2af1005cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/addfa9b7e234254d26e9c7f2af1005cb-Metadata.json},
 openalex = {W2164530010},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/addfa9b7e234254d26e9c7f2af1005cb-Supplemental.zip},
 title = {Approximate Inference by Compilation to Arithmetic Circuits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_aeb3135b,
 abstract = {We propose a computationally efficient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efficient method for implementing mixture forecasting strategies.},
 author = {Narayanan, Hariharan and Rakhlin, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aeb3135b436aa55373822c010763dd54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aeb3135b436aa55373822c010763dd54-Metadata.json},
 openalex = {W2110947625},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aeb3135b436aa55373822c010763dd54-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Random Walk Approach to Regret Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/aeb3135b436aa55373822c010763dd54-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_afda3322,
 abstract = {We apply the framework of kernel dimension reduction, originally designed for supervised problems, to unsupervised dimensionality reduction. In this framework, kernel-based measures of independence are used to derive low-dimensional representations that maximally capture information in covariates in order to predict responses. We extend this idea and develop similarly motivated measures for unsupervised problems where covariates and responses are the same. Our empirical studies show that the resulting compact representation yields meaningful and appealing visualization and clustering of data. Furthermore, when used in conjunction with supervised learners for classification, our methods lead to lower classification errors than state-of-the-art methods, especially when embedding data in spaces of very few dimensions.},
 author = {Wang, Meihong and Sha, Fei and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/afda332245e2af431fb7b672a68b659d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/afda332245e2af431fb7b672a68b659d-Metadata.json},
 openalex = {W2116470445},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/afda332245e2af431fb7b672a68b659d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/afda332245e2af431fb7b672a68b659d-Supplemental.zip},
 title = {Unsupervised Kernel Dimension Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/afda332245e2af431fb7b672a68b659d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_aff16212,
 author = {Quadrianto, Novi and Petterson, James and Caetano, Tib\'{e}rio and Smola, Alex and Vishwanathan, S.v.n.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aff1621254f7c1be92f64550478c56e6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aff1621254f7c1be92f64550478c56e6-Metadata.json},
 openalex = {W2276230513},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/aff1621254f7c1be92f64550478c56e6-Supplemental.zip},
 title = {Multitask Learning without Label Correspondences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/aff1621254f7c1be92f64550478c56e6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_b20bb95a,
 abstract = {The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized algorithmic approach, whereas most sparse PCA methods are framed as convex optimization problems. In this paper, we try to understand CUR from a sparse optimization viewpoint. We show that CUR is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse PCA method. We also observe that the sparsity attained by CUR possesses an interesting structure, which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.},
 author = {Bien, Jacob and Xu, Ya and Mahoney, Michael W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b20bb95ab626d93fd976af958fbc61ba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b20bb95ab626d93fd976af958fbc61ba-Metadata.json},
 openalex = {W2134327945},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {CUR from a Sparse Optimization Viewpoint},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/b20bb95ab626d93fd976af958fbc61ba-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_b2eb7349,
 abstract = {In this paper, we regard clustering as ensembles of k-ary affinity relations and clusters correspond to subsets of objects with maximal average affinity relations. The average affinity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efficient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a unified solution to clustering from k-ary affinity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.},
 author = {Liu, Hairong and Latecki, Longin and Yan, Shuicheng},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b2eb7349035754953b57a32e2841bda5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b2eb7349035754953b57a32e2841bda5-Metadata.json},
 openalex = {W2136440888},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b2eb7349035754953b57a32e2841bda5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Robust Clustering as Ensembles of Affinity Relations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/b2eb7349035754953b57a32e2841bda5-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_b2f627ff,
 abstract = {We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overfitting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: first, on the regularity of the target regression function and second, on the intrinsic dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if the true regression function belongs to the reproducing kernel Hilbert space. If this assumption is not fulfilled, we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates match state-of-the-art results that were recently obtained for least squares support vector machines and for linear regularization operators.},
 author = {Blanchard, Gilles and Kr\"{a}mer, Nicole},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b2f627fff19fda463cb386442eac2b3d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b2f627fff19fda463cb386442eac2b3d-Metadata.json},
 openalex = {W2949929740},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b2f627fff19fda463cb386442eac2b3d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b2f627fff19fda463cb386442eac2b3d-Supplemental.zip},
 title = {Optimal learning rates for Kernel Conjugate Gradient regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/b2f627fff19fda463cb386442eac2b3d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_b534ba68,
 abstract = {For a density f on ℝd, a high-density cluster is any connected component of {x : f(x) ≥ λ}, for some λ < 0. The set of all high-density clusters form a hierarchy called the cluster tree of f. We present a procedure for estimating the cluster tree given samples from f. We give finite-sample convergence rates for our algorithm, as well as lower bounds on the sample complexity of this estimation problem.},
 author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b534ba68236ba543ae44b22bd110a1d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b534ba68236ba543ae44b22bd110a1d6-Metadata.json},
 openalex = {W2169224215},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b534ba68236ba543ae44b22bd110a1d6-Supplemental.zip},
 title = {Rates of convergence for the cluster tree},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/b534ba68236ba543ae44b22bd110a1d6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_b56a18e0,
 abstract = {We study a setting in which Poisson processes generate sequences of decision-making events. The optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions. We model the problem as a Poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efficiently. This problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events. We report on experiments on abuse detection for an email service.},
 author = {Dick, Uwe and Haider, Peter and Vanck, Thomas and Br\"{u}ckner, Michael and Scheffer, Tobias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b56a18e0eacdf51aa2a5306b0f533204-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b56a18e0eacdf51aa2a5306b0f533204-Metadata.json},
 openalex = {W2103380815},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Throttling Poisson Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/b56a18e0eacdf51aa2a5306b0f533204-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_b5dc4e5d,
 abstract = {Algorithms based on iterative local approximations present a practical approach to optimal control in robotic systems. However, they generally require the temporal parameters (for e.g. the movement duration or the time point of reaching an intermediate goal) to be specified a priori. Here, we present a methodology that is capable of jointly optimizing the temporal parameters in addition to the control command profiles. The presented approach is based on a Bayesian canonical time formulation of the optimal control problem, with the temporal mapping from canonical to real time parametrised by an additional control variable. An approximate EM algorithm is derived that efficiently optimizes both the movement duration and control commands offering, for the first time, a practical approach to tackling generic via point problems in a systematic way under the optimal control framework. The proposed approach, which is applicable to plants with non-linear dynamics as well as arbitrary state dependent and quadratic control costs, is evaluated on realistic simulations of a redundant robotic plant.},
 author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b5dc4e5d9b495d0196f61d45b26ef33e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b5dc4e5d9b495d0196f61d45b26ef33e-Metadata.json},
 openalex = {W2117567237},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b5dc4e5d9b495d0196f61d45b26ef33e-Supplemental.zip},
 title = {An Approximate Inference Approach to Temporal Optimization in Optimal Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/b5dc4e5d9b495d0196f61d45b26ef33e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_b73ce398,
 abstract = {Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date.},
 author = {Dahl, George and Ranzato, Marc\textquotesingle aurelio and Mohamed, Abdel-rahman and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b73ce398c39f506af761d2277d853a92-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b73ce398c39f506af761d2277d853a92-Metadata.json},
 openalex = {W2103359087},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/b73ce398c39f506af761d2277d853a92-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/b73ce398c39f506af761d2277d853a92-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_bbf94b34,
 abstract = {Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms.},
 author = {Kim, Taehwan and Shakhnarovich, Gregory and Urtasun, Raquel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bbf94b34eb32268ada57a3be5062fe7d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bbf94b34eb32268ada57a3be5062fe7d-Metadata.json},
 openalex = {W2163084993},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bbf94b34eb32268ada57a3be5062fe7d-Supplemental.zip},
 title = {Sparse Coding for Learning Interpretable Spatio-Temporal Primitives},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/bbf94b34eb32268ada57a3be5062fe7d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_bcc0d400,
 abstract = {We consider multivariate regression problems involving high-dimensional predictor and response spaces. To efficiently address such problems, we propose a variable selection method, Multivariate Group Orthogonal Matching Pursuit, which extends the standard Orthogonal Matching Pursuit technique. This extension accounts for arbitrary sparsity patterns induced by domain-specific groupings over both input and output variables, while also taking advantage of the correlation that may exist between the multiple outputs. Within this framework, we then formulate the problem of inferring causal relationships over a collection of high-dimensional time series variables. When applied to time-evolving social media content, our models yield a new family of causality-based influence measures that may be seen as an alternative to the classic PageRank algorithm traditionally applied to hyperlink graphs. Theoretical guarantees, extensive simulations and empirical studies confirm the generality and value of our framework.},
 author = {Sindhwani, Vikas and Lozano, Aurelie C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Metadata.json},
 openalex = {W2168263014},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_be3159ad,
 abstract = {Multi-label classification is the task of predicting potentially multiple labels for a given instance. This is common in several applications such as image annotation, document classification and gene function prediction. In this paper we present a formulation for this problem based on reverse prediction: we predict sets of instances given the labels. By viewing the problem from this perspective, the most popular quality measures for assessing the performance of multi-label classification admit relaxations that can be efficiently optimised. We optimise these relaxations with standard algorithms and compare our results with several state-of-the-art methods, showing excellent performance.},
 author = {Petterson, James and Caetano, Tib\'{e}rio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/be3159ad04564bfb90db9e32851ebf9c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/be3159ad04564bfb90db9e32851ebf9c-Metadata.json},
 openalex = {W2110075577},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/be3159ad04564bfb90db9e32851ebf9c-Supplemental.zip},
 title = {Reverse Multi-Label Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/be3159ad04564bfb90db9e32851ebf9c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_bf62768c,
 abstract = {Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this flexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efficient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efficiency; and on the other hand, the CVI algorithm efficiently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efficient yet does not sacrifice its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is significantly more efficient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.},
 author = {Lao, Ni and Zhu, Jun and Liu, Liu and Liu, Yandong and Cohen, William W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bf62768ca46b6c3b5bea9515d1a1fc45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bf62768ca46b6c3b5bea9515d1a1fc45-Metadata.json},
 openalex = {W2148774118},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Relational Learning with Hidden Variable Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/bf62768ca46b6c3b5bea9515d1a1fc45-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c0f168ce,
 abstract = {We provide a sound and consistent foundation for the use of nonrandom exploration data in contextual bandit or partially labeled settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which offline data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.},
 author = {Strehl, Alex and Langford, John and Li, Lihong and Kakade, Sham M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c0f168ce8900fa56e57789e2a2f2c9d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c0f168ce8900fa56e57789e2a2f2c9d0-Metadata.json},
 openalex = {W2113065326},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning from Logged Implicit Exploration Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c0f168ce8900fa56e57789e2a2f2c9d0-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c2626d85,
 abstract = {We consider structured multi-armed bandit problems based on the Generalized Linear Model (GLM) framework of statistics. For these bandits, we propose a new algorithm, called GLM-UCB. We derive finite time, high probability bounds on the regret of the algorithm, extending previous analyses developed for the linear bandits to the non-linear case. The analysis highlights a key difficulty in generalizing linear bandit algorithms to the non-linear case, which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual effectiveness of current parameterized bandit algorithms is often poor in practice, we provide a tuning method based on asymptotic arguments, which leads to significantly better practical performance. We present two numerical experiments on real-world data that illustrate the potential of the GLM-UCB approach.},
 author = {Filippi, Sarah and Cappe, Olivier and Garivier, Aur\'{e}lien and Szepesv\'{a}ri, Csaba},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Metadata.json},
 openalex = {W2160163723},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Supplemental.zip},
 title = {Parametric Bandits: The Generalized Linear Case},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c2626d850c80ea07e7511bbae4c76f4b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c2aee861,
 abstract = {This paper explores links between basis construction methods in Markov decision processes and power series expansions of value functions. This perspective provides a useful framework to analyze properties of existing bases, as well as provides insight into constructing more effective bases. Krylov and Bellman error bases are based on the Neumann series expansion. These bases incur very large initial Bellman errors, and can converge rather slowly as the discount factor approaches unity. The Laurent series expansion, which relates discounted and average-reward formulations, provides both an explanation for this slow convergence as well as suggests a way to construct more efficient basis representations. The first two terms in the Laurent series represent the scaled average-reward and the average-adjusted sum of rewards, and subsequent terms expand the discounted value function using powers of a generalized inverse called the Drazin (or group inverse) of a singular matrix derived from the transition matrix. Experiments show that Drazin bases converge considerably more quickly than several other bases, particularly for large values of the discount factor. An incremental variant of Drazin bases called Bellman average-reward bases (BARBs) is described, which provides some of the same benefits at lower computational cost.},
 author = {Mahadevan, Sridhar and Liu, Bo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c2aee86157b4a40b78132f1e71a9e6f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c2aee86157b4a40b78132f1e71a9e6f1-Metadata.json},
 openalex = {W2151283311},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Basis Construction from Power Series Expansions of Value Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c2aee86157b4a40b78132f1e71a9e6f1-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c410003e,
 abstract = {From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classification based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach.},
 author = {Fisher, Nicholas and Banerjee, Arunava},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c410003ef13d451727aeff9082c29a5c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c410003ef13d451727aeff9082c29a5c-Metadata.json},
 openalex = {W2152982954},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c410003ef13d451727aeff9082c29a5c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Novel Kernel for Learning a Neuron Model from Spike Train Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c410003ef13d451727aeff9082c29a5c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c44e5038,
 abstract = {We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning.},
 author = {Doshi-velez, Finale and Wingate, David and Roy, Nicholas and Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c44e503833b64e9f27197a484f4257c0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c44e503833b64e9f27197a484f4257c0-Metadata.json},
 openalex = {W2119678437},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c44e503833b64e9f27197a484f4257c0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Nonparametric Bayesian Policy Priors for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c44e503833b64e9f27197a484f4257c0-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c73dfe6c,
 abstract = {In Learning Using Privileged Information (LUPI) paradigm, along with the standard training data in the decision space, a teacher supplies a learner with the privileged information in the correcting space. The goal of the learner is to find a classifier with a low generalization error in the decision space. We consider a new version of empirical risk minimization algorithm, called Privileged ERM, that takes into account the privileged information in order to find a good function in the decision space. We outline the conditions on the correcting space that, if satisfied, allow Privileged ERM to have much faster learning rate in the decision space than the one of the regular empirical risk minimization.},
 author = {Pechyony, Dmitry and Vapnik, Vladimir},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c73dfe6c630edb4c1692db67c510f65c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c73dfe6c630edb4c1692db67c510f65c-Metadata.json},
 openalex = {W2141225381},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c73dfe6c630edb4c1692db67c510f65c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c73dfe6c630edb4c1692db67c510f65c-Supplemental.zip},
 title = {On the Theory of Learnining with Privileged Information},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c73dfe6c630edb4c1692db67c510f65c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c7e1249f,
 abstract = {Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive field or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable influence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the firing rate of many individual units recorded simultaneously from V1 with a 96-electrode "Utah" array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron's response, in addition to the neuron's receptive field transfer function. We also found that the same spikes could be accounted for with the local field potentials, a surrogate measure of global network states. This work shows that accounting for network fluctuations can improve estimates of single trial firing rate and stimulus-response transfer functions.},
 author = {Kelly, Ryan and Smith, Matthew and Kass, Robert and Lee, Tai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c7e1249ffc03eb9ded908c236bd1996d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c7e1249ffc03eb9ded908c236bd1996d-Metadata.json},
 openalex = {W2097064168},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c7e1249ffc03eb9ded908c236bd1996d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Accounting for network effects in neuronal responses using L1 regularized point process models.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c7e1249ffc03eb9ded908c236bd1996d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c850371f,
 abstract = {We propose a novel method for inferring whether X causes Y or vice versa from joint observations of X and Y. The basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise. To this end, we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term (not necessarily additive). An important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause. The causal direction can then be inferred by using standard Bayesian model selection. We evaluate our approach on synthetic data and real-world data and report encouraging results.},
 author = {Stegle, Oliver and Janzing, Dominik and Zhang, Kun and Mooij, Joris M and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c850371fda6892fbfd1c5a5b457e5777-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c850371fda6892fbfd1c5a5b457e5777-Metadata.json},
 openalex = {W2164001983},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c850371fda6892fbfd1c5a5b457e5777-Supplemental.zip},
 title = {Probabilistic latent variable models for distinguishing between cause and effect},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c850371fda6892fbfd1c5a5b457e5777-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c8ffe9a5,
 abstract = {This paper is concerned with the generalization analysis on learning to rank for information retrieval (IR). In IR, data are hierarchically organized, i.e., consisting of queries and documents. Previous generalization analysis for ranking, however, has not fully considered this structure, and cannot explain how the simultaneous change of query number and document number in the training data will affect the performance of the learned ranking model. In this paper, we propose performing generalization analysis under the assumption of two-layer sampling, i.e., the i.i.d. sampling of queries and the conditional i.i.d sampling of documents per query. Such a sampling can better describe the generation mechanism of real data, and the corresponding generalization analysis can better explain the real behaviors of learning to rank algorithms. However, it is challenging to perform such analysis, because the documents associated with different queries are not identically distributed, and the documents associated with the same query become no longer independent after represented by features extracted from query-document matching. To tackle the challenge, we decompose the expected risk according to the two layers, and make use of the new concept of two-layer Rademacher average. The generalization bounds we obtained are quite intuitive and are in accordance with previous empirical studies on the performances of ranking algorithms.},
 author = {Chen, Wei and Liu, Tie-yan and Ma, Zhi-ming},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c8ffe9a587b126f152ed3d89a146b445-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c8ffe9a587b126f152ed3d89a146b445-Metadata.json},
 openalex = {W2098929588},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c8ffe9a587b126f152ed3d89a146b445-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c8ffe9a587b126f152ed3d89a146b445-Supplemental.zip},
 title = {Two-Layer Generalization Analysis for Ranking Using Rademacher Average},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c8ffe9a587b126f152ed3d89a146b445-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_c9e1074f,
 abstract = {In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classifiers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difficult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efficient algorithm to solve it. Experiments conducted on artificial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to an SVM trained with the ground-truth labels, and outperforms other baselines.},
 author = {Luo, Jie and Orabona, Francesco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c9e1074f5b3f9fc8ea15d152add07294-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c9e1074f5b3f9fc8ea15d152add07294-Metadata.json},
 openalex = {W2111478152},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning from Candidate Labeling Sets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/c9e1074f5b3f9fc8ea15d152add07294-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_ca8155f4,
 abstract = {In discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance, or loss. In binary classification one typically tries to minimizes the error rate. But in structured prediction each task often has its own measure of performance such as the BLEU score in machine translation or the intersection-over-union score in PASCAL segmentation. The most common approaches to structured prediction, structural SVMs and CRFs, do not minimize the task loss: the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss. The main contribution of this paper is a theorem stating that a certain perceptron-like learning rule, involving features vectors derived from loss-adjusted inference, directly corresponds to the gradient of task loss. We give empirical results on phonetic alignment of a standard test set from the TIMIT corpus, which surpasses all previously reported results on this problem.},
 author = {Hazan, Tamir and Keshet, Joseph and McAllester, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ca8155f4d27f205953f9d3d7974bdd70-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ca8155f4d27f205953f9d3d7974bdd70-Metadata.json},
 openalex = {W2099119623},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Direct Loss Minimization for Structured Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/ca8155f4d27f205953f9d3d7974bdd70-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_cb70ab37,
 abstract = {Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems [1, 2, 3], theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference [4]. Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models [5]; unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments. We propose a new framework that extends variational inference to a wide range of combinatorial spaces. Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset [6].},
 author = {Bouchard-c\^{o}t\'{e}, Alexandre and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cb70ab375662576bd1ac5aaf16b3fca4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cb70ab375662576bd1ac5aaf16b3fca4-Metadata.json},
 openalex = {W2129391075},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cb70ab375662576bd1ac5aaf16b3fca4-Supplemental.zip},
 title = {Variational Inference over Combinatorial Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_cc1aa436,
 abstract = {When software developers modify one or more files in a large code base, they must also identify and update other related files. Many file dependencies can be detected by mining the development history of the code base: in essence, groups of related files are revealed by the logs of previous workflows. From data of this form, we show how to detect dependent files by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Fire-fox, Eclipse Subversive, and Gimp. In all of these applications, we find that LVMs improve the performance of related file prediction over current leading methods.},
 author = {Hu, Diane and Maaten, Laurens and Cho, Youngmin and Lerner, Sorin and Saul, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cc1aa436277138f61cda703991069eaf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cc1aa436277138f61cda703991069eaf-Metadata.json},
 openalex = {W2153404345},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cc1aa436277138f61cda703991069eaf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/cc1aa436277138f61cda703991069eaf-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_ccb09896,
 abstract = {In multi-instance learning, there are two kinds of prediction failure, i.e., false negative and false positive. Current research mainly focus on avoiding the former. We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. Based on kernel principal component analysis, we define a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.},
 author = {Han, Yanjun and Tao, Qing and Wang, Jue},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ccb0989662211f61edae2e26d58ea92f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ccb0989662211f61edae2e26d58ea92f-Metadata.json},
 openalex = {W2129395285},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Avoiding False Positive in Multi-Instance Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/ccb0989662211f61edae2e26d58ea92f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_ccb1d45f,
 abstract = {Studying connectivities among functional brain regions and the functional dynamics on brain networks has drawn increasing interest. A fundamental issue that affects functional connectivity and dynamics studies is how to determine the best possible functional brain regions or ROIs (regions of interest) for a group of individuals, since the connectivity measurements are heavily dependent on ROI locations. Essentially, identification of accurate, reliable and consistent corresponding ROIs is challenging due to the unclear boundaries between brain regions, variability across individuals, and nonlinearity of the ROIs. In response to these challenges, this paper presents a novel methodology to computationally optimize ROIs locations derived from task-based fMRI data for individuals so that the optimized ROIs are more consistent, reproducible and predictable across brains. Our computational strategy is to formulate the individual ROI location optimization as a group variance minimization problem, in which group-wise consistencies in functional/structural connectivity patterns and anatomic profiles are defined as optimization constraints. Our experimental results from multimodal fMRI and DTI data show that the optimized ROIs have significantly improved consistency in structural and functional profiles across individuals. These improved functional ROIs with better consistency could contribute to further study of functional interaction and dynamics in the human brain.},
 author = {Li, Kaiming and Guo, Lei and Faraco, Carlos and Zhu, Dajiang and Deng, Fan and Zhang, Tuo and Jiang, Xi and Zhang, Degang and Chen, Hanbo and Hu, Xintao and Miller, Steve and Liu, Tianming},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Metadata.json},
 openalex = {W2017166818},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Individual Functional ROI Optimization Via Maximization of Group-Wise Consistency of Structural and Functional Profiles},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/ccb1d45fb76f7c5a0bf619f979c6cf36-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_cd758e8f,
 abstract = {In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1-like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks of thousands of nodes in a matter of minutes.},
 author = {Myers, Seth and Leskovec, Jure},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cd758e8f59dfdf06a852adad277986ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cd758e8f59dfdf06a852adad277986ca-Metadata.json},
 openalex = {W2949064044},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cd758e8f59dfdf06a852adad277986ca-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {On the Convexity of Latent Social Network Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/cd758e8f59dfdf06a852adad277986ca-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_cfbce4c1,
 abstract = {The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the first stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits significantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output.},
 author = {Arora, Nimar and Russell, Stuart J and Kidwell, Paul and Sudderth, Erik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cfbce4c1d7c425baf21d6b6f2babe6be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cfbce4c1d7c425baf21d6b6f2babe6be-Metadata.json},
 openalex = {W2132231663},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Global seismic monitoring as probabilistic inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/cfbce4c1d7c425baf21d6b6f2babe6be-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d09bf415,
 abstract = {We present a technique for exact simulation of Gaussian Markov random fields (GMRFs), which can be interpreted as locally injecting noise to each Gaussian factor independently, followed by computing the mean/mode of the perturbed GMRF. Coupled with standard iterative techniques for the solution of symmetric positive definite systems, this yields a very efficient sampling algorithm with essentially linear complexity in terms of speed and memory requirements, well suited to extremely large scale probabilistic models. Apart from synthesizing data under a Gaussian model, the proposed technique directly leads to an efficient unbiased estimator of marginal variances. Beyond Gaussian models, the proposed algorithm is also very useful for handling highly non-Gaussian continuously-valued MRFs such as those arising in statistical image modeling or in the first layer of deep belief networks describing real-valued data, where the non-quadratic potentials coupling different sites can be represented as finite or infinite mixtures of Gaussians with the help of local or distributed latent mixture assignment variables. The Bayesian treatment of such models most naturally involves a block Gibbs sampler which alternately draws samples of the conditionally independent latent mixture assignments and the conditionally multivariate Gaussian continuous vector and we show that it can directly benefit from the proposed methods.},
 author = {Papandreou, George and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d09bf41544a3365a46c9077ebb5e35c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d09bf41544a3365a46c9077ebb5e35c3-Metadata.json},
 openalex = {W2133840092},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Gaussian sampling by local perturbations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d09bf41544a3365a46c9077ebb5e35c3-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d2ed45a5,
 abstract = {The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. We present a Deep Boltzmann Machine model of CBS, exploring two core hypotheses: First, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery. And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. We reproduce a variety of qualitative findings in CBS. We also introduce a modification to the DBM that allows us to model a possible role of acetylcholine in CBS as mediating the balance of feed-forward and feed-back processing. Our model might provide new insights into CBS and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception.},
 author = {Series, Peggy and Reichert, David and Storkey, Amos J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d2ed45a52bc0edfa11c2064e9edee8bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d2ed45a52bc0edfa11c2064e9edee8bf-Metadata.json},
 openalex = {W2151725582},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d2ed45a52bc0edfa11c2064e9edee8bf-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d490d7b4,
 abstract = {We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-defined tree structure is based on a group-Lasso penalty, where one group is defined for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efficient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efficient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efficiency and effectiveness of the proposed algorithm.},
 author = {Liu, Jun and Ye, Jieping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d490d7b4576290fa60eb31b5fc917ad1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d490d7b4576290fa60eb31b5fc917ad1-Metadata.json},
 openalex = {W2109907994},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Moreau-Yosida Regularization for Grouped Tree Structure Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d490d7b4576290fa60eb31b5fc917ad1-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d56b9fc4,
 abstract = {This paper tackles the complex problem of visually matching people in similar pose but with different clothes, background, and other appearance changes. We achieve this with a novel method for learning a nonlinear embedding based on several extensions to the Neighborhood Component Analysis (NCA) framework. Our method is convolutional, enabling it to scale to realistically-sized images. By cheaply labeling the head and hands in large video databases through Amazon Mechanical Turk (a crowd-sourcing service), we can use the task of localizing the head and hands as a proxy for determining body pose. We apply our method to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose. We evaluate our method quantitatively against other embedding methods. We also demonstrate that real-world performance can be improved through the use of synthetic data.},
 author = {Taylor, Graham W and Fergus, Rob and Williams, George and Spiro, Ian and Bregler, Christoph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Metadata.json},
 openalex = {W2166440240},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Supplemental.zip},
 title = {Pose-Sensitive Embedding by Nonlinear NCA Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d56b9fc4b0f1be8871f5e1c40c0067e7-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d64a340b,
 abstract = {This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that referto them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these.},
 author = {Johnson, Mark and Demuth, Katherine and Jones, Bevan and Black, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d64a340bcb633f536d56e51874281454-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d64a340bcb633f536d56e51874281454-Metadata.json},
 openalex = {W2170240579},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d64a340bcb633f536d56e51874281454-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Synergies in learning words and their referents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d64a340bcb633f536d56e51874281454-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d6723e7c,
 abstract = {We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We first consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efficient mean field approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.},
 author = {Opper, Manfred and Ruttor, Andreas and Sanguinetti, Guido},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d6723e7cd6735df68d1ce4c704c29a04-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d6723e7cd6735df68d1ce4c704c29a04-Metadata.json},
 openalex = {W2109462896},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d6723e7cd6735df68d1ce4c704c29a04-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Approximate inference in continuous time Gaussian-Jump processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d6723e7cd6735df68d1ce4c704c29a04-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d6ef5f7f,
 abstract = {We present original empirical Bernstein inequalities for U-statistics with bounded symmetric kernels q. They are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the Bernstein-type inequality for U-statistics derived by Arcones [2]. Our result subsumes other existing empirical Bernstein inequalities, as it reduces to them when U-statistics of order 1 are considered. In addition, it is based on a rather direct argument using two applications of the same (non-empirical) Bernstein inequality for U-statistics. We discuss potential applications of our new inequalities, especially in the realm of learning ranking/scoring functions. In the process, we exhibit an efficient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument. We also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions.},
 author = {Peel, Thomas and Anthoine, Sandrine and Ralaivola, Liva},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d6ef5f7fa914c19931a55bb262ec879c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d6ef5f7fa914c19931a55bb262ec879c-Metadata.json},
 openalex = {W2101723727},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Empirical Bernstein Inequalities for U-Statistics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d6ef5f7fa914c19931a55bb262ec879c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d7a728a6,
 abstract = {We address the problem of estimating the Fα-measure of a given model as accurately as possible on a fixed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reflect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of Fα-measures are more accurate than estimates based on instances sampled from the test distribution.},
 author = {Sawade, Christoph and Landwehr, Niels and Scheffer, Tobias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d7a728a67d909e714c0774e22cb806f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d7a728a67d909e714c0774e22cb806f2-Metadata.json},
 openalex = {W2128511958},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d7a728a67d909e714c0774e22cb806f2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d7a728a67d909e714c0774e22cb806f2-Supplemental.zip},
 title = {Active Estimation of F-Measures},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d7a728a67d909e714c0774e22cb806f2-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d86ea612,
 abstract = {In this paper we consider the problem of semi-supervised kernel function learning. We first propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. To this end, we introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the k-NN classification accuracy significantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies.},
 author = {Jain, Prateek and Kulis, Brian and Dhillon, Inderjit},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d86ea612dec96096c5e0fcc8dd42ab6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d86ea612dec96096c5e0fcc8dd42ab6d-Metadata.json},
 openalex = {W2168505892},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d86ea612dec96096c5e0fcc8dd42ab6d-Supplemental.zip},
 title = {Inductive Regularized Learning of Kernel Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_d93ed5b6,
 abstract = {While clinicians can accurately identify different types of heartbeats in electrocardiograms (ECGs) from different patients, researchers have had limited success in applying supervised machine learning to the same task. The problem is made challenging by the variety of tasks, inter- and intra-patient differences, an often severe class imbalance, and the high cost of getting cardiologists to label data for individual patients. We address these difficulties using active learning to perform patient-adaptive and task-adaptive heartbeat classification. When tested on a benchmark database of cardiologist annotated ECG recordings, our method had considerably better performance than other recently proposed methods on the two primary classification tasks recommended by the Association for the Advancement of Medical Instrumentation. Additionally, our method required over 90% less patient-specific training data than the methods to which we compared it.},
 author = {Wiens, Jenna and Guttag, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d93ed5b6db83be78efb0d05ae420158e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d93ed5b6db83be78efb0d05ae420158e-Metadata.json},
 openalex = {W2114527042},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/d93ed5b6db83be78efb0d05ae420158e-Supplemental.zip},
 title = {Active Learning Applied to Patient-Adaptive Heartbeat Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/d93ed5b6db83be78efb0d05ae420158e-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_da4fb5c6,
 abstract = {Matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (SfM), non-rigid SfM and photometric stereo. We formulate the problem of matrix factorization with missing data as a low-rank semidefinite program (LRSDP) with the advantage that: 1) an efficient quasi-Newton implementation of the LRSDP enables us to solve large-scale factorization problems, and 2) additional constraints such as ortho-normality, required in orthographic SfM, can be directly incorporated in the new formulation. Our empirical evaluations suggest that, under the conditions of matrix completion theory, the proposed algorithm finds the optimal solution, and also requires fewer observations compared to the current state-of-the-art algorithms. We further demonstrate the effectiveness of the proposed algorithm in solving the affine SfM problem, non-rigid SfM and photometric stereo problems.},
 author = {Mitra, Kaushik and Sheorey, Sameer and Chellappa, Rama},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/da4fb5c6e93e74d3df8527599fa62642-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/da4fb5c6e93e74d3df8527599fa62642-Metadata.json},
 openalex = {W2126708984},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/da4fb5c6e93e74d3df8527599fa62642-Supplemental.zip},
 title = {Large-Scale Matrix Factorization with Missing Data under Additional Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/da4fb5c6e93e74d3df8527599fa62642-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_dabd8d2c,
 abstract = {We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic finite automata (PDFA). We define and develop a sampler for a PDFA with an infinite number of states which we call the probabilistic deterministic infinite automata (PDIA). Posterior predictive inference in this model, given a finite training sequence, can be interpreted as averaging over multiple PDFAs of varying structure, where each PDFA is biased towards having few states. We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing. We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks. The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models.},
 author = {Pfau, David and Bartlett, Nicholas and Wood, Frank},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/dabd8d2ce74e782c65a973ef76fd540b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/dabd8d2ce74e782c65a973ef76fd540b-Metadata.json},
 openalex = {W2131537383},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Probabilistic Deterministic Infinite Automata},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/dabd8d2ce74e782c65a973ef76fd540b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_db576a7d,
 abstract = {Spontaneous brain activity, as observed in functional neuroimaging, has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies. An important view of modern neuroscience is that such large-scale structure of coherent activity reflects modularity properties of brain connectivity graphs. However, to date, there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data. Learning such models entails two main challenges: i) modeling full brain connectivity is a difficult estimation problem that faces the curse of dimensionality and ii) variability between subjects, coupled with the variability of functional signals between experimental runs, makes the use of multiple datasets challenging. We describe subject-level brain functional connectivity structure as a multivariate Gaussian process and introduce a new strategy to estimate it from group data, by imposing a common structure on the graphical model in the population. We show that individual models learned from functional Magnetic Resonance Imaging (fMRI) data using this population prior generalize better to unseen data than models based on alternative regularization schemes. To our knowledge, this is the first report of a cross-validated model of spontaneous brain activity. Finally, we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the first time that known cognitive networks appear as the integrated communities of functional connectivity graph.},
 author = {Varoquaux, Gael and Gramfort, Alexandre and Poline, Jean-baptiste and Thirion, Bertrand},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db576a7d2453575f29eab4bac787b919-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db576a7d2453575f29eab4bac787b919-Metadata.json},
 openalex = {W2138356120},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db576a7d2453575f29eab4bac787b919-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db576a7d2453575f29eab4bac787b919-Supplemental.zip},
 title = {Brain covariance selection: better individual functional connectivity models using population prior},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/db576a7d2453575f29eab4bac787b919-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_db85e259,
 abstract = {We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model.},
 author = {Petterson, James and Buntine, Wray and Narayanamurthy, Shravan and Caetano, Tib\'{e}rio and Smola, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db85e2590b6109813dafa101ceb2faeb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db85e2590b6109813dafa101ceb2faeb-Metadata.json},
 openalex = {W2145768976},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db85e2590b6109813dafa101ceb2faeb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/db85e2590b6109813dafa101ceb2faeb-Supplemental.zip},
 title = {Word Features for Latent Dirichlet Allocation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/db85e2590b6109813dafa101ceb2faeb-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_dc912a25,
 abstract = {In this paper we propose an approximated structured prediction framework for large scale graphical models and derive message-passing algorithms for learning their parameters efficiently. We first relate CRFs and structured SVMs and show that in CRFs a variant of the log-partition function, known as the soft-max, smoothly approximates the hinge loss function of structured SVMs. We then propose an intuitive approximation for the structured prediction problem, using duality, based on a local entropy approximation and derive an efficient message-passing algorithm that is guaranteed to converge. Unlike existing approaches, this allows us to learn efficiently graphical models with cycles and very large number of parameters.},
 author = {Hazan, Tamir and Urtasun, Raquel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/dc912a253d1e9ba40e2c597ed2376640-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/dc912a253d1e9ba40e2c597ed2376640-Metadata.json},
 openalex = {W2111161343},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/dc912a253d1e9ba40e2c597ed2376640-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_df7f28ac,
 abstract = {In this paper we consider the problem of learning an n x n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in O(m2 log n/e2) iterations; in each iteration one solves an MKL involving m kernels and m eigen-decomposition of n x n matrices. By suitably defining a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efficacy of the proposed algorithms.},
 author = {Kundu, Achintya and Tankasali, Vikram and Bhattacharyya, Chiranjib and Ben-tal, Aharon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Metadata.json},
 openalex = {W2121212585},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/df7f28ac89ca37bf1abd2f6c184fe1cf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/df7f28ac89ca37bf1abd2f6c184fe1cf-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e0040614,
 abstract = {We develop a theory of online learning by defining several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fat-shattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting.},
 author = {Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e00406144c1e7e35240afed70f34166a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e00406144c1e7e35240afed70f34166a-Metadata.json},
 openalex = {W2121367301},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e00406144c1e7e35240afed70f34166a-Supplemental.zip},
 title = {Online Learning: Random Averages, Combinatorial Parameters, and Learnability},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e00406144c1e7e35240afed70f34166a-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e0c64119,
 abstract = {The problem of controlling the margin of a classifier is studied. A detailed analytical study is presented on how properties of the classification risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classification margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the fixed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter.},
 author = {Masnadi-shirazi, Hamed and Vasconcelos, Nuno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e0c641195b27425bb056ac56f8953d24-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e0c641195b27425bb056ac56f8953d24-Metadata.json},
 openalex = {W2108897205},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e0c641195b27425bb056ac56f8953d24-Supplemental.zip},
 title = {Variable margin losses for classifier design},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e1e32e23,
 abstract = {In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial benefits over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems.},
 author = {Hannah, Lauren and Powell, Warren and Blei, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e1e32e235eee1f970470a3a6658dfdd5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e1e32e235eee1f970470a3a6658dfdd5-Metadata.json},
 openalex = {W2153034112},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e1e32e235eee1f970470a3a6658dfdd5-Supplemental.zip},
 title = {Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e2230b85,
 abstract = {Applications of Brain-Machine-Interfaces typically estimate user intent based on biological signals that are under voluntary control. For example, we might want to estimate how a patient with a paralyzed arm wants to move based on residual muscle activity. To solve such problems it is necessary to integrate obtained information over time. To do so, state of the art approaches typically use a probabilistic model of how the state, e.g. position and velocity of the arm, evolves over time - a so-called trajectory model. We wanted to further develop this approach using two intuitive insights: (1) At any given point of time there may be a small set of likely movement targets, potentially identified by the location of objects in the workspace or by gaze information from the user. (2) The user may want to produce movements at varying speeds. We thus use a generative model with a trajectory model incorporating these insights. Approximate inference on that generative model is implemented using a mixture of extended Kalman filters. We find that the resulting algorithm allows us to decode arm movements dramatically better than when we use a trajectory model with linear dynamics.},
 author = {Corbett, Elaine and Perreault, Eric and Koerding, Konrad},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e2230b853516e7b05d79744fbd4c9c13-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e2230b853516e7b05d79744fbd4c9c13-Metadata.json},
 openalex = {W2166867829},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e2230b853516e7b05d79744fbd4c9c13-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Mixture of time-warped trajectory models for movement decoding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e2230b853516e7b05d79744fbd4c9c13-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e2ef524f,
 abstract = {We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth region-to-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.},
 author = {Wang, Yang and Mori, Greg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Metadata.json},
 openalex = {W2156867888},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {A Discriminative Latent Model of Image Region and Object Tag Correspondence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e2ef524fbf3d9fe611d5a8e90fefdc9c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e53a0a29,
 abstract = {In a recent paper Joachims [1] presented SVM-Perf, a cutting plane method (CPM) for training linear Support Vector Machines (SVMs) which converges to an e accurate solution in O(1/e2) iterations. By tightening the analysis, Teo et al. [2] showed that O(1/e) iterations suffice. Given the impressive convergence speed of CPM on a number of practical problems, it was conjectured that these rates could be further improved. In this paper we disprove this conjecture. We present counter examples which are not only applicable for training linear SVMs with hinge loss, but also hold for support vector methods which optimize a multivariate performance score. However, surprisingly, these problems are not inherently hard. By exploiting the structure of the objective function we can devise an algorithm that converges in O(1/√e) iterations.},
 author = {Zhang, Xinhua and Saha, Ankan and Vishwanathan, S.v.n.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e53a0a2978c28872a4505bdb51db06dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e53a0a2978c28872a4505bdb51db06dc-Metadata.json},
 openalex = {W2163976407},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e53a0a2978c28872a4505bdb51db06dc-Supplemental.zip},
 title = {Lower Bounds on Rate of Convergence of Cutting Plane Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e57c6b95,
 abstract = {Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.},
 author = {Kumar, M. and Packer, Benjamin and Koller, Daphne},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e57c6b956a6521b28495f2886ca0977a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e57c6b956a6521b28495f2886ca0977a-Metadata.json},
 openalex = {W2132984949},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Self-Paced Learning for Latent Variable Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e57c6b956a6521b28495f2886ca0977a-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e5e63da7,
 abstract = {We present an algorithm for learning high-treewidth Markov networks where inference is still tractable. This is made possible by exploiting context-specific independence and determinism in the domain. The class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed-form weight learning, etc., but is much broader. Our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature and its negation) and recurses on each subspace/subset of variables until no useful new features can be found. We provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is bounded by a constant k (the treewidth can be much larger) and dependences are of bounded strength. We also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efficient. Experiments on a variety of domains show that our approach outperforms many state-of-the-art Markov network structure learners.},
 author = {Gogate, Vibhav and Webb, William and Domingos, Pedro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Metadata.json},
 openalex = {W2119008358},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning Efficient Markov Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e5e63da79fcd2bebbd7cb8bf1c1d0274-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e5f6ad6c,
 abstract = {We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ ℝn x m (m ≫ n) and a noisy observation vector y ∈ ℝn satisfying y = X β* + e where e is the noise vector following a Gaussian distribution N(0, σ2I), how to recover the signal (or parameter vector) β* when the signal is sparse?

The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively refines the target signal β*. We show that if X obeys a certain condition, then with a large probability the difference between the solution ^β estimated by the proposed method and the true solution β* measured in terms of the lp norm (p ≥ 1) is bounded as

‖^β - β ‖p ≤ (C(s - N)1/p √log m + Δ) σ,

where C is a constant, s is the number of nonzero entries in β*, A is independent of m and is much smaller than the first term, and N is the number of entries of β* larger than a certain value in the order of O(σ/√log m). The proposed method improves the estimation bound of the standard Dantzig selector approximately from Cs1/p √log mσ to C(s - N)1/p√log mσ where the value N depends on the number of large entries in β*. When N = s, the proposed algorithm achieves the oracle solution with a high probability. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector.},
 author = {Liu, Ji and Wonka, Peter and Ye, Jieping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e5f6ad6ce374177eef023bf5d0c018b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e5f6ad6ce374177eef023bf5d0c018b6-Metadata.json},
 openalex = {W2134326839},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e5f6ad6ce374177eef023bf5d0c018b6-Supplemental.zip},
 title = {Multi-Stage Dantzig Selector},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e5f6ad6ce374177eef023bf5d0c018b6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e702e51d,
 abstract = {Bayesian optimization methods are often used to optimize unknown functions that are costly to evaluate. Typically, these methods sequentially select inputs to be evaluated one at a time based on a posterior over the unknown function that is updated after each evaluation. In many applications, however, it is desirable to perform multiple evaluations in parallel, which requires selecting batches of multiple inputs to evaluate at once. In this paper, we propose a novel approach to batch Bayesian optimization, providing a policy for selecting batches of inputs with the goal of optimizing the function as efficiently as possible. The key idea is to exploit the availability of high-quality and efficient sequential policies, by using Monte-Carlo simulation to select input batches that closely match their expected behavior. Our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages over a top sequential approach in terms of performance per unit time.},
 author = {Azimi, Javad and Fern, Alan and Fern, Xiaoli},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Metadata.json},
 openalex = {W2112485705},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Batch Bayesian Optimization via Simulation Matching},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e702e51da2c0f5be4dd354bb3e295d37-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_e995f98d,
 abstract = {Heavy-tailed distributions naturally occur in many real life problems. Unfortunately, it is typically not possible to compute inference in closed-form in graphical models which involve such heavy-tailed distributions. 
In this work, we propose a novel simple linear graphical model for independent latent random variables, called linear characteristic model (LCM), defined in the characteristic function domain. Using stable distributions, a heavy-tailed family of distributions which is a generalization of Cauchy, L\'evy and Gaussian distributions, we show for the first time, how to compute both exact and approximate inference in such a linear multivariate graphical model. LCMs are not limited to stable distributions, in fact LCMs are always defined for any random variables (discrete, continuous or a mixture of both). 
We provide a realistic problem from the field of computer networks to demonstrate the applicability of our construction. Other potential application is iterative decoding of linear channels with non-Gaussian noise.},
 author = {Bickson, Danny and Guestrin, Carlos},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e995f98d56967d946471af29d7bf99f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e995f98d56967d946471af29d7bf99f1-Metadata.json},
 openalex = {W2144800733},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/e995f98d56967d946471af29d7bf99f1-Supplemental.zip},
 title = {Inference with Multivariate Heavy-Tails in Linear Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/e995f98d56967d946471af29d7bf99f1-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_eb160de1,
 abstract = {Scene understanding includes many related sub-tasks, such as scene categorization, depth estimation, object detection, etc. Each of these sub-tasks is often notoriously hard, and state-of-the-art classifiers already exist for many of them. These classifiers operate on the same raw image and provide correlated outputs. It is desirable to have an algorithm that can capture such correlation without requiring any changes to the inner workings of any classifier. We propose Feedback Enabled Cascaded Classification Models (FE-CCM), that jointly optimizes all the sub-tasks, while requiring only a `black-box' interface to the original classifier for each sub-task. We use a two-layer cascade of classifiers, which are repeated instantiations of the original ones, with the output of the first layer fed into the second layer as input. Our training method involves a feedback step that allows later classifiers to provide earlier classifiers information about which error modes to focus on. We show that our method significantly improves performance in all the sub-tasks in the domain of scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection. Our method also improves performance in two robotic applications: an object-grasping robot and an object-finding robot.},
 author = {Li, Congcong and Kowdle, Adarsh and Saxena, Ashutosh and Chen, Tsuhan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eb160de1de89d9058fcb0b968dbbbd68-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eb160de1de89d9058fcb0b968dbbbd68-Metadata.json},
 openalex = {W2952279306},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eb160de1de89d9058fcb0b968dbbbd68-Supplemental.zip},
 title = {Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/eb160de1de89d9058fcb0b968dbbbd68-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_ebd9629f,
 abstract = {This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is fit to multiple data sets, and provides a parsimonious method for describing how humans learn context specific conceptual representations.},
 author = {Navarro, Dan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Metadata.json},
 openalex = {W2160460326},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning the context of a category},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/ebd9629fc3ae5e9f6611e2ee05a31cef-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_eddea82a,
 abstract = {We introduce a new family of online learning algorithms based upon constraining the velocity flow over a distribution of weight vectors. In particular, we show how to effectively herd a Gaussian weight vector distribution by trading off velocity constraints with a loss function. By uniformly bounding this loss function, we demonstrate how to solve the resulting optimization analytically. We compare the resulting algorithms on a variety of real world datasets, and demonstrate how these algorithms achieve state-of-the-art robust performance, especially with high label noise in the training data.},
 author = {Crammer, Koby and Lee, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eddea82ad2755b24c4e168c5fc2ebd40-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eddea82ad2755b24c4e168c5fc2ebd40-Metadata.json},
 openalex = {W2151380595},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning via Gaussian Herding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_edfbe1af,
 abstract = {This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10 x 10 battleship and partially observable PacMan, with approximately 1018 and 1056 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.},
 author = {Silver, David and Veness, Joel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/edfbe1afcf9246bb0d40eb4d8027d90f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/edfbe1afcf9246bb0d40eb4d8027d90f-Metadata.json},
 openalex = {W2171084228},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Monte-Carlo Planning in Large POMDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/edfbe1afcf9246bb0d40eb4d8027d90f-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_eecca5b6,
 abstract = {Support vector machines (SVM) are increasingly used in brain image analyses since they allow capturing complex multivariate relationships in the data. Moreover, when the kernel is linear, SVMs can be used to localize spatial patterns of discrimination between two groups of subjects. However, the features' spatial distribution is not taken into account. As a consequence, the optimal margin hyperplane is often scattered and lacks spatial coherence, making its anatomical interpretation difficult. This paper introduces a framework to spatially regularize SVM for brain image analysis. We show that Laplacian regularization provides a flexible framework to integrate various types of constraints and can be applied to both cortical surfaces and 3D brain images. The proposed framework is applied to the classification of MR images based on gray matter concentration maps and cortical thickness measures from 30 patients with Alzheimer's disease and 30 elderly controls. The results demonstrate that the proposed method enables natural spatial and anatomical regularization of the classifier.},
 author = {Cuingnet, Remi and Chupin, Marie and Benali, Habib and Colliot, Olivier},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eecca5b6365d9607ee5a9d336962c534-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eecca5b6365d9607ee5a9d336962c534-Metadata.json},
 openalex = {W2151150214},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Spatial and anatomical regularization of SVM for brain image analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/eecca5b6365d9607ee5a9d336962c534-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_f033ab37,
 abstract = {Divisive normalization (DN) has been advocated as an effective nonlinear efficient coding transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture some important statistical properties of natural sensory signals. The multivariate t model justifies DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate t model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations confirm DN as an effective efficient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small.},
 author = {Lyu, Siwei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f033ab37c30201f73f142449d037028d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f033ab37c30201f73f142449d037028d-Metadata.json},
 openalex = {W2162637870},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f033ab37c30201f73f142449d037028d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/f033ab37c30201f73f142449d037028d-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_f2201f51,
 abstract = {Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-fit using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural firing rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-fit tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models.},
 author = {Gerhard, Felipe and Gerstner, Wulfram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f2201f5191c4e92cc5af043eebfd0946-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f2201f5191c4e92cc5af043eebfd0946-Metadata.json},
 openalex = {W4297747369},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/f2201f5191c4e92cc5af043eebfd0946-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_f29c21d4,
 abstract = {Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although matrix partition is an NP-hard combinatorial optimization problem, we show that a good local solution can be obtained by exploiting an effective local optimization technique on a relaxed continuous optimization problem. The proposed active learning approach is independent of employed classification models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods.},
 author = {Guo, Yuhong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f29c21d4897f78948b91f03172341b7b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f29c21d4897f78948b91f03172341b7b-Metadata.json},
 openalex = {W2132948751},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Active Instance Sampling via Matrix Partition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/f29c21d4897f78948b91f03172341b7b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_f4552671,
 abstract = {It has been speculated that the human motion system combines noisy measurements with prior expectations in an optimal, or rational, manner. The basic goal of our work is to discover experimentally which prior distribution is used. More specifically, we seek to infer the functional form of the motion prior from the performance of human subjects on motion estimation tasks. We restricted ourselves to priors which combine three terms for motion slowness, first-order smoothness, and second-order smoothness. We focused on two functional forms for prior distributions: L2-norm and L1-norm regularization corresponding to the Gaussian and Laplace distributions respectively. In our first experimental session we estimate the weights of the three terms for each functional form to maximize the fit to human performance. We then measured human performance for motion tasks and found that we obtained better fit for the L1-norm (Laplace) than for the L2-norm (Gaussian). We note that the L1-norm is also a better fit to the statistics of motion in natural environments. In addition, we found large weights for the second-order smoothness term, indicating the importance of high-order smoothness compared to slowness and lower-order smoothness. To validate our results further, we used the best fit models using the L1-norm to predict human performance in a second session with different experimental setups. Our results showed excellent agreement between human performance and model prediction - ranging from 3% to 8% for five human subjects over ten experimental conditions - and give further support that the human visual system uses an L1-norm (Laplace) prior.},
 author = {Lu, Hongjing and Lin, Tungyou and Lee, Alan and Vese, Luminita and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f4552671f8909587cf485ea990207f3b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f4552671f8909587cf485ea990207f3b-Metadata.json},
 openalex = {W2128422509},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f4552671f8909587cf485ea990207f3b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Functional form of motion priors in human motion perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/f4552671f8909587cf485ea990207f3b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_f4733064,
 abstract = {We consider the problem of learning a coefficient vector x0 ∈ ℝN from noisy linear observation y = Ax0 + w ∈ ℝn. In many contexts (ranging from model selection to image processing) it is desirable to construct a sparse estimator $\hat x$. In this case, a popular approach consists in solving an l1-penalized least squares problem known as the LASSO or Basis Pursuit DeNoising (BPDN).

For sequences of matrices A of increasing dimensions, with independent gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the first rigorous derivation of an explicit formula for the asymptotic mean square error of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efficient algorithm, that is inspired from graphical models ideas.

Through simulations on real data matrices (gene expression data and hospital medical records) we observe that these results can be relevant in a broad array of practical applications.},
 author = {Bayati, Mohsen and Pereira, Jos\'{e} and Montanari, Andrea},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f47330643ae134ca204bf6b2481fec47-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f47330643ae134ca204bf6b2481fec47-Metadata.json},
 openalex = {W2130342952},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {The LASSO risk: asymptotic results and real world examples},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/f47330643ae134ca204bf6b2481fec47-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_f7664060,
 abstract = {Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches. In particular, we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally, a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.},
 author = {Sun, Deqing and Sudderth, Erik and Black, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f7664060cc52bc6f3d620bcedc94a4b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f7664060cc52bc6f3d620bcedc94a4b6-Metadata.json},
 openalex = {W2154220963},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f7664060cc52bc6f3d620bcedc94a4b6-Supplemental.zip},
 title = {Layered image motion with explicit occlusions, temporal consistency, and depth ordering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_f93882cb,
 abstract = {Clustering is a basic data mining task with a wide variety of applications. Not surprisingly, there exist many clustering algorithms. However, clustering is an ill defined problem - given a data set, it is not clear what a correct clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. Faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm. Currently, such decisions are often made in a very ad hoc, if not completely random, manner. Given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable. In this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data. This is, of course, a very ambitious endeavor, and in this paper, we make some first steps towards this goal. We propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms.

In this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms. On top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering. We also study relationships between the properties, independent of any particular algorithm. In particular, we strengthen Kleinberg's famous impossibility result, while providing a simpler proof.},
 author = {Ackerman, Margareta and Ben-David, Shai and Loker, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f93882cbd8fc7fb794c1011d63be6fb6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f93882cbd8fc7fb794c1011d63be6fb6-Metadata.json},
 openalex = {W2161173962},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f93882cbd8fc7fb794c1011d63be6fb6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f93882cbd8fc7fb794c1011d63be6fb6-Supplemental.zip},
 title = {Towards Property-Based Classification of Clustering Paradigms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/f93882cbd8fc7fb794c1011d63be6fb6-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_f9a40a47,
 abstract = {Optimal coding provides a guiding principle for understanding the representation of sensory variables in neural populations. Here we consider the influence of a prior probability distribution over sensory variables on the optimal allocation of neurons and spikes in a population. We model the spikes of each cell as samples from an independent Poisson process with rate governed by an associated tuning curve. For this response model, we approximate the Fisher information in terms of the density and amplitude of the tuning curves, under the assumption that tuning width varies inversely with cell density. We consider a family of objective functions based on the expected value, over the sensory prior, of a functional of the Fisher information. This family includes lower bounds on mutual information and perceptual discriminability as special cases. In all cases, we find a closed form expression for the optimum, in which the density and gain of the cells in the population are power law functions of the stimulus prior. This also implies a power law relationship between the prior and perceptual discriminability. We show preliminary evidence that the theory successfully predicts the relationship between empirically measured stimulus priors, physiologically measured neural response properties (cell density, tuning widths, and firing rates), and psychophysically measured discrimination thresholds.},
 author = {Ganguli, Deep and Simoncelli, Eero},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f9a40a4780f5e1306c46f1c8daecee3b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f9a40a4780f5e1306c46f1c8daecee3b-Metadata.json},
 openalex = {W2168544128},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Implicit encoding of prior probabilities in optimal neural populations.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/f9a40a4780f5e1306c46f1c8daecee3b-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_faa9afea,
 abstract = {The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks.},
 author = {Agarwal, Alekh and Wainwright, Martin J and Duchi, John C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/faa9afea49ef2ff029a833cccc778fd0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/faa9afea49ef2ff029a833cccc778fd0-Metadata.json},
 openalex = {W2120293976},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/faa9afea49ef2ff029a833cccc778fd0-Supplemental.zip},
 title = {Distributed Dual Averaging In Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/faa9afea49ef2ff029a833cccc778fd0-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_fb60d411,
 abstract = {We identify and investigate a strong connection between probabilistic inference and differential privacy, the latter being a recent privacy definition that permits only indirect observation of data through noisy measurement. Previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own. We consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof. We find that probabilistic inference can improve accuracy, integrate multiple observations, measure uncertainty, and even provide posterior distributions over quantities that were not directly measured.},
 author = {Williams, Oliver and Mcsherry, Frank},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Metadata.json},
 openalex = {W2106882672},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Probabilistic Inference and Differential Privacy},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_fc8001f8,
 abstract = {We define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility (GCPV), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We find our model can outperform GARCH on simulated and financial data. And unlike GARCH, GCPV can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.},
 author = {Wilson, Andrew G and Ghahramani, Zoubin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fc8001f834f6a5f0561080d134d53d29-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fc8001f834f6a5f0561080d134d53d29-Metadata.json},
 openalex = {W2295202330},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fc8001f834f6a5f0561080d134d53d29-Supplemental.zip},
 title = {Copula Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/fc8001f834f6a5f0561080d134d53d29-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_fccb60fb,
 abstract = {Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to define a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features.},
 author = {Austerweil, Joseph and Griffiths, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fccb60fb512d13df5083790d64c4d5dd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fccb60fb512d13df5083790d64c4d5dd-Metadata.json},
 openalex = {W2139093058},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fccb60fb512d13df5083790d64c4d5dd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning invariant features using the Transformed Indian Buffet Process},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/fccb60fb512d13df5083790d64c4d5dd-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_fd06b8ea,
 abstract = {Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to finding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems.},
 author = {Hein, Matthias and B\"{u}hler, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Metadata.json},
 openalex = {W1543963486},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fd06b8ea02fe5b1c2496fe1700e9d16c-Supplemental.zip},
 title = {An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/fd06b8ea02fe5b1c2496fe1700e9d16c-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_fe709c65,
 abstract = {We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results.},
 author = {Vert, Jean-philippe and Bleakley, Kevin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe709c654eac84d5239d1a12a4f71877-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe709c654eac84d5239d1a12a4f71877-Metadata.json},
 openalex = {W2099376851},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe709c654eac84d5239d1a12a4f71877-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Fast detection of multiple change-points shared by many signals using group LARS},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/fe709c654eac84d5239d1a12a4f71877-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_fe73f687,
 abstract = {We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object).

Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efficiently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization.

The proposed framework is very flexible as it can accept any domain-specific visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data.},
 author = {Lempitsky, Victor and Zisserman, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe73f687e5bc5280214e0486b273a5f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe73f687e5bc5280214e0486b273a5f9-Metadata.json},
 openalex = {W2145983039},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe73f687e5bc5280214e0486b273a5f9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning To Count Objects in Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/fe73f687e5bc5280214e0486b273a5f9-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_fe8c15fe,
 abstract = {Singular-value decomposition (SVD) [and principal component analysis (PCA)] is one of the most widely used techniques for dimensionality reduction: successful and efficiently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA, such as robust collaborative filtering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted. We present an efficient convex optimization-based algorithm that we call outlier pursuit, which under some mild assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace and identifies the corrupted points. Such identification of corrupted points that do not conform to the low-dimensional approximation is of paramount interest in bioinformatics, financial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization; however, our results, setup, and approach necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself. In any problem where one seeks to recover a structure rather than the exact initial matrices, techniques developed thus far relying on certificates of optimality will fail. We present an important extension of these methods, which allows the treatment of such problems.},
 author = {Xu, Huan and Caramanis, Constantine and Sanghavi, Sujay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe8c15fed5f808006ce95eddb7366e35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe8c15fed5f808006ce95eddb7366e35-Metadata.json},
 openalex = {W1995168330},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Robust PCA via Outlier Pursuit},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/fe8c15fed5f808006ce95eddb7366e35-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_feab05aa,
 abstract = {Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efficient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to find the optimal kernel combination that benefits all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis shows that the complexity of our algorithm is O(m1/3√lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classification accuracy, the proposed method is significantly more efficient than the state-of-the-art algorithms for ML-MKL.},
 author = {Bucak, Serhat and Jin, Rong and Jain, Anil},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/feab05aa91085b7a8012516bc3533958-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/feab05aa91085b7a8012516bc3533958-Metadata.json},
 openalex = {W2163207321},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/feab05aa91085b7a8012516bc3533958-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2010/file/feab05aa91085b7a8012516bc3533958-Supplemental.zip},
 title = {Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/feab05aa91085b7a8012516bc3533958-Abstract.html},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2010_ffeabd22,
 abstract = {We introduce a new Bayesian nonparametric approach to identification of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as defined by the recently introduced Stable Spline kernel. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a definite advantage over a group LAR algorithm and state-of-the-art parametric identification techniques based on prediction error minimization.},
 author = {Chiuso, Alessandro and Pillonetto, Gianluigi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ffeabd223de0d4eacb9a3e6e53e5448d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ffeabd223de0d4eacb9a3e6e53e5448d-Metadata.json},
 openalex = {W2142066851},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2010/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 title = {Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/ffeabd223de0d4eacb9a3e6e53e5448d-Abstract.html},
 volume = {23},
 year = {2010}
}
