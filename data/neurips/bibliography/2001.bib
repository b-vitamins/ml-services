@inproceedings{NIPS2001_0004d0b5,
 abstract = {The partition function for a Boltzmann machine can be bounded from above and below. We can use this to bound the means and the correlations. For networks with small weights, the values of these statistics can be restricted to non-trivial regions (i.e. a subset of [–1, 1]). Experimental results show that reasonable bounding occurs for weight sizes where mean field expansions generally give good results.},
 author = {Leisink, Martijn and Kappen, Bert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0004d0b59e19461ff126e3a08a814c33-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0004d0b59e19461ff126e3a08a814c33-Metadata.json},
 openalex = {W2124231144},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0004d0b59e19461ff126e3a08a814c33-Paper.pdf},
 publisher = {MIT Press},
 title = {Means, Correlations and Bounds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/0004d0b59e19461ff126e3a08a814c33-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_0070d23b,
 abstract = {Partial information can trigger a complete memory. At the same time, human memory is not perfect. A cue can contain enough information to specify an item in memory, but fail to trigger that item. In the context of word memory, we present experiments that demonstrate some basic patterns in human memory errors. We use cues that consist of word fragments. We show that short and long cues are completed more accurately than medium length ones and study some of the factors that lead to this behavior. We then present a novel computational model that shows some of the flexibility and patterns of errors that occur in human memory. This model iterates between bottom-up and top-down computations. These are tied together using a Markov model of words that allows memory to be accessed with a simple feature set, and enables a bottom-up process to compute a probability distribution of possible completions of word fragments, in a manner similar to models of visual perceptual completion.},
 author = {Jacobs, David and Rokers, Bas and Rudra, Archisman and Liu, Zili},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0070d23b06b1486a538c0eaa45dd167a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0070d23b06b1486a538c0eaa45dd167a-Metadata.json},
 openalex = {W2118791062},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0070d23b06b1486a538c0eaa45dd167a-Paper.pdf},
 publisher = {MIT Press},
 title = {Fragment Completion in Humans and Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/0070d23b06b1486a538c0eaa45dd167a-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_008bd5ad,
 abstract = {Neural activity appears to be a crucial component for shaping the receptive fields of cortical simple cells into adjacent, oriented subregions alternately receiving ON- and OFF-center excitatory geniculate inputs. It is known that the orientation selective responses of V1 neurons are refined by visual experience. After eye opening, the spatiotemporal structure of neural activity in the early stages of the visual pathway depends both on the visual environment and on how the environment is scanned. We have used computational modeling to investigate how eye movements might affect the refinement of the orientation tuning of simple cells in the presence of a Hebbian scheme of synaptic plasticity. Levels of correlation between the activity of simulated cells were examined while natural scenes were scanned so as to model sequences of saccades and fixational eye movements, such as microsaccades, tremor and ocular drift. The specific patterns of activity required for a quantitatively accurate development of simple cell receptive fields with segregated ON and OFF subregions were observed during fixational eye movements, but not in the presence of saccades or with static presentation of natural visual input. These results suggest an important role for the eye movements occurring during visual fixation in the refinement of orientation selectivity.},
 author = {Casile, Antonino and Rucci, Michele},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/008bd5ad93b754d500338c253d9c1770-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/008bd5ad93b754d500338c253d9c1770-Metadata.json},
 openalex = {W2102469520},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/008bd5ad93b754d500338c253d9c1770-Paper.pdf},
 publisher = {MIT Press},
 title = {Eye movements and the maturation of cortical orientation selectivity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/008bd5ad93b754d500338c253d9c1770-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_0189caa5,
 abstract = {We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-fire neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be configured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.},
 author = {Liu, Shih-Chii and Kramer, J\"{o}rg and Indiveri, Giacomo and Delbr\"{u}ck, Tobi and Douglas, Rodney},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0189caa552598b845b29b17a427692d1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0189caa552598b845b29b17a427692d1-Metadata.json},
 openalex = {W2000294650},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0189caa552598b845b29b17a427692d1-Paper.pdf},
 publisher = {MIT Press},
 title = {Orientation-selective aVLSI spiking neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/0189caa552598b845b29b17a427692d1-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_01931a69,
 abstract = {In this paper, it is shown how to extract a hypothesis with small risk from the ensemble of hypotheses generated by an arbitrary on-line learning algorithm run on an independent and identically distributed (i.i.d.) sample of data. Using a simple large deviation argument, we prove tight data-dependent bounds for the risk of this hypothesis in terms of an easily computable statistic M/sub n/ associated with the on-line performance of the ensemble. Via sharp pointwise bounds on M/sub n/, we then obtain risk tail bounds for kernel perceptron algorithms in terms of the spectrum of the empirical kernel matrix. These bounds reveal that the linear hypotheses found via our approach achieve optimal tradeoffs between hinge loss and margin size over the class of all linear functions, an issue that was left open by previous results. A distinctive feature of our approach is that the key tools for our analysis come from the model of prediction of individual sequences; i.e., a model making no probabilistic assumptions on the source generating the data. In fact, these tools turn out to be so powerful that we only need very elementary statistical facts to obtain our final risk bounds.},
 author = {Cesa-bianchi, Nicol\`{o} and Conconi, Alex and Gentile, Claudio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/01931a6925d3de09e5f87419d9d55055-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/01931a6925d3de09e5f87419d9d55055-Metadata.json},
 openalex = {W2109339818},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/01931a6925d3de09e5f87419d9d55055-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Generalization Ability of On-Line Learning Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/01931a6925d3de09e5f87419d9d55055-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_02b1be0d,
 abstract = {The recent introduction of the 'relevance vector machine' has effectively demonstrated how sparsity may be obtained in generalised linear models within a Bayesian framework. Using a particular form of Gaussian parameter prior, 'learning' is the maximisation, with respect to hyperparameters, of the marginal likelihood of the data. This paper studies the properties of that objective function, and demonstrates that conditioned on an individual hyper-parameter, the marginal likelihood has a unique maximum which is computable in closed form. It is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model.},
 author = {Faul, Anita and Tipping, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/02b1be0d48924c327124732726097157-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/02b1be0d48924c327124732726097157-Metadata.json},
 openalex = {W2115606304},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/02b1be0d48924c327124732726097157-Paper.pdf},
 publisher = {MIT Press},
 title = {Analysis of Sparse Bayesian Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/02b1be0d48924c327124732726097157-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_03e7d2eb,
 abstract = {A cortical model for motion-in-depth selectivity of complex cells in the visual cortex is proposed. The model is based on a time extension of the phase-based techniques for disparity estimation. We consider the computation of the total temporal derivative of the time-varying disparity through the combination of the responses of disparity energy units. To take into account the physiological plausibility, the model is based on the combinations of binocular cells characterized by different ocular dominance indices. The resulting cortical units of the model show a sharp selectivity for motion-in-depth that has been compared with that reported in the literature for real cortical ceils.},
 author = {Sabatini, Silvio and Solari, Fabio and Andreani, Giulia and Bartolozzi, Chiara and Bisio, Giacomo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/03e7d2ebec1e820ac34d054df7e68f48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/03e7d2ebec1e820ac34d054df7e68f48-Metadata.json},
 openalex = {W2120155617},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/03e7d2ebec1e820ac34d054df7e68f48-Paper.pdf},
 publisher = {MIT Press},
 title = {A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/03e7d2ebec1e820ac34d054df7e68f48-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_05192834,
 abstract = {Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy.},
 author = {Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/051928341be67dcba03f0e04104d9047-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/051928341be67dcba03f0e04104d9047-Metadata.json},
 openalex = {W2139612737},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/051928341be67dcba03f0e04104d9047-Paper.pdf},
 publisher = {MIT Press},
 title = {Intrinsically Motivated Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/051928341be67dcba03f0e04104d9047-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_05546b0e,
 abstract = {We describe an algorithm for automatically learning discriminative components of objects with SVM classifiers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classifiers are then combined in a second stage to yield a hierarchical SVM classifier. Experimental results in face classification show considerable robustness against rotations in depth and suggest performance at significantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classification experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classifier which may be relevant for biological models of visual recognition.},
 author = {Heisele, Bernd and Serre, Thomas and Pontil, Massimiliano and Vetter, Thomas and Poggio, Tomaso},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/05546b0e38ab9175cd905eebcc6ebb76-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/05546b0e38ab9175cd905eebcc6ebb76-Metadata.json},
 openalex = {W2110822444},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf},
 publisher = {MIT Press},
 title = {Categorization by Learning and Combining Object Parts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_05a5cf06,
 abstract = {The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant. only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that, the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis.},
 author = {Chechik, Gal and Globerson, Amir and Anderson, M. and Young, E. and Nelken, Israel and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/05a5cf06982ba7892ed2a6d38fe832d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/05a5cf06982ba7892ed2a6d38fe832d6-Metadata.json},
 openalex = {W2120670853},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/05a5cf06982ba7892ed2a6d38fe832d6-Paper.pdf},
 publisher = {MIT Press},
 title = {Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/05a5cf06982ba7892ed2a6d38fe832d6-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_06964dce,
 abstract = {Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural firing conditioned on hand kinematics. We learn a non-parametric representation of this firing activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned firing models of multiple cells are used to define a non-Gaussian likelihood term which is combined with a prior probability for the kinematics. A particle filtering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear filtering methods; the results suggest that it may be appropriate for neural prosthetic applications.},
 author = {Gao, Yun and Black, Michael and Bienenstock, Elie and Shoham, Shy and Donoghue, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/06964dce9addb1c5cb5d6e3d9838f733-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/06964dce9addb1c5cb5d6e3d9838f733-Metadata.json},
 openalex = {W2105884083},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/06964dce9addb1c5cb5d6e3d9838f733-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_06b1338b,
 abstract = {We consider noisy Euclidean traveling salesman problems in the plane, which are random combinatorial problems with underlying structure. Gibbs sampling is used to compute average trajectories, which estimate the underlying structure common to all instances. This procedure requires identifying the exact relationship between permutations and tours. In a learning setting, the average trajectory is used as a model to construct solutions to new instances sampled from the same source. Experimental results show that the average trajectory can in fact estimate the underlying structure and that overfitting effects occur if the trajectory adapts too closely to a single instance.},
 author = {Braun, Mikio and Buhmann, Joachim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/06b1338ba02add2b5d2da67663b19ebe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/06b1338ba02add2b5d2da67663b19ebe-Metadata.json},
 openalex = {W2152454054},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/06b1338ba02add2b5d2da67663b19ebe-Paper.pdf},
 publisher = {MIT Press},
 title = {The Noisy Euclidean Traveling Salesman Problem and Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/06b1338ba02add2b5d2da67663b19ebe-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_07811dc6,
 author = {Chapelle, Olivier and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/07811dc6c422334ce36a09ff5cd6fe71-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/07811dc6c422334ce36a09ff5cd6fe71-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/07811dc6c422334ce36a09ff5cd6fe71-Paper.pdf},
 publisher = {MIT Press},
 title = {Incorporating Invariances in Non-Linear Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/07811dc6c422334ce36a09ff5cd6fe71-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_07cb5f86,
 abstract = {We propose techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, rounding in evaluating the expansions, and random projections in evaluating the itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the function k by a randomized kernel which behaves like k in expectation.},
 author = {Achlioptas, Dimitris and Mcsherry, Frank and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/07cb5f86508f146774a2fac4373a8e50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/07cb5f86508f146774a2fac4373a8e50-Metadata.json},
 openalex = {W2144414190},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/07cb5f86508f146774a2fac4373a8e50-Paper.pdf},
 publisher = {MIT Press},
 title = {Sampling Techniques for Kernel Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/07cb5f86508f146774a2fac4373a8e50-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_08f90c1a,
 abstract = {Animal data on delayed-reward conditioning experiments shows a striking property — the data for different time intervals collapses into a single curve when the data is scaled by the time interval. This is called the scalar property of interval timing. Here a simple model of a neural clock is presented and shown to give rise to the scalar property. The model is an accumulator consisting of noisy, linear spiking neurons. It is analytically tractable and contains only three parameters. When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial covariances.},
 author = {Shapiro, Jonathan and Wearden, J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/08f90c1a417155361a5c4b8d297e0d78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/08f90c1a417155361a5c4b8d297e0d78-Metadata.json},
 openalex = {W2111503368},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/08f90c1a417155361a5c4b8d297e0d78-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning and Time Perception -- a Model of Animal Experiments},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/08f90c1a417155361a5c4b8d297e0d78-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_093b60fd,
 abstract = {We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a SOM (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.},
 author = {Ontrup, Jorg and Ritter, Helge},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/093b60fd0557804c8ba0cbf1453da22f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/093b60fd0557804c8ba0cbf1453da22f-Metadata.json},
 openalex = {W2147438222},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/093b60fd0557804c8ba0cbf1453da22f-Paper.pdf},
 publisher = {MIT Press},
 title = {Hyperbolic Self-Organizing Maps for Semantic Navigation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/093b60fd0557804c8ba0cbf1453da22f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_0ae3f79a,
 abstract = {Many domains are naturally organized in an abstraction hierarchy or taxonomy, where the instances in nearby classes in the taxonomy are similar. In this paper, we provide a general probabilistic framework for clustering data into a set of classes organized as a taxonomy, where each class is associated with a probabilistic model from which the data was generated. The clustering algorithm simultaneously optimizes three things: the assignment of data instances to clusters, the models associated with the clusters, and the structure of the abstraction hierarchy. A unique feature of our approach is that it utilizes global optimization algorithms for both of the last two steps, reducing the sensitivity to noise and the propensity to local maxima that are characteristic of algorithms such as hierarchical agglomerative clustering that only take local steps. We provide a theoretical analysis for our algorithm, showing that it converges to a local maximum of the joint likelihood of model and data. We present experimental results on synthetic data, and on real data in the domains of gene expression and text.},
 author = {Segal, Eran and Koller, Daphne and Ormoneit, Dirk},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0ae3f79a30234b6c45a6f7d298ba1310-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0ae3f79a30234b6c45a6f7d298ba1310-Metadata.json},
 openalex = {W2113709557},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0ae3f79a30234b6c45a6f7d298ba1310-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic Abstraction Hierarchies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/0ae3f79a30234b6c45a6f7d298ba1310-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_0b1ec366,
 abstract = {This paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed (e.g. face detection or database retrieval). In such domains a cascade of simple classifiers each trained to achieve high detection rates and modest false positive rates can yield a final detector with many desirable features: including high detection rates, very low false positive rates, and fast performance. Achieving extremely high detection rates, rather than low error, is not a task typically addressed by machine learning algorithms. We propose a new variant of AdaBoost as a mechanism for training the simple classifiers used in the cascade. Experimental results in the domain of face detection show the training algorithm yields significant improvements in performance over conventional AdaBoost. The final face detection system can process 15 frames per second, achieves over 90% detection, and a false positive rate of 1 in a 1,000,000.},
 author = {Viola, Paul and Jones, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0b1ec366924b26fc98fa7b71a9c249cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0b1ec366924b26fc98fa7b71a9c249cf-Metadata.json},
 openalex = {W2104955141},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/0b1ec366924b26fc98fa7b71a9c249cf-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/0b1ec366924b26fc98fa7b71a9c249cf-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_1113d7a7,
 abstract = {The information bottleneck method is an unsupervised model independent data organization technique. Given a joint distribution P(A, B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B, In a recent paper, we introduced a general principled framework for multivariate extensions of the information bottleneck method that allows us to consider multiple systems of data partitions that are inter-related. In this paper, we present a new family of simple agglomerative algorithms to construct such systems of inter-related clusters. We analyze the behavior of these algorithms and apply them to several real-life datasets.},
 author = {Slonim, Noam and Friedman, Nir and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1113d7a76ffceca1bb350bfe145467c6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1113d7a76ffceca1bb350bfe145467c6-Metadata.json},
 openalex = {W2154122057},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf},
 publisher = {MIT Press},
 title = {Agglomerative Multivariate Information Bottleneck},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/1113d7a76ffceca1bb350bfe145467c6-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_1359aa93,
 abstract = {Guided by an initial idea of building a complex (non linear) decision surface with maximal local margin in input space, we give a possible geometrical intuition as to why K-Nearest Neighbor (KNN) algorithms often perform more poorly than SVMs on classification tasks. We then propose modified K-Nearest Neighbor algorithms to overcome the perceived problem. The approachis similar in spirit to Tangent Distance, but with invariances inferred from the local neighborhood rather than prior knowledge. Experimental results on real world classification tasks suggest that the modified KNN algorithms often give a dramatic improvement over standard KNN and perform as well or better than SVMs.},
 author = {Vincent, Pascal and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1359aa933b48b754a2f54adb688bfa77-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1359aa933b48b754a2f54adb688bfa77-Metadata.json},
 openalex = {W2158019970},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1359aa933b48b754a2f54adb688bfa77-Paper.pdf},
 publisher = {MIT Press},
 title = {K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/1359aa933b48b754a2f54adb688bfa77-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_16ba7217,
 abstract = {Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data.},
 author = {Griffiths, Thomas and Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/16ba72172e6a4f1de54d11ab6967e371-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/16ba72172e6a4f1de54d11ab6967e371-Metadata.json},
 openalex = {W2163681711},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/16ba72172e6a4f1de54d11ab6967e371-Paper.pdf},
 publisher = {MIT Press},
 title = {Using Vocabulary Knowledge in Bayesian Multinomial Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/16ba72172e6a4f1de54d11ab6967e371-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_194cf6c2,
 abstract = {This paper proposes an approach to classification of adjacent segments of a time series as being either of K classes. We use a hierarchical model that consists of a feature extraction stage and a generative classifier which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty.

The classifier is implemented as hidden Markov model with Gaussian and Multinomial observation distributions defined on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classifications will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classification of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a significant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.},
 author = {Sykacek, Peter and Roberts, Stephen J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/194cf6c2de8e00c05fcf16c498adc7bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/194cf6c2de8e00c05fcf16c498adc7bf-Metadata.json},
 openalex = {W2160973997},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/194cf6c2de8e00c05fcf16c498adc7bf-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian time series classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/194cf6c2de8e00c05fcf16c498adc7bf-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_1a0a283b,
 abstract = {Recurrent neural networks of analog units are computers for real-valued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to.},
 author = {Schmitt, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1a0a283bfe7c549dee6c638a05200e32-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1a0a283bfe7c549dee6c638a05200e32-Metadata.json},
 openalex = {W2154402823},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1a0a283bfe7c549dee6c638a05200e32-Paper.pdf},
 publisher = {MIT Press},
 title = {Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/1a0a283bfe7c549dee6c638a05200e32-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_1b36ea1c,
 abstract = {We propose a new classification for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classification, we review the optimality of existing algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long-run against fair opponents.},
 author = {Chang, Yu-Han and Kaelbling, Leslie Pack},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1b36ea1c9b7a1c3ad668b8bb5df7963f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1b36ea1c9b7a1c3ad668b8bb5df7963f-Metadata.json},
 openalex = {W2134112261},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1b36ea1c9b7a1c3ad668b8bb5df7963f-Paper.pdf},
 publisher = {MIT Press},
 title = {Playing is believing: The role of beliefs in multi-agent learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/1b36ea1c9b7a1c3ad668b8bb5df7963f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_1b5230e3,
 abstract = {To find out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the conditional probabilities of the constituent fragments, and (2) the value of Barlow's criterion of suspicious coincidence (the ratio of joint probability to the product of marginals). We then compared the part verification response times for various probe/target combinations before and after the exposure. For composite probes, the speedup was much larger for targets that contained pairs of fragments perfectly predictive of each other, compared to those that did not. This effect was modulated by the significance of their co-occurrence as estimated by Barlow's criterion. For lone-fragment probes, the speedup in all conditions was generally lower than for composites. These results shed light on the brain's strategies for unsupervised acquisition of structural information in vision.},
 author = {Edelman, Shimon and Hiles, Benjamin and Yang, Hwajin and Intrator, Nathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1b5230e3ea6d7123847ad55a1e06fffd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1b5230e3ea6d7123847ad55a1e06fffd-Metadata.json},
 openalex = {W2120662407},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1b5230e3ea6d7123847ad55a1e06fffd-Paper.pdf},
 publisher = {MIT Press},
 title = {Probabilistic principles in unsupervised learning of visual structure: human data and a model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/1b5230e3ea6d7123847ad55a1e06fffd-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_1e4d3617,
 abstract = {We show that states of a dynamical system can be usefully represented by multi-step, action-conditional predictions of future observations. State representations that are grounded in data in this way may be easier to learn, generalize better, and be less dependent on accurate prior models than, for example, POMDP state representations. Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear specialization of the predictive approach with the state representations used in POMDPs and in k-order Markov models. Ours is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls). We show that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model.},
 author = {Littman, Michael and Sutton, Richard S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1e4d36177d71bbb3558e43af9577d70e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1e4d36177d71bbb3558e43af9577d70e-Metadata.json},
 openalex = {W2158282517},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1e4d36177d71bbb3558e43af9577d70e-Paper.pdf},
 publisher = {MIT Press},
 title = {Predictive Representations of State},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/1e4d36177d71bbb3558e43af9577d70e-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_1f36c15d,
 abstract = {In many scientific and engineering applications, detecting and understanding differences between two groups of examples can be reduced to a classical problem of training a classifier for labeling new examples while making as few mistakes as possible. In the traditional classification setting, the resulting classifier is rarely analyzed in terms of the properties of the input data captured by the discriminative model. However, such analysis is crucial if we want to understand and visualize the detected differences. We propose an approach to interpretation of the statistical model in the original feature space that allows us to argue about the model in terms of the relevant changes to the input vectors. For each point in the input space, we define a discriminative direction to be the direction that moves the point towards the other class while introducing as little irrelevant change as possible with respect to the classifier function. We derive the discriminative direction for kernel-based classifiers, demonstrate the technique on several examples and briefly discuss its use in the statistical shape analysis, an application that originally motivated this work.},
 author = {Golland, Polina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f36c15d6a3d18d52e8d493bc8187cb9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f36c15d6a3d18d52e8d493bc8187cb9-Metadata.json},
 openalex = {W2140658437},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf},
 publisher = {MIT Press},
 title = {Discriminative Direction for Kernel Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_1f4fe6a4,
 abstract = {The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. In this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes.},
 author = {Torralba, Antonio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f4fe6a4411edc2ff625888b4093e917-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f4fe6a4411edc2ff625888b4093e917-Metadata.json},
 openalex = {W2128007081},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f4fe6a4411edc2ff625888b4093e917-Paper.pdf},
 publisher = {MIT Press},
 title = {Contextual Modulation of Target Saliency},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/1f4fe6a4411edc2ff625888b4093e917-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_1f71e393,
 abstract = {Kernel based methods are increasingly being used for data modeling because of their conceptual simplicity and outstanding performance on many tasks. However, in practice the kernel function is often chosen using trial-and-error heuristics. In this paper we address the problem of measuring the degree of agreement between a kernel and a learning task. We propose a quantity to capture this notion, which we call alignment. We study its theoretical properties, and derive a series of simple algorithms for adapting a kernel to the targets. This produces a series of novel methods for both transductive and inductive inference, kernel combination and kernel selection for both classification and regression problems that are computationally feasible for large problems. The algorithms are tested on publicly available datasets and are shown to exhibit good performance.},
 author = {Cristianini, Nello and Shawe-Taylor, John and Elisseeff, Andr\'{e} and Kandola, Jaz},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f71e393b3809197ed66df836fe833e5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f71e393b3809197ed66df836fe833e5-Metadata.json},
 openalex = {W2142387771},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f71e393b3809197ed66df836fe833e5-Paper.pdf},
 publisher = {MIT Press},
 title = {On Kernel Target Alignment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/1f71e393b3809197ed66df836fe833e5-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_23d2e157,
 abstract = {Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity.},
 author = {Haffner, Patrick},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/23d2e1578544b172cca332ff74bddf5f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/23d2e1578544b172cca332ff74bddf5f-Metadata.json},
 openalex = {W2138603399},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/23d2e1578544b172cca332ff74bddf5f-Paper.pdf},
 publisher = {MIT Press},
 title = {Escaping the Convex Hull with Extrapolated Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/23d2e1578544b172cca332ff74bddf5f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_24f0d2c9,
 abstract = {In contrast to standard statistical learning theory which studies uniform bounds on the expected error we present a framework that exploits the specific learning algorithm used. Motivated by the luckiness framework [8] we are also able to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits both the margin and the distribution of the data in feature space.},
 author = {Herbrich, Ralf and Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/24f0d2c90473b2bc949ad962e61d9bcb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/24f0d2c90473b2bc949ad962e61d9bcb-Metadata.json},
 openalex = {W2294659999},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/24f0d2c90473b2bc949ad962e61d9bcb-Paper.pdf},
 publisher = {MIT Press},
 title = {Algorithmic Luckiness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/24f0d2c90473b2bc949ad962e61d9bcb-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_253614bb,
 abstract = {With the increasing number of users of mobile computing devices (e.g. personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail. In this paper we explore the question of the optimal strategy for synchronising such replicas. We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. We then formulate objective functions which can be minimised with respect to the synchronisation timings. We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach.},
 author = {Lawrence, Neil and Rowstron, Antony I. T. and Bishop, Christopher and Taylor, Michael J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/253614bbac999b38b5b60cae531c4969-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/253614bbac999b38b5b60cae531c4969-Metadata.json},
 openalex = {W2141730834},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/253614bbac999b38b5b60cae531c4969-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimising Synchronisation Times for Mobile Devices},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/253614bbac999b38b5b60cae531c4969-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_2557911c,
 abstract = {In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra finches, naturalistic stimuli can be defined as sounds that they encounter in a colony of conspecific birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra finches, and then analyzed the response of single neurons in the songbird central auditory area (field L) to continuous playback of long segments from this ensemble. Following methods developed in the fly visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in field L, temporal patterns give at least ~20% extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a significant role.},
 author = {Wright, B. and Sen, Kamal and Bialek, William and Doupe, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2557911c1bf75c2b643afb4ecbfc8ec2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2557911c1bf75c2b643afb4ecbfc8ec2-Metadata.json},
 openalex = {W2949212107},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2557911c1bf75c2b643afb4ecbfc8ec2-Paper.pdf},
 publisher = {MIT Press},
 title = {Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/2557911c1bf75c2b643afb4ecbfc8ec2-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_26f5bd4a,
 abstract = {We combine the replica approach from statistical physics with a varia-tional approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.},
 author = {Malzahn, D\"{o}rthe and Opper, Manfred},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/26f5bd4aa64fdadf96152ca6e6408068-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/26f5bd4aa64fdadf96152ca6e6408068-Metadata.json},
 openalex = {W2158529569},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/26f5bd4aa64fdadf96152ca6e6408068-Paper.pdf},
 publisher = {MIT Press},
 title = {A Variational Approach to Learning Curves},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/26f5bd4aa64fdadf96152ca6e6408068-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_277a78fc,
 abstract = {Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neoromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the additive STDP more robust. Finally, wo give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes.},
 author = {Tegn\'{e}r, Jesper and Kepecs, \'{A}d\'{a}m},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/277a78fc05c8864a170e9a56ceeabc4c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/277a78fc05c8864a170e9a56ceeabc4c-Metadata.json},
 openalex = {W2106089457},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/277a78fc05c8864a170e9a56ceeabc4c-Paper.pdf},
 publisher = {MIT Press},
 title = {Why Neuronal Dynamics should Control Synaptic Learning Rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/277a78fc05c8864a170e9a56ceeabc4c-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_296472c9,
 abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
 author = {Blei, David and Ng, Andrew and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/296472c9542ad4d4788d543508116cbc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/296472c9542ad4d4788d543508116cbc-Metadata.json},
 openalex = {W1880262756},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/296472c9542ad4d4788d543508116cbc-Paper.pdf},
 publisher = {MIT Press},
 title = {Latent dirichlet allocation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_2b0f658c,
 abstract = {We describe a computer system that provides a real-time musical accompaniment for a live soloist in a piece of non-improvised music for soloist and accompaniment. A Bayesian network is developed that represents the joint distribution on the times at which the solo and accompaniment notes are played, relating the two parts through a layer of hidden variables. The network is first constructed using the rhythmic information contained in the musical score. The network is then trained to capture the musical interpretations of the soloist and accompanist in an off-line rehearsal phase. During live accompaniment the learned distribution of the network is combined with a real-time analysis of the soloist's acoustic signal, performed with a hidden Markov model, to generate a musically principled accompaniment that respects all available sources of knowledge. A live demonstration will be provided.},
 author = {Raphael, Christopher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2b0f658cbffd284984fb11d90254081f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2b0f658cbffd284984fb11d90254081f-Metadata.json},
 openalex = {W2125046726},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2b0f658cbffd284984fb11d90254081f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Bayesian Network for Real-Time Musical Accompaniment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/2b0f658cbffd284984fb11d90254081f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_2cad8fa4,
 abstract = {We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. We demonstrate a tradeoff between the computational efficiency with which these kernels can be computed and the generalization ability of the resulting classifier. We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Percep-tron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Win-now's behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efficiently computable.},
 author = {Khardon, Roni and Roth, Dan and Servedio, Rocco A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2cad8fa47bbef282badbb8de5374b894-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2cad8fa47bbef282badbb8de5374b894-Metadata.json},
 openalex = {W2118131514},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/2cad8fa47bbef282badbb8de5374b894-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_2cd4e8a2,
 abstract = {When designing a two-alternative classifier, one ordinarily aims to maximize the classifier's ability to discriminate between members of the two classes. We describe a situation in a real-world business application of machine-learning prediction in which an additional constraint is placed on the nature of the solution: that the classifier achieve a specified correct acceptance or correct rejection rate (i.e., that it achieve a fixed accuracy on members of one class or the other). Our domain is predicting churn in the telecommunications industry. Churn refers to customers who switch from one service provider to another. We propose four algorithms for training a classifier subject to this domain constraint, and present results showing that each algorithm yields a reliable improvement in performance. Although the improvement is modest in magnitude, it is nonetheless impressive given the difficulty of the problem and the financial return that it achieves to the service provider.},
 author = {Mozer, Michael C and Dodier, Robert and Colagrosso, Michael and Guerra-Salcedo, Cesar and Wolniewicz, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Metadata.json},
 openalex = {W2098142655},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf},
 publisher = {MIT Press},
 title = {Prodding the ROC Curve: Constrained Optimization of Classifier Performance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/2cd4e8a2ce081c3d7c32c3cde4312ef7-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_2d00f43f,
 abstract = {This paper presents a novel approach to the unsupervised learning of syntactic analyses of natural language text. Most previous work has focused on maximizing likelihood according to generative PCFG models. In contrast, we employ a simpler probabilistic model over trees based directly on constituent identity and linear context, and use an EM-like iterative procedure to induce structure. This method produces much higher quality analyses, giving the best published results on the ATIS dataset.},
 author = {Klein, Dan and Manning, Christopher D},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2d00f43f07911355d4151f13925ff292-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2d00f43f07911355d4151f13925ff292-Metadata.json},
 openalex = {W2149830985},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2d00f43f07911355d4151f13925ff292-Paper.pdf},
 publisher = {MIT Press},
 title = {Natural Language Grammar Induction using a Constituent-Context Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/2d00f43f07911355d4151f13925ff292-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_2d405b36,
 abstract = {Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are discrete, and the very peculiar shape of the noise distribution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that function approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers.},
 author = {Chapados, Nicolas and Bengio, Yoshua and Vincent, Pascal and Ghosn, Joumana and Dugas, Charles and Takeuchi, Ichiro and Meng, Linyan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2d405b367158e3f12d7c1e31a96b3af3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2d405b367158e3f12d7c1e31a96b3af3-Metadata.json},
 openalex = {W2169801781},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2d405b367158e3f12d7c1e31a96b3af3-Paper.pdf},
 publisher = {MIT Press},
 title = {Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/2d405b367158e3f12d7c1e31a96b3af3-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_2d579dc2,
 author = {Blankertz, Benjamin and Curio, Gabriel and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2d579dc29360d8bbfbb4aa541de5afa9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2d579dc29360d8bbfbb4aa541de5afa9-Metadata.json},
 openalex = {W4248337693},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2d579dc29360d8bbfbb4aa541de5afa9-Paper.pdf},
 publisher = {MIT Press},
 title = {Classifying Single Trial EEG: Towards Brain Computer Interfacing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/2d579dc29360d8bbfbb4aa541de5afa9-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_2eace51d,
 abstract = {The support vector machine (SVM) is known for its good performance in two-class classification, but its extension to multiclass classification is still an ongoing research issue. In this article, we propose a new approach for classification, called the import vector machine (IVM), which is built on kernel logistic regression (KLR). We show that the IVM not only performs as well as the SVM in two-class classification, but also can naturally be generalized to the multiclass case. Furthermore, the IVM provides an estimate of the underlying probability. Similar to the support points of the SVM, the IVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM. This gives the IVM a potential computational advantage over the SVM.},
 author = {Zhu, Ji and Hastie, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2eace51d8f796d04991c831a07059758-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2eace51d8f796d04991c831a07059758-Metadata.json},
 openalex = {W2043182541},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/2eace51d8f796d04991c831a07059758-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernel Logistic Regression and the Import Vector Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/2eace51d8f796d04991c831a07059758-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_312351bf,
 abstract = {We present a neural network model that shows how the prefrontal cortex, interacting with the basal ganglia, can maintain a sequence of phonological information in activation-based working memory (i.e., the phonological loop). The primary function of this phonological loop may be to transiently encode arbitrary bindings of information necessary for tasks — the combinatorial expressive power of language enables very flexible binding of essentially arbitrary pieces of information. Our model takes advantage of the closed-class nature of phonemes, which allows different neural representations of all possible phonemes at each sequential position to be encoded. To make this work, we suggest that the basal ganglia provide a region-specific update signal that allocates phonemes to the appropriate sequential coding slot. To demonstrate that flexible, arbitrary binding of novel sequences can be supported by this mechanism, we show that the model can generalize to novel sequences after moderate amounts of training.},
 author = {O\textquotesingle Reilly, Randall and Soto, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/312351bff07989769097660a56395065-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/312351bff07989769097660a56395065-Metadata.json},
 openalex = {W2104404380},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/312351bff07989769097660a56395065-Paper.pdf},
 publisher = {MIT Press},
 title = {A Model of the Phonological Loop: Generalization and Binding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/312351bff07989769097660a56395065-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_325995af,
 abstract = {We present an algorithm that induces a class of models with thin junction trees—models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph. By ensuring that the junction tree is thin, inference in our models remains tractable throughout the learning process. This allows both an efficient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efficiently with the final model. We illustrate the approach with applications in handwritten digit recognition and DNA splice site detection.},
 author = {Bach, Francis and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/325995af77a0e8b06d1204a171010b3a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/325995af77a0e8b06d1204a171010b3a-Metadata.json},
 openalex = {W2142241599},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/325995af77a0e8b06d1204a171010b3a-Paper.pdf},
 publisher = {MIT Press},
 title = {Thin Junction Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/325995af77a0e8b06d1204a171010b3a-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_3683af9d,
 abstract = {Reaching movements require the brain to generate motor commands that rely on an internal model of the task's dynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Using a framework from function approximation, we argue that the sequence of errors should reflect the process of gradient descent. If so, then the sequence of errors should obey hidden state transitions of a simple dynamical system. Fitting the system to human data, we find a surprisingly good fit accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis elements used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force field, and in a random sequence of force fields where learning is not possible. Remarkably, we find that the basis remains invariant.},
 author = {Donchin, O. and Shadmehr, Reza},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3683af9d6f6c06acee72992f2977f67e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3683af9d6f6c06acee72992f2977f67e-Metadata.json},
 openalex = {W2120663621},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3683af9d6f6c06acee72992f2977f67e-Paper.pdf},
 publisher = {MIT Press},
 title = {Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/3683af9d6f6c06acee72992f2977f67e-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_36ac8e55,
 abstract = {Support vector machines (SVMs) are the state-of-the-art models for many classification problems, but they suffer from the complexity of their training algorithm, which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundred thousand examples with SVMs. This article proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole data set. Experiments on a large benchmark data set (Forest) yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples). In addition, and surprisingly, a significant improvement in generalization was observed.},
 author = {Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/36ac8e558ac7690b6f44e2cb5ef93322-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/36ac8e558ac7690b6f44e2cb5ef93322-Metadata.json},
 openalex = {W2125126592},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf},
 publisher = {MIT Press},
 title = {A Parallel Mixture of SVMs for Very Large Scale Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/36ac8e558ac7690b6f44e2cb5ef93322-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_378a063b,
 abstract = {In packet switches, packets queue at switch inputs and contend for outputs. The contention arbitration policy directly affects switch performance. The best policy depends on the current state of the switch and current traffic patterns. This problem is hard because the state space, possible transitions, and set of actions all grow exponentially with the size of the switch. We present a reinforcement learning formulation of the problem that decomposes the value function into many small independent value functions and enables an efficient action selection.},
 author = {Brown, Timothy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/378a063b8fdb1db941e34f4bde584c7d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/378a063b8fdb1db941e34f4bde584c7d-Metadata.json},
 openalex = {W2101955036},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf},
 publisher = {MIT Press},
 title = {Switch Packet Arbitration via Queue-Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_3875115b,
 abstract = {We present a new method for the blind separation of sources, which do not fulfill the independence assumption. In contrast to standard methods we consider groups of neighboring samples (patches) within the observed mixtures.

First we extract independent features from the observed patches. It turns out that the average dependencies between these features in different sources is in general lower than the dependencies between the amplitudes of different sources. We show that it might be the case that most of the dependencies is carried by only a small number of features. Is this case - provided these features can be identified by some heuristic - we project all patches into the subspace which is orthogonal to the subspace spanned by the correlated features.

Standard ICA is then performed on the elements of the transformed patches (for which the independence assumption holds) and robustly yields a good estimate of the mixing matrix.},
 author = {Vollgraf, Roland and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3875115bacc48cca24ac51ee4b0e7975-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3875115bacc48cca24ac51ee4b0e7975-Metadata.json},
 openalex = {W2157093409},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3875115bacc48cca24ac51ee4b0e7975-Paper.pdf},
 publisher = {MIT Press},
 title = {Multi Dimensional ICA to Separate Correlated Sources},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/3875115bacc48cca24ac51ee4b0e7975-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_3937230d,
 abstract = {Inferior temporal cortex (IT) neurons have large receptive fields when a single effective object stimulus is shown against a blank background, but have much smaller receptive fields when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magnification factor than the peripheral visual field. Furthermore, we show that top-down object bias can increase the receptive field size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive field size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.},
 author = {Trappenberg, Thomas and Rolls, Edmund and Stringer, Simon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3937230de3c8041e4da6ac3246a888e8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3937230de3c8041e4da6ac3246a888e8-Metadata.json},
 openalex = {W2163224984},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3937230de3c8041e4da6ac3246a888e8-Paper.pdf},
 publisher = {MIT Press},
 title = {Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/3937230de3c8041e4da6ac3246a888e8-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_39dcaf7a,
 abstract = {This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems. Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common properties with SVMs. We tested it on a Yeast gene functional classification problem with positive results.},
 author = {Elisseeff, Andr\'{e} and Weston, Jason},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/39dcaf7a053dc372fbc391d4e6b5d693-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/39dcaf7a053dc372fbc391d4e6b5d693-Metadata.json},
 openalex = {W2118712128},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/39dcaf7a053dc372fbc391d4e6b5d693-Paper.pdf},
 publisher = {MIT Press},
 title = {A Kernel Method for Multi-Labelled Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/39dcaf7a053dc372fbc391d4e6b5d693-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_3a824154,
 abstract = {We present a hybrid learning method bridging the fields of recurrent neural networks, unsupervised Hebbian learning, vector quantization, and supervised learning to implement a sophisticated image and feature segmentation architecture. This architecture is based on the competitive layer model (CLM), a dynamic feature binding model, which is applicable on a wide range of perceptual grouping and segmentation problems. A predefined target segmentation can be achieved as attractor states of this linear threshold recurrent network, if the lateral weights are chosen by Hebbian learning. The weight matrix is given by the correlation matrix of special pattern vectors with a structure dependent on the target labeling. Generalization is achieved by applying vector quantization on pair-wise feature relations, like proximity and similarity, defined by external knowledge. We show the successful application of the method to a number of artifical test examples and a medical image segmentation problem of fluorescence microscope cell images.},
 author = {Wersing, Heiko},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3a824154b16ed7dab899bf000b80eeee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3a824154b16ed7dab899bf000b80eeee-Metadata.json},
 openalex = {W2132798810},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3a824154b16ed7dab899bf000b80eeee-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Lateral Interactions for Feature Binding and Sensory Segmentation from Prototypic Basis Interactions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/3a824154b16ed7dab899bf000b80eeee-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_3b92d18a,
 abstract = {Since the discovery that the best error-correcting decoding algorithm can be viewed as belief in a cycle-bound graph, researchers have been trying to determine under what circumstances belief propagation is effective for probabilistic inference. Despite several theoretical advances in our understanding of loopy belief propagation, to our knowledge, the only problem that has been solved using loopy belief is error-correcting decoding on Gaussian channels. We propose a new representation for the two-dimensional phase unwrapping problem, and we show that loopy belief produces results that are superior to existing techniques. This is an important result, since many imaging techniques, including magnetic resonance imaging and interfer-ometric synthetic aperture radar, produce phase-wrapped images. Interestingly, the graph that we use has a very large number of very short cycles, supporting evidence that a large minimum cycle length is not needed for excellent results using belief propagation.},
 author = {Frey, Brendan J and Koetter, Ralf and Petrovic, Nemanja},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3b92d18aa7a6176dd37d372bc2f1eb71-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3b92d18aa7a6176dd37d372bc2f1eb71-Metadata.json},
 openalex = {W2152551018},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3b92d18aa7a6176dd37d372bc2f1eb71-Paper.pdf},
 publisher = {MIT Press},
 title = {Very loopy belief propagation for unwrapping phase images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/3b92d18aa7a6176dd37d372bc2f1eb71-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_3d863b36,
 abstract = {In this paper, I make a case for the modularity of the motor system. I start where many do in discussions of modularity, by considering the extent to which the motor system is cognitively penetrabl...},
 author = {D\textquotesingle avella, A. and Tresch, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3d863b367aa379f71c7afc0c9cdca41d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3d863b367aa379f71c7afc0c9cdca41d-Metadata.json},
 openalex = {W2978195077},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3d863b367aa379f71c7afc0c9cdca41d-Paper.pdf},
 publisher = {MIT Press},
 title = {Modularity in the Motor System: Decomposition of Muscle Patterns as Combinations of Time-Varying Synergies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/3d863b367aa379f71c7afc0c9cdca41d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_3e9e39fe,
 abstract = {We propose a novel clustering method that is an extension of ideas inherent to scale-space clustering and support-vector clustering. Like the latter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scale-space probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schrodinger equation of which the probability function is a solution. This Schrodinger equation contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. The method has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schrodinger potential to the locations of data points, we can apply this method to problems in high dimensions.},
 author = {Horn, David and Gottlieb, Assaf},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3e9e39fed3b8369ed940f52cf300cf88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3e9e39fed3b8369ed940f52cf300cf88-Metadata.json},
 openalex = {W2103559643},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3e9e39fed3b8369ed940f52cf300cf88-Paper.pdf},
 publisher = {MIT Press},
 title = {The Method of Quantum Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/3e9e39fed3b8369ed940f52cf300cf88-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_3f088ebe,
 abstract = {Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network's nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by specific spatio-temporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.},
 author = {Xie, Xiaohui and Giese, Martin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3f088ebeda03513be71d34d214291986-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3f088ebeda03513be71d34d214291986-Metadata.json},
 openalex = {W2136054788},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/3f088ebeda03513be71d34d214291986-Paper.pdf},
 publisher = {MIT Press},
 title = {Generating Velocity Tuning by Asymmetric Recurrent Connections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/3f088ebeda03513be71d34d214291986-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_405e2890,
 abstract = {We propose a framework based on a parametric quadratic programming (QP) technique to solve the support vector machine (SVM) training problem. This framework, can be specialized to obtain two SVM optimization methods. The first solves the fixed bias problem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. The later method can be applied in conjunction with any other existing technique which obtains a fixed bias solution. Moreover, the second method can also be used independently to solve the complete SVM training problem. A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft, Margin SVM optimization problem. Applying Selective Sampling techniques may further boost convergence.},
 author = {Fine, Shai and Scheinberg, Katya},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/405e28906322882c5be9b4b27f4c35fd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/405e28906322882c5be9b4b27f4c35fd-Metadata.json},
 openalex = {W2103105543},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/405e28906322882c5be9b4b27f4c35fd-Paper.pdf},
 publisher = {MIT Press},
 title = {Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/405e28906322882c5be9b4b27f4c35fd-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_44cd7a8f,
 abstract = {Massive transaction data sets are recorded in a routine manner in telecommunications, retail commerce, and Web site management. In this paper we address the problem of inferring predictive individual profiles from such historical transaction data. We describe a generative mixture model for count data and use an an approximate Bayesian estimation framework that effectively combines an individual's specific history with more general population patterns. We use a large real-world retail transaction data set to illustrate how these profiles consistently outperform non-mixture and non-Bayesian techniques in predicting customer behavior in out-of-sample data.},
 author = {Cadez, Igor and Smyth, Padhraic},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/44cd7a8f7f9f85129b9953950665064d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/44cd7a8f7f9f85129b9953950665064d-Metadata.json},
 openalex = {W2109374703},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/44cd7a8f7f9f85129b9953950665064d-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Predictive Profiles With Applications to Retail Transaction Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/44cd7a8f7f9f85129b9953950665064d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_4a3e0096,
 abstract = {We are interested in the mechanisms by which individuals monitor and adjust their performance of simple cognitive tasks. We model a speeded discrimination task in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). Response conflict arises when one stimulus class is infrequent relative to another, resulting in more errors and slower reaction times for the infrequent class. How do control processes modulate behavior based on the relative class frequencies? We explain performance from a rational perspective that casts the goal of individuals as minimizing a cost that depends both on error rate and reaction time. With two additional assumptions of rationality—that class prior probabilities are accurately estimated and that inference is optimal subject to limitations on rate of information transmission—we obtain a good fit to overall RT and error data, as well as trial-by-trial variations in performance.},
 author = {Mozer, Michael C and Colagrosso, Michael and Huber, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4a3e00961a08879c34f91ca0070ea2f5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4a3e00961a08879c34f91ca0070ea2f5-Metadata.json},
 openalex = {W2133827981},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4a3e00961a08879c34f91ca0070ea2f5-Paper.pdf},
 publisher = {MIT Press},
 title = {A Rational Analysis of Cognitive Control in a Speeded Discrimination Task},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4a3e00961a08879c34f91ca0070ea2f5-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_4afd521d,
 abstract = {We have designed and fabricated a VLSI synapse that can learn a conditional probability or correlation between spike-based inputs and feedback signals. The synapse is low power, compact, provides nonvolatile weight storage, and can perform simultaneous multiplication and adaptation. We can calibrate arrays of synapses to ensure uniform adaptation characteristics. Finally, adaptation in our synapse does not necessarily depend on the signals used for computation. Consequently, our synapse can implement learning rules that correlate past and present synaptic activity. We provide analysis and experimental chip results demonstrating the operation in learning and calibration mode, and show how to use our synapse to implement various learning rules in silicon.},
 author = {Shon, Aaron and Hsu, David and Diorio, Chris},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4afd521d77158e02aed37e2274b90c9c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4afd521d77158e02aed37e2274b90c9c-Metadata.json},
 openalex = {W2114118267},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4afd521d77158e02aed37e2274b90c9c-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Spike-Based Correlations and Conditional Probabilities in Silicon},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4afd521d77158e02aed37e2274b90c9c-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_4b86abe4,
 abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
 author = {Kakade, Sham M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Metadata.json},
 openalex = {W2130801532},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf},
 publisher = {MIT Press},
 title = {A Natural Policy Gradient},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_4ba29b9f,
 abstract = {In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results.},
 author = {Cristianini, Nello and Shawe-Taylor, John and Kandola, Jaz},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4ba29b9f9e5732ed33761840f4ba6c53-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4ba29b9f9e5732ed33761840f4ba6c53-Metadata.json},
 openalex = {W2123320529},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4ba29b9f9e5732ed33761840f4ba6c53-Paper.pdf},
 publisher = {MIT Press},
 title = {Spectral Kernel Methods for Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4ba29b9f9e5732ed33761840f4ba6c53-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_4ba3c163,
 abstract = {We present three ways of combining linear programming with the kernel trick to find value function approximations for reinforcement learning. One formulation is based on SVM regression; the second is based on the Bellman equation; and the third seeks only to ensure that good moves have an advantage over bad moves. All formulations attempt to minimize the number of support vectors while fitting the data. Experiments in a difficult, synthetic maze problem show that all three formulations give excellent performance, but the advantage formulation is much easier to train. Unlike policy gradient methods, the kernel methods described here can easily adjust the complexity of the function approximator to fit the complexity of the value function.},
 author = {Dietterich, Thomas and Wang, Xin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4ba3c163cd1efd4c14e3a415fa0a3010-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4ba3c163cd1efd4c14e3a415fa0a3010-Metadata.json},
 openalex = {W2106451198},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4ba3c163cd1efd4c14e3a415fa0a3010-Paper.pdf},
 publisher = {MIT Press},
 title = {Batch Value Function Approximation via Support Vectors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4ba3c163cd1efd4c14e3a415fa0a3010-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_4c144c47,
 abstract = {The rule-based bootstrapping introduced by Yarowsky, and its co-training variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justifies both the use of confidences — partial rules and partial labeling of the unlabeled data — and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of labels for k ≥ 2.},
 author = {Dasgupta, Sanjoy and Littman, Michael and McAllester, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4c144c47ecba6f8318128703ca9e2601-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4c144c47ecba6f8318128703ca9e2601-Metadata.json},
 openalex = {W2140076625},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4c144c47ecba6f8318128703ca9e2601-Paper.pdf},
 publisher = {MIT Press},
 title = {PAC Generalization Bounds for Co-training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4c144c47ecba6f8318128703ca9e2601-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_4d6b3e38,
 abstract = {Principal Component Analysis and Fisher Linear Discriminant methods have demonstrated their success in face detection, recognition, and tracking. The representation in these subspace methods is based on second order statistics of the image set, and does not address higher order statistical dependencies such as the relationships among three or more pixels. Recently Higher Order Statistics and Independent Component Analysis (ICA) have been used as informative low dimensional representations for visual recognition. In this paper, we investigate the use of Kernel Principal Component Analysis and Kernel Fisher Linear Discriminant for learning low dimensional representations for face recognition, which we call Kernel Eigenface and Kernel Fisherface methods. While Eigenface and Fisherface methods aim to find projection directions based on the second order correlation of samples, Kernel Eigenface and Kernel Fisherface methods provide generalizations which take higher order correlations into account. We compare the performance of kernel methods with Eigenface, Fisherface and ICA-based methods for face recognition with variation in pose, scale, lighting and expression. Experimental results show that kernel methods provide better representations and achieve lower error rates for face recognition.},
 author = {Yang, Ming-Hsuan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4d6b3e38b952600251ee92fe603170ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4d6b3e38b952600251ee92fe603170ff-Metadata.json},
 openalex = {W2153054748},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4d6b3e38b952600251ee92fe603170ff-Paper.pdf},
 publisher = {MIT Press},
 title = {Face Recognition Using Kernel Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4d6b3e38b952600251ee92fe603170ff-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_4d855669,
 abstract = {This paper deals with a neural network architecture which establishes a portfolio management system similar to the Black / Litterman approach. This allocation scheme distributes funds across various securities or financial markets while simultaneously complying with specific allocation constraints which meet the requirements of an investor.

The portfolio optimization algorithm is modeled by a feedforward neural network. The underlying expected return forecasts are based on error correction neural networks (ECNN), which utilize the last model error as an auxiliary input to evaluate their own misspecification.

The portfolio optimization is implemented such that (i.) the allocations comply with investor's constraints and that (ii.) the risk of the portfolio can be controlled. We demonstrate the profitability of our approach by constructing internationally diversified portfolios across 21 different financial markets of the G7 contries. It turns out, that our approach is superior to a preset benchmark portfolio.},
 author = {Zimmermann, Hans-Georg and Neuneier, Ralph and Grothmann, Ralph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4d8556695c262ab91ff51a943fdd6058-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4d8556695c262ab91ff51a943fdd6058-Metadata.json},
 openalex = {W2165817097},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4d8556695c262ab91ff51a943fdd6058-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Portfolio-Management based on Error Correction Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4d8556695c262ab91ff51a943fdd6058-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_4e62e752,
 abstract = {We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.},
 author = {Zhang, T.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4e62e752ae53fb6a6eebd0f6146aa702-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4e62e752ae53fb6a6eebd0f6146aa702-Metadata.json},
 openalex = {W2163549696},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4e62e752ae53fb6a6eebd0f6146aa702-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalization Performance of Some Learning Problems in Hilbert Functional Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4e62e752ae53fb6a6eebd0f6146aa702-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_51174add,
 abstract = {A flexible analog pattern-matching classifier has been developed and its performance is demonstrated in conjunction with a robust image representation algorithm called projected principal-edge distribution (PPED). In the circuit, the functional form of matching is made tunable in terms of the peak position, the peak height and the sharpness of the similarity evaluation by employing the floating-gate MOS technology. The test chip was fabricated in a 0.6-/spl mu/m complimentary metal-oxide semiconductor technology and successfully applied to the recognition of simple handwritten patterns and Arabic numerals using the PPED algorithm for robust image coding. The separation and classification of overlapping patterns have been also experimentally demonstrated.},
 author = {Yamasaki, Toshihiko and Shibata, Tadashi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/51174add1c52758f33d414ceaf3fe6ba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/51174add1c52758f33d414ceaf3fe6ba-Metadata.json},
 openalex = {W2153827622},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/51174add1c52758f33d414ceaf3fe6ba-Paper.pdf},
 publisher = {MIT Press},
 title = {Analog soft-pattern-matching classifier using floating-gate mos technology},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/51174add1c52758f33d414ceaf3fe6ba-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_519c8415,
 abstract = {We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efficiency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly specific and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.},
 author = {O\textquotesingle Reilly, Randall and Busby, R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/519c84155964659375821f7ca576f095-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/519c84155964659375821f7ca576f095-Metadata.json},
 openalex = {W2100296711},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/519c84155964659375821f7ca576f095-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalizable Relational Binding from Coarse-coded Distributed Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/519c84155964659375821f7ca576f095-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_5227b6aa,
 abstract = {We consider the problem of measuring the eigenvalues of a randomly drawn sample of points. We show that these values can be reliably estimated as can the sum of the tail of eigenvalues. Furthermore, the residuals when data is projected into a subspace is shown to be reliably estimated on a random sample. Experiments are presented that confirm the theoretical results.},
 author = {Shawe-Taylor, John and Cristianini, Nello and Kandola, Jaz},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5227b6aaf294f5f027273aebf16015f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5227b6aaf294f5f027273aebf16015f2-Metadata.json},
 openalex = {W2108912954},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5227b6aaf294f5f027273aebf16015f2-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Concentration of Spectral Properties},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/5227b6aaf294f5f027273aebf16015f2-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_52dbb068,
 abstract = {The problem of searching for information in networks like the World Wide Web can be approached in a variety of ways, ranging from centralized indexing schemes to decentralized mechanisms that navigate the underlying network without knowledge of its global structure. The decentralized approach appears in a variety of settings: in the behavior of users browsing the Web by following hyperlinks; in the design of focused crawlers [4, 5, 8] and other agents that explore the Web’s links to gather information; and in the search protocols underlying decentralized peer-to-peer systems such as Gnutella [10], Freenet [7], and recent research prototypes [21, 22, 23], through which users can share resources without a central server. In recent work, we have been investigating the problem of decentralized search in large information networks [14, 15]. Our initial motivation was an experiment that dealt directly with the search problem in a decidedly pre-Internet context: Stanley Milgram’s famous study of the small-world phenomenon [16, 17]. Milgram was seeking to determine whether most pairs of people in society were linked by short chains of acquaintances, and for this purpose he recruited individuals to try forwarding a letter to a designated “target” through people they knew on a first-name basis. The starting individuals were given basic information about the target — his name, address, occupation, and a few other personal details — and had to choose a single acquaintance to send the letter to, with goal of reaching the target as quickly as possible; subsequent recipients followed the same procedure, and the chain closed in on its destination. Of the chains that completed, the median number of steps required was six — a result that has since entered popular culture as the “six degrees of separation” principle [11]. Milgram’s experiment contains two striking discoveries — that short chains are pervasive, and that people are able to find them. This latter point is concerned precisely with a type of decentralized navigation in a social network, consisting of people as nodes and links joining},
 author = {Kleinberg, Jon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/52dbb0686f8bd0c0c757acf716e28ec0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/52dbb0686f8bd0c0c757acf716e28ec0-Metadata.json},
 openalex = {W2164195254},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/52dbb0686f8bd0c0c757acf716e28ec0-Paper.pdf},
 publisher = {MIT Press},
 title = {Small-World Phenomena and the Dynamics of Information},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/52dbb0686f8bd0c0c757acf716e28ec0-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_5352696a,
 abstract = {A mixed-signal paradigm is presented for high-resolution parallel inner-product computation in very high dimensions, suitable for efficient implementation of kernels in image processing. At the core of the externally digital architecture is a high-density, low-power analog array performing binary-binary partial matrix-vector multiplication. Full digital resolution is maintained even with low-resolution analog-to-digital conversion, owing to random statistics in the analog summation of binary products. A random modulation scheme produces near-Bernoulli statistics even for highly correlated inputs. The approach is validated with real image data, and with experimental results from a CID/DRAM analog array prototype in 0.5 µm CMOS.},
 author = {Genov, Roman and Cauwenberghs, Gert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5352696a9ca3397beb79f116f3a33991-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5352696a9ca3397beb79f116f3a33991-Metadata.json},
 openalex = {W2143587466},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5352696a9ca3397beb79f116f3a33991-Paper.pdf},
 publisher = {MIT Press},
 title = {Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/5352696a9ca3397beb79f116f3a33991-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_54ff9e9e,
 abstract = {Tangential hand velocity profiles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efficiently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochastic-ity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simplified dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simplified model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm's end point close to the target by one fast submovement and then apply a few slow submovementsto accurately drive the arm's end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.},
 author = {Kositsky, Michael and Barto, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/54ff9e9e3a2ec0300d4ce11261f5169f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/54ff9e9e3a2ec0300d4ce11261f5169f-Metadata.json},
 openalex = {W2149830233},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/54ff9e9e3a2ec0300d4ce11261f5169f-Paper.pdf},
 publisher = {MIT Press},
 title = {The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/54ff9e9e3a2ec0300d4ce11261f5169f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_5531a583,
 abstract = {We discuss the problem of ranking instances. In our framework each instance is associated with a rank or a rating, which is an integer from 1 to k. Our goal is to find a rank-predict ion rule that assigns each instance a rank which is as close as possible to the instance's true rank. We describe a simple and efficient online algorithm, analyze its performance in the mistake bound model, and prove its correctness. We describe two sets of experiments, with synthetic data and with the EachMovie dataset for collaborative filtering. In the experiments we performed, our algorithm outperforms online algorithms for regression and classification applied to ranking.},
 author = {Crammer, Koby and Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5531a5834816222280f20d1ef9e95f69-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5531a5834816222280f20d1ef9e95f69-Metadata.json},
 openalex = {W2171541062},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5531a5834816222280f20d1ef9e95f69-Paper.pdf},
 publisher = {MIT Press},
 title = {Pranking with Ranking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/5531a5834816222280f20d1ef9e95f69-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_584b98aa,
 abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
 author = {Greensmith, Evan and Bartlett, Peter and Baxter, Jonathan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/584b98aac2dddf59ee2cf19ca4ccb75e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/584b98aac2dddf59ee2cf19ca4ccb75e-Metadata.json},
 openalex = {W2108682071},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/584b98aac2dddf59ee2cf19ca4ccb75e-Paper.pdf},
 publisher = {MIT Press},
 title = {Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/584b98aac2dddf59ee2cf19ca4ccb75e-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_5a1e3a5a,
 abstract = {Using methods of Statistical Physics, we investigate the role of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel.},
 author = {Opper, Manfred and Urbanczik, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5a1e3a5aede16d438c38862cac1a78db-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5a1e3a5aede16d438c38862cac1a78db-Metadata.json},
 openalex = {W2146387286},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5a1e3a5aede16d438c38862cac1a78db-Paper.pdf},
 publisher = {MIT Press},
 title = {Asymptotic Universality for Learning Curves of Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/5a1e3a5aede16d438c38862cac1a78db-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_5a7f963e,
 abstract = {In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance, The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the system responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of information that can be presented to the user per iteration.},
 author = {Jaakkola, Tommi and Siegelmann, Hava},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5a7f963e5e0504740c3a6b10bb6d4fa5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5a7f963e5e0504740c3a6b10bb6d4fa5-Metadata.json},
 openalex = {W2105503493},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5a7f963e5e0504740c3a6b10bb6d4fa5-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Information Retrieval},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/5a7f963e5e0504740c3a6b10bb6d4fa5-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_5ec829de,
 abstract = {We present a probabilistic generative model for timing deviations in expressive music performance. The structure of the proposed model is equivalent to a switching state space model. We formulate two well known music recognition problems, namely tempo tracking and automatic transcription (rhythm quantization) as filtering and maximum a posteriori (MAP) state estimation tasks. The inferences are carried out using sequential Monte Carlo integration (particle filtering) techniques. For this purpose, we have derived a novel Viterbi algorithm for Rao-Blackwellized particle filters, where a subset of the hidden variables is integrated out. The resulting model is suitable for realtime tempo tracking and transcription and hence useful in a number of music applications such as adaptive automatic accompaniment and score typesetting.},
 author = {Cemgil, Ali Taylan and Kappen, Bert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5ec829debe54b19a5f78d9a65b900a39-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5ec829debe54b19a5f78d9a65b900a39-Metadata.json},
 openalex = {W2160591062},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/5ec829debe54b19a5f78d9a65b900a39-Paper.pdf},
 publisher = {MIT Press},
 title = {Tempo tracking and rhythm quantization by sequential Monte Carlo},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/5ec829debe54b19a5f78d9a65b900a39-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_6351bf9d,
 abstract = {This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users' playlists than a reasonable hand-designed kernel.},
 author = {Platt, John and Burges, Christopher J. C. and Swenson, Steven and Weare, Christopher and Zheng, Alice},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6351bf9dce654515bf1ddbd6426dfa97-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6351bf9dce654515bf1ddbd6426dfa97-Metadata.json},
 openalex = {W2169178495},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6351bf9dce654515bf1ddbd6426dfa97-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning a Gaussian Process Prior for Automatically Generating Music Playlists},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/6351bf9dce654515bf1ddbd6426dfa97-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_6492d38d,
 abstract = {Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate-and-Fire type neurons, these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate-and-Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent.},
 author = {Eggert, Julian and B\"{a}uml, Berthold},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6492d38d732122c58b44e3fdc3e9e9f3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6492d38d732122c58b44e3fdc3e9e9f3-Metadata.json},
 openalex = {W2103238495},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6492d38d732122c58b44e3fdc3e9e9f3-Paper.pdf},
 publisher = {MIT Press},
 title = {Exact differential equation population dynamics for Integrate-and-Fire neurons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/6492d38d732122c58b44e3fdc3e9e9f3-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_654ad60e,
 abstract = {This article presents a 2-phase computational learning model and application. As a demonstration, a system has been built, called CHIME for Computer Human Interacting Musical Entity. In phase 1 of training, recurrent back-propagationtrains the machinetoreproduce 3 jazz melodies. The recurrent network is expanded and is further trained in phase 2 with a reinforcement learning algorithmand a critique producedby a set of basic rules for jazz improvisation. After each phase CHIME can interactively improvise with a human in real time.},
 author = {Franklin, Judy A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/654ad60ebd1ae29cedc37da04b6b0672-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/654ad60ebd1ae29cedc37da04b6b0672-Metadata.json},
 openalex = {W2147169896},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/654ad60ebd1ae29cedc37da04b6b0672-Paper.pdf},
 publisher = {MIT Press},
 title = {Improvisation and Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/654ad60ebd1ae29cedc37da04b6b0672-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_65d2ea03,
 abstract = {Singularities are ubiquitous in the parameter space of hierarchical models such as multilayer perceptrons. At singularities, the Fisher information matrix degenerates, and the Cramer-Rao paradigm does no more hold, implying that the classical model selection theory such as AIC and MDL cannot be applied. It is important to study the relation between the generalization error and the training error at singularities. The present paper demonstrates a method of analyzing these errors both for the maximum likelihood estimator and the Bayesian predictive distribution in terms of Gaussian random fields, by using simple models.},
 author = {Amari, Shun-ichi and Park, Hyeyoung and Ozeki, Tomoko},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/65d2ea03425887a717c435081cfc5dbb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/65d2ea03425887a717c435081cfc5dbb-Metadata.json},
 openalex = {W2138988196},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/65d2ea03425887a717c435081cfc5dbb-Paper.pdf},
 publisher = {MIT Press},
 title = {Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/65d2ea03425887a717c435081cfc5dbb-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_6a508a60,
 abstract = {A novel approach for comparing sequences of observations using an explicit-expansion kernel is demonstrated. The kernel is derived using the assumption of the independence of the sequence of observations and a mean-squared error training criterion. The use of an explicit expansion kernel reduces classifier model size and computation dramatically, resulting in model sizes and computation one-hundred times smaller in our application. The explicit expansion also preserves the computational advantages of an earlier architecture based on mean-squared error training. Training using standard support vector machine methodology gives accuracy that significantly exceeds the performance of state-of-the-art mean-squared error training for a speaker recognition task.},
 author = {Campbell, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6a508a60aa3bf9510ea6acb021c94b48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6a508a60aa3bf9510ea6acb021c94b48-Metadata.json},
 openalex = {W2096599588},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6a508a60aa3bf9510ea6acb021c94b48-Paper.pdf},
 publisher = {MIT Press},
 title = {A Sequence Kernel and its Application to Speaker Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/6a508a60aa3bf9510ea6acb021c94b48-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_6f2688a5,
 abstract = {We show the convergence of two deterministic variants of Q-learning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guarantees the converges to an e-optimal policy. The second is a new and novel algorithm incremental Q-learning, which gradually promotes the values of actions that are not taken. We show that incremental Q-learning converges, in the limit, to the optimal policy Our incremental Q-learning algorithm can be viewed as derandomization of the e-greedy Q-learning.},
 author = {Even-dar, Eyal and Mansour, Yishay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6f2688a5fce7d48c8d19762b88c32c3b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6f2688a5fce7d48c8d19762b88c32c3b-Metadata.json},
 openalex = {W2129620190},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6f2688a5fce7d48c8d19762b88c32c3b-Paper.pdf},
 publisher = {MIT Press},
 title = {Convergence of Optimistic and Incremental Q-Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/6f2688a5fce7d48c8d19762b88c32c3b-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_6f4920ea,
 abstract = {In this paper, we extend the Rao-Blackwellised particle filtering method to more complex hybrid models consisting of Gaussian latent variables and discrete observations. This is accomplished by augmenting the models with artificial variables that enable us to apply Rao-Blackwellisation. Other improvements include the design of an optimal importance proposal distribution and being able to swap the sampling an selection steps to handle outliers. We focus on sequential binary classifiers that consist of linear combinations of basis functions, whose coefficients evolve according to a Gaussian smoothness prior. Our results show significant improvements.},
 author = {Andrieu, Christophe and Freitas, Nando and Doucet, Arnaud},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6f4920ea25403ec77bee9efce43ea25e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6f4920ea25403ec77bee9efce43ea25e-Metadata.json},
 openalex = {W2163682810},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/6f4920ea25403ec77bee9efce43ea25e-Paper.pdf},
 publisher = {MIT Press},
 title = {Rao-Blackwellised Particle Filtering via Data Augmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/6f4920ea25403ec77bee9efce43ea25e-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_71e09b16,
 abstract = {We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression.},
 author = {Lebanon, Guy and Lafferty, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/71e09b16e21f7b6919bbfc43f6a5b2f0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/71e09b16e21f7b6919bbfc43f6a5b2f0-Metadata.json},
 openalex = {W2157112198},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/71e09b16e21f7b6919bbfc43f6a5b2f0-Paper.pdf},
 publisher = {MIT Press},
 title = {Boosting and Maximum Likelihood for Exponential Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/71e09b16e21f7b6919bbfc43f6a5b2f0-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_798cebcc,
 abstract = {This paper studies the Iterative Double Clustering (IDC) meta-clustering algorithm, a new extension of the recent Double Clustering (DC) method of Slonim and Tishby that exhibited impressive performance on text categorization tasks [1]. Using synthetically gener ated data we empirically demonstrate that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a dramatically better classification, with minor additional computational resources.We demonstrate that the IDC algorithm is especially advantageous when the data exhibits high attribute noise. Our simulation results also show the effectiveness of IDC in text categorization problems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we propose a natural extension of IDC for (semi-supervised) transductive learning where we are given both labeled and unlabeled examples, and present preliminary empirical results showing the plausibility of the extended method in a semi-supervised setting.},
 author = {El-Yaniv, Ran and Souroujon, Oren},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/798cebccb32617ad94123450fd137104-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/798cebccb32617ad94123450fd137104-Metadata.json},
 openalex = {W1676212923},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/798cebccb32617ad94123450fd137104-Paper.pdf},
 publisher = {MIT Press},
 title = {Iterative Double Clustering for Unsupervised and Semi-supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/798cebccb32617ad94123450fd137104-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_7af6266c,
 abstract = {We present a principled and efficient planning algorithm for cooperative multiagent dynamic systems. A striking feature of our method is that the coordination and communication between the agents is not imposed, but derived directly from the system dynamics and function approximation architecture. We view the entire multiagent system as a single, large Markov decision process (MDP), which we assume can be represented in a factored way using a dynamic Bayesian network (DBN). The action space of the resulting MDP is the joint action space of the entire set of agents. Our approach is based on the use of factored linear value functions as an approximation to the joint value function. This factorization of the value function allows the agents to coordinate their actions at runtime using a natural message passing scheme. We provide a simple and efficient method for computing such an approximate value function by solving a single linear program, whose size is determined by the interaction between the value function structure and the DBN. We thereby avoid the exponential blowup in the state and action space. We show that our approach compares favorably with approaches based on reward sharing. We also show that our algorithm is an efficient alternative to more complicated algorithms even in the single agent case.},
 author = {Guestrin, Carlos and Koller, Daphne and Parr, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7af6266cc52234b5aa339b16695f7fc4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7af6266cc52234b5aa339b16695f7fc4-Metadata.json},
 openalex = {W2134779831},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7af6266cc52234b5aa339b16695f7fc4-Paper.pdf},
 publisher = {MIT Press},
 title = {Multiagent Planning with Factored MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/7af6266cc52234b5aa339b16695f7fc4-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_7b1ce3d7,
 abstract = {Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity. We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between bottom-up inference, influenced by input stimuli, and top-down inference, influenced by contextual information. We illustrate our proposal using a hierarchical hidden Markov model.},
 author = {Dayan, Peter and Yu, Angela J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7b1ce3d73b70f1a7246e7b76a35fb552-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7b1ce3d73b70f1a7246e7b76a35fb552-Metadata.json},
 openalex = {W2130152984},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7b1ce3d73b70f1a7246e7b76a35fb552-Paper.pdf},
 publisher = {MIT Press},
 title = {ACh, Uncertainty, and Cortical Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/7b1ce3d73b70f1a7246e7b76a35fb552-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_7b7a53e2,
 abstract = {We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation—which is borne out in repeated experiments—that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.},
 author = {Ng, Andrew and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Metadata.json},
 openalex = {W2163614729},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf},
 publisher = {MIT Press},
 title = {On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_7ca57a9f,
 abstract = {We propose a new particle filter that incorporates a model of costs when generating particles. The approach is motivated by the observation that the costs of accidentally not tracking hypotheses might be significant in some areas of state space, and next to irrelevant in others. By incorporating a cost model into particle filtering, states that are more critical to the system performance are more likely to be tracked. Automatic calculation of the cost model is implemented using an MDP value function calculation that estimates the value of tracking a particular state. Experiments in two mobile robot domains illustrate the appropriateness of the approach.},
 author = {Thrun, Sebastian and Langford, John and Verma, Vandi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7ca57a9f85a19a6e4b9a248c1daca185-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7ca57a9f85a19a6e4b9a248c1daca185-Metadata.json},
 openalex = {W2098157757},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7ca57a9f85a19a6e4b9a248c1daca185-Paper.pdf},
 publisher = {MIT Press},
 title = {Risk Sensitive Particle Filters},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/7ca57a9f85a19a6e4b9a248c1daca185-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_7d2b92b6,
 abstract = {Spike-triggered averaging techniques are effective for linear characterization of neural responses. But neurons exhibit important nonlinear behaviors, such as gain control, that are not captured by such analyses. We describe a spike-triggered covariance method for retrieving suppressive components of the gain control signal in a neuron. We demonstrate the method in simulation and on retinal ganglion cell data. Analysis of physiological data reveals significant suppressive axes and explains neural nonlinearities. This method should be applicable to other sensory areas and modalities.},
 author = {Schwartz, Odelia and Chichilnisky, E.J. and Simoncelli, Eero},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7d2b92b6726c241134dae6cd3fb8c182-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7d2b92b6726c241134dae6cd3fb8c182-Metadata.json},
 openalex = {W2171351784},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7d2b92b6726c241134dae6cd3fb8c182-Paper.pdf},
 publisher = {MIT Press},
 title = {Characterizing Neural Gain Control using Spike-triggered Covariance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/7d2b92b6726c241134dae6cd3fb8c182-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_7f16109f,
 abstract = {In the 'missing data' approach to improving the robustness of automatic speech recognition to added noise, an initial process identifies spectral-temporal regions which are dominated by the speech source. The remaining regions are considered to be 'missing'. In this paper we develop a connectionist approach to the problem of adapting speech recognition to the missing data case, using Recurrent Neural Networks. In contrast to methods based on Hidden Markov Models, RNNs allow us to make use of long-term time constraints and to make the problems of classification with incomplete data and imputing missing values interact. We report encouraging results on an isolated digit recognition task.},
 author = {Parveen, S. and Green, P.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7f16109f1619fd7a733daf5a84c708c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7f16109f1619fd7a733daf5a84c708c1-Metadata.json},
 openalex = {W2157310573},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7f16109f1619fd7a733daf5a84c708c1-Paper.pdf},
 publisher = {MIT Press},
 title = {Speech Recognition with Missing Data using Recurrent Neural Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/7f16109f1619fd7a733daf5a84c708c1-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_801272ee,
 abstract = {Despite many empirical successes of spectral clustering methods— algorithms that cluster points using eigenvectors of matrices derived from the data—there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
 author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Metadata.json},
 openalex = {W2165874743},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf},
 publisher = {MIT Press},
 title = {On Spectral Clustering: Analysis and an algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/801272ee79cfde7fa5960571fee36b9b-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_8038da89,
 abstract = {With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblem. We demonstrate that simple priors can greatly improve image segmentation results.},
 author = {Yu, Stella X. and Shi, Jianbo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8038da89e49ac5eabb489cfc6cea9fc1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8038da89e49ac5eabb489cfc6cea9fc1-Metadata.json},
 openalex = {W2097649450},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8038da89e49ac5eabb489cfc6cea9fc1-Paper.pdf},
 publisher = {MIT Press},
 title = {Grouping with Bias},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/8038da89e49ac5eabb489cfc6cea9fc1-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_814a9c18,
 abstract = {We present Linear Relational Embedding (LRE), a new method of learning a distributed representation of concepts from data consisting of instances of relations between given concepts. Its final goal is to be able to generalize, i.e. infer new instances of these relations among the concepts. On a task involving family relationships we show that LRE can generalize better than any previously published method. We then show how LRE can be used effectively to find compact distributed representations for variable-sized recursive data structures, such as trees and lists.},
 author = {Paccanaro, Alberto and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/814a9c18f5abff398787c9cfcbf3d80c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/814a9c18f5abff398787c9cfcbf3d80c-Metadata.json},
 openalex = {W2164695327},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/814a9c18f5abff398787c9cfcbf3d80c-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Hierarchical Structures with Linear Relational Embedding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/814a9c18f5abff398787c9cfcbf3d80c-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_819e3d6c,
 abstract = {If the promise of computational modeling is to be fully realized in higher-level cognitive domains such as language processing, principled methods must be developed to construct the semantic representations used in such models. In this paper, we propose the use of an established formalism from mathematical psychology, additive clustering, as a means of automatically constructing binary representations for objects using only pair-wise similarity data. However, existing methods for the unsupervised learning of additive clustering models do not scale well to large problems. We present a new algorithm for additive clustering, based on a novel heuristic technique for combinatorial optimization. The algorithm is simpler than previous formulations and makes fewer independence assumptions. Extensive empirical tests on both human and synthetic data suggest that it is more effective than previous methods and that it also scales better to larger problems. By making additive clustering practical, we take a significant step toward scaling connectionist models beyond hand-coded examples.},
 author = {Ruml, Wheeler},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/819e3d6c1381eac87c17617e5165f38c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/819e3d6c1381eac87c17617e5165f38c-Metadata.json},
 openalex = {W2128868611},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/819e3d6c1381eac87c17617e5165f38c-Paper.pdf},
 publisher = {MIT Press},
 title = {Constructing Distributed Representations Using Additive Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/819e3d6c1381eac87c17617e5165f38c-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_8232e119,
 abstract = {Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of 1-factor PPCA models and (3) a products of experts construction for an AR(1) process.},
 author = {Williams, Christopher and Agakov, Felix and Felderhof, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8232e119d8f59aa83050a741631803a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8232e119d8f59aa83050a741631803a6-Metadata.json},
 openalex = {W2157992637},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8232e119d8f59aa83050a741631803a6-Paper.pdf},
 publisher = {MIT Press},
 title = {Products of Gaussians},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/8232e119d8f59aa83050a741631803a6-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_84ddfb34,
 abstract = {A key question in neuroscience is how to encode sensory stimuli such as images and sounds. Motivated by studies of response properties of neurons in the early cortical areas, we propose an encoding scheme that dispenses with absolute measures of signal intensity or contrast and uses, instead, only local ordinal measures. In this scheme, the structure of a signal is represented by a set of equalities and inequalities across adjacent regions. In this paper, we focus on characterizing the fidelity of this representation strategy. We develop a regularization approach for image reconstruction from ordinal measures and thereby demonstrate that the ordinal representation scheme can faithfully encode signal structure. We also present a neurally plausible implementation of this computation that uses only local update rules. The results highlight the robustness and generalization ability of local ordinal encodings for the task of pattern classification.},
 author = {Sadr, Javid and Mukherjee, Sayan and Thoresz, Keith and Sinha, Pawan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/84ddfb34126fc3a48ee38d7044e87276-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/84ddfb34126fc3a48ee38d7044e87276-Metadata.json},
 openalex = {W2147686436},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/84ddfb34126fc3a48ee38d7044e87276-Paper.pdf},
 publisher = {MIT Press},
 title = {The Fidelity of Local Ordinal Encoding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/84ddfb34126fc3a48ee38d7044e87276-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_850af92f,
 abstract = {High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description, however, does not provide a parameterization of the manifold—arguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difficult problem. Our local linear models are represented by a mixture of factor analyzers, and the global coordination of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model's parameter space, favoring models whose internal coordinate systems are aligned in a consistent way. As a result, the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold—even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones.},
 author = {Roweis, Sam and Saul, Lawrence and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/850af92f8d9903e7a4e0559a98ecc857-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/850af92f8d9903e7a4e0559a98ecc857-Metadata.json},
 openalex = {W2119548491},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/850af92f8d9903e7a4e0559a98ecc857-Paper.pdf},
 publisher = {MIT Press},
 title = {Global Coordination of Local Linear Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/850af92f8d9903e7a4e0559a98ecc857-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_88ef51f0,
 abstract = {Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly suppressed by long range correlations. Here we study the efficiency of coding information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses.},
 author = {Shamir, Maoz and Sompolinsky, Haim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/88ef51f0bf911e452e8dbb1d807a81ab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/88ef51f0bf911e452e8dbb1d807a81ab-Metadata.json},
 openalex = {W2167368834},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/88ef51f0bf911e452e8dbb1d807a81ab-Paper.pdf},
 publisher = {MIT Press},
 title = {Correlation Codes in Neuronal Populations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/88ef51f0bf911e452e8dbb1d807a81ab-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_89885ff2,
 abstract = {Unsupervised learning algorithms have been derived for several statistical models of English grammar, but their computational complexity makes applying them to large data sets intractable. This paper presents a probabilistic model of English grammar that is much simpler than conventional models, but which admits an efficient EM training algorithm. The model is based upon grammatical bigrams, i.e., syntactic relationships between pairs of words. We present the results of experiments that quantify the representational adequacy of the grammatical bigram model, its ability to generalize from labelled data, and its ability to induce syntactic structure from large amounts of raw text.},
 author = {Paskin, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/89885ff2c83a10305ee08bd507c1049c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/89885ff2c83a10305ee08bd507c1049c-Metadata.json},
 openalex = {W2294835092},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/89885ff2c83a10305ee08bd507c1049c-Paper.pdf},
 publisher = {MIT Press},
 title = {Grammatical Bigrams},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/89885ff2c83a10305ee08bd507c1049c-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_8c249675,
 abstract = {We consider the problem of learning to attain multiple goals in a dynamic environment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature. This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games. This algorithm combines, in an appropriate way, a finite set of standard, scalar-reward learning algorithms. Sufficient conditions are given for the convergence of the learning algorithm to a general target set. The specialization of these results to the single-controller Markov decision problem are discussed as well.},
 author = {Mannor, Shie and Shimkin, Nahum},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8c249675aea6c3cbd91661bbae767ff1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8c249675aea6c3cbd91661bbae767ff1-Metadata.json},
 openalex = {W2097031964},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8c249675aea6c3cbd91661bbae767ff1-Paper.pdf},
 publisher = {MIT Press},
 title = {The Steering Approach for Multi-Criteria Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/8c249675aea6c3cbd91661bbae767ff1-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_8d8818c8,
 abstract = {We address the problem of non-convergence of online reinforcement learning algorithms (e.g., Q learning and SARSA (λ)) by adopting an incremental-batch approach that separates the exploration process from the function fitting process. Our BFBP (Batch Fit to Best Paths) algorithm alternates between an exploration phase (during which trajectories are generated to try to find fragments of the optimal policy) and a function fitting phase (during which a function approximator is fit to the best known paths from start states to terminal states). An advantage of this approach is that batch value-function fitting is a global process, which allows it to address the tradeoffs in function approximation that cannot be handled by local, online algorithms. This approach was pioneered by Boyan and Moore with their GROWSUPPORT and ROUT algorithms. We show how to improve upon their work by applying a better exploration process and by enriching the function fitting procedure to incorporate Bellman error and advantage error measures into the objective function. The results show improved performance on several benchmark problems.},
 author = {Wang, Xin and Dietterich, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8d8818c8e140c64c743113f563cf750f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8d8818c8e140c64c743113f563cf750f-Metadata.json},
 openalex = {W2096289569},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf},
 publisher = {MIT Press},
 title = {Stabilizing Value Function Approximation with the BFBP Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/8d8818c8e140c64c743113f563cf750f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_9185f3ec,
 abstract = {We develop a tree-based reparameterization framework that provides a new conceptual view of a large class of iterative algorithms for computing approximate marginals in graphs with cycles. It includes belief propagation (BP), which can be reformulated as a very local form of reparameterization. More generally, we consider algorithms that perform exact computations over spanning trees of the full graph. On the practical side, we find that such tree reparameterization (TRP) algorithms have convergence properties superior to BP. The reparameterization perspective also provides a number of theoretical insights into approximate inference, including a new characterization of fixed points; and an invariance intrinsic to TRP/BP. These two properties enable us to analyze and bound the error between the TRP/BP approximations and the actual marginals. While our results arise naturally from the TRP perspective, most of them apply in an algorithm-independent manner to any local minimum of the Bethe free energy. Our results also have natural extensions to more structured approximations [e.g., 1, 2].},
 author = {Wainwright, Martin J and Jaakkola, Tommi and Willsky, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9185f3ec501c674c7c788464a36e7fb3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9185f3ec501c674c7c788464a36e7fb3-Metadata.json},
 openalex = {W2135997060},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9185f3ec501c674c7c788464a36e7fb3-Paper.pdf},
 publisher = {MIT Press},
 title = {Tree-based reparameterization for approximate inference on loopy graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/9185f3ec501c674c7c788464a36e7fb3-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_92bbd31f,
 abstract = {We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modifing his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment.},
 author = {Isbell, Charles and Shelton, Christian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/92bbd31f8e0e43a7da8a6295b251725f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/92bbd31f8e0e43a7da8a6295b251725f-Metadata.json},
 openalex = {W2116372557},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/92bbd31f8e0e43a7da8a6295b251725f-Paper.pdf},
 publisher = {MIT Press},
 title = {Cobot: A Social Reinforcement Learning Agent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/92bbd31f8e0e43a7da8a6295b251725f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_931af583,
 abstract = {In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost. We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data.},
 author = {d\textquotesingle Alch\'{e}-Buc, Florence and Grandvalet, Yves and Ambroise, Christophe},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/931af583573227f0220bc568c65ce104-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/931af583573227f0220bc568c65ce104-Metadata.json},
 openalex = {W2153818524},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/931af583573227f0220bc568c65ce104-Paper.pdf},
 publisher = {MIT Press},
 title = {Semi-Supervised MarginBoost},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/931af583573227f0220bc568c65ce104-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_95192c98,
 abstract = {A challenging, unsolved problem in the speech recognition community is recognizing speech signals that are corrupted by loud, highly nonstationary noise. One approach to noisy speech recognition is to automatically remove the noise from the cepstrum sequence before feeding it in to a clean speech recognizer. In previous work published in Eurospeech, we showed how a probability model trained on clean speech and a separate probability model trained on noise could be combined for the purpose of estimating the noise-free speech from the noisy speech. We showed how an iterative 2nd order vector Taylor series approximation could be used for probabilistic inference in this model. In many circumstances, it is not possible to obtain examples of noise without speech. Noise statistics may change significantly during an utterance, so that speech-free frames are not sufficient for estimating the noise model. In this paper, we show how the noise model can be learned even when the data contains speech. In particular, the noise model can be learned from the test utterance and then used to denoise the test utterance. The approximate inference technique is used as an approximate E step in a generalized EM algorithm that learns the parameters of the noise model from a test utterance. For both Wall Street Journal data with added noise samples and the Aurora benchmark, we show that the new noise adaptive technique performs as well as or significantly better than the non-adaptive algorithm, without the need for a separate training set of noise examples.},
 author = {Frey, Brendan J and Kristjansson, Trausti and Deng, Li and Acero, Alex},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/95192c98732387165bf8e396c0f2dad2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/95192c98732387165bf8e396c0f2dad2-Metadata.json},
 openalex = {W2112089769},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/95192c98732387165bf8e396c0f2dad2-Paper.pdf},
 publisher = {MIT Press},
 title = {ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/95192c98732387165bf8e396c0f2dad2-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_955cb567,
 abstract = {The nearest neighbor technique is a simple and appealing method to address classification problems. It relies on the assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. We propose a technique that computes a locally flexible metric by means of Support Vector Machines (SVMs). The maximum margin boundary found by the SVM is used to determine the most discriminant direction over the query's neighborhood. Such direction provides a local weighting scheme for input features. We present experimental evidence of classification performance improvement over the SVM algorithm alone and over a variety of adaptive learning schemes, by using both simulated and real data sets.},
 author = {Domeniconi, Carlotta and Gunopulos, Dimitrios},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/955cb567b6e38f4c6b3f28cc857fc38c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/955cb567b6e38f4c6b3f28cc857fc38c-Metadata.json},
 openalex = {W2150470820},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/955cb567b6e38f4c6b3f28cc857fc38c-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive Nearest Neighbor Classification using Support Vector Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/955cb567b6e38f4c6b3f28cc857fc38c-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_95f6870f,
 abstract = {In previous work on transformed mixtures of Gaussians and transformed hidden Markov models, we showed how the EM algorithm in a discrete latent variable model can be used to jointly normalize data (e.g., center images, pitch-normalize spectrograms) and learn a mixture model of the normalized data. The only input to the algorithm is the data, a list of possible transformations, and the number of clusters to find. The main criticism of this work was that the exhaustive computation of the posterior probabilities over transformations would make scaling up to large feature vectors and large sets of transformations intractable. Here, we describe how a tremendous speed-up is acheived through the use of a variational technique for decoupling transformations, and a fast Fourier transform method for computing posterior probabilities. For N × N images, learning C clusters under N rotations, N scales, N x-translations and N y-translations takes only (C + 2 log N)N2 scalar operations per iteration. In contrast, the original algorithm takes CN6 operations to account for these transformations. We give results on learning a 4-component mixture model from a video sequence with frames of size 320×240. The model accounts for 360 rotations and 76,800 translations. Each iteration of EM takes only 10 seconds per frame in MATLAB, which is over 5 million times faster than the original algorithm.},
 author = {Frey, Brendan J and Jojic, Nebojsa},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/95f6870ff3dcd442254e334a9033d349-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/95f6870ff3dcd442254e334a9033d349-Metadata.json},
 openalex = {W2096198554},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/95f6870ff3dcd442254e334a9033d349-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast, Large-Scale Transformation-Invariant Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/95f6870ff3dcd442254e334a9033d349-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_96055f5b,
 abstract = {We describe a neural network that enhances and completes salient closed contours in images. Our work is different from all previous work in three important ways. First, like the input provided to primary visual cortex (V1) by the lateral geniculate nucleus (LGN), the input to our computation is isotropic. That is, it is composed of spots, not edges. Second, our network computes a well-defined function of the input based on a distribution of closed contours characterized by a random process. Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input image.},
 author = {Williams, Lance and Zweck, John W.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/96055f5b06bf9381ac43879351642cf5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/96055f5b06bf9381ac43879351642cf5-Metadata.json},
 openalex = {W2144398377},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/96055f5b06bf9381ac43879351642cf5-Paper.pdf},
 publisher = {MIT Press},
 title = {A rotation and translation invariant discrete saliency network},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/96055f5b06bf9381ac43879351642cf5-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_978d7667,
 abstract = {Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thala-mus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their firing is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?].

Integration is a difficult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a firing-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed.

Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We find that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.},
 author = {Hahnloser, Richard and Xie, Xiaohui and Seung, H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/978d76676f5e7918f81d28e7d092ca0d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/978d76676f5e7918f81d28e7d092ca0d-Metadata.json},
 openalex = {W2172248326},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/978d76676f5e7918f81d28e7d092ca0d-Paper.pdf},
 publisher = {MIT Press},
 title = {A Theory of Neural Integration in the Head-Direction System},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/978d76676f5e7918f81d28e7d092ca0d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_98c72428,
 abstract = {We present a new approach to bounding the true error rate of a continuous valued classifier based upon PAC-Bayes bounds. The method first constructs a distribution over classifiers by determining how sensitive each parameter in the model is to noise. The true error rate of the stochastic classifier found with the sensitivity analysis can then be tightly bounded using a PAC-Bayes bound. In this paper we demonstrate the method on artificial neural networks with results of a 2 - 3 order of magnitude improvement vs. the best deterministic neural net bounds.},
 author = {Langford, John and Caruana, Rich},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/98c7242894844ecd6ec94af67ac8247d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/98c7242894844ecd6ec94af67ac8247d-Metadata.json},
 openalex = {W2115065944},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/98c7242894844ecd6ec94af67ac8247d-Paper.pdf},
 publisher = {MIT Press},
 title = {Not) Bounding the True Error},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/98c7242894844ecd6ec94af67ac8247d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_9a49a25d,
 abstract = {The Cluster Variation method is a class of approximation methods containing the Bethe and Kikuchi approximations as special cases. We derive two novel iteration schemes for the Cluster Variation Method. One is a fixed point iteration scheme which gives a significant improvement over loopy BP. mean field and TAP methods on directed graphical models. The other is a gradient based method, that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration. We conclude that the methods are of significant practical value for large inference problems.},
 author = {Kappen, Hilbert and Wiegerinck, Wim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9a49a25d845a483fae4be7e341368e36-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9a49a25d845a483fae4be7e341368e36-Metadata.json},
 openalex = {W2136526868},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf},
 publisher = {MIT Press},
 title = {Novel Iteration Schemes for the Cluster Variation Method},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_9afefc52,
 abstract = {We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an infinite number of Experts. Inference in this model may be done efficiently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.},
 author = {Rasmussen, Carl and Ghahramani, Zoubin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9afefc52942cb83c7c1f14b2139b09ba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9afefc52942cb83c7c1f14b2139b09ba-Metadata.json},
 openalex = {W2098949458},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9afefc52942cb83c7c1f14b2139b09ba-Paper.pdf},
 publisher = {MIT Press},
 title = {Infinite Mixtures of Gaussian Process Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/9afefc52942cb83c7c1f14b2139b09ba-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_9bb6dee7,
 author = {Brown, Andrew and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9bb6dee73b8b0ca97466ccb24fff3139-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9bb6dee73b8b0ca97466ccb24fff3139-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9bb6dee73b8b0ca97466ccb24fff3139-Paper.pdf},
 publisher = {MIT Press},
 title = {Relative Density Nets: A New Way to Combine Backpropagation with HMM\textquotesingle s},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/9bb6dee73b8b0ca97466ccb24fff3139-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_9d7311ba,
 abstract = {This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning.},
 author = {Wu, Si and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9d7311ba459f9e45ed746755a32dcd11-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9d7311ba459f9e45ed746755a32dcd11-Metadata.json},
 openalex = {W2124306941},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural Implementation of Bayesian Inference in Population Codes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/9d7311ba459f9e45ed746755a32dcd11-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_9f62b862,
 abstract = {The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a specific sequential minimization of the free energy leads to a generalization of Minka's expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classification and density estimation with Gaussian processes and on an independent component analysis problem.},
 author = {Csat\'{o}, Lehel and Opper, Manfred and Winther, Ole},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9f62b8625f914a002496335037e9ad97-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9f62b8625f914a002496335037e9ad97-Metadata.json},
 openalex = {W2114412769},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/9f62b8625f914a002496335037e9ad97-Paper.pdf},
 publisher = {MIT Press},
 title = {TAP Gibbs Free Energy, Belief Propagation and Sparsity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/9f62b8625f914a002496335037e9ad97-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a00e5eb0,
 abstract = {We suggest a nonparametric framework for unsupervised learning of projection models in terms of density estimation on quantized sample spaces. The objective is not to optimally reconstruct the data but instead the quantizer is chosen to optimally reconstruct the density of the data. For the resulting quantizing density estimator (QDE) we present a general method for parameter estimation and model selection. We show how projection sets which correspond to traditional unsupervised methods like vector quantization or PCA appear in the new framework. For a principal component quantizer we present results on synthetic and real-world data, which show that the QDE can improve the generalization of the kernel density estimator although its estimate is based on significantly lower-dimensional projection indices of the data.},
 author = {Meinicke, Peter and Ritter, Helge},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a00e5eb0973d24649a4a920fc53d9564-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a00e5eb0973d24649a4a920fc53d9564-Metadata.json},
 openalex = {W2114207451},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a00e5eb0973d24649a4a920fc53d9564-Paper.pdf},
 publisher = {MIT Press},
 title = {Quantizing Density Estimators},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a00e5eb0973d24649a4a920fc53d9564-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a0128693,
 abstract = {We introduce the Concave-Convex procedure (CCCP) which constructs discrete time iterative dynamical systems which are guaranteed to monotonically decrease global optimization/energy functions. It can be applied to (almost) any optimization problem and many existing algorithms can be interpreted in terms of CCCP. In particular, we prove relationships to some applications of Legendre transform techniques. We then illustrate CCCP by applications to Potts models, linear assignment, EM algorithms, and Generalized Iterative Scaling (GIS). CCCP can be used both as a new way to understand existing optimization algorithms and as a procedure for generating new algorithms.},
 author = {Yuille, Alan L and Rangarajan, Anand},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a012869311d64a44b5a0d567cd20de04-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a012869311d64a44b5a0d567cd20de04-Metadata.json},
 openalex = {W2147102238},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a012869311d64a44b5a0d567cd20de04-Paper.pdf},
 publisher = {MIT Press},
 title = {The Concave-Convex Procedure (CCCP)},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a012869311d64a44b5a0d567cd20de04-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a03fa308,
 abstract = {We investigate the following data mining problem from Computational Chemistry: From a large data set of compounds, find those that bind to a target molecule in as few iterations of biological testing as possible. In each iteration a comparatively small batch of compounds is screened for binding to the target. We apply active learning techniques for selecting the successive batches.

One selection strategy picks unlabeled examples closest to the maximum margin hyperplane. Another produces many weight vectors by running perceptrons over multiple permutations of the data. Each weight vector votes with its ± prediction and we pick the unlabeled examples for which the prediction is most evenly split between + and -. For a third selection strategy note that each unlabeled example bisects the version space of consistent weight vectors. We estimate the volume on both sides of the split by bouncing a billiard through the version space and select un-labeled examples that cause the most even split of the version space.

We demonstrate that on two data sets provided by DuPont Pharmaceuticals that all three selection strategies perform comparably well and are much better than selecting random batches for testing.},
 author = {Warmuth, Manfred K. K and R\"{a}tsch, Gunnar and Mathieson, Michael and Liao, Jun and Lemmen, Christian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a03fa30821986dff10fc66647c84c9c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a03fa30821986dff10fc66647c84c9c3-Metadata.json},
 openalex = {W2152798516},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a03fa30821986dff10fc66647c84c9c3-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Learning in the Drug Discovery Process},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a03fa30821986dff10fc66647c84c9c3-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a088ea20,
 abstract = {In this work, we introduce an information-theoretic based correction term to the likelihood ratio classification method for multiple classes. Under certain conditions, the term is sufficient for optimally correcting the difference between the true and estimated likelihood ratio, and we analyze this in the Gaussian case. We find that the new correction term significantly improves the classification results when tested on medium vocabulary speech recognition tasks. Moreover, the addition of this term makes the class comparisons analogous to an intransitive game and we therefore use several tournament-like strategies to deal with this issue. We find that further small improvements are obtained by using an appropriate tournament. Lastly, we find that intransitivity appears to be a good measure of classification confidence.},
 author = {Bilmes, Jeff and Ji, Gang and Meila, Marina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a088ea2078cd92b0b8a0e78a32c5c082-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a088ea2078cd92b0b8a0e78a32c5c082-Metadata.json},
 openalex = {W2133494643},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a088ea2078cd92b0b8a0e78a32c5c082-Paper.pdf},
 publisher = {MIT Press},
 title = {Intransitive Likelihood-Ratio Classifiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a088ea2078cd92b0b8a0e78a32c5c082-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a1d7311f,
 abstract = {Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e.g. pedestrian images) using a representation based on pixel-wise similarities, similarity templates. Because of its invariance to the colors of particular components of an object, this representation enables detection of instances of an object class and enables alignment of those instances. Further, this model implicitly represents the regions of color regularity in the class-specific image set enabling a decomposition of that object class into component regions.},
 author = {Stauffer, Chris and Miller, Erik and Tieu, Kinh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a1d7311f2a312426d710e1c617fcbc8c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a1d7311f2a312426d710e1c617fcbc8c-Metadata.json},
 openalex = {W2142359735},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a1d7311f2a312426d710e1c617fcbc8c-Paper.pdf},
 publisher = {MIT Press},
 title = {Transform-invariant Image Decomposition with Similarity Templates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a1d7311f2a312426d710e1c617fcbc8c-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a29d1598,
 abstract = {Cortical neurons in vivo show fluctuations in their membrane potential of the order of several milli-volts. Using simple and biophysically realistic models of a single neuron we demonstrate that noise induced fluctuations can be used to adaptively optimize the sensitivity of the neuron's output to ensembles of subthreshold inputs of different average strengths. Optimal information transfer is achieved by changing the strength of the noise such that the neuron's average firing rate remains constant. Adaptation is fast, because only crude estimates of the output rate are required at any time.},
 author = {Wenning, Gregor and Obermayer, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a29d1598024f9e87beab4b98411d48ce-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a29d1598024f9e87beab4b98411d48ce-Metadata.json},
 openalex = {W2053913015},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a29d1598024f9e87beab4b98411d48ce-Paper.pdf},
 publisher = {MIT Press},
 title = {Activity Driven Adaptive Stochastic Resonance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a29d1598024f9e87beab4b98411d48ce-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a38b1617,
 abstract = {This paper presents reinforcement learning with a Long Short-Term Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage (λ) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task.},
 author = {Bakker, Bram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5-Metadata.json},
 openalex = {W2096533821},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5-Paper.pdf},
 publisher = {MIT Press},
 title = {Reinforcement Learning with Long Short-Term Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a38b16173474ba8b1a95bcbc30d3b8a5-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a4856405,
 abstract = {We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system.},
 author = {Kohlmorgen, Jens and Lemm, Steven},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a48564053b3c7b54800246348c7fa4a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a48564053b3c7b54800246348c7fa4a0-Metadata.json},
 openalex = {W2135858662},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a48564053b3c7b54800246348c7fa4a0-Paper.pdf},
 publisher = {MIT Press},
 title = {A Dynamic HMM for On-line Segmentation of Sequential Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a48564053b3c7b54800246348c7fa4a0-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a501bebf,
 abstract = {The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. PageRank assigns to a page a score proportional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. We propose to improve PageRank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made possible by precomputing at crawl time (and thus once for all queries) the necessary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today's large search engines.},
 author = {Richardson, Matthew and Domingos, Pedro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a501bebf79d570651ff601788ea9d16d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a501bebf79d570651ff601788ea9d16d-Metadata.json},
 openalex = {W2096041903},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a501bebf79d570651ff601788ea9d16d-Paper.pdf},
 publisher = {MIT Press},
 title = {The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a5910243,
 author = {Koenig, S. and Likhachev, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a591024321c5e2bdbd23ed35f0574dde-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a591024321c5e2bdbd23ed35f0574dde-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a591024321c5e2bdbd23ed35f0574dde-Paper.pdf},
 publisher = {MIT Press},
 title = {Incremental A\ast},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a591024321c5e2bdbd23ed35f0574dde-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a5a61717,
 abstract = {Locally Linear Embedding (LLE) is an elegant nonlinear dimensionality-reduction technique recently introduced by Roweis and Saul [2]. It fails when the data is divided into separate groups. We study a variant of LLE that can simultaneously group the data and calculate local embedding of each group. An estimate for the upper bound on the intrinsic dimension of the data set is obtained automatically.},
 author = {Polito, Marzia and Perona, Pietro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a5a61717dddc3501cfdf7a4e22d7dbaa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a5a61717dddc3501cfdf7a4e22d7dbaa-Metadata.json},
 openalex = {W2100078691},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a5a61717dddc3501cfdf7a4e22d7dbaa-Paper.pdf},
 publisher = {MIT Press},
 title = {Grouping and dimensionality reduction by locally linear embedding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a5a61717dddc3501cfdf7a4e22d7dbaa-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a82d922b,
 abstract = {To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems.},
 author = {Szummer, Martin and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a82d922b133be19c1171534e6594f754-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a82d922b133be19c1171534e6594f754-Metadata.json},
 openalex = {W2122837498},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a82d922b133be19c1171534e6594f754-Paper.pdf},
 publisher = {MIT Press},
 title = {Partially labeled classification with Markov random walks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a82d922b133be19c1171534e6594f754-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a869ccbc,
 abstract = {A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classification algorithms can be employed without further modifications. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs).},
 author = {Shimodaira, Hiroshi and Noma, Ken-ichi and Nakai, Mitsuru and Sagayama, Shigeki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a869ccbcbd9568808b8497e28275c7c8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a869ccbcbd9568808b8497e28275c7c8-Metadata.json},
 openalex = {W2108482774},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a869ccbcbd9568808b8497e28275c7c8-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamic Time-Alignment Kernel in Support Vector Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a869ccbcbd9568808b8497e28275c7c8-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a96d3afe,
 abstract = {Almost two decades ago, Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2,3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented.},
 author = {Latham, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a96d3afec184766bfeca7a9f989fc7e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a96d3afec184766bfeca7a9f989fc7e7-Metadata.json},
 openalex = {W2104205956},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a96d3afec184766bfeca7a9f989fc7e7-Paper.pdf},
 publisher = {MIT Press},
 title = {Associative memory in realistic neuronal networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a96d3afec184766bfeca7a9f989fc7e7-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_a9813e95,
 abstract = {Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.},
 author = {Zhang, T.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a9813e9550fee3110373c21fa012eee7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a9813e9550fee3110373c21fa012eee7-Metadata.json},
 openalex = {W2145709095},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/a9813e9550fee3110373c21fa012eee7-Paper.pdf},
 publisher = {MIT Press},
 title = {A General Greedy Approximation Algorithm with Applications},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/a9813e9550fee3110373c21fa012eee7-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_abdbeb4d,
 abstract = {This paper presents a method for obtaining class membership probability estimates for multiclass classification problems by coupling the probability estimates produced by binary classifiers. This is an extension for arbitrary code matrices of a method due to Hastie and Tibshirani for pairwise coupling of probability estimates. Experimental results with Boosted Naive Bayes show that our method produces calibrated class membership probability estimates, while having similar classification accuracy as loss-based decoding, a method for obtaining the most likely class that does not generate probability estimates.},
 author = {Zadrozny, B.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/abdbeb4d8dbe30df8430a8394b7218ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/abdbeb4d8dbe30df8430a8394b7218ef-Metadata.json},
 openalex = {W2123755669},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/abdbeb4d8dbe30df8430a8394b7218ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Reducing multiclass to binary by coupling probability estimates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/abdbeb4d8dbe30df8430a8394b7218ef-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_aebf7782,
 abstract = {The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original inference algorithm is rather complicated, and takes O(T3) time, where is the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes O(T) time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference.},
 author = {Murphy, Kevin P and Paskin, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/aebf7782a3d445f43cf30ee2c0d84dee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/aebf7782a3d445f43cf30ee2c0d84dee-Metadata.json},
 openalex = {W2146143523},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/aebf7782a3d445f43cf30ee2c0d84dee-Paper.pdf},
 publisher = {MIT Press},
 title = {Linear-time inference in Hierarchical HMMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/aebf7782a3d445f43cf30ee2c0d84dee-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_b2ab0019,
 abstract = {We present a sequential Monte Carlo method applied to additive noise compensation for robust speech recognition in time-varying noise. The method generates a set of samples according to the prior distribution given by clean speech models and noise prior evolved from previous estimation. An explicit model representing noise effects on speech features is used, so that an extended Kalman filter is constructed for each sample, generating the updated continuous state estimate as the estimation of the noise parameter, and prediction likelihood for weighting each sample. Minimum mean square error (MMSE) inference of the time-varying noise parameter is carried out over these samples by fusion the estimation of samples according to their weights. A residual resampling selection step and a Metropolis-Hastings smoothing step are used to improve calculation efficiency. Experiments were conducted on speech recognition in simulated non-stationary noises, where noise power changed artificially, and highly non-stationary Machinegun noise. In all the experiments carried out, we observed that the method can have significant recognition performance improvement, over that achieved by noise compensation with stationary noise assumption.},
 author = {Yao, K. and Nakamura, S.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b2ab001909a8a6f04b51920306046ce5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b2ab001909a8a6f04b51920306046ce5-Metadata.json},
 openalex = {W2162737895},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf},
 publisher = {MIT Press},
 title = {Sequential Noise Compensation by Sequential Monte Carlo Method},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/b2ab001909a8a6f04b51920306046ce5-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_b3b4d2db,
 abstract = {We present a simple approach for computing reasonable policies for factored Markov decision processes (MDPs), when the optimal value function can be approximated by a compact linear form. Our method is based on solving a single linear program that approximates the best linear fit to the optimal value function. By applying an efficient constraint generation procedure we obtain an iterative solution method that tackles concise linear programs. This direct linear programming approach experimentally yields a significant reduction in computation time over approximate value- and policy-iteration methods (sometimes reducing several hours to a few seconds). However, the quality of the solutions produced by linear programming is weaker—usually about twice the approximation error for the same approximating class. Nevertheless, the speed advantage allows one to use larger approximation classes to achieve similar error in reasonable time.},
 author = {Schuurmans, Dale and Patrascu, Relu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b3b4d2dbedc99fe843fd3dedb02f086f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b3b4d2dbedc99fe843fd3dedb02f086f-Metadata.json},
 openalex = {W2149385746},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b3b4d2dbedc99fe843fd3dedb02f086f-Paper.pdf},
 publisher = {MIT Press},
 title = {Direct value-approximation for factored MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/b3b4d2dbedc99fe843fd3dedb02f086f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_b5f1e8fb,
 abstract = {Theories of cue combination suggest the possibility of constructing visual stimuli that evoke different patterns of neural activity in areas of the brain, but that cannot be distinguished by any behavioral measure of perception. Such stimuli, if they exist, would be interesting for two reasons. First, one could know that none of the differences between the stimuli survive past the computations used to build the percepts. Second, it can be difficult to distinguish stimulus-driven components of measured neural activity from top-down components (such as those due to the interestingness of the stimuli). Changing the stimulus without changing the percept could be exploited to measure the stimulus-driven activity. Here we describe stimuli in which vertical and horizontal disparities trade during the construction of percepts of slanted surfaces, yielding stimulus equivalence classes. Equivalence class membership changed after a change of vergence eye posture alone, without changes to the retinal images. A formal correspondence can be drawn between these perceptual metamers and more familiar sensory metamers such as color metamers.},
 author = {Backus, B.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b5f1e8fb36cd7fbeb7988e8639ac79e9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b5f1e8fb36cd7fbeb7988e8639ac79e9-Metadata.json},
 openalex = {W2104895121},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b5f1e8fb36cd7fbeb7988e8639ac79e9-Paper.pdf},
 publisher = {MIT Press},
 title = {Perceptual Metamers in Stereoscopic Vision},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/b5f1e8fb36cd7fbeb7988e8639ac79e9-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_b8b4b727,
 abstract = {We study the dynamics of a Hebbian ICA algorithm extracting a single non-Gaussian component from a high-dimensional Gaussian background. For both on-line and batch learning we find that a surprisingly large number of examples are required to avoid trapping in a sub-optimal state close to the initial conditions. To extract a skewed signal at least O(N2) examples are required for N-dimensional data and O(N3) examples are required to extract a symmetrical signal with non-zero kurtosis.},
 author = {Rattray, Magnus and Basalyga, Gleb},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b8b4b727d6f5d1b61fff7be687f7970f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b8b4b727d6f5d1b61fff7be687f7970f-Metadata.json},
 openalex = {W2142240647},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/b8b4b727d6f5d1b61fff7be687f7970f-Paper.pdf},
 publisher = {MIT Press},
 title = {Scaling Laws and Local Minima in Hebbian ICA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/b8b4b727d6f5d1b61fff7be687f7970f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_bd5af7cd,
 abstract = {Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.},
 author = {Kivinen, Jyrki and Smola, Alex and Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/bd5af7cd922fd2603be4ee3dc43b0b77-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/bd5af7cd922fd2603be4ee3dc43b0b77-Metadata.json},
 openalex = {W2150621701},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/bd5af7cd922fd2603be4ee3dc43b0b77-Paper.pdf},
 publisher = {MIT Press},
 title = {Online Learning with Kernels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/bd5af7cd922fd2603be4ee3dc43b0b77-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_bdc4626a,
 author = {Kepecs, \'{A}d\'{a}m and Raghavachari, S.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/bdc4626aa1d1df8e14d80d345b2a442d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/bdc4626aa1d1df8e14d80d345b2a442d-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/bdc4626aa1d1df8e14d80d345b2a442d-Paper.pdf},
 publisher = {MIT Press},
 title = {3 state neurons for contextual processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/bdc4626aa1d1df8e14d80d345b2a442d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_bf424cb7,
 abstract = {We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.},
 author = {Collins, Michael and Duffy, Nigel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/bf424cb7b0dea050a42b9739eb261a3a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/bf424cb7b0dea050a42b9739eb261a3a-Metadata.json},
 openalex = {W2127713198},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf},
 publisher = {MIT Press},
 title = {Convolution Kernels for Natural Language},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/bf424cb7b0dea050a42b9739eb261a3a-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c215b446,
 abstract = {We give an unified convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-Square-Boost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes l1-norm regularized cost functions leading to a clean and general way to regularize ensemble learning.},
 author = {R\"{a}tsch, Gunnar and Mika, Sebastian and Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c215b446bcdf956d848a8419c1b5a920-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c215b446bcdf956d848a8419c1b5a920-Metadata.json},
 openalex = {W2163308697},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c215b446bcdf956d848a8419c1b5a920-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Convergence of Leveraging},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c215b446bcdf956d848a8419c1b5a920-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c3395dd4,
 abstract = {In this paper we explore two quantitative approaches to the modelling of counterfactual reasoning – a linear and a noisy-OR model – based on information contained in conceptual dependency networks. Empirical data is acquired in a study and the fit of the models compared to it. We conclude by considering the appropriateness of non-parametric approaches to counterfactual reasoning, and examining the prospects for other parametric approaches in the future.},
 author = {Yarlett, Daniel and Ramscar, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c3395dd46c34fa7fd8d729d8cf88b7a8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c3395dd46c34fa7fd8d729d8cf88b7a8-Metadata.json},
 openalex = {W2167638052},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c3395dd46c34fa7fd8d729d8cf88b7a8-Paper.pdf},
 publisher = {MIT Press},
 title = {A Quantitative Model of Counterfactual Reasoning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c3535feb,
 abstract = {A model of auditory grouping is described in which auditory attention plays a key role. The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. The model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies. In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone.},
 author = {Wrigley, Stuart and Brown, Guy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c3535febaff29fcb7c0d20cbe94391c7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c3535febaff29fcb7c0d20cbe94391c7-Metadata.json},
 openalex = {W2136271792},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c3535febaff29fcb7c0d20cbe94391c7-Paper.pdf},
 publisher = {MIT Press},
 title = {A Neural Oscillator Model of Auditory Selective Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c3535febaff29fcb7c0d20cbe94391c7-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c4de8ced,
 abstract = {Recent biological experimental findings have shown that synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes. This determines whether long-term potentiation (LTP) or long-term depression (LTD) is induced. This synaptic plasticity has been called temporally asymmetric Hebbian plasticity (TAH). Many authors have numerically demonstrated that neural networks are capable of storing spatiotemporal patterns. However, the mathematical mechanism of the storage of spatiotemporal patterns is still unknown, and the effect of LTD is particularly unknown. In this article, we employ a simple neural network model and show that interference between LTP and LTD disappears in a sparse coding scheme. On the other hand, the covariance learning rule is known to be indispensable for the storage of sparse patterns. We also show that TAH has the same qualitative effect as the covariance rule when spatiotemporal patterns are embedded in the network.},
 author = {Matsumoto, N. and Okada, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c4de8ced6214345614d33fb0b16a8acd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c4de8ced6214345614d33fb0b16a8acd-Metadata.json},
 openalex = {W2113930652},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c4de8ced6214345614d33fb0b16a8acd-Paper.pdf},
 publisher = {MIT Press},
 title = {Self-Regulation Mechanism of Temporally Asymmetric Hebbian Plasticity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c4de8ced6214345614d33fb0b16a8acd-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c5866e93,
 abstract = {We describe a new algorithm for computing a Nash equilibrium in graphical games, a compact representation for multi-agent systems that we introduced in previous work. The algorithm is the first to compute equilibria both efficiently and exactly for a non-trivial class of graphical games.},
 author = {Littman, Michael and Kearns, Michael and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c5866e93cab1776890fe343c9e7063fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c5866e93cab1776890fe343c9e7063fb-Metadata.json},
 openalex = {W2160150462},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c5866e93cab1776890fe343c9e7063fb-Paper.pdf},
 publisher = {MIT Press},
 title = {An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c5866e93cab1776890fe343c9e7063fb-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c5a4e7e6,
 abstract = {A theory of categorization is presented in which knowledge of causal relationships between category features is represented as a Bayesian network. Referred to as causal-model theory, this theory predicts that objects are classified as category members to the extent they are likely to have been produced by a categorys causal model. On this view, people have models of the world that lead them to expect a certain distribution of features in category members (e.g.. correlations between feature pairs that are directly connected by causal relationships), and consider exemplars good category members when they manifest those expectations. These expectations include sensitivity to higher-order feature interactions that emerge from the asymmetries inherent in causal relationships.},
 author = {Rehder, Bob},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c5a4e7e6882845ea7bb4d9462868219b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c5a4e7e6882845ea7bb4d9462868219b-Metadata.json},
 openalex = {W2165626593},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c5a4e7e6882845ea7bb4d9462868219b-Paper.pdf},
 publisher = {MIT Press},
 title = {Causal Categorization with Bayes Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c5a4e7e6882845ea7bb4d9462868219b-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c5b2cebf,
 author = {Fox, Dieter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c5b2cebf15b205503560c4e8e6d1ea78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c5b2cebf15b205503560c4e8e6d1ea78-Metadata.json},
 openalex = {W4233290410},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c5b2cebf15b205503560c4e8e6d1ea78-Paper.pdf},
 publisher = {MIT Press},
 title = {KLD-Sampling: Adaptive Particle Filters},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c5b2cebf15b205503560c4e8e6d1ea78-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c6335734,
 abstract = {This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. The distinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences.},
 author = {Song, Yang and Goncalves, Luis and Perona, Pietro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c6335734dbc0b1ded766421cfc611750-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c6335734dbc0b1ded766421cfc611750-Metadata.json},
 openalex = {W2155680873},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c6335734dbc0b1ded766421cfc611750-Paper.pdf},
 publisher = {MIT Press},
 title = {Unsupervised Learning of Human Motion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c6335734dbc0b1ded766421cfc611750-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c8758b51,
 abstract = {We present a model of the firing of place and head-direction cells in rat hippocampus. The model can predict the response of individual cells and populations to parametric manipulations of both geometric (e.g. O'Keefe & Burgess, 1996) and orientational (Fenton et al., 2000a) cues, extending a previous geometric model (Hartley et al., 2000). It provides a functional description of how these cells' spatial responses are derived from the rat's environment and makes easily testable quantitative predictions. Consideration of the phenomenon of remapping (Muller & Kubie, 1987; Bostock et al., 1991) indicates that the model may also be consistent with non-parametric changes in firing, and provides constraints for its future development.},
 author = {Burgess, Neil and Hartley, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c8758b517083196f05ac29810b924aca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c8758b517083196f05ac29810b924aca-Metadata.json},
 openalex = {W2143314814},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c8758b517083196f05ac29810b924aca-Paper.pdf},
 publisher = {MIT Press},
 title = {Orientational and Geometric Determinants of Place and Head-direction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c8758b517083196f05ac29810b924aca-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c902b497,
 abstract = {We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant.},
 author = {Seeger, Matthias},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c902b497eb972281fb5b4e206db38ee6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c902b497eb972281fb5b4e206db38ee6-Metadata.json},
 openalex = {W2123152930},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c902b497eb972281fb5b4e206db38ee6-Paper.pdf},
 publisher = {MIT Press},
 title = {Covariance Kernels from Bayesian Generative Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c902b497eb972281fb5b4e206db38ee6-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_c92a1032,
 abstract = {The Temporal Coding Hypothesis of Miller and colleagues [7] suggests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a constrained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order conditioning experiments of Miller et al. [1, 2, 3], which other models are unable to explain.},
 author = {Courville, Aaron C and Touretzky, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c92a10324374fac681719d63979d00fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c92a10324374fac681719d63979d00fe-Metadata.json},
 openalex = {W2121857208},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/c92a10324374fac681719d63979d00fe-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling Temporal Structure in Classical Conditioning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/c92a10324374fac681719d63979d00fe-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_cbef4632,
 abstract = {We present new simulation results, in which a computational model of interacting visual neurons simultaneously predicts the modulation of spatial vision thresholds by focal visual attention, for five dual-task human psychophysics experiments. This new study complements our previous findings that attention activates a winner-take-all among early visual neurons within one cortical hypercolumn. This competition hypothesis assumed that attention equally affects all neurons, and yielded two single-unit predictions: an increase in gain and a sharpening of tuning with attention. While both effects have been separately observed in electrophysiology, no single-unit study has yet shown them simultaneously. Hence, we here explore whether our model could still predict our data if attention might only modulate neuronal gain, but do so non-uniformly across neurons and tasks. Specifically, we investigate whether modulating the gain of only the neurons that are loudest, best-tuned, or most informative about the stimulus, or of all neurons equally but in a task-dependent manner, may account for the data. We find that none of these hypotheses yields predictions as plausible as the intensified hypothesis, hence providing additional support for our original findings.},
 author = {Itti, Laurent and Braun, Jochen and Koch, Christof},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cbef46321026d8404bc3216d4774c8a9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cbef46321026d8404bc3216d4774c8a9-Metadata.json},
 openalex = {W2136509425},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cbef46321026d8404bc3216d4774c8a9-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling the Modulatory Effect of Attention on Human Spatial Vision},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/cbef46321026d8404bc3216d4774c8a9-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_cd4bb35c,
 abstract = {The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems. We study an efficient method based on linear programming for approximating solutions to such problems. The approach fits a linear combination of pre-selected basis functions to the dynamic programming cost-to-go function. We develop bounds on the approximation error and present experimental results in the domain of queueing network control, providing empirical support for the methodology.},
 author = {Farias, Daniela and Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cd4bb35c75ba84b4f39e547b1416fd35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cd4bb35c75ba84b4f39e547b1416fd35-Metadata.json},
 openalex = {W2103263079},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cd4bb35c75ba84b4f39e547b1416fd35-Paper.pdf},
 publisher = {MIT Press},
 title = {Approximate Dynamic Programming via Linear Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/cd4bb35c75ba84b4f39e547b1416fd35-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_cee8d6b7,
 abstract = {Recently, Jaakkola and Haussler (1999) proposed a method for constructing kernel functions from probabilistic models. Their so-called Fisher kernel has been combined with discriminative classifiers such as support vector machines and applied successfully in, for example, DNA and protein analysis. Whereas the Fisher kernel is calculated from the marginal log-likelihood, we propose the TOP kernel derived; from tangent vectors of posterior log-odds. Furthermore, we develop a theoretical framework on feature extractors from probabilistic models and use it for analyzing the TOP kernel. In experiments, our new discriminative TOP kernel compares favorably to the Fisher kernel.},
 author = {Tsuda, Koji and Kawanabe, Motoaki and R\"{a}tsch, Gunnar and Sonnenburg, S\"{o}ren and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cee8d6b7ce52554fd70354e37bbf44a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cee8d6b7ce52554fd70354e37bbf44a2-Metadata.json},
 openalex = {W2106868411},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cee8d6b7ce52554fd70354e37bbf44a2-Paper.pdf},
 publisher = {MIT Press},
 title = {A New Discriminative Kernel from Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/cee8d6b7ce52554fd70354e37bbf44a2-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_cf2226dd,
 abstract = {In kernel based learning the data is mapped to a kernel feature space of a dimension that corresponds to the number of training data points. In practice, however, the data forms a smaller submanifold in feature space, a fact that has been used e.g. by reduced set techniques for SVMs. We propose a new mathematical construction that permits to adapt to the intrinsic dimension and to find an orthonormal basis of this submanifold. In doing so, computations get much simpler and more important our theoretical framework allows to derive elegant kernelized blind source separation (BSS) algorithms for arbitrary invertible nonlinear mixings. Experiments demonstrate the good performance and high computational efficiency of our kTDSEP algorithm for the problem of nonlinear BSS.},
 author = {Harmeling, Stefan and Ziehe, Andreas and Kawanabe, Motoaki and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cf2226ddd41b1a2d0ae51dab54d32c36-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cf2226ddd41b1a2d0ae51dab54d32c36-Metadata.json},
 openalex = {W2102596092},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/cf2226ddd41b1a2d0ae51dab54d32c36-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernel Feature Spaces and Nonlinear Blind Souce Separation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/cf2226ddd41b1a2d0ae51dab54d32c36-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d0fb963f,
 abstract = {In this paper we will show that a restricted class of constrained minimum divergence problems, named generalized inference problems, can be solved by approximating the KL divergence with a Bethe free energy. The algorithm we derive is closely related to both loopy belief propagation and iterative scaling. This unified propagation and scaling algorithm reduces to a convergent alternative to loopy belief propagation when no constraints are present. Experiments show the viability of our algorithm.},
 author = {Teh, Yee and Welling, Max},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d0fb963ff976f9c37fc81fe03c21ea7b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d0fb963ff976f9c37fc81fe03c21ea7b-Metadata.json},
 openalex = {W2168205753},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d0fb963ff976f9c37fc81fe03c21ea7b-Paper.pdf},
 publisher = {MIT Press},
 title = {The Unified Propagation and Scaling Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d0fb963ff976f9c37fc81fe03c21ea7b-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d198bd73,
 abstract = {Observer translation relative to the world creates image flow that expands from the observer's direction of translation (heading) from which the observer can recover heading direction. Yet, the image flow is often more complex, depending on rotation of the eye, scene layout and translation velocity. A number of models [1-4] have been proposed on how the human visual system extracts heading from flow in a neurophysiologically plausible way. These models represent heading by a set of neurons that respond to large image flow patterns and receive input from motion sensed at different image locations. We analysed these models to determine the exact receptive field of these heading detectors. We find most models predict that, contrary to widespread believe, the contributing motion sensors have a preferred motion directed circularly rather than radially around the detector's preferred heading. Moreover, the results suggest to look for more refined structure within the circular flow, such as bi-circularity or local motion-opponency.},
 author = {Beintema, J. and Lappe, M. and Berg, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d198bd736a97e7cecfdf8f4f2027ef80-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d198bd736a97e7cecfdf8f4f2027ef80-Metadata.json},
 openalex = {W2106499978},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d198bd736a97e7cecfdf8f4f2027ef80-Paper.pdf},
 publisher = {MIT Press},
 title = {Receptive Field Structure of Flow Detectors for Heading Perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d198bd736a97e7cecfdf8f4f2027ef80-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d3a7f48c,
 abstract = {Visual inspection of neurons suggests that dendritic orientation may be determined both by internal constraints (e.g. membrane tension) and by external vector fields (e.g. neurotrophic gradients). For example, basal dendrites of pyramidal cells appear nicely fan-out. This regular orientation is hard to justify completel y with a general tendency to grow straight, given the zigzags observed experimentally. Instead, dendrites could (A) favor a fixed (external) direction, or (B) repel from their own soma. To investigate these possibilities quantitatively, reconstructed hippocampal cells were subjected to Bayesian analysis. The statistical model combined linearly fact ors A and B, as well as the tendency to grow straight. For all morphological classes, B was found to be significantly positive and consistently greater than A. In addition, when dendrites were artificially re-oriented according to this model, the resulting structures closely resembled real morphologies. These results suggest that somatodendritic repulsion may play a role in determining dendritic orientation. Since hippocampal cells are very densely packed and their dendritic trees highly overlap, the repulsion must be cell-specific. We discuss possible mechanisms underlying such specificity.},
 author = {Ascoli, Giorgio and Samsonovich, Alexei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d3a7f48c12e697d50c8a7ae7684644ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d3a7f48c12e697d50c8a7ae7684644ef-Metadata.json},
 openalex = {W2167151745},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d3a7f48c12e697d50c8a7ae7684644ef-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Morphometry of Hippocampal Cells Suggests Same-Cell Somatodendritic Repulsion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d3a7f48c12e697d50c8a7ae7684644ef-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d46e1fcf,
 abstract = {We study properties of popular near–uniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam–style phase space argument expands the priors into their infinite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions.},
 author = {Nemenman, Ilya and Shafee, F. and Bialek, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d46e1fcf4c07ce4a69ee07e4134bcef1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d46e1fcf4c07ce4a69ee07e4134bcef1-Metadata.json},
 openalex = {W2097173970},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d46e1fcf4c07ce4a69ee07e4134bcef1-Paper.pdf},
 publisher = {MIT Press},
 title = {Entropy and Inference, Revisited},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d46e1fcf4c07ce4a69ee07e4134bcef1-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d47268e9,
 abstract = {It is well known that under noisy conditions we can hear speech much more clearly when we read the speaker's lips. This suggests the utility of audio-visual information for the task of speech enhancement. We propose a method to exploit audio-visual cues to enable speech separation under non-stationary noise and with a single microphone. We revise and extend HMM-based speech enhancement techniques, in which signal and noise models are factorially combined, to incorporate visual lip information and employ novel signal HMMs in which the dynamics of narrow-band and wide band components are factorial. We avoid the combinatorial explosion in the factorial model by using a simple approximate inference technique to quickly estimate the clean signals in a mixture. We present a preliminary evaluation of this approach using a small-vocabulary audio-visual database, showing promising improvements in machine intelligibility for speech enhanced using audio and visual information.},
 author = {Hershey, John and Casey, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d47268e9db2e9aa3827bba3afb7ff94a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d47268e9db2e9aa3827bba3afb7ff94a-Metadata.json},
 openalex = {W2099128937},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d47268e9db2e9aa3827bba3afb7ff94a-Paper.pdf},
 publisher = {MIT Press},
 title = {Audio-Visual Sound Separation Via Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d47268e9db2e9aa3827bba3afb7ff94a-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d5c18698,
 abstract = {The popular K-means clustering partitions a data set by minimizing a sum-of-squares cost function. A coordinate descend method is then used to find local minima. In this paper we show that the minimization can be reformulated as a trace maximization problem associated with the Gram matrix of the data vectors. Furthermore, we show that a relaxed version of the trace maximization problem possesses global optimal solutions which can be obtained by computing a partial eigendecomposition of the Gram matrix, and the cluster assignment for each data vectors can be found by computing a pivoted QR decomposition of the eigenvector matrix. As a by-product we also derive a lower bound for the minimum of the sum-of-squares cost function.},
 author = {Zha, Hongyuan and He, Xiaofeng and Ding, Chris and Gu, Ming and Simon, Horst},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d5c186983b52c4551ee00f72316c6eaa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d5c186983b52c4551ee00f72316c6eaa-Metadata.json},
 openalex = {W2139850885},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d5c186983b52c4551ee00f72316c6eaa-Paper.pdf},
 publisher = {MIT Press},
 title = {Spectral Relaxation for K-means Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d5c186983b52c4551ee00f72316c6eaa-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d68a1827,
 abstract = {Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations.},
 author = {Sollich, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d68a18275455ae3eaa2c291eebb46e6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d68a18275455ae3eaa2c291eebb46e6d-Metadata.json},
 openalex = {W2117010397},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d68a18275455ae3eaa2c291eebb46e6d-Paper.pdf},
 publisher = {MIT Press},
 title = {Gaussian Process Regression with Mismatched Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d68a18275455ae3eaa2c291eebb46e6d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d77f0076,
 abstract = {We give results about the learnability and required complexity of logical formulae to solve classification problems. These results are obtained by linking propositional logic with kernel machines. In particular we show that decision trees and disjunctive normal forms (DNF) can be represented by the help of a special kernel, linking regularized risk to separation margin. Subsequently we derive a number of lower bounds on the required complexity of logic formulae using properties of algorithms for generation of linear estimators, such as perceptron and maximal perceptron learning.},
 author = {Kowalczyk, Adam and Smola, Alex and Williamson, Robert C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d77f00766fd3be3f2189c843a6af3fb2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d77f00766fd3be3f2189c843a6af3fb2-Metadata.json},
 openalex = {W2118734388},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d77f00766fd3be3f2189c843a6af3fb2-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernel Machines and Boolean Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d77f00766fd3be3f2189c843a6af3fb2-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d79c6256,
 abstract = {It is desirable that a complex decision-making problem in an uncertain world be adequately modeled by a Markov Decision Process (MDP) whose structural representation is adaptively designed by a parsimonious resources allocation process. Resources include time and cost of exploration, amount of memory and computational time allowed for the policy or value function representation. Concerned about making the best use of the available resources, we address the problem of efficiently estimating where adding extra resources is highly needed in order to improve the expected performance of the resulting policy. Possible application in reinforcement learning (RL), when real-world exploration is highly costly, concerns the detection of those areas of the state-space that need primarily to be explored in order to improve the policy. Another application concerns approximation of continuous state-space stochastic control problems using adaptive discretization techniques for which highly efficient grid points allocation is mandatory to survive high dimensionality. Maybe surprisingly these two problems can be formulated under a common framework: for a given resource allocation, which defines a belief state over possible MDPs, find where adding new resources (thus decreasing the uncertainty of some parameters -transition probabilities or rewards) will most likely increase the expected performance of the new policy. To do so, we use sampling techniques for estimating the contribution of each parameter's probability distribution function (pdf) to the expected loss of using an approximate policy (such as the optimal policy of the most probable MDP) instead of the true (but unknown) policy.},
 author = {Munos, R\'{e}mi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d79c6256b9bdac53a55801a066b70da3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d79c6256b9bdac53a55801a066b70da3-Metadata.json},
 openalex = {W2126282678},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d79c6256b9bdac53a55801a066b70da3-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient Resources Allocation for Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d79c6256b9bdac53a55801a066b70da3-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d7a84628,
 abstract = {The mystery of propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy network (BN) of turbo codes makes it difficult to obtain the true belief by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.},
 author = {Ikeda, Shiro and Tanaka, Toshiyuki and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d7a84628c025d30f7b2c52c958767e76-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d7a84628c025d30f7b2c52c958767e76-Metadata.json},
 openalex = {W2152932893},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d7a84628c025d30f7b2c52c958767e76-Paper.pdf},
 publisher = {MIT Press},
 title = {Information Geometrical Framework for Analyzing Belief Propagation Decoder},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d7a84628c025d30f7b2c52c958767e76-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d8330f85,
 abstract = {An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood.},
 author = {Smith, N. and Gales, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d8330f857a17c53d217014ee776bfd50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d8330f857a17c53d217014ee776bfd50-Metadata.json},
 openalex = {W2140409019},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf},
 publisher = {MIT Press},
 title = {Speech Recognition using SVMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_d860edd1,
 abstract = {The marriage of Renyi entropy with Parzen density estimation has been shown to be a viable tool in learning discriminative feature transforms. However, it suffers from computational complexity proportional to the square of the number of samples in the training data. This sets a practical limit to using large databases. We suggest immediate divorce of the two methods and remarriage of Renyi entropy with a semi-parametric density estimation method, such as a Gaussian Mixture Models (GMM). This allows all of the computation to take place in the low dimensional target space, and it reduces computational complexity proportional to square of the number of components in the mixtures. Furthermore, a convenient extension to Hidden Markov Models as commonly used in speech recognition becomes possible.},
 author = {Torkkola, Kari},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d860edd1dd83b36f02ce52bde626c653-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d860edd1dd83b36f02ce52bde626c653-Metadata.json},
 openalex = {W2160773882},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/d860edd1dd83b36f02ce52bde626c653-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/d860edd1dd83b36f02ce52bde626c653-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_dc513ea4,
 abstract = {We report a result of perturbation analysis on decoding error of the belief propagation decoder for Gallager codes. The analysis is based on information geometry, and it shows that the principal term of decoding error at equilibrium comes from the m-embedding curvature of the log-linear submanifold spanned by the estimated pseudoposteriors, one for the full marginal, and K for partial posteriors, each of which takes a single check into account, where K is the number of checks in the Gallager code. It is then shown that the principal error term vanishes when the parity-check matrix of the code is so sparse that there are no two columns with overlap greater than 1.},
 author = {Tanaka, Toshiyuki and Ikeda, Shiro and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/dc513ea4fbdaa7a14786ffdebc4ef64e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/dc513ea4fbdaa7a14786ffdebc4ef64e-Metadata.json},
 openalex = {W2120492328},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/dc513ea4fbdaa7a14786ffdebc4ef64e-Paper.pdf},
 publisher = {MIT Press},
 title = {Information-Geometrical Significance of Sparsity in Gallager Codes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/dc513ea4fbdaa7a14786ffdebc4ef64e-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_dd055f53,
 abstract = {In this paper we introduce a new sparseness inducing prior which does not involve any (hyper) parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classification. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparseness-controlling hyper-parameters.},
 author = {Figueiredo, M\'{a}rio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/dd055f53a45702fe05e449c30ac80df9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/dd055f53a45702fe05e449c30ac80df9-Metadata.json},
 openalex = {W2150962169},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/dd055f53a45702fe05e449c30ac80df9-Paper.pdf},
 publisher = {MIT Press},
 title = {Adaptive Sparseness Using Jeffreys' Prior},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/dd055f53a45702fe05e449c30ac80df9-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_de03beff,
 abstract = {This paper describes a clustering algorithm for vector quantizers using a stochastic association model. It offers a new simple and powerful soft-max adaptation rule. The adaptation process is the same as the on-line K-means clustering method except for adding random fluctuation in the distortion error evaluation process. Simulation results demonstrate that the new algorithm can achieve efficient adaptation as high as the neural gas algorithm, which is reported as one of the most efficient clustering methods. It is a key to add uncorrelated random fluctuation in the similarity evaluation process for each reference vector. For hardware implementation of this process, we propose a nanostructure, whose operation is described by a single-electron circuit. It positively uses fluctuation in quantum mechanical tunneling processes.},
 author = {Morie, Takashi and Matsuura, Tomohiro and Nagata, Makoto and Iwata, Atsushi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/de03beffeed9da5f3639a621bcab5dd4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/de03beffeed9da5f3639a621bcab5dd4-Metadata.json},
 openalex = {W2099611211},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/de03beffeed9da5f3639a621bcab5dd4-Paper.pdf},
 publisher = {MIT Press},
 title = {An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/de03beffeed9da5f3639a621bcab5dd4-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_de739988,
 abstract = {It has been known that people, after being exposed to sentences generated by an artificial grammar, acquire implicit grammatical knowledge and are able to transfer the knowledge to inputs that are generated by a modified grammar. We show that a second order recurrent neural network is able to transfer grammatical knowledge from one language (generated by a Finite State Machine) to another language which differ both in vocabularies and syntax. Representation of the grammatical knowledge in the network is analyzed using linear discriminant analysis.},
 author = {Negishi, Michiro and Hanson, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/de73998802680548b916f1947ffbad76-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/de73998802680548b916f1947ffbad76-Metadata.json},
 openalex = {W2138158293},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/de73998802680548b916f1947ffbad76-Paper.pdf},
 publisher = {MIT Press},
 title = {Grammar Transfer in a Second Order Recurrent Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/de73998802680548b916f1947ffbad76-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_deb54ffb,
 abstract = {Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. In factor the observations are modeled as a linear combination of normally distributed hidden variables. We describe a nonlinear generalization of factor called analysis, that models the observed variables as a linear combination of products of normally distributed hidden variables. Just as factor analysis can be viewed as unsupervised linear regression on unobserved, normally distributed hidden variables, product analysis can be viewed as unsupervised linear regression on products of unobserved, normally distributed hidden variables. The mapping between the data and the hidden space is nonlinear, so we use an approximate variational technique for inference and learning. Since product analysis is a generalization of factor product analysis always finds a higher data likelihood than factor analysis. We give results on pattern recognition and illumination-invariant image clustering.},
 author = {Frey, Brendan J and Kannan, Anitha and Jojic, Nebojsa},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/deb54ffb41e085fd7f69a75b6359c989-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/deb54ffb41e085fd7f69a75b6359c989-Metadata.json},
 openalex = {W2132812757},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/deb54ffb41e085fd7f69a75b6359c989-Paper.pdf},
 publisher = {MIT Press},
 title = {Product Analysis: Learning to Model Observations as Products of Hidden Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/deb54ffb41e085fd7f69a75b6359c989-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_df4fe8a8,
 abstract = {Motivated by our recent work on rooted tree matching, in this paper we provide a solution to the problem of matching two free (i.e., unrooted) trees by constructing an association graph whose maximal cliques are in one-to-one correspondence with maximal common subtrees. We then solve the problem using simple replicator dynamics from evolutionary game theory. Experiments on hundreds of uniformly random trees are presented. The results are impressive: despite the inherent inability of these simple dynamics to escape from local optima, they always returned a globally optimal solution.},
 author = {Pelillo, Marcello},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/df4fe8a8bcd5c95cdb640aa9793bb32b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/df4fe8a8bcd5c95cdb640aa9793bb32b-Metadata.json},
 openalex = {W2134706826},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/df4fe8a8bcd5c95cdb640aa9793bb32b-Paper.pdf},
 publisher = {MIT Press},
 title = {Matching Free Trees with Replicator Equations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/df4fe8a8bcd5c95cdb640aa9793bb32b-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_e0a20953,
 abstract = {We investigate a learning algorithm for the classification of nonnegative data by mixture models. Multiplicative update rules are derived that directly optimize the performance of these models as classifiers. The update rules have a simple closed form and an intuitive appeal. Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm—its guarantee of monotonic improvement, and its absence of tuning parameters—with the added advantage of optimizing a discriminative objective function. The algorithm reduces as a special case to the method of generalized iterative scaling for log-linear models. The learning rate of the algorithm is controlled by the sparseness of the training data. We use the method of nonnegative matrix factorization (NMF) to discover sparse distributed representations of the data. This form of feature selection greatly accelerates learning and makes the algorithm practical on large problems. Experiments show that discriminatively trained mixture models lead to much better classification than comparably sized models trained by EM.},
 author = {Saul, Lawrence and Lee, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e0a209539d1e74ab9fe46b9e01a19a97-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e0a209539d1e74ab9fe46b9e01a19a97-Metadata.json},
 openalex = {W2125810619},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e0a209539d1e74ab9fe46b9e01a19a97-Paper.pdf},
 publisher = {MIT Press},
 title = {Multiplicative Updates for Classification by Mixture Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/e0a209539d1e74ab9fe46b9e01a19a97-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_e3408432,
 abstract = {We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite— consider, for example, symbols being possible words appearing in English text.},
 author = {Beal, Matthew and Ghahramani, Zoubin and Rasmussen, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e3408432c1a48a52fb6c74d926b38886-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e3408432c1a48a52fb6c74d926b38886-Metadata.json},
 openalex = {W2158190429},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e3408432c1a48a52fb6c74d926b38886-Paper.pdf},
 publisher = {MIT Press},
 title = {The Infinite Hidden Markov Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/e3408432c1a48a52fb6c74d926b38886-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_e4dd5528,
 abstract = {We present a new multiple-instance (MI) learning technique (EM-DD) that combines EM with the diverse density (DD) algorithm. EM-DD is a general-purpose MI algorithm that can be applied with boolean or real-value labels and makes real-value predictions. On the boolean Musk benchmarks, the EM-DD algorithm without any tuning significantly outperforms all previous algorithms. EM-DD is relatively insensitive to the number of relevant attributes in the data set and scales up well to large bag sizes. Furthermore, EM-DD provides a new framework for MI learning, in which the MI problem is converted to a single-instance setting by using EM to estimate the instance responsible for the label of the bag.},
 author = {Zhang, Qi and Goldman, Sally},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e4dd5528f7596dcdf871aa55cfccc53c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e4dd5528f7596dcdf871aa55cfccc53c-Metadata.json},
 openalex = {W2163474322},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf},
 publisher = {MIT Press},
 title = {EM-DD: An Improved Multiple-Instance Learning Technique},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/e4dd5528f7596dcdf871aa55cfccc53c-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_e7e23670,
 author = {Wong, K. and Li, F.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e7e23670481ac78b3c4122a99ba60573-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e7e23670481ac78b3c4122a99ba60573-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/e7e23670481ac78b3c4122a99ba60573-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Parameter Estimation Using Green\textquotesingle s Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/e7e23670481ac78b3c4122a99ba60573-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_ea5a486c,
 abstract = {We present an information-geometric measure to systematically investigate neuronal firing patterns, taking account not only of the second-order but also of higher-order interactions. We begin with the case of two neurons for illustration and show how to test whether or not any pairwise correlation in one period is significantly different from that in the other period. In order to test such a hypothesis of different firing rates, the correlation term needs to be singled out 'orthogonally' to the firing rates, where the null hypothesis might not be of independent firing. This method is also shown to directly associate neural firing with behavior via their mutual information, which is decomposed into two types of information, conveyed by mean firing rate and coincident firing, respectively. Then, we show that these results, using the 'orthogonal' decomposition, are naturally extended to the case of three neurons and n neurons in general.},
 author = {Nakahara, Hiroyuki and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ea5a486c712a91e48443cd802642223d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ea5a486c712a91e48443cd802642223d-Metadata.json},
 openalex = {W2141065054},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ea5a486c712a91e48443cd802642223d-Paper.pdf},
 publisher = {MIT Press},
 title = {Information-geometric decomposition in spike analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/ea5a486c712a91e48443cd802642223d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_ea6b2efb,
 abstract = {A nonlinear supervised learning model, the Specialized Mappings Architecture (SMA), is described and applied to the estimation of human body pose from monocular images. The SMA consists of several specialized forward mapping functions and an inverse mapping function. Each specialized function maps certain domains of the input space (image features) onto the output space (body pose parameters). The key algorithmic problems faced are those of learning the specialized domains and mapping functions in an optimal way, as well as performing inference given inputs and knowledge of the inverse function. Solutions to these problems employ the EM algorithm and alternating choices of conditional independence assumptions. Performance of the approach is evaluated with synthetic and real video sequences of human motion.},
 author = {Rosales, R\'{o}mer and Sclaroff, Stan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Metadata.json},
 openalex = {W2169433763},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ea6b2efbdd4255a9f1b3bbc6399b58f4-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Body Pose via Specialized Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/ea6b2efbdd4255a9f1b3bbc6399b58f4-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_ef41d488,
 abstract = {Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We aTgue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a 0.6µm CMOS process are given.},
 author = {Bofill, A. and Thompson, D. and Murray, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ef41d488755367316f04fc0e0e9dc9fc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ef41d488755367316f04fc0e0e9dc9fc-Metadata.json},
 openalex = {W2135826827},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ef41d488755367316f04fc0e0e9dc9fc-Paper.pdf},
 publisher = {MIT Press},
 title = {Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/ef41d488755367316f04fc0e0e9dc9fc-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_ef8446f3,
 abstract = {Probabilistic mixture models are used for a broad range of data analysis tasks such as clustering, classification, predictive modeling, etc. Due to their inherent probabilistic nature, mixture models can easily be combined with other probabilistic or non-probabilistic techniques thus forming more complex data analysis systems. In the case of online data (where there is a stream of data available) models can be constantly updated to reflect the most current distribution of the incoming data. However, in many business applications the models themselves represent a parsimonious summary of the data and therefore it is not desirable to change models frequently, much less with every new data point. In such a framework it becomes crucial to track the applicability of the mixture model and detect the point in time when the model fails to adequately represent the data. In this paper we formulate the problem of change detection and propose a principled solution. Empirical results over both synthetic and real-life data sets are presented.},
 author = {Cadez, Igor and Bradley, P. S.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ef8446f35513a8d6aa2308357a268a7e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ef8446f35513a8d6aa2308357a268a7e-Metadata.json},
 openalex = {W2109581903},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/ef8446f35513a8d6aa2308357a268a7e-Paper.pdf},
 publisher = {MIT Press},
 title = {Model Based Population Tracking and Automatic Detection of Distribution Changes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/ef8446f35513a8d6aa2308357a268a7e-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f106b7f9,
 abstract = {Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.},
 author = {Belkin, Mikhail and Niyogi, Partha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Metadata.json},
 openalex = {W2156718197},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf},
 publisher = {MIT Press},
 title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f106b7f99d2cb30c3db1c3cc0fde9ccb-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f15d337c,
 abstract = {Narayanan and Jurafsky (1998) proposed that human language comprehension can be modeled by treating human comprehenders as Bayesian reasoners, and modeling the comprehension process with Bayesian decision trees. In this paper we extend the Narayanan and Jurafsky model to make further predictions about reading time given the probability of difference parses or interpretations, and test the model against reading time data from a psycholinguistic experiment.},
 author = {Narayanan, S. and Jurafsky, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f15d337c70078947cfe1b5d6f0ed3f13-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f15d337c70078947cfe1b5d6f0ed3f13-Metadata.json},
 openalex = {W2163600564},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f15d337c70078947cfe1b5d6f0ed3f13-Paper.pdf},
 publisher = {MIT Press},
 title = {A Bayesian Model Predicts Human Parse Preference and Reading Times in Sentence Processing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f15d337c70078947cfe1b5d6f0ed3f13-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f1981e4b,
 abstract = {Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation fixed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille's algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development.},
 author = {Rangarajan, Anand and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f1981e4bd8a0d6d8462016d2fc6276b3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f1981e4bd8a0d6d8462016d2fc6276b3-Metadata.json},
 openalex = {W2169099927},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f1981e4bd8a0d6d8462016d2fc6276b3-Paper.pdf},
 publisher = {MIT Press},
 title = {MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f1981e4bd8a0d6d8462016d2fc6276b3-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f3e52c30,
 abstract = {We address two open theoretical questions in Policy Gradient Reinforcement Learning. The first concerns the efficacy of using function approximation to represent the state action value function, Q. Theory is presented showing that linear function approximation representations of Q can degrade the rate of convergence of performance gradient estimates by a factor of O(ML) relative to when no function approximation of Q is used, where M is the number of possible actions and L is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by 0(1 (1/M)), where M is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to significant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.},
 author = {Grudic, Gregory and Ungar, Lyle},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f3e52c300b822a8123e7ace55fe15c08-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f3e52c300b822a8123e7ace55fe15c08-Metadata.json},
 openalex = {W2161796206},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f3e52c300b822a8123e7ace55fe15c08-Paper.pdf},
 publisher = {MIT Press},
 title = {Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f3e52c300b822a8123e7ace55fe15c08-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f410588e,
 abstract = {Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data.},
 author = {Collins, Michael and Dasgupta, S. and Schapire, Robert E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Metadata.json},
 openalex = {W2135001774},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Paper.pdf},
 publisher = {MIT Press},
 title = {A Generalization of Principal Components Analysis to the Exponential Family},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f410588e48dc83f2822a880a68f78923-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f48c04ff,
 abstract = {Minimax probability machine (MPM) is an interesting discriminative classifier based on generative prior knowledge. It can directly estimate the probabilistic accuracy bound by minimizing the maximum probability of misclassification. The structural information of data is an effective way to represent prior knowledge, and has been found to be vital for designing classifiers in real-world problems. However, MPM only considers the prior probability distribution of each class with a given mean and covariance matrix, which does not efficiently exploit the structural information of data. In this paper, we use two finite mixture models to capture the structural information of the data from binary classification. For each subdistribution in a finite mixture model, only its mean and covariance matrix are assumed to be known. Based on the finite mixture models, we propose a structural MPM (SMPM). SMPM can be solved effectively by a sequence of the second-order cone programming problems. Moreover, we extend a linear model of SMPM to a nonlinear model by exploiting kernelization techniques. We also show that the SMPM can be interpreted as a large margin classifier and can be transformed to support vector machine and maxi-min margin machine under certain special conditions. Experimental results on both synthetic and real-world data sets demonstrate the effectiveness of SMPM.},
 author = {Lanckriet, Gert and Ghaoui, Laurent and Bhattacharyya, Chiranjib and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f48c04ffab49ff0e5d1176244fdfb65c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f48c04ffab49ff0e5d1176244fdfb65c-Metadata.json},
 openalex = {W2336484677},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f48c04ffab49ff0e5d1176244fdfb65c-Paper.pdf},
 publisher = {MIT Press},
 title = {Structural Minimax Probability Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f48c04ffab49ff0e5d1176244fdfb65c-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f6e794a7,
 abstract = {We develop an intuitive geometric framework for support vector regression (SVR). By examining when ∊-tubes exist, we show that SVR can be regarded as a classification problem in the dual space. Hard and soft ∊-tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by ∊. A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets. Maximizing the margin corresponds to shrinking the effective ∊-tube. In the proposed approach the effects of the choices of all parameters become clear geometrically.},
 author = {Bi, J. and Bennett, Kristin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f6e794a75c5d51de081dbefa224304f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f6e794a75c5d51de081dbefa224304f9-Metadata.json},
 openalex = {W2171798377},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f6e794a75c5d51de081dbefa224304f9-Paper.pdf},
 publisher = {MIT Press},
 title = {Duality, Geometry, and Support Vector Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f6e794a75c5d51de081dbefa224304f9-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f80bf055,
 abstract = {We consider a problem of blind source separation from a set of instantaneous linear mixtures, where the mixing matrix is unknown. It was discovered recently, that exploiting the sparsity of sources in an appropriate representation according to some signal dictionary, dramatically improves the quality of separation. In this work we use the property of multiscale transforms, such as wavelet or wavelet packets, to decompose signals into sets of local features with various degrees of sparsity. We use this intrinsic property for selecting the best (most sparse) subsets of features for further separation. The performance of the algorithm is verified on noise-free and noisy data. Experiments with simulated signals, musical sounds and images demonstrate significant improvement of separation quality over previously reported results.},
 author = {Zibulevsky, Michael and Kisilev, Pavel and Zeevi, Yehoshua and Pearlmutter, Barak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f80bf05527157a8c2a7bb63b22f49aaa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f80bf05527157a8c2a7bb63b22f49aaa-Metadata.json},
 openalex = {W2107998429},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f80bf05527157a8c2a7bb63b22f49aaa-Paper.pdf},
 publisher = {MIT Press},
 title = {Blind Source Separation via Multinode Sparse Representation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f80bf05527157a8c2a7bb63b22f49aaa-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f80ff32e,
 abstract = {When applying unsupervised learning techniques like ICA or temporal decorrelation, a key question is whether the discovered projections are reliable. In other words: can we give error bars or can we assess the quality of our separation? We use resampling methods to tackle these questions and show experimentally that our proposed variance estimations are strongly correlated to the separation error. We demonstrate that this reliability estimation can be used to choose the appropriate ICA-model, to enhance significantly the separation performance, and, most important, to mark the components that have a actual physical meaning. Application to 49-channel-data from an magnetoencephalography (MEG) experiment underlines the usefulness of our approach.},
 author = {Meinecke, Frank and Ziehe, Andreas and Kawanabe, Motoaki and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f80ff32e08a25270b5f252ce39522f72-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f80ff32e08a25270b5f252ce39522f72-Metadata.json},
 openalex = {W2171974804},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f80ff32e08a25270b5f252ce39522f72-Paper.pdf},
 publisher = {MIT Press},
 title = {Estimating the Reliability of ICA Projections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f80ff32e08a25270b5f252ce39522f72-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f8bf09f5,
 abstract = {We propose the following general method for scaling learning algorithms to arbitrarily large data sets. Consider the model Mn→ learned by the algorithm using ni examples in step i (n→ = (n1,..., nm)), and the model M∞ that would be learned using infinite examples. Upper-bound the loss L(Mn→,M,∞) between them as a function of n→, and then minimize the algorithm's time complexity ƒ(n→) subject to the constraint that L(M∞, Mn→) be at most e with probability at most δ. We apply this method to the EM algorithm for mixtures of Gaussians. Preliminary experiments on a series of large data sets provide evidence of the potential of this approach.},
 author = {Domingos, Pedro and Hulten, Geoff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f8bf09f5fceaea80e1f864a1b48938bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f8bf09f5fceaea80e1f864a1b48938bf-Metadata.json},
 openalex = {W2100188668},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f8bf09f5fceaea80e1f864a1b48938bf-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning from Infinite Data in Finite Time},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f8bf09f5fceaea80e1f864a1b48938bf-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_f8c0c968,
 abstract = {We describe the g-factor, which relates probability distributions on image features to distributions on the images themselves. The g-factor depends only on our choice of features and lattice quantization and is independent of the training image data. We illustrate the importance of the g-factor by analyzing how the parameters of Markov Random Field (i.e. Gibbs or log-linear) probability models of images are learned from data by maximum likelihood estimation. In particular, we study homogeneous MRF models which learn image distributions in terms of clique potentials corresponding to feature histogram statistics (cf. Minimax Entropy Learning (MEL) by Zhu, Wu and Mumford 1997 [11]). We first use our analysis of the g-factor to determine when the clique potentials decouple for different features. Second, we show that clique potentials can be computed analytically by approximating the g-factor. Third, we demonstrate a connection between this approximation and the Generalized Iterative Scaling algorithm (GIS), due to Darroch and Ratcliff 1972 [2], for calculating potentials. This connection enables us to use GIS to improve our multinomial approximation, using Bethe-Kikuchi[8] approximations to simplify the GIS procedure. We support our analysis by computer simulations.},
 author = {Coughlan, James and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f8c0c968632845cd133308b1a494967f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f8c0c968632845cd133308b1a494967f-Metadata.json},
 openalex = {W2167294229},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f8c0c968632845cd133308b1a494967f-Paper.pdf},
 publisher = {MIT Press},
 title = {The g Factor: Relating Distributions on Features to Distributions on Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/f8c0c968632845cd133308b1a494967f-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_fb2e2032,
 abstract = {The mutual information of two random variables i and j with joint probabilities t_ij is commonly used in learning Bayesian nets as well as in many other fields. The chances t_ij are usually estimated by the empirical sampling frequency n_ij/n leading to a point estimate I(n_ij/n) for the mutual information. To answer questions like "is I(n_ij/n) consistent with zero?" or "what is the probability that the true mutual information is much larger than the point estimate?" one has to go beyond the point estimate. In the Bayesian framework one can answer these questions by utilizing a (second order) prior distribution p(t) comprising prior information about t. From the prior p(t) one can compute the posterior p(t|n), from which the distribution p(I|n) of the mutual information can be calculated. We derive reliable and quickly computable approximations for p(I|n). We concentrate on the mean, variance, skewness, and kurtosis, and non-informative priors. For the mean we also give an exact expression. Numerical issues and the range of validity are discussed.},
 author = {Hutter, Marcus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/fb2e203234df6dee15934e448ee88971-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/fb2e203234df6dee15934e448ee88971-Metadata.json},
 openalex = {W2950213143},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/fb2e203234df6dee15934e448ee88971-Paper.pdf},
 publisher = {MIT Press},
 title = {Distribution of Mutual Information},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/fb2e203234df6dee15934e448ee88971-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_fb875828,
 abstract = {Multisensory response enhancement (MRE) is the augmentation of the response of a neuron to sensory input of one modality by simultaneous input from another modality. The maximum likelihood (ML) model presented here modifies the Bayesian model for MRE (Anastasio et al.) by incorporating a decision strategy to maximize the number of correct decisions. Thus the ML model can also deal with the important tasks of stimulus discrimination and identification in the presence of incongruent visual and auditory cues. It accounts for the inverse effectiveness observed in neurophysiological recording data, and it predicts a functional relation between uni- and bimodal levels of discriminability that is testable both in neurophysiological and behavioral experiments.},
 author = {Colonius, H. and Diederich, A.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/fb87582825f9d28a8d42c5e5e5e8b23d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/fb87582825f9d28a8d42c5e5e5e8b23d-Metadata.json},
 openalex = {W2143034606},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/fb87582825f9d28a8d42c5e5e5e8b23d-Paper.pdf},
 publisher = {MIT Press},
 title = {A Maximum-Likelihood Approach to Modeling Multisensory Enhancement},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/fb87582825f9d28a8d42c5e5e5e8b23d-Abstract.html},
 volume = {14},
 year = {2001}
}

@inproceedings{NIPS2001_fca0789e,
 abstract = {We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efficient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly influenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efficiently by merely observing a relatively small number of completely random trials.},
 author = {Lagoudakis, Michail G. and Parr, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2001/file/fca0789e7891cbc0583298a238316122-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2001/file/fca0789e7891cbc0583298a238316122-Metadata.json},
 openalex = {W2099833070},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2001/file/fca0789e7891cbc0583298a238316122-Paper.pdf},
 publisher = {MIT Press},
 title = {Model-Free Least-Squares Policy Iteration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/fca0789e7891cbc0583298a238316122-Abstract.html},
 volume = {14},
 year = {2001}
}
