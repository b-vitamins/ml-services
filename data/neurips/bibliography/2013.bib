@inproceedings{NIPS2013_01386bd6,
 abstract = {Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for stitching together sequentially imaged sets of neurons into one model by phrasing the problem as fitting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs.},
 author = {Turaga, Srini and Buesing, Lars and Packer, Adam M and Dalgleish, Henry and Pettit, Noah and Hausser, Michael and Macke, Jakob H},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/01386bd6d8e091c2ab4c7c7de644d37b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/01386bd6d8e091c2ab4c7c7de644d37b-Metadata.json},
 openalex = {W2104979528},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/01386bd6d8e091c2ab4c7c7de644d37b-Reviews.html},
 title = {Inferring neural population dynamics from multiple partial recordings of the same neural circuit},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_021bbc7e,
 abstract = {We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from incomplete observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression.},
 author = {Ruttor, Andreas and Batz, Philipp and Opper, Manfred},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/021bbc7ee20b71134d53e20206bd6feb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/021bbc7ee20b71134d53e20206bd6feb-Metadata.json},
 openalex = {W2148132038},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/021bbc7ee20b71134d53e20206bd6feb-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/021bbc7ee20b71134d53e20206bd6feb-Supplemental.zip},
 title = {Approximate Gaussian process inference for the drift function in stochastic differential equations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_024d7f84,
 abstract = {Association field models have attempted to explain human contour grouping performance, and to explain the mean frequency of long-range horizontal connections across cortical columns in V1. However, association fields only depend on the pairwise statistics of edges in natural scenes. We develop a spectral test of the sufficiency of pairwise statistics and show there is significant higher order structure. An analysis using a probabilistic spectral embedding reveals curvature-dependent components.},
 author = {Lawlor, Matthew and Zucker, Steven W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/024d7f84fff11dd7e8d9c510137a2381-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/024d7f84fff11dd7e8d9c510137a2381-Metadata.json},
 openalex = {W2949950285},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/024d7f84fff11dd7e8d9c510137a2381-Reviews.html},
 title = {Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/024d7f84fff11dd7e8d9c510137a2381-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_02522a2b,
 abstract = {This paper addresses the problem of mz-transportability, that is, transferring causal knowledge collected in several heterogeneous domains to a target domain in which only passive observations and limited experimental data can be collected. The paper first establishes a necessary and sufficient condition for deciding the feasibility of mz-transportability, i.e., whether causal effects in the target domain are estimable from the information available. It further proves that a previously established algorithm for computing transport formula is in fact complete, that is, failure of the algorithm implies non-existence of a transport formula. Finally, the paper shows that the do-calculus is complete for the mz-transportability class.},
 author = {Bareinboim, Elias and Lee, Sanghack and Honavar, Vasant and Pearl, Judea},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/02522a2b2726fb0a03bb19f2d8d9524d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/02522a2b2726fb0a03bb19f2d8d9524d-Metadata.json},
 openalex = {W2105897901},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/02522a2b2726fb0a03bb19f2d8d9524d-Reviews.html},
 title = {Transportability from Multiple Environments with Limited Experiments: Completeness Results},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/02522a2b2726fb0a03bb19f2d8d9524d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_0266e33d,
 abstract = {Penalized M-estimators are used in diverse areas of science and engineering to fit high-dimensional models with some low-dimensional structure. Often, the penalties are \emph{geometrically decomposable}, \ie can be expressed as a sum of (convex) support functions. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning.},
 author = {Lee, Jason D and Sun, Yuekai and Taylor, Jonathan E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0266e33d3f546cb5436a10798e657d97-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0266e33d3f546cb5436a10798e657d97-Metadata.json},
 openalex = {W2143072755},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0266e33d3f546cb5436a10798e657d97-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0266e33d3f546cb5436a10798e657d97-Reviews.html},
 title = {On model selection consistency of penalized M-estimators: a geometric theory},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/0266e33d3f546cb5436a10798e657d97-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_043c3d7e,
 abstract = {This paper presents an approach to multilabel classification (MLC) with a large number of labels. Our approach is a reduction to binary classification in which label sets are represented by low dimensional binary vectors. This representation follows the principle of Bloom filters, a space-efficient data structure originally designed for approximate membership testing. We show that a naive application of Bloom filters in MLC is not robust to individual binary classifiers' errors. We then present an approach that exploits a specific feature of real-world datasets when the number of labels is large: many labels (almost) never appear together. Our approach is provably robust, has sublinear training and inference complexity with respect to the number of labels, and compares favorably to state-of-the-art algorithms on two large scale multilabel datasets.},
 author = {Cisse, Moustapha M and Usunier, Nicolas and Arti\`{e}res, Thierry and Gallinari, Patrick},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/043c3d7e489c69b48737cc0c92d0f3a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/043c3d7e489c69b48737cc0c92d0f3a2-Metadata.json},
 openalex = {W2154109204},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/043c3d7e489c69b48737cc0c92d0f3a2-Reviews.html},
 title = {Robust Bloom Filters for Large MultiLabel Classification Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/043c3d7e489c69b48737cc0c92d0f3a2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_05311655,
 abstract = {We investigate the relationship between three fundamental problems in machine learning: binary classification, bipartite ranking, and binary class probability estimation (CPE). It is known that a good binary CPE model can be used to obtain a good binary classification model (by thresholding at 0.5), and also to obtain a good bipartite ranking model (by using the CPE model directly as a ranking model); it is also known that a binary classification model does not necessarily yield a CPE model. However, not much is known about other directions. Formally, these relationships involve regret transfer bounds. In this paper, we introduce the notion of weak regret transfer bounds, where the mapping needed to transform a model from one problem to another depends on the underlying probability distribution (and in practice, must be estimated from data). We then show that, in this weaker sense, a good bipartite ranking model can be used to construct a good classification model (by thresholding at a suitable point), and more surprisingly, also to construct a good binary CPE model (by calibrating the scores of the ranking model).},
 author = {Narasimhan, Harikrishna and Agarwal, Shivani},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/05311655a15b75fab86956663e1819cd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/05311655a15b75fab86956663e1819cd-Metadata.json},
 openalex = {W2153863985},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/05311655a15b75fab86956663e1819cd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/05311655a15b75fab86956663e1819cd-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/05311655a15b75fab86956663e1819cd-Supplemental.zip},
 title = {On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/05311655a15b75fab86956663e1819cd-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_062ddb6c,
 abstract = {Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of \textit{sequential transfer in online learning}, notably in the multi-armed bandit framework, where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it.},
 author = {Gheshlaghi azar, Mohammad and Lazaric, Alessandro and Brunskill, Emma},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/062ddb6c727310e76b6200b7c71f63b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/062ddb6c727310e76b6200b7c71f63b5-Metadata.json},
 openalex = {W2107132870},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/062ddb6c727310e76b6200b7c71f63b5-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/062ddb6c727310e76b6200b7c71f63b5-Supplemental.zip},
 title = {Sequential Transfer in Multi-armed Bandit with Finite Set of Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/062ddb6c727310e76b6200b7c71f63b5-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_0768281a,
 abstract = {We study the Maximum Weight Matching (MWM) problem for general graphs through the max-product Belief Propagation (BP) and related Linear Programming (LP). The BP approach provides distributed heuristics for finding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM) and respective LPs can be considered as continuous relaxations of the discrete MAP problem. It was recently shown that a BP algorithm converges to the correct MWM assignment under a simple GM formulation of MAP/MWM as long as the corresponding LP relaxation is tight. First, under the motivation for forcing the tightness condition, we consider a new GM formulation of MWM, say C-GM, using non-intersecting odd-sized cycles in the graph: the new corresponding LP relaxation, say C-LP, becomes tight for more MWM instances. However, the tightness of C-LP now does not guarantee such convergence and correctness of the new BP on C-GM. To address the issue, we introduce a novel graph transformation applied to C-GM, which results in another GM formulation of MWM, and prove that the respective BP on it converges to the correct MAP/MWM assignment as long as C-LP is tight. Finally, we also show that C-LP always has half-integral solutions, which leads to an efficient BP-based MWM heuristic consisting of making sequential, `cutting plane', modifications to the underlying GM. Our experiments show that this BP-based cutting plane heuristic performs as well as that based on traditional LP solvers.},
 author = {Shin, Jinwoo and Gelfand, Andrew E and Chertkov, Misha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0768281a05da9f27df178b5c39a51263-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0768281a05da9f27df178b5c39a51263-Metadata.json},
 openalex = {W2104339301},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0768281a05da9f27df178b5c39a51263-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0768281a05da9f27df178b5c39a51263-Reviews.html},
 title = {A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/0768281a05da9f27df178b5c39a51263-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_076a0c97,
 abstract = {We introduce kernel nonparametric tests for Lancaster three-variable interaction and for total independence, using embeddings of signed measures into a reproducing kernel Hilbert space. The resulting test statistics are straightforward to compute, and are used in powerful interaction tests, which are consistent against all alternatives for a large family of reproducing kernels. We show the Lancaster test to be sensitive to cases where two independent causes individually have weak influence on a third dependent variable, but their combined effect has a strong influence. This makes the Lancaster test especially suited to finding structure in directed graphical models, where it outperforms competing nonparametric tests in detecting such V-structures.},
 author = {Sejdinovic, Dino and Gretton, Arthur and Bergsma, Wicher},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Metadata.json},
 openalex = {W2098199599},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Supplemental.zip},
 title = {A Kernel Test for Three-Variable Interactions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/076a0c97d09cf1a0ec3e19c7f2529f2b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_077e29b1,
 abstract = {Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the mini-batch setting that is often used in practice. Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of \cite{nesterov2007gradient}.},
 author = {Shalev-Shwartz, Shai and Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/077e29b11be80ab57e1a2ecabb7da330-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/077e29b11be80ab57e1a2ecabb7da330-Metadata.json},
 openalex = {W2951833684},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/077e29b11be80ab57e1a2ecabb7da330-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/077e29b11be80ab57e1a2ecabb7da330-Reviews.html},
 title = {Accelerated Mini-Batch Stochastic Dual Coordinate Ascent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/077e29b11be80ab57e1a2ecabb7da330-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_07cdfd23,
 abstract = {We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efficient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction.},
 author = {Yin, Junming and Ho, Qirong and Xing, Eric P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/07cdfd23373b17c6b337251c22b7ea57-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/07cdfd23373b17c6b337251c22b7ea57-Metadata.json},
 openalex = {W2142092798},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/07cdfd23373b17c6b337251c22b7ea57-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/07cdfd23373b17c6b337251c22b7ea57-Supplemental.zip},
 title = {A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/07cdfd23373b17c6b337251c22b7ea57-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_0bb4aec1,
 abstract = {We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MP-DBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classification, classification with missing inputs, and mean field prediction tasks.1},
 author = {Goodfellow, Ian and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0bb4aec1710521c12ee76289d9440817-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0bb4aec1710521c12ee76289d9440817-Metadata.json},
 openalex = {W2098617596},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0bb4aec1710521c12ee76289d9440817-Reviews.html},
 title = {Multi-Prediction Deep Boltzmann Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/0bb4aec1710521c12ee76289d9440817-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_0c0a7566,
 abstract = {Language users are remarkably good at making inferences about speakers' intentions in context, and children learning their native language also display substantial skill in acquiring the meanings of unknown words. These two cases are deeply related: Language users invent new terms in conversation, and language learners learn the literal meanings of words based on their pragmatic inferences about how those words are used. While pragmatic inference and word learning have both been independently characterized in probabilistic terms, no current work unifies these two. We describe a model in which language learners assume that they jointly approximate a shared, external lexicon and reason recursively about the goals of others in using this lexicon. This model captures phenomena in word learning and pragmatic inference; it additionally leads to insights about the emergence of communicative systems in conversation and the mechanisms by which pragmatic inferences become incorporated into word meanings.},
 author = {Smith, Nathaniel J and Goodman, Noah and Frank, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0c0a7566915f4f24853fc4192689aa7e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0c0a7566915f4f24853fc4192689aa7e-Metadata.json},
 openalex = {W2118508845},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0c0a7566915f4f24853fc4192689aa7e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0c0a7566915f4f24853fc4192689aa7e-Reviews.html},
 title = {Learning and using language via recursive pragmatic reasoning about other agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/0c0a7566915f4f24853fc4192689aa7e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_0deb1c54,
 abstract = {An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case.},
 author = {Lim, Shiau Hong and Xu, Huan and Mannor, Shie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0deb1c54814305ca9ad266f53bc82511-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0deb1c54814305ca9ad266f53bc82511-Metadata.json},
 openalex = {W2136503687},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0deb1c54814305ca9ad266f53bc82511-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0deb1c54814305ca9ad266f53bc82511-Supplemental.zip},
 title = {Reinforcement Learning in Robust Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/0deb1c54814305ca9ad266f53bc82511-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_0ed94223,
 abstract = {Spectral clustering is a fast and popular algorithm for finding clusters in networks. Recently, Chaudhuri et al. [1] and Amini et al. [2] proposed inspired variations on the algorithm that artificially inflate the node degrees for improved statistical performance. The current paper extends the previous statistical estimation results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and provides guidance on the choice of the tuning parameter. Moreover, our results show how the star shape in the eigenvectors-a common feature of empirical networks-can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical models that allow for highly heterogeneous degrees. Throughout, the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models.},
 author = {Qin, Tai and Rohe, Karl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ed9422357395a0d4879191c66f4faa2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ed9422357395a0d4879191c66f4faa2-Metadata.json},
 openalex = {W2963664118},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ed9422357395a0d4879191c66f4faa2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ed9422357395a0d4879191c66f4faa2-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ed9422357395a0d4879191c66f4faa2-Supplemental.zip},
 title = {Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/0ed9422357395a0d4879191c66f4faa2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_0ff39bbb,
 abstract = {Cross language text classification is an important learning task in natural language processing. A critical challenge of cross language learning arises from the fact that words of different languages are in disjoint feature spaces. In this paper, we propose a two-step representation learning method to bridge the feature spaces of different languages by exploiting a set of parallel bilingual documents. Specifically, we first formulate a matrix completion problem to produce a complete parallel document-term matrix for all documents in two languages, and then induce a low dimensional cross-lingual document representation by applying latent semantic indexing on the obtained matrix. We use a projected gradient descent algorithm to solve the formulated matrix completion problem with convergence guarantees. The proposed method is evaluated by conducting a set of experiments with cross language sentiment classification tasks on Amazon product reviews. The experimental results demonstrate that the proposed learning method outperforms a number of other cross language representation learning methods, especially when the number of parallel bilingual documents is small.},
 author = {Xiao, Min and Guo, Yuhong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff39bbbf981ac0151d340c9aa40e63e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff39bbbf981ac0151d340c9aa40e63e-Metadata.json},
 openalex = {W2125666396},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff39bbbf981ac0151d340c9aa40e63e-Reviews.html},
 title = {A Novel Two-Step Method for Cross Language Representation Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/0ff39bbbf981ac0151d340c9aa40e63e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_0ff8033c,
 abstract = {We address the problem of recoverability i.e. deciding whether there exists a consistent estimator of a given relation Q, when data are missing not at random. We employ a formal representation called 'Missingness Graphs' to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured. Using this representation, we derive conditions that the graph should satisfy to ensure recoverability and devise algorithms to detect the presence of these conditions in the graph.},
 author = {Mohan, Karthika and Pearl, Judea and Tian, Jin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff8033cf9437c213ee13937b1c4c455-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff8033cf9437c213ee13937b1c4c455-Metadata.json},
 openalex = {W2158492410},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff8033cf9437c213ee13937b1c4c455-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/0ff8033cf9437c213ee13937b1c4c455-Supplemental.zip},
 title = {Graphical Models for Inference with Missing Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/0ff8033cf9437c213ee13937b1c4c455-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_109a0ca3,
 abstract = {We study a new class of structured Schatten norms for tensors that includes two recently proposed norms (overlapped and latent) for convex-optimization-based tensor decomposition. We analyze the performance of latent approach for tensor decomposition, which was empirically found to perform better than the overlapped approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a specific unknown mode, this approach performs as well as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structured Schatten norms, which is also interesting in the general context of structured sparsity. We confirm through numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error.},
 author = {Tomioka, Ryota and Suzuki, Taiji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/109a0ca3bc27f3e96597370d5c8cf03d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/109a0ca3bc27f3e96597370d5c8cf03d-Metadata.json},
 openalex = {W2963112220},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/109a0ca3bc27f3e96597370d5c8cf03d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/109a0ca3bc27f3e96597370d5c8cf03d-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/109a0ca3bc27f3e96597370d5c8cf03d-Supplemental.zip},
 title = {Convex Tensor Decomposition via Structured Schatten Norm Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/109a0ca3bc27f3e96597370d5c8cf03d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_115f8950,
 abstract = {We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering datasets with high-dimensional inputs.},
 author = {Titsias RC AUEB, Michalis and Lazaro-Gredilla, Miguel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/115f89503138416a242f40fb7d7f338e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/115f89503138416a242f40fb7d7f338e-Metadata.json},
 openalex = {W2154382426},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/115f89503138416a242f40fb7d7f338e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/115f89503138416a242f40fb7d7f338e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/115f89503138416a242f40fb7d7f338e-Supplemental.zip},
 title = {Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/115f89503138416a242f40fb7d7f338e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_13f320e7,
 abstract = {Stochastic block models characterize observed network relationships via latent community memberships. In large social networks, we expect entities to participate in multiple communities, and the number of communities to grow with the network size. We introduce a new model for these phenomena, the hierarchical Dirichlet process relational model, which allows nodes to have mixed membership in an unbounded set of communities. To allow scalable learning, we derive an online stochastic variational inference algorithm. Focusing on assortative models of undirected networks, we also propose an efficient structured mean field variational bound, and online methods for automatically pruning unused communities. Compared to state-of-the-art online learning methods for parametric relational models, we show significantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes. We also showcase an analysis of Little-Sis, a large network of who-knows-who at the heights of business and government.},
 author = {Kim, Dae Il and Gopalan, Prem K and Blei, David and Sudderth, Erik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/13f320e7b5ead1024ac95c3b208610db-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/13f320e7b5ead1024ac95c3b208610db-Metadata.json},
 openalex = {W2164773304},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/13f320e7b5ead1024ac95c3b208610db-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/13f320e7b5ead1024ac95c3b208610db-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/13f320e7b5ead1024ac95c3b208610db-Supplemental.zip},
 title = {Efficient Online Inference for Bayesian Nonparametric Relational Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/13f320e7b5ead1024ac95c3b208610db-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1579779b,
 abstract = {We study Monte Carlo tree search (MCTS) in zero-sum extensive-form games with perfect information and simultaneous moves. We present a general template of MCTS algorithms for these games, which can be instantiated by various selection methods. We formally prove that if a selection method is $ε$-Hannan consistent in a matrix game and satisfies additional requirements on exploration, then the MCTS algorithm eventually converges to an approximate Nash equilibrium (NE) of the extensive-form game. We empirically evaluate this claim using regret matching and Exp3 as the selection methods on randomly generated games and empirically selected worst case games. We confirm the formal result and show that additional MCTS variants also converge to approximate NE on the evaluated games.},
 author = {Lisy, Viliam and Kovarik, Vojta and Lanctot, Marc and Bosansky, Branislav},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1579779b98ce9edb98dd85606f2c119d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1579779b98ce9edb98dd85606f2c119d-Metadata.json},
 openalex = {W2113092063},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1579779b98ce9edb98dd85606f2c119d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1579779b98ce9edb98dd85606f2c119d-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1579779b98ce9edb98dd85606f2c119d-Supplemental.zip},
 title = {Convergence of Monte Carlo Tree Search in Simultaneous Move Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1579779b98ce9edb98dd85606f2c119d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1714726c,
 abstract = {Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex non-Gaussian factors, there is still a significant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difficult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model (e.g., a neural network or random forest) to map EP message inputs to EP message outputs. We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising.},
 author = {Heess, Nicolas and Tarlow, Daniel and Winn, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1714726c817af50457d810aae9d27a2e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1714726c817af50457d810aae9d27a2e-Metadata.json},
 openalex = {W2124654932},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1714726c817af50457d810aae9d27a2e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1714726c817af50457d810aae9d27a2e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1714726c817af50457d810aae9d27a2e-Supplemental.zip},
 title = {Learning to Pass Expectation Propagation Messages},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1714726c817af50457d810aae9d27a2e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_17c276c8,
 abstract = {With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efficiently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for significant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial.},
 author = {Shababo, Ben and Paige, Brooks and Pakman, Ari and Paninski, Liam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/17c276c8e723eb46aef576537e9d56d0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/17c276c8e723eb46aef576537e9d56d0-Metadata.json},
 openalex = {W2101918431},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/17c276c8e723eb46aef576537e9d56d0-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/17c276c8e723eb46aef576537e9d56d0-Supplemental.zip},
 title = {Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/17c276c8e723eb46aef576537e9d56d0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_18426034,
 abstract = {We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efficiently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classification, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classification, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classification label and localized latent paths.},
 author = {Shapovalova, Nataliya and Raptis, Michalis and Sigal, Leonid and Mori, Greg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/184260348236f9554fe9375772ff966e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/184260348236f9554fe9375772ff966e-Metadata.json},
 openalex = {W2096037448},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/184260348236f9554fe9375772ff966e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/184260348236f9554fe9375772ff966e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/184260348236f9554fe9375772ff966e-Supplemental.zip},
 title = {Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/184260348236f9554fe9375772ff966e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1896a3bf,
 abstract = {We present a non-factorized variational method for full posterior inference in Bayesian hierarchical models, with the goal of capturing the posterior variable dependencies via efficient and possibly parallel computation. Our approach unifies the integrated nested Laplace approximation (INLA) under the variational framework. The proposed method is applicable in more challenging scenarios than typically assumed by INLA, such as Bayesian Lasso, which is characterized by the non-differentiability of the l1 norm arising from independent Laplace priors. We derive an upper bound for the Kullback-Leibler divergence, which yields a fast closed-form solution via decoupled optimization. Our method is a reliable analytic alternative to Markov chain Monte Carlo (MCMC), and it results in a tighter evidence lower bound than that of mean-field variational Bayes (VB) method.},
 author = {Han, Shaobo and Liao, Xuejun and Carin, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1896a3bf730516dd643ba67b4c447d36-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1896a3bf730516dd643ba67b4c447d36-Metadata.json},
 openalex = {W2118812382},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1896a3bf730516dd643ba67b4c447d36-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1896a3bf730516dd643ba67b4c447d36-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1896a3bf730516dd643ba67b4c447d36-Supplemental.zip},
 title = {Integrated Non-Factorized Variational Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1896a3bf730516dd643ba67b4c447d36-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_18997733,
 abstract = {Multi-armed bandit problems formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems. In many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance, content may be served to a group of users by taking advantage of an underlying network of social relationships among them. In this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems. More specifically, we design and analyze a global recommendation strategy which allocates a bandit algorithm to each network node (user) and allows it to share signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments, carried out on synthetic and real-world datasets, show a consistent increase in prediction performance obtained by exploiting the network structure.},
 author = {Cesa-Bianchi, Nicol\`{o} and Gentile, Claudio and Zappella, Giovanni},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/18997733ec258a9fcaf239cc55d53363-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/18997733ec258a9fcaf239cc55d53363-Metadata.json},
 openalex = {W2149571656},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/18997733ec258a9fcaf239cc55d53363-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/18997733ec258a9fcaf239cc55d53363-Supplemental.zip},
 title = {A Gang of Bandits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/18997733ec258a9fcaf239cc55d53363-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_19bc9161,
 abstract = {Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches.},
 author = {Bresson, Xavier and Laurent, Thomas and Uminsky, David and von Brecht, James},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/19bc916108fc6938f52cb96f7e087941-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/19bc916108fc6938f52cb96f7e087941-Metadata.json},
 openalex = {W2962872734},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/19bc916108fc6938f52cb96f7e087941-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/19bc916108fc6938f52cb96f7e087941-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/19bc916108fc6938f52cb96f7e087941-Supplemental.zip},
 title = {Multiclass Total Variation Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/19bc916108fc6938f52cb96f7e087941-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1aa48fc4,
 abstract = {In this work, we propose a general method for recovering low-rank three-order tensors, in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors. Since the unfolding matrices of a tensor are interdependent, we introduce auxiliary variables and relax the hard equality constraints by the augmented Lagrange multiplier method. To improve the computational efficiency, we introduce a proximal gradient step to the alternating direction minimization method. We have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm. Both simulations and experiments show that our methods are more efficient and effective than previous work. The proposed method can be easily applied to simultaneously rectify and align multiple images or videos frames. In this context, the state-of-the-art algorithms RASL and TILT can be viewed as two special cases of our work, and yet each only performs part of the function of our method.},
 author = {Zhang, Xiaoqin and Wang, Di and Zhou, Zhengyuan and Ma, Yi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Metadata.json},
 openalex = {W2103053668},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1aa48fc4880bb0c9b8a3bf979d3b917e-Supplemental.zip},
 title = {Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1aa48fc4880bb0c9b8a3bf979d3b917e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1abb1e1e,
 author = {Hsieh, Cho-Jui and Sustik, Matyas A and Dhillon, Inderjit S and Ravikumar, Pradeep K and Poldrack, Russell},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1abb1e1ea5f481b589da52303b091cbb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1abb1e1ea5f481b589da52303b091cbb-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1abb1e1ea5f481b589da52303b091cbb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1abb1e1ea5f481b589da52303b091cbb-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1abb1e1ea5f481b589da52303b091cbb-Supplemental.zip},
 title = {BIG \&amp; QUIC: Sparse Inverse Covariance Estimation for a Million Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1abb1e1ea5f481b589da52303b091cbb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1afa34a7,
 abstract = {Graph matching is a challenging problem with very important applications in a wide range of fields, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsity-related techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efficiently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data. The code is publicly available.},
 author = {Fiori, Marcelo and Sprechmann, Pablo and Vogelstein, Joshua and Muse, Pablo and Sapiro, Guillermo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1afa34a7f984eeabdbb0a7d494132ee5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1afa34a7f984eeabdbb0a7d494132ee5-Metadata.json},
 openalex = {W2949941173},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1afa34a7f984eeabdbb0a7d494132ee5-Reviews.html},
 title = {Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1afa34a7f984eeabdbb0a7d494132ee5-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1baff70e,
 abstract = {How do humans perceive the speed of a coherent motion stimulus that contains motion energy in multiple spatiotemporal frequency bands? Here we tested the idea that perceived speed is the result of an integration process that optimally combines speed information across independent spatiotemporal frequency channels. We formalized this hypothesis with a Bayesian observer model that combines the likelihood functions provided by the individual channel responses (cues). We experimentally validated the model with a 2AFC speed discrimination experiment that measured subjects' perceived speed of drifting sinusoidal gratings with different contrasts and spatial frequencies, and of various combinations of these single gratings. We found that the perceived speeds of the combined stimuli are independent of the relative phase of the underlying grating components. The results also show that the discrimination thresholds are smaller for the combined stimuli than for the individual grating components, supporting the cue combination hypothesis. The proposed Bayesian model fits the data well, accounting for the full psychometric functions of both simple and combined stimuli. Fits are improved if we assume that the channel responses are subject to divisive normalization. Our results provide an important step toward a more complete model of visual motion perception that can predict perceived speeds for coherent motion stimuli of arbitrary spatial structure.},
 author = {Jogan, Matjaz and Stocker, Alan A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1baff70e2669e8376347efd3a874a341-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1baff70e2669e8376347efd3a874a341-Metadata.json},
 openalex = {W2150783909},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1baff70e2669e8376347efd3a874a341-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1baff70e2669e8376347efd3a874a341-Reviews.html},
 title = {Optimal integration of visual speed across different spatiotemporal frequency channels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1baff70e2669e8376347efd3a874a341-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1cecc7a7,
 abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
 author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Metadata.json},
 openalex = {W2127795553},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Reviews.html},
 title = {Translating Embeddings for Modeling Multi-relational Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1e1d1841,
 abstract = {Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain. In this paper, we first introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model. We then show an approach to compiling the problem of finding robust plans to the conformant probabilistic planning problem, and present experimental results with Probabilistic-FF planner.},
 author = {Nguyen, Tuan A and Kambhampati, Subbarao and Do, Minh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1e1d184167ca7676cf665225e236a3d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1e1d184167ca7676cf665225e236a3d2-Metadata.json},
 openalex = {W1880783585},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1e1d184167ca7676cf665225e236a3d2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1e1d184167ca7676cf665225e236a3d2-Reviews.html},
 title = {Synthesizing Robust Plans under Incomplete Domain Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1e1d184167ca7676cf665225e236a3d2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1f4477ba,
 abstract = {Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely used in many applications, and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity $O(k^{2}n)$ using message-passing algorithms, where k is the size of the FVS, and n is the total number of nodes. We propose efficient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or flight networks where the FVS nodes often correspond to a small number of high-degree nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate in $O(kn^2+n^2\log n)$ if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing a inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efficient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank correction with complexity $O(kn^{2}+n^{2}\log n)$ per iteration. We also perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with FVSs of various sizes.},
 author = {Liu, Ying and Willsky, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f4477bad7af3616c1f933a02bfabe4e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f4477bad7af3616c1f933a02bfabe4e-Metadata.json},
 openalex = {W2951455783},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f4477bad7af3616c1f933a02bfabe4e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f4477bad7af3616c1f933a02bfabe4e-Supplemental.zip},
 title = {Learning Gaussian Graphical Models with Observed or Latent FVSs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1f50893f,
 abstract = {Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identification of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the K-SVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We fit the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The flexibility of the block-based representation is reflected in the variability of the recovered cell shapes.},
 author = {Pachitariu, Marius and Packer, Adam M and Pettit, Noah and Dalgleish, Henry and Hausser, Michael and Sahani, Maneesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f50893f80d6830d62765ffad7721742-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f50893f80d6830d62765ffad7721742-Metadata.json},
 openalex = {W2118128164},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f50893f80d6830d62765ffad7721742-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f50893f80d6830d62765ffad7721742-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1f50893f80d6830d62765ffad7721742-Supplemental.zip},
 title = {Extracting regions of interest from biological images with convolutional sparse block coding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1f50893f80d6830d62765ffad7721742-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_1ff8a7b5,
 abstract = {Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent. We also offer an analysis of the different emergent time scales.},
 author = {Hermans, Michiel and Schrauwen, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Metadata.json},
 openalex = {W2159505618},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Reviews.html},
 title = {Training and Analysing Deep Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2050e03c,
 abstract = {We study low rank matrix and tensor completion and propose novel algorithms that employ adaptive sampling schemes to obtain strong performance guarantees. Our algorithms exploit adaptivity to identify entries that are highly informative for learning the column space of the matrix (tensor) and consequently, our results hold even when the row space is highly coherent, in contrast with previous analyses. In the absence of noise, we show that one can exactly recover a $n \times n$ matrix of rank $r$ from merely $Ω(n r^{3/2}\log(r))$ matrix entries. We also show that one can recover an order $T$ tensor using $Ω(n r^{T-1/2}T^2 \log(r))$ entries. For noisy recovery, our algorithm consistently estimates a low rank matrix corrupted with noise using $Ω(n r^{3/2} \textrm{polylog}(n))$ entries. We complement our study with simulations that verify our theory and demonstrate the scalability of our algorithms.},
 author = {Krishnamurthy, Akshay and Singh, Aarti},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2050e03ca119580f74cca14cc6e97462-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2050e03ca119580f74cca14cc6e97462-Metadata.json},
 openalex = {W2952509110},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2050e03ca119580f74cca14cc6e97462-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2050e03ca119580f74cca14cc6e97462-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2050e03ca119580f74cca14cc6e97462-Supplemental.zip},
 title = {Low-Rank Matrix and Tensor Completion via Adaptive Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2050e03ca119580f74cca14cc6e97462-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_20d135f0,
 abstract = {Determinantal Point Process (DPP) has gained much popularity for modeling sets of diverse items. The gist of DPP is that the probability of choosing a particular set of items is proportional to the determinant of a positive definite matrix that defines the similarity of those items. However, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets. In this paper, we address this problem by constructing a rapidly mixing Markov chain, from which we can acquire a sample from the given DPP in sub-cubic time. In addition, we show that this framework can be extended to sampling from cardinality-constrained DPPs. As an application, we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters, resulting in better clustering.},
 author = {Kang, Byungkon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/20d135f0f28185b84a4cf7aa51f29500-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/20d135f0f28185b84a4cf7aa51f29500-Metadata.json},
 openalex = {W2148516480},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/20d135f0f28185b84a4cf7aa51f29500-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/20d135f0f28185b84a4cf7aa51f29500-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/20d135f0f28185b84a4cf7aa51f29500-Supplemental.zip},
 title = {Fast Determinantal Point Process Sampling with Application to Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/20d135f0f28185b84a4cf7aa51f29500-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_226d1f15,
 abstract = {Motivated by an application in computational biology, we consider low-rank matrix factorization with {0,1}-constraints on one of the factors and optionally convex constraints on the second one. In addition to the non-convexity shared with other matrix factorization schemes, our problem is further complicated by a combinatorial constraint set of size 2m·r, where m is the dimension of the data points and r the rank of the factorization. Despite apparent intractability, we provide — in the line of recent work on non-negative matrix factorization by Arora et al. (2012)— an algorithm that provably recovers the underlying factorization in the exact case with O(mr2r + mnr + r2n) operations for n datapoints. To obtain this result, we use theory around the Littlewood-Offord lemma from combinatorics.},
 author = {Slawski, Martin and Hein, Matthias and Lutsik, Pavlo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/226d1f15ecd35f784d2a20c3ecf56d7f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/226d1f15ecd35f784d2a20c3ecf56d7f-Metadata.json},
 openalex = {W2962776144},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/226d1f15ecd35f784d2a20c3ecf56d7f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/226d1f15ecd35f784d2a20c3ecf56d7f-Supplemental.zip},
 title = {Matrix factorization with binary components},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/226d1f15ecd35f784d2a20c3ecf56d7f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2291d2ec,
 abstract = {In visual recognition problems, the common data distribution mismatches between training and testing make domain adaptation essential. However, image data is difficult to manually divide into the discrete domains required by adaptation algorithms, and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways (lighting, pose, background, resolution, etc.) We propose an approach to automatically discover latent domains in image or video datasets. Our formulation imposes two key properties on domains: maximum distinctiveness and maximum learnability. By maximum distinctiveness, we require the underlying distributions of the identified domains to be different from each other to the maximum extent; by maximum learnability, we ensure that a strong discriminative model can be learned from the domain. We devise a nonparametric formulation and efficient optimization procedure that can successfully discover domains among both training and test data. We extensively evaluate our approach on object recognition and human activity recognition tasks.},
 author = {Gong, Boqing and Grauman, Kristen and Sha, Fei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Metadata.json},
 openalex = {W2157989183},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Supplemental.zip},
 title = {Reshaping Visual Datasets for Domain Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_22fb0cee,
 abstract = {Recent extensions of the Perceptron as the Tempotron and the Chronotron suggest that this theoretical concept is highly relevant for understanding networks of spiking neurons in the brain. It is not known, however, how the computational power of the Perceptron might be accomplished by the plasticity mechanisms of real synapses. Here we prove that spike-timing-dependent plasticity having an anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of Hebbian shape for inhibitory synapses are sufficient for realizing the original Perceptron Learning Rule if these respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons. We also show that with these simple yet biologically realistic dynamics Tempotrons and Chronotrons are learned. The proposed mechanism enables incremental associative learning from a continuous stream of patterns and might therefore underly the acquisition of long term memories in cortex. Our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.},
 author = {Albers, Christian and Westkott, Maren and Pawelzik, Klaus},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/22fb0cee7e1f3bde58293de743871417-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/22fb0cee7e1f3bde58293de743871417-Metadata.json},
 openalex = {W2134820110},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/22fb0cee7e1f3bde58293de743871417-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/22fb0cee7e1f3bde58293de743871417-Supplemental.zip},
 title = {Perfect Associative Learning with Spike-Timing-Dependent Plasticity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/22fb0cee7e1f3bde58293de743871417-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_23350907,
 abstract = {Structure learning algorithms for graphical models have focused almost exclusively on stable environments in which the underlying generative process does not change; that is, they assume that the generating model is globally stationary. In real-world environments, however, such changes often occur without warning or signal. Real-world data often come from generating models that are only locally stationary. In this paper, we present LoSST, a novel, heuristic structure learning algorithm that tracks changes in graphical model structure or parameters in a dynamic, real-time manner. We show by simulation that the algorithm performs comparably to batch-mode learning when the generating graphical structure is globally stationary, and significantly better when it is only locally stationary.},
 author = {Kummerfeld, Erich and Danks, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/233509073ed3432027d48b1a83f5fbd2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/233509073ed3432027d48b1a83f5fbd2-Metadata.json},
 openalex = {W2122738254},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/233509073ed3432027d48b1a83f5fbd2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/233509073ed3432027d48b1a83f5fbd2-Reviews.html},
 title = {Tracking Time-varying Graphical Structure},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/233509073ed3432027d48b1a83f5fbd2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_242c100d,
 abstract = {Phase retrieval problems involve solving linear equations, but with missing sign (or phase, for complex numbers) information. More than four decades after it was first proposed, the seminal error reduction algorithm of Gerchberg and Saxton and Fienup is still the popular choice for solving many variants of this problem. The algorithm is based on alternating minimization; i.e., it alternates between estimating the missing phase information, and the candidate solution. Despite its wide usage in practice, no global convergence guarantees for this algorithm are known. In this paper, we show that a (resampling) variant of this approach converges geometrically to the solution of one such problem-finding a vector x from y, A, where y = |A <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">T</sup> x| and |z| denotes a vector of element-wise magnitudes of z-under the assumption that A is Gaussian. Empirically, we demonstrate that alternating minimization performs similar to recently proposed convex techniques for this problem (which are based on “lifting” to a convex matrix problem) in sample complexity and robustness to noise. However, it is much more efficient and can scale to large problems. Analytically, for a resampling version of alternating minimization, we show geometric convergence to the solution, and sample complexity that is off by log factors from obvious lower bounds. We also establish close to optimal scaling for the case when the unknown vector is sparse. Our work represents the first theoretical guarantee for alternating minimization (albeit with resampling) for any variant of phase retrieval problems in the non-convex setting.},
 author = {Netrapalli, Praneeth and Jain, Prateek and Sanghavi, Sujay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/242c100dc94f871b6d7215b868a875f8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/242c100dc94f871b6d7215b868a875f8-Metadata.json},
 openalex = {W2963443408},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/242c100dc94f871b6d7215b868a875f8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/242c100dc94f871b6d7215b868a875f8-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/242c100dc94f871b6d7215b868a875f8-Supplemental.zip},
 title = {Phase Retrieval Using Alternating Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/242c100dc94f871b6d7215b868a875f8-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_24681928,
 abstract = {Stochastic And-Or grammars compactly represent both compositionality and re-configurability and have been used to model different types of data such as images and events. We present a unified formalization of stochastic And-Or grammars that is agnostic to the type of the data being modeled, and propose an unsupervised approach to learning the structures as well as the parameters of such grammars. Starting from a trivial initial grammar, our approach iteratively induces compositions and reconfigurations in a unified manner and optimizes the posterior probability of the grammar. In our empirical evaluation, we applied our approach to learning event grammars and image grammars and achieved comparable or better performance than previous approaches.},
 author = {Tu, Kewei and Pavlovskaia, Maria and Zhu, Song-Chun},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/24681928425f5a9133504de568f5f6df-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/24681928425f5a9133504de568f5f6df-Metadata.json},
 openalex = {W2147583489},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/24681928425f5a9133504de568f5f6df-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/24681928425f5a9133504de568f5f6df-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/24681928425f5a9133504de568f5f6df-Supplemental.zip},
 title = {Unsupervised Structure Learning of Stochastic And-Or Grammars},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/24681928425f5a9133504de568f5f6df-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_26337353,
 abstract = {Bilinear approximation of a matrix is a powerful paradigm of unsupervised learning. In some applications, however, there is a natural hierarchy of concepts that ought to be reflected in the unsupervised analysis. For example, in the neuro-sciences image sequence considered here, there are the semantic concepts of pixel → neuron → assembly that should find their counterpart in the unsupervised analysis. Driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels. In contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts. In addition, we learn the nature of these relations rather than imposing them. Finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion.

The proposed bilevel SHMF (sparse heterarchical matrix factorization) is the first formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies. Experiments show that the proposed model fully recovers the structure from difficult synthetic data designed to imitate the experimental data. More importantly, bilevel SHMF yields plausible interpretations of real-world Calcium imaging data.},
 author = {Diego Andilla, Ferran and Hamprecht, Fred A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/26337353b7962f533d78c762373b3318-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/26337353b7962f533d78c762373b3318-Metadata.json},
 openalex = {W2123480521},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/26337353b7962f533d78c762373b3318-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/26337353b7962f533d78c762373b3318-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/26337353b7962f533d78c762373b3318-Supplemental.zip},
 title = {Learning Multi-level Sparse Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/26337353b7962f533d78c762373b3318-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2812e5cf,
 abstract = {We study stochastic optimization problems when the data is sparse, which is in a sense dual to current perspectives on high-dimensional statistical learning and optimization. We highlight both the difficulties—in terms of increased sample complexity that sparse data necessitates—and the potential benefits, in terms of allowing parallelism and asynchrony in the design of algorithms. Concretely, we derive matching upper and lower bounds on the minimax rate for optimization and learning with sparse data, and we exhibit algorithms achieving these rates. We also show how leveraging sparsity leads to (still minimax optimal) parallel and asynchronous algorithms, providing experimental evidence complementing our theoretical results on several medium to large-scale learning tasks.},
 author = {Duchi, John and Jordan, Michael I and McMahan, Brendan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2812e5cf6d8f21d69c91dddeefb792a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2812e5cf6d8f21d69c91dddeefb792a7-Metadata.json},
 openalex = {W2106709023},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2812e5cf6d8f21d69c91dddeefb792a7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2812e5cf6d8f21d69c91dddeefb792a7-Reviews.html},
 title = {Estimation, Optimization, and Parallelism when Data is Sparse},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2812e5cf6d8f21d69c91dddeefb792a7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_28267ab8,
 abstract = {We informally call a stochastic process learnable if it admits a generalization error approaching zero in probability for any concept class with finite VC-dimension (IID processes are the simplest example). A mixture of learnable processes need not be learnable itself, and certainly its generalization error need not decay at the same rate. In this paper, we argue that it is natural in predictive PAC to condition not on the past observations but on the mixture component of the sample path. This definition not only matches what a realistic learner might demand, but also allows us to sidestep several otherwise grave problems in learning from dependent data. In particular, we give a novel PAC generalization bound for mixtures of learnable processes with a generalization error that is not worse than that of each mixture component. We also provide a characterization of mixtures of absolutely regular (β-mixing) processes, of independent probability-theoretic interest.},
 author = {Shalizi, Cosma and Kontorovich, Aryeh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28267ab848bcf807b2ed53c3a8f8fc8a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28267ab848bcf807b2ed53c3a8f8fc8a-Metadata.json},
 openalex = {W2145290714},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28267ab848bcf807b2ed53c3a8f8fc8a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28267ab848bcf807b2ed53c3a8f8fc8a-Reviews.html},
 title = {Predictive PAC Learning and Process Decompositions.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/28267ab848bcf807b2ed53c3a8f8fc8a-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_285f89b8,
 abstract = {Logistic-normal topic models can effectively discover correlation structures among latent topics. However, their inference remains a challenge because of the non-conjugacy between the logistic-normal prior and multinomial topic mixing proportions. Existing algorithms either make restricting mean-field assumptions or are not scalable to large-scale applications. This paper presents a partially collapsed Gibbs sampling algorithm that approaches the provably correct distribution by exploring the ideas of data augmentation. To improve time efficiency, we further present a parallel implementation that can deal with large-scale applications and learn the correlation structures of thousands of topics from millions of documents. Extensive empirical results demonstrate the promise.},
 author = {Chen, Jianfei and Zhu, Jun and Wang, Zi and Zheng, Xun and Zhang, Bo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/285f89b802bcb2651801455c86d78f2a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/285f89b802bcb2651801455c86d78f2a-Metadata.json},
 openalex = {W2151383095},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/285f89b802bcb2651801455c86d78f2a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/285f89b802bcb2651801455c86d78f2a-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/285f89b802bcb2651801455c86d78f2a-Supplemental.zip},
 title = {Scalable Inference for Logistic-Normal Topic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/285f89b802bcb2651801455c86d78f2a-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_286674e3,
 abstract = {In a closed-loop brain-computer interface (BCI), adaptive decoders are used to learn parameters suited to decoding the user's neural response. Feedback to the user provides information which permits the neural tuning to also adapt. We present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic Gaussian (LQG) control problem. In simulation we characterize how decoding performance improves as the neural encoding and adaptive decoder optimize, qualitatively resembling experimentally demonstrated closed-loop improvement. We then propose a novel, modified decoder update rule which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics. Our modeling approach offers promise for gaining insights into co-adaptation as well as improving user learning of BCI control in practical settings.},
 author = {Merel, Josh S and Fox, Roy and Jebara, Tony and Paninski, Liam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/286674e3082feb7e5afb92777e48821f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/286674e3082feb7e5afb92777e48821f-Metadata.json},
 openalex = {W2130039165},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/286674e3082feb7e5afb92777e48821f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/286674e3082feb7e5afb92777e48821f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/286674e3082feb7e5afb92777e48821f-Supplemental.zip},
 title = {A multi-agent control framework for co-adaptation in brain-computer interfaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/286674e3082feb7e5afb92777e48821f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_28f0b864,
 abstract = {Conditional random fields, which model the distribution of a multivariate response conditioned on a set of covariates using undirected graphs, are widely used in a variety of multivariate prediction applications. Popular instances of this class of models, such as categorical-discrete CRFs, Ising CRFs, and conditional Gaussian based CRFs, are not well suited to the varied types of response variables in many applications, including count-valued responses. We thus introduce a novel subclass of CRFs, derived by imposing node-wise conditional distributions of response variables conditioned on the rest of the responses and the covariates as arising from univariate exponential families. This allows us to derive novel multivariate CRFs given any univariate exponential distribution, including the Poisson, negative binomial, and exponential distributions. Also in particular, it addresses the common CRF problem of specifying feature functions determining the interactions between response variables and covariates. We develop a class of tractable penalized M-estimators to learn these CRF distributions from data, as well as a unified sparsistency analysis for this general class of CRFs showing exact structure recovery can be achieved with high probability.},
 author = {Yang, Eunho and Ravikumar, Pradeep K and Allen, Genevera I and Liu, Zhandong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28f0b864598a1291557bed248a998d4e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28f0b864598a1291557bed248a998d4e-Metadata.json},
 openalex = {W2157894547},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28f0b864598a1291557bed248a998d4e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28f0b864598a1291557bed248a998d4e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28f0b864598a1291557bed248a998d4e-Supplemental.zip},
 title = {Conditional Random Fields via Univariate Exponential Families},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/28f0b864598a1291557bed248a998d4e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_28fc2782,
 abstract = {We present the first result for kernel regression where the procedure adapts locally at a point x to both the unknown local dimension of the metric space X and the unknown Holder-continuity of the regression function at x. The result holds with high probability simultaneously at all points x in a general metric space X of unknown structure.},
 author = {Kpotufe, Samory and Garg, Vikas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28fc2782ea7ef51c1104ccf7b9bea13d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28fc2782ea7ef51c1104ccf7b9bea13d-Metadata.json},
 openalex = {W2166560602},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28fc2782ea7ef51c1104ccf7b9bea13d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/28fc2782ea7ef51c1104ccf7b9bea13d-Reviews.html},
 title = {Adaptivity to Local Smoothness and Dimension in Kernel Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/28fc2782ea7ef51c1104ccf7b9bea13d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_291597a1,
 abstract = {This paper introduces the online probing problem: In each round, the learner is able to purchase the values of a subset of feature values. After the learner uses this information to come up with a prediction for the given round, he then has the option of paying to see the loss function that he is evaluated against. Either way, the learner pays for both the errors of his predictions and also whatever he chooses to observe, including the cost of observing the loss function for the given round and the cost of the observed features. We consider two variations of this problem, depending on whether the learner can observe the label for free or not. We provide algorithms and upper and lower bounds on the regret for both variants. We show that a positive cost for observing the label significantly increases the regret of the problem.},
 author = {Zolghadr, Navid and Bartok, Gabor and Greiner, Russell and Gy\"{o}rgy, Andr\'{a}s and Szepesvari, Csaba},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/291597a100aadd814d197af4f4bab3a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/291597a100aadd814d197af4f4bab3a7-Metadata.json},
 openalex = {W2134625044},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/291597a100aadd814d197af4f4bab3a7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/291597a100aadd814d197af4f4bab3a7-Supplemental.zip},
 title = {Online Learning with Costly Features and Labels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/291597a100aadd814d197af4f4bab3a7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2a50e9c2,
 abstract = {Many problems in machine learning can be solved by rounding the solution of an appropriate linear program (LP). This paper shows that we can recover solutions of comparable quality by rounding an approximate LP solution instead of the exact one. These approximate LP solutions can be computed efficiently by applying a parallel stochastic-coordinate-descent method to a quadratic-penalty formulation of the LP. We derive worst-case runtime and solution quality guarantees of this scheme using novel perturbation and convergence analysis. Our experiments demonstrate that on such combinatorial problems as vertex cover, independent set and multiway-cut, our approximate rounding scheme is up to an order of magnitude faster than Cplex (a commercial LP solver) while producing solutions of similar quality.},
 author = {Sridhar, Srikrishna and Wright, Stephen and Re, Christopher and Liu, Ji and Bittorf, Victor and Zhang, Ce},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a50e9c2d6b89b95bcb416d6857f8b45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a50e9c2d6b89b95bcb416d6857f8b45-Metadata.json},
 openalex = {W2135900350},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a50e9c2d6b89b95bcb416d6857f8b45-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a50e9c2d6b89b95bcb416d6857f8b45-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a50e9c2d6b89b95bcb416d6857f8b45-Supplemental.zip},
 title = {An Approximate, Efficient LP Solver for LP Rounding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2a50e9c2d6b89b95bcb416d6857f8b45-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2a9d121c,
 abstract = {We consider the problem of maintaining the data-structures of a partition-based regression procedure in a setting where the training data arrives sequentially over time. We prove that it is possible to maintain such a structure in time O (log n) at any time step n while achieving a nearly-optimal regression rate of O (n-2/(2+d)) in terms of the unknown metric dimension d. Finally we prove a new regression lower-bound which is independent of a given data size, and hence is more appropriate for the streaming setting.},
 author = {Kpotufe, Samory and Orabona, Francesco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Metadata.json},
 openalex = {W2105409877},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Supplemental.zip},
 title = {Regression-tree Tuning in a Streaming Setting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2b8a6159,
 abstract = {We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefficient vector θ0 ∈ ℝp from noisy linear observations y = Xθ0 + w ∈ ℝn (p > n) and the popular estimation procedure of solving the l1-penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the l2 estimation risk $\|\hat{\theta}-\theta_0\|_2$ and the variance of the noise when distributions of θ0 and w are unknown. These can be used to select the regularization parameter optimally. Our approach combines Stein's unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b] on the analysis of approximate message passing and the risk of LASSO.

We establish high-dimensional consistency of our estimators for sequences of matrices X of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on a certain conjecture from statistical physics.

To the best of our knowledge, this result is the first that provides an asymptotically consistent risk estimator for the LASSO solely based on data. In addition, we demonstrate through simulations that our variance estimation outperforms several existing methods in the literature.},
 author = {Bayati, Mohsen and Erdogdu, Murat A and Montanari, Andrea},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2b8a61594b1f4c4db0902a8a395ced93-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2b8a61594b1f4c4db0902a8a395ced93-Metadata.json},
 openalex = {W2126166357},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2b8a61594b1f4c4db0902a8a395ced93-Reviews.html},
 title = {Estimating LASSO Risk and Noise Level},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2b8a61594b1f4c4db0902a8a395ced93-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2bcab9d9,
 abstract = {The olfactory system faces a difficult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons. Here we derive neural implementations of two approximate inference algorithms that could be used by the brain. One is a variational algorithm (which builds on the work of Beck. et al., 2012), the other is based on sampling. Importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a spike and slab prior, for which most odors have zero concentration. After mapping the two algorithms onto neural dynamics, we find that both can infer correct odors in less than 100 ms. Thus, at the behavioral level, the two algorithms make very similar predictions. However, they make different assumptions about connectivity and neural computations, and make different predictions about neural activity. Thus, they should be distinguishable experimentally. If so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities.},
 author = {Grabska-Barwinska, Agnieszka and Beck, Jeff and Pouget, Alexandre and Latham, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2bcab9d935d219641434683dd9d18a03-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2bcab9d935d219641434683dd9d18a03-Metadata.json},
 openalex = {W2103289252},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2bcab9d935d219641434683dd9d18a03-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2bcab9d935d219641434683dd9d18a03-Supplemental.zip},
 title = {Demixing odors - fast inference in olfaction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2bcab9d935d219641434683dd9d18a03-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2d6cc4b2,
 abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.},
 author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Metadata.json},
 openalex = {W4297754112},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Reviews.html},
 title = {Zero-Shot Learning Through Cross-Modal Transfer},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2d6cc4b2d139a53512fb8cbb3086ae2e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2dace78f,
 abstract = {Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of soft-greedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.},
 author = {Wagner, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2dace78f80bc92e6d7493423d729448e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2dace78f80bc92e6d7493423d729448e-Metadata.json},
 openalex = {W2105675791},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2dace78f80bc92e6d7493423d729448e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2dace78f80bc92e6d7493423d729448e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2dace78f80bc92e6d7493423d729448e-Supplemental.zip},
 title = {Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2dace78f80bc92e6d7493423d729448e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_2dffbc47,
 abstract = {State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference \emph{and learning} (i.e. state estimation and system identification) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena. To enable efficient inference, we marginalize over the transition dynamics function and infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity.},
 author = {Frigola, Roger and Lindsten, Fredrik and Sch\"{o}n, Thomas B and Rasmussen, Carl Edward},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2dffbc474aa176b6dc957938c15d0c8b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2dffbc474aa176b6dc957938c15d0c8b-Metadata.json},
 openalex = {W2952680595},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2dffbc474aa176b6dc957938c15d0c8b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/2dffbc474aa176b6dc957938c15d0c8b-Reviews.html},
 title = {Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/2dffbc474aa176b6dc957938c15d0c8b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_309928d4,
 abstract = {In this paper we investigate the use of Langevin Monte Carlo methods on the probability simplex and propose a new method, Stochastic gradient Riemannian Langevin dynamics, which is simple to implement and can be applied to large scale data. We apply this method to latent Dirichlet allocation in an online mini-batch setting, and demonstrate that it achieves substantial performance improvements over the state of the art online variational Bayesian methods.},
 author = {Patterson, Sam and Teh, Yee Whye},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/309928d4b100a5d75adff48a9bfc1ddb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/309928d4b100a5d75adff48a9bfc1ddb-Metadata.json},
 openalex = {W2110529144},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/309928d4b100a5d75adff48a9bfc1ddb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/309928d4b100a5d75adff48a9bfc1ddb-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/309928d4b100a5d75adff48a9bfc1ddb-Supplemental.zip},
 title = {Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/309928d4b100a5d75adff48a9bfc1ddb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_31b3b31a,
 abstract = {Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. Our sufficient conditions for identifiability involve a novel set of expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allow for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition.},
 author = {Anandkumar, Anima and Hsu, Daniel J and Janzamin, Majid and Kakade, Sham M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/31b3b31a1c2f8a370206f111127c0dbd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/31b3b31a1c2f8a370206f111127c0dbd-Metadata.json},
 openalex = {W2149375393},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/31b3b31a1c2f8a370206f111127c0dbd-Reviews.html},
 title = {When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/31b3b31a1c2f8a370206f111127c0dbd-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_3210ddbe,
 abstract = {The method of stable random projections is useful for efficiently approximating the lα distance (0 < α ≤ 2) in high dimension and it is naturally suitable for data streams. In this paper, we propose to use only the signs of the projected data and we analyze the probability of collision (i.e., when the two signs differ). Interestingly, when α = 1 (i.e., Cauchy random projections), we show that the probability of collision can be accurately approximated as functions of the chi-square (Χ2) similarity. In text and vision applications, the Χ2 similarity is a popular measure when the features are generated from histograms (which are a typical example of data streams). Experiments confirm that the proposed method is promising for large-scale learning applications. The full paper is available at arXiv:1308.1009.

There are many future research problems. For example, when α → 0, the collision probability is a function of the resemblance (of the binary-quantized data). This provides an effective mechanism for resemblance estimation in data streams.},
 author = {Li, Ping and Samorodnitsk, Gennady and Hopcroft, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3210ddbeaa16948a702b6049b8d9a202-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3210ddbeaa16948a702b6049b8d9a202-Metadata.json},
 openalex = {W2152513411},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3210ddbeaa16948a702b6049b8d9a202-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3210ddbeaa16948a702b6049b8d9a202-Reviews.html},
 title = {Sign Cauchy Projections and Chi-Square Kernel},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/3210ddbeaa16948a702b6049b8d9a202-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_3295c76a,
 abstract = {Category models for objects or activities typically rely on supervised learning requiring sufficiently large training sets. Transferring knowledge from known categories to novel classes with no or only a few labels is far less researched even though it is a common scenario. In this work, we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances. Our proposed approach Propagated Semantic Transfer combines three techniques. First, we transfer information from known to novel categories by incorporating external knowledge, such as linguistic or expert-specified information, e.g., by a mid-level layer of semantic attributes. Second, we exploit the manifold structure of novel classes. More specifically we adapt a graph-based learning algorithm - so far only used for semi-supervised learning -to zero-shot and few-shot learning. Third, we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation. We evaluate our approach on three challenging datasets in two different applications, namely on Animals with Attributes and ImageNet for image classification and on MPII Composites for activity recognition. Our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets.},
 author = {Rohrbach, Marcus and Ebert, Sandra and Schiele, Bernt},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3295c76acbf4caaed33c36b1b5fc2cb1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3295c76acbf4caaed33c36b1b5fc2cb1-Metadata.json},
 openalex = {W2151575489},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3295c76acbf4caaed33c36b1b5fc2cb1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3295c76acbf4caaed33c36b1b5fc2cb1-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3295c76acbf4caaed33c36b1b5fc2cb1-Supplemental.zip},
 title = {Transfer Learning in a Transductive Setting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/3295c76acbf4caaed33c36b1b5fc2cb1-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_32b30a25,
 abstract = {The Markov chain is a convenient tool to represent the dynamics of complex systems such as traffic and social systems, where probabilistic transition takes place between internal states. A Markov chain is characterized by initial-state probabilities and a state-transition probability matrix. In the traditional setting, a major goal is to study properties of a Markov chain when those probabilities are known. This paper tackles an inverse version of the problem: we find those probabilities from partial observations at a limited number of states. The observations include the frequency of visiting a state and the rate of reaching a state from another. Practical examples of this task include traffic monitoring systems in cities, where we need to infer the traffic volume on single link on a road network from a limited number of observation points. We formulate this task as a regularized optimization problem, which is efficiently solved using the notion of natural gradient. Using synthetic and real-world data sets including city traffic monitoring data, we demonstrate the effectiveness of our method.},
 author = {Morimura, Tetsuro and Osogami, Takayuki and Ide, Tsuyoshi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/32b30a250abd6331e03a2a1f16466346-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/32b30a250abd6331e03a2a1f16466346-Metadata.json},
 openalex = {W2106317109},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/32b30a250abd6331e03a2a1f16466346-Reviews.html},
 title = {Solving inverse problem of Markov chain with partial observations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/32b30a250abd6331e03a2a1f16466346-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_33e8075e,
 abstract = {An increasing number of applications require processing of signals defined on weighted graphs. While wavelets provide a flexible tool for signal processing in the classical setting of regular domains, the existing graph wavelet constructions are less flexible—they are guided solely by the structure of the underlying graph and do not take directly into consideration the particular class of signals to be processed. This chapter introduces a machine learning framework for constructing graph wavelets that can sparsely represent a given class of signals. Our construction uses the lifting scheme, and is based on the observation that the recurrent nature of the lifting scheme gives rise to a structure resembling a deep auto-encoder network. Particular properties that the resulting wavelets must satisfy determine the training objective and the structure of the involved neural networks. The training is unsupervised, and is conducted similarly to the greedy pre-training of a stack of auto-encoders. After training is completed, we obtain a linear wavelet transform that can be applied to any graph signal in time and memory linear in the size of the graph. Improved sparsity of our wavelet transform for the test signals is confirmed via experiments both on synthetic and real data.},
 author = {Rustamov, Raif and Guibas, Leonidas J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/33e8075e9970de0cfea955afd4644bb2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/33e8075e9970de0cfea955afd4644bb2-Metadata.json},
 openalex = {W2121244641},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/33e8075e9970de0cfea955afd4644bb2-Reviews.html},
 title = {Wavelets on Graphs via Deep Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_3493894f,
 abstract = {In this paper, we are interested in the development of efficient algorithms for convex optimization problems in the simultaneous presence of multiple objectives and stochasticity in the first-order information. We cast the stochastic multiple objective optimization problem into a constrained optimization problem by choosing one function as the objective and try to bound other objectives by appropriate thresholds. We first examine a two stages exploration-exploitation based algorithm which first approximates the stochastic objectives by sampling and then solves a constrained stochastic optimization problem by projected gradient method. This method attains a suboptimal convergence rate even under strong assumption on the objectives. Our second approach is an efficient primal-dual stochastic algorithm. It leverages on the theory of Lagrangian method in constrained optimization and attains the optimal convergence rate of O(1/√T) in high probability for general Lipschitz continuous objectives.},
 author = {Mahdavi, Mehrdad and Yang, Tianbao and Jin, Rong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3493894fa4ea036cfc6433c3e2ee63b0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3493894fa4ea036cfc6433c3e2ee63b0-Metadata.json},
 openalex = {W2160374150},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3493894fa4ea036cfc6433c3e2ee63b0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3493894fa4ea036cfc6433c3e2ee63b0-Reviews.html},
 title = {Stochastic Convex Optimization with Multiple Objectives},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/3493894fa4ea036cfc6433c3e2ee63b0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_35cf8659,
 abstract = {We propose an efficient Bayesian nonparametric model for discovering hierarchical community structure in social networks. Our model is a tree-structured mixture of potentially exponentially many stochastic blockmodels. We describe a family of greedy agglomerative model selection algorithms that take just one pass through the data to learn a fully probabilistic, hierarchical community model. In the worst case, Our algorithms scale quadratically in the number of vertices of the network, but independent of the number of nested communities. In practice, the run time of our algorithms are two orders of magnitude faster than the Infinite Relational Model, achieving comparable or better accuracy.},
 author = {Blundell, Charles and Teh, Yee Whye},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/35cf8659cfcb13224cbd47863a34fc58-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/35cf8659cfcb13224cbd47863a34fc58-Metadata.json},
 openalex = {W2096297741},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/35cf8659cfcb13224cbd47863a34fc58-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/35cf8659cfcb13224cbd47863a34fc58-Reviews.html},
 title = {Bayesian Hierarchical Community Discovery},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/35cf8659cfcb13224cbd47863a34fc58-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_36a16a25,
 abstract = {In many natural settings, the analysis goal is not to characterize a single data set in isolation, but rather to understand the difference between one set of observations and another. For example, given a background corpus of news articles together with writings of a particular author, one may want a topic model that explains word patterns and themes specific to the author. Another example comes from genomics, in which biological signals may be collected from different regions of a genome, and one wants a model that captures the differential statistics observed in these regions. This paper formalizes this notion of contrastive learning for mixture models, and develops spectral algorithms for inferring mixture components specific to a foreground data set when contrasted with a background data set. The method builds on recent moment-based estimators and tensor decompositions for latent variable models, and has the intuitive feature of using background data statistics to appropriately modify moments estimated from foreground data. A key advantage of the method is that the background data need only be coarsely modeled, which is important when the background is too complex, noisy, or not of interest. The method is demonstrated on applications in contrastive topic modeling and genomic sequence analysis.},
 author = {Zou, James Y and Hsu, Daniel J and Parkes, David C and Adams, Ryan P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/36a16a2505369e0c922b6ea7a23a56d2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/36a16a2505369e0c922b6ea7a23a56d2-Metadata.json},
 openalex = {W2141986476},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/36a16a2505369e0c922b6ea7a23a56d2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/36a16a2505369e0c922b6ea7a23a56d2-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/36a16a2505369e0c922b6ea7a23a56d2-Supplemental.zip},
 title = {Contrastive Learning Using Spectral Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/36a16a2505369e0c922b6ea7a23a56d2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_37a749d8,
 abstract = {As massively parallel computations have become broadly available with modern GPUs, deep architectures trained on very large datasets have risen in popularity. Discriminatively trained convolutional neural networks, in particular, were recently shown to yield state-of-the-art performance in challenging image classification benchmarks such as ImageNet. However, elements of these architectures are similar to standard hand-crafted representations used in computer vision. In this paper, we explore the extent of this analogy, proposing a version of the state-of-the-art Fisher vector image encoding that can be stacked in multiple layers. This architecture significantly improves on standard Fisher vectors, and obtains competitive results with deep convolutional networks at a smaller computational learning cost. Our hybrid architecture allows us to assess how the performance of a conventional hand-crafted image classification pipeline changes with increased depth. We also show that convolutional networks and Fisher vector encodings are complementary in the sense that their combination further improves the accuracy.},
 author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/37a749d808e46495a8da1e5352d03cae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/37a749d808e46495a8da1e5352d03cae-Metadata.json},
 openalex = {W2165146474},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/37a749d808e46495a8da1e5352d03cae-Reviews.html},
 title = {Deep Fisher Networks for Large-Scale Image Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/37a749d808e46495a8da1e5352d03cae-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_37f0e884,
 abstract = {For smooth and strongly convex optimizations, the optimal iteration complexity of the gradient-based algorithm is O(√k log 1/e), where k is the condition number. In the case that the optimization problem is ill-conditioned, we need to evaluate a large number of full gradients, which could be computationally expensive. In this paper, we propose to remove the dependence on the condition number by allowing the algorithm to access stochastic gradients of the objective function. To this end, we present a novel algorithm named Epoch Mixed Gradient Descent (EMGD) that is able to utilize two kinds of gradients. A distinctive step in EMGD is the mixed gradient descent, where we use a combination of the full and stochastic gradients to update the intermediate solution. Theoretical analysis shows that EMGD is able to find an e-optimal solution by computing O(log 1/e) full gradients and O(k2 log 1/e) stochastic gradients.},
 author = {Zhang, Lijun and Mahdavi, Mehrdad and Jin, Rong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/37f0e884fbad9667e38940169d0a3c95-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/37f0e884fbad9667e38940169d0a3c95-Metadata.json},
 openalex = {W2098741260},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/37f0e884fbad9667e38940169d0a3c95-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/37f0e884fbad9667e38940169d0a3c95-Reviews.html},
 title = {Linear Convergence with Condition Number Independent Access of Full Gradients},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/37f0e884fbad9667e38940169d0a3c95-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_3871bd64,
 abstract = {In this paper, we theoretically study the problem of binary classification in the presence of random classification noise—the learner, instead of seeing the true labels, sees labels that have independently been flipped with some small probability. Moreover, random label noise is class-conditional— the flip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that the method leads to an efficient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence — methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88% accuracy even when 40% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets.},
 author = {Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Metadata.json},
 openalex = {W2113290770},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3871bd64012152bfb53fdf04b401193f-Supplemental.zip},
 title = {Learning with Noisy Labels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/3871bd64012152bfb53fdf04b401193f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_38af8613,
 abstract = {In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and high-dimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks.},
 author = {Levine, Sergey and Koltun, Vladlen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/38af86134b65d0f10fe33d30dd76442e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/38af86134b65d0f10fe33d30dd76442e-Metadata.json},
 openalex = {W2098284983},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/38af86134b65d0f10fe33d30dd76442e-Reviews.html},
 title = {Variational Policy Search via Trajectory Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/38af86134b65d0f10fe33d30dd76442e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_38db3aed,
 abstract = {Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learning algorithm, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.},
 author = {Wager, Stefan and Wang, Sida and Liang, Percy S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/38db3aed920cf82ab059bfccbd02be6a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/38db3aed920cf82ab059bfccbd02be6a-Metadata.json},
 openalex = {W2152722485},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/38db3aed920cf82ab059bfccbd02be6a-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/38db3aed920cf82ab059bfccbd02be6a-Supplemental.zip},
 title = {Dropout Training as Adaptive Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/38db3aed920cf82ab059bfccbd02be6a-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_39461a19,
 abstract = {We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We are interested in studying prior-free and prior-dependent regret bounds, very much in the same spirit as the usual distribution-free and distribution-dependent bounds for the non-Bayesian stochastic bandit. Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by $14 \sqrt{n K}$. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by $\frac{1}{20} \sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.},
 author = {Bubeck, Sebastien and Liu, Che-Yu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39461a19e9eddfb385ea76b26521ea48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39461a19e9eddfb385ea76b26521ea48-Metadata.json},
 openalex = {W2951802169},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39461a19e9eddfb385ea76b26521ea48-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39461a19e9eddfb385ea76b26521ea48-Supplemental.zip},
 title = {Prior-free and prior-dependent regret bounds for Thompson Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/39461a19e9eddfb385ea76b26521ea48-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_3948ead6,
 abstract = {Hermitian positive definite matrices (HPD) recur throughout statistics and machine learning. In this paper we develop \emph{geometric optimisation} for globally optimising certain nonconvex loss functions arising in the modelling of data via elliptically contoured distributions (ECDs). We exploit the remarkable structure of the convex cone of positive definite matrices which allows one to uncover hidden geodesic convexity of objective functions that are nonconvex in the ordinary Euclidean sense. Going even beyond manifold convexity we show how further metric properties of HPD matrices can be exploited to globally optimise several ECD log-likelihoods that are not even geodesic convex. We present key results that help recognise this geometric structure, as well as obtain efficient fixed-point algorithms to optimise the corresponding objective functions. To our knowledge, ours are the most general results on geometric optimisation of HPD matrices known so far. Experiments reveal the benefits of our approach---it avoids any eigenvalue computations which makes it very competitive.},
 author = {Sra, Suvrit and Hosseini, Reshad},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3948ead63a9f2944218de038d8934305-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3948ead63a9f2944218de038d8934305-Metadata.json},
 openalex = {W2170257405},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3948ead63a9f2944218de038d8934305-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3948ead63a9f2944218de038d8934305-Reviews.html},
 title = {Geometric optimisation on positive definite matrices for elliptically contoured distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/3948ead63a9f2944218de038d8934305-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_39e4973b,
 abstract = {We solve the mean field equations for a stochastic Hopfield network with temperature (noise) in the presence of strong, i.e., multiply stored, patterns, and use this solution to obtain the storage capacity of such a network. Our result provides for the first time a rigorous solution of the mean filed equations for the standard Hopfield model and is in contrast to the mathematically unjustifiable replica technique that has been used hitherto for this derivation. We show that the critical temperature for stability of a strong pattern is equal to its degree or multiplicity, when the sum of the squares of degrees of the patterns is negligible compared to the network size. In the case of a single strong pattern, when the ratio of the number of all stored pattens and the network size is a positive constant, we obtain the distribution of the overlaps of the patterns with the mean field and deduce that the storage capacity for retrieving a strong pattern exceeds that for retrieving a simple pattern by a multiplicative factor equal to the square of the degree of the strong pattern. This square law property provides justification for using strong patterns to model attachment types and behavioural prototypes in psychology and psychotherapy.},
 author = {Edalat, Abbas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39e4973ba3321b80f37d9b55f63ed8b8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39e4973ba3321b80f37d9b55f63ed8b8-Metadata.json},
 openalex = {W2128335855},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39e4973ba3321b80f37d9b55f63ed8b8-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/39e4973ba3321b80f37d9b55f63ed8b8-Supplemental.zip},
 title = {Capacity of strong attractor patterns to model behavioural and cognitive prototypes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/39e4973ba3321b80f37d9b55f63ed8b8-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_3a835d32,
 abstract = {Label propagation is one of the state-of-the-art methods for semi-supervised learning, which estimates labels by propagating label information through a graph. Label propagation assumes that data points (nodes) connected in a graph should have similar labels. Consequently, the label estimation heavily depends on edge weights in a graph which represent similarity of each node pair. We propose a method for a graph to capture the manifold structure of input features using edge weights parameterized by a similarity function. In this approach, edge weights represent both similarity and local reconstruction weight simultaneously, both being reasonable for label propagation. For further justification, we provide analytical considerations including an interpretation as a cross-validation of a propagation model in the feature space, and an error analysis based on a low dimensional manifold model. Experimental results demonstrated the effectiveness of our approach both in synthetic and real datasets.},
 author = {Karasuyama, Masayuki and Mamitsuka, Hiroshi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3a835d3215755c435ef4fe9965a3f2a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3a835d3215755c435ef4fe9965a3f2a0-Metadata.json},
 openalex = {W2166688141},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3a835d3215755c435ef4fe9965a3f2a0-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3a835d3215755c435ef4fe9965a3f2a0-Supplemental.zip},
 title = {Manifold-based Similarity Adaptation for Label Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/3a835d3215755c435ef4fe9965a3f2a0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_3cec07e9,
 abstract = {We address the problem of fast estimation of ordinary least squares (OLS) from large amounts of data (n ≫ p). We propose three methods which solve the big data problem by subsampling the covariance matrix using either a single or two stage estimation. All three run in the order of size of input i.e. O(np) and our best method, Uluru, gives an error bound of O(√p/n) which is independent of the amount of subsampling as long as it is above a threshold. We provide theoretical bounds for our algorithms in the fixed design (with Randomized Hadamard preconditioning) as well as sub-Gaussian random design setting. We also compare the performance of our methods on synthetic and real-world datasets and show that if observations are i.i.d., sub-Gaussian then one can directly subsample without the expensive Randomized Hadamard preconditioning without loss of accuracy.},
 author = {Dhillon, Paramveer and Lu, Yichao and Foster, Dean P and Ungar, Lyle},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Metadata.json},
 openalex = {W2170582628},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Supplemental.zip},
 title = {New Subsampling Algorithms for Fast Least Squares Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/3cec07e9ba5f5bb252d13f5f431e4bbb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_3dd48ab3,
 abstract = {We describe a novel approach for computing collision-free \emph{global} trajectories for $p$ agents with specified initial and final configurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with $p$ for several cost functionals. We also show that a specialization of our algorithm can be used for {\em local} motion planning by solving the problem of joint optimization in velocity space.},
 author = {Bento, Jos\'{e} and Derbinsky, Nate and Alonso-Mora, Javier and Yedidia, Jonathan S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Metadata.json},
 openalex = {W2950295088},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Supplemental.zip},
 title = {A message-passing algorithm for multi-agent trajectory planning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/3dd48ab31d016ffcbf3314df2b3cb9ce-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_3df1d4b9,
 abstract = {The problem of matching not just two, but m different sets of objects to each other arises in many contexts, including finding the correspondence between feature points across multiple images in computer vision. At present it is usually solved by matching the sets pairwise, in series. In contrast, we propose a new method, Permutation Synchronization, which finds all the matchings jointly, in one shot, via a relaxation to eigenvector decomposition. The resulting algorithm is both computationally efficient, and, as we demonstrate with theoretical arguments as well as experimental results, much more stable to noise than previous methods.},
 author = {Pachauri, Deepti and Kondor, Risi and Singh, Vikas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3df1d4b96d8976ff5986393e8767f5b2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3df1d4b96d8976ff5986393e8767f5b2-Metadata.json},
 openalex = {W2151975094},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/3df1d4b96d8976ff5986393e8767f5b2-Reviews.html},
 title = {Solving the multi-way matching problem by permutation synchronization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/3df1d4b96d8976ff5986393e8767f5b2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_40008b9a,
 abstract = {We propose a learning setting in which unlabeled data is free, and the cost of a label depends on its value, which is not known in advance. We study binary classification in an extreme case, where the algorithm only pays for negative labels. Our motivation are applications such as fraud detection, in which investigating an honest transaction should be avoided if possible. We term the setting auditing, and consider the auditing complexity of an algorithm: the number of negative labels the algorithm requires in order to learn a hypothesis with low relative error. We design auditing algorithms for simple hypothesis classes (thresholds and rectangles), and show that with these algorithms, the auditing complexity can be significantly lower than the active label complexity. We also discuss a general competitive approach for auditing and possible modifications to the framework.},
 author = {Sabato, Sivan and Sarwate, Anand D and Srebro, Nati},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/40008b9a5380fcacce3976bf7c08af5b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/40008b9a5380fcacce3976bf7c08af5b-Metadata.json},
 openalex = {W2949704722},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/40008b9a5380fcacce3976bf7c08af5b-Reviews.html},
 title = {Auditing: Active Learning with Outcome-Dependent Query Costs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_4122cb13,
 abstract = {Distributions over matrices with exchangeable rows and infinitely many columns are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution.},
 author = {Williamson, Sinead A and MacEachern, Steve N and Xing, Eric P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4122cb13c7a474c1976c9706ae36521d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4122cb13c7a474c1976c9706ae36521d-Metadata.json},
 openalex = {W2141658583},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4122cb13c7a474c1976c9706ae36521d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4122cb13c7a474c1976c9706ae36521d-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4122cb13c7a474c1976c9706ae36521d-Supplemental.zip},
 title = {Restricting exchangeable nonparametric distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/4122cb13c7a474c1976c9706ae36521d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_41ae36ec,
 abstract = {Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm-regularized problem, which may be of independent interest.},
 author = {Hou, Ke and Zhou, Zirui and So, Anthony Man-Cho and Luo, Zhi-Quan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41ae36ecb9b3eee609d05b90c14222fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41ae36ecb9b3eee609d05b90c14222fb-Metadata.json},
 openalex = {W2128336967},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41ae36ecb9b3eee609d05b90c14222fb-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41ae36ecb9b3eee609d05b90c14222fb-Supplemental.zip},
 title = {On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/41ae36ecb9b3eee609d05b90c14222fb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_41bfd20a,
 abstract = {This paper considers the sample complexity of the multi-armed bandit with dependencies among the arms. Some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration. The clearest example of this is the class of upper confidence bound (UCB) algorithms, but recent work has shown that a simple posterior sampling algorithm, sometimes called Thompson sampling, can be analyzed in the same manner as optimistic approaches. In this paper, we develop a regret bound that holds for both classes of algorithms. This bound applies broadly and can be specialized to many model classes. It depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models.},
 author = {Russo, Daniel and Van Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41bfd20a38bb1b0bec75acf0845530a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41bfd20a38bb1b0bec75acf0845530a7-Metadata.json},
 openalex = {W2163840227},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41bfd20a38bb1b0bec75acf0845530a7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/41bfd20a38bb1b0bec75acf0845530a7-Supplemental.zip},
 title = {Eluder Dimension and the Sample Complexity of Optimistic Exploration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_428fca9b,
 abstract = {We study differentially private mechanisms for answering smooth queries on databases consisting of data points in ℝd. A K-smooth query is specified by a function whose partial derivatives up to order K are all bounded. We develop an e-differentially private mechanism which for the class of K-smooth queries has accuracy $O (\left{n}^{-\frac{K}{2d+K}}/\epsilon)$). The mechanism first outputs a summary of the database. To obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database. Outputting the summary runs in time $O(n^{1+\frac{d}{2d+K}})$, and the evaluation algorithm for answering a query runs in time $\tilde O (n^{\frac{d+2+\frac{2d}{K}}{2d+K}} )$. Our mechanism is based on L∞-approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efficiently computable coefficients.},
 author = {Wang, Ziteng and Fan, Kai and Zhang, Jiaqi and Wang, Liwei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/428fca9bc1921c25c5121f9da7815cde-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/428fca9bc1921c25c5121f9da7815cde-Metadata.json},
 openalex = {W2142799870},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/428fca9bc1921c25c5121f9da7815cde-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/428fca9bc1921c25c5121f9da7815cde-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/428fca9bc1921c25c5121f9da7815cde-Supplemental.zip},
 title = {Efficient Algorithm for Privately Releasing Smooth Queries},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/428fca9bc1921c25c5121f9da7815cde-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_43baa676,
 abstract = {In many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time. This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch. In this work, we study the label complexity of active learning algorithms that request labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufficient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once. In particular, we find that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increase the total number of labels requested, it reduces the total cost required for learning.},
 author = {Yang, Liu and Carbonell, Jaime},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/43baa6762fa81bb43b39c62553b2970d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/43baa6762fa81bb43b39c62553b2970d-Metadata.json},
 openalex = {W2140128091},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/43baa6762fa81bb43b39c62553b2970d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/43baa6762fa81bb43b39c62553b2970d-Reviews.html},
 title = {Buy-in-Bulk Active Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/43baa6762fa81bb43b39c62553b2970d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_43feaeee,
 abstract = {Undirected graphical models, such as Gaussian graphical models, Ising, and multinomial/categorical graphical models, are widely used in a variety of applications for modeling distributions over a large number of variables. These standard instances, however, are ill-suited to modeling count data, which are increasingly ubiquitous in big-data settings such as genomic sequencing data, user-ratings data, spatial incidence data, climate studies, and site visits. Existing classes of Poisson graphical models, which arise as the joint distributions that correspond to Poisson distributed node-conditional distributions, have a major drawback: they can only model negative conditional dependencies for reasons of normalizability given its infinite domain. In this paper, our objective is to modify the Poisson graphical model distribution so that it can capture a rich dependence structure between count-valued variables. We begin by discussing two strategies for truncating the Poisson distribution and show that only one of these leads to a valid joint distribution. While this model can accommodate a wider range of conditional dependencies, some limitations still remain. To address this, we investigate two additional novel variants of the Poisson distribution and their corresponding joint graphical model distributions. Our three novel approaches provide classes of Poisson-like graphical models that can capture both positive and negative conditional dependencies between count-valued variables. One can learn the graph structure of our models via penalized neighborhood selection, and we demonstrate the performance of our methods by learning simulated networks as well as a network from microRNA-sequencing data.},
 author = {Yang, Eunho and Ravikumar, Pradeep K and Allen, Genevera I and Liu, Zhandong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/43feaeeecd7b2fe2ae2e26d917b6477d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/43feaeeecd7b2fe2ae2e26d917b6477d-Metadata.json},
 openalex = {W2100398996},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/43feaeeecd7b2fe2ae2e26d917b6477d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/43feaeeecd7b2fe2ae2e26d917b6477d-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/43feaeeecd7b2fe2ae2e26d917b6477d-Supplemental.zip},
 title = {On Poisson Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/43feaeeecd7b2fe2ae2e26d917b6477d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_443cb001,
 abstract = {In this paper we describe how MAP inference can be used to sample efficiently from Gibbs distributions. Specifically, we provide means for drawing either approximate or unbiased samples from Gibbs' distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical high signal -high coupling regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds.},
 author = {Hazan, Tamir and Maji, Subhransu and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/443cb001c138b2561a0d90720d6ce111-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/443cb001c138b2561a0d90720d6ce111-Metadata.json},
 openalex = {W2118853346},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/443cb001c138b2561a0d90720d6ce111-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/443cb001c138b2561a0d90720d6ce111-Reviews.html},
 title = {On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/443cb001c138b2561a0d90720d6ce111-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_45645a27,
 abstract = {This paper extends asymptotic Bayesian (FAB) inference for latent feature models (LFMs). FAB inference has not been applicable to models, including LFMs, without a specific condition on the Hessian matrix of a complete log-likelihood, which is required to derive a factorized information criterion (FIC). Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs has the same form as those of mixture models. FAB/LFMs have several desirable properties (e.g., automatic hidden states selection and parameter identifiability) and empirically perform better than state-of-the-art Indian Buffet processes in terms of model selection, prediction, and computational efficiency.},
 author = {Hayashi, Kohei and Fujimaki, Ryohei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/45645a27c4f1adc8a7a835976064a86d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/45645a27c4f1adc8a7a835976064a86d-Metadata.json},
 openalex = {W2170139541},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/45645a27c4f1adc8a7a835976064a86d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/45645a27c4f1adc8a7a835976064a86d-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/45645a27c4f1adc8a7a835976064a86d-Supplemental.zip},
 title = {Factorized Asymptotic Bayesian Inference for Latent Feature Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/45645a27c4f1adc8a7a835976064a86d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_456ac9b0,
 abstract = {While several papers have investigated computationally and statistically efficient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efficient procedure. Our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering.},
 author = {Azizyan, Martin and Singh, Aarti and Wasserman, Larry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/456ac9b0d15a8b7f1e71073221059886-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/456ac9b0d15a8b7f1e71073221059886-Metadata.json},
 openalex = {W2137945041},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/456ac9b0d15a8b7f1e71073221059886-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/456ac9b0d15a8b7f1e71073221059886-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/456ac9b0d15a8b7f1e71073221059886-Supplemental.zip},
 title = {Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/456ac9b0d15a8b7f1e71073221059886-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_46922a08,
 abstract = {We propose an efficient optimization algorithm to select a subset of training data as the inducing set for sparse Gaussian process regression. Previous methods either use different objective functions for inducing set and hyperparameter selection, or else optimize the inducing set by gradient-based continuous optimization. The former approaches are harder to interpret and suboptimal, whereas the latter cannot be applied to discrete input domains or to kernel functions that are not differentiable with respect to the input. The algorithm proposed in this work estimates an inducing set and the hyperparameters using a single objective. It can be used to optimize either the marginal likelihood or a variational free energy. Space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in discrete cases, competitive prediction results as well as a favorable trade-off between training and test time in continuous cases.},
 author = {Cao, Yanshuai and Brubaker, Marcus A and Fleet, David J and Hertzmann, Aaron},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/46922a0880a8f11f8f69cbb52b1396be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/46922a0880a8f11f8f69cbb52b1396be-Metadata.json},
 openalex = {W2144533338},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/46922a0880a8f11f8f69cbb52b1396be-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/46922a0880a8f11f8f69cbb52b1396be-Supplemental.zip},
 title = {Efficient Optimization for Sparse Gaussian Process Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/46922a0880a8f11f8f69cbb52b1396be-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_47a65822,
 abstract = {Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be fit efficiently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global influences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to fitting linear dynamical models without nuclear norm smoothing.},
 author = {Pfau, David and Pnevmatikakis, Eftychios A and Paninski, Liam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47a658229eb2368a99f1d032c8848542-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47a658229eb2368a99f1d032c8848542-Metadata.json},
 openalex = {W2117950425},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47a658229eb2368a99f1d032c8848542-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47a658229eb2368a99f1d032c8848542-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47a658229eb2368a99f1d032c8848542-Supplemental.zip},
 title = {Robust learning of low-dimensional dynamics from large neural ensembles},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/47a658229eb2368a99f1d032c8848542-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_47d1e990,
 abstract = {Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions: (l) Theoretical: By restricting the model class (e.g. to additive noise) we provide general identifiability results. They cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. We show empirically that when the data are causally insufficient or the model is misspecified, the method avoids incorrect answers. We extend the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays. TiMINo is applied to artificial and real data and code is provided.},
 author = {Peters, Jonas and Janzing, Dominik and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47d1e990583c9c67424d369f3414728e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47d1e990583c9c67424d369f3414728e-Metadata.json},
 openalex = {W2135631431},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47d1e990583c9c67424d369f3414728e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/47d1e990583c9c67424d369f3414728e-Reviews.html},
 title = {Causal Inference on Time Series using Restricted Structural Equation Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/47d1e990583c9c67424d369f3414728e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_49182f81,
 abstract = {It is a common practice to approximate complicated functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. The new approximation is justified using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims.},
 author = {Yu, Yao-Liang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49182f81e6a13cf5eaa496d51fea6406-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49182f81e6a13cf5eaa496d51fea6406-Metadata.json},
 openalex = {W2138972044},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49182f81e6a13cf5eaa496d51fea6406-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49182f81e6a13cf5eaa496d51fea6406-Supplemental.zip},
 title = {Better Approximation and Faster Algorithm Using the Proximal Average},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/49182f81e6a13cf5eaa496d51fea6406-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_49b8b4f9,
 abstract = {Kernel embedding of distributions has led to many recent advances in machine learning. However, latent and low rank structures prevalent in real world distributions have rarely been taken into account in this setting. Furthermore, no prior work in kernel embedding literature has addressed the issue of robust embedding when the latent and low rank information are misspecified. In this paper, we propose a hierarchical low rank decomposition of kernels embeddings which can exploit such low rank structures in data while being robust to model misspecification. We also illustrate with empirical evidence that the estimated low rank embeddings lead to improved performance in density estimation.},
 author = {Song, Le and Dai, Bo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49b8b4f95f02e055801da3b4f58e28b7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49b8b4f95f02e055801da3b4f58e28b7-Metadata.json},
 openalex = {W2114099644},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49b8b4f95f02e055801da3b4f58e28b7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/49b8b4f95f02e055801da3b4f58e28b7-Supplemental.zip},
 title = {Robust Low Rank Kernel Embeddings of Multivariate Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/49b8b4f95f02e055801da3b4f58e28b7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_4a213d37,
 abstract = {Motivated by recent progress in natural image statistics, we use newly available datasets with ground truth optical flow to learn the local statistics of optical flow and compare the learned models to prior models assumed by computer vision researchers. We find that a Gaussian mixture model (GMM) with 64 components provides a significantly better model for local flow statistics when compared to commonly used models. We investigate the source of the GMM's success and show it is related to an explicit representation of flow boundaries. We also learn a model that jointly models the local intensity pattern and the local optical flow. In accordance with the assumptions often made in computer vision, the model learns that flow boundaries are more likely at intensity boundaries. However, when evaluated on a large dataset, this dependency is very weak and the benefit of conditioning flow estimation on the local intensity pattern is marginal.},
 author = {Rosenbaum, Dan and Zoran, Daniel and Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4a213d37242bdcad8e7300e202e7caa4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4a213d37242bdcad8e7300e202e7caa4-Metadata.json},
 openalex = {W2131177796},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4a213d37242bdcad8e7300e202e7caa4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4a213d37242bdcad8e7300e202e7caa4-Reviews.html},
 title = {Learning the Local Statistics of Optical Flow},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/4a213d37242bdcad8e7300e202e7caa4-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_4d2e7bd3,
 abstract = {The performance of standard algorithms for Independent Component Analysis quickly deteriorates under the addition of Gaussian noise. This is partially due to a common first step that typically consists of whitening, i.e., applying Principal Component Analysis (PCA) and rescaling the components to have identity covariance, which is not invariant under Gaussian noise.

In our paper we develop the first practical algorithm for Independent Component Analysis that is provably invariant under Gaussian noise. The two main contributions of this work are as follows:

1. We develop and implement an efficient, Gaussian noise invariant decorrelation (quasi-orthogonalization) algorithm using Hessians of the cumulant functions.

2. We propose a very simple and efficient fixed-point GI-ICA (Gradient Iteration ICA) algorithm, which is compatible with quasi-orthogonalization, as well as with the usual PCA-based whitening in the noiseless case. The algorithm is based on a special form of gradient iteration (different from gradient descent). We provide an analysis of our algorithm demonstrating fast convergence following from the basic properties of cumulants. We also present a number of experimental comparisons with the existing methods, showing superior results on noisy data and very competitive performance in the noiseless case.},
 author = {Voss, James R and Rademacher, Luis and Belkin, Mikhail},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4d2e7bd33c475784381a64e43e50922f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4d2e7bd33c475784381a64e43e50922f-Metadata.json},
 openalex = {W2167682446},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4d2e7bd33c475784381a64e43e50922f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4d2e7bd33c475784381a64e43e50922f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4d2e7bd33c475784381a64e43e50922f-Supplemental.zip},
 title = {Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/4d2e7bd33c475784381a64e43e50922f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_4da04049,
 abstract = {Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function. Because of its simplicity and its wide applicability, this principle has been very popular in statistics and in signal processing. In this paper, we intend to make this principle scalable. We introduce a stochastic majorization-minimization scheme which is able to deal with large-scale or possibly infinite data sets. When applied to convex optimization problems under suitable assumptions, we show that it achieves an expected convergence rate of $O(1/\sqrt{n})$ after $n$ iterations, and of $O(1/n)$ for strongly convex functions. Equally important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efficient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale $\ell_1$-logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our approach for solving large-scale structured matrix factorization problems.},
 author = {Mairal, Julien},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4da04049a062f5adfe81b67dd755cecc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4da04049a062f5adfe81b67dd755cecc-Metadata.json},
 openalex = {W2111200589},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4da04049a062f5adfe81b67dd755cecc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4da04049a062f5adfe81b67dd755cecc-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4da04049a062f5adfe81b67dd755cecc-Supplemental.zip},
 title = {Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/4da04049a062f5adfe81b67dd755cecc-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_4f284803,
 abstract = {We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves O(√T log |II| + log |II|) regret with respect to a comparison set of policies II. The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efficiently and the comparison set II has polynomial size, this algorithm is efficient.

We also consider the episodic adversarial online shortest path problem. Here, in each episode an adversary may choose a weighted directed acyclic graph with an identified start and finish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to finish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a fixed policy for selecting paths. This problem is a special case of the online MDP problem. It was shown that for randomly chosen graphs and adversarial losses, the problem can be efficiently solved. We show that it also can be efficiently solved for adversarial graphs and randomly chosen losses. When both graphs and losses are adversarially chosen, we show that designing efficient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difficult problem that has been used to design efficient cryptographic schemes. Finally, we present an efficient algorithm whose regret scales linearly with the number of distinct graphs.},
 author = {Abbasi Yadkori, Yasin and Bartlett, Peter L and Kanade, Varun and Seldin, Yevgeny and Szepesvari, Csaba},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4f284803bd0966cc24fa8683a34afc6e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4f284803bd0966cc24fa8683a34afc6e-Metadata.json},
 openalex = {W2154806059},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4f284803bd0966cc24fa8683a34afc6e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4f284803bd0966cc24fa8683a34afc6e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/4f284803bd0966cc24fa8683a34afc6e-Supplemental.zip},
 title = {Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/4f284803bd0966cc24fa8683a34afc6e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_502e4a16,
 abstract = {Given a Markov decision process (MDP) with n states and a total number m of actions, we study the number of iterations needed by policy iteration (PI) algorithms to converge to the optimal γ-discounted policy. We consider two variations of PI: Howard’s PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard’s PI terminates after at most O((m/(1 − γ))log(1/(1 − γ))) iterations, improving by a factor O(log n) a result by Hansen et al. [Hansen TD, Miltersen PB, Zwick U (2013) Strategy iteration is strongly polynomial for two-player turn-based stochastic games with a constant discount factor. J. ACM 60(1):1:1–1:16.], whereas Simplex-PI terminates after at most O((n m/(1 − γ))log(1/(1 − γ))) iterations, improving by a factor O(log n) a result by Ye [Ye Y (2011) The simplex and policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate. Math. Oper. Res. 36(4):593–603.]. Under some structural properties of the MDP, we then consider bounds that are independent of the discount factor γ: quantities of interest are bounds τ t and τ r —uniform on all states and policies—respectively, on the expected time spent in transient states and the inverse of the frequency of visits in recurrent states given that the process starts from the uniform distribution. Indeed, we show that Simplex-PI terminates after at most Õ(n 3 m 2 τ t τ r ) iterations. This extends a recent result for deterministic MDPs by Post and Ye [Post I, Ye Y (2013) The simplex method is strongly polynomial for deterministic Markov decision processes. Khanna S, ed. Proc. 24th ACM-SIAM Sympos. Discrete Algorithms, SODA '13 (SIAM, Philadelphia), 1465–1473.] in which τ t ≤ 1 and τ r ≤ n; in particular it shows that Simplex-PI is strongly polynomial for a much larger class of MDPs. We explain why similar results seem hard to derive for Howard’s PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively, states that are transient and recurrent for all policies, we show that both Howard’s PI and Simplex-PI terminate after at most Õ(m(n 2 τ t + nτ r )) iterations.},
 author = {Scherrer, Bruno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/502e4a16930e414107ee22b6198c578f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/502e4a16930e414107ee22b6198c578f-Metadata.json},
 openalex = {W2569529055},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/502e4a16930e414107ee22b6198c578f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/502e4a16930e414107ee22b6198c578f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/502e4a16930e414107ee22b6198c578f-Supplemental.zip},
 title = {Improved and Generalized Upper Bounds on the Complexity of Policy Iteration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/502e4a16930e414107ee22b6198c578f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_50c3d761,
 abstract = {Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and finite base set. This discrete setting admits an efficient sampling algorithm based on the eigendecomposition of the defining kernel matrix. Recently, there has been growing interest in using DPPs defined on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efficient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystrom and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces.},
 author = {Affandi, Raja Hafiz and Fox, Emily and Taskar, Ben},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/50c3d7614917b24303ee6a220679dab3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/50c3d7614917b24303ee6a220679dab3-Metadata.json},
 openalex = {W2962775423},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/50c3d7614917b24303ee6a220679dab3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/50c3d7614917b24303ee6a220679dab3-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/50c3d7614917b24303ee6a220679dab3-Supplemental.zip},
 title = {Approximate Inference in Continuous Determinantal Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/50c3d7614917b24303ee6a220679dab3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_51ef186e,
 abstract = {We present SDA-Bayes, a framework for (S)treaming, (D)istributed, (A)synchronous computation of a Bayesian posterior. The framework makes streaming updates to the estimated posterior according to a user-specified approximation batch primitive. We demonstrate the usefulness of our framework, with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet allocation model to two large-scale document collections. We demonstrate the advantages of our algorithm over stochastic variational inference (SVI) by comparing the two after a single pass through a known amount of data---a case where SVI may be applied---and in the streaming setting, where SVI does not apply.},
 author = {Broderick, Tamara and Boyd, Nicholas and Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/51ef186e18dc00c2d31982567235c559-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/51ef186e18dc00c2d31982567235c559-Metadata.json},
 openalex = {W2951057540},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/51ef186e18dc00c2d31982567235c559-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/51ef186e18dc00c2d31982567235c559-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/51ef186e18dc00c2d31982567235c559-Supplemental.zip},
 title = {Streaming Variational Bayes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/51ef186e18dc00c2d31982567235c559-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_52292e0c,
 abstract = {People can learn a new class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on com-positionality and causality that can learn a wide range of natural (although simple) concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classification task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a visual Turing test to show that our model produces human-like performance.},
 author = {Lake, Brenden M and Salakhutdinov, Russ R and Tenenbaum, Josh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52292e0c763fd027c6eba6b8f494d2eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52292e0c763fd027c6eba6b8f494d2eb-Metadata.json},
 openalex = {W2114168642},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52292e0c763fd027c6eba6b8f494d2eb-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52292e0c763fd027c6eba6b8f494d2eb-Supplemental.zip},
 title = {One-shot learning by inverting a compositional causal process},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/52292e0c763fd027c6eba6b8f494d2eb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_52720e00,
 abstract = {We consider the problem of sparse precision matrix estimation in high dimensions using the CLIME estimator, which has several desirable theoretical properties. We present an inexact alternating direction method of multiplier (ADMM) algorithm for CLIME, and establish rates of convergence for both the objective and optimality conditions. Further, we develop a large scale distributed framework for the computations, which scales to millions of dimensions and trillions of parameters, using hundreds of cores. The proposed framework solves CLIME in column-blocks and only involves elementwise operations and parallel matrix multiplications. We evaluate our algorithm on both shared-memory and distributed-memory architectures, which can use block cyclic distribution of data and parameters to achieve load balance and improve the efficiency in the use of memory hierarchies. Experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores.},
 author = {Wang, Huahua and Banerjee, Arindam and Hsieh, Cho-Jui and Ravikumar, Pradeep K and Dhillon, Inderjit S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52720e003547c70561bf5e03b95aa99f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52720e003547c70561bf5e03b95aa99f-Metadata.json},
 openalex = {W2095938605},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52720e003547c70561bf5e03b95aa99f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52720e003547c70561bf5e03b95aa99f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/52720e003547c70561bf5e03b95aa99f-Supplemental.zip},
 title = {Large Scale Distributed Sparse Precision Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/52720e003547c70561bf5e03b95aa99f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_539fd53b,
 abstract = {The Bayesian online change point detection (BOCPD) algorithm provides an efficient way to do exact inference when the parameters of an underlying model may suddenly change over time. BOCPD requires computation of the underlying model's posterior predictives, which can only be computed online in O(1) time and memory for exponential family models. We develop variational approximations to the posterior on change point times (formulated as run lengths) for efficient inference when the underlying model is not in the exponential family, and does not have tractable posterior predictive distributions. In doing so, we develop improvements to online variational inference. We apply our methodology to a tracking problem using radar data with a signal-to-noise feature that is Rice distributed. We also develop a variational method for inferring the parameters of the (non-exponential family) Rice distribution.},
 author = {Turner, Ryan D and Bottone, Steven and Stanek, Clay J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/539fd53b59e3bb12d203f45a912eeaf2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/539fd53b59e3bb12d203f45a912eeaf2-Metadata.json},
 openalex = {W2104064333},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/539fd53b59e3bb12d203f45a912eeaf2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/539fd53b59e3bb12d203f45a912eeaf2-Reviews.html},
 title = {Online Variational Approximations to non-Exponential Family Change Point Models: With Application to Radar Tracking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/539fd53b59e3bb12d203f45a912eeaf2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_53adaf49,
 abstract = {We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.},
 author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53adaf494dc89ef7196d73636eb2451b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53adaf494dc89ef7196d73636eb2451b-Metadata.json},
 openalex = {W2952366348},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53adaf494dc89ef7196d73636eb2451b-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53adaf494dc89ef7196d73636eb2451b-Supplemental.zip},
 title = {RNADE: The real-valued neural autoregressive density-estimator},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/53adaf494dc89ef7196d73636eb2451b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_53c04118,
 abstract = {Recently, Valiant and Valiant [1, 2] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. Specifically, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size O(n/ log n).We propose a novel modification of this approach and show: 1) theoretically, this estimator is optimal (to constant factors, over worst-case instances), and 2) in practice, it performs exceptionally well for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters. Perhaps unsurprisingly, the key step in our approach is to first use the sample to characterize the unseen portion of the distribution. This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the shape of the unobserved portion of the distribution. This approach is robust, general, and theoretically principled; we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems.},
 author = {Valiant, Paul and Valiant, Gregory},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c04118df112c13a8c34b38343b9c10-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c04118df112c13a8c34b38343b9c10-Metadata.json},
 openalex = {W2124055802},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c04118df112c13a8c34b38343b9c10-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c04118df112c13a8c34b38343b9c10-Supplemental.zip},
 title = {Estimating the Unseen: Improved Estimators for Entropy and other Properties},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_53c3bce6,
 abstract = {This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a low-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets.},
 author = {Campbell, Trevor and Liu, Miao and Kulis, Brian and How, Jonathan P and Carin, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c3bce66e43be4f209556518c2fcb54-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c3bce66e43be4f209556518c2fcb54-Metadata.json},
 openalex = {W2118607269},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53c3bce66e43be4f209556518c2fcb54-Reviews.html},
 title = {Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_53e3a716,
 abstract = {We introduce an extended formulation of multi-task learning (MTL) called parametric task learning (PTL) that can systematically handle infinitely many tasks parameterized by a continuous parameter. Our key finding is that, for a certain class of PTL problems, the path of the optimal task-wise solutions can be represented as piecewise-linear functions of the continuous task parameter. Based on this fact, we employ a parametric programming technique to obtain the common shared representation across all the continuously parameterized tasks. We show that our PTL formulation is useful in various scenarios such as learning under non-stationarity, cost-sensitive learning, and quantile regression. We demonstrate the advantage of our approach in these scenarios.},
 author = {Takeuchi, Ichiro and Hongo, Tatsuya and Sugiyama, Masashi and Nakajima, Shinichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53e3a7161e428b65688f14b84d61c610-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53e3a7161e428b65688f14b84d61c610-Metadata.json},
 openalex = {W2145178286},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53e3a7161e428b65688f14b84d61c610-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53e3a7161e428b65688f14b84d61c610-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/53e3a7161e428b65688f14b84d61c610-Supplemental.zip},
 title = {Parametric Task Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/53e3a7161e428b65688f14b84d61c610-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_559cb990,
 abstract = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).},
 author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/559cb990c9dffd8675f6bc2186971dc2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/559cb990c9dffd8675f6bc2186971dc2-Metadata.json},
 openalex = {W4293433088},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/559cb990c9dffd8675f6bc2186971dc2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/559cb990c9dffd8675f6bc2186971dc2-Reviews.html},
 title = {Generalized Denoising Auto-Encoders as Generative Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/559cb990c9dffd8675f6bc2186971dc2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5807a685,
 abstract = {We provide a detailed study of the estimation of probability distributions---discrete and continuous---in a stringent setting in which data is kept private even from the statistician. We give sharp minimax rates of convergence for estimation in these locally private settings, exhibiting fundamental tradeoffs between privacy and convergence rate, as well as providing tools to allow movement along the privacy-statistical efficiency continuum. One of the consequences of our results is that Warner's classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents.},
 author = {Duchi, John and Wainwright, Martin J and Jordan, Michael I},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5807a685d1a9ab3b599035bc566ce2b9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5807a685d1a9ab3b599035bc566ce2b9-Metadata.json},
 openalex = {W2103096545},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5807a685d1a9ab3b599035bc566ce2b9-Reviews.html},
 title = {Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5807a685d1a9ab3b599035bc566ce2b9-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_58c54802,
 abstract = {We consider how to transfer knowledge from previous tasks (MDPs) to a current task in long-lived and bounded agents that must solve a sequence of tasks over a finite lifetime. A novel aspect of our transfer approach is that we reuse reward functions. While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent's behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent. Specifically, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent's performance relative to other approaches, including an approach that transfers policies.},
 author = {Guo, Xiaoxiao and Singh, Satinder and Lewis, Richard L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/58c54802a9fb9526cd0923353a34a7ae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/58c54802a9fb9526cd0923353a34a7ae-Metadata.json},
 openalex = {W2168984293},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/58c54802a9fb9526cd0923353a34a7ae-Reviews.html},
 title = {Reward Mapping for Transfer in Long-Lived Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/58c54802a9fb9526cd0923353a34a7ae-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_598b3e71,
 abstract = {We study exploration in Multi-Armed Bandits in a setting where k players collaborate in order to identify an e-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the k players to communicate only once, they are able to learn √k times faster than a single player. That is, distributing learning to k players gives rise to a factor √k parallel speedup. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor k speed-up in learning performance, with communication only logarithmic in 1/e.},
 author = {Hillel, Eshcar and Karnin, Zohar S and Koren, Tomer and Lempel, Ronny and Somekh, Oren},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/598b3e71ec378bd83e0a727608b5db01-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/598b3e71ec378bd83e0a727608b5db01-Metadata.json},
 openalex = {W2138911442},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/598b3e71ec378bd83e0a727608b5db01-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/598b3e71ec378bd83e0a727608b5db01-Reviews.html},
 title = {Distributed Exploration in Multi-Armed Bandits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/598b3e71ec378bd83e0a727608b5db01-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_59c33016,
 abstract = {Multi-task prediction methods are widely used to couple regressors or classification models by sharing information across related tasks. We propose a multi-task Gaussian process approach for modeling both the relatedness between regressors and the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term in form of a sum of Kronecker products, for which efficient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics, we find substantial benefits of modeling structured noise compared to established alternatives.},
 author = {Rakitsch, Barbara and Lippert, Christoph and Borgwardt, Karsten and Stegle, Oliver},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/59c33016884a62116be975a9bb8257e3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/59c33016884a62116be975a9bb8257e3-Metadata.json},
 openalex = {W2139259843},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/59c33016884a62116be975a9bb8257e3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/59c33016884a62116be975a9bb8257e3-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/59c33016884a62116be975a9bb8257e3-Supplemental.zip},
 title = {It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/59c33016884a62116be975a9bb8257e3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5a4b25aa,
 abstract = {Inference in general Ising models is difficult, due to high treewidth making tree-based algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We find that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.},
 author = {Domke, Justin and Liu, Xianghang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5a4b25aaed25c2ee1b74de72dc03c14e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5a4b25aaed25c2ee1b74de72dc03c14e-Metadata.json},
 openalex = {W2962915628},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5a4b25aaed25c2ee1b74de72dc03c14e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5a4b25aaed25c2ee1b74de72dc03c14e-Supplemental.zip},
 title = {Projecting Ising Model Parameters for Fast Mixing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5b69b9cb,
 abstract = {We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem, by reformulating it as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd's K-means algorithm.},
 author = {Matsushita, Ryosuke and Tanaka, Toshiyuki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5b69b9cb83065d403869739ae7f0995e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5b69b9cb83065d403869739ae7f0995e-Metadata.json},
 openalex = {W2097538064},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5b69b9cb83065d403869739ae7f0995e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5b69b9cb83065d403869739ae7f0995e-Supplemental.zip},
 title = {Low-rank matrix reconstruction and clustering via approximate message passing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5b69b9cb83065d403869739ae7f0995e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5c572eca,
 abstract = {In this paper we address the problem of estimating the ratio $\frac{q}{p}$ where $p$ is a density function and $q$ is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration, in particular, when one needs to average a function with respect to one probability distribution, given a sample from another. It is often referred as {\it importance sampling} in statistical inference and is also closely related to the problem of {\it covariate shift} in transfer learning as well as to various MCMC methods. It may also be useful for separating the underlying geometry of a space, say a manifold, from the density function defined on it. Our approach is based on reformulating the problem of estimating $\frac{q}{p}$ as an inverse problem in terms of an integral operator corresponding to a kernel, and thus reducing it to an integral equation, known as the Fredholm problem of the first kind. This formulation, combined with the techniques of regularization and kernel methods, leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel in the case of densities defined on $\R^d$, compact domains in $\R^d$ and smooth $d$-dimensional sub-manifolds of the Euclidean space. We also show experimental results including applications to classification and semi-supervised learning within the covariate shift framework and demonstrate some encouraging experimental comparisons. We also show how the parameters of our algorithms can be chosen in a completely unsupervised manner.},
 author = {Que, Qichao and Belkin, Mikhail},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5c572eca050594c7bc3c36e7e8ab9550-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5c572eca050594c7bc3c36e7e8ab9550-Metadata.json},
 openalex = {W2119464254},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5c572eca050594c7bc3c36e7e8ab9550-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5c572eca050594c7bc3c36e7e8ab9550-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5c572eca050594c7bc3c36e7e8ab9550-Supplemental.zip},
 title = {Inverse Density as an Inverse Problem: The Fredholm Equation Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5c572eca050594c7bc3c36e7e8ab9550-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5caf41d6,
 abstract = {We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic block-model (MMSB). Our model integrates two basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on nine real-world networks with up to 60,000 nodes, and on simulated networks with degree distributions that follow a power law. We demonstrate that the AMP predicts significantly better than the MMSB.},
 author = {Gopalan, Prem K and Wang, Chong and Blei, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5caf41d62364d5b41a893adc1a9dd5d4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5caf41d62364d5b41a893adc1a9dd5d4-Metadata.json},
 openalex = {W2118085414},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5caf41d62364d5b41a893adc1a9dd5d4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5caf41d62364d5b41a893adc1a9dd5d4-Reviews.html},
 title = {Modeling Overlapping Communities with Node Popularities},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5caf41d62364d5b41a893adc1a9dd5d4-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5dd9db5e,
 abstract = {Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efficient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reflections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the benefits of our method on two image segmentation tasks.},
 author = {Jegelka, Stefanie and Bach, Francis and Sra, Suvrit},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Metadata.json},
 openalex = {W2134220221},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Reviews.html},
 title = {Reflection methods for user-friendly submodular optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5e1b18c4,
 abstract = {This paper addresses the problem of unsupervised feature learning for text data. Our method is grounded in the principle of minimum description length and uses a dictionary-based compression scheme to extract a succinct feature set. Specifically, our method finds a set of word k-grams that minimizes the cost of reconstructing the text losslessly. We formulate document compression as a binary optimization task and show how to solve it approximately via a sequence of reweighted linear programs that are efficient to solve and parallelizable. As our method is unsupervised, features may be extracted once and subsequently used in a variety of tasks. We demonstrate the performance of these features over a range of scenarios including unsupervised exploratory analysis and supervised text categorization. Our compressed feature space is two orders of magnitude smaller than the full k-gram space and matches the text categorization accuracy achieved in the full feature space. This dimensionality reduction not only results in faster training times, but it can also help elucidate structure in unsupervised learning tasks and reduce the amount of training data necessary for supervised learning.},
 author = {Paskov, Hristo S and West, Robert and Mitchell, John C and Hastie, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Metadata.json},
 openalex = {W2158717066},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e1b18c4c6a6d31695acbae3fd70ecc6-Supplemental.zip},
 title = {Compressive Feature Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5e1b18c4c6a6d31695acbae3fd70ecc6-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5e9f92a0,
 abstract = {We propose a compressed sensing (CS) calcium imaging framework for monitoring large neuronal populations, where we image randomized projections of the spatial calcium concentration at each timestep, instead of measuring the concentration at individual locations. We develop scalable nonnegative deconvolution methods for extracting the neuronal spike time series from such observations. We also address the problem of demixing the spatial locations of the neurons using rank-penalized matrix factorization methods. By exploiting the sparsity of neural spiking we demonstrate that the number of measurements needed per timestep is significantly smaller than the total number of neurons, a result that can potentially enable imaging of larger populations at considerably faster rates compared to traditional raster-scanning techniques. Unlike traditional CS setups, our problem involves a block-diagonal sensing matrix and a non-orthogonal sparse basis that spans multiple timesteps. We provide tight approximations to the number of measurements needed for perfect deconvolution for certain classes of spiking processes, and show that this number undergoes a phase transition, which we characterize using modern tools relating conic geometry to compressed sensing.},
 author = {Pnevmatikakis, Eftychios A and Paninski, Liam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e9f92a01c986bafcabbafd145520b13-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e9f92a01c986bafcabbafd145520b13-Metadata.json},
 openalex = {W2132067227},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e9f92a01c986bafcabbafd145520b13-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e9f92a01c986bafcabbafd145520b13-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5e9f92a01c986bafcabbafd145520b13-Supplemental.zip},
 title = {Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5e9f92a01c986bafcabbafd145520b13-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5ef0b4eb,
 abstract = {We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an Expectation-Maximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefficients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion.},
 author = {Todeschini, Adrien and Caron, Fran\c{c}ois and Chavent, Marie},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Metadata.json},
 openalex = {W2117091609},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5ef0b4eba35ab2d6180b0bca7e46b6f9-Supplemental.zip},
 title = {Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5ef0b4eba35ab2d6180b0bca7e46b6f9-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_5f2c22cb,
 abstract = {When a probabilistic model and its prior are given, Bayesian learning offers inference with automatic parameter tuning. However, Bayesian learning is often obstructed by computational difficulty: the rigorous Bayesian learning is intractable in many models, and its variational Bayesian (VB) approximation is prone to suffer from local minima. In this paper, we overcome this difficulty for low-rank subspace clustering (LRSC) by providing an exact global solver and its efficient approximation. LRSC extracts a low-dimensional structure of data by embedding samples into the union of low-dimensional subspaces, and its variational Bayesian variant has shown good performance. We first prove a key property that the VB-LRSC model is highly redundant. Thanks to this property, the optimization problem of VB-LRSC can be separated into small subproblems, each of which has only a small number of unknown variables. Our exact global solver relies on another key property that the stationary condition of each subproblem consists of a set of polynomial equations, which is solvable with the homotopy method. For further computational efficiency, we also propose an efficient approximate variant, of which the stationary condition can be written as a polynomial equation with a single variable. Experimental results show the usefulness of our approach.},
 author = {Nakajima, Shinichi and Takeda, Akiko and Babacan, S. Derin and Sugiyama, Masashi and Takeuchi, Ichiro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5f2c22cb4a5380af7ca75622a6426917-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5f2c22cb4a5380af7ca75622a6426917-Metadata.json},
 openalex = {W2164686043},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5f2c22cb4a5380af7ca75622a6426917-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/5f2c22cb4a5380af7ca75622a6426917-Reviews.html},
 title = {Global Solver and Its Efficient Approximation for Variational Bayesian Low-rank Subspace Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/5f2c22cb4a5380af7ca75622a6426917-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_60815949,
 abstract = {We propose to train an ensemble with the help of a reservoir in which the learning algorithm can store a limited number of samples.

This novel approach lies in the area between offline and online ensemble approaches and can be seen either as a restriction of the former or an enhancement of the latter. We identify some basic strategies that can be used to populate this reservoir and present our main contribution, dubbed Greedy Edge Expectation Maximization (GEEM), that maintains the reservoir content in the case of Boosting by viewing the samples through their projections into the weak classifier response space. We propose an efficient algorithmic implementation which makes it tractable in practice, and demonstrate its efficiency experimentally on several compute-vision data-sets, on which it outperforms both online and offline methods in a memory constrained setting.},
 author = {Lefakis, Leonidas and Fleuret, Fran\c{c}ois},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6081594975a764c8e3a691fa2b3a321d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6081594975a764c8e3a691fa2b3a321d-Metadata.json},
 openalex = {W2099664102},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6081594975a764c8e3a691fa2b3a321d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6081594975a764c8e3a691fa2b3a321d-Reviews.html},
 title = {Reservoir Boosting : Between Online and Offline Ensemble Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6081594975a764c8e3a691fa2b3a321d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_621bf66d,
 abstract = {We propose a fast algorithm for ridge regression when the number of features is much larger than the number of observations (p ≫ n). The standard way to solve ridge regression in this setting works in the dual space and gives a running time of O(n2p). Our algorithm Subsampled Randomized Hadamard Transform- Dual Ridge Regression (SRHT-DRR) runs in time O(np log(n)) and works by preconditioning the design matrix by a Randomized Walsh-Hadamard Transform with a subsequent subsampling of features. We provide risk bounds for our SRHT-DRR algorithm in the fixed design setting and show experimental results on synthetic and real datasets.},
 author = {Lu, Yichao and Dhillon, Paramveer and Foster, Dean P and Ungar, Lyle},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/621bf66ddb7c962aa0d22ac97d69b793-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/621bf66ddb7c962aa0d22ac97d69b793-Metadata.json},
 openalex = {W2135921993},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/621bf66ddb7c962aa0d22ac97d69b793-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/621bf66ddb7c962aa0d22ac97d69b793-Supplemental.zip},
 title = {Faster Ridge Regression via the Subsampled Randomized Hadamard Transform},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/621bf66ddb7c962aa0d22ac97d69b793-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_647bba34,
 abstract = {Seriation seeks to reconstruct a linear order between variables using unsorted, pairwise similarity information. It has direct applications in archeology and shotgun gene sequencing for example. We write seriation as an optimization problem by proving the equivalence between the seriation and combinatorial 2-SUM problems on similarity matrices (2-SUM is a quadratic minimization problem over permutations). The seriation problem can be solved exactly by a spectral algorithm in the noiseless case and we derive several convex relaxations for 2-SUM to improve the robustness of seriation solutions in noisy settings. These convex relaxations also allow us to impose structural constraints on the solution, hence solve semi-supervised seriation problems. We derive new approximation bounds for some of these relaxations and present numerical experiments on archeological data, Markov chains and DNA assembly from shotgun gene sequencing data.},
 author = {Fogel, Fajwel and Jenatton, Rodolphe and Bach, Francis and D\textquotesingle Aspremont, Alexandre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/647bba344396e7c8170902bcf2e15551-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/647bba344396e7c8170902bcf2e15551-Metadata.json},
 openalex = {W2951512459},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/647bba344396e7c8170902bcf2e15551-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/647bba344396e7c8170902bcf2e15551-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/647bba344396e7c8170902bcf2e15551-Supplemental.zip},
 title = {Convex Relaxations for Permutation Problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/647bba344396e7c8170902bcf2e15551-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_65cc2c82,
 abstract = {This paper addresses the problem of online learning in a dynamic setting. We consider a social network in which each individual observes a private signal about the underlying state of the world and communicates with her neighbors at each time period. Unlike many existing approaches, the underlying state is dynamic, and evolves according to a geometric random walk. We view the scenario as an optimization problem where agents aim to learn the true state while suffering the smallest possible loss. Based on the decomposition of the global loss function, we introduce two update mechanisms, each of which generates an estimate of the true state. We establish a tight bound on the rate of change of the underlying state, under which individuals can track the parameter with a bounded variance. Then, we characterize explicit expressions for the steady state mean-square deviation(MSD) of the estimates from the truth, per individual. We observe that only one of the estimators recovers the optimal MSD, which underscores the impact of the objective function decomposition on the learning quality. Finally, we provide an upper bound on the regret of the proposed methods, measured as an average of errors in estimating the parameter in a finite time.},
 author = {Shahrampour, Shahin and Rakhlin, Sasha and Jadbabaie, Ali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/65cc2c8205a05d7379fa3a6386f710e1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/65cc2c8205a05d7379fa3a6386f710e1-Metadata.json},
 openalex = {W2950435480},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/65cc2c8205a05d7379fa3a6386f710e1-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/65cc2c8205a05d7379fa3a6386f710e1-Supplemental.zip},
 title = {Online Learning of Dynamic Parameters in Social Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/65cc2c8205a05d7379fa3a6386f710e1-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_678a1491,
 abstract = {We give a polynomial-time algorithm for provably learning the structure and parameters of bipartite noisy-or Bayesian networks of binary variables where the top layer is completely hidden. Unsupervised learning of these models is a form of discrete factor analysis, enabling the discovery of hidden variables and their causal relationships with observed data. We obtain an efficient learning algorithm for a family of Bayesian networks that we call quartet-learnable. For each latent variable, the existence of a singly-coupled quartet allows us to uniquely identify and learn all parameters involving that latent variable. We give a proof of the polynomial sample complexity of our learning algorithm, and experimentally compare it to variational EM.},
 author = {Jernite, Yacine and Halpern, Yonatan and Sontag, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/678a1491514b7f1006d605e9161946b1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/678a1491514b7f1006d605e9161946b1-Metadata.json},
 openalex = {W2113046264},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/678a1491514b7f1006d605e9161946b1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/678a1491514b7f1006d605e9161946b1-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/678a1491514b7f1006d605e9161946b1-Supplemental.zip},
 title = {Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/678a1491514b7f1006d605e9161946b1-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_67d16d00,
 abstract = {The estimation of dependencies between multiple variables is a central problem in the analysis of financial time series. A common approach is to express these dependencies in terms of a copula function. Typically the copula function is assumed to be constant but this may be inaccurate when there are covariates that could have a large influence on the dependence structure of the data. To account for this, a Bayesian framework for the estimation of conditional copulas is proposed. In this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables. We evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other time-varying copula methods.},
 author = {Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Lloyd, James R and Hern\'{a}ndez-Lobato, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/67d16d00201083a2b118dd5128dd6f59-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/67d16d00201083a2b118dd5128dd6f59-Metadata.json},
 openalex = {W2123234703},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/67d16d00201083a2b118dd5128dd6f59-Reviews.html},
 title = {Gaussian Process Conditional Copulas with Applications to Financial Time Series},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/67d16d00201083a2b118dd5128dd6f59-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_67d96d45,
 abstract = {Typical blur from camera shake often deviates from the standard uniform convolutional assumption, in part because of problematic rotations which create greater blurring away from some unknown center point. Consequently, successful blind deconvolution for removing shake artifacts requires the estimation of a spatially-varying or non-uniform blur operator. Using ideas from Bayesian inference and convex analysis, this paper derives a simple non-uniform blind deblurring algorithm with a spatially-adaptive image penalty. Through an implicit normalization process, this penalty automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted. Remaining regions with modest blur and revealing edges therefore dominate on average without explicitly incorporating structure-selection heuristics. The algorithm can be implemented using an optimization strategy that is virtually tuning-parameter free and simpler than existing methods, and likely can be applied in other settings such as dictionary learning. Detailed theoretical analysis and empirical comparisons on real images serve as validation.},
 author = {Zhang, Haichao and Wipf, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/67d96d458abdef21792e6d8e590244e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/67d96d458abdef21792e6d8e590244e7-Metadata.json},
 openalex = {W2125566565},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/67d96d458abdef21792e6d8e590244e7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/67d96d458abdef21792e6d8e590244e7-Supplemental.zip},
 title = {Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/67d96d458abdef21792e6d8e590244e7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_68053af2,
 abstract = {We study the problem of online learning in finite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret defined as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a finite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2√L|X||A|T log (|X||A|/L) in the bandit setting and 2L √T log(|X||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be significantly improved under general assumptions.},
 author = {Zimin, Alexander and Neu, Gergely},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68053af2923e00204c3ca7c6a3150cf7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68053af2923e00204c3ca7c6a3150cf7-Metadata.json},
 openalex = {W2150234726},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68053af2923e00204c3ca7c6a3150cf7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68053af2923e00204c3ca7c6a3150cf7-Supplemental.zip},
 title = {Online Learning in Episodic Markovian Decision Processes by Relative Entropy Policy Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_68a83eeb,
 abstract = {The receptive field (RF) of a sensory neuron describes how the neuron integrates sensory stimuli over time and space. In typical experiments with naturalistic or flickering spatiotemporal stimuli, RFs are very high-dimensional, due to the large number of coefficients needed to specify an integration profile across time and space. Estimating these coefficients from small amounts of data poses a variety of challenging statistical and computational problems. Here we address these challenges by developing Bayesian reduced rank regression methods for RF estimation. This corresponds to modeling the RF as a sum of space-time separable (i.e., rank-1) filters. This approach substantially reduces the number of parameters needed to specify the RF, from 1K-10K down to mere 100s in the examples we consider, and confers substantial benefits in statistical power and computational efficiency. We introduce a novel prior over low-rank RFs using the restriction of a matrix normal prior to the manifold of low-rank matrices, and use row and column covariances to obtain sparse, smooth, localized estimates of the spatial and temporal RF components. We develop two methods for inference in the resulting hierarchical model: (1) a fully Bayesian method using blocked-Gibbs sampling; and (2) a fast, approximate method that employs alternating ascent of conditional marginal likelihoods. We develop these methods for Gaussian and Poisson noise models, and show that low-rank estimates substantially outperform full rank estimates using neural data from retina and V1.},
 author = {Park, Mijung and Pillow, Jonathan W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68a83eeb494a308fe5295da69428a507-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68a83eeb494a308fe5295da69428a507-Metadata.json},
 openalex = {W2170449570},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68a83eeb494a308fe5295da69428a507-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68a83eeb494a308fe5295da69428a507-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/68a83eeb494a308fe5295da69428a507-Supplemental.zip},
 title = {Bayesian inference for low rank spatiotemporal neural receptive fields},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/68a83eeb494a308fe5295da69428a507-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_692f93be,
 abstract = {We consider energy minimization for undirected graphical models, also known as the MAP-inference problem for Markov random fields. Although combinatorial methods, which return a provably optimal integral solution of the problem, made a significant progress in the past decade, they are still typically unable to cope with large-scale datasets. On the other hand, large scale datasets are often defined on sparse graphs and convex relaxation methods, such as linear programming relaxations then provide good approximations to integral solutions.

We propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem. Based on the information obtained from the solution of the convex relaxation, our method confines application of the combinatorial solver to a small fraction of the initial graphical model, which allows to optimally solve much larger problems. We demonstrate the efficacy of our approach on a computer vision energy minimization benchmark.},
 author = {Savchynskyy, Bogdan and Kappes, J\"{o}rg Hendrik and Swoboda, Paul and Schn\"{o}rr, Christoph},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/692f93be8c7a41525c0baf2076aecfb4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/692f93be8c7a41525c0baf2076aecfb4-Metadata.json},
 openalex = {W2144164641},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/692f93be8c7a41525c0baf2076aecfb4-Reviews.html},
 title = {Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/692f93be8c7a41525c0baf2076aecfb4-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_69a5b599,
 abstract = {We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods.},
 author = {Kiraly, Franz and Theran, Louis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69a5b5995110b36a9a347898d97a610e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69a5b5995110b36a9a347898d97a610e-Metadata.json},
 openalex = {W2964048930},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69a5b5995110b36a9a347898d97a610e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69a5b5995110b36a9a347898d97a610e-Supplemental.zip},
 title = {Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/69a5b5995110b36a9a347898d97a610e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_69adc1e1,
 abstract = {Randomized decision trees and forests have a rich history in machine learning and have seen considerable success in application, perhaps particularly so for computer vision. However, they face a fundamental limitation: given enough data, the number of nodes in decision trees will grow exponentially with depth. For certain applications, for example on mobile or embedded processors, memory is a limited resource, and so the exponential growth of trees limits their depth, and thus their potential accuracy. This paper proposes decision jungles, revisiting the idea of ensembles of rooted decision directed acyclic graphs (DAGs), and shows these to be compact and powerful discriminative models for classification. Unlike conventional decision trees that only allow one path to every node, a DAG in a decision jungle allows multiple paths from the root to each leaf. We present and compare two new node merging algorithms that jointly optimize both the features and the structure of the DAGs efficiently. During training, node splitting and node merging are driven by the minimization of exactly the same objective function, here the weighted sum of entropies at the leaves. Results on varied datasets show that, compared to decision forests and several other baselines, decision jungles require dramatically less memory while considerably improving generalization.},
 author = {Shotton, Jamie and Sharp, Toby and Kohli, Pushmeet and Nowozin, Sebastian and Winn, John and Criminisi, Antonio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69adc1e107f7f7d035d7baf04342e1ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69adc1e107f7f7d035d7baf04342e1ca-Metadata.json},
 openalex = {W2098458263},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69adc1e107f7f7d035d7baf04342e1ca-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/69adc1e107f7f7d035d7baf04342e1ca-Supplemental.zip},
 title = {Decision Jungles: Compact and Rich Models for Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6a10bbd4,
 abstract = {In large-scale applications of undirected graphical models, such as social networks and biological networks, similar patterns occur frequently and give rise to similar parameters. In this situation, it is beneficial to group the parameters for more efficient learning. We show that even when the grouping is unknown, we can infer these parameter groups during learning via a Bayesian approach. We impose a Dirichlet process prior on the parameters. Posterior inference usually involves calculating intractable terms, and we propose two approximation algorithms, namely a Metropolis-Hastings algorithm with auxiliary variables and a Gibbs sampling algorithm with "stripped" Beta approximation (Gibbs_SBA). Simulations show that both algorithms outperform conventional maximum likelihood estimation (MLE). Gibbs_SBA's performance is close to Gibbs sampling with exact likelihood calculation. Models learned with Gibbs_SBA also generalize better than the models learned by MLE on real-world Senate voting data.},
 author = {Liu, Jie and Page, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a10bbd480e4c5573d8f3af73ae0454b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a10bbd480e4c5573d8f3af73ae0454b-Metadata.json},
 openalex = {W2130045679},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a10bbd480e4c5573d8f3af73ae0454b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a10bbd480e4c5573d8f3af73ae0454b-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a10bbd480e4c5573d8f3af73ae0454b-Supplemental.zip},
 title = {Bayesian Estimation of Latently-grouped Parameters in Undirected Graphical Models.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6a10bbd480e4c5573d8f3af73ae0454b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6a5889bb,
 abstract = {Most provably-efficient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an O(τS/√AT) bound on expected regret, where T is time, τ is the episode length and S and A are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.},
 author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a5889bb0190d0211a991f47bb19a777-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a5889bb0190d0211a991f47bb19a777-Metadata.json},
 openalex = {W2111764152},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a5889bb0190d0211a991f47bb19a777-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a5889bb0190d0211a991f47bb19a777-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6a5889bb0190d0211a991f47bb19a777-Supplemental.zip},
 title = {More) Efficient Reinforcement Learning via Posterior Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6a5889bb0190d0211a991f47bb19a777-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6c14da10,
 abstract = {How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects' choices, on a trial-to-trial basis, are best captured by a forgetful Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. This model accounts for subjects' trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, e-greedy and win-stay-lose-shift. It has the added benefit of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are significantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment.},
 author = {Zhang, Shunan and Yu, Angela J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6c14da109e294d1e8155be8aa4b1ce8e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6c14da109e294d1e8155be8aa4b1ce8e-Metadata.json},
 openalex = {W2111019584},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6c14da109e294d1e8155be8aa4b1ce8e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6c14da109e294d1e8155be8aa4b1ce8e-Supplemental.zip},
 title = {Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6c14da109e294d1e8155be8aa4b1ce8e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6d0f8463,
 abstract = {We study PCA as a stochastic optimization problem and propose a novel stochastic approximation algorithm which we refer to as "Matrix Stochastic Gradient" (MSG), as well as a practical variant, Capped MSG. We study the method both theoretically and empirically.},
 author = {Arora, Raman and Cotter, Andy and Srebro, Nati},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d0f846348a856321729a2f36734d1a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d0f846348a856321729a2f36734d1a7-Metadata.json},
 openalex = {W2166021275},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d0f846348a856321729a2f36734d1a7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d0f846348a856321729a2f36734d1a7-Supplemental.zip},
 title = {Stochastic Optimization of PCA with Capped MSG},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6d0f846348a856321729a2f36734d1a7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6d70cb65,
 abstract = {We consider the problem of sampling from a probability distribution defined over a high-dimensional discrete set, specified for instance by a graphical model. We propose a sampling algorithm, called PAWS, based on embedding the set into a higher-dimensional space which is then randomly projected using universal hash functions to a lower-dimensional subspace and explored using combinatorial search methods. Our scheme can leverage fast combinatorial optimization tools as a blackbox and, unlike MCMC methods, samples produced are guaranteed to be within an (arbitrarily small) constant factor of the true probability distribution. We demonstrate that by using state-of-the-art combinatorial search tools, PAWS can efficiently sample from Ising grids with strong interactions and from software verification instances, while MCMC and variational methods fail in both cases.},
 author = {Ermon, Stefano and Gomes, Carla P and Sabharwal, Ashish and Selman, Bart},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d70cb65d15211726dcce4c0e971e21c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d70cb65d15211726dcce4c0e971e21c-Metadata.json},
 openalex = {W2112904372},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d70cb65d15211726dcce4c0e971e21c-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6d70cb65d15211726dcce4c0e971e21c-Supplemental.zip},
 title = {Embed and Project: Discrete Sampling with Universal Hashing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6d70cb65d15211726dcce4c0e971e21c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6da9003b,
 abstract = {How does neural population process sensory information? Optimal coding theories assume that neural tuning curves are adapted to the prior distribution of the stimulus variable. Most of the previous work has discussed optimal solutions for only one-dimensional stimulus variables. Here, we expand some of these ideas and present new solutions that define optimal tuning curves for high-dimensional stimulus variables. We consider solutions for a minimal case where the number of neurons in the population is equal to the number of stimulus dimensions (diffeomorphic). In the case of two-dimensional stimulus variables, we analytically derive optimal solutions for different optimal criteria such as minimal L2 reconstruction error or maximal mutual information. For higher dimensional case, the learning rule to improve the population code is provided.},
 author = {Wang, Zhuo and Stocker, Alan A and Lee, Daniel D},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6da9003b743b65f4c0ccd295cc484e57-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6da9003b743b65f4c0ccd295cc484e57-Metadata.json},
 openalex = {W2138505967},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6da9003b743b65f4c0ccd295cc484e57-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6da9003b743b65f4c0ccd295cc484e57-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6da9003b743b65f4c0ccd295cc484e57-Supplemental.zip},
 title = {Optimal Neural Population Codes for High-dimensional Stimulus Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6da9003b743b65f4c0ccd295cc484e57-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6e0721b2,
 abstract = {We consider the problem of selecting non-zero entries of a matrix $A$ in order to produce a sparse sketch of it, $B$, that minimizes $\|A-B\|_2$. For large $m \times n$ matrices, such that $n \gg m$ (for example, representing $n$ observations over $m$ attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding $A$. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with $O(1)$ computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution. Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.},
 author = {Achlioptas, Dimitris and Karnin, Zohar S and Liberty, Edo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e0721b2c6977135b916ef286bcb49ec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e0721b2c6977135b916ef286bcb49ec-Metadata.json},
 openalex = {W2168274347},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e0721b2c6977135b916ef286bcb49ec-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e0721b2c6977135b916ef286bcb49ec-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e0721b2c6977135b916ef286bcb49ec-Supplemental.zip},
 title = {Near-Optimal Entrywise Sampling for Data Matrices},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6e0721b2c6977135b916ef286bcb49ec-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6e2713a6,
 abstract = {The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX, Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difficult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions.},
 author = {Wauthier, Fabian L and Jojic, Nebojsa and Jordan, Michael I},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e2713a6efee97bacb63e52c54f0ada0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e2713a6efee97bacb63e52c54f0ada0-Metadata.json},
 openalex = {W2158471709},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e2713a6efee97bacb63e52c54f0ada0-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e2713a6efee97bacb63e52c54f0ada0-Supplemental.zip},
 title = {A Comparative Framework for Preconditioned Lasso Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6e7d2da6,
 abstract = {Probabilistic models for binary spike patterns provide a powerful tool for understanding the statistical dependencies in large-scale neural recordings. Maximum entropy (or maxent) models, which seek to explain dependencies in terms of low-order interactions between neurons, have enjoyed remarkable success in modeling such patterns, particularly for small groups of neurons. However, these models are computationally intractable for large populations, and low-order max-ent models have been shown to be inadequate for some datasets. To overcome these limitations, we propose a family of models for binary spike patterns, where universality refers to the ability to model arbitrary distributions over all 2m binary patterns. We construct universal models using a Dirichlet process centered on a well-behaved parametric base measure, which naturally combines the flexibility of a histogram and the parsimony of a parametric model. We derive computationally efficient inference methods using Bernoulli and cascaded logistic base measures, which scale tractably to large populations. We also establish a condition for equivalence between the cascaded logistic and the 2nd-order maxent or Ising model, making cascaded logistic a reasonable choice for base measure in a universal model. We illustrate the performance of these models using neural data.},
 author = {Park, Il Memming and Archer, Evan W and Latimer, Kenneth and Pillow, Jonathan W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e7d2da6d3953058db75714ac400b584-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e7d2da6d3953058db75714ac400b584-Metadata.json},
 openalex = {W2110223115},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e7d2da6d3953058db75714ac400b584-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6e7d2da6d3953058db75714ac400b584-Supplemental.zip},
 title = {Universal models for binary spike patterns using centered Dirichlet processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6e7d2da6d3953058db75714ac400b584-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_6f3ef77a,
 abstract = {We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the first time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We first investigated encodings learned by the model using artificial data with mutually occluding components. We find that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive fields associated with the model's hidden units. We find many Gabor-like or globular receptive fields as well as fields sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efficiently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex.},
 author = {Dai, Zhenwen and Exarchakis, Georgios and L\"{u}cke, J\"{o}rg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6f3ef77ac0e3619e98159e9b6febf557-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6f3ef77ac0e3619e98159e9b6febf557-Metadata.json},
 openalex = {W2128531008},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6f3ef77ac0e3619e98159e9b6febf557-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/6f3ef77ac0e3619e98159e9b6febf557-Supplemental.zip},
 title = {What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/6f3ef77ac0e3619e98159e9b6febf557-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_705f2172,
 abstract = {It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate.},
 author = {Savin, Cristina and Dayan, Peter and Lengyel, Mate},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/705f2172834666788607efbfca35afb3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/705f2172834666788607efbfca35afb3-Metadata.json},
 openalex = {W2134876084},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/705f2172834666788607efbfca35afb3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/705f2172834666788607efbfca35afb3-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/705f2172834666788607efbfca35afb3-Supplemental.zip},
 title = {Correlations strike back (again): the case of associative memory retrieval},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/705f2172834666788607efbfca35afb3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_71f6278d,
 abstract = {Dropout is a relatively new algorithm for training neural networks which relies on stochastically dropping out neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function.},
 author = {Baldi, Pierre and Sadowski, Peter J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/71f6278d140af599e06ad9bf1ba03cb0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/71f6278d140af599e06ad9bf1ba03cb0-Metadata.json},
 openalex = {W2294567968},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/71f6278d140af599e06ad9bf1ba03cb0-Reviews.html},
 title = {Understanding Dropout},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7380ad8a,
 abstract = {In this paper, we propose a new and computationally efficient framework for learning sparse models. We formulate a unified approach that contains as particular cases models promoting sparse synthesis and analysis type of priors, and mixtures thereof. The supervised training of the proposed model is formulated as a bilevel optimization problem, in which the operators are optimized to achieve the best possible performance on a specific task, e.g., reconstruction or classification. By restricting the operators to be shift invariant, our approach can be thought as a way of learning analysis+synthesis sparsity-promoting convolutional operators. Leveraging recent ideas on fast trainable regressors designed to approximate exact sparse codes, we propose a way of constructing feed-forward neural networks capable of approximating the learned models at a fraction of the computational cost of exact solvers. In the shift-invariant case, this leads to a principled way of constructing task-specific convolutional networks. We illustrate the proposed models on several experiments in music analysis and image processing applications.},
 author = {Sprechmann, Pablo and Litman, Roee and Ben Yakar, Tal and Bronstein, Alexander M and Sapiro, Guillermo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7380ad8a673226ae47fce7bff88e9c33-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7380ad8a673226ae47fce7bff88e9c33-Metadata.json},
 openalex = {W2162221686},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7380ad8a673226ae47fce7bff88e9c33-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7380ad8a673226ae47fce7bff88e9c33-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7380ad8a673226ae47fce7bff88e9c33-Supplemental.zip},
 title = {Supervised Sparse Analysis and Synthesis Operators},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7380ad8a673226ae47fce7bff88e9c33-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_74071a67,
 abstract = {Performance guarantees for online learning algorithms typically take the form of regret bounds, which express that the cumulative loss overhead compared to the best expert in hindsight is small. In the common case of large but structured expert sets we typically wish to keep the regret especially small compared to simple experts, at the cost of modest additional overhead compared to more complex others. We study which such regret trade-offs can be achieved, and how.

We analyse regret w.r.t. each individual expert as a multi-objective criterion in the simple but fundamental case of absolute loss. We characterise the achievable and Pareto optimal trade-offs, and the corresponding optimal strategies for each sample size both exactly for each finite horizon and asymptotically.},
 author = {Koolen, Wouter M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/74071a673307ca7459bcf75fbd024e09-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/74071a673307ca7459bcf75fbd024e09-Metadata.json},
 openalex = {W2137107147},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/74071a673307ca7459bcf75fbd024e09-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/74071a673307ca7459bcf75fbd024e09-Supplemental.zip},
 title = {The Pareto Regret Frontier},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/74071a673307ca7459bcf75fbd024e09-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7504adad,
 abstract = {Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A look at the literature of this game shows that while ADP algorithms that have been (almost) entirely based on approximating the value function (value function based) have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classification-based modified policy iteration (CBMPI), to the game of Tetris. Our experimental results show that for the first time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small 10 x 10 and large 10 x 20 boards. Although the CBMPI's results are similar to those of the CE method in the large board, CBMPI uses considerably fewer (almost 1/6) samples (calls to the generative model) than CE.},
 author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Scherrer, Bruno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7504adad8bb96320eb3afdd4df6e1f60-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7504adad8bb96320eb3afdd4df6e1f60-Metadata.json},
 openalex = {W2133435356},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7504adad8bb96320eb3afdd4df6e1f60-Reviews.html},
 title = {Approximate Dynamic Programming Finally Performs Well in the Game of Tetris},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7504adad8bb96320eb3afdd4df6e1f60-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_75fc093c,
 abstract = {A probabilistic model based on the horseshoe prior is proposed for learning dependencies in the process of identifying relevant features for prediction. Exact inference is intractable in this model. However, expectation propagation offers an approximate alternative. Because the process of estimating feature selection dependencies may suffer from over-fitting in the model proposed, additional data from a multi-task learning scenario are considered for induction. The same model can be used in this setting with few modifications. Furthermore, the assumptions made are less restrictive than in other multi-task methods: The different tasks must share feature selection dependencies, but can have different relevant features and model coefficients. Experiments with real and synthetic data show that this model performs better than other multi-task alternatives from the literature. The experiments also show that the model is able to induce suitable feature selection dependencies for the problems considered, only from the training data.},
 author = {Hern\'{a}ndez-Lobato, Daniel and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/75fc093c0ee742f6dddaa13fff98f104-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/75fc093c0ee742f6dddaa13fff98f104-Metadata.json},
 openalex = {W2149917474},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/75fc093c0ee742f6dddaa13fff98f104-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/75fc093c0ee742f6dddaa13fff98f104-Supplemental.zip},
 title = {Learning Feature Selection Dependencies in Multi-task Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/75fc093c0ee742f6dddaa13fff98f104-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7634ea65,
 abstract = {I present a new online learning algorithm that extends the exponentiated gradient framework to infinite dimensional spaces. My analysis shows that the algorithm is implicitly able to estimate the L2 norm of the unknown competitor, U, achieving a regret bound of the order of O(U log(U T + 1))√T), instead of the standard O((U2 + 1)√T), achievable without knowing U. For this analysis, I introduce novel tools for algorithms with time-varying regularizers, through the use of local smoothness. Through a lower bound, I also show that the algorithm is optimal up to √log(UT) term for linear and Lipschitz losses.},
 author = {Orabona, Francesco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7634ea65a4e6d9041cfd3f7de18e334a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7634ea65a4e6d9041cfd3f7de18e334a-Metadata.json},
 openalex = {W2116606500},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7634ea65a4e6d9041cfd3f7de18e334a-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7634ea65a4e6d9041cfd3f7de18e334a-Supplemental.zip},
 title = {Dimension-Free Exponentiated Gradient},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_76cf99d3,
 abstract = {We consider streaming, one-pass principal component analysis (PCA), in the high-dimensional regime, with limited memory. Here, $p$-dimensional samples are presented sequentially, and the goal is to produce the $k$-dimensional subspace that best approximates these points. Standard algorithms require $O(p^2)$ memory; meanwhile no algorithm can do better than $O(kp)$ memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the {\em spiked covariance model}, where $p$-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, $n$, scales proportionally with the dimension, $p$. Yet, all algorithms that provably achieve this, have memory complexity $O(p^2)$. Meanwhile, algorithms with memory-complexity $O(kp)$ do not have provable bounds on sample complexity comparable to $p$. We present an algorithm that achieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is able to compute the $k$-dimensional spike with $O(p \log p)$ sample-complexity -- the first algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data.},
 author = {Mitliagkas, Ioannis and Caramanis, Constantine and Jain, Prateek},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/76cf99d3614e23eabab16fb27e944bf9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/76cf99d3614e23eabab16fb27e944bf9-Metadata.json},
 openalex = {W2953310052},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/76cf99d3614e23eabab16fb27e944bf9-Reviews.html},
 title = {Memory Limited, Streaming PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/76cf99d3614e23eabab16fb27e944bf9-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7810ccd4,
 author = {Ma, Yifei and Garnett, Roman and Schneider, Jeff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7810ccd41bf26faaa2c4e1f20db70a71-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7810ccd41bf26faaa2c4e1f20db70a71-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7810ccd41bf26faaa2c4e1f20db70a71-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7810ccd41bf26faaa2c4e1f20db70a71-Reviews.html},
 title = {\Sigma -Optimality for Active Learning on Gaussian Random Fields},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7810ccd41bf26faaa2c4e1f20db70a71-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7895fc13,
 abstract = {Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-filter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be fit exactly and quickly. The CGLM can also be seen as a generalisation of a low-rank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data.},
 author = {Pachitariu, Marius and Petreska, Biljana and Sahani, Maneesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7895fc13088ee37f511913bac71fa66f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7895fc13088ee37f511913bac71fa66f-Metadata.json},
 openalex = {W2169269711},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7895fc13088ee37f511913bac71fa66f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7895fc13088ee37f511913bac71fa66f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7895fc13088ee37f511913bac71fa66f-Supplemental.zip},
 title = {Recurrent linear models of simultaneously-recorded neural populations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7895fc13088ee37f511913bac71fa66f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7940ab47,
 abstract = {Lifted inference algorithms exploit symmetries in probabilistic models to speed up inference. They show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities. The reason is that conditioning on evidence breaks many of the model's symmetries, which can preempt standard lifting techniques. Recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is #P-hard, suggesting that no lifting is to be expected in the worst case. In this paper, we balance this negative result by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference. In particular, we show that conditioning on binary evidence with bounded Boolean rank is efficient. This opens up the possibility of approximating evidence by a low-rank Boolean matrix factorization, which we investigate both theoretically and empirically.},
 author = {Van den Broeck, Guy and Darwiche, Adnan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7940ab47468396569a906f75ff3f20ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7940ab47468396569a906f75ff3f20ef-Metadata.json},
 openalex = {W2138164508},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7940ab47468396569a906f75ff3f20ef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7940ab47468396569a906f75ff3f20ef-Reviews.html},
 title = {On the Complexity and Approximation of Binary Evidence in Lifted Inference},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7940ab47468396569a906f75ff3f20ef-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7a53928f,
 abstract = {The goal of unsupervised feature selection is to identify a small number of important features that can represent the data. We propose a new algorithm, a modification of the classical pivoted QR algorithm of Businger and Golub, that requires a small number of passes over the data. The improvements are based on two ideas: keeping track of multiple features in each pass, and skipping calculations that can be shown not to affect the final selection. Our algorithm selects the exact same features as the classical pivoted QR algorithm, and has the same favorable numerical stability. We describe experiments on real-world datasets which sometimes show improvements of several orders of magnitude over the classical algorithm. These results appear to be competitive with recently proposed randomized algorithms in terms of pass efficiency and run time. On the other hand, the randomized algorithms may produce more accurate features, at the cost of small probability of failure.},
 author = {Maung, Crystal and Schweitzer, Haim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7a53928fa4dd31e82c6ef826f341daec-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7a53928fa4dd31e82c6ef826f341daec-Metadata.json},
 openalex = {W2129314216},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7a53928fa4dd31e82c6ef826f341daec-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7a53928fa4dd31e82c6ef826f341daec-Reviews.html},
 title = {Pass-efficient unsupervised feature selection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7a53928fa4dd31e82c6ef826f341daec-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7b5b23f4,
 abstract = {Recently, it was shown that deep neural networks can perform very well if the activities of hidden units are regularized during learning, e.g, by randomly dropping out 50% of their activities. We describe a method called 'standout' in which a binary belief network is overlaid on a neural network and is used to regularize of its hidden units by selectively setting activities to zero. This 'adaptive dropout network' can be trained jointly with the neural network by approximately computing local expectations of binary dropout variables, computing derivatives using back-propagation, and using stochastic gradient descent. Interestingly, experiments show that the learnt dropout network parameters recapitulate the neural network parameters, suggesting that a good dropout network regularizes activities according to magnitude. When evaluated on the MNIST and NORB datasets, we found that our method achieves lower classification error rates than other feature learning methods, including standard dropout, denoising auto-encoders, and restricted Boltzmann machines. For example, our method achieves 0.80% and 5.8% errors on the MNIST and NORB test sets, which is better than state-of-the-art results obtained using feature learning methods, including those that use convolutional architectures.},
 author = {Ba, Jimmy and Frey, Brendan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7b5b23f4aadf9513306bcd59afb6e4c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7b5b23f4aadf9513306bcd59afb6e4c9-Metadata.json},
 openalex = {W2136836265},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7b5b23f4aadf9513306bcd59afb6e4c9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7b5b23f4aadf9513306bcd59afb6e4c9-Reviews.html},
 title = {Adaptive dropout for training deep neural networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7b5b23f4aadf9513306bcd59afb6e4c9-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7bb06076,
 abstract = {This paper examines the question: What kinds of distributions can be efficiently represented by Restricted Boltzmann Machines (RBMs)? We characterize the RBM's unnormalized log-likelihood function as a type of neural network, and through a series of simulation results relate these networks to ones whose representational properties are better understood. We show the surprising result that RBMs can efficiently capture any distribution whose density depends on the number of 1's in their input. We also provide the first known example of a particular type of distribution that provably cannot be efficiently represented by an RBM, assuming a realistic exponential upper bound on the weights. By formally demonstrating that a relatively simple distribution cannot be represented efficiently by an RBM our results provide a new rigorous justification for the use of potentially more expressive generative models, such as deeper ones.},
 author = {Martens, James and Chattopadhya, Arkadev and Pitassi, Toni and Zemel, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bb060764a818184ebb1cc0d43d382aa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bb060764a818184ebb1cc0d43d382aa-Metadata.json},
 openalex = {W2126229007},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bb060764a818184ebb1cc0d43d382aa-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bb060764a818184ebb1cc0d43d382aa-Supplemental.zip},
 title = {On the Representational Efficiency of Restricted Boltzmann Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7bb060764a818184ebb1cc0d43d382aa-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7bcdf75a,
 abstract = {The efficiency of Brain-Computer Interfaces (BCI) largely depends upon a reliable extraction of informative features from the high-dimensional EEG signal. A crucial step in this protocol is the computation of spatial filters. The Common Spatial Patterns (CSP) algorithm computes filters that maximize the difference in band power between two conditions, thus it is tailored to extract the relevant information in motor imagery experiments. However, CSP is highly sensitive to artifacts in the EEG data, i.e. few outliers may alter the estimate drastically and decrease classification performance. Inspired by concepts from the field of information geometry we propose a novel approach for robustifying CSP. More precisely, we formulate CSP as a divergence maximization problem and utilize the property of a particular type of divergence, namely beta divergence, for robustifying the estimation of spatial filters in the presence of artifacts in the data. We demonstrate the usefulness of our method on toy data and on EEG recordings from 80 subjects.},
 author = {Samek, Wojciech and Blythe, Duncan and M\"{u}ller, Klaus-Robert and Kawanabe, Motoaki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bcdf75ad237b8e02e301f4091fb6bc8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bcdf75ad237b8e02e301f4091fb6bc8-Metadata.json},
 openalex = {W2163710753},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bcdf75ad237b8e02e301f4091fb6bc8-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7bcdf75ad237b8e02e301f4091fb6bc8-Supplemental.zip},
 title = {Robust Spatial Filtering with Beta Divergence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7bcdf75ad237b8e02e301f4091fb6bc8-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7cce53cf,
 abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model.},
 author = {Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Ranzato, Marc\textquotesingle Aurelio and Mikolov, Tomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7cce53cf90577442771720a370c3c723-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7cce53cf90577442771720a370c3c723-Metadata.json},
 openalex = {W2123024445},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7cce53cf90577442771720a370c3c723-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7cce53cf90577442771720a370c3c723-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7cce53cf90577442771720a370c3c723-Supplemental.zip},
 title = {DeViSE: A Deep Visual-Semantic Embedding Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7d771e0e,
 abstract = {This paper addresses the scalability of symbolic planning under uncertainty with factored states and actions. Our first contribution is a symbolic implementation of Modified Policy Iteration (MPI) for factored actions that views policy evaluation as policy-constrained value iteration (VI). Unfortunately, a naive approach to enforce policy constraints can lead to large memory requirements, sometimes making symbolic MPI worse than VI. We address this through our second and main contribution, symbolic Opportunistic Policy Iteration (OPI), which is a novel convergent algorithm lying between VI and MPI, that applies policy constraints if it does not increase the size of the value function representation, and otherwise performs VI backups. We also give a memory bounded version of this algorithm allowing a space-time tradeoff. Empirical results show significantly improved scalability over state-of-the-art symbolic planners.},
 author = {Raghavan, Aswin and Khardon, Roni and Fern, Alan and Tadepalli, Prasad},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7d771e0e8f3633ab54856925ecdefc5d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7d771e0e8f3633ab54856925ecdefc5d-Metadata.json},
 openalex = {W2131312351},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7d771e0e8f3633ab54856925ecdefc5d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7d771e0e8f3633ab54856925ecdefc5d-Reviews.html},
 title = {Symbolic Opportunistic Policy Iteration for Factored-Action MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7d771e0e8f3633ab54856925ecdefc5d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7f100b7b,
 abstract = {We present a novel non-parametric method for finding a subspace of stimulus features that contains all information about the response of a system. Our method generalizes similar approaches to this problem such as spike triggered average, spike triggered covariance, or maximally informative dimensions. Instead of maximizing the mutual information between features and responses directly, we use integral probability metrics in kernel Hilbert spaces to minimize the information between uninformative features and the combination of informative features and responses. Since estimators of these metrics access the data via kernels, are easy to compute, and exhibit good theoretical convergence properties, our method can easily be generalized to populations of neurons or spike patterns. By using a particular expansion of the mutual information, we can show that the informative features must contain all information if we can make the uninformative features independent of the rest.},
 author = {Sinz, Fabian and Stockl, Anna and Grewe, Jan and Benda, Jan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f100b7b36092fb9b06dfb4fac360931-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f100b7b36092fb9b06dfb4fac360931-Metadata.json},
 openalex = {W2130107494},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f100b7b36092fb9b06dfb4fac360931-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f100b7b36092fb9b06dfb4fac360931-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f100b7b36092fb9b06dfb4fac360931-Supplemental.zip},
 title = {Least Informative Dimensions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7f100b7b36092fb9b06dfb4fac360931-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7f24d240,
 abstract = {An incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses. To understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states. Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited memory capacity. This raises the fundamental question, how does synaptic complexity give rise to memory? To address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks. Moreover, in proving such theorems, we uncover a framework, based on first passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function.},
 author = {Lahiri, Subhaneil and Ganguli, Surya},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f24d240521d99071c93af3917215ef7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f24d240521d99071c93af3917215ef7-Metadata.json},
 openalex = {W2110563098},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f24d240521d99071c93af3917215ef7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f24d240521d99071c93af3917215ef7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f24d240521d99071c93af3917215ef7-Supplemental.zip},
 title = {A memory frontier for complex synapses},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7f24d240521d99071c93af3917215ef7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7f39f831,
 abstract = {We consider robust optimization for polynomial optimization problems where the uncertainty set is a set of candidate probability density functions. This set is a ball around a density function estimated from data samples, i.e., it is data-driven and random. Polynomial optimization problems are inherently hard due to nonconvex objectives and constraints. However, we show that by employing polynomial and histogram density estimates, we can introduce robustness with respect to distributional uncertainty sets without making the problem harder. We show that the optimum to the distributionally robust problem is the limit of a sequence of tractable semidefinite programming relaxations. We also give finite-sample consistency guarantees for the data-driven uncertainty sets. Finally, we apply our model and solution method in a water network optimization problem.},
 author = {Mevissen, Martin and Ragnoli, Emanuele and Yu, Jia Yuan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f39f8317fbdb1988ef4c628eba02591-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f39f8317fbdb1988ef4c628eba02591-Metadata.json},
 openalex = {W2170246608},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f39f8317fbdb1988ef4c628eba02591-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f39f8317fbdb1988ef4c628eba02591-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f39f8317fbdb1988ef4c628eba02591-Supplemental.zip},
 title = {Data-driven Distributionally Robust Polynomial Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7f39f8317fbdb1988ef4c628eba02591-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7f53f8c6,
 abstract = {We describe a class of algorithms for amortized inference in Bayesian networks. In this setting, we invest computation upfront to support rapid online inference for a wide range of queries. Our approach is based on learning an inverse factorization of a model's joint distribution: a factorization that turns observations into root nodes. Our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization. These stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly find a likely explanation. We show that estimated inverses converge asymptotically in number of (prior or posterior) training samples. To make use of inverses before convergence, we describe the Inverse MCMC algorithm, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler. We explore the efficiency of this sampler for a variety of parameter regimes and Bayes nets.},
 author = {Stuhlm\"{u}ller, Andreas and Taylor, Jacob and Goodman, Noah},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f53f8c6c730af6aeb52e66eb74d8507-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f53f8c6c730af6aeb52e66eb74d8507-Metadata.json},
 openalex = {W2098694378},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f53f8c6c730af6aeb52e66eb74d8507-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f53f8c6c730af6aeb52e66eb74d8507-Reviews.html},
 title = {Learning Stochastic Inverses},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7f53f8c6c730af6aeb52e66eb74d8507-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7f5d04d1,
 abstract = {Sparse high-dimensional data vectors are common in many application domains where a very large number of rarely non-zero features can be devised. Unfortunately, this creates a computational bottleneck for unsupervised feature learning algorithms such as those based on auto-encoders and RBMs, because they involve a reconstruction step where the whole input vector is predicted from the current feature values. An algorithm was recently developed to successfully handle the case of auto-encoders, based on an importance sampling scheme stochastically selecting which input elements to actually reconstruct during training for each particular example. To generalize this idea to RBMs, we propose a stochastic ratio-matching algorithm that inherits all the computational advantages and unbiasedness of the importance sampling scheme. We show that stochastic ratio matching is a good estimator, allowing the approach to beat the state-of-the-art on two bag-of-word text classification benchmarks (20 Newsgroups and RCV1), while keeping computational cost linear in the number of non-zeros.},
 author = {Dauphin, Yann and Bengio, Yoshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f5d04d189dfb634e6a85bb9d9adf21e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f5d04d189dfb634e6a85bb9d9adf21e-Metadata.json},
 openalex = {W2114960120},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f5d04d189dfb634e6a85bb9d9adf21e-Reviews.html},
 title = {Stochastic Ratio Matching of RBMs for Sparse High-Dimensional Inputs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7f5d04d189dfb634e6a85bb9d9adf21e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7f975a56,
 abstract = {This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by \cite{har2004coresets}, we reduce the problem of finding a clustering with low cost to the problem of finding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms.},
 author = {Balcan, Maria-Florina F and Ehrlich, Steven and Liang, Yingyu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f975a56c761db6506eca0b37ce6ec87-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f975a56c761db6506eca0b37ce6ec87-Metadata.json},
 openalex = {W2144335278},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f975a56c761db6506eca0b37ce6ec87-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f975a56c761db6506eca0b37ce6ec87-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7f975a56c761db6506eca0b37ce6ec87-Supplemental.zip},
 title = {Distributed k-Means and k-Median Clustering on General Topologies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7f975a56c761db6506eca0b37ce6ec87-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7fe1f8ab,
 abstract = {We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/n^{1/2}). We consider and analyze two algorithms that achieve a rate of O(1/n) for classical supervised learning problems. For least-squares regression, we show that averaged stochastic gradient descent with constant step-size achieves the desired rate. For logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent. For these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments on standard machine learning benchmarks showing that they often outperform existing approaches.},
 author = {Bach, Francis and Moulines, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Metadata.json},
 openalex = {W2159930037},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7fe1f8abaad094e0b5cb1b01d712f708-Reviews.html},
 title = {Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7fe1f8abaad094e0b5cb1b01d712f708-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_7fec306d,
 abstract = {We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy.},
 author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc\textquotesingle Aurelio and de Freitas, Nando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7fec306d1e665bc9c748b5d2b99a6e97-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7fec306d1e665bc9c748b5d2b99a6e97-Metadata.json},
 openalex = {W2952899695},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7fec306d1e665bc9c748b5d2b99a6e97-Reviews.html},
 title = {Predicting Parameters in Deep Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/7fec306d1e665bc9c748b5d2b99a6e97-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_801c14f0,
 abstract = {In search advertising, the search engine needs to select the most profitable advertisements to display, which can be formulated as an instance of online learning with partial feedback, also known as the stochastic multi-armed bandit (MAB) problem. In this paper, we show that the naive application of MAB algorithms to search advertising for advertisement selection will produce sample selection bias that harms the search engine by decreasing expected revenue and estimation of the largest mean (ELM) bias that harms the advertisers by increasing game-theoretic player-regret. We then propose simple bias-correction methods with benefits to both the search engine and the advertisers.},
 author = {Xu, Min and Qin, Tao and Liu, Tie-Yan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/801c14f07f9724229175b8ef8b4585a8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/801c14f07f9724229175b8ef8b4585a8-Metadata.json},
 openalex = {W2169728355},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/801c14f07f9724229175b8ef8b4585a8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/801c14f07f9724229175b8ef8b4585a8-Reviews.html},
 title = {Estimation Bias in Multi-Armed Bandit Algorithms for Search Advertising},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/801c14f07f9724229175b8ef8b4585a8-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8065d07d,
 abstract = {In this work we develop efficient methods for learning random MAP predictors for structured label problems. In particular, we construct posterior distributions over perturbations that can be adjusted via stochastic gradient methods. We show that any smooth posterior distribution would suffice to define a smooth PAC-Bayesian risk bound suitable for gradient methods. In addition, we relate the posterior distributions to computational properties of the MAP predictors. We suggest multiplicative posteriors to learn super-modular potential functions that accompany specialized MAP predictors such as graph-cuts. We also describe label-augmented posterior models that can use efficient MAP approximations, such as those arising from linear program relaxations.},
 author = {Hazan, Tamir and Maji, Subhransu and Keshet, Joseph and Jaakkola, Tommi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8065d07da4a77621450aa84fee5656d9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8065d07da4a77621450aa84fee5656d9-Metadata.json},
 openalex = {W2108678517},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8065d07da4a77621450aa84fee5656d9-Reviews.html},
 title = {Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_819f46e5,
 abstract = {In this paper we introduce a novel method that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions. Our method can be regarded as a natural extension of the one-class SVM (OCSVM) algorithm that finds multiple parallel separating hyperplanes in a reproducing kernel Hilbert space. We call our method q-OCSVM, as it can be used to estimate q quantiles of a high-dimensional distribution. For this purpose, we introduce a new global convex optimization program that finds all estimated sets at once and show that it can be solved efficiently. We prove the correctness of our method and present empirical results that demonstrate its superiority over existing methods.},
 author = {Glazer, Assaf and Lindenbaum, Michael and Markovitch, Shaul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/819f46e52c25763a55cc642422644317-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/819f46e52c25763a55cc642422644317-Metadata.json},
 openalex = {W2139373187},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/819f46e52c25763a55cc642422644317-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/819f46e52c25763a55cc642422644317-Reviews.html},
 title = {q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/819f46e52c25763a55cc642422644317-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_81e5f81d,
 abstract = {We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The convex problem can be solved efficiently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of d = 1, our result implies the near-optimality of DSPCA (d'Aspremont et al. [1]) even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings. We demonstrate this with an application to Kendall's tau correlation matrices and transelliptical component analysis.},
 author = {Vu, Vincent Q and Cho, Juhee and Lei, Jing and Rohe, Karl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/81e5f81db77c596492e6f1a5a792ed53-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/81e5f81db77c596492e6f1a5a792ed53-Metadata.json},
 openalex = {W2103804774},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/81e5f81db77c596492e6f1a5a792ed53-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/81e5f81db77c596492e6f1a5a792ed53-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/81e5f81db77c596492e6f1a5a792ed53-Supplemental.zip},
 title = {Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/81e5f81db77c596492e6f1a5a792ed53-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_82965d4e,
 abstract = {Applying linear templates is an integral part of many object detection systems and accounts for a significant portion of computation time. We describe a method that achieves a substantial end-to-end speedup over the best current methods, without loss of accuracy. Our method is a combination of approximating scores by vector quantizing feature windows and a number of speedup techniques including cascade. Our procedure allows speed and accuracy to be traded off in two ways: by choosing the number of Vector Quantization levels, and by choosing to rescore windows or not. Our method can be directly plugged into any recognition system that relies on linear templates. We demonstrate our method to speed up the original Exemplar SVM detector [1] by an order of magnitude and Deformable Part models [2] by two orders of magnitude with no loss of accuracy.},
 author = {Sadeghi, Mohammad Amin and Forsyth, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/82965d4ed8150294d4330ace00821d77-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/82965d4ed8150294d4330ace00821d77-Metadata.json},
 openalex = {W2097202741},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/82965d4ed8150294d4330ace00821d77-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/82965d4ed8150294d4330ace00821d77-Reviews.html},
 title = {Fast Template Evaluation with Vector Quantization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/82965d4ed8150294d4330ace00821d77-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_82aa4b0a,
 abstract = {The sparse additive model for text modeling involves the sum-of-exp computing, whose cost is consuming for large scales. Moreover, the assumption of equal background across all classes/topics may be too strong. This paper extends to propose sparse additive model with low rank background (SAM-LRB) and obtains simple yet efficient estimation. Particularly, employing a double majorization bound, we approximate log-likelihood into a quadratic lower-bound without the log-sum-exp terms. The constraints of low rank and sparsity are then simply embodied by nuclear norm and l1-norm regularizers. Interestingly, we find that the optimization task of SAM-LRB can be transformed into the same form as in Robust PCA. Consequently, parameters of supervised SAM-LRB can be efficiently learned using an existing algorithm for Robust PCA based on accelerated proximal gradient. Besides the supervised case, we extend SAM-LRB to favor unsupervised and multifaceted scenarios. Experiments on three real data demonstrate the effectiveness and efficiency of SAM-LRB, compared with a few state-of-the-art models.},
 author = {Shi, Lei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/82aa4b0af34c2313a562076992e50aa3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/82aa4b0af34c2313a562076992e50aa3-Metadata.json},
 openalex = {W2130119912},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/82aa4b0af34c2313a562076992e50aa3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/82aa4b0af34c2313a562076992e50aa3-Reviews.html},
 title = {Sparse Additive Text Models with Low Rank Background},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/82aa4b0af34c2313a562076992e50aa3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_839ab468,
 abstract = {This paper presents Correlated Nystrom Views (XNV), a fast semi-supervised algorithm for regression and classification. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, XNV applies multiview regression using Canonical Correlation Analysis (CCA) on unlabeled data to bias the regression towards useful features. It has been shown that, if the views contains accurate estimators, CCA regression can substantially reduce variance with a minimal increase in bias. Random views are justified by recent theoretical and empirical work showing that regression with random features closely approximates kernel regression, implying that random views can be expected to contain accurate estimators. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude.},
 author = {McWilliams, Brian and Balduzzi, David and Buhmann, Joachim M},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/839ab46820b524afda05122893c2fe8e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/839ab46820b524afda05122893c2fe8e-Metadata.json},
 openalex = {W2950664535},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/839ab46820b524afda05122893c2fe8e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/839ab46820b524afda05122893c2fe8e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/839ab46820b524afda05122893c2fe8e-Supplemental.zip},
 title = {Correlated random features for fast semi-supervised learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/839ab46820b524afda05122893c2fe8e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_83adc922,
 abstract = {Markov Decision Processes (MDPs) are extremely useful for modeling and solving sequential decision making problems. Graph-based MDPs provide a compact representation for MDPs with large numbers of random variables. However, the complexity of exactly solving a graph-based MDP usually grows exponentially in the number of variables, which limits their application. We present a new variational framework to describe and solve the planning problem of MDPs, and derive both exact and approximate planning algorithms. In particular, by exploiting the graph structure of graph-based MDPs, we propose a factored variational value iteration algorithm in which the value function is first approximated by the multiplication of local-scope value functions, then solved by minimizing a Kullback-Leibler (KL) divergence. The KL divergence is optimized using the belief propagation algorithm, with complexity exponential in only the cluster size of the graph. Experimental comparison on different models shows that our algorithm outperforms existing approximation algorithms at finding good policies.},
 author = {Cheng, Qiang and Liu, Qiang and Chen, Feng and Ihler, Alexander T},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/83adc9225e4deb67d7ce42d58fe5157c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/83adc9225e4deb67d7ce42d58fe5157c-Metadata.json},
 openalex = {W2143685055},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/83adc9225e4deb67d7ce42d58fe5157c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/83adc9225e4deb67d7ce42d58fe5157c-Reviews.html},
 title = {Variational Planning for Graph-based MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/83adc9225e4deb67d7ce42d58fe5157c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_83cdcec0,
 abstract = {The adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy. This problem leads to a generalization of k-anonymity to the b-matching setting. Novel algorithms and theory are provided to implement this type of anonymity. The relaxation achieves better utility, admits theoretical privacy guarantees that are as strong, and, most importantly, accommodates a variable level of anonymity for each individual. Empirical results confirm improved utility on benchmark and social data-sets.},
 author = {Choromanski, Krzysztof M and Jebara, Tony and Tang, Kui},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/83cdcec08fbf90370fcf53bdd56604ff-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/83cdcec08fbf90370fcf53bdd56604ff-Metadata.json},
 openalex = {W2163815085},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/83cdcec08fbf90370fcf53bdd56604ff-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/83cdcec08fbf90370fcf53bdd56604ff-Supplemental.zip},
 title = {Adaptive Anonymity via b-Matching},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/83cdcec08fbf90370fcf53bdd56604ff-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_84117275,
 abstract = {We describe a framework for designing efficient active learning algorithms that are tolerant to random classification noise and are differentially-private. The framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of filtered random examples. It builds on the powerful statistical query framework of Kearns (1993). We show that any efficient active statistical learning algorithm can be automatically converted to an efficient active learning algorithm which is tolerant to random classification noise as well as other forms of uncorrelated noise. The complexity of the resulting algorithms has information-theoretically optimal quadratic dependence on $1/(1-2\eta)$, where $\eta$ is the noise rate. We show that commonly studied concept classes including thresholds, rectangles, and linear separators can be efficiently actively learned in our framework. These results combined with our generic conversion lead to the first computationally-efficient algorithms for actively learning some of these concept classes in the presence of random classification noise that provide exponential improvement in the dependence on the error $\epsilon$ over their passive counterparts. In addition, we show that our algorithms can be automatically converted to efficient active differentially-private algorithms. This leads to the first differentially-private active learning algorithms with exponential label savings over the passive case.},
 author = {Balcan, Maria-Florina F and Feldman, Vitaly},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84117275be999ff55a987b9381e01f96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84117275be999ff55a987b9381e01f96-Metadata.json},
 openalex = {W2963129126},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84117275be999ff55a987b9381e01f96-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84117275be999ff55a987b9381e01f96-Reviews.html},
 title = {Statistical Active Learning Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/84117275be999ff55a987b9381e01f96-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_84438b7a,
 abstract = {When approximating binary similarity using the hamming distance between short binary hashes, we show that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between $x$ and $x'$ as the hamming distance between $f(x)$ and $g(x')$, for two distinct binary codes $f,g$, rather than as the hamming distance between $f(x)$ and $f(x')$.},
 author = {Neyshabur, Behnam and Srebro, Nati and Salakhutdinov, Russ R and Makarychev, Yury and Yadollahpour, Payman},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84438b7aae55a0638073ef798e50b4ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84438b7aae55a0638073ef798e50b4ef-Metadata.json},
 openalex = {W2953192649},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84438b7aae55a0638073ef798e50b4ef-Reviews.html},
 title = {The Power of Asymmetry in Binary Hashing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/84438b7aae55a0638073ef798e50b4ef-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_846c260d,
 abstract = {Monte-Carlo tree search (MCTS) has been drawing great interest in recent years for planning and learning under uncertainty. One of the key challenges is the trade-off between exploration and exploitation. To address this, we present a novel approach for MCTS using Bayesian mixture modeling and inference based Thompson sampling and apply it to the problem of online planning in MDPs. Our algorithm, named Dirichlet-NormalGamma MCTS (DNG-MCTS), models the uncertainty of the accumulated reward for actions in the search tree as a mixture of Normal distributions. We perform inferences on the mixture in Bayesian settings by choosing conjugate priors in the form of combinations of Dirichlet and NormalGamma distributions and select the best action at each decision node using Thompson sampling. Experimental results confirm that our algorithm advances the state-of-the-art UCT approach with better values on several benchmark problems.},
 author = {Bai, Aijun and Wu, Feng and Chen, Xiaoping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/846c260d715e5b854ffad5f70a516c88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/846c260d715e5b854ffad5f70a516c88-Metadata.json},
 openalex = {W2164102968},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/846c260d715e5b854ffad5f70a516c88-Reviews.html},
 title = {Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_84d2004b,
 abstract = {Many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.) require selecting, out of a massive data set, a manageable yet representative subset. Such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints. Classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two-stage protocol GREEDI, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show, that under certain natural conditions, performance close to the (impractical) centralized approach can be achieved. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar-based clustering, on tens of millions of data points using Hadoop.},
 author = {Mirzasoleiman, Baharan and Karbasi, Amin and Sarkar, Rik and Krause, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84d2004bf28a2095230e8e14993d398d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84d2004bf28a2095230e8e14993d398d-Metadata.json},
 openalex = {W2170735271},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84d2004bf28a2095230e8e14993d398d-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/84d2004bf28a2095230e8e14993d398d-Supplemental.zip},
 title = {Distributed Submodular Maximization: Identifying Representative Elements in Massive Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/84d2004bf28a2095230e8e14993d398d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_856fc816,
 abstract = {We find that various well-known graph-based models exhibit a common important harmonic structure in its target function - the value of a vertex is approximately the weighted average of the values of its adjacent neighbors. Understanding of such structure and analysis of the loss defined over such structure help reveal important properties of the target function over a graph. In this paper, we show that the variation of the target function across a cut can be upper and lower bounded by the ratio of its harmonic loss and the cut cost. We use this to develop an analytical tool and analyze five popular graph-based models: absorbing random walks, partially absorbing random walks, hitting times, pseudo-inverse of the graph Laplacian, and eigenvectors of the Laplacian matrices. Our analysis sheds new insights into several open questions related to these models, and provides theoretical justifications and guidelines for their practical use. Simulations on synthetic and real datasets confirm the potential of the proposed theory and tool.},
 author = {Wu, Xiao-Ming and Li, Zhenguo and Chang, Shih-Fu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/856fc81623da2150ba2210ba1b51d241-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/856fc81623da2150ba2210ba1b51d241-Metadata.json},
 openalex = {W2113063048},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/856fc81623da2150ba2210ba1b51d241-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/856fc81623da2150ba2210ba1b51d241-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/856fc81623da2150ba2210ba1b51d241-Supplemental.zip},
 title = {Analyzing the Harmonic Structure in Graph-Based Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/856fc81623da2150ba2210ba1b51d241-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_860320be,
 abstract = {The detection of anomalous activity in graphs is a statistical problem that arises in many applications, such as network surveillance, disease outbreak detection, and activity monitoring in social networks. Beyond its wide applicability, graph structured anomaly detection serves as a case study in the difficulty of balancing computational complexity with statistical power. In this work, we develop from first principles the generalized likelihood ratio test for determining if there is a well connected region of activation over the vertices in the graph in Gaussian noise. Because this test is computationally infeasible, we provide a relaxation, called the Lovasz extended scan statistic (LESS) that uses submodularity to approximate the intractable generalized likelihood ratio. We demonstrate a connection between LESS and maximum a-posteriori inference in Markov random fields, which provides us with a poly-time algorithm for LESS. Using electrical network theory, we are able to control type 1 error for LESS and prove conditions under which LESS is risk consistent. Finally, we consider specific graph models, the torus, k-nearest neighbor graphs, and epsilon-random graphs. We show that on these graphs our results provide near-optimal performance by matching our results to known lower bounds.},
 author = {Sharpnack, James L and Krishnamurthy, Akshay and Singh, Aarti},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/860320be12a1c050cd7731794e231bd3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/860320be12a1c050cd7731794e231bd3-Metadata.json},
 openalex = {W2950237939},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/860320be12a1c050cd7731794e231bd3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/860320be12a1c050cd7731794e231bd3-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/860320be12a1c050cd7731794e231bd3-Supplemental.zip},
 title = {Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/860320be12a1c050cd7731794e231bd3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_86e8f7ab,
 abstract = {This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. Bellman Error Basis Functions (BEBFs) have been shown to improve policy evaluation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging.},
 author = {Milani Fard, Mahdi and Grinberg, Yuri and Farahmand, Amir-massoud and Pineau, Joelle and Precup, Doina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/86e8f7ab32cfd12577bc2619bc635690-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/86e8f7ab32cfd12577bc2619bc635690-Metadata.json},
 openalex = {W2157544132},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/86e8f7ab32cfd12577bc2619bc635690-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/86e8f7ab32cfd12577bc2619bc635690-Supplemental.zip},
 title = {Bellman Error Based Feature Generation using Random Projections on Sparse Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_872488f8,
 abstract = {Measuring similarity is crucial to many learning tasks. To this end, metric learning has been the dominant paradigm. However, similarity is a richer and broader notion than what metrics entail. For example, similarity can arise from the process of aggregating the decisions of multiple latent components, where each latent component compares data in its own way by focusing on a different subset of features. In this paper, we propose Similarity Component Analysis (SCA), a probabilistic graphical model that discovers those latent components from data. In SCA, a latent component generates a local similarity value, computed with its own metric, independently of other components. The final similarity measure is then obtained by combining the local similarity values with a (noisy-)OR gate. We derive an EM-based algorithm for fitting the model parameters with similarity-annotated data from pairwise comparisons. We validate the SCA model on synthetic datasets where SCA discovers the ground-truth about the latent components. We also apply SCA to a multiway classification task and a link prediction task. For both tasks, SCA attains significantly better prediction accuracies than competing methods. Moreover, we show how SCA can be instrumental in exploratory analysis of data, where we gain insights about the data by examining patterns hidden in its latent components' local similarity values.},
 author = {Changpinyo, Soravit and Liu, Kuan and Sha, Fei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/872488f88d1b2db54d55bc8bba2fad1b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/872488f88d1b2db54d55bc8bba2fad1b-Metadata.json},
 openalex = {W2103955738},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/872488f88d1b2db54d55bc8bba2fad1b-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/872488f88d1b2db54d55bc8bba2fad1b-Supplemental.zip},
 title = {Similarity Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/872488f88d1b2db54d55bc8bba2fad1b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_892c91e0,
 abstract = {In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netflix prize.

A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially specified matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using.

We present a means to obtain performance guarantees with respect to any set of initial observations. The first step remains the same: find a matrix of lowest possible complexity that agrees with the partially specified matrix. We give a new way to interpret the output of this algorithm by next finding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error.},
 author = {Lee, Troy and Shraibman, Adi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/892c91e0a653ba19df81a90f89d99bcd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/892c91e0a653ba19df81a90f89d99bcd-Metadata.json},
 openalex = {W2150368571},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/892c91e0a653ba19df81a90f89d99bcd-Reviews.html},
 title = {Matrix Completion From any Given Set of Observations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/892c91e0a653ba19df81a90f89d99bcd-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8a0e1141,
 abstract = {Many machine learning problems can be interpreted as learning for matching two types of objects (e.g., images and captions, users and products, queries and documents, etc.). The matching level of two objects is usually measured as the inner product in a certain feature space, while the modeling effort focuses on mapping of objects from the original space to the feature space. This schema, although proven successful on a range of matching tasks, is insufficient for capturing the rich structure in the matching process of more complicated objects. In this paper, we propose a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains. More specifically, we apply this model to matching tasks in natural language, e.g., finding sensible responses for a tweet, or relevant answers to a given question. This new architecture naturally combines the localness and hierarchy intrinsic to the natural language problems, and therefore greatly improves upon the state-of-the-art models.},
 author = {Lu, Zhengdong and Li, Hang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Metadata.json},
 openalex = {W2128892113},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Reviews.html},
 title = {A Deep Architecture for Matching Short Texts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8a0e1141fd37fa5b98d5bb769ba1a7cc-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8a1e808b,
 abstract = {We consider the sensor selection problem on multivariate Gaussian distributions where only a subset of latent variables is of inferential interest. For pairs of vertices connected by a unique path in the graph, we show that there exist decompositions of nonlocal mutual information into local information measures that can be computed efficiently from the output of message passing algorithms. We integrate these decompositions into a computationally efficient greedy selector where the computational expense of quantification can be distributed across nodes in the network. Experimental results demonstrate the comparative efficiency of our algorithms for sensor selection in high-dimensional distributions. We additionally derive an online-computable performance bound based on augmentations of the relevant latent variable set that, when such a valid augmentation exists, is applicable for any distribution with nuisances.},
 author = {Levine, Daniel S and How, Jonathan P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a1e808b55fde9455cb3d8857ed88389-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a1e808b55fde9455cb3d8857ed88389-Metadata.json},
 openalex = {W2113648148},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a1e808b55fde9455cb3d8857ed88389-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a1e808b55fde9455cb3d8857ed88389-Supplemental.zip},
 title = {Sensor Selection in High-Dimensional Gaussian Trees with Nuisances},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8a1e808b55fde9455cb3d8857ed88389-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8a3363ab,
 abstract = {Hypergraphs allow one to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions. In this paper, we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element is a family of regularization functionals based on the total variation on hypergraphs.},
 author = {Hein, Matthias and Setzer, Simon and Jost, Leonardo and Rangapuram, Syama Sundar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a3363abe792db2d8761d6403605aeb7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a3363abe792db2d8761d6403605aeb7-Metadata.json},
 openalex = {W2102306708},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a3363abe792db2d8761d6403605aeb7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8a3363abe792db2d8761d6403605aeb7-Supplemental.zip},
 title = {The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8b16ebc0,
 abstract = {Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no exact screening rule for group Lasso. We have evaluated our screening rule using many real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso.},
 author = {Wang, Jie and Zhou, Jiayu and Wonka, Peter and Ye, Jieping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b16ebc056e613024c057be590b542eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b16ebc056e613024c057be590b542eb-Metadata.json},
 openalex = {W2097620183},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b16ebc056e613024c057be590b542eb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b16ebc056e613024c057be590b542eb-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b16ebc056e613024c057be590b542eb-Supplemental.zip},
 title = {Lasso Screening Rules via Dual Polytope Projection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8b16ebc056e613024c057be590b542eb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8b5040a8,
 abstract = {Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efficiently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.},
 author = {Petralia, Francesca and Vogelstein, Joshua T and Dunson, David B},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b5040a8a5baf3e0e67386c2e3a9b903-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b5040a8a5baf3e0e67386c2e3a9b903-Metadata.json},
 openalex = {W2950910813},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b5040a8a5baf3e0e67386c2e3a9b903-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8b5040a8a5baf3e0e67386c2e3a9b903-Supplemental.zip},
 title = {Multiscale Dictionary Learning for Estimating Conditional Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8bf1211f,
 abstract = {We provide a unified framework for the high-dimensional analysis of superposition-structured or dirty statistical models: where the model parameters are a superposition of structurally constrained parameters. We allow for any number and types of structures, and any statistical model. We consider the general class of M-estimators that minimize the sum of any loss function, and an instance of what we call a hybrid regularization, that is the infimal convolution of weighted regularization functions, one for each structural component. We provide corollaries showcasing our unified framework for varied statistical models such as linear regression, multiple regression and principal component analysis, over varied superposition structures.},
 author = {Yang, Eunho and Ravikumar, Pradeep K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8bf1211fd4b7b94528899de0a43b9fb3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8bf1211fd4b7b94528899de0a43b9fb3-Metadata.json},
 openalex = {W2100243645},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8bf1211fd4b7b94528899de0a43b9fb3-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8bf1211fd4b7b94528899de0a43b9fb3-Supplemental.zip},
 title = {Dirty Statistical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8bf1211fd4b7b94528899de0a43b9fb3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8c19f571,
 abstract = {Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem, we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm - random initialization plus iterative update, we take an progressive approach. Starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process, new components will be incorporated on the fly when needed. The algorithm can reliably estimate a DP mixture model in one pass, making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efficiency - orders of magnitude speed-up compared to the state-of-the-art.},
 author = {Lin, Dahua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c19f571e251e61cb8dd3612f26d5ecf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c19f571e251e61cb8dd3612f26d5ecf-Metadata.json},
 openalex = {W2122340819},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c19f571e251e61cb8dd3612f26d5ecf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c19f571e251e61cb8dd3612f26d5ecf-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c19f571e251e61cb8dd3612f26d5ecf-Supplemental.zip},
 title = {Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8c19f571e251e61cb8dd3612f26d5ecf-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8c235f89,
 abstract = {Tensor completion from incomplete observations is a problem of significant practical interest. However, it is unlikely that there exists an efficient algorithm with provable guarantee to recover a general tensor from a limited number of observations. In this paper, we study the recovery algorithm for pairwise interaction tensors, which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness. Specifically, in the absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from O(nr log2(n)) observations. For the noisy cases, we also prove error bounds for a constrained convex program for recovering the tensors. Our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory. In addition, we apply our algorithm on a temporal collaborative filtering task and obtain state-of-the-art results.},
 author = {Chen, Shouyuan and Lyu, Michael R and King, Irwin and Xu, Zenglin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c235f89a8143a28a1d6067e959dd858-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c235f89a8143a28a1d6067e959dd858-Metadata.json},
 openalex = {W2126707628},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c235f89a8143a28a1d6067e959dd858-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8c235f89a8143a28a1d6067e959dd858-Supplemental.zip},
 title = {Exact and Stable Recovery of Pairwise Interaction Tensors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8c235f89a8143a28a1d6067e959dd858-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8ce6790c,
 author = {Xiang, Jing and Kim, Seyoung},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8ce6790cc6a94e65f17f908f462fae85-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8ce6790cc6a94e65f17f908f462fae85-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8ce6790cc6a94e65f17f908f462fae85-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8ce6790cc6a94e65f17f908f462fae85-Reviews.html},
 title = {A\ast Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8ce6790cc6a94e65f17f908f462fae85-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8d34201a,
 abstract = {Many applications in machine learning require optimizing unknown functions defined over a high-dimensional space from noisy samples that are expensive to obtain. We address this notoriously hard challenge, under the assumptions that the function varies only along some low-dimensional subspace and is smooth (i.e., it has a low norm in a Reproducible Kernel Hilbert Space). In particular, we present the SI-BO algorithm, which leverages recent low-rank matrix recovery techniques to learn the underlying subspace of the unknown function and applies Gaussian Process Upper Confidence sampling for optimization of the function. We carefully calibrate the exploration-exploitation tradeoff by allocating the sampling budget to subspace estimation and function optimization, and obtain the first subexponential cumulative regret bounds and convergence rates for Bayesian optimization in high-dimensions under noisy observations. Numerical results demonstrate the effectiveness of our approach in difficult scenarios.},
 author = {Djolonga, Josip and Krause, Andreas and Cevher, Volkan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d34201a5b85900908db6cae92723617-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d34201a5b85900908db6cae92723617-Metadata.json},
 openalex = {W2156375221},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d34201a5b85900908db6cae92723617-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d34201a5b85900908db6cae92723617-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d34201a5b85900908db6cae92723617-Supplemental.zip},
 title = {High-Dimensional Gaussian Process Bandits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8d34201a5b85900908db6cae92723617-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8d6dc35e,
 abstract = {Analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties. We show that the proof of consistency requires bounds on the growth rates of eigenvalues and their dispersion, which are often violated in data. We prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data. In addition, we propose an extension of analytic shrinkage -orthogonal complement shrinkage- which adapts to the covariance structure. Finally we demonstrate the superior performance of our novel approach on data from the domains of finance, spoken letter and optical character recognition, and neuroscience.},
 author = {Bartz, Daniel and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d6dc35e506fc23349dd10ee68dabb64-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d6dc35e506fc23349dd10ee68dabb64-Metadata.json},
 openalex = {W2118047803},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d6dc35e506fc23349dd10ee68dabb64-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8d6dc35e506fc23349dd10ee68dabb64-Supplemental.zip},
 title = {Generalizing Analytic Shrinkage for Arbitrary Covariance Structures},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8d6dc35e506fc23349dd10ee68dabb64-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8dd48d6a,
 abstract = {Many methods have been proposed to solve the problems of recovering intrinsic scene properties such as shape, reflectance and illumination from a single image, and object class segmentation separately. While these two problems are mutually informative, in the past not many papers have addressed this topic. In this work we explore such joint estimation of intrinsic scene properties recovered from an image, together with the estimation of the objects and attributes present in the scene. In this way, our unified framework is able to capture the correlations between intrinsic properties (reflectance, shape, illumination), objects (table, tv-monitor), and materials (wooden, plastic) in a given scene. For example, our model is able to enforce the condition that if a set of pixels take same object label, e.g. table, most likely those pixels would receive similar reflectance values. We cast the problem in an energy minimization framework and demonstrate the qualitative and quantitative improvement in the overall accuracy on the NYU and Pascal datasets.},
 author = {Vineet, Vibhav and Rother, Carsten and Torr, Philip},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8dd48d6a2e2cad213179a3992c0be53c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8dd48d6a2e2cad213179a3992c0be53c-Metadata.json},
 openalex = {W2125711766},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8dd48d6a2e2cad213179a3992c0be53c-Reviews.html},
 title = {Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8f121ce0,
 abstract = {Robust PCA methods are typically based on batch optimization and have to load all the samples into memory during optimization. This prevents them from efficiently processing big data. In this paper, we develop an Online Robust PCA (OR-PCA) that processes one sample per time instance and hence its memory cost is independent of the number of samples, significantly enhancing the computation and storage efficiency. The proposed OR-PCA is based on stochastic optimization of an equivalent reformulation of the batch RPCA. Indeed, we show that OR-PCA provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust to sparse corruption. Moreover, OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efficiency advantages of the OR-PCA over online PCA and batch RPCA methods.},
 author = {Feng, Jiashi and Xu, Huan and Yan, Shuicheng},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f121ce07d74717e0b1f21d122e04521-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f121ce07d74717e0b1f21d122e04521-Metadata.json},
 openalex = {W2154992274},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f121ce07d74717e0b1f21d122e04521-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f121ce07d74717e0b1f21d122e04521-Supplemental.zip},
 title = {Online Robust PCA via Stochastic Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8f121ce07d74717e0b1f21d122e04521-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8f1d4362,
 abstract = {Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.},
 author = {Srivastava, Rupesh K and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f1d43620bc6bb580df6e80b0dc05c48-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f1d43620bc6bb580df6e80b0dc05c48-Metadata.json},
 openalex = {W2099049980},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f1d43620bc6bb580df6e80b0dc05c48-Reviews.html},
 title = {Compete to Compute},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8f468c87,
 abstract = {All the existing multi-task local learning methods are defined on homogeneous neighborhood which consists of all data points from only one task. In this paper, different from existing methods, we propose local learning methods for multitask classification and regression problems based on heterogeneous neighborhood which is defined on data points from all tasks. Specifically, we extend the k-nearest-neighbor classifier by formulating the decision function for each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific. By defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one, a regularized objective function is proposed and an efficient coordinate descent method is developed to solve it. For regression problems, we extend the kernel regression to multi-task setting in a similar way to the classification case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods.},
 author = {Zhang, Yu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f468c873a32bb0619eaeb2050ba45d1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f468c873a32bb0619eaeb2050ba45d1-Metadata.json},
 openalex = {W2144393430},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8f468c873a32bb0619eaeb2050ba45d1-Reviews.html},
 title = {Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8f468c873a32bb0619eaeb2050ba45d1-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_8fb21ee7,
 abstract = {If a piece of information is released from a media site, can it spread, in 1 month, to a million web pages? This influence estimation problem is very challenging since both the time-sensitive nature of the problem and the issue of scalability need to be addressed simultaneously. In this paper, we propose a randomized algorithm for influence estimation in continuous-time diffusion networks. Our algorithm can estimate the influence of every node in a network with |V| nodes and |E| edges to an accuracy of $\varepsilon$ using $n=O(1/\varepsilon^2)$ randomizations and up to logarithmic factors O(n|E|+n|V|) computations. When used as a subroutine in a greedy influence maximization algorithm, our proposed method is guaranteed to find a set of nodes with an influence of at least (1-1/e)OPT-2$\varepsilon$, where OPT is the optimal value. Experiments on both synthetic and real-world data show that the proposed method can easily scale up to networks of millions of nodes while significantly improves over previous state-of-the-arts in terms of the accuracy of the estimated influence and the quality of the selected nodes in maximizing the influence.},
 author = {Du, Nan and Song, Le and Gomez Rodriguez, Manuel and Zha, Hongyuan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8fb21ee7a2207526da55a679f0332de2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8fb21ee7a2207526da55a679f0332de2-Metadata.json},
 openalex = {W2950468916},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8fb21ee7a2207526da55a679f0332de2-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/8fb21ee7a2207526da55a679f0332de2-Supplemental.zip},
 title = {Scalable Influence Estimation in Continuous-Time Diffusion Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/8fb21ee7a2207526da55a679f0332de2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_903ce922,
 abstract = {The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task?

We give the first positive answer to this question for a natural supervised learning problem — we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {-1,1,0}n. This class is inefficiently learnable using O(n/e2) examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efficiently learn this class using only O(n/e2) examples. We further show that under stronger hardness assumptions, even O(n1.499/e2) examples do not suffice. On the other hand, we show a new algorithm that learns this class efficiently using $\tilde{\Omega}\left(n^2/\epsilon^2\right)$ examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem.},
 author = {Daniely, Amit and Linial, Nati and Shalev-Shwartz, Shai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/903ce9225fca3e988c2af215d4e544d3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/903ce9225fca3e988c2af215d4e544d3-Metadata.json},
 openalex = {W2155831576},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/903ce9225fca3e988c2af215d4e544d3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/903ce9225fca3e988c2af215d4e544d3-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/903ce9225fca3e988c2af215d4e544d3-Supplemental.zip},
 title = {More data speeds up training time in learning halfspaces over sparse vectors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/903ce9225fca3e988c2af215d4e544d3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9232fe81,
 abstract = {Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results.},
 author = {Goh, Hanlin and Thome, Nicolas and Cord, Matthieu and Lim, Joo-Hwee},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9232fe81225bcaef853ae32870a2b0fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9232fe81225bcaef853ae32870a2b0fe-Metadata.json},
 openalex = {W2171157299},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9232fe81225bcaef853ae32870a2b0fe-Reviews.html},
 title = {Top-Down Regularization of Deep Belief Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9232fe81225bcaef853ae32870a2b0fe-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_92cc2275,
 abstract = {Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difficulties that entail sophisticated algorithms. Our first contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efficiently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efficient latent fused lasso.},
 author = {Zhang, Xinhua and Yu, Yao-Liang and Schuurmans, Dale},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/92cc227532d17e56e07902b254dfad10-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/92cc227532d17e56e07902b254dfad10-Metadata.json},
 openalex = {W2131437840},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/92cc227532d17e56e07902b254dfad10-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/92cc227532d17e56e07902b254dfad10-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/92cc227532d17e56e07902b254dfad10-Supplemental.zip},
 title = {Polar Operators for Structured Sparse Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/92cc227532d17e56e07902b254dfad10-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_93d65641,
 abstract = {Incorporating invariance information is important for many learning problems. To exploit invariances, most existing methods resort to approximations that either lead to expensive optimization problems such as semi-definite programming, or rely on separation oracles to retain tractability. Some methods further limit the space of functions and settle for non-convex models. In this paper, we propose a framework for learning in reproducing kernel Hilbert spaces (RKHS) using local invariances that explicitly characterize the behavior of the target function around data instances. These invariances are compactly encoded as linear functionals whose value are penalized by some loss function. Based on a representer theorem that we establish, our formulation can be efficiently optimized via a convex program. For the representer theorem to hold, the linear functionals are required to be bounded in the RKHS, and we show that this is true for a variety of commonly used RKHS and invariances. Experiments on learning with unlabeled data and transform invariances show that the proposed method yields better or similar results compared with the state of the art.},
 author = {Zhang, Xinhua and Lee, Wee Sun and Teh, Yee Whye},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/93d65641ff3f1586614cf2c1ad240b6c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/93d65641ff3f1586614cf2c1ad240b6c-Metadata.json},
 openalex = {W2148651989},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/93d65641ff3f1586614cf2c1ad240b6c-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/93d65641ff3f1586614cf2c1ad240b6c-Supplemental.zip},
 title = {Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/93d65641ff3f1586614cf2c1ad240b6c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_944bdd96,
 abstract = {With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-the-art. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we find several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain.},
 author = {Carlson, David E and Rao, Vinayak and Vogelstein, Joshua T and Carin, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/944bdd9636749a0801c39b6e449dbedc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/944bdd9636749a0801c39b6e449dbedc-Metadata.json},
 openalex = {W2133887216},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/944bdd9636749a0801c39b6e449dbedc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/944bdd9636749a0801c39b6e449dbedc-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/944bdd9636749a0801c39b6e449dbedc-Supplemental.zip},
 title = {Real-Time Inference for a Gamma Process Model of Neural Spiking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/944bdd9636749a0801c39b6e449dbedc-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9461cce2,
 abstract = {We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing empirical classification error over labeled training examples; once the training classification error is reduced to a local coordinatewise minimum, Direct-Boost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an n'th order bottom sample margin.},
 author = {Zhai, Shaodan and Xia, Tian and Tan, Ming and Wang, Shaojun},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9461cce28ebe3e76fb4b931c35a169b0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9461cce28ebe3e76fb4b931c35a169b0-Metadata.json},
 openalex = {W2109687783},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9461cce28ebe3e76fb4b931c35a169b0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9461cce28ebe3e76fb4b931c35a169b0-Reviews.html},
 title = {Direct 0-1 Loss Minimization and Margin Maximization with Boosting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9461cce28ebe3e76fb4b931c35a169b0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_94c7bb58,
 abstract = {We consider a number of classical and new computational problems regarding marginal distributions, and inference in models specifying a full joint distribution. We prove general and efficient reductions between a number of these problems, which demonstrate that algorithmic progress in inference automatically yields progress for pure data problems. Our main technique involves formulating the problems as linear programs, and proving that the dual separation oracle required by the ellipsoid method is provided by the target problem. This technique may be of independent interest in probabilistic inference.},
 author = {Roughgarden, Tim and Kearns, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/94c7bb58efc3b337800875b5d382a072-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/94c7bb58efc3b337800875b5d382a072-Metadata.json},
 openalex = {W2123589260},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/94c7bb58efc3b337800875b5d382a072-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/94c7bb58efc3b337800875b5d382a072-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/94c7bb58efc3b337800875b5d382a072-Supplemental.zip},
 title = {Marginals-to-Models Reducibility},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/94c7bb58efc3b337800875b5d382a072-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9683cc5f,
 abstract = {Motivated by the desire to extend fast randomized techniques to nonlinear lp regression, we consider a class of structured regression problems. These problems involve Vandermonde matrices which arise naturally in various statistical modeling settings, including classical polynomial fitting problems, additive models and approximations to recently developed randomized techniques for scalable kernel methods. We show that this structure can be exploited to further accelerate the solution of the regression problem, achieving running times that are faster than input sparsity. We present empirical results confirming both the practical value of our modeling framework, as well as speedup benefits of randomized regression.},
 author = {Avron, Haim and Sindhwani, Vikas and Woodruff, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9683cc5f89562ea48e72bb321d9f03fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9683cc5f89562ea48e72bb321d9f03fb-Metadata.json},
 openalex = {W2151744781},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9683cc5f89562ea48e72bb321d9f03fb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9683cc5f89562ea48e72bb321d9f03fb-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9683cc5f89562ea48e72bb321d9f03fb-Supplemental.zip},
 title = {Sketching Structured Matrices for Faster Nonlinear Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9683cc5f89562ea48e72bb321d9f03fb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9766527f,
 abstract = {Stochastic gradient optimization is a class of widely used algorithms for training machine learning models. To optimize an objective, it uses the noisy gradient computed from the random data samples instead of the true gradient computed from the entire dataset. However, when the variance of the noisy gradient is large, the algorithm might spend much time bouncing around, leading to slower convergence and worse performance. In this paper, we develop a general approach of using control variate for variance reduction in stochastic gradient. Data statistics such as low-order moments (pre-computed or estimated online) is used to form the control variate. We demonstrate how to construct the control variate for two practical problems using stochastic gradient optimization. One is convex—the MAP estimation for logistic regression, and the other is non-convex—stochastic variational inference for latent Dirichlet allocation. On both problems, our approach shows faster convergence and better performance than the classical approach.},
 author = {Wang, Chong and Chen, Xi and Smola, Alexander J and Xing, Eric P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9766527f2b5d3e95d4a733fcfb77bd7e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9766527f2b5d3e95d4a733fcfb77bd7e-Metadata.json},
 openalex = {W2145832734},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9766527f2b5d3e95d4a733fcfb77bd7e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9766527f2b5d3e95d4a733fcfb77bd7e-Supplemental.zip},
 title = {Variance Reduction for Stochastic Gradient Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9766527f2b5d3e95d4a733fcfb77bd7e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_98dce83d,
 abstract = {The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory.},
 author = {Yu, Yao-Liang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/98dce83da57b0395e163467c9dae521b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/98dce83da57b0395e163467c9dae521b-Metadata.json},
 openalex = {W2155827809},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/98dce83da57b0395e163467c9dae521b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/98dce83da57b0395e163467c9dae521b-Reviews.html},
 title = {On Decomposing the Proximal Map},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/98dce83da57b0395e163467c9dae521b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_98f13708,
 abstract = {In text analysis documents are often represented as disorganized bags of words; models of such count features are typically based on mixing a small number of topics [1,2]. Recently, it has been observed that for many text corpora documents evolve into one another in a smooth way, with some features dropping and new ones being introduced. The counting grid [3] models this spatial metaphor literally: it is a grid of word distributions learned in such a way that a document's own distribution of features can be modeled as the sum of the histograms found in a window into the grid. The major drawback of this method is that it is essentially a mixture and all the content must be generated by a single contiguous area on the grid. This may be problematic especially for lower dimensional grids. In this paper, we overcome this issue by introducing the Componential Counting Grid which brings the componential nature of topic models to the basic counting grid. We evaluated our approach on document classification and multimodal retrieval obtaining state of the art results on standard benchmarks.},
 author = {Perina, Alessandro and Jojic, Nebojsa and Bicego, Manuele and Truski, Andrzej},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/98f13708210194c475687be6106a3b84-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/98f13708210194c475687be6106a3b84-Metadata.json},
 openalex = {W2134808888},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/98f13708210194c475687be6106a3b84-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/98f13708210194c475687be6106a3b84-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/98f13708210194c475687be6106a3b84-Supplemental.zip},
 title = {Documents as multiple overlapping windows into grids of counts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/98f13708210194c475687be6106a3b84-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9908279e,
 abstract = {In this paper we focus on the principal component regression and its application to high dimension non-Gaussian data. The major contributions are two folds. First, in low dimensions and under the Gaussian model, by borrowing the strength from recent development in minimax optimal principal component estimation, we first time sharply characterize the potential advantage of classical principal component regression over least square estimation. Secondly, we propose and analyze a new robust sparse principal component regression on high dimensional elliptically distributed data. The elliptical distribution is a semiparametric generalization of the Gaussian, including many well known distributions such as multivariate Gaussian, rank-deficient Gaussian, t, Cauchy, and logistic. It allows the random vector to be heavy tailed and have tail dependence. These extra flexibilities make it very suitable for modeling finance and biomedical imaging data. Under the elliptical model, we prove that our method can estimate the regression coefficients in the optimal parametric rate and therefore is a good alternative to the Gaussian based methods. Experiments on synthetic and real world data are conducted to illustrate the empirical usefulness of the proposed method.},
 author = {Han, Fang and Liu, Han},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9908279ebbf1f9b250ba689db6a0222b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9908279ebbf1f9b250ba689db6a0222b-Metadata.json},
 openalex = {W2148594261},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9908279ebbf1f9b250ba689db6a0222b-Reviews.html},
 title = {Robust Sparse Principal Component Regression under the High Dimensional Elliptical Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9908279ebbf1f9b250ba689db6a0222b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_99566564,
 abstract = {Psychologists are interested in developing instructional policies that boost student learning. An instructional policy specifies the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence. Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only difficult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more difficult (known as fading). We propose an alternative to the traditional methodology in which we define a parameterized space of policies and search this space to identify the optimal policy. For example, in concept learning, policies might be described by a fading function that specifies exemplar difficulty over time. We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects. Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and to identify the global optimum, and is as efficient in its subject budget as a traditional A-B comparison. We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans outside the educational arena.},
 author = {Lindsey, Robert V and Mozer, Michael C and Huggins, William J and Pashler, Harold},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995665640dc319973d3173a74a03860c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995665640dc319973d3173a74a03860c-Metadata.json},
 openalex = {W2145183542},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995665640dc319973d3173a74a03860c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995665640dc319973d3173a74a03860c-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995665640dc319973d3173a74a03860c-Supplemental.zip},
 title = {Optimizing Instructional Policies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/995665640dc319973d3173a74a03860c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_995e1fda,
 abstract = {We consider the design of strategies for market making in an exchange. A market maker generally seeks to profit from the difference between the buy and sell price of an asset, yet the market maker also takes exposure risk in the event of large price movements. Profit guarantees for market making strategies have typically required certain stochastic assumptions on the price fluctuations of the asset in question; for example, assuming a model in which the price process is mean reverting. We propose a class of spread-based market making strategies whose performance can be controlled even under worst-case (adversarial) settings. We prove structural properties of these strategies which allows us to design a master algorithm which obtains low regret relative to the best such strategy in hindsight. We run a set of experiments showing favorable performance on recent real-world stock price data.},
 author = {Abernethy, Jacob and Kale, Satyen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995e1fda4a2b5f55ef0df50868bf2a8f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995e1fda4a2b5f55ef0df50868bf2a8f-Metadata.json},
 openalex = {W2126056473},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995e1fda4a2b5f55ef0df50868bf2a8f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995e1fda4a2b5f55ef0df50868bf2a8f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/995e1fda4a2b5f55ef0df50868bf2a8f-Supplemental.zip},
 title = {Adaptive Market Making via Online Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/995e1fda4a2b5f55ef0df50868bf2a8f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_996a7fa0,
 abstract = {Inspired by real-time ad exchanges for online display advertising, we consider the problem of inferring a buyer's value distribution for a good when the buyer is repeatedly interacting with a seller through a posted-price mechanism. We model the buyer as a strategic agent, whose goal is to maximize her long-term surplus, and we are interested in mechanisms that maximize the seller's long-term revenue. We define the natural notion of strategic regret --- the lost revenue as measured against a truthful (non-strategic) buyer. We present seller algorithms that are no-(strategic)-regret when the buyer discounts her future surplus --- i.e. the buyer prefers showing advertisements to users sooner rather than later. We also give a lower bound on strategic regret that increases as the buyer's discounting weakens and shows, in particular, that any seller algorithm will suffer linear strategic regret if there is no discounting.},
 author = {Amin, Kareem and Rostamizadeh, Afshin and Syed, Umar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/996a7fa078cc36c46d02f9af3bef918b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/996a7fa078cc36c46d02f9af3bef918b-Metadata.json},
 openalex = {W2951702501},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/996a7fa078cc36c46d02f9af3bef918b-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/996a7fa078cc36c46d02f9af3bef918b-Supplemental.zip},
 title = {Learning Prices for Repeated Auctions with Strategic Buyers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/996a7fa078cc36c46d02f9af3bef918b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9996535e,
 abstract = {Data in the sciences frequently occur as sequences of multidimensional arrays called tensors. How can hidden, evolving trends in such data be extracted while preserving the tensor structure? The model that is traditionally used is the linear dynamical system (LDS) with Gaussian noise, which treats the latent state and observation at each time slice as a vector. We present the multilinear dynamical system (MLDS) for modeling tensor time series and an expectation-maximization (EM) algorithm to estimate the parameters. The MLDS models each tensor observation in the time series as the multilinear projection of the corresponding member of a sequence of latent tensors. The latent tensors are again evolving with respect to a multilinear projection. Compared to the LDS with an equal number of parameters, the MLDS achieves higher prediction accuracy and marginal likelihood for both artificial and real datasets.},
 author = {Rogers, Mark and Li, Lei and Russell, Stuart J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9996535e07258a7bbfd8b132435c5962-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9996535e07258a7bbfd8b132435c5962-Metadata.json},
 openalex = {W2139845993},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9996535e07258a7bbfd8b132435c5962-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9996535e07258a7bbfd8b132435c5962-Reviews.html},
 title = {Multilinear Dynamical Systems for Tensor Time Series},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9996535e07258a7bbfd8b132435c5962-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_99bcfcd7,
 abstract = {Computing the stationary distribution of a large finite or countably infinite state space Markov Chain (MC) has become central in many problems such as statistical inference and network analysis. Standard methods involve large matrix multiplications as in power iteration, or simulations of long random walks, as in Markov Chain Monte Carlo (MCMC). Power iteration is costly, as it involves computation at every state. For MCMC, it is difficult to determine whether the random walks are long enough to guarantee convergence. In this paper, we provide a novel algorithm that answers whether a chosen state in a MC has stationary probability larger than some Δ ∈ (0,1), and outputs an estimate of the stationary probability. Our algorithm is constant time, using information from a local neighborhood of the state on the graph induced by the MC, which has constant size relative to the state space. The multiplicative error of the estimate is upper bounded by a function of the mixing properties of the MC. Simulation results show MCs for which this method gives tight estimates.},
 author = {Lee, Christina E. and Ozdaglar, Asuman and Shah, Devavrat},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/99bcfcd754a98ce89cb86f73acc04645-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/99bcfcd754a98ce89cb86f73acc04645-Metadata.json},
 openalex = {W2134282564},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/99bcfcd754a98ce89cb86f73acc04645-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/99bcfcd754a98ce89cb86f73acc04645-Reviews.html},
 title = {Computing the Stationary Distribution Locally},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/99bcfcd754a98ce89cb86f73acc04645-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9a115815,
 abstract = {We present a maximum margin framework that clusters data using latent variables. Using latent representations enables our framework to model unobserved information embedded in the data. We implement our idea by large margin learning, and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem. We instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags. Experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches.},
 author = {Zhou, Guang-Tong and Lan, Tian and Vahdat, Arash and Mori, Greg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1158154dfa42caddbd0694a4e9bdc8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1158154dfa42caddbd0694a4e9bdc8-Metadata.json},
 openalex = {W2149951699},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1158154dfa42caddbd0694a4e9bdc8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1158154dfa42caddbd0694a4e9bdc8-Reviews.html},
 title = {Latent Maximum Margin Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9a1158154dfa42caddbd0694a4e9bdc8-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9a1756fd,
 abstract = {Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural firing rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition.},
 author = {Yamins, Daniel L and Hong, Ha and Cadieu, Charles and DiCarlo, James J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1756fd0c741126d7bbd4b692ccbd91-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1756fd0c741126d7bbd4b692ccbd91-Metadata.json},
 openalex = {W2107366978},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1756fd0c741126d7bbd4b692ccbd91-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a1756fd0c741126d7bbd4b692ccbd91-Reviews.html},
 title = {Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9a1756fd0c741126d7bbd4b692ccbd91-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9a96876e,
 abstract = {We consider the online Principal Component Analysis (PCA) where contaminated samples (containing outliers) are revealed sequentially to the Principal Components (PCs) estimator. Due to their sensitiveness to outliers, previous online PCA algorithms fail in this case and their results can be arbitrarily skewed by the outliers. Here we propose the online robust PCA algorithm, which is able to improve the PCs estimation upon an initial one steadily, even when faced with a constant fraction of outliers. We show that the final result of the proposed online RPCA has an acceptable degradation from the optimum. Actually, under mild conditions, online RPCA achieves the maximal robustness with a 50% breakdown point. Moreover, online RPCA is shown to be efficient for both storage and computation, since it need not re-explore the previous samples as in traditional robust PCA algorithms. This endows online RPCA with scalability for large scale data.},
 author = {Feng, Jiashi and Xu, Huan and Mannor, Shie and Yan, Shuicheng},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Metadata.json},
 openalex = {W2142701288},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9a96876e2f8f3dc4f3cf45f02c61c0c1-Supplemental.zip},
 title = {Online PCA for Contaminated Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9a96876e2f8f3dc4f3cf45f02c61c0c1-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9aa42b31,
 abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.

An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of Canada and cannot be easily combined to obtain Air Canada. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Metadata.json},
 openalex = {W2153579005},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Reviews.html},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9ab0d884,
 abstract = {We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd's algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models.

We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufficiently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efficiently. We demonstrate the robustness benefits of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a fixed percentage of adversarial unbounded outliers.},
 author = {Vainsencher, Daniel and Mannor, Shie and Xu, Huan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ab0d88431732957a618d4a469a0d4c3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ab0d88431732957a618d4a469a0d4c3-Metadata.json},
 openalex = {W2169477532},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ab0d88431732957a618d4a469a0d4c3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ab0d88431732957a618d4a469a0d4c3-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ab0d88431732957a618d4a469a0d4c3-Supplemental.zip},
 title = {Learning Multiple Models via Regularized Weighting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9ab0d88431732957a618d4a469a0d4c3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9ac403da,
 abstract = {High capacity classifiers, such as deep neural networks, often struggle on classes that have very few training examples. We propose a method for improving classification performance for such classes by discovering similar classes and transferring knowledge among them. Our method learns to organize the classes into a tree hierarchy. This tree structure imposes a prior over the classifier's parameters. We show that the performance of deep neural networks can be improved by applying these priors to the weights in the last layer. Our method combines the strength of discriminatively trained deep neural networks, which typically require large amounts of training data, with tree-based priors, making deep neural networks work well on infrequent classes as well. We also propose an algorithm for learning the underlying tree structure. Starting from an initial pre-specified tree, this algorithm modifies the tree to make it more pertinent to the task being solved, for example, removing semantic relationships in favour of visual ones for an image classification task. Our method achieves state-of-the-art classification results on the CIFAR-100 image data set and the MIR Flickr image-text data set.},
 author = {Srivastava, Nitish and Salakhutdinov, Russ R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ac403da7947a183884c18a67d3aa8de-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ac403da7947a183884c18a67d3aa8de-Metadata.json},
 openalex = {W2107215754},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ac403da7947a183884c18a67d3aa8de-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ac403da7947a183884c18a67d3aa8de-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9ac403da7947a183884c18a67d3aa8de-Supplemental.zip},
 title = {Discriminative Transfer Learning with Tree-based Priors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9ac403da7947a183884c18a67d3aa8de-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9c01802d,
 abstract = {What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner? We propose an optimal teaching framework aimed at learners who employ Bayesian models. Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher. This optimization problem is in general hard. In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for finding the optimal teaching set. Our algorithm optimizes the aggregate sufficient statistics, then unpacks them into actual teaching examples. We give several examples to illustrate our framework.},
 author = {Zhu, Jerry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9c01802ddb981e6bcfbec0f0516b8e35-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9c01802ddb981e6bcfbec0f0516b8e35-Metadata.json},
 openalex = {W2951122980},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9c01802ddb981e6bcfbec0f0516b8e35-Reviews.html},
 title = {Machine Teaching for Bayesian Learners in the Exponential Family},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9c01802ddb981e6bcfbec0f0516b8e35-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9cf81d80,
 abstract = {We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player's performance using a new notion of regret, also known as policy regret, which better captures the adversary's adaptiveness to the player's behavior. In a setting where losses are allowed to drift, we characterize —in a nearly complete manner— the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switching costs, the attainable rate with bandit feedback is Θ(T2/3). Interestingly, this rate is significantly worse than the Θ(√T) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also show that a bounded memory adversary can force ****Θ(T2/3) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies.},
 author = {Cesa-Bianchi, Nicol\`{o} and Dekel, Ofer and Shamir, Ohad},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9cf81d8026a9018052c429cc4e56739b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9cf81d8026a9018052c429cc4e56739b-Metadata.json},
 openalex = {W2101177173},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9cf81d8026a9018052c429cc4e56739b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9cf81d8026a9018052c429cc4e56739b-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9cf81d8026a9018052c429cc4e56739b-Supplemental.zip},
 title = {Online Learning with Switching Costs and Other Adaptive Adversaries},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9cf81d8026a9018052c429cc4e56739b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_9e3cfc48,
 abstract = {We consider the partial observability model for multi-armed bandits, introduced by Mannor and Shamir [14]. Our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph (which must be accessible before selecting an action). In the undirected case, we show that the learner can achieve optimal regret without even accessing the observability graph before selecting an action. Both results are shown using variants of the Exp3 algorithm operating on the observability graph in a time-efficient manner.},
 author = {Alon, Noga and Cesa-Bianchi, Nicol\`{o} and Gentile, Claudio and Mansour, Yishay},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9e3cfc48eccf81a0d57663e129aef3cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9e3cfc48eccf81a0d57663e129aef3cb-Metadata.json},
 openalex = {W2152951864},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9e3cfc48eccf81a0d57663e129aef3cb-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9e3cfc48eccf81a0d57663e129aef3cb-Supplemental.zip},
 title = {From Bandits to Experts: A Tale of Domination and Independence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a01a0380,
 abstract = {We consider the task of nearest-neighbor search with the class of binary-space-partitioning trees, includes kd-trees, principal axis trees and random projection trees, and try to rigorously answer the question which tree to use for nearest-neighbor search? To this end, we present the theoretical results imply that trees with better vector quantization performance have better search performance guarantees. We also explore another factor affecting the search performance -margins of the partitions in these trees. We demonstrate, both theoretically and empirically, that large margin partitions can improve tree search performance.},
 author = {Ram, Parikshit and Gray, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a01a0380ca3c61428c26a231f0e49a09-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a01a0380ca3c61428c26a231f0e49a09-Metadata.json},
 openalex = {W2150719578},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a01a0380ca3c61428c26a231f0e49a09-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a01a0380ca3c61428c26a231f0e49a09-Supplemental.zip},
 title = {Which Space Partitioning Tree to Use for Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a01a0380ca3c61428c26a231f0e49a09-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a0e2a2c5,
 abstract = {Small-variance asymptotics provide an emerging technique for obtaining scalable combinatorial algorithms from rich probabilistic models. We present a small-variance asymptotic analysis of the Hidden Markov Model and its infinite-state Bayesian nonparametric extension. Starting with the standard HMM, we first derive a hard inference algorithm analogous to k-means that arises when particular variances in the model tend to zero. This analysis is then extended to the Bayesian nonparametric case, yielding a simple, scalable, and flexible algorithm for discrete-state sequence data with a non-fixed number of states. We also derive the corresponding combinatorial objective functions arising from our analysis, which involve a k-means-like term along with penalties based on state transitions and the number of states. A key property of such algorithms is that— particularly in the nonparametric setting—standard probabilistic inference algorithms lack scalability and are heavily dependent on good initialization. A number of results on synthetic and real data sets demonstrate the advantages of the proposed framework.},
 author = {Roychowdhury, Anirban and Jiang, Ke and Kulis, Brian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a0e2a2c563d57df27213ede1ac4ac780-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a0e2a2c563d57df27213ede1ac4ac780-Metadata.json},
 openalex = {W2112321969},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a0e2a2c563d57df27213ede1ac4ac780-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a0e2a2c563d57df27213ede1ac4ac780-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a0e2a2c563d57df27213ede1ac4ac780-Supplemental.zip},
 title = {Small-Variance Asymptotics for Hidden Markov Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a0e2a2c563d57df27213ede1ac4ac780-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a1519de5,
 abstract = {Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multi- subject fMRI studies in which functional activity is classified using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso.},
 author = {Rao, Nikhil and Cox, Christopher and Nowak, Rob and Rogers, Timothy T},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a1519de5b5d44b31a01de013b9b51a80-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a1519de5b5d44b31a01de013b9b51a80-Metadata.json},
 openalex = {W2950559108},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a1519de5b5d44b31a01de013b9b51a80-Reviews.html},
 title = {Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a1519de5b5d44b31a01de013b9b51a80-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a1d50185,
 abstract = {We investigate two new optimization problems -- minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [14, 35] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.},
 author = {Iyer, Rishabh K and Bilmes, Jeff A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a1d50185e7426cbb0acad1e6ca74b9aa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a1d50185e7426cbb0acad1e6ca74b9aa-Metadata.json},
 openalex = {W2952001554},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a1d50185e7426cbb0acad1e6ca74b9aa-Reviews.html},
 title = {Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a1d50185e7426cbb0acad1e6ca74b9aa-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a223c6b3,
 abstract = {In the high-dimensional regression model a response variable is linearly related to $p$ covariates, but the sample size $n$ is smaller than $p$. We assume that only a small subset of covariates is `active' (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefficients through the Lasso ($\ell_1$-regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called `irrepresentability' condition. In this paper we study the `Gauss-Lasso' selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate `generalized irrepresentability condition' (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.},
 author = {Javanmard, Adel and Montanari, Andrea},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a223c6b3710f85df22e9377d6c4f7553-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a223c6b3710f85df22e9377d6c4f7553-Metadata.json},
 openalex = {W2101156246},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a223c6b3710f85df22e9377d6c4f7553-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a223c6b3710f85df22e9377d6c4f7553-Reviews.html},
 title = {Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a223c6b3710f85df22e9377d6c4f7553-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a2557a7b,
 abstract = {While graphs with continuous node attributes arise in many applications, state-of-the-art graph kernels for comparing continuous-attributed graphs suffer from a high runtime complexity. For instance, the popular shortest path kernel scales as O(n4), where n is the number of nodes. In this paper, we present a class of graph kernels with computational complexity O(n2(m + log n + δ2 + d)), where δ is the graph diameter, m is the number of edges, and d is the dimension of the node attributes. Due to the sparsity and small diameter of real-world graphs, these kernels typically scale comfortably to large graphs. In our experiments, the presented kernels outperform state-of-the-art kernels in terms of speed and accuracy on classification benchmark datasets.},
 author = {Feragen, Aasa and Kasenburg, Niklas and Petersen, Jens and de Bruijne, Marleen and Borgwardt, Karsten},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a2557a7b2e94197ff767970b67041697-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a2557a7b2e94197ff767970b67041697-Metadata.json},
 openalex = {W2124824205},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a2557a7b2e94197ff767970b67041697-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a2557a7b2e94197ff767970b67041697-Reviews.html},
 title = {Scalable kernels for graphs with continuous attributes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a2557a7b2e94197ff767970b67041697-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a3f390d8,
 abstract = {Many real-world problems have complicated objective functions. To optimize such functions, humans utilize sophisticated sequential decision-making strategies. Many optimization algorithms have also been developed for this same purpose, but how do they compare to humans in terms of both performance and behavior? We try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1D function. Subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location. Their task is to find the function's maximum in as few clicks as possible. Subjects win if they get close enough to the maximum location. Analysis over 23 non-maths undergraduates, optimizing 25 functions from different families, shows that humans outperform 24 well-known optimization algorithms. Bayesian Optimization based on Gaussian Processes, which exploits all the x values tried and all the f (x) values obtained so far to pick the next x, predicts human performance and searched locations better. In 6 follow-up controlled experiments over 76 subjects, covering interpolation, extrapolation, and optimization tasks, we further confirm that Gaussian Processes provide a general and unified theoretical account to explain passive and active function learning and search in humans.},
 author = {Borji, Ali and Itti, Laurent},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a3f390d88e4c41f2747bfa2f1b5f87db-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a3f390d88e4c41f2747bfa2f1b5f87db-Metadata.json},
 openalex = {W2133261881},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a3f390d88e4c41f2747bfa2f1b5f87db-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a3f390d88e4c41f2747bfa2f1b5f87db-Supplemental.zip},
 title = {Bayesian optimization explains human active search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a3f390d88e4c41f2747bfa2f1b5f87db-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a49e9411,
 abstract = {We propose a family of maximum mean discrepancy (MMD) kernel two-sample tests that have low sample complexity and are consistent. The test has a hyperparameter that allows one to control the tradeoff between sample complexity and computational time. Our family of tests, which we denote as B-tests, is both computationally and statistically efficient, combining favorable properties of previously proposed MMD two-sample tests. It does so by better leveraging samples to produce low variance estimates in the finite sample case, while avoiding a quadratic number of kernel evaluations and complex null-hypothesis approximation as would be required by tests relying on one sample U-statistics. The B-test uses a smaller than quadratic number of kernel evaluations and avoids completely the computational burden of complex null-hypothesis approximation while maintaining consistency and probabilistically conservative thresholds on Type I error. Finally, recent results of combining multiple kernels transfer seamlessly to our hypothesis test, allowing a further increase in discriminative power and decrease in sample complexity.},
 author = {Zaremba, Wojciech and Gretton, Arthur and Blaschko, Matthew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a49e9411d64ff53eccfdd09ad10a15b3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a49e9411d64ff53eccfdd09ad10a15b3-Metadata.json},
 openalex = {W2140142253},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a49e9411d64ff53eccfdd09ad10a15b3-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a49e9411d64ff53eccfdd09ad10a15b3-Supplemental.zip},
 title = {B-test: A Non-parametric, Low Variance Kernel Two-sample Test},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a50abba8,
 abstract = {Suppose $k$ centers are fit to $m$ points by heuristically minimizing the $k$-means cost; what is the corresponding fit over the source distribution? This question is resolved here for distributions with $p\geq 4$ bounded moments; in particular, the difference between the sample cost and distribution cost decays with $m$ and $p$ as $m^{\min\{-1/4, -1/2+2/p\}}$. The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions. To further demonstrate this mechanism, a soft clustering variant of $k$-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum. Lastly, a rate with refined constants is provided for $k$-means instances possessing some cluster structure.},
 author = {Telgarsky, Matus J and Dasgupta, Sanjoy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a50abba8132a77191791390c3eb19fe7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a50abba8132a77191791390c3eb19fe7-Metadata.json},
 openalex = {W2156089267},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a50abba8132a77191791390c3eb19fe7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a50abba8132a77191791390c3eb19fe7-Supplemental.zip},
 title = {Moment-based Uniform Deviation Bounds for $k$-means and Friends},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a50abba8132a77191791390c3eb19fe7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a5cdd4aa,
 abstract = {The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement.},
 author = {Ramaswamy, Harish G and Agarwal, Shivani and Tewari, Ambuj},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Metadata.json},
 openalex = {W2101767118},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Supplemental.zip},
 title = {Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a5cdd4aa0048b187f7182f1b9ce7a6a7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a7d8ae45,
 abstract = {We present a new approach to sample from generic binary distributions, based on an exact Hamiltonian Monte Carlo algorithm applied to a piecewise continuous augmentation of the binary distribution of interest. An extension of this idea to distributions over mixtures of binary and possibly-truncated Gaussian or exponential variables allows us to sample from posteriors of linear and probit regression models with spike-and-slab priors and truncated parameters. We illustrate the advantages of these algorithms in several examples in which they outperform the Metropolis or Gibbs samplers.},
 author = {Pakman, Ari and Paninski, Liam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a7d8ae4569120b5bec12e7b6e9648b86-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a7d8ae4569120b5bec12e7b6e9648b86-Metadata.json},
 openalex = {W2137576513},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a7d8ae4569120b5bec12e7b6e9648b86-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a7d8ae4569120b5bec12e7b6e9648b86-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a7d8ae4569120b5bec12e7b6e9648b86-Supplemental.zip},
 title = {Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a8240cb8,
 abstract = {We describe a set of fast, tractable methods for characterizing neural responses to high-dimensional sensory stimuli using a model we refer to as the generalized quadratic model (GQM). The GQM consists of a low-rank quadratic function followed by a point nonlinearity and exponential-family noise. The quadratic function characterizes the neuron's stimulus selectivity in terms of a set linear receptive fields followed by a quadratic combination rule, and the invertible nonlinearity maps this output to the desired response range. Special cases of the GQM include the 2nd-order Volterra model [1, 2] and the elliptical Linear-Nonlinear-Poisson model [3]. Here we show that for canonical form GQMs, spectral decomposition of the first two response-weighted moments yields approximate maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes moment-based estimators such as the spike-triggered co-variance, and, in the Gaussian noise case, provides closed-form estimators under a large class of non-Gaussian stimulus distributions. We show that these estimators are fast and provide highly accurate estimates with far lower computational cost than full maximum likelihood. Moreover, the GQM provides a natural framework for combining multi-dimensional stimulus sensitivity and spike-history dependencies within a single model. We show applications to both analog and spiking data using intracellular recordings of V1 membrane potential and extracellular recordings of retinal spike trains.},
 author = {Park, Il Memming and Archer, Evan W and Priebe, Nicholas and Pillow, Jonathan W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a8240cb8235e9c493a0c30607586166c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a8240cb8235e9c493a0c30607586166c-Metadata.json},
 openalex = {W2098412159},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a8240cb8235e9c493a0c30607586166c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a8240cb8235e9c493a0c30607586166c-Reviews.html},
 title = {Spectral methods for neural characterization using generalized quadratic models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a8240cb8235e9c493a0c30607586166c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a8849b05,
 abstract = {For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren't actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a "weighted majority voting" classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such "trending topics" in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%.},
 author = {Chen, George H and Nikolov, Stanislav and Shah, Devavrat},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a8849b052492b5106526b2331e526138-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a8849b052492b5106526b2331e526138-Metadata.json},
 openalex = {W2950743550},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a8849b052492b5106526b2331e526138-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a8849b052492b5106526b2331e526138-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a8849b052492b5106526b2331e526138-Supplemental.zip},
 title = {A Latent Source Model for Nonparametric Time Series Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a8849b052492b5106526b2331e526138-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a97da629,
 abstract = {We present a PAC-Bayes-Empirical-Bernstein inequality. The inequality is based on a combination of the PAC-Bayesian bounding technique with an Empirical Bernstein bound. We show that when the empirical variance is significantly smaller than the empirical loss the PAC-Bayes-Empirical-Bernstein inequality is significantly tighter than the PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. Our theoretical analysis is confirmed empirically on a synthetic example and several UCI datasets. The PAC-Bayes-Empirical-Bernstein inequality is an interesting example of an application of the PAC-Bayesian bounding technique to self-bounding functions.},
 author = {Tolstikhin, Ilya O and Seldin, Yevgeny},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a97da629b098b75c294dffdc3e463904-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a97da629b098b75c294dffdc3e463904-Metadata.json},
 openalex = {W2107074784},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a97da629b098b75c294dffdc3e463904-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a97da629b098b75c294dffdc3e463904-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a97da629b098b75c294dffdc3e463904-Supplemental.zip},
 title = {PAC-Bayes-Empirical-Bernstein Inequality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a97da629b098b75c294dffdc3e463904-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_a9be4c2a,
 abstract = {Latent variable prediction models, such as multi-layer networks, impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction. Unfortunately, such models are difficult to train because inference over latent variables must be performed concurrently with parameter optimization—creating a highly non-convex problem. Instead of proposing another local training method, we develop a convex relaxation of hidden-layer conditional models that admits global training. Our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer. The resulting methods are able to acquire two-layer models that cannot be represented by any single-layer model over the same features, while improving training quality over local heuristics.},
 author = {Aslan, \"{O}zlem and Cheng, Hao and Zhang, Xinhua and Schuurmans, Dale},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a9be4c2a4041cadbf9d61ae16dd1389e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a9be4c2a4041cadbf9d61ae16dd1389e-Metadata.json},
 openalex = {W2096433175},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a9be4c2a4041cadbf9d61ae16dd1389e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a9be4c2a4041cadbf9d61ae16dd1389e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/a9be4c2a4041cadbf9d61ae16dd1389e-Supplemental.zip},
 title = {Convex Two-Layer Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/a9be4c2a4041cadbf9d61ae16dd1389e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_aab32389,
 abstract = {We introduce the Randomized Dependence Coefficient (RDC), a measure of non-linear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R\'enyi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper.},
 author = {Lopez-Paz, David and Hennig, Philipp and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aab3238922bcc25a6f606eb525ffdc56-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aab3238922bcc25a6f606eb525ffdc56-Metadata.json},
 openalex = {W4293737279},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aab3238922bcc25a6f606eb525ffdc56-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aab3238922bcc25a6f606eb525ffdc56-Reviews.html},
 title = {The Randomized Dependence Coefficient},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/aab3238922bcc25a6f606eb525ffdc56-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_aace49c7,
 abstract = {We propose a semiparametric procedure for estimating high dimensional sparse inverse covariance matrix. Our method, named ALICE, is applicable to the elliptical family. Computationally, we develop an efficient dual inexact iterative projection (${\rm D_2}$P) algorithm based on the alternating direction method of multipliers (ADMM). Theoretically, we prove that the ALICE estimator achieves the parametric rate of convergence in both parameter estimation and model selection. Moreover, ALICE calibrates regularizations when estimating each column of the inverse covariance matrix. So it not only is asymptotically tuning free, but also achieves an improved finite sample performance. We present numerical simulations to support our theory, and a real data example to illustrate the effectiveness of the proposed estimator.},
 author = {Zhao, Tuo and Liu, Han},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aace49c7d80767cffec0e513ae886df0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aace49c7d80767cffec0e513ae886df0-Metadata.json},
 openalex = {W2123876144},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aace49c7d80767cffec0e513ae886df0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aace49c7d80767cffec0e513ae886df0-Reviews.html},
 title = {Sparse Inverse Covariance Estimation with Calibration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/aace49c7d80767cffec0e513ae886df0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_aba3b6fd,
 abstract = {Thompson Sampling has been demonstrated in many complex bandit models, however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the Bernoulli case. Here we extend them by proving asymptotic optimality of the algorithm using the Jeffreys prior for 1-dimensional exponential family bandits. Our proof builds on previous work, but also makes extensive use of closed forms for Kullback-Leibler divergence and Fisher information (through the Jeffreys prior) available in an exponential family. This allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right. Moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed, including heavy-tailed exponential families.},
 author = {Korda, Nathaniel and Kaufmann, Emilie and Munos, Remi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aba3b6fd5d186d28e06ff97135cade7f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aba3b6fd5d186d28e06ff97135cade7f-Metadata.json},
 openalex = {W2110632090},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aba3b6fd5d186d28e06ff97135cade7f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/aba3b6fd5d186d28e06ff97135cade7f-Supplemental.zip},
 title = {Thompson Sampling for 1-Dimensional Exponential Family Bandits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/aba3b6fd5d186d28e06ff97135cade7f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ac1dd209,
 abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
 author = {Johnson, Rie and Zhang, Tong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Metadata.json},
 openalex = {W2107438106},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Reviews.html},
 title = {Accelerating Stochastic Gradient Descent using Predictive Variance Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ac796a52,
 abstract = {We investigate a spiking neuron model of multisensory integration. Multiple stimuli from different sensory modalities are encoded by a single neural circuit comprised of a multisensory bank of receptive fields in cascade with a population of biophysical spike generators. We demonstrate that stimuli of different dimensions can be faithfully multiplexed and encoded in the spike domain and derive tractable algorithms for decoding each stimulus from the common pool of spikes. We also show that the identification of multisensory processing in a single neuron is dual to the recovery of stimuli encoded with a population of multisensory neurons, and prove that only a projection of the circuit onto input stimuli can be identified. We provide an example of multisensory integration using natural audio and video and discuss the performance of the proposed decoding and identification algorithms.},
 author = {Lazar, Aurel A and Slutskiy, Yevgeniy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ac796a52db3f16bbdb6557d3d89d1c5a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ac796a52db3f16bbdb6557d3d89d1c5a-Metadata.json},
 openalex = {W2131761878},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ac796a52db3f16bbdb6557d3d89d1c5a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ac796a52db3f16bbdb6557d3d89d1c5a-Reviews.html},
 title = {Multisensory Encoding, Decoding, and Identification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ac796a52db3f16bbdb6557d3d89d1c5a-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ad3019b8,
 abstract = {One approach to computer object recognition and modeling the brain's ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D affine transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformation-invariance [1], we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identity-preserving transformations. The model's wiring can be learned from videos of transforming objects—or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically confirm theoretical predictions (from [1]) for the case of 2D affine transformations. Next, we apply the model to non-affine transformations; as expected, it performs well on face verification tasks requiring invariance to the relatively smooth of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter transformations which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical findings, we tested the same model on face verification benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig [2, 3, 4] and a new dataset we gathered—achieving strong performance in these highly unconstrained cases as well.},
 author = {Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ad3019b856147c17e82a5bead782d2a8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ad3019b856147c17e82a5bead782d2a8-Metadata.json},
 openalex = {W2116914011},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ad3019b856147c17e82a5bead782d2a8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ad3019b856147c17e82a5bead782d2a8-Reviews.html},
 title = {Learning invariant representations and applications to face verification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ad3019b856147c17e82a5bead782d2a8-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ae0eb3ee,
 abstract = {Research on distributed machine learning algorithms has focused primarily on one of two extremes—algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked. We view this optimistic concurrency control paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment.},
 author = {Pan, Xinghao and Gonzalez, Joseph E and Jegelka, Stefanie and Broderick, Tamara and Jordan, Michael I},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae0eb3eed39d2bcef4622b2499a05fe6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae0eb3eed39d2bcef4622b2499a05fe6-Metadata.json},
 openalex = {W2163799778},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae0eb3eed39d2bcef4622b2499a05fe6-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae0eb3eed39d2bcef4622b2499a05fe6-Supplemental.zip},
 title = {Optimistic Concurrency Control for Distributed Unsupervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ae5e3ce4,
 abstract = {Many applications require the analysis of complex interactions between time series. These interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings. Here we provide a general framework for the statistical analysis of these dependencies when random variables are sampled from stationary time-series of arbitrary objects. To achieve this goal, we study the properties of the Kernel Cross-Spectral Density (KCSD) operator induced by positive definite kernels on arbitrary input domains. This framework enables us to develop an independence test between time series, as well as a similarity measure to compare different types of coupling. The performance of our test is compared to the HSIC test using i.i.d. assumptions, showing improvements in terms of detection errors, as well as the suitability of this approach for testing dependency in complex dynamical systems. This similarity measure enables us to identify different types of interactions in electrophysiological neural time series.},
 author = {Besserve, Michel and Logothetis, Nikos K and Sch\"{o}lkopf, Bernhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae5e3ce40e0404a45ecacaaf05e5f735-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae5e3ce40e0404a45ecacaaf05e5f735-Metadata.json},
 openalex = {W2117962331},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae5e3ce40e0404a45ecacaaf05e5f735-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ae5e3ce40e0404a45ecacaaf05e5f735-Supplemental.zip},
 title = {Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_af21d0c9,
 abstract = {Optimal transport distances are a fundamental family of distances for probability measures and histograms of features. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost can quickly become prohibitive whenever the size of the support of these measures or the histograms' dimension exceeds a few hundred. We propose in this work a new family of optimal transport distances that look at transport problems from a maximum-entropy perspective. We smooth the classic optimal transport problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transport solvers. We also show that this regularized distance improves upon classic optimal transport distances on the MNIST classification problem.},
 author = {Cuturi, Marco},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Metadata.json},
 openalex = {W2158131535},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Reviews.html},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_afd48367,
 abstract = {Relational data-like graphs, networks, and matrices-is often dynamic, where the relational structure evolves over time. A fundamental problem in the analysis of time-varying network data is to extract a summary of the common structure and the dynamics of the underlying relations between the entities. Here we build on the intuition that changes in the network structure are driven by the dynamics at the level of groups of nodes. We propose a nonparametric multi-group membership model for dynamic networks. Our model contains three main components: We model the birth and death of individual groups with respect to the dynamics of the network structure via a distance dependent Indian Buffet Process. We capture the evolution of individual node group memberships via a Factorial Hidden Markov model. And, we explain the dynamics of the network structure by explicitly modeling the connectivity structure of groups. We demonstrate our model's capability of identifying the dynamics of latent groups in a number of different types of network data. Experimental results show that our model provides improved predictive performance over existing dynamic network models on future network forecasting and missing link prediction.},
 author = {Kim, Myunghwan and Leskovec, Jure},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/afd4836712c5e77550897e25711e1d96-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/afd4836712c5e77550897e25711e1d96-Metadata.json},
 openalex = {W2952779184},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/afd4836712c5e77550897e25711e1d96-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/afd4836712c5e77550897e25711e1d96-Reviews.html},
 title = {Nonparametric Multi-group Membership Model for Dynamic Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/afd4836712c5e77550897e25711e1d96-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b056eb15,
 abstract = {EDML is a recently proposed algorithm for learning parameters in Bayesian networks. It was originally derived in terms of approximate inference on a meta-network, which underlies the Bayesian approach to parameter estimation. While this initial derivation helped discover EDML in the first place and provided a concrete context for identifying some of its properties (e.g., in contrast to EM), the formal setting was somewhat tedious in the number of concepts it drew on. In this paper, we propose a greatly simplified perspective on EDML, which casts it as a general approach to continuous optimization. The new perspective has several advantages. First, it makes immediate some results that were non-trivial to prove initially. Second, it facilitates the design of EDML algorithms for new graphical models, leading to a new algorithm for learning parameters in Markov networks. We derive this algorithm in this paper, and show, empirically, that it can sometimes learn estimates more efficiently from complete data, compared to commonly used optimization methods, such as conjugate gradient and L-BFGS.},
 author = {Refaat, Khaled S and Choi, Arthur and Darwiche, Adnan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b056eb1587586b71e2da9acfe4fbd19e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b056eb1587586b71e2da9acfe4fbd19e-Metadata.json},
 openalex = {W2139938591},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b056eb1587586b71e2da9acfe4fbd19e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b056eb1587586b71e2da9acfe4fbd19e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b056eb1587586b71e2da9acfe4fbd19e-Supplemental.zip},
 title = {EDML for Learning Parameters in Directed and Undirected Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b056eb1587586b71e2da9acfe4fbd19e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b20bb95a,
 abstract = {Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parametrization of joint distributions. Among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increases quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efficient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size.},
 author = {Kalaitzis, Alfredo and Silva, Ricardo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b20bb95ab626d93fd976af958fbc61ba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b20bb95ab626d93fd976af958fbc61ba-Metadata.json},
 openalex = {W2949422027},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b20bb95ab626d93fd976af958fbc61ba-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b20bb95ab626d93fd976af958fbc61ba-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b20bb95ab626d93fd976af958fbc61ba-Supplemental.zip},
 title = {Flexible sampling of discrete data correlations without the marginal distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b20bb95ab626d93fd976af958fbc61ba-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b2f627ff,
 abstract = {We consider design of linear projection measurements for a vector Poisson signal model. The projections are performed on the vector Poisson rate, X∈ ℝn+, and the observed data are a vector of counts, Y ∈ ℤm+. The projection matrix is designed by maximizing mutual information between Y and X, I (Y; X). When there is a latent class label C ∈ {1,..., L) associated with X, we consider the mutual information with respect to Y and C, I(Y; C). New analytic expressions for the gradient of I(Y; X) and I(Y; C) are presented, with gradient performed with respect to the measurement matrix. Connections are made to the more widely studied Gaussian measurement model. Example results are presented for compressive topic modeling of a document corpora (word counting), and hyperspectral compressive sensing for chemical classification (photon counting).},
 author = {Wang, Liming and Carlson, David E and Rodrigues, Miguel and Wilcox, David and Calderbank, Robert and Carin, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b2f627fff19fda463cb386442eac2b3d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b2f627fff19fda463cb386442eac2b3d-Metadata.json},
 openalex = {W2140664634},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b2f627fff19fda463cb386442eac2b3d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b2f627fff19fda463cb386442eac2b3d-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b2f627fff19fda463cb386442eac2b3d-Supplemental.zip},
 title = {Designed Measurements for Vector Count Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b2f627fff19fda463cb386442eac2b3d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b337e84d,
 abstract = {Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the Sumatran tiger and Bengal tiger. Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.},
 author = {Socher, Richard and Chen, Danqi and Manning, Christopher D and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b337e84de8752b27eda3a12363109e80-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b337e84de8752b27eda3a12363109e80-Metadata.json},
 openalex = {W2127426251},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b337e84de8752b27eda3a12363109e80-Reviews.html},
 title = {Reasoning With Neural Tensor Networks for Knowledge Base Completion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b3ba8f1b,
 abstract = {Automatic music recommendation has become an increasingly relevant problem in recent years, since a lot of music is now sold and consumed digitally. Most recommender systems rely on collaborative filtering. However, this approach suffers from the cold start problem: it fails when no usage data is available, so it is not effective for recommending new and unpopular songs. In this paper, we propose to use a latent factor model for recommendation, and predict the latent factors from music audio when they cannot be obtained from usage data. We compare a traditional approach using a bag-of-words representation of the audio signals with deep convolutional neural networks, and evaluate the predictions quantitatively and qualitatively on the Million Song Dataset. We show that using predicted latent factors produces sensible recommendations, despite the fact that there is a large semantic gap between the characteristics of a song that affect user preference and the corresponding audio signal. We also show that recent advances in deep learning translate very well to the music recommendation setting, with deep convolutional neural networks significantly outperforming the traditional approach.},
 author = {van den Oord, Aaron and Dieleman, Sander and Schrauwen, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b3ba8f1bee1238a2f37603d90b58898d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b3ba8f1bee1238a2f37603d90b58898d-Metadata.json},
 openalex = {W2137028279},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b3ba8f1bee1238a2f37603d90b58898d-Reviews.html},
 title = {Deep content-based music recommendation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b3ba8f1bee1238a2f37603d90b58898d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b4d168b4,
 abstract = {Numerous datasets ranging from group memberships within social networks to purchase histories on e-commerce sites are represented by binary matrices. While this data is often either proprietary or sensitive, aggregated data, notably row and column marginals, is often viewed as much less sensitive, and may be furnished for analysis. Here, we investigate how these data can be exploited to make inferences about the underlying matrix H. Instead of assuming a generative model for H, we view the input marginals as constraints on the dataspace of possible realizations of H and compute the probability density function of particular entries H(i, j) of interest. We do this for all the cells of H simultaneously, without generating realizations, but rather via implicitly sampling the datasets that satisfy the input marginals. The end result is an efficient algorithm with asymptotic running time the same as that required by standard sampling techniques to generate a single dataset from the same dataspace. Our experimental evaluation demonstrates the efficiency and the efficacy of our framework in multiple settings.},
 author = {Golshan, Behzad and Byers, John and Terzi, Evimaria},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b4d168b48157c623fbd095b4a565b5bb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b4d168b48157c623fbd095b4a565b5bb-Metadata.json},
 openalex = {W2146463561},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b4d168b48157c623fbd095b4a565b5bb-Reviews.html},
 title = {What do row and column marginals reveal about your dataset},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b4d168b48157c623fbd095b4a565b5bb-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b51a15f3,
 abstract = {Sampling inference methods are computationally difficult to scale for many models in part because global dependencies can reduce opportunities for parallel computation. Without strict conditional independence structure among variables, standard Gibbs sampling theory requires sample updates to be performed sequentially, even if dependence between most variables is not strong. Empirical work has shown that some models can be sampled effectively by going and simply running Gibbs updates in parallel with only periodic global communication, but the successes and limitations of such a strategy are not well understood.

As a step towards such an understanding, we study the Hogwild Gibbs sampling strategy in the context of Gaussian distributions. We develop a framework which provides convergence conditions and error bounds along with simple proofs and connections to methods in numerical linear algebra. In particular, we show that if the Gaussian precision matrix is generalized diagonally dominant, then any Hogwild Gibbs sampler, with any update schedule or allocation of variables to processors, yields a stable sampling process with the correct sample mean.},
 author = {Johnson, Matthew J and Saunderson, James and Willsky, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b51a15f382ac914391a58850ab343b00-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b51a15f382ac914391a58850ab343b00-Metadata.json},
 openalex = {W2103133389},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b51a15f382ac914391a58850ab343b00-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b51a15f382ac914391a58850ab343b00-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b51a15f382ac914391a58850ab343b00-Supplemental.zip},
 title = {Analyzing Hogwild Parallel Gaussian Gibbs Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b51a15f382ac914391a58850ab343b00-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b6f0479a,
 abstract = {In this paper we present active learning algorithms in the context of structured prediction problems. To reduce the amount of labeling necessary to learn good models, our algorithms operate with weakly labeled data and we query additional examples based on entropies of local marginals, which are a good surrogate for uncertainty. We demonstrate the effectiveness of our approach in the task of 3D layout prediction from single images, and show that good models are learned when labeling only a handful of random variables. In particular, the same performance as using the full training set can be obtained while only labeling ~10% of the random variables.},
 author = {Luo, Wenjie and Schwing, Alex and Urtasun, Raquel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b6f0479ae87d244975439c6124592772-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b6f0479ae87d244975439c6124592772-Metadata.json},
 openalex = {W2119720396},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b6f0479ae87d244975439c6124592772-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b6f0479ae87d244975439c6124592772-Reviews.html},
 title = {Latent Structured Active Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b6f0479ae87d244975439c6124592772-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b73dfe25,
 abstract = {Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p-values.

We consider here a broad class of regression problems, and propose an efficient algorithm for constructing confidence intervals and p-values. The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power.

Our approach is based on constructing a 'de-biased' version of regularized M-estimators. The new construction improves over recent work in the field in that it does not assume a special structure on the design matrix. Furthermore, proofs are remarkably simple. We test our method on a diabetes prediction problem.},
 author = {Javanmard, Adel and Montanari, Andrea},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b73dfe25b4b8714c029b37a6ad3006fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b73dfe25b4b8714c029b37a6ad3006fa-Metadata.json},
 openalex = {W2108562800},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b73dfe25b4b8714c029b37a6ad3006fa-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b73dfe25b4b8714c029b37a6ad3006fa-Reviews.html},
 title = {Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b73dfe25b4b8714c029b37a6ad3006fa-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b7b16ecf,
 abstract = {Non-parametric approaches for analyzing network data based on exchangeable graph models (ExGM) have recently gained interest. The key object that defines an ExGM is often referred to as a graphon. This non-parametric perspective on network modeling poses challenging questions on how to make inference on the graphon underlying observed network data. In this paper, we propose a computationally efficient procedure to estimate a graphon from a set of observed networks generated from it. This procedure is based on a stochastic blockmodel approximation (SBA) of the graphon. We show that, by approximating the graphon with a stochastic block model, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity.},
 author = {Airoldi, Edo M and Costa, Thiago B and Chan, Stanley H},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7b16ecf8ca53723593894116071700c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7b16ecf8ca53723593894116071700c-Metadata.json},
 openalex = {W2123400271},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7b16ecf8ca53723593894116071700c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7b16ecf8ca53723593894116071700c-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7b16ecf8ca53723593894116071700c-Supplemental.zip},
 title = {Stochastic blockmodel approximation of a graphon: Theory and consistent estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b7b16ecf8ca53723593894116071700c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_b7bb35b9,
 abstract = {We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model's values (parameters and variables), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This significantly increases the proportion of time workers spend computing, as opposed to waiting. Furthermore, the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes.},
 author = {Ho, Qirong and Cipar, James and Cui, Henggang and Lee, Seunghak and Kim, Jin Kyu and Gibbons, Phillip B. and Gibson, Garth A and Ganger, Greg and Xing, Eric P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Metadata.json},
 openalex = {W2132737349},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Supplemental.zip},
 title = {More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/b7bb35b9c6ca2aee2df08cf09d7016c2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_bac9162b,
 abstract = {Nonnegative matrix factorization (NMF) is a popular data analysis method, the objective of which is to approximate a matrix with all nonnegative components into the product of two nonnegative matrices. In this work, we describe a new simple and efficient algorithm for multi-factor nonnegative matrix factorization (mfNMF) problem that generalizes the original NMF problem to more than two factors. Furthermore, we extend the mfNMF algorithm to incorporate a regularizer based on the Dirichlet distribution to encourage the sparsity of the components of the obtained factors. Our sparse mfNMF algorithm affords a closed form and an intuitive interpretation, and is more efficient in comparison with previous works that use fix point iterations. We demonstrate the effectiveness and efficiency of our algorithms on both synthetic and real data sets.},
 author = {Lyu, Siwei and Wang, Xin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bac9162b47c56fc8a4d2a519803d51b3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bac9162b47c56fc8a4d2a519803d51b3-Metadata.json},
 openalex = {W2122932202},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bac9162b47c56fc8a4d2a519803d51b3-Reviews.html},
 title = {On Algorithms for Sparse Multi-factor NMF},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/bac9162b47c56fc8a4d2a519803d51b3-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_bad5f337,
 abstract = {We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function Q* lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE[Q] episodes, where dimE denotes the eluder dimension. We establish further efficiency and asymptotic performance guarantees that apply even if Q* does not lie in Q, for the special case where Q is the span of pre-specified indicator functions over disjoint sets.},
 author = {Wen, Zheng and Van Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bad5f33780c42f2588878a9d07405083-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bad5f33780c42f2588878a9d07405083-Metadata.json},
 openalex = {W2115044435},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bad5f33780c42f2588878a9d07405083-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bad5f33780c42f2588878a9d07405083-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bad5f33780c42f2588878a9d07405083-Supplemental.zip},
 title = {Efficient Exploration and Value Function Generalization in Deterministic Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/bad5f33780c42f2588878a9d07405083-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_bca82e41,
 abstract = {We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve significant computational gains. We combine a non-ergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two sub-clusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for finite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods.},
 author = {Chang, Jason and Fisher III, John W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bca82e41ee7b0833588399b1fcd177c7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bca82e41ee7b0833588399b1fcd177c7-Metadata.json},
 openalex = {W2128032727},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bca82e41ee7b0833588399b1fcd177c7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bca82e41ee7b0833588399b1fcd177c7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bca82e41ee7b0833588399b1fcd177c7-Supplemental.zip},
 title = {Parallel Sampling of DP Mixture Models using Sub-Cluster Splits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/bca82e41ee7b0833588399b1fcd177c7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_bcc0d400,
 abstract = {Humans and animals readily utilize active sensing, or the use of self-motion, to focus sensory and cognitive resources on the behaviorally most relevant stimuli and events in the environment. Understanding the computational basis of natural active sensing is important both for advancing brain sciences and for developing more powerful artificial systems. Recently, we proposed a goal-directed, context-sensitive, Bayesian control strategy for active sensing, C-DAC (Context-Dependent Active Controller) (Ahmad & Yu, 2013). In contrast to previously proposed algorithms for human active vision, which tend to optimize abstract statistical objectives and therefore cannot adapt to changing behavioral context or task goals, C-DAC directly minimizes behavioral costs and thus, automatically adapts itself to different task conditions. However, C-DAC is limited as a model of human active sensing, given its computational/representational requirements, especially for more complex, real-world situations. Here, we propose a myopic approximation to C-DAC, which also takes behavioral costs into account, but achieves a significant reduction in complexity by looking only one step ahead. We also present data from a human active visual search experiment, and compare the performance of the various models against human behavior. We find that C-DAC and its myopic variant both achieve better fit to human data than Infomax (Butko & Movellan, 2010), which maximizes expected cumulative future information gain. In summary, this work provides novel experimental results that differentiate theoretical models for human active sensing, as well as a novel active sensing algorithm that retains the context-sensitivity of the optimal controller while achieving significant computational savings.},
 author = {Ahmad, Sheeraz and Huang, He and Yu, Angela J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Metadata.json},
 openalex = {W2135477696},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Reviews.html},
 title = {Context-sensitive active sensing in humans},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_bdb106a0,
 abstract = {A large number of algorithms in machine learning, from principal component analysis (PCA), and its non-linear (kernel) extensions, to more recent spectral embedding and support estimation methods, rely on estimating a linear subspace from samples. In this paper we introduce a general formulation of this problem and derive novel learning error estimates. Our results rely on natural assumptions on the spectral properties of the covariance operator associated to the data distribution, and hold for a wide class of metrics between subspaces. As special cases, we discuss sharp error estimates for the reconstruction properties of PCA and spectral support estimation. Key to our analysis is an operator theoretic approach that has broad applicability to spectral learning methods.},
 author = {Rudi, Alessandro and Canas, Guillermo D and Rosasco, Lorenzo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bdb106a0560c4e46ccc488ef010af787-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bdb106a0560c4e46ccc488ef010af787-Metadata.json},
 openalex = {W2137752431},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/bdb106a0560c4e46ccc488ef010af787-Reviews.html},
 title = {On the Sample Complexity of Subspace Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/bdb106a0560c4e46ccc488ef010af787-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c042f4db,
 abstract = {A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multitask learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-specific decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for specific a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state of the art.},
 author = {Becker, Carlos J and Christoudias, Christos M and Fua, Pascal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c042f4db68f23406c6cecf84a7ebb0fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c042f4db68f23406c6cecf84a7ebb0fe-Metadata.json},
 openalex = {W2150210531},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c042f4db68f23406c6cecf84a7ebb0fe-Reviews.html},
 title = {Non-Linear Domain Adaptation with Boosting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c042f4db68f23406c6cecf84a7ebb0fe-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c058f544,
 abstract = {We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion defining a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalizability of our algorithm on a variety of grocery checkout tasks, for whom, the preferences were not only influenced by the object being manipulated but also by the surrounding environment.\footnote{For more details and a demonstration video, visit: \url{http://pr.cs.cornell.edu/coactive}}},
 author = {Jain, Ashesh and Wojcik, Brian and Joachims, Thorsten and Saxena, Ashutosh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c058f544c737782deacefa532d9add4c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c058f544c737782deacefa532d9add4c-Metadata.json},
 openalex = {W2952217334},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c058f544c737782deacefa532d9add4c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c058f544c737782deacefa532d9add4c-Reviews.html},
 title = {Learning Trajectory Preferences for Manipulators via Iterative Improvement},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c058f544c737782deacefa532d9add4c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c06d06da,
 abstract = {We investigate the problem of learning the structure of a Markov network from data. It is shown that the structure of such networks can be described in terms of constraints which enables the use of existing solver technology with optimization capabilities to compute optimal networks starting from initial scores computed from the data. To achieve efficient encodings, we develop a novel characterization of Markov network structure using a balancing condition on the separators between cliques forming the network. The resulting translations into propositional satisfiability and its extensions such as maximum satisfiability, satisfiability modulo theories, and answer set programming, enable us to prove optimal certain network structures which have been previously found by stochastic search.},
 author = {Corander, Jukka and Janhunen, Tomi and Rintanen, Jussi and Nyman, Henrik and Pensar, Johan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c06d06da9666a219db15cf575aff2824-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c06d06da9666a219db15cf575aff2824-Metadata.json},
 openalex = {W2951456999},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c06d06da9666a219db15cf575aff2824-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c06d06da9666a219db15cf575aff2824-Reviews.html},
 title = {Learning Chordal Markov Networks by Constraint Satisfaction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c06d06da9666a219db15cf575aff2824-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c1e39d91,
 abstract = {We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [28]), and constrained minimization of submodular functions. We show that the complexity of all three problems depends on the of the submodular function, and provide lower and upper bounds that refine and improve previous results [2, 6, 8, 27]. Our proof techniques are fairly generic. We either use a black-box transformation of the function (for approximation and learning), or a transformation of algorithms to use an appropriate surrogate function (for minimization). Curiously, curvature has been known to influence approximations for submodular maximization [3, 29], but its effect on minimization, approximation and learning has hitherto been open. We complete this picture, and also support our theoretical claims by empirical results.},
 author = {Iyer, Rishabh K and Jegelka, Stefanie and Bilmes, Jeff A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c1e39d912d21c91dce811d6da9929ae8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c1e39d912d21c91dce811d6da9929ae8-Metadata.json},
 openalex = {W2168237161},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c1e39d912d21c91dce811d6da9929ae8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c1e39d912d21c91dce811d6da9929ae8-Reviews.html},
 title = {Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c1e39d912d21c91dce811d6da9929ae8-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c26820b8,
 abstract = {We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.},
 author = {Romera-Paredes, Bernardino and Pontil, Massimiliano},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c26820b8a4c1b3c2aa868d6d57e14a79-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c26820b8a4c1b3c2aa868d6d57e14a79-Metadata.json},
 openalex = {W4293876768},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c26820b8a4c1b3c2aa868d6d57e14a79-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c26820b8a4c1b3c2aa868d6d57e14a79-Supplemental.zip},
 title = {A New Convex Relaxation for Tensor Completion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c26820b8a4c1b3c2aa868d6d57e14a79-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c2aee861,
 abstract = {POMDPs provide a principled framework for planning under uncertainty, but are computationally intractable, due to the curse of dimensionality and the curse of history. This paper presents an online POMDP algorithm that alleviates these difficulties by focusing the search on a set of randomly sampled scenarios. A Determinized Sparse Partially Observable Tree (DESPOT) compactly captures the execution of all policies on these scenarios. Our Regularized DESPOT (R-DESPOT) algorithm searches the DESPOT for a policy, while optimally balancing the size of the policy and its estimated value obtained under the sampled scenarios. We give an output-sensitive performance bound for all policies derived from a DESPOT, and show that R-DESPOT works well if a small optimal policy exists. We also give an anytime algorithm that approximates R-DESPOT. Experiments show strong results, compared with two of the fastest online POMDP algorithms. Source code along with experimental settings are available at http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/.},
 author = {Somani, Adhiraj and Ye, Nan and Hsu, David and Lee, Wee Sun},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c2aee86157b4a40b78132f1e71a9e6f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c2aee86157b4a40b78132f1e71a9e6f1-Metadata.json},
 openalex = {W2124595631},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c2aee86157b4a40b78132f1e71a9e6f1-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c2aee86157b4a40b78132f1e71a9e6f1-Supplemental.zip},
 title = {DESPOT: Online POMDP Planning with Regularization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c2aee86157b4a40b78132f1e71a9e6f1-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c3c59e5f,
 abstract = {Multiple hypothesis testing is a significant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non-parametric method of estimating the FWER for a given $α$-threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we show that permutation testing in fact amounts to populating the columns of a very large matrix ${\bf P}$. By analyzing the spectrum of this matrix, under certain conditions, we see that ${\bf P}$ has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub--sampled --- on the order of $0.5\%$ --- matrix completion methods. Based on this observation, we propose a novel permutation testing methodology which offers a large speedup, without sacrificing the fidelity of the estimated FWER. Our evaluations on four different neuroimaging datasets show that a computational speedup factor of roughly $50\times$ can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated $α$-threshold is also recovered faithfully, and is stable.},
 author = {Hinrichs, Chris and Ithapu, Vamsi K and Sun, Qinyuan and Johnson, Sterling C and Singh, Vikas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c3c59e5f8b3e9753913f4d435b53c308-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c3c59e5f8b3e9753913f4d435b53c308-Metadata.json},
 openalex = {W2950484225},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c3c59e5f8b3e9753913f4d435b53c308-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c3c59e5f8b3e9753913f4d435b53c308-Supplemental.zip},
 title = {Speeding up Permutation Testing in Neuroimaging},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c3c59e5f8b3e9753913f4d435b53c308-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c3e878e2,
 abstract = {The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufficient information for the decoding of actor invariant to action, action invariant to actor, as well as the specific conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We find that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action snippets, produces surprisingly good fits to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights.},
 author = {Tan, Cheston and Singer, Jedediah M and Serre, Thomas and Sheinberg, David and Poggio, Tomaso},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c3e878e27f52e2a57ace4d9a76fd9acf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c3e878e27f52e2a57ace4d9a76fd9acf-Metadata.json},
 openalex = {W2157821749},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c3e878e27f52e2a57ace4d9a76fd9acf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c3e878e27f52e2a57ace4d9a76fd9acf-Reviews.html},
 title = {Neural representation of action sequences: how far can a simple snippet-matching model take us?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c3e878e27f52e2a57ace4d9a76fd9acf-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c45147de,
 abstract = {Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects defined as groupings of superpixels that are similar in intensity, color, and gradient orientation features. We introduce a novel parametric method of clustering superpixels by modeling mixture of Weibulls on Earth Mover's Distance statistics, then taking the normalized number of proto-objects following partitioning as our estimate of clutter perception. We validated this model using a new 90-image dataset of real world scenes rank ordered by human raters for clutter, and showed that our method not only predicted clutter extremely well (Spearman's ρ = 0.8038, p < 0.001), but also outperformed all existing clutter perception models and even a behavioral object segmentation ground truth. We conclude that the number of proto-objects in an image affects clutter perception more than the number of objects or features.},
 author = {Yu, Chen-Ping and Hua, Wen-Yu and Samaras, Dimitris and Zelinsky, Greg},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c45147dee729311ef5b5c3003946c48f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c45147dee729311ef5b5c3003946c48f-Metadata.json},
 openalex = {W2147915981},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c45147dee729311ef5b5c3003946c48f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c45147dee729311ef5b5c3003946c48f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c45147dee729311ef5b5c3003946c48f-Supplemental.zip},
 title = {Modeling Clutter Perception using Parametric Proto-object Partitioning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c45147dee729311ef5b5c3003946c48f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c4851e8e,
 abstract = {Unstructured social group activity recognition in web videos is a challenging task due to 1) the semantic gap between class labels and low-level visual features and 2) the lack of labeled training data. To tackle this problem, we propose a relevance topic for jointly learning meaningful mid-level representations upon bag-of-words (BoW) video representations and a classifier with sparse weights. In our approach, sparse Bayesian learning is incorporated into an undirected topic model (i.e., Replicated Softmax) to discover topics which are relevant to video classes and suitable for prediction. Rectified linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model. An efficient variational EM algorithm is presented for model parameter estimation and inference. Experimental results on the Unstructured Social Activity Attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classification accuracy, particularly in the case of a very small number of labeled training videos.},
 author = {Zhao, Fang and Huang, Yongzhen and Wang, Liang and Tan, Tieniu},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c4851e8e264415c4094e4e85b0baa7cc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c4851e8e264415c4094e4e85b0baa7cc-Metadata.json},
 openalex = {W2128604765},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c4851e8e264415c4094e4e85b0baa7cc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c4851e8e264415c4094e4e85b0baa7cc-Reviews.html},
 title = {Relevance Topic Model for Unstructured Social Group Activity Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c4851e8e264415c4094e4e85b0baa7cc-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c7e1249f,
 abstract = {We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents' types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classification of agents' types using agent-level data. We focus on applications involving data on agents' ranking over alternatives, and present theoretical conditions that establish the identifiability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach.},
 author = {Azari Soufiani, Hossein and Diao, Hansheng and Lai, Zhenyu and Parkes, David C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c7e1249ffc03eb9ded908c236bd1996d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c7e1249ffc03eb9ded908c236bd1996d-Metadata.json},
 openalex = {W2128720307},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c7e1249ffc03eb9ded908c236bd1996d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c7e1249ffc03eb9ded908c236bd1996d-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c7e1249ffc03eb9ded908c236bd1996d-Supplemental.zip},
 title = {Generalized Random Utility Models with Multiple Types},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c7e1249ffc03eb9ded908c236bd1996d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c850371f,
 abstract = {We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader.

The technique leads to the first nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T, of the optimal nonprivate regret bounds up to logarithmic factors in T. Our algorithms require logarithmic space and update time.},
 author = {Guha Thakurta, Abhradeep and Smith, Adam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c850371fda6892fbfd1c5a5b457e5777-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c850371fda6892fbfd1c5a5b457e5777-Metadata.json},
 openalex = {W2150230313},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c850371fda6892fbfd1c5a5b457e5777-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c850371fda6892fbfd1c5a5b457e5777-Supplemental.zip},
 title = {(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c850371fda6892fbfd1c5a5b457e5777-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c913303f,
 abstract = {We consider a situation in which we see samples in $\mathbb{R}^d$ drawn i.i.d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and incrementally adjusts the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates for both.},
 author = {Balsubramani, Akshay and Dasgupta, Sanjoy and Freund, Yoav},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c913303f392ffc643f7240b180602652-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c913303f392ffc643f7240b180602652-Metadata.json},
 openalex = {W2963867975},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c913303f392ffc643f7240b180602652-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c913303f392ffc643f7240b180602652-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c913303f392ffc643f7240b180602652-Supplemental.zip},
 title = {The Fast Convergence of Incremental PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c913303f392ffc643f7240b180602652-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_c9e1074f,
 abstract = {We present four major results towards solving decentralized partially observable Markov decision problems (DecPOMDPs) culminating in an algorithm that outperforms all existing algorithms on all but one standard infinite-horizon benchmark problems. (1) We give an integer program that solves collaborative Bayesian games (CBGs). The program is notable because its linear relaxation is very often integral. (2) We show that a DecPOMDP with bounded belief can be converted to a POMDP (albeit with actions exponential in the number of beliefs). These actions correspond to strategies of a CBG. (3) We present a method to transform any DecPOMDP into a DecPOMDP with bounded beliefs (the number of beliefs is a free parameter) using optimal (not lossless) belief compression. (4) We show that the combination of these results opens the door for new classes of DecPOMDP algorithms based on previous POMDP algorithms. We choose one such algorithm, point-based valued iteration, and modify it to produce the first tractable value iteration method for DecPOMDPs that outperforms existing algorithms.},
 author = {MacDermed, Liam C and Isbell, Charles L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c9e1074f5b3f9fc8ea15d152add07294-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c9e1074f5b3f9fc8ea15d152add07294-Metadata.json},
 openalex = {W2164851390},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/c9e1074f5b3f9fc8ea15d152add07294-Reviews.html},
 title = {Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/c9e1074f5b3f9fc8ea15d152add07294-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ca46c1b9,
 abstract = {Infinite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or find its maximum a posteriori solution by optimization. However, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based definition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.},
 author = {Fidaner, Isik B and Cemgil, Taylan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ca46c1b9512a7a8315fa3c5a946e8265-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ca46c1b9512a7a8315fa3c5a946e8265-Metadata.json},
 openalex = {W2128558710},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ca46c1b9512a7a8315fa3c5a946e8265-Reviews.html},
 title = {Summary Statistics for Partitionings and Feature Allocations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_cb70ab37,
 abstract = {Learning dynamic models from observed data has been a central issue in many scientific studies or engineering tasks. The usual setting is that data are collected sequentially from trajectories of some dynamical system operation. In quite a few modern scientific modeling tasks, however, it turns out that reliable sequential data are rather difficult to gather, whereas out-of-order snapshots are much easier to obtain. Examples include the modeling of galaxies, chronic diseases such Alzheimer's, or certain biological processes.

Existing methods for learning dynamic model from non-sequence data are mostly based on Expectation-Maximization, which involves non-convex optimization and is thus hard to analyze. Inspired by recent advances in spectral learning methods, we propose to study this problem from a different perspective: moment matching and spectral decomposition. Under that framework, we identify reasonable assumptions on the generative process of non-sequence data, and propose learning algorithms based on the tensor decomposition method [2] to provably recover first-order Markov models and hidden Markov models. To the best of our knowledge, this is the first formal guarantee on learning from non-sequence data. Preliminary simulation results confirm our theoretical findings.},
 author = {Huang, Tzu-Kuo and Schneider, Jeff},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cb70ab375662576bd1ac5aaf16b3fca4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cb70ab375662576bd1ac5aaf16b3fca4-Metadata.json},
 openalex = {W2167809057},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cb70ab375662576bd1ac5aaf16b3fca4-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cb70ab375662576bd1ac5aaf16b3fca4-Supplemental.zip},
 title = {Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_cbb6a3b8,
 abstract = {We study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies. To this end, we first propose a multiclass, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. We then introduce another type of bound targeting the approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.},
 author = {Babbar, Rohit and Partalas, Ioannis and Gaussier, Eric and Amini, Massih R.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Metadata.json},
 openalex = {W2109708711},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Supplemental.zip},
 title = {On Flat versus Hierarchical Classification in Large-Scale Taxonomies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/cbb6a3b884f4f88b3a8e3d44c636cbd8-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_cc1aa436,
 abstract = {We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd's answers is that workers' reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers' performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods.},
 author = {Liu, Qiang and Ihler, Alexander T and Steyvers, Mark},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cc1aa436277138f61cda703991069eaf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cc1aa436277138f61cda703991069eaf-Metadata.json},
 openalex = {W2136150876},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cc1aa436277138f61cda703991069eaf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cc1aa436277138f61cda703991069eaf-Reviews.html},
 title = {Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/cc1aa436277138f61cda703991069eaf-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_cd758e8f,
 abstract = {In this paper we investigate the problem of estimating the cluster tree for a density f supported on or near a smooth d-dimensional manifold M isometrically embedded in ℝD. We analyze a modified version of a k-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta (2010). The main results of this paper show that under mild assumptions on f and M, we obtain rates of convergence that depend on d only but not on the ambient dimension D. Finally, we sketch a construction of a sample complexity lower bound instance for a natural class of manifold oblivious clustering algorithms.},
 author = {Balakrishnan, Sivaraman and Narayanan, Srivatsan and Rinaldo, Alessandro and Singh, Aarti and Wasserman, Larry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cd758e8f59dfdf06a852adad277986ca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cd758e8f59dfdf06a852adad277986ca-Metadata.json},
 openalex = {W2141051596},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cd758e8f59dfdf06a852adad277986ca-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cd758e8f59dfdf06a852adad277986ca-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/cd758e8f59dfdf06a852adad277986ca-Supplemental.zip},
 title = {Cluster Trees on Manifolds},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/cd758e8f59dfdf06a852adad277986ca-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d10ec7c1,
 abstract = {We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as specific instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples.},
 author = {Amini, Arash and Nguyen, XuanLong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Metadata.json},
 openalex = {W2964250039},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d10ec7c16cbe9de8fbb1c42787c3ec26-Reviews.html},
 title = {Bayesian inference as iterated random functions with applications to sequential inference in graphical models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d10ec7c16cbe9de8fbb1c42787c3ec26-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d1f255a3,
 author = {Mostafa, Hesham and Mueller, Lorenz. K and Indiveri, Giacomo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d1f255a373a3cef72e03aa9d980c7eca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d1f255a373a3cef72e03aa9d980c7eca-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d1f255a373a3cef72e03aa9d980c7eca-Reviews.html},
 title = {Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d1f255a373a3cef72e03aa9d980c7eca-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d296c101,
 abstract = {Distance-based approaches to outlier detection are popular in data mining, as they do not require to model the underlying probability distribution, which is particularly challenging for high-dimensional data. We present an empirical comparison of various approaches to distance-based outlier detection across a large number of datasets. We report the surprising observation that a simple, sampling-based scheme outperforms state-of-the-art techniques in terms of both efficiency and effectiveness. To better understand this phenomenon, we provide a theoretical analysis why the sampling-based approach outperforms alternative methods based on k-nearest neighbor search.},
 author = {Sugiyama, Mahito and Borgwardt, Karsten},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d296c101daa88a51f6ca8cfc1ac79b50-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d296c101daa88a51f6ca8cfc1ac79b50-Metadata.json},
 openalex = {W2113060550},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d296c101daa88a51f6ca8cfc1ac79b50-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d296c101daa88a51f6ca8cfc1ac79b50-Supplemental.zip},
 title = {Rapid Distance-Based Outlier Detection via Sampling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d296c101daa88a51f6ca8cfc1ac79b50-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d2ed45a5,
 abstract = {Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classifiers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a significant advantage results from combining visual classifiers with the ability to identify an appropriate level of abstraction using Bayesian generalization.},
 author = {Jia, Yangqing and Abbott, Joshua T and Austerweil, Joseph L and Griffiths, Tom and Darrell, Trevor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d2ed45a52bc0edfa11c2064e9edee8bf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d2ed45a52bc0edfa11c2064e9edee8bf-Metadata.json},
 openalex = {W2124678650},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d2ed45a52bc0edfa11c2064e9edee8bf-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d2ed45a52bc0edfa11c2064e9edee8bf-Supplemental.zip},
 title = {Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d2ed45a52bc0edfa11c2064e9edee8bf-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d490d7b4,
 abstract = {Variational inference algorithms provide the most effective framework for large-scale training of Bayesian nonparametric models. Stochastic online approaches are promising, but are sensitive to the chosen learning rate and often converge to poor local optima. We present a new algorithm, memoized online variational inference, which scales to very large (yet finite) datasets while avoiding the complexities of stochastic gradient. Our algorithm maintains finite-dimensional sufficient statistics from batches of the full dataset, requiring some additional memory but still scaling to millions of examples. Exploiting nested families of variational bounds for infinite nonparametric models, we develop principled birth and merge moves allowing non-local optimization. Births adaptively add components to the model to escape local optima, while merges remove redundancy and improve speed. Using Dirichlet process mixture models for image clustering and denoising, we demonstrate major improvements in robustness and accuracy.},
 author = {Hughes, Michael C and Sudderth, Erik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d490d7b4576290fa60eb31b5fc917ad1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d490d7b4576290fa60eb31b5fc917ad1-Metadata.json},
 openalex = {W2131593562},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d490d7b4576290fa60eb31b5fc917ad1-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d490d7b4576290fa60eb31b5fc917ad1-Supplemental.zip},
 title = {Memoized Online Variational Inference for Dirichlet Process Mixture Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d490d7b4576290fa60eb31b5fc917ad1-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d554f7bb,
 abstract = {In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such locally adaptive smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to miscalibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a continuous multivariate stochastic process for time series having locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions in time, which are given nested Gaussian process priors and linearly related to the observed data through a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application.},
 author = {Durante, Daniele and Scarpa, Bruno and Dunson, David B},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d554f7bb7be44a7267068a7df88ddd20-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d554f7bb7be44a7267068a7df88ddd20-Metadata.json},
 openalex = {W2151044107},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d554f7bb7be44a7267068a7df88ddd20-Reviews.html},
 title = {Locally Adaptive Bayesian Multivariate Time Series},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d554f7bb7be44a7267068a7df88ddd20-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d64a340b,
 abstract = {We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise. It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns. We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further. SWAP is surprisingly effective in handling measurement matrices with high correlations. We prove that SWAP can easily be used as a wrapper around standard sparse recovery algorithms for improved performance. We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.},
 author = {Vats, Divyanshu and Baraniuk, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d64a340bcb633f536d56e51874281454-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d64a340bcb633f536d56e51874281454-Metadata.json},
 openalex = {W2113254621},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d64a340bcb633f536d56e51874281454-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d64a340bcb633f536d56e51874281454-Reviews.html},
 title = {When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d64a340bcb633f536d56e51874281454-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d6ef5f7f,
 abstract = {We establish lower bounds on minimax risks for distributed statistical estimation under a communication budget. Such lower bounds reveal the minimum amount of communication required by any procedure to achieve the centralized minimax-optimal rates for statistical estimation. We study two classes of protocols: one in which machines send messages independently, and a second allowing for interactive communication. We establish lower bounds for several problems, including various types of location models, as well as for parameter estimation in regression models.},
 author = {Zhang, Yuchen and Duchi, John and Jordan, Michael I and Wainwright, Martin J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d6ef5f7fa914c19931a55bb262ec879c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d6ef5f7fa914c19931a55bb262ec879c-Metadata.json},
 openalex = {W2162576315},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d6ef5f7fa914c19931a55bb262ec879c-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d6ef5f7fa914c19931a55bb262ec879c-Supplemental.zip},
 title = {Information-theoretic lower bounds for distributed statistical estimation with communication constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d6ef5f7fa914c19931a55bb262ec879c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d81f9c1b,
 abstract = {Multilayer perceptrons (MLPs) or neural networks are popular models used for nonlinear regression and classification tasks. As regressors, MLPs model the conditional distribution of the predictor variables Y given the input variables X. However, this predictive distribution is assumed to be unimodal (e.g. Gaussian). For tasks involving structured prediction, the conditional distribution should be multi-modal, resulting in one-to-many mappings. By using stochastic hidden variables rather than deterministic ones, Sigmoid Belief Nets (SBNs) can induce a rich multimodal distribution in the output space. However, previously proposed learning algorithms for SBNs are not efficient and unsuitable for modeling real-valued data. In this paper, we propose a stochastic feedforward network with hidden layers composed of both deterministic and stochastic variables. A new Generalized EM training procedure using importance sampling allows us to efficiently learn complicated conditional distributions. Our model achieves superior performance on synthetic and facial expressions datasets compared to conditional Restricted Boltzmann Machines and Mixture Density Networks. In addition, the latent features of our model improves classification and can learn to generate colorful textures of objects.},
 author = {Tang, Charlie and Salakhutdinov, Russ R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d81f9c1be2e08964bf9f24b15f0e4900-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d81f9c1be2e08964bf9f24b15f0e4900-Metadata.json},
 openalex = {W2135354436},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d81f9c1be2e08964bf9f24b15f0e4900-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d81f9c1be2e08964bf9f24b15f0e4900-Supplemental.zip},
 title = {Learning Stochastic Feedforward Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d81f9c1be2e08964bf9f24b15f0e4900-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d86ea612,
 abstract = {Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Specifically, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components specific to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efficient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and significantly outperform both standard PCA and robust PCA with rank constraints.},
 author = {Guo, Yuhong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d86ea612dec96096c5e0fcc8dd42ab6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d86ea612dec96096c5e0fcc8dd42ab6d-Metadata.json},
 openalex = {W2125092888},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d86ea612dec96096c5e0fcc8dd42ab6d-Reviews.html},
 title = {Robust Transfer Principal Component Analysis with Rank Constraints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d93ed5b6,
 abstract = {Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data.},
 author = {Snoek, Jasper and Zemel, Richard and Adams, Ryan P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d93ed5b6db83be78efb0d05ae420158e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d93ed5b6db83be78efb0d05ae420158e-Metadata.json},
 openalex = {W2131029730},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d93ed5b6db83be78efb0d05ae420158e-Reviews.html},
 title = {A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d93ed5b6db83be78efb0d05ae420158e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d9d4f495,
 abstract = {Psychophysical experiments have demonstrated that the brain integrates information from multiple sensory cues in a near Bayesian optimal manner. The present study proposes a novel mechanism to achieve this. We consider two reciprocally connected networks, mimicking the integration of heading direction information between the dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas. Each network serves as a local estimator and receives an independent cue, either the visual or the vestibular, as direct input for the external stimulus. We find that positive reciprocal interactions can improve the decoding accuracy of each individual network as if it implements Bayesian inference from two cues. Our model successfully explains the experimental finding that both MSTd and VIP achieve Bayesian multisensory integration, though each of them only receives a single cue as direct external input. Our result suggests that the brain may implement optimal information integration distributively at each local estimator through the reciprocal connections between cortical regions.},
 author = {Zhang, Wen-Hao and Wu, Si},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9d4f495e875a2e075a1a4a6e1b9770f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9d4f495e875a2e075a1a4a6e1b9770f-Metadata.json},
 openalex = {W2148231164},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9d4f495e875a2e075a1a4a6e1b9770f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9d4f495e875a2e075a1a4a6e1b9770f-Supplemental.zip},
 title = {Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_d9fc5b73,
 abstract = {A successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages, and iterate between updates to each. This paper observes that if the inference problem is smoothed through the addition of entropy terms, for fixed messages, the learning objective reduces to a traditional (non-structured) logistic regression problem with respect to parameters. In these logistic regression problems, each training example has a bias term determined by the current set of messages. Based on this insight, the structured energy function can be extended from linear factors to any function class where an oracle exists to minimize a logistic loss.},
 author = {Domke, Justin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9fc5b73a8d78fad3d6dffe419384e70-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9fc5b73a8d78fad3d6dffe419384e70-Metadata.json},
 openalex = {W2964190853},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9fc5b73a8d78fad3d6dffe419384e70-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/d9fc5b73a8d78fad3d6dffe419384e70-Supplemental.zip},
 title = {Structured Learning via Logistic Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/d9fc5b73a8d78fad3d6dffe419384e70-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_db2b4182,
 abstract = {Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor.

We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-the-art method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones.},
 author = {Mnih, Andriy and Kavukcuoglu, Koray},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Metadata.json},
 openalex = {W2097732278},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Reviews.html},
 title = {Learning word embeddings efficiently with noise-contrastive estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/db2b4182156b2f1f817860ac9f409ad7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_dc4c44f6,
 abstract = {In this paper we propose a class of efficient Generalized Method-of-Moments (GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run significantly faster than the classical Minorize-Maximization (MM) algorithm, while achieving competitive statistical efficiency.},
 author = {Azari Soufiani, Hossein and Chen, William and Parkes, David C and Xia, Lirong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc4c44f624d600aa568390f1f1104aa0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc4c44f624d600aa568390f1f1104aa0-Metadata.json},
 openalex = {W2122913573},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc4c44f624d600aa568390f1f1104aa0-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc4c44f624d600aa568390f1f1104aa0-Supplemental.zip},
 title = {Generalized Method-of-Moments for Rank Aggregation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/dc4c44f624d600aa568390f1f1104aa0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_dc58e3a3,
 author = {Gribonval, Remi and Machart, Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc58e3a306451c9d670adcd37004f48f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc58e3a306451c9d670adcd37004f48f-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc58e3a306451c9d670adcd37004f48f-Reviews.html},
 title = {Reconciling "priors" \&amp; "priors" without prejudice?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/dc58e3a306451c9d670adcd37004f48f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_dc6a6489,
 abstract = {In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background. In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem. Specifically, by using auxiliary natural images, we train a stacked de-noising autoencoder offline to learn generic image features that are more robust against variations. This is then followed by knowledge transfer from offline training to the online tracking process. Online tracking involves a classification neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classification layer. Both the feature extractor and the classifier can be further tuned to adapt to appearance changes of the moving object. Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is more accurate while maintaining low computational cost with real-time performance when our MATLAB implementation of the tracker is used with a modest graphics processing unit (GPU).},
 author = {Wang, Naiyan and Yeung, Dit-Yan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc6a6489640ca02b0d42dabeb8e46bb7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc6a6489640ca02b0d42dabeb8e46bb7-Metadata.json},
 openalex = {W2118097920},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc6a6489640ca02b0d42dabeb8e46bb7-Reviews.html},
 title = {Learning a Deep Compact Image Representation for Visual Tracking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/dc6a6489640ca02b0d42dabeb8e46bb7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_dc912a25,
 abstract = {We present and study a distributed optimization algorithm by employing a stochastic dual coordinate ascent method. Stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochastic gradient descent methods in optimizing regularized loss minimization problems. It still lacks of efforts in studying them in a distributed framework. We make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network, with an analysis of the tradeoff between computation and communication. We verify our analysis by experiments on real data sets. Moreover, we compare the proposed algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing SVMs in the same distributed framework, and observe competitive performances.},
 author = {Yang, Tianbao},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc912a253d1e9ba40e2c597ed2376640-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc912a253d1e9ba40e2c597ed2376640-Metadata.json},
 openalex = {W2123154536},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc912a253d1e9ba40e2c597ed2376640-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dc912a253d1e9ba40e2c597ed2376640-Supplemental.zip},
 title = {Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/dc912a253d1e9ba40e2c597ed2376640-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_dd77279f,
 abstract = {Natural actor-critics form a popular class of policy search algorithms for finding locally optimal policies for Markov decision processes. In this paper we address a drawback of natural actor-critics that limits their real-world applicability—their lack of safety guarantees. We present a principled algorithm for performing natural gradient descent over a constrained domain. In the context of reinforcement learning, this allows for natural actor-critic algorithms that are guaranteed to remain within a known safe region of policy space. While deriving our class of constrained natural actor-critic algorithms, which we call Projected Natural Actor-Critics (PNACs), we also elucidate the relationship between natural gradient descent and mirror descent.},
 author = {Thomas, Philip S. and Dabney, William C and Giguere, Stephen and Mahadevan, Sridhar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dd77279f7d325eec933f05b1672f6a1f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dd77279f7d325eec933f05b1672f6a1f-Metadata.json},
 openalex = {W2162831327},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dd77279f7d325eec933f05b1672f6a1f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/dd77279f7d325eec933f05b1672f6a1f-Reviews.html},
 title = {Projected Natural Actor-Critic},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/dd77279f7d325eec933f05b1672f6a1f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e0040614,
 abstract = {We design and analyze minimax-optimal algorithms for online linear optimization games where the player's choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. While the standard benchmark is the loss of the best strategy chosen from a bounded comparator set, we consider a very broad range of benchmark functions. The problem is cast as a sequential multi-stage zero-sum game, and we give a thorough analysis of the minimax behavior of the game, providing characterizations for the value of the game, as well as both the player's and the adversary's optimal strategy. We show how these objects can be computed efficiently under certain circumstances, and by selecting an appropriate benchmark, we construct a novel hedging strategy for an unconstrained betting game.},
 author = {McMahan, Brendan and Abernethy, Jacob},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e00406144c1e7e35240afed70f34166a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e00406144c1e7e35240afed70f34166a-Metadata.json},
 openalex = {W2109775151},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e00406144c1e7e35240afed70f34166a-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e00406144c1e7e35240afed70f34166a-Supplemental.zip},
 title = {Minimax Optimal Algorithms for Unconstrained Linear Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e00406144c1e7e35240afed70f34166a-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e034fb6b,
 abstract = {A long term goal of Interactive Reinforcement Learning is to incorporate nonexpert human feedback to solve complex tasks. Some state-of-the-art methods have approached this problem by mapping human information to rewards and values and iterating over them to compute better control policies. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct policy labels. We compare Advise to state-of-the-art approaches and show that it can outperform them and is robust to infrequent and inconsistent human feedback.},
 author = {Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e034fb6b66aacc1d48f445ddfb08da98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e034fb6b66aacc1d48f445ddfb08da98-Metadata.json},
 openalex = {W2098441518},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e034fb6b66aacc1d48f445ddfb08da98-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e034fb6b66aacc1d48f445ddfb08da98-Supplemental.zip},
 title = {Policy Shaping: Integrating Human Feedback with Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e034fb6b66aacc1d48f445ddfb08da98-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e1e32e23,
 abstract = {In this paper, we seek robust policies for uncertain Markov Decision Processes (MDPs). Most robust optimization approaches for these problems have focussed on the computation of maximin policies which maximize the value corresponding to the worst realization of the uncertainty. Recent work has proposed minimax regret as a suitable alternative to the maximin objective for robust optimization. However, existing algorithms for handling minimax regret are restricted to models with uncertainty over rewards only. We provide algorithms that employ sampling to improve across multiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence of model uncertainties across state, action pairs and decision epochs; (c) Scalability and quality bounds. Finally, to demonstrate the empirical effectiveness of our sampling approaches, we provide comparisons against benchmark algorithms on two domains from literature. We also provide a Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.},
 author = {Ahmed, Asrar and Varakantham, Pradeep and Adulyasak, Yossiri and Jaillet, Patrick},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e1e32e235eee1f970470a3a6658dfdd5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e1e32e235eee1f970470a3a6658dfdd5-Metadata.json},
 openalex = {W2161534526},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e1e32e235eee1f970470a3a6658dfdd5-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e1e32e235eee1f970470a3a6658dfdd5-Supplemental.zip},
 title = {Regret based Robust Solutions for Uncertain Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e3796ae8,
 abstract = {Despite growing interest and practical use in various scientific areas, variable importances derived from tree-based ensemble methods are not well understood from a theoretical point of view. In this work we characterize the Mean Decrease Impurity (MDI) variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions. We derive a three-level decomposition of the information jointly provided by all input variables about the output in terms of i) the MDI importance of each input variable, ii) the degree of interaction of a given input variable with the other input variables, iii) the different interaction terms of a given degree. We then show that this MDI importance of a variable is equal to zero if and only if the variable is irrelevant and that the MDI importance of a relevant variable is invariant with respect to the removal or the addition of irrelevant variables. We illustrate these properties on a simple example and discuss how they may change in the case of non-totally randomized trees such as Random Forests and Extra-Trees.},
 author = {Louppe, Gilles and Wehenkel, Louis and Sutera, Antonio and Geurts, Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e3796ae838835da0b6f6ea37bcf8bcb7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e3796ae838835da0b6f6ea37bcf8bcb7-Metadata.json},
 openalex = {W2169178923},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e3796ae838835da0b6f6ea37bcf8bcb7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e3796ae838835da0b6f6ea37bcf8bcb7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e3796ae838835da0b6f6ea37bcf8bcb7-Supplemental.zip},
 title = {Understanding variable importances in forests of randomized trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e3796ae838835da0b6f6ea37bcf8bcb7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e48e1320,
 abstract = {Several attempts to understand the success of simple decision heuristics have examined heuristics as an approximation to a linear decision rule. This research has identified three environmental structures that aid heuristics: dominance, cumulative dominance, and noncompensatoriness. This paper develops these ideas further and examines their empirical relevance in 51 natural environments. The results show that all three structures are prevalent, making it possible for simple rules to reach, and occasionally exceed, the accuracy of the linear decision rule, using less information and less computation.},
 author = {\c{S}im\c{s}ek, \"{O}zg\"{u}r},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e48e13207341b6bffb7fb1622282247b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e48e13207341b6bffb7fb1622282247b-Metadata.json},
 openalex = {W2143012969},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e48e13207341b6bffb7fb1622282247b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e48e13207341b6bffb7fb1622282247b-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e48e13207341b6bffb7fb1622282247b-Supplemental.zip},
 title = {Linear decision rule as aspiration for simple decision heuristics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e48e13207341b6bffb7fb1622282247b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e49b8b40,
 abstract = {Stacked sparse denoising autoencoders (SSDAs) have recently been shown to be successful at removing noise from corrupted images. However, like most denoising techniques, the SSDA is not robust to variation in noise types beyond what it has seen during training. To address this limitation, we present the adaptive multi-column stacked sparse denoising autoencoder (AMC-SSDA), a novel technique of combining multiple SSDAs by (1) computing optimal column weights via solving a nonlinear optimization program and (2) training a separate network to predict the optimal weights. We eliminate the need to determine the type of noise, let alone its statistics, at test time and even show that the system can be robust to noise not seen in the training set. We show that state-of-the-art denoising performance can be achieved with a single system on a variety of different noise types. Additionally, we demonstrate the efficacy of AMC-SSDA as a preprocessing (denoising) algorithm by achieving strong classification performance on corrupted MNIST digits.},
 author = {Agostinelli, Forest and Anderson, Michael R and Lee, Honglak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e49b8b4053df9505e1f48c3a701c0682-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e49b8b4053df9505e1f48c3a701c0682-Metadata.json},
 openalex = {W2151503710},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e49b8b4053df9505e1f48c3a701c0682-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e49b8b4053df9505e1f48c3a701c0682-Reviews.html},
 title = {Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e49b8b4053df9505e1f48c3a701c0682-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e53a0a29,
 abstract = {Movement Primitives (MP) are a well-established approach for representing modular and re-usable robot movement generators. Many state-of-the-art robot learning successes are based MPs, due to their compact representation of the inherently continuous and high dimensional robot movements. A major goal in robot learning is to combine multiple MPs as building blocks in a modular control architecture to solve complex tasks. To this effect, a MP representation has to allow for blending between motions, adapting to altered task variables, and co-activating multiple MPs in parallel. We present a probabilistic formulation of the MP concept that maintains a distribution over trajectories. Our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework. In order to use such a trajectory distribution for robot movement control, we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution. We evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios.},
 author = {Paraschos, Alexandros and Daniel, Christian and Peters, Jan R and Neumann, Gerhard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e53a0a2978c28872a4505bdb51db06dc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e53a0a2978c28872a4505bdb51db06dc-Metadata.json},
 openalex = {W2140801763},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e53a0a2978c28872a4505bdb51db06dc-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e53a0a2978c28872a4505bdb51db06dc-Supplemental.zip},
 title = {Probabilistic Movement Primitives},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e58cc5ca,
 abstract = {In standard matrix completion theory, it is required to have at least O(n ln2 n) observed entries to perfectly recover a low-rank matrix M of size n x n, leading to a large number of observations when n is large. In many real tasks, side information in addition to the observed entries is often available. In this work, we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries. We show that, under appropriate conditions, with the assistance of side information matrices, the number of observed entries needed for a perfect recovery of matrix M can be dramatically reduced to O(ln n). We demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning.},
 author = {Xu, Miao and Jin, Rong and Zhou, Zhi-Hua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e58cc5ca94270acaceed13bc82dfedf7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e58cc5ca94270acaceed13bc82dfedf7-Metadata.json},
 openalex = {W2120387782},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e58cc5ca94270acaceed13bc82dfedf7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e58cc5ca94270acaceed13bc82dfedf7-Supplemental.zip},
 title = {Speedup Matrix Completion with Side Information: Application to Multi-Label Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e58cc5ca94270acaceed13bc82dfedf7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e5e63da7,
 abstract = {Recent work on molecular programming has explored new possibilities for computational abstractions with biomolecules, including logic gates, neural networks, and linear systems. In the future such abstractions might enable nanoscale devices that can sense and control the world at a molecular scale. Just as in macroscale robotics, it is critical that such devices can learn about their environment and reason under uncertainty. At this small scale, systems are typically modeled as chemical reaction networks. In this work, we develop a procedure that can take arbitrary probabilistic graphical models, represented as factor graphs over discrete random variables, and compile them into chemical reaction networks that implement inference. In particular, we show that marginalization based on sum-product message passing can be implemented in terms of reactions between chemical species whose concentrations represent probabilities. We show algebraically that the steady state concentration of these species correspond to the marginal distributions of the random variables in the graph and validate the results in simulations. As with standard sum-product inference, this procedure yields exact results for tree-structured graphs, and approximate solutions for loopy graphs.},
 author = {Napp, Nils E and Adams, Ryan P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Metadata.json},
 openalex = {W2166918926},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Reviews.html},
 title = {Message Passing Inference with Chemical Reaction Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e5e63da79fcd2bebbd7cb8bf1c1d0274-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e5f6ad6c,
 abstract = {Discovering hierarchical regularities in data is a key problem in interacting with large datasets, modeling cognition, and encoding knowledge. A previous Bayesian solution—Kingman's coalescent—provides a probabilistic model for data represented as a binary tree. Unfortunately, this is inappropriate for data better described by bushier trees. We generalize an existing belief propagation framework of Kingman's coalescent to the beta coalescent, which models a wider range of tree structures. Because of the complex combinatorial search over possible structures, we develop new sampling schemes using sequential Monte Carlo and Dirichlet process mixture models, which render inference efficient and tractable. We present results on synthetic and real data that show the beta coalescent outperforms Kingman's coalescent and is qualitatively better at capturing data in bushy hierarchies.},
 author = {Hu, Yuening and Ying, Jordan L and Daume III, Hal and Ying, Z. Irene},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e5f6ad6ce374177eef023bf5d0c018b6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e5f6ad6ce374177eef023bf5d0c018b6-Metadata.json},
 openalex = {W2169923060},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e5f6ad6ce374177eef023bf5d0c018b6-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e5f6ad6ce374177eef023bf5d0c018b6-Supplemental.zip},
 title = {Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e5f6ad6ce374177eef023bf5d0c018b6-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e6d8545d,
 abstract = {Differential privacy is a cryptographically motivated definition of privacy which has gained considerable attention in the algorithms, machine-learning and data-mining communities. While there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application.

In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning - training a regularized linear classifier and building a histogram density estimator that result in end-to-end differentially private solutions for these problems.},
 author = {Chaudhuri, Kamalika and Vinterbo, Staal A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e6d8545daa42d5ced125a4bf747b3688-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e6d8545daa42d5ced125a4bf747b3688-Metadata.json},
 openalex = {W2104743167},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e6d8545daa42d5ced125a4bf747b3688-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e6d8545daa42d5ced125a4bf747b3688-Supplemental.zip},
 title = {A Stability-based Validation Procedure for Differentially Private Machine Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e836d813,
 abstract = {Finite-State Transducers (FST) are a standard tool for modeling paired input-output sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. presented a spectral algorithm for learning FST from samples of aligned input-output sequences. In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as finding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identifiability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efficiently.},
 author = {Bailly, Raphael and Carreras, Xavier and Quattoni, Ariadna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e836d813fd184325132fca8edcdfb40e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e836d813fd184325132fca8edcdfb40e-Metadata.json},
 openalex = {W2127630476},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e836d813fd184325132fca8edcdfb40e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e836d813fd184325132fca8edcdfb40e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e836d813fd184325132fca8edcdfb40e-Supplemental.zip},
 title = {Unsupervised Spectral Learning of Finite State Transducers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e836d813fd184325132fca8edcdfb40e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e96ed478,
 abstract = {We model a one-shot learning situation, where very few observations y1,...,yn ∈ ℝd are available. Associated with each observation yi is a very high-dimensional vector xi ∈ ℝd, which provides context for yi and enables us to predict subsequent observations, given their own context. One of the salient features of our analysis is that the problems studied here are easier when the dimension of xi is large; in other words, prediction becomes easier when more context is provided. The proposed methodology is a variant of principal component regression (PCR). Our rigorous analysis sheds new light on PCR. For instance, we show that classical PCR estimators may be inconsistent in the specified setting, unless they are multiplied by a scalar c > 1; that is, unless the classical estimator is expanded. This expansion phenomenon appears to be somewhat novel and contrasts with shrinkage methods (c < 1), which are far more common in big data analyses.},
 author = {Dicker, Lee H and Foster, Dean P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e96ed478dab8595a7dbda4cbcbee168f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e96ed478dab8595a7dbda4cbcbee168f-Metadata.json},
 openalex = {W2138244273},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e96ed478dab8595a7dbda4cbcbee168f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e96ed478dab8595a7dbda4cbcbee168f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e96ed478dab8595a7dbda4cbcbee168f-Supplemental.zip},
 title = {One-shot learning and big data with n=2},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e96ed478dab8595a7dbda4cbcbee168f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_e97ee205,
 abstract = {Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to find commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the first demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies.},
 author = {Schwartz, Yannick and Thirion, Bertrand and Varoquaux, Gael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e97ee2054defb209c35fe4dc94599061-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e97ee2054defb209c35fe4dc94599061-Metadata.json},
 openalex = {W2099580808},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/e97ee2054defb209c35fe4dc94599061-Reviews.html},
 title = {Mapping paradigm ontologies to and from the brain},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/e97ee2054defb209c35fe4dc94599061-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_eae27d77,
 abstract = {Consider an unweighted k-nearest neighbor graph on n points that have been sampled i.i.d. from some unknown density p on ℝd. We prove how one can estimate the density p just from the unweighted adjacency matrix of the graph, without knowing the points themselves or any distance or similarity scores. The key insights are that local differences in link numbers can be used to estimate a local function of the gradient of p, and that integrating this function along shortest paths leads to an estimate of the underlying density.},
 author = {Von Luxburg, Ulrike and Alamgir, Morteza},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eae27d77ca20db309e056e3d2dcd7d69-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eae27d77ca20db309e056e3d2dcd7d69-Metadata.json},
 openalex = {W2111288402},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eae27d77ca20db309e056e3d2dcd7d69-Reviews.html},
 title = {Density estimation from unweighted k-nearest neighbor graphs: a roadmap},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/eae27d77ca20db309e056e3d2dcd7d69-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_eb163727,
 abstract = {In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion. Variance-related risk measures are among the most common risk-sensitive criteria in finance and operations research. However, optimizing many such criteria is known to be a hard problem. In this paper, we consider both discounted and average reward Markov decision processes. For each formulation, we first define a measure of variability for a policy, which in turn gives us a set of risk-sensitive criteria to optimize. For each of these criteria, we derive a formula for computing its gradient. We then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in a traffic signal control application.},
 author = {L.A., Prashanth and Ghavamzadeh, Mohammad},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb163727917cbba1eea208541a643e74-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb163727917cbba1eea208541a643e74-Metadata.json},
 openalex = {W2141203641},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb163727917cbba1eea208541a643e74-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb163727917cbba1eea208541a643e74-Reviews.html},
 title = {Actor-Critic Algorithms for Risk-Sensitive MDPs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/eb163727917cbba1eea208541a643e74-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_eb6fdc36,
 abstract = {Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is defined as a geometric fit to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images.},
 author = {Zhang, Miaomiao and Fletcher, Tom},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb6fdc36b281b7d5eabf33396c2683a2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb6fdc36b281b7d5eabf33396c2683a2-Metadata.json},
 openalex = {W2148862398},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb6fdc36b281b7d5eabf33396c2683a2-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb6fdc36b281b7d5eabf33396c2683a2-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb6fdc36b281b7d5eabf33396c2683a2-Supplemental.zip},
 title = {Probabilistic Principal Geodesic Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/eb6fdc36b281b7d5eabf33396c2683a2-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_eb86d510,
 abstract = {In this paper, we study the following new variant of prototype learning, called k-prototype learning problem for 3D rigid structures: Given a set of 3D rigid structures, find a set of k rigid structures so that each of them is a prototype for a cluster of the given rigid structures and the total cost (or dissimilarity) is minimized. Prototype learning is a core problem in machine learning and has a wide range of applications in many areas. Existing results on this problem have mainly focused on the graph domain. In this paper, we present the first algorithm for learning multiple prototypes from 3D rigid structures. Our result is based on a number of new insights to rigid structures alignment, clustering, and prototype reconstruction, and is practically efficient with quality guarantee. We validate our approach using two type of data sets, random data and biological data of chromosome territories. Experiments suggest that our approach can effectively learn prototypes in both types of data.},
 author = {Ding, Hu and Berezney, Ronald and Xu, Jinhui},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb86d510361fc23b59f18c1bc9802cc6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb86d510361fc23b59f18c1bc9802cc6-Metadata.json},
 openalex = {W2135033879},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eb86d510361fc23b59f18c1bc9802cc6-Reviews.html},
 title = {k-Prototype Learning for 3D Rigid Structures},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/eb86d510361fc23b59f18c1bc9802cc6-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ebd9629f,
 abstract = {Discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations. However, the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications, often dominating the cost of inference in the models. Significant efforts have been devoted to sparsity-based model selection to decrease this cost. Such feature selection methods control computation statically and miss the opportunity to fine-tune feature extraction to each input at run-time. We address the key challenge of learning to control fine-grained feature extraction adaptively, exploiting non-homogeneity of the data. We propose an architecture that uses a rich feedback loop between extraction and prediction. The run-time control policy is learned using efficient value-function approximation, which adaptively determines the value of information of features at the level of individual variables for each input. We demonstrate significant speedups over state-of-the-art methods on two challenging datasets. For articulated pose estimation in video, we achieve a more accurate state-of-the-art model that is also faster, with similar results on an OCR task.},
 author = {Weiss, David J and Taskar, Ben},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Metadata.json},
 openalex = {W2108599331},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Reviews.html},
 title = {Learning Adaptive Value of Information for Structured Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ebd9629fc3ae5e9f6611e2ee05a31cef-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_eddb904a,
 abstract = {Entropy rate quantifies the amount of disorder in a stochastic process. For spiking neurons, the entropy rate places an upper bound on the rate at which the spike train can convey stimulus information, and a large literature has focused on the problem of estimating entropy rate from spike train data. Here we present Bayes least squares and empirical Bayesian entropy rate estimators for binary spike trains using hierarchical Dirichlet process (HDP) priors. Our estimator leverages the fact that the entropy rate of an ergodic Markov Chain with known transition probabilities can be calculated analytically, and many stochastic processes that are non-Markovian can still be well approximated by Markov processes of sufficient depth. Choosing an appropriate depth of Markov model presents challenges due to possibly long time dependencies and short data sequences: a deeper model can better account for long time dependencies, but is more difficult to infer from limited data. Our approach mitigates this difficulty by using a hierarchical prior to share statistical power across Markov chains of different depths. We present both a fully Bayesian and empirical Bayes entropy rate estimator based on this model, and demonstrate their performance on simulated and real neural spike train data.},
 author = {Knudson, Karin C and Pillow, Jonathan W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eddb904a6db773755d2857aacadb1cb0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eddb904a6db773755d2857aacadb1cb0-Metadata.json},
 openalex = {W2135765767},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/eddb904a6db773755d2857aacadb1cb0-Reviews.html},
 title = {Spike train entropy-rate estimation using hierarchical Dirichlet process priors},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/eddb904a6db773755d2857aacadb1cb0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ef0d3930,
 abstract = {We provide novel theoretical results regarding local optima of regularized M-estimators, allowing for nonconvexity in both loss and penalty functions. Under restricted strong convexity on the loss and suitable regularity conditions on the penalty, we prove that any stationary point of the composite objective function will lie within statistical precision of the underlying parameter vector. Our theory covers many nonconvex objective functions of interest, including the corrected Lasso for errors-in-variables linear models; regression for generalized linear models with nonconvex penalties such as SCAD, MCP, and capped-l 1; and high-dimensional graphical model estimation. We quantify statistical accuracy by providing bounds on the l1-, l2-, and prediction error between stationary points and the population-level optimum. We also propose a simple modification of composite gradient descent that may be used to obtain a near-global optimum within statistical precision estat in log(1/estat) steps, which is the fastest possible rate of any first-order method. We provide simulation studies illustrating the sharpness of our theoretical results.},
 author = {Loh, Po-Ling and Wainwright, Martin J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ef0d3930a7b6c95bd2b32ed45989c61f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ef0d3930a7b6c95bd2b32ed45989c61f-Metadata.json},
 openalex = {W2616050959},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ef0d3930a7b6c95bd2b32ed45989c61f-Reviews.html},
 title = {Regularized M-estimators with nonconvexity: statistical and algorithmic theory for local optima},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ef0d3930a7b6c95bd2b32ed45989c61f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ef575e88,
 abstract = {In stochastic optimal control the distribution of the exogenous noise is typically unknown and must be inferred from limited data before dynamic programming (DP)-based solution schemes can be applied. If the conditional expectations in the DP recursions are estimated via kernel regression, however, the historical sample paths enter the solution procedure directly as they determine the evaluation points of the cost-to-go functions. The resulting data-driven DP scheme is asymptotically consistent and admits an efficient computational solution when combined with parametric value function approximations. If training data is sparse, however, the estimated cost-to-go functions display a high variability and an optimistic bias, while the corresponding control policies perform poorly in out-of-sample tests. To mitigate these small sample effects, we propose a robust data-driven DP scheme, which replaces the expectations in the DP recursions with worst-case expectations over a set of distributions close to the best estimate. We show that the arising min-max problems in the DP recursions reduce to tractable conic programs. We also demonstrate that the proposed robust DP algorithm dominates various non-robust schemes in out-of-sample tests across several application domains.},
 author = {Hanasusanto, Grani Adiwena and Kuhn, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ef575e8837d065a1683c022d2077d342-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ef575e8837d065a1683c022d2077d342-Metadata.json},
 openalex = {W2147860430},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ef575e8837d065a1683c022d2077d342-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ef575e8837d065a1683c022d2077d342-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ef575e8837d065a1683c022d2077d342-Supplemental.zip},
 title = {Robust Data-Driven Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ef575e8837d065a1683c022d2077d342-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f033ab37,
 abstract = {Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both considered as the state-of-the-art methods for subspace clustering. The two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of Self-Expressiveness. The main difference is that SSC minimizes the vector l1 norm of the representation matrix to induce sparsity while LRR minimizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the representation matrix is often simultaneously sparse and low-rank, we propose a new algorithm, termed Low-Rank Sparse Subspace Clustering (LRSSC), by combining SSC and LRR, and develops theoretical guarantees of when the algorithm succeeds. The results reveal interesting insights into the strength and weakness of SSC and LRR and demonstrate how LRSSC can take the advantages of both methods in preserving the Self-Expressiveness Property and Graph Connectivity at the same time.},
 author = {Wang, Yu-Xiang and Xu, Huan and Leng, Chenlei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f033ab37c30201f73f142449d037028d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f033ab37c30201f73f142449d037028d-Metadata.json},
 openalex = {W2144902590},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f033ab37c30201f73f142449d037028d-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f033ab37c30201f73f142449d037028d-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f033ab37c30201f73f142449d037028d-Supplemental.zip},
 title = {Provable Subspace Clustering: When LRR meets SSC},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f033ab37c30201f73f142449d037028d-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f0dd4a99,
 abstract = {We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. First, we recover the Mirror Prox algorithm for offline optimization, prove an extension to Holder-smooth functions, and apply the results to saddle-point type problems. Next, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a finite zero-sum matrix game to converge to the minimax equilibrium at the rate of O((log T)/T). This addresses a question of Daskalakis et al [6]. Further, we consider a partial information version of the problem. We then apply the results to convex programming and exhibit a simple algorithm for the approximate Max Flow problem.},
 author = {Rakhlin, Sasha and Sridharan, Karthik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f0dd4a99fba6075a9494772b58f95280-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f0dd4a99fba6075a9494772b58f95280-Metadata.json},
 openalex = {W2143343660},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f0dd4a99fba6075a9494772b58f95280-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f0dd4a99fba6075a9494772b58f95280-Reviews.html},
 title = {Optimization, Learning, and Games with Predictable Sequences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f0dd4a99fba6075a9494772b58f95280-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f1b6f285,
 abstract = {We go beyond the notion of pairwise similarity and look into search problems with k-way similarity functions. In this paper, we focus on problems related to 3-way Jaccard similarity: $\mathcal{R}^{3way}= \frac{|S_1 \cap S_2 \cap S_3|}{|S_1 \cup S_2 \cup S_3|}$, $S_1, S_2, S_3 \in \mathcal{C}$, where $\mathcal{C}$ is a size n collection of sets (or binary vectors). We show that approximate R3way similarity search problems admit fast algorithms with provable guarantees, analogous to the pairwise case. Our analysis and speedup guarantees naturally extend to k-way resemblance. In the process, we extend traditional framework of locality sensitive hashing (LSH) to handle higher-order similarities, which could be of independent theoretical interest. The applicability of $\mathcal{R}^{3way}$ search is shown on the Google Sets application. In addition, we demonstrate the advantage of $\mathcal{R}^{3way}$ resemblance over the pairwise case in improving retrieval quality.},
 author = {Shrivastava, Anshumali and Li, Ping},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Metadata.json},
 openalex = {W2151596444},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f1b6f2857fb6d44dd73c7041e0aa0f19-Reviews.html},
 title = {Beyond Pairwise: Provably Fast Algorithms for Approximate k-Way Similarity Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f1b6f2857fb6d44dd73c7041e0aa0f19-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f2201f51,
 abstract = {How are firing rates in a spiking network related to neural input, connectivity and network function? This is an important problem because firing rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difficult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating firing rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate firing rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate firing rates by finding the solution to the algorithm. Our firing rate calculation relates network firing rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems.},
 author = {Barrett, David G and Den\`{e}ve, Sophie and Machens, Christian K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f2201f5191c4e92cc5af043eebfd0946-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f2201f5191c4e92cc5af043eebfd0946-Metadata.json},
 openalex = {W2168097621},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f2201f5191c4e92cc5af043eebfd0946-Reviews.html},
 title = {Firing rate predictions in optimal balanced networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f2201f5191c4e92cc5af043eebfd0946-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f33ba15e,
 abstract = {Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efficiency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to find optimal hyperparameter settings more efficiently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method significantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyper-parameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost.},
 author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f33ba15effa5c10e873bf3842afb46a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f33ba15effa5c10e873bf3842afb46a6-Metadata.json},
 openalex = {W2113145584},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f33ba15effa5c10e873bf3842afb46a6-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f33ba15effa5c10e873bf3842afb46a6-Supplemental.zip},
 title = {Multi-Task Bayesian Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f340f1b1,
 abstract = {In the mixture models problem it is assumed that there are K distributions θ1,..., θK and one gets to observe a sample from a mixture of these distributions with unknown coefficients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same K underlying distributions, but with different mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sample, we prove that it is possible to use the differences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data.},
 author = {Lee, Jason D and Gilad-Bachrach, Ran and Caruana, Rich},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f340f1b1f65b6df5b5e3f94d95b11daf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f340f1b1f65b6df5b5e3f94d95b11daf-Metadata.json},
 openalex = {W2133949054},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f340f1b1f65b6df5b5e3f94d95b11daf-Reviews.html},
 title = {Using multiple samples to learn mixture models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f340f1b1f65b6df5b5e3f94d95b11daf-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f387624d,
 abstract = {Lifting attempts to speedup probabilistic inference by exploiting symmetries in the model. Exact lifted inference methods, like their propositional counterparts, work by recursively decomposing the model and the problem. In the propositional case, there exist formal structures, such as decomposition trees (dtrees), that represent such a decomposition and allow us to determine the complexity of inference a priori. However, there is currently no equivalent structure nor analogous complexity results for lifted inference. In this paper, we introduce FO-dtrees, which upgrade propositional dtrees to the first-order level. We show how these trees can characterize a lifted inference solution for a probabilistic logical model (in terms of a sequence of lifted operations), and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree.},
 author = {Taghipour, Nima and Davis, Jesse and Blockeel, Hendrik},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f387624df552cea2f369918c5e1e12bc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f387624df552cea2f369918c5e1e12bc-Metadata.json},
 openalex = {W2137820498},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f387624df552cea2f369918c5e1e12bc-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f387624df552cea2f369918c5e1e12bc-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f387624df552cea2f369918c5e1e12bc-Supplemental.zip},
 title = {First-order Decomposition Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f387624df552cea2f369918c5e1e12bc-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f4552671,
 abstract = {Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a specified threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional benefit to noisy neurons in biological neuronal networks.},
 author = {Karbasi, Amin and Salavati, Amir Hesam and Shokrollahi, Amin and Varshney, Lav R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4552671f8909587cf485ea990207f3b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4552671f8909587cf485ea990207f3b-Metadata.json},
 openalex = {W2136191326},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4552671f8909587cf485ea990207f3b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4552671f8909587cf485ea990207f3b-Reviews.html},
 title = {Noise-Enhanced Associative Memories},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f4552671f8909587cf485ea990207f3b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f4573fc7,
 abstract = {Maximization of submodular functions has wide applications in machine learning and artificial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known. In this paper, we study the setting where the expected gain is initially unknown, and it is learned by interacting repeatedly with the optimized function. We propose an efficient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem.},
 author = {Gabillon, Victor and Kveton, Branislav and Wen, Zheng and Eriksson, Brian and Muthukrishnan, S.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4573fc71c731d5c362f0d7860945b88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4573fc71c731d5c362f0d7860945b88-Metadata.json},
 openalex = {W2130488484},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4573fc71c731d5c362f0d7860945b88-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4573fc71c731d5c362f0d7860945b88-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4573fc71c731d5c362f0d7860945b88-Supplemental.zip},
 title = {Adaptive Submodular Maximization in Bandit Setting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f4573fc71c731d5c362f0d7860945b88-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f4be0027,
 abstract = {We propose an approximate inference algorithm for continuous time Gaussian Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid fixed point iteration consisting of (1) expectation propagation updates for discrete time terms and (2) variational updates for the continuous time term. We introduce post-inference corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and significant computational savings compared to discrete-time approaches in a neural application.},
 author = {Cseke, Botond and Opper, Manfred and Sanguinetti, Guido},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4be00279ee2e0a53eafdaa94a151e2c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4be00279ee2e0a53eafdaa94a151e2c-Metadata.json},
 openalex = {W2117804315},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4be00279ee2e0a53eafdaa94a151e2c-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4be00279ee2e0a53eafdaa94a151e2c-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f4be00279ee2e0a53eafdaa94a151e2c-Supplemental.zip},
 title = {Approximate inference in latent Gaussian-Markov models from continuous time observations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f4be00279ee2e0a53eafdaa94a151e2c-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f5deaeea,
 abstract = {Inspired by a two-level theory from political science that unifies agenda setting and ideological framing, we propose supervised hierarchical latent Dirichlet allocation (SHLDA), which jointly captures documents' multi-level topic structure and their polar response variables. Our model extends the nested Chinese restaurant processes to discover tree-structured topic hierarchies and uses both per-topic hierarchical and per-word lexical regression parameters to model response variables. SHLDA improves prediction on political affiliation and sentiment tasks in addition to providing insight into how topics under discussion are framed.},
 author = {Nguyen, Viet-An and Ying, Jordan L and Resnik, Philip},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f5deaeeae1538fb6c45901d524ee2f98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f5deaeeae1538fb6c45901d524ee2f98-Metadata.json},
 openalex = {W2152677317},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f5deaeeae1538fb6c45901d524ee2f98-Reviews.html},
 title = {Lexical and Hierarchical Topic Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f5deaeeae1538fb6c45901d524ee2f98-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f64eac11,
 abstract = {In the last decade, policy gradient methods have significantly grown in popularity in the reinforcement-learning field. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identification of effective gradient directions and the proposal of efficient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly influenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step-size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second-order polynomial of the step size, and we show how a simplified version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear-quadratic regulator problem.},
 author = {Pirotta, Matteo and Restelli, Marcello and Bascetta, Luca},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f64eac11f2cd8f0efa196f8ad173178e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f64eac11f2cd8f0efa196f8ad173178e-Metadata.json},
 openalex = {W2112964839},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f64eac11f2cd8f0efa196f8ad173178e-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f64eac11f2cd8f0efa196f8ad173178e-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f64eac11f2cd8f0efa196f8ad173178e-Supplemental.zip},
 title = {Adaptive Step-Size for Policy Gradient Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f64eac11f2cd8f0efa196f8ad173178e-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f73b76ce,
 abstract = {It is well known that the optimal convergence rate for stochastic optimization of smooth functions is O(1/√T), which is same as stochastic optimization of Lipschitz continuous convex functions. This is in contrast to optimizing smooth functions using full gradients, which yields a convergence rate of O(1/T2). In this work, we consider a new setup for optimizing smooth functions, termed as Mixed Optimization, which allows to access both a stochastic oracle and a full gradient oracle. Our goal is to significantly improve the convergence rate of stochastic optimization of smooth functions by having an additional small number of accesses to the full gradient oracle. We show that, with an O(ln T) calls to the full gradient oracle and an O(T) calls to the stochastic oracle, the proposed mixed optimization algorithm is able to achieve an optimization error of O(1/T).},
 author = {Mahdavi, Mehrdad and Zhang, Lijun and Jin, Rong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f73b76ce8949fe29bf2a537cfa420e8f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f73b76ce8949fe29bf2a537cfa420e8f-Metadata.json},
 openalex = {W2162069736},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f73b76ce8949fe29bf2a537cfa420e8f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f73b76ce8949fe29bf2a537cfa420e8f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f73b76ce8949fe29bf2a537cfa420e8f-Supplemental.zip},
 title = {Mixed Optimization for Smooth Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f73b76ce8949fe29bf2a537cfa420e8f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f7cade80,
 abstract = {Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We define a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC.},
 author = {Szegedy, Christian and Toshev, Alexander and Erhan, Dumitru},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f7cade80b7cc92b991cf4d2806d6bd78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f7cade80b7cc92b991cf4d2806d6bd78-Metadata.json},
 openalex = {W2130306094},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f7cade80b7cc92b991cf4d2806d6bd78-Reviews.html},
 title = {Deep Neural Networks for Object Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f7cade80b7cc92b991cf4d2806d6bd78-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f7e6c855,
 abstract = {For data assumed to come from a finite mixture with an unknown number of components, it has become common to use Dirichlet process mixtures (DPMs) not only for density estimation, but also for inferences about the number of components. The typical approach is to use the posterior distribution on the number of components occurring so far --- that is, the posterior on the number of clusters in the observed data. However, it turns out that this posterior is not consistent --- it does not converge to the true number of components. In this note, we give an elementary demonstration of this inconsistency in what is perhaps the simplest possible setting: a DPM with normal components of unit variance, applied to data from a "mixture" with one standard normal component. Further, we find that this example exhibits severe inconsistency: instead of going to 1, the posterior probability that there is one cluster goes to 0.},
 author = {Miller, Jeffrey W and Harrison, Matthew T},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f7e6c85504ce6e82442c770f7c8606f0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f7e6c85504ce6e82442c770f7c8606f0-Metadata.json},
 openalex = {W2950724769},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f7e6c85504ce6e82442c770f7c8606f0-Reviews.html},
 title = {A simple example of Dirichlet process mixture inconsistency for the number of components},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f7e6c85504ce6e82442c770f7c8606f0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f91e24df,
 abstract = {We use the notion of local Rademacher complexity to design new algorithms for learning kernels. Our algorithms thereby benefit from the sharper learning bounds based on that notion which, under certain general conditions, guarantee a faster convergence rate. We devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efficient solution using existing learning kernel techniques, and another one that can be formulated as a DC-programming problem for which we describe a solution in detail. We also report the results of experiments with both algorithms in both binary and multi-class classification tasks.},
 author = {Cortes, Corinna and Kloft, Marius and Mohri, Mehryar},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f91e24dfe80012e2a7984afa4480a6d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f91e24dfe80012e2a7984afa4480a6d6-Metadata.json},
 openalex = {W2105463715},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f91e24dfe80012e2a7984afa4480a6d6-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f91e24dfe80012e2a7984afa4480a6d6-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f91e24dfe80012e2a7984afa4480a6d6-Supplemental.zip},
 title = {Learning Kernels Using Local Rademacher Complexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f91e24dfe80012e2a7984afa4480a6d6-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f9a40a47,
 abstract = {Shannon's entropy is a basic quantity in information theory, and a fundamental building block for the analysis of neural codes. Estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable attention in statistics and theoretical neuroscience. However, neural responses have characteristic statistical structure that generic entropy estimators fail to exploit. For example, existing Bayesian entropy estimators make the naive assumption that all spike words are equally likely a priori, which makes for an inefficient allocation of prior probability mass in cases where spikes are sparse. Here we develop Bayesian estimators for the entropy of binary spike trains using priors designed to flexibly exploit the statistical structure of simultaneously-recorded spike responses. We define two prior distributions over spike words using mixtures of Dirichlet distributions centered on simple parametric models. The parametric model captures high-level statistical features of the data, such as the average spike count in a spike word, which allows the posterior over entropy to concentrate more rapidly than with standard estimators (e.g., in cases where the probability of spiking differs strongly from 0.5). Conversely, the Dirichlet distributions assign prior mass to distributions far from the parametric model, ensuring consistent estimates for arbitrary distributions. We devise a compact representation of the data and prior that allow for computationally efficient implementations of Bayesian least squares and empirical Bayes entropy estimators with large numbers of neurons. We apply these estimators to simulated and real neural data and show that they substantially outperform traditional methods.},
 author = {Archer, Evan W and Park, Il Memming and Pillow, Jonathan W},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f9a40a4780f5e1306c46f1c8daecee3b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f9a40a4780f5e1306c46f1c8daecee3b-Metadata.json},
 openalex = {W2134876548},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f9a40a4780f5e1306c46f1c8daecee3b-Reviews.html},
 title = {Bayesian entropy estimation for binary spike train data using parametric prior knowledge},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f9a40a4780f5e1306c46f1c8daecee3b-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_f9b902fc,
 abstract = {Recent work on mid-level representations aims to capture information at the level of complexity higher than typical visual words, but lower than full-blown semantic objects. Several approaches [5,6,12,23] have been proposed to discover mid-level elements, that are both 1) representative, i.e., frequently occurring within a dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difficult to analyze and evaluate. In this work, we pose element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm [2, 1, 4, 8]. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different discovery approaches, and compare our method against prior work on the Paris Street View dataset of [5]. We also evaluate our method on the task of scene classification, demonstrating state-of-the-art performance on the MIT Scene-67 dataset.},
 author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f9b902fc3289af4dd08de5d1de54f68f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f9b902fc3289af4dd08de5d1de54f68f-Metadata.json},
 openalex = {W2115628259},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f9b902fc3289af4dd08de5d1de54f68f-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f9b902fc3289af4dd08de5d1de54f68f-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/f9b902fc3289af4dd08de5d1de54f68f-Supplemental.zip},
 title = {Mid-level Visual Element Discovery as Discriminative Mode Seeking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/f9b902fc3289af4dd08de5d1de54f68f-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_fa14d4fe,
 abstract = {The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difficult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs (GPGP) consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer's output and the data, and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood. Representations and algorithms from computer graphics are used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on general-purpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and yields accurate, approximately Bayesian inferences about real-world images.},
 author = {Mansinghka, Vikash K and Kulkarni, Tejas D and Perov, Yura N and Tenenbaum, Josh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fa14d4fe2f19414de3ebd9f63d5c0169-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fa14d4fe2f19414de3ebd9f63d5c0169-Metadata.json},
 openalex = {W2133059703},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fa14d4fe2f19414de3ebd9f63d5c0169-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fa14d4fe2f19414de3ebd9f63d5c0169-Reviews.html},
 title = {Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/fa14d4fe2f19414de3ebd9f63d5c0169-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_fb60d411,
 abstract = {Many powerful Monte Carlo techniques for estimating partition functions, such as annealed importance sampling (AIS), are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and the intractable target distribution. The near-universal practice is to use geometric averages of the initial and target distributions, but alternative paths can perform substantially better. We present a novel sequence of intermediate distributions for exponential families defined by averaging the moments of the initial and target distributions. We analyze the asymptotic performance of both the geometric and moment averages paths and derive an asymptotically optimal piecewise linear schedule. AIS with moment averaging performs well empirically at estimating partition functions of restricted Boltzmann machines (RBMs), which form the building blocks of many deep learning models.},
 author = {Grosse, Roger B and Maddison, Chris J and Salakhutdinov, Russ R},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Metadata.json},
 openalex = {W2131939418},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Supplemental.zip},
 title = {Annealing between distributions by averaging moments},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_fb7b9ffa,
 abstract = {Compressed sensing (CS) is a concept that allows to acquire compressible signals with a small number of measurements. As such it is very attractive for hardware implementations. Therefore, correct calibration of the hardware is a central is- sue. In this paper we study the so-called blind calibration, i.e. when the training signals that are available to perform the calibration are sparse but unknown. We extend the approximate message passing (AMP) algorithm used in CS to the case of blind calibration. In the calibration-AMP, both the gains on the sensors and the elements of the signals are treated as unknowns. Our algorithm is also applica- ble to settings in which the sensors distort the measurements in other ways than multiplication by a gain, unlike previously suggested blind calibration algorithms based on convex relaxations. We study numerically the phase diagram of the blind calibration problem, and show that even in cases where convex relaxation is pos- sible, our algorithm requires a smaller number of measurements and/or signals in order to perform well.},
 author = {Schulke, Christophe and Caltagirone, Francesco and Krzakala, Florent and Zdeborov\'{a}, Lenka},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Metadata.json},
 openalex = {W4293774001},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb7b9ffa5462084c5f4e7e85a093e6d7-Supplemental.zip},
 title = {Blind Calibration in Compressed Sensing using Message Passing Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/fb7b9ffa5462084c5f4e7e85a093e6d7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_fb89705a,
 abstract = {We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classifier drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classifier selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a fixed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random fields and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classification task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models.},
 author = {Cuong, Nguyen Viet and Lee, Wee Sun and Ye, Nan and Chai, Kian Ming A and Chieu, Hai Leong},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb89705ae6d743bf1e848c206e16a1d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb89705ae6d743bf1e848c206e16a1d7-Metadata.json},
 openalex = {W2118996030},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb89705ae6d743bf1e848c206e16a1d7-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb89705ae6d743bf1e848c206e16a1d7-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fb89705ae6d743bf1e848c206e16a1d7-Supplemental.zip},
 title = {Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/fb89705ae6d743bf1e848c206e16a1d7-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_fc2c7c47,
 abstract = {We consider an infinite-armed bandit problem with Bernoulli rewards. The mean rewards are independent, uniformly distributed over [0,1]. Rewards 0 and 1 are referred to as a success and a failure, respectively. We propose a novel algorithm where the decision to exploit any arm is based on two successive targets, namely, the total number of successes until the first failure and until the first m failures, respectively, where m is a fixed parameter. This two-target algorithm achieves a long-term average regret in √2n for a large parameter m and a known time horizon n. This regret is optimal and strictly less than the regret achieved by the best known algorithms, which is in 2√n. The results are extended to any mean-reward distribution whose support contains 1 and to unknown time horizons. Numerical experiments show the performance of the algorithm for finite time horizons.},
 author = {Bonald, Thomas and Proutiere, Alexandre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc2c7c47b918d0c2d792a719dfb602ef-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc2c7c47b918d0c2d792a719dfb602ef-Metadata.json},
 openalex = {W2101398190},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc2c7c47b918d0c2d792a719dfb602ef-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc2c7c47b918d0c2d792a719dfb602ef-Supplemental.zip},
 title = {Two-Target Algorithms for Infinite-Armed Bandits with Bernoulli Rewards},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/fc2c7c47b918d0c2d792a719dfb602ef-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_fc8001f8,
 abstract = {Our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces. We employ a VP-tree and explore two simple yet effective learning-to-prune approaches: density estimation through sampling and stretching of the triangle inequality. Both methods are evaluated using data sets with metric (Euclidean) and non-metric (KL-divergence and Itakura-Saito) distance functions. Conditions on spaces where the VP-tree is applicable are discussed. The VP-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches: the bbtree, the multi-probe locality sensitive hashing (LSH), and permutation methods. Our method was competitive against state-of-the-art methods and, in most cases, was more efficient for the same rank approximation quality.},
 author = {Boytsov, Leonid and Naidan, Bilegsaikhan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc8001f834f6a5f0561080d134d53d29-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc8001f834f6a5f0561080d134d53d29-Metadata.json},
 openalex = {W2164353462},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc8001f834f6a5f0561080d134d53d29-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fc8001f834f6a5f0561080d134d53d29-Supplemental.zip},
 title = {Learning to Prune in Metric and Non-Metric Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/fc8001f834f6a5f0561080d134d53d29-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_fd5c905b,
 abstract = {We propose a Learning from Demonstration (LfD) algorithm which leverages expert data, even if they are very few or inaccurate. We achieve this by using both expert data, as well as reinforcement signals gathered through trial-and-error interactions with the environment. The key idea of our approach, Approximate Policy Iteration with Demonstration (APID), is that expert's suggestions are used to define linear constraints which guide the optimization performed by Approximate Policy Iteration. We prove an upper bound on the Bellman error of the estimate computed by APID at each iteration. Moreover, we show empirically that APID outperforms pure Approximate Policy Iteration, a state-of-the-art LfD algorithm, and supervised learning in a variety of scenarios, including when very few and/or suboptimal demonstrations are available. Our experiments include simulations as well as a real robot path-finding task.},
 author = {Kim, Beomjoon and Farahmand, Amir-massoud and Pineau, Joelle and Precup, Doina},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Metadata.json},
 openalex = {W2133552775},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Supplemental.zip},
 title = {Learning from Limited Demonstrations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/fd5c905bcd8c3348ad1b35d7231ee2b1-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_fe709c65,
 abstract = {This paper addresses the problem of online planning in Markov decision processes using a randomized simulator, under a budget constraint. We propose a new algorithm which is based on the construction of a forest of planning trees, where each tree corresponds to a random realization of the stochastic environment. The trees are constructed using a safe optimistic planning strategy combining the optimistic principle (in order to explore the most promising part of the search space first) with a safety principle (which guarantees a certain amount of uniform exploration). In the decision-making step of the algorithm, the individual trees are aggregated and an immediate action is recommended. We provide a finite-sample analysis and discuss the trade-off between the principles of optimism and safety. We also report numerical results on a benchmark problem. Our algorithm performs as well as state-of-the-art optimistic planning algorithms, and better than a related algorithm which additionally assumes the knowledge of all transition distributions.},
 author = {Kedenburg, Gunnar and Fonteneau, Raphael and Munos, Remi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fe709c654eac84d5239d1a12a4f71877-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fe709c654eac84d5239d1a12a4f71877-Metadata.json},
 openalex = {W2106225708},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fe709c654eac84d5239d1a12a4f71877-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fe709c654eac84d5239d1a12a4f71877-Reviews.html},
 title = {Aggregating optimistic planning trees for solving markov decision processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/fe709c654eac84d5239d1a12a4f71877-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_fec8d47d,
 abstract = {Human eye movements provide a rich source of information into the human visual information processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difficult to develop reliable eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement recordings collected under the primary task constraint of action recognition, as well as, separately, for context recognition, in order to analyze the impact of different tasks. Our dataset is unique among the eyetracking datasets of still images in terms of large scale (over 1 million fixations recorded in 9157 images) and different task controls. Second, we propose Markov models to automatically discover areas of interest (AOI) and introduce novel sequential consistency metrics based on them. Our methods can automatically determine the number, the spatial support and the transitions between AOIs, in addition to their locations. Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have significant influence on the human visual search patterns and are stable across subjects. Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. The methodology achieves state of the art scanpath modeling results.},
 author = {Mathe, Stefan and Sminchisescu, Cristian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fec8d47d412bcbeece3d9128ae855a7a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fec8d47d412bcbeece3d9128ae855a7a-Metadata.json},
 openalex = {W2120697543},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/fec8d47d412bcbeece3d9128ae855a7a-Reviews.html},
 title = {Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/fec8d47d412bcbeece3d9128ae855a7a-Abstract.html},
 volume = {26},
 year = {2013}
}

@inproceedings{NIPS2013_ffeed84c,
 abstract = {We consider a popular problem in finance, option pricing, through the lens of an online learning game between Nature and an Investor. In the Black-Scholes option pricing model from 1973, the Investor can continuously hedge the risk of an option by trading the underlying asset, assuming that the asset's price fluctuates according to Geometric Brownian Motion (GBM). We consider a worst-case model, in which Nature chooses a sequence of price fluctuations under a cumulative quadratic volatility constraint, and the Investor can make a sequence of hedging decisions. Our main result is to show that the value of our proposed game, which is the regret of hedging strategy, converges to the Black-Scholes option price. We use significantly weaker assumptions than previous work—for instance, we allow large jumps in the asset price—and show that the Black-Scholes hedging strategy is near-optimal for the Investor even in this non-stochastic framework.},
 author = {Abernethy, Jacob and Bartlett, Peter L and Frongillo, Rafael and Wibisono, Andre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Metadata.json},
 openalex = {W2096728797},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Paper.pdf},
 publisher = {Curran Associates, Inc.},
 review = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Reviews.html},
 supplemental = {https://proceedings.neurips.cc/paper_files/paper/2013/file/ffeed84c7cb1ae7bf4ec4bd78275bb98-Supplemental.zip},
 title = {How to Hedge an Option Against an Adversary: Black-Scholes Pricing is Minimax Optimal},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/ffeed84c7cb1ae7bf4ec4bd78275bb98-Abstract.html},
 volume = {26},
 year = {2013}
}
