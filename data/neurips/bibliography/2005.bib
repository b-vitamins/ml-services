@inproceedings{NIPS2005_0172d289,
 abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer's beliefs yield surprise, irrespectively of how rare or informative in Shannon's sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.},
 author = {Itti, Laurent and Baldi, Pierre},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0172d289da48c48de8c5ebf3de9f7ee1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0172d289da48c48de8c5ebf3de9f7ee1-Metadata.json},
 openalex = {W2039546655},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian surprise attracts human attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/0172d289da48c48de8c5ebf3de9f7ee1-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_02180771,
 abstract = {We consider the scaling of the number of examples necessary to achieve good performance in distributed, cooperative, multi-agent reinforcement learning, as a function of the the number of agents n. We prove a worst-case lower bound showing that algorithms that rely solely on a global reward signal to learn policies confront a fundamental limit: They require a number of real-world examples that scales roughly linearly in the number of agents. For settings of interest with a very large number of agents, this is impractical. We demonstrate, however, that there is a class of algorithms that, by taking advantage of local reward signals in large distributed Markov Decision Processes, are able to ensure good performance with a number of samples that scales as O(log n). This makes them applicable even in settings with a very large number of agents n.},
 author = {Bagnell, Drew and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/02180771a9b609a26dcea07f272e141f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/02180771a9b609a26dcea07f272e141f-Metadata.json},
 openalex = {W2131648741},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/02180771a9b609a26dcea07f272e141f-Paper.pdf},
 publisher = {MIT Press},
 title = {On Local Rewards and Scaling Distributed Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/02180771a9b609a26dcea07f272e141f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_030e65da,
 abstract = {We propose an algorithm that uses Gaussian process regression to learn common hidden structure shared between corresponding sets of heterogenous observations. The observation spaces are linked via a single, reduced-dimensionality latent variable space. We present results from two datasets demonstrating the algorithms's ability to synthesize novel data from learned correspondences. We first show that the method can learn the nonlinear mapping between corresponding views of objects, filling in missing data as needed to synthesize novel views. We then show that the method can learn a mapping between human degrees of freedom and robotic degrees of freedom for a humanoid robot, allowing robotic imitation of human poses from motion capture data.},
 author = {Shon, Aaron and Grochow, Keith and Hertzmann, Aaron and Rao, Rajesh PN},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/030e65da2b1c944090548d36b244b28d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/030e65da2b1c944090548d36b244b28d-Metadata.json},
 openalex = {W2103510282},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/030e65da2b1c944090548d36b244b28d-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Shared Latent Structure for Image Synthesis and Robotic Imitation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/030e65da2b1c944090548d36b244b28d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_044a23ca,
 abstract = {Compressed sensing is an emerging field based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruction. In this paper we introduce a new theory for distributed compressed sensing (DCS) that enables new distributed coding algorithms for multi-signal ensembles that exploit both intra- and inter-signal correlation structures. The DCS theory rests on a new concept that we term the joint sparsity of a signal ensemble. We study three simple models for jointly sparse signals, propose algorithms for joint recovery of multiple signals from incoherent projections, and characterize theoretically and empirically the number of measurements per sensor required for accurate reconstruction. In some sense DCS is a framework for distributed compression of sources with memory, which has remained a challenging problem in information theory for some time. DCS is immediately applicable to a range of problems in sensor networks and arrays.},
 author = {Wakin, Michael B. and Duarte, Marco and Sarvotham, Shriram and Baron, Dror and Baraniuk, Richard G.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/044a23cadb567653eb51d4eb40acaa88-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/044a23cadb567653eb51d4eb40acaa88-Metadata.json},
 openalex = {W2146195822},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/044a23cadb567653eb51d4eb40acaa88-Paper.pdf},
 publisher = {MIT Press},
 title = {Recovery of Jointly Sparse Signals from Few Random Projections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/044a23cadb567653eb51d4eb40acaa88-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_045cf83a,
 abstract = {We discuss two intrinsic weaknesses of the spectral graph partitioning method, both of which have practical consequences. The first is that spectral embeddings tend to hide the best cuts from the commonly used hyperplane rounding method. Rather than cleaning up the resulting sub-optimal cuts with local search, we recommend the adoption of flow-based rounding. The second weakness is that for many power law graphs, the spectral method produces cuts that are highly unbalanced, thus decreasing the usefulness of the method for visualization (see figure 4(b)) or as a basis for divide-and-conquer algorithms. These balance problems, which occur even though the spectral method's quotient-style objective function does encourage balance, can be fixed with a stricter balance constraint that turns the spectral mathematical program into an SDP that can be solved for million-node graphs by a method of Burer and Monteiro.},
 author = {Lang, Kevin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/045cf83ab0722e782cf72d14e44adf98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/045cf83ab0722e782cf72d14e44adf98-Metadata.json},
 openalex = {W2098651744},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/045cf83ab0722e782cf72d14e44adf98-Paper.pdf},
 publisher = {MIT Press},
 title = {Fixing two weaknesses of the Spectral Method},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/045cf83ab0722e782cf72d14e44adf98-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_0738069b,
 abstract = {A model of bottom-up overt attention is proposed based on the principle of maximizing information sampled from a scene. The proposed operation is based on Shannon's self-information measure and is achieved in a neural circuit, which is demonstrated as having close ties with the circuitry existent in die primate visual cortex. It is further shown that the proposed salicney measure may be extended to address issues that currently elude explanation in the domain of saliency based models. Results on natural images are compared with experimental eye tracking data revealing the efficacy of the model in predicting the deployment of overt attention as compared with existing efforts.},
 author = {Bruce, Neil and Tsotsos, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0738069b244a1c43c83112b735140a16-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0738069b244a1c43c83112b735140a16-Metadata.json},
 openalex = {W2139047169},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0738069b244a1c43c83112b735140a16-Paper.pdf},
 publisher = {MIT Press},
 title = {Saliency Based on Information Maximization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/0738069b244a1c43c83112b735140a16-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_09b69adc,
 abstract = {We consider the problem of modeling a helicopter's dynamics based on state-action trajectories collected from it. The contribution of this paper is two-fold. First, we consider the linear models such as learned by CIFER (the industry standard in helicopter identification), and show that the linear parameterization makes certain properties of dynamical systems, such as inertia, fundamentally difficult to capture. We propose an alternative, acceleration based, parameterization that does not suffer from this deficiency, and that can be learned as efficiently from data. Second, a Markov decision process model of a helicopter's dynamics would explicitly model only the one-step transitions, but we are often interested in a model's predictive performance over longer timescales. In this paper, we present an efficient algorithm for (approximately) minimizing the prediction error over long time scales. We present empirical results on two different helicopters. Although this work was motivated by the problem of modeling helicopters, the ideas presented here are general, and can be applied to modeling large classes of vehicular dynamics.},
 author = {Abbeel, Pieter and Ganapathi, Varun and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/09b69adcd7cbae914c6204984097d2da-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/09b69adcd7cbae914c6204984097d2da-Metadata.json},
 openalex = {W2169080882},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/09b69adcd7cbae914c6204984097d2da-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning vehicular dynamics, with application to modeling helicopters},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/09b69adcd7cbae914c6204984097d2da-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_0c1c995b,
 abstract = {Active learning is the problem in supervised learning to design the locations of training input points so that the generalization error is minimized. Existing active learning methods often assume that the model used for learning is correctly specified, i.e., the learning target function can be expressed by the model at hand. In many practical situations, however, this assumption may not be fulfilled. In this paper, we first show that the existing active learning method can be theoretically justified under slightly weaker condition: the model does not have to be correctly specified, but slightly misspecified models are also allowed. However, it turns out that the weakened condition is still restrictive in practice. To cope with this problem, we propose an alternative active learning method which can be theoretically justified for a wider class of misspecified models. Thus, the proposed method has a broader range of applications than the existing method. Numerical studies show that the proposed active learning method is robust against the misspecification of models and is thus reliable.},
 author = {Sugiyama, Masashi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0c1c995b77ea7312f887ddd9f9d35de5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0c1c995b77ea7312f887ddd9f9d35de5-Metadata.json},
 openalex = {W2139174822},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0c1c995b77ea7312f887ddd9f9d35de5-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Learning for Misspecified Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/0c1c995b77ea7312f887ddd9f9d35de5-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_0d9095b0,
 abstract = {The category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [1]. It has yet to be seen whether object identity can be inferred from this activity. We present fMRI data measuring responses in human extrastriate cortex to a set of 12 distinct object images. We use a simple winner-take-all classifier, using half the data from each recording session as a training set, to evaluate encoding of object identity across fMRI voxels. Since this approach is sensitive to the inclusion of noisy voxels, we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity. One method characterizes the reliability of each voxel within subsets of the data, while another estimates the mutual information of each voxel with the stimulus set. We find that both metrics can identify subsets of the data which reliably encode object identity, even when noisy measurements are artificially added to the data. The mutual information metric is less efficient at this task, likely due to constraints in fMRI data.},
 author = {Sayres, Rory and Ress, David and Grill-spector, Kalanit},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Metadata.json},
 openalex = {W2126289167},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Paper.pdf},
 publisher = {MIT Press},
 title = {Identifying Distributed Object Representations in Human Extrastriate Visual Cortex},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/0d9095b0d6bbe98ea0c9c02b11b59ee3-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_0fc170ec,
 abstract = {This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.},
 author = {Bengio, Yoshua and Roux, Nicolas and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0fc170ecbb8ff1afb2c6de48ea5343e7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0fc170ecbb8ff1afb2c6de48ea5343e7-Metadata.json},
 openalex = {W2963266548},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0fc170ecbb8ff1afb2c6de48ea5343e7-Paper.pdf},
 publisher = {MIT Press},
 title = {Input Convex Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/0fc170ecbb8ff1afb2c6de48ea5343e7-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_12311d05,
 abstract = {We present a generalization of temporal-difference networks to include temporally abstract options on the links of the question network. Temporal-difference (TD) networks have been proposed as a way of representing and learning a wide variety of predictions about the interaction between an agent and its environment. These predictions are compositional in that their targets are defined in terms of other predictions, and subjunctive in that that they are about what would happen if an action or sequence of actions were taken. In conventional TD networks, the inter-related predictions are at successive time steps and contingent on a single action; here we generalize them to accommodate extended time intervals and contingency on whole ways of behaving. Our generalization is based on the options framework for temporal abstraction. The primary contribution of this paper is to introduce a new algorithm for intra-option learning in TD networks with function approximation and eligibility traces. We present empirical examples of our algorithm's effectiveness and of the greater representational expressiveness of temporally-abstract TD networks.},
 author = {Rafols, Eddie and Koop, Anna and Sutton, Richard S},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/12311d05c9aa67765703984239511212-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/12311d05c9aa67765703984239511212-Metadata.json},
 openalex = {W2146823374},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/12311d05c9aa67765703984239511212-Paper.pdf},
 publisher = {MIT Press},
 title = {Temporal Abstraction in Temporal-difference Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/12311d05c9aa67765703984239511212-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_1264a061,
 abstract = {Fisher linear discriminant analysis (LDA) can be sensitive to the problem data. Robust Fisher LDA can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classification problem and optimizing for the worst-case scenario under this model. The main contribution of this paper is show that with general convex uncertainty models on the problem data, robust Fisher LDA can be carried out using convex optimization. For a certain type of product form uncertainty model, robust Fisher LDA can be carried out at a cost comparable to standard Fisher LDA. The method is demonstrated with some numerical examples. Finally, we show how to extend these results to robust kernel Fisher discriminant analysis, i.e., robust Fisher LDA in a high dimensional feature space.},
 author = {Kim, Seung-jean and Magnani, Alessandro and Boyd, Stephen},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1264a061d82a2edae1574b07249800d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1264a061d82a2edae1574b07249800d6-Metadata.json},
 openalex = {W2123157895},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1264a061d82a2edae1574b07249800d6-Paper.pdf},
 publisher = {MIT Press},
 title = {Robust Fisher Discriminant Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/1264a061d82a2edae1574b07249800d6-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_12a1d073,
 abstract = {Most nervous systems encode information about stimuli in the responding activity of large neuronal networks. This activity often manifests itself as dynamically coordinated sequences of action potentials. Since multiple electrode recordings are now a standard tool in neuroscience research, it is important to have a measure of such network-wide behavioral coordination and information sharing, applicable to multiple neural spike train data. We propose a new statistic, informational coherence, which measures how much better one unit can be predicted by knowing the dynamical state of another. We argue informational coherence is a measure of association and shared information which is superior to traditional pairwise measures of synchronization and correlation. To find the dynamical states, we use a recently-introduced algorithm which reconstructs effective state spaces from stochastic time series. We then extend the pairwise measure to a multivariate analysis of the network by estimating the network multi-information. We illustrate our method by testing it on a detailed model of the transition from gamma to beta rhythms.},
 author = {Klinkner, Kristina and Shalizi, Cosma and Camperi, Marcelo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/12a1d073d5ed3fa12169c67c4e2ce415-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/12a1d073d5ed3fa12169c67c4e2ce415-Metadata.json},
 openalex = {W4293863082},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/12a1d073d5ed3fa12169c67c4e2ce415-Paper.pdf},
 publisher = {MIT Press},
 title = {Measuring Shared Information and Coordinated Activity in Neuronal Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/12a1d073d5ed3fa12169c67c4e2ce415-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_1397386b,
 abstract = {In this paper we derive an algorithm that computes the entire solution path of the support vector regression, with essentially the same computational cost as fitting one SVR model. We also propose an unbiased estimate for the degrees of freedom of the SVR model, which allows convenient selection of the regularization parameter.},
 author = {Gunter, Lacey and Zhu, Ji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1397386b7a1507535c59764a15ee0c98-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1397386b7a1507535c59764a15ee0c98-Metadata.json},
 openalex = {W2159067232},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1397386b7a1507535c59764a15ee0c98-Paper.pdf},
 publisher = {MIT Press},
 title = {Computing the Solution Path for the Regularized Support Vector Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/1397386b7a1507535c59764a15ee0c98-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_15c00b52,
 abstract = {We propose a simple clustering framework on graphs encoding pairwise data similarities. Unlike usual similarity-based methods, the approach softly assigns data to clusters in a probabilistic way. More importantly, a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones. A random walk analysis indicates that the algorithm exposes clustering structures in various resolutions, i.e., a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure. Finally we provide very encouraging experimental results.},
 author = {Yu, Kai and Yu, Shipeng and Tresp, Volker},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/15c00b5250ddedaabc203b67f8b034fd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/15c00b5250ddedaabc203b67f8b034fd-Metadata.json},
 openalex = {W2146058975},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/15c00b5250ddedaabc203b67f8b034fd-Paper.pdf},
 publisher = {MIT Press},
 title = {Soft Clustering on Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/15c00b5250ddedaabc203b67f8b034fd-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_15cf7646,
 abstract = {Neurons can have rapidly changing spike train statistics dictated by the underlying network excitability or behavioural state of an animal. To estimate the time course of such state dynamics from single- or multiple neuron recordings, we have developed an algorithm that maximizes the likelihood of observed spike trains by optimizing the state lifetimes and the state-conditional interspike-interval (ISI) distributions. Our non-parametric algorithm is free of time-binning and spike-counting problems and has the computational complexity of a Mixed-state Markov Model operating on a state sequence of length equal to the total number of recorded spikes. As an example, we fit a two-state model to paired recordings of premotor neurons in the sleeping songbird. We find that the two state-conditional ISI functions are highly similar to the ones measured during waking and singing, respectively.},
 author = {Danoczy, Marton G. and Hahnloser, Richard},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/15cf76466b97264765356fcc56d801d1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/15cf76466b97264765356fcc56d801d1-Metadata.json},
 openalex = {W2145977470},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/15cf76466b97264765356fcc56d801d1-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient estimation of hidden state dynamics from spike trains},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/15cf76466b97264765356fcc56d801d1-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_17693c91,
 abstract = {It is well-known that everything that is learnable in the difficult online setting, where an arbitrary sequences of examples must be labeled one at a time, is also learnable in the batch setting, where examples are drawn independently from a distribution. We show a result in the opposite direction. We give an efficient conversion algorithm from batch to online that is transductive: it uses future unlabeled data. This demonstrates the equivalence between what is properly and efficiently learnable in a batch model and a transductive online model.},
 author = {Kakade, Sham and Kalai, Adam Tauman},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17693c91d9204b7a7646284bb3adb603-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17693c91d9204b7a7646284bb3adb603-Metadata.json},
 openalex = {W2100714909},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17693c91d9204b7a7646284bb3adb603-Paper.pdf},
 publisher = {MIT Press},
 title = {From Batch to Transductive Online Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/17693c91d9204b7a7646284bb3adb603-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_17d8da81,
 abstract = {We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.},
 author = {Saxena, Ashutosh and Chung, Sung and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17d8da815fa21c57af9829fb0a869602-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17d8da815fa21c57af9829fb0a869602-Metadata.json},
 openalex = {W2158211626},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Depth from Single Monocular Images},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/17d8da815fa21c57af9829fb0a869602-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_17eb7ecc,
 abstract = {To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density estimator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models.},
 author = {Bengio, Yoshua and Larochelle, Hugo and Vincent, Pascal},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17eb7ecc4c38e4705361cccd903ad8c6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17eb7ecc4c38e4705361cccd903ad8c6-Metadata.json},
 openalex = {W2131672785},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17eb7ecc4c38e4705361cccd903ad8c6-Paper.pdf},
 publisher = {MIT Press},
 title = {Non-Local Manifold Parzen Windows},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/17eb7ecc4c38e4705361cccd903ad8c6-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_182e6c2d,
 abstract = {Inspired by Sets, we consider the problem of retrieving items from a concept or cluster, given a query consisting of a few items from that cluster. We formulate this as a Bayesian inference problem and describe a very simple algorithm for solving it. Our algorithm uses a model-based concept of a cluster and ranks items using a score which evaluates the marginal probability that each item belongs to a cluster containing the query items. For exponential family models with conjugate priors this marginal probability is a simple function of sufficient statistics. We focus on sparse binary data and show that our score can be evaluated exactly using a single sparse matrix multiplication, making it possible to apply our algorithm to very large datasets. We evaluate our algorithm on three datasets: retrieving movies from EachMovie, finding completions of author sets from the NIPS dataset, and finding completions of sets of words appearing in the Grolier encyclopedia. We compare to Google™ Sets and show that Bayesian Sets gives very reasonable set completions.},
 author = {Ghahramani, Zoubin and Heller, Katherine A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/182e6c2d3d78eef40e5dac7da77a748f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/182e6c2d3d78eef40e5dac7da77a748f-Metadata.json},
 openalex = {W2293997450},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/182e6c2d3d78eef40e5dac7da77a748f-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian Sets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/182e6c2d3d78eef40e5dac7da77a748f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_1a672771,
 abstract = {Functional Magnetic Resonance Imaging (fMRI) has enabled scientists to look into the active brain. However, interactivity between functional brain regions, is still little studied. In this paper, we contribute a novel framework for modeling the interactions between multiple active brain regions, using Dynamic Bayesian Networks (DBNs) as generative models for brain activation patterns. This framework is applied to modeling of neuronal circuits associated with reward. The novelty of our framework from a Machine Learning perspective lies in the use of DBNs to reveal the brain connectivity and interactivity. Such interactivity models which are derived from fMRI data are then validated through a group classification task. We employ and compare four different types of DBNs: Parallel Hidden Markov Models, Coupled Hidden Markov Models, Fully-linked Hidden Markov Models and Dynamically Multi-Linked HMMs (DML-HMM). Moreover, we propose and compare two schemes of learning DML-HMMs. Experimental results show that by using DBNs, group classification can be performed even if the DBNs are constructed from as few as 5 brain regions. We also demonstrate that, by using the proposed learning algorithms, different DBN structures characterize drug addicted subjects vs. control subjects. This finding provides an independent test for the effect of psychopathology on brain function. In general, we demonstrate that incorporation of computer science principles into functional neuroimaging clinical studies provides a novel approach for probing human brain function.},
 author = {Zhang, Lei and Samaras, Dimitris and Alia-klein, Nelly and Volkow, Nora and Goldstein, Rita},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1a6727711b84fd1efbb87fc565199d13-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1a6727711b84fd1efbb87fc565199d13-Metadata.json},
 openalex = {W2098899472},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1a6727711b84fd1efbb87fc565199d13-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling Neuronal Interactivity using Dynamic Bayesian Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/1a6727711b84fd1efbb87fc565199d13-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_1a99f682,
 abstract = {Biological sensory systems are faced with the problem of encoding a high-fidelity sensory signal with a population of noisy, low-fidelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and two-dimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of high-dimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets.},
 author = {Doi, Eizaburo and Balcan, Doru and Lewicki, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1a99f6821980ac99136dcd2f1e9c8740-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1a99f6821980ac99136dcd2f1e9c8740-Metadata.json},
 openalex = {W2112812053},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1a99f6821980ac99136dcd2f1e9c8740-Paper.pdf},
 publisher = {MIT Press},
 title = {A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/1a99f6821980ac99136dcd2f1e9c8740-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_1b79b52d,
 abstract = {Spectral clustering enjoys its success in both data clustering and semi-supervised learning. But, most spectral clustering algorithms cannot handle multi-class clustering problems directly. Additional strategies are needed to extend spectral clustering algorithms to multi-class clustering problems. Furthermore, most spectral clustering algorithms employ hard cluster membership, which is likely to be trapped by the local optimum. In this paper, we present a new spectral clustering algorithm, named Soft Cut. It improves the normalized cut algorithm by introducing soft membership, and can be efficiently computed using a bound optimization algorithm. Our experiments with a variety of datasets have shown the promising performance of the proposed clustering algorithm.},
 author = {Jin, Rong and Kang, Feng and Ding, Chris},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1b79b52d1bf6f71b2b1eb7ca08ed0776-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1b79b52d1bf6f71b2b1eb7ca08ed0776-Metadata.json},
 openalex = {W2171583777},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1b79b52d1bf6f71b2b1eb7ca08ed0776-Paper.pdf},
 publisher = {MIT Press},
 title = {A Probabilistic Approach for Optimizing Spectral Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/1b79b52d1bf6f71b2b1eb7ca08ed0776-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_1cd138d0,
 abstract = {We propose a probabilistic model based on Independent Component Analysis for learning multiple related tasks. In our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks. We use Laplace distributions to model hidden sources which makes it possible to identify the hidden, independent components instead of just modeling correlations. Furthermore, our model enjoys a sparsity property which makes it both parsimonious and robust. We also propose efficient algorithms for both empirical Bayes method and point estimation. Our experimental results on two multi-label text classification data sets show that the proposed approach is promising.},
 author = {Zhang, Jian and Ghahramani, Zoubin and Yang, Yiming},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1cd138d0499a68f4bb72bee04bbec2d7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1cd138d0499a68f4bb72bee04bbec2d7-Metadata.json},
 openalex = {W2136979193},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Multiple Related Tasks using Latent Independent Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_1dba5eed,
 author = {Mochihashi, Daichi and Matsumoto, Yuji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1dba5eed8838571e1c80af145184e515-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1dba5eed8838571e1c80af145184e515-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1dba5eed8838571e1c80af145184e515-Paper.pdf},
 publisher = {MIT Press},
 title = {Context as Filtering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/1dba5eed8838571e1c80af145184e515-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_1ec3e7af,
 abstract = {In this paper we propose a new basis selection criterion for building sparse GP regression models that provides promising gains in accuracy as well as efficiency over previous methods. Our algorithm is much faster than that of Smola and Bartlett, while, in generalization it greatly outperforms the information gain approach proposed by Seeger et al, especially on the quality of predictive distributions.},
 author = {Keerthi, Sathiya and Chu, Wei},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1ec3e7af38e33222bde173fecaef6bfa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1ec3e7af38e33222bde173fecaef6bfa-Metadata.json},
 openalex = {W2169749653},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1ec3e7af38e33222bde173fecaef6bfa-Paper.pdf},
 publisher = {MIT Press},
 title = {A matching pursuit approach to sparse Gaussian process regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/1ec3e7af38e33222bde173fecaef6bfa-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_2217ad1d,
 abstract = {Motor imagery attenuates EEG µ and β rhythms over sensorimotor cortices. These amplitude changes are most successfully captured by the method of Common Spatial Patterns (CSP) and widely used in brain-computer interfaces (BCI). BCI methods based on amplitude information, however, have not incoporated the rich phase dynamics in the EEG rhythm. This study reports on a BCI method based on phase synchrony rate (SR). SR, computed from binarized phase locking value, describes the number of discrete synchronization events within a window. Statistical nonparametric tests show that SRs contain significant differences between 2 types of motor imageries. Classifiers trained on SRs consistently demonstrate satisfactory results for all 5 subjects. It is further observed that, for 3 subjects, phase is more discriminative than amplitude in the first 1.5-2.0 s, which suggests that phase has the potential to boost the information transfer rate in BCIs.},
 author = {Song, Le and Gordon, Evian and Gysels, Elly},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2217ad1dd50c1017d3df6b44b7c45508-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2217ad1dd50c1017d3df6b44b7c45508-Metadata.json},
 openalex = {W2124154068},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2217ad1dd50c1017d3df6b44b7c45508-Paper.pdf},
 publisher = {MIT Press},
 title = {Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/2217ad1dd50c1017d3df6b44b7c45508-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_227f6afd,
 abstract = {Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed offline, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efficiency and (iii) connections to active research areas.},
 author = {Kreidl, O. and Willsky, Alan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/227f6afd3b7f89b96c4bb91f95d50f6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/227f6afd3b7f89b96c4bb91f95d50f6d-Metadata.json},
 openalex = {W2108030590},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/227f6afd3b7f89b96c4bb91f95d50f6d-Paper.pdf},
 publisher = {MIT Press},
 title = {Inference with Minimal Communication: a Decision-Theoretic Variational Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_25766f01,
 abstract = {We consider criteria for variational representations of non-Gaussian latent variables, and derive variational EM algorithms in general form. We establish a general equivalence among convex bounding methods, evidence based methods, and ensemble learning/Variational Bayes methods, which has previously been demonstrated only for particular cases.},
 author = {Palmer, Jason and Kreutz-Delgado, Kenneth and Rao, Bhaskar and Wipf, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/25766f01628f3d34b93a36a2301dffc9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/25766f01628f3d34b93a36a2301dffc9-Metadata.json},
 openalex = {W2136870201},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/25766f01628f3d34b93a36a2301dffc9-Paper.pdf},
 publisher = {MIT Press},
 title = {Variational EM Algorithms for Non-Gaussian Latent Variable Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/25766f01628f3d34b93a36a2301dffc9-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_25ef0d88,
 abstract = {Calculations that quantify the dependencies between variables are vital to many operations with graphical models, e.g., active learning and sensitivity analysis. Previously, pairwise information gain calculation has involved a cost quadratic in network size. In this work, we show how to perform a similar computation with cost linear in network size. The loss function that allows this is of a form amenable to computation by dynamic programming. The message-passing algorithm that results is described and empirical results demonstrate large speedups without decrease in accuracy. In the cost-sensitive domains examined, superior accuracy is achieved.},
 author = {Anderson, Brigham S. and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/25ef0d887bc7a2b30089a025618e1c62-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/25ef0d887bc7a2b30089a025618e1c62-Metadata.json},
 openalex = {W2118582230},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/25ef0d887bc7a2b30089a025618e1c62-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Information Value for Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/25ef0d887bc7a2b30089a025618e1c62-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_2650d608,
 abstract = {We investigate the problem of automatically constructing efficient representations or basis functions for approximating value functions based on analyzing the structure and topology of the state space. In particular, two novel approaches to value function approximation are explored based on automatically constructing basis functions on state spaces that can be represented as graphs or manifolds: one approach uses the eigenfunctions of the Laplacian, in effect performing a global Fourier analysis on the graph; the second approach is based on diffusion wavelets, which generalize classical wavelets to graphs using multiscale dilations induced by powers of a diffusion operator or random walk on the graph. Together, these approaches form the foundation of a new generation of methods for solving large Markov decision processes, in which the underlying representation and policies are simultaneously learned.},
 author = {Mahadevan, Sridhar and Maggioni, Mauro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2650d6089a6d640c5e85b2b88265dc2b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2650d6089a6d640c5e85b2b88265dc2b-Metadata.json},
 openalex = {W2129570755},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf},
 publisher = {MIT Press},
 title = {Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_28acfe2d,
 abstract = {We propose a new method for spatial cluster detection, the Bayesian spatial scan statistic, and compare this method to the standard (frequentist) scan statistic approach. We demonstrate that the statistic has several advantages over the frequentist approach, including increased power to detect clusters and (since randomization testing is unnecessary) much faster runtime. We evaluate the and frequentist methods on the task of prospective disease surveillance: detecting spatial clusters of disease cases resulting from emerging disease outbreaks. We demonstrate that our methods are successful in rapidly detecting outbreaks while keeping number of false positives low.},
 author = {Neill, Daniel and Moore, Andrew and Cooper, Gregory},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/28acfe2da49d2b9a7f177458256f2540-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/28acfe2da49d2b9a7f177458256f2540-Metadata.json},
 openalex = {W2107579427},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/28acfe2da49d2b9a7f177458256f2540-Paper.pdf},
 publisher = {MIT Press},
 title = {A Bayesian Spatial Scan Statistic},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/28acfe2da49d2b9a7f177458256f2540-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_2a0f97f8,
 abstract = {This paper presents a diffusion based probabilistic interpretation of spectral clustering and dimensionality reduction algorithms that use the eigenvectors of the normalized graph Laplacian. Given the pairwise adjacency matrix of all points, we define a diffusion distance between any two data points and show that the low dimensional representation of the data by the first few eigenvectors of the corresponding Markov matrix is optimal under a certain mean squared error criterion. Furthermore, assuming that data points are random samples from a density $p(\x) = e^{-U(\x)}$ we identify these eigenvectors as discrete approximations of eigenfunctions of a Fokker-Planck operator in a potential $2U(\x)$ with reflecting boundary conditions. Finally, applying known results regarding the eigenvalues and eigenfunctions of the continuous Fokker-Planck operator, we provide a mathematical justification for the success of spectral clustering and dimensional reduction algorithms based on these first few eigenvectors. This analysis elucidates, in terms of the characteristics of diffusion processes, many empirical findings regarding spectral clustering algorithms.},
 author = {Nadler, Boaz and Lafon, Stephane and Kevrekidis, Ioannis and Coifman, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2a0f97f81755e2878b264adf39cba68e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2a0f97f81755e2878b264adf39cba68e-Metadata.json},
 openalex = {W2951394901},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2a0f97f81755e2878b264adf39cba68e-Paper.pdf},
 publisher = {MIT Press},
 title = {Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck operators},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/2a0f97f81755e2878b264adf39cba68e-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_2b64c2f1,
 abstract = {This paper explores the statistical relationship between natural images and their underlying range (depth) images. We look at how this relationship changes over scale, and how this information can be used to enhance low resolution range data using a full resolution intensity image. Based on our findings, we propose an extension to an existing technique known as shape recipes [3], and the success of the two methods are compared using images and laser scans of real scenes. Our extension is shown to provide a two-fold improvement over the current method. Furthermore, we demonstrate that ideal linear shape-from-shading filters, when learned from natural scenes, may derive even more strength from shadow cues than from the traditional linear-Lambertian shading cues.},
 author = {Lee, Tai-sing and Potetz, Brian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2b64c2f19d868305aa8bbc2d72902cc5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2b64c2f19d868305aa8bbc2d72902cc5-Metadata.json},
 openalex = {W2101665174},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf},
 publisher = {MIT Press},
 title = {Scaling Laws in Natural Scenes and the Inference of 3D Shape},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/2b64c2f19d868305aa8bbc2d72902cc5-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_2b6921f2,
 abstract = {This paper presents a non-asymptotic statistical analysis of Kernel-PCA with a focus different from the one proposed in previous work on this topic. Here instead of considering the reconstruction error of KPCA we are interested in approximation error bounds for the eigenspaces themselves. We prove an upper bound depending on the spacing between eigenvalues but not on the dimensionality of the eigenspace. As a consequence this allows to infer stability results for these estimated spaces.},
 author = {Zwald, Laurent and Blanchard, Gilles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2b6921f2c64dee16ba21ebf17f3c2c92-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2b6921f2c64dee16ba21ebf17f3c2c92-Metadata.json},
 openalex = {W2155100959},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2b6921f2c64dee16ba21ebf17f3c2c92-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Convergence of Eigenspaces in Kernel Principal Component Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/2b6921f2c64dee16ba21ebf17f3c2c92-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_2bc8ae25,
 abstract = {A novel video representation, the layered dynamic texture (LDT), is proposed. The LDT is a generative model, which represents a video as a collection of stochastic layers of different appearance and dynamics. Each layer is modeled as a temporal texture sampled from a different linear dynamical system. The LDT model includes these systems, a collection of hidden layer assignment variables (which control the assignment of pixels to layers), and a Markov random field prior on these variables (which encourages smooth segmentations). An EM algorithm is derived for maximum-likelihood estimation of the model parameters from a training video. It is shown that exact inference is intractable, a problem which is addressed by the introduction of two approximate inference procedures: a Gibbs sampler and a computationally efficient variational approximation. The trade-off between the quality of the two approximations and their complexity is studied experimentally. The ability of the LDT to segment videos into layers of coherent appearance and dynamics is also evaluated, on both synthetic and natural videos. These experiments show that the model possesses an ability to group regions of globally homogeneous, but locally heterogeneous, stochastic dynamics currently unparalleled in the literature.},
 author = {Chan, Antoni and Vasconcelos, Nuno},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Metadata.json},
 openalex = {W2037769578},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf},
 publisher = {MIT Press},
 title = {Layered Dynamic Textures},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/2bc8ae25856bc2a6a1333d1331a3b7a6-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_2e0bff75,
 abstract = {We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach.},
 author = {Mooney, Raymond and Bunescu, Razvan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2e0bff759d057e28460eaa5b2cb118e5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2e0bff759d057e28460eaa5b2cb118e5-Metadata.json},
 openalex = {W2163362093},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2e0bff759d057e28460eaa5b2cb118e5-Paper.pdf},
 publisher = {MIT Press},
 title = {Subsequence Kernels for Relation Extraction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/2e0bff759d057e28460eaa5b2cb118e5-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_2eb5657d,
 abstract = {Survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey and predators in cluttered environments. We investigate whether our visual system selects cues that guide search in an optimal manner. We formally obtain the optimal cue selection strategy by maximizing the signal to noise ratio (SNR) between a search target and surrounding distractors. This optimal strategy successfully accounts for several phenomena in visual search behavior, including the effect of target-distractor discriminability, uncertainty in target's features, distractor heterogeneity, and linear separability. Furthermore, the theory generates a new prediction, which we verify through psychophysical experiments with human subjects. Our results provide direct experimental evidence that humans select visual cues so as to maximize S N R between the targets and surrounding clutter.},
 author = {Navalpakkam, Vidhya and Itti, Laurent},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2eb5657d37f474e4c4cf01e4882b8962-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2eb5657d37f474e4c4cf01e4882b8962-Metadata.json},
 openalex = {W2156951449},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2eb5657d37f474e4c4cf01e4882b8962-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimal cue selection strategy},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/2eb5657d37f474e4c4cf01e4882b8962-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_2ef35a8b,
 abstract = {We define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We identify a simple generative process that results in the same distribution over equivalence classes, which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algorithm for inference in this model and applying the algorithm to an image dataset.},
 author = {Ghahramani, Zoubin and Griffiths, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2ef35a8b78b572a47f56846acbeef5d3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2ef35a8b78b572a47f56846acbeef5d3-Metadata.json},
 openalex = {W2128002512},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2ef35a8b78b572a47f56846acbeef5d3-Paper.pdf},
 publisher = {MIT Press},
 title = {Infinite latent feature models and the Indian buffet process},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/2ef35a8b78b572a47f56846acbeef5d3-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_3070e6ad,
 abstract = {We propose a new linear method for dimension reduction to identify non-Gaussian components in high dimensional data. Our method, NGCA (non-Gaussian component analysis), uses a very general semi-parametric framework. In contrast to existing projection methods we define what is uninteresting (Gaussian): by projecting out uninterestingness, we can estimate the relevant non-Gaussian subspace. We show that the estimation error of finding the non-Gaussian components tends to zero at a parametric rate. Once NGCA components are identified and extracted, various tasks can be applied in the data analysis process, like data visualization, clustering, denoising or classification. A numerical study demonstrates the usefulness of our method.},
 author = {Blanchard, Gilles and Sugiyama, Masashi and Kawanabe, Motoaki and Spokoiny, Vladimir and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3070e6addcd702cb58de5d7897bfdae1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3070e6addcd702cb58de5d7897bfdae1-Metadata.json},
 openalex = {W2115008562},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3070e6addcd702cb58de5d7897bfdae1-Paper.pdf},
 publisher = {MIT Press},
 title = {Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/3070e6addcd702cb58de5d7897bfdae1-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_31445061,
 abstract = {In this paper, we provide a general theorem that establishes a correspondence between surrogate loss functions in classification and the family of f-divergences. Moreover, we provide constructive procedures for determining the f-divergence induced by a given surrogate loss, and conversely for finding all surrogate loss functions that realize a given f-divergence. Next we introduce the notion of universal equivalence among loss functions and corresponding f-divergences, and provide necessary and sufficient conditions for universal equivalence to hold. These ideas have applications to classification problems that also involve a component of experiment design; in particular, we leverage our results to prove consistency of a procedure for learning a classifier under decentralization requirements.},
 author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/314450613369e0ee72d0da7f6fee773c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/314450613369e0ee72d0da7f6fee773c-Metadata.json},
 openalex = {W2169397395},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/314450613369e0ee72d0da7f6fee773c-Paper.pdf},
 publisher = {MIT Press},
 title = {Divergences, surrogate loss functions and experimental design},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/314450613369e0ee72d0da7f6fee773c-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_327708dd,
 abstract = {Clustering is a fundamental problem in machine learning and has been approached in many ways. Two general and quite different approaches include iteratively fitting a mixture model (e.g., using EM) and linking together pairs of training cases that have high (e.g., using spectral methods). Pair-wise clustering algorithms need not compute sufficient statistics and avoid poor solutions by directly placing similar examples in the same cluster. However, many applications require that each cluster of data be accurately described by a prototype or model, so affinity-based clustering - and its benefits - cannot be directly realized. We describe a technique called affinity propagation, which combines the advantages of both approaches. The method learns a mixture model of the data by recursively propagating messages. We demonstrate propagation on the problems of clustering image patches for image segmentation and learning mixtures of gene expression models from microar-ray data. We find that propagation obtains better solutions than mixtures of Gaussians, the K-medoids algorithm, spectral clustering and hierarchical clustering, and is both able to find a pre-specified number of clusters and is able to automatically determine the number of clusters. Interestingly, propagation can be viewed as belief propagation in a graphical model that accounts for pairwise training case likelihood functions and the identification of cluster centers.},
 author = {Frey, Brendan J and Dueck, Delbert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/327708dd10d68b1361ad3addbaca01f2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/327708dd10d68b1361ad3addbaca01f2-Metadata.json},
 openalex = {W2151933617},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/327708dd10d68b1361ad3addbaca01f2-Paper.pdf},
 publisher = {MIT Press},
 title = {Mixture Modeling by Affinity Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/327708dd10d68b1361ad3addbaca01f2-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_332647f4,
 abstract = {Previous work has demonstrated that the image variations of many objects (human faces in particular) under variable lighting can be effectively modeled by low dimensional linear spaces. The typical linear sub-space learning algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Locality Preserving Projection (LPP). All of these methods consider an n1 x n2 image as a high dimensional vector in ℝn1 x n2, while an image represented in the plane is intrinsically a matrix. In this paper, we propose a new algorithm called Tensor Subspace Analysis (TSA). TSA considers an image as the second order tensor in Rn1 ⊗ Rn2, where Rn1 and Rn2 are two vector spaces. The relationship between the column vectors of the image matrix and that between the row vectors can be naturally characterized by TSA. TSA detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace. We compare our proposed approach with PCA, LDA and LPP methods on two standard databases. Experimental results demonstrate that TSA achieves better recognition rate, while being much more efficient.},
 author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/332647f433a1c10fa2e2ae04abfdf83e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/332647f433a1c10fa2e2ae04abfdf83e-Metadata.json},
 openalex = {W2131335073},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/332647f433a1c10fa2e2ae04abfdf83e-Paper.pdf},
 publisher = {MIT Press},
 title = {Tensor Subspace Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/332647f433a1c10fa2e2ae04abfdf83e-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_340a3904,
 abstract = {Training a learning algorithm is a costly task. A major goal of active learning is to reduce this cost. In this paper we introduce a new algorithm, KQBC, which is capable of actively learning large scale problems by using selective sampling. The algorithm overcomes the costly sampling step of the well known Query By Committee (QBC) algorithm by projecting onto a low dimensional space. KQBC also enables the use of kernels, providing a simple way of extending QBC to the non-linear scenario. Sampling the low dimension space is done using the hit and run random walk. We demonstrate the success of this novel algorithm by applying it to both artificial and a real world problems.},
 author = {Gilad-bachrach, Ran and Navot, Amir and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/340a39045c40d50dda207bcfdece883a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/340a39045c40d50dda207bcfdece883a-Metadata.json},
 openalex = {W2128076875},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/340a39045c40d50dda207bcfdece883a-Paper.pdf},
 publisher = {MIT Press},
 title = {Query by Committee Made Real},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/340a39045c40d50dda207bcfdece883a-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_353de269,
 abstract = {This paper describes a highly successful application of MRFs to the problem of generating high-resolution range images. A new generation of range sensors combines the capture of low-resolution range images with the acquisition of registered high-resolution camera images. The MRF in this paper exploits the fact that discontinuities in range and coloring tend to co-align. This enables it to generate high-resolution, low-noise range images by integrating regular camera images into the range data. We show that by using such an MRF, we can substantially improve over existing range imaging technology.},
 author = {Diebel, James and Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/353de26971b93af88da102641069b440-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/353de26971b93af88da102641069b440-Metadata.json},
 openalex = {W2109945199},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/353de26971b93af88da102641069b440-Paper.pdf},
 publisher = {MIT Press},
 title = {An Application of Markov Random Fields to Range Sensing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/353de26971b93af88da102641069b440-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_35c5a2cb,
 abstract = {We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis.},
 author = {Zhang, Tong and Ando, Rie Kubota},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/35c5a2cb362c4d214156f930e7d13252-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/35c5a2cb362c4d214156f930e7d13252-Metadata.json},
 openalex = {W2166151737},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/35c5a2cb362c4d214156f930e7d13252-Paper.pdf},
 publisher = {MIT Press},
 title = {Analysis of Spectral Kernel Design based Semi-supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/35c5a2cb362c4d214156f930e7d13252-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_370bfb31,
 abstract = {There is little consensus about the computational function of top-down synaptic connections in the visual system. Here we explore the hypothesis that top-down connections, like bottom-up connections, reflect part-whole relationships. We analyze a recurrent network with bidirectional synaptic interactions between a layer of neurons representing parts and a layer of neurons representing wholes. Within each layer, there is lateral inhibition. When the network detects a whole, it can rigorously enforce part-whole relationships by ignoring parts that do not belong. The network can complete the whole by filling in missing parts. The network can refuse to recognize a whole, if the activated parts do not conform to a stored part-whole relationship. Parameter regimes in which these behaviors happen are identified using the theory of permitted and forbidden sets [3, 4]. The network behaviors are illustrated by recreating Rumelhart and McClelland's interactive activation model [7].},
 author = {Jain, Viren and Zhigulin, Valentin and Seung, H.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/370bfb31abd222b582245b977ea5f25a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/370bfb31abd222b582245b977ea5f25a-Metadata.json},
 openalex = {W2108623295},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/370bfb31abd222b582245b977ea5f25a-Paper.pdf},
 publisher = {MIT Press},
 title = {Representing Part-Whole Relationships in Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/370bfb31abd222b582245b977ea5f25a-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_379a7ba0,
 abstract = {We present a novel spectral clustering method that enables users to incorporate prior knowledge of the size of clusters into the clustering process. The cost function, which is named size regularized cut (SRcut), is defined as the sum of the inter-cluster similarity and a regularization term measuring the relative size of two clusters. Finding a partition of the data set to minimize SRcut is proved to be NP-complete. An approximation algorithm is proposed to solve a relaxed version of the optimization problem as an eigenvalue problem. Evaluations over different data sets demonstrate that the method is not sensitive to outliers and performs better than normalized cut.},
 author = {Chen, Yixin and Zhang, Ya and Ji, Xiang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/379a7ba015d8bf1c70b8add2c287c6fa-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/379a7ba015d8bf1c70b8add2c287c6fa-Metadata.json},
 openalex = {W2097253747},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/379a7ba015d8bf1c70b8add2c287c6fa-Paper.pdf},
 publisher = {MIT Press},
 title = {Size Regularized Cut for Data Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/379a7ba015d8bf1c70b8add2c287c6fa-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_3812f9a5,
 abstract = {Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and benefits of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic findings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine.},
 author = {Niv, Yael and Daw, Nathaniel and Dayan, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3812f9a59b634c2a9c574610eaba5bed-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3812f9a59b634c2a9c574610eaba5bed-Metadata.json},
 openalex = {W2126805559},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3812f9a59b634c2a9c574610eaba5bed-Paper.pdf},
 publisher = {MIT Press},
 title = {How fast to work: Response vigor, motivation and tonic dopamine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/3812f9a59b634c2a9c574610eaba5bed-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_3953630d,
 abstract = {We address the problem of robust, computationally-efficient design of biological experiments. Classical experiment design methods have not been widely adopted in biological practice, in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor, and in part because of computational constraints. We present a method for robust experiment design based on a semidefinite programming relaxation. We present an application of this method to the design of experiments for a complex calcium signal transduction pathway, where we have found that the parameter estimates obtained from the robust design are better than those obtained from an optimal design.},
 author = {Flaherty, Patrick and Arkin, Adam and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3953630da28e5181cffca1278517e3cf-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3953630da28e5181cffca1278517e3cf-Metadata.json},
 openalex = {W2156602393},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3953630da28e5181cffca1278517e3cf-Paper.pdf},
 publisher = {MIT Press},
 title = {Robust design of biological experiments},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/3953630da28e5181cffca1278517e3cf-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_39d352b0,
 abstract = {Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-the-best searches for a sufficiently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efficient algorithm can approximate the optimum to within any constant factor, if P ≠ NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best.},
 author = {Schmitt, Michael and Martignon, Laura},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/39d352b0395ba768e18f042c6e2a8621-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/39d352b0395ba768e18f042c6e2a8621-Metadata.json},
 openalex = {W2126827998},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/39d352b0395ba768e18f042c6e2a8621-Paper.pdf},
 publisher = {MIT Press},
 title = {On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/39d352b0395ba768e18f042c6e2a8621-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_3c333aad,
 abstract = {Gaussian processes are attractive models for probabilistic classification but unfortunately exact inference is analytically intractable. We compare Laplace's method and Expectation Propagation (EP) focusing on marginal likelihood estimates and predictive performance. We explain theoretically and corroborate empirically that EP is superior to Laplace. We also compare to a sophisticated MCMC scheme and show that EP is surprisingly accurate.},
 author = {Kuss, Malte and Rasmussen, Carl},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3c333aadfc3ee8ecb8d77ee31197d96a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3c333aadfc3ee8ecb8d77ee31197d96a-Metadata.json},
 openalex = {W2162114812},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3c333aadfc3ee8ecb8d77ee31197d96a-Paper.pdf},
 publisher = {MIT Press},
 title = {Assessing Approximations for Gaussian Process Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/3c333aadfc3ee8ecb8d77ee31197d96a-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_3fc2c60b,
 abstract = {Reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill-behaved optimization problems. We improve its robustness and speed of convergence with stochastic meta-descent, a gain vector adaptation method that employs fast Hessian-vector products. In our experiments the resulting algorithms outperform previously employed online stochastic, offline conjugate, and natural policy gradient methods.},
 author = {Yu, Jin and Aberdeen, Douglas and Schraudolph, Nicol},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3fc2c60b5782f641f76bcefc39fb2392-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3fc2c60b5782f641f76bcefc39fb2392-Metadata.json},
 openalex = {W2097568041},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3fc2c60b5782f641f76bcefc39fb2392-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/3fc2c60b5782f641f76bcefc39fb2392-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_4191ef5f,
 abstract = {The classical Bayes rule computes the posterior model probability from the prior probability and the data likelihood. We generalize this rule to the case when the prior is a density matrix (symmetric positive definite and trace one) and the data likelihood a covariance matrix. The classical Bayes rule is retained as the special case when the matrices are diagonal.

In the classical setting, the calculation of the probability of the data is an expected likelihood, where the expectation is over the prior distribution. In the generalized setting, this is replaced by an expected variance calculation where the variance is computed along the eigenvectors of the prior density matrix and the expectation is over the eigenvalues of the density matrix (which form a probability vector). The variances along any direction is determined by the covariance matrix. Curiously enough this expected variance calculation is a quantum measurement where the co-variance matrix specifies the instrument and the prior density matrix the mixture state of the particle. We motivate both the classical and the generalized Bayes rule with a minimum relative entropy principle, where the Kullbach-Leibler version gives the classical Bayes rule and Umegaki's quantum relative entropy the new Bayes rule for density matrices.},
 author = {Warmuth, Manfred K. K},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4191ef5f6c1576762869ac49281130c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4191ef5f6c1576762869ac49281130c9-Metadata.json},
 openalex = {W2100687344},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4191ef5f6c1576762869ac49281130c9-Paper.pdf},
 publisher = {MIT Press},
 title = {A Bayes Rule for Density Matrices},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/4191ef5f6c1576762869ac49281130c9-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_42853a61,
 abstract = {A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the 'k' in nearest neighbors.},
 author = {Argyriou, Andreas and Herbster, Mark and Pontil, Massimiliano},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/42853a61b26fef79e2ae788d97356799-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/42853a61b26fef79e2ae788d97356799-Metadata.json},
 openalex = {W2133026717},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/42853a61b26fef79e2ae788d97356799-Paper.pdf},
 publisher = {MIT Press},
 title = {Combining Graph Laplacians for Semi--Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/42853a61b26fef79e2ae788d97356799-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_42a6845a,
 abstract = {Note: inproceedings Reference LCN-CONF-2006-001 Record created on 2006-12-12, modified on 2017-05-12},
 author = {Jolivet, Renaud and Rauch, Alexander and L\"{u}scher, Hans-rudolf and Gerstner, Wulfram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/42a6845a557bef704ad8ac9cb4461d43-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/42a6845a557bef704ad8ac9cb4461d43-Metadata.json},
 openalex = {W2106787520},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf},
 publisher = {MIT Press},
 title = {Integrate-and-Fire models with adaptation are good enough},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/42a6845a557bef704ad8ac9cb4461d43-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_445e1050,
 abstract = {We present a method for performing transductive inference on very large datasets. Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufficiently fast. This holds, for instance, for certain graph and string kernels. Transduction is achieved by varia-tional inference over the unlabeled data subject to a balancing constraint.},
 author = {G\"{a}rtner, Thomas and Le, Quoc and Burton, Simon and Smola, Alex J. and Vishwanathan, Vishy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/445e1050156c6ae8c082a8422bb7dfc0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/445e1050156c6ae8c082a8422bb7dfc0-Metadata.json},
 openalex = {W2130526125},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/445e1050156c6ae8c082a8422bb7dfc0-Paper.pdf},
 publisher = {MIT Press},
 title = {Large-Scale Multiclass Transduction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/445e1050156c6ae8c082a8422bb7dfc0-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_4491777b,
 abstract = {We present a new Gaussian process (GP) regression model whose co-variance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M ≪ N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M2N) training cost and O(M2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime.},
 author = {Snelson, Edward and Ghahramani, Zoubin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Metadata.json},
 openalex = {W2099768828},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf},
 publisher = {MIT Press},
 title = {Sparse Gaussian Processes using Pseudo-inputs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/4491777b1aa8b5b32c2e8666dbe1a495-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_44ac09ac,
 abstract = {In this paper, we present our design and experiments of a planar biped robot (RunBot) under pure reflexive neuronal control. The goal of this study is to combine neuronal mechanisms with biomechanics to obtain very fast speed and the on-line learning of circuit parameters. Our controller is built with biologically inspired sensor- and motor-neuron models, including local reflexes and not employing any kind of position or trajectory-tracking control algorithm. Instead, this reflexive controller allows RunBot to exploit its own natural dynamics during critical stages of its walking gait cycle. To our knowledge, this is the first time that dynamic biped walking is achieved using only a pure reflexive controller. In addition, this structure allows using a policy gradient reinforcement learning algorithm to tune the parameters of the reflexive controller in real-time during walking. This way RunBot can reach a relative speed of 3.5 leg-lengths per second after a few minutes of online learning, which is faster than that of any other biped robot, and is also comparable to the fastest relative speed of human walking. In addition, the stability domain of stable walking is quite large supporting this design strategy.},
 author = {Geng, Tao and Porr, Bernd and W\"{o}rg\"{o}tter, Florentin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/44ac09ac6a149136a4102ee4b4103ae6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/44ac09ac6a149136a4102ee4b4103ae6-Metadata.json},
 openalex = {W2156377376},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/44ac09ac6a149136a4102ee4b4103ae6-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast biped walking with a reflexive controller and real-time policy searching},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/44ac09ac6a149136a4102ee4b4103ae6-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_465636eb,
 abstract = {We initiate the study of learning from multiple sources of limited data, each of which may be corrupted at a different rate. We develop a complete theory of which data sources should be used for two fundamental problems: estimating the bias of a coin, and learning a classifier in the presence of label noise. In both cases, efficient algorithms are provided for computing the optimal subset of data.},
 author = {Crammer, Koby and Kearns, Michael and Wortman, Jennifer},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/465636eb4a7ff4b267f3b765d07a02da-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/465636eb4a7ff4b267f3b765d07a02da-Metadata.json},
 openalex = {W2165503745},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/465636eb4a7ff4b267f3b765d07a02da-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning from Data of Variable Quality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/465636eb4a7ff4b267f3b765d07a02da-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_46b2644c,
 abstract = {Kernel methods make it relatively easy to define complex high-dimensional feature spaces. This raises the question of how we can identify the relevant subspaces for a particular learning task. When two views of the same phenomenon are available kernel Canonical Correlation Analysis (KCCA) has been shown to be an effective preprocessing step that can improve the performance of classification algorithms such as the Support Vector Machine (SVM). This paper takes this observation to its logical conclusion and proposes a method that combines this two stage learning (KCCA followed by SVM) into a single optimisation termed SVM-2K. We present both experimental and theoretical analysis of the approach showing encouraging results and insights.},
 author = {Farquhar, Jason and Hardoon, David and Meng, Hongying and Shawe-taylor, John and Szedm\'{a}k, S\'{a}ndor},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/46b2644cbdf489fac0e2d192212d206d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/46b2644cbdf489fac0e2d192212d206d-Metadata.json},
 openalex = {W2145234365},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/46b2644cbdf489fac0e2d192212d206d-Paper.pdf},
 publisher = {MIT Press},
 title = {Two view learning: SVM-2K, Theory and Practice},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/46b2644cbdf489fac0e2d192212d206d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_46f76a4b,
 abstract = {Spiking activity from neurophysiological experiments often exhibits dynamics beyond that driven by external stimulation, presumably reflecting the extensive recurrence of neural circuitry. Characterizing these dynamics may reveal important features of neural computation, particularly during internally-driven cognitive operations. For example, the activity of premotor cortex (PMd) neurons during an instructed delay period separating movement-target specification and a movement-initiation cue is believed to be involved in motor planning. We show that the dynamics underlying this activity can be captured by a low-dimensional non-linear dynamical systems model, with underlying recurrent structure and stochastic point-process output. We present and validate latent variable methods that simultaneously estimate the system parameters and the trial-by-trial dynamical trajectories. These methods are applied to characterize the dynamics in PMd data recorded from a chronically-implanted 96-electrode array while monkeys perform delayed-reach tasks.},
 author = {Yu, Byron M and Afshar, Afsheen and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna V and Sahani, Maneesh},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/46f76a4bda9a9579eab38a8f6eabcda1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/46f76a4bda9a9579eab38a8f6eabcda1-Metadata.json},
 openalex = {W2156168178},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf},
 publisher = {MIT Press},
 title = {Extracting Dynamical Structure Embedded in Neural Activity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/46f76a4bda9a9579eab38a8f6eabcda1-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_488c1e03,
 abstract = {Our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models. The large number of parameters needing hand tuning in these models has, however, somewhat hampered their applicability and interpretability. Here we propose a simple and well-founded method for automatic estimation of many of these key parameters: 1) the spatial distribution of channel densities on the cell's membrane; 2) the spatiotemporal pattern of synaptic input; 3) the channels' reversal potentials; 4) the intercompartmental conductances; and 5) the noise level in each compartment. We assume experimental access to: a) the spatiotemporal voltage signal in the dendrite (or some contiguous subpart thereof, e.g. via voltage sensitive imaging techniques), b) an approximate kinetic description of the channels and synapses present in each compartment, and c) the morphology of the part of the neuron under investigation. The key observation is that, given data a)-c), all of the parameters 1)-4) may be simultaneously inferred by a version of constrained linear regression; this regression, in turn, is efficiently solved using standard algorithms, without any local minima problems despite the large number of parameters and complex dynamics. The noise level 5) may also be estimated by standard techniques. We demonstrate the method's accuracy on several model datasets, and describe techniques for quantifying the uncertainty in our estimates.},
 author = {Ahrens, Misha and Paninski, Liam and Huys, Quentin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/488c1e0332065eb80e1129139a67d6e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/488c1e0332065eb80e1129139a67d6e0-Metadata.json},
 openalex = {W2142045719},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/488c1e0332065eb80e1129139a67d6e0-Paper.pdf},
 publisher = {MIT Press},
 title = {Large-scale biophysical parameter estimation in single neurons via constrained linear regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/488c1e0332065eb80e1129139a67d6e0-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_4a5876b4,
 abstract = {Online learning algorithms are typically fast, memory efficient, and simple to implement. However, many common learning problems fit more naturally in the batch learning setting. The power of online learning algorithms can be exploited in batch settings by using online-to-batch conversions techniques which build a new batch algorithm from an existing online algorithm. We first give a unified overview of three existing online-to-batch conversion techniques which do not use training data in the conversion process. We then build upon these data-independent conversions to derive and analyze data-driven conversions. Our conversions find hypotheses with a small risk by explicitly minimizing data-dependent generalization bounds. We experimentally demonstrate the usefulness of our approach and in particular show that the data-driven conversions consistently outperform the data-independent conversions.},
 author = {Dekel, Ofer and Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4a5876b450b45371f6cfe5047ac8cd45-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4a5876b450b45371f6cfe5047ac8cd45-Metadata.json},
 openalex = {W2148742679},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4a5876b450b45371f6cfe5047ac8cd45-Paper.pdf},
 publisher = {MIT Press},
 title = {Data-Driven Online to Batch Conversions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/4a5876b450b45371f6cfe5047ac8cd45-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_4ab52371,
 abstract = {We propose efficient algorithms for learning ranking functions from order constraints between sets—i.e. classes—of training samples. Our algorithms may be used for maximizing the generalized Wilcoxon Mann Whitney statistic that accounts for the partial ordering of the classes: special cases include maximizing the area under the ROC curve for binary classification and its generalization for ordinal regression. Experiments on public benchmarks indicate that: (a) the proposed algorithm is at least as accurate as the current state-of-the-art; (b) computationally, it is several orders of magnitude faster and—unlike current methods—it is easily able to handle even large datasets with over 20,000 samples.},
 author = {Fung, Glenn and Rosales, R\'{o}mer and Krishnapuram, Balaji},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4ab52371762b735317125e6446a51e8f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4ab52371762b735317125e6446a51e8f-Metadata.json},
 openalex = {W2128583507},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4ab52371762b735317125e6446a51e8f-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Rankings via Convex Hull Separation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/4ab52371762b735317125e6446a51e8f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_4b21cf96,
 abstract = {Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that generically produce power-laws, augmenting standard generative models with an adaptor that produces the appropriate pattern of token frequencies. We show that taking a particular stochastic process - the Pitman-Yor process - as an adaptor justifies the appearance of type frequencies in formal analyses of natural language, and improves the performance of a model for unsupervised learning of morphology.},
 author = {Goldwater, Sharon and Johnson, Mark and Griffiths, Thomas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4b21cf96d4cf612f239a6c322b10c8fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4b21cf96d4cf612f239a6c322b10c8fe-Metadata.json},
 openalex = {W2159399018},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf},
 publisher = {MIT Press},
 title = {Interpolating between types and tokens by estimating power-law generators},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/4b21cf96d4cf612f239a6c322b10c8fe-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_4b29fa4e,
 abstract = {In this paper, we aim at analyzing the characteristic of neuronal population responses to instantaneous or time-dependent inputs and the role of synapses in neural information processing. We have derived an evolution equation of the membrane potential density function with synaptic depression, and obtain the formulas for analytic computing the response of instantaneous fire rate. Through a technical analysis, we arrive at several significant conclusions: The background inputs play an important role in information processing and act as a switch betwee temporal integration and coincidence detection. the role of synapses can be regarded as a spatio-temporal filter; it is important in neural information processing for the spatial distribution of synapses and the spatial and temporal relation of inputs. The instantaneous input frequency can affect the response amplitude and phase delay.},
 author = {Huang, Wentao and Jiao, Licheng and Tan, Shan and Gong, Maoguo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4b29fa4efe4fb7bc667c7b301b74d52d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4b29fa4efe4fb7bc667c7b301b74d52d-Metadata.json},
 openalex = {W2169464564},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4b29fa4efe4fb7bc667c7b301b74d52d-Paper.pdf},
 publisher = {MIT Press},
 title = {Response Analysis of Neuronal Population with Synaptic Depression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/4b29fa4efe4fb7bc667c7b301b74d52d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_4e2a6330,
 abstract = {This paper proposes a new approach to feature selection based on a statistical feature mining technique for sequence and tree kernels. Since natural language data take discrete structures, convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and accuracy of many natural language processing tasks. However, experiments have shown that the best results can only be achieved when limited small sub-structures are dealt with by these kernels. This paper discusses this issue of convolution kernels and then proposes a statistical feature selection that enable us to use larger sub-structures effectively. The proposed method, in order to execute efficiently, can be embedded into an original kernel calculation process by using sub-structure mining algorithms. Experiments on real NLP tasks confirm the problem in the conventional method and compare the performance of a conventional method to that of the proposed method.},
 author = {Suzuki, Jun and Isozaki, Hideki},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4e2a6330465c8ffcaa696a5a16639176-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4e2a6330465c8ffcaa696a5a16639176-Metadata.json},
 openalex = {W2151787203},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4e2a6330465c8ffcaa696a5a16639176-Paper.pdf},
 publisher = {MIT Press},
 title = {Sequence and Tree Kernels with Statistical Feature Mining},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/4e2a6330465c8ffcaa696a5a16639176-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_4ea6a546,
 abstract = {This paper presents a rigorous statistical analysis characterizing regimes in which active learning significantly outperforms classical passive learning. Active learning algorithms are able to make queries or select sample locations in an online fashion, depending on the results of the previous queries. In some regimes, this extra flexibility leads to significantly faster rates of error decay than those possible in classical passive learning settings. The nature of these regimes is explored by studying fundamental performance limits of active and passive learning in two illustrative nonparametric function classes. In addition to examining the theoretical potential of active learning, this paper describes a practical algorithm capable of exploiting the extra flexibility of the active setting and provably improving upon the classical passive techniques. Our active learning theory and methods show promise in a number of applications, including field estimation using wireless sensor networks and fault line detection.},
 author = {Willett, Rebecca and Nowak, Robert and Castro, Rui},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4ea6a546c19499318091a9df40a13181-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4ea6a546c19499318091a9df40a13181-Metadata.json},
 openalex = {W2160828669},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4ea6a546c19499318091a9df40a13181-Paper.pdf},
 publisher = {MIT Press},
 title = {Faster Rates in Regression via Active Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/4ea6a546c19499318091a9df40a13181-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_4f1f2988,
 abstract = {Theories of visual attention commonly posit that early parallel processes extract conspicuous features such as color contrast and motion from the visual field. These features are then combined into a saliency map, and attention is directed to the most salient regions first. Top-down attentional control is achieved by modulating the contribution of different feature types to the saliency map. A key source of data concerning attentional control comes from behavioral studies in which the effect of recent experience is examined as individuals repeatedly perform a perceptual discrimination task (e.g., what shape is the odd-colored object?). The robust finding is that repetition of features of recent trials (e.g., target color) facilitates performance. We view this facilitation as an adaptation to the statistical structure of the environment. We propose a probabilistic model of the environment that is updated after each trial. Under the assumption that attentional control operates so as to make performance more efficient for more likely environmental states, we obtain parsimonious explanations for data from four different experiments. Further, our model provides a rational explanation for why the influence of past experience on attentional control is short lived.},
 author = {Shettel, Michael and Vecera, Shaun and Mozer, Michael C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4f1f29888cabf5d45f866fe457737a23-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4f1f29888cabf5d45f866fe457737a23-Metadata.json},
 openalex = {W2165217178},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4f1f29888cabf5d45f866fe457737a23-Paper.pdf},
 publisher = {MIT Press},
 title = {Top-Down Control of Visual Attention: A Rational Account},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/4f1f29888cabf5d45f866fe457737a23-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_564645fb,
 abstract = {To investigate how top-down (TD) and bottom-up (BU) information is weighted in the guidance of human search behavior, we manipulated the proportions of BU and TD components in a saliency-based model. The model is biologically plausible and implements an artificial retina and a neuronal population code. The BU component is based on feature-contrast. The TD component is defined by a feature-template match to a stored target representation. We compared the model's behavior at different mixtures of TD and BU components to the eye movement behavior of human observers performing the identical search task. We found that a purely TD model provides a much closer match to human behavior than any mixture model using BU information. Only when biological constraints are removed (e.g., eliminating the retina) did a BU/TD mixture model begin to approximate human behavior.},
 author = {Zelinsky, Gregory and Zhang, Wei and Yu, Bing and Chen, Xin and Samaras, Dimitris},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/564645fbd0332f066cbd9d083ddd077c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/564645fbd0332f066cbd9d083ddd077c-Metadata.json},
 openalex = {W2101162742},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/564645fbd0332f066cbd9d083ddd077c-Paper.pdf},
 publisher = {MIT Press},
 title = {The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/564645fbd0332f066cbd9d083ddd077c-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_566a9968,
 abstract = {The misjudgement of tilt in images lies at the heart of entertaining visual illusions and rigorous perceptual psychophysics. A wealth of findings has attracted many mechanistic models, but few clear computational principles. We adopt a Bayesian approach to perceptual tilt estimation, showing how a smoothness prior offers a powerful way of addressing much confusing data. In particular, we faithfully model recent results showing that confidence in estimation can be systematically affected by the same aspects of images that affect bias. Confidence is central to Bayesian modeling approaches, and is applicable in many other perceptual domains.},
 author = {Schwartz, Odelia and Dayan, Peter and Sejnowski, Terrence J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/566a9968b43628588e76be5a85a0f9e8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/566a9968b43628588e76be5a85a0f9e8-Metadata.json},
 openalex = {W2126399818},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/566a9968b43628588e76be5a85a0f9e8-Paper.pdf},
 publisher = {MIT Press},
 title = {A Bayesian Framework for Tilt Perception and Confidence},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/566a9968b43628588e76be5a85a0f9e8-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_56c82ccd,
 abstract = {A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MIL-Boost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classifier.},
 author = {Zhang, Cha and Platt, John and Viola, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/56c82ccd658e09e829f16bb99457bcbc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/56c82ccd658e09e829f16bb99457bcbc-Metadata.json},
 openalex = {W2166010828},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/56c82ccd658e09e829f16bb99457bcbc-Paper.pdf},
 publisher = {MIT Press},
 title = {Multiple Instance Boosting for Object Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/56c82ccd658e09e829f16bb99457bcbc-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_56cb94cb,
 abstract = {We analyze classification error on unseen cases, i.e. cases that are different from those in the training set. Unlike standard generalization error, this off-training-set error may differ significantly from the empirical error with high probability even with large sample sizes. We derive a data-dependent bound on the difference between off-training-set and standard generalization error. Our result is based on a new bound on the missing mass, which for small samples is stronger than existing bounds based on Good-Turing estimators. As we demonstrate on UCI data-sets, our bound gives nontrivial generalization guarantees in many practical cases. In light of these results, we show that certain claims made in the No Free Lunch literature are overly pessimistic.},
 author = {Roos, Teemu and Gr\"{u}nwald, Peter and Myllym\"{a}ki, Petri and Tirri, Henry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/56cb94cb34617aeadff1e79b53f38354-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/56cb94cb34617aeadff1e79b53f38354-Metadata.json},
 openalex = {W2125792641},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/56cb94cb34617aeadff1e79b53f38354-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalization to Unseen Cases},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/56cb94cb34617aeadff1e79b53f38354-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_5a2756a3,
 abstract = {We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.},
 author = {Navot, Amir and Shpigelman, Lavi and Tishby, Naftali and Vaadia, Eilon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5a2756a3cb9cde852cad3c97e120b656-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5a2756a3cb9cde852cad3c97e120b656-Metadata.json},
 openalex = {W2114987560},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5a2756a3cb9cde852cad3c97e120b656-Paper.pdf},
 publisher = {MIT Press},
 title = {Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/5a2756a3cb9cde852cad3c97e120b656-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_5b4130c9,
 abstract = {Under natural viewing conditions, small movements of the eye and body prevent the maintenance of a steady direction of gaze. It is known that stimuli tend to fade when they are stabilized on the retina for several seconds. However, it is unclear whether the physiological self-motion of the retinal image serves a visual purpose during the brief periods of natural visual fixation. This study examines the impact of fixational instability on the statistics of visual input to the retina and on the structure of neural activity in the early visual system. Fixational instability introduces fluctuations in the retinal input signals that, in the presence of natural images, lack spatial correlations. These input fluctuations strongly influence neural activity in a model of the LGN. They decorrelate cell responses, even if the contrast sensitivity functions of simulated cells are not perfectly tuned to counter-balance the power-law spectrum of natural images. A decorrelation of neural activity has been proposed to be beneficial for discarding statistical redundancies in the input signals. Fixational instability might, therefore, contribute to establishing efficient representations of natural stimuli.},
 author = {Rucci, Michele},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5b4130c9e891d39891289001cc97d86b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5b4130c9e891d39891289001cc97d86b-Metadata.json},
 openalex = {W2163119847},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5b4130c9e891d39891289001cc97d86b-Paper.pdf},
 publisher = {MIT Press},
 title = {Visual Encoding with Jittering Eyes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/5b4130c9e891d39891289001cc97d86b-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_5be278a9,
 abstract = {The Octopus arm is a highly versatile and complex limb. How the Octopus controls such a hyper-redundant arm (not to mention eight of them!) is as yet unknown. Robotic arms based on the same mechanical principles may render present day robotic arms obsolete. In this paper, we tackle this control problem using an online reinforcement learning algorithm, based on a Bayesian approach to policy evaluation known as Gaussian process temporal difference (GPTD) learning. Our substitute for the real arm is a computer simulation of a 2-dimensional model of an Octopus arm. Even with the simplifications inherent to this model, the state space we face is a high-dimensional one. We apply a GPTD-based algorithm to this domain, and demonstrate its operation on several learning tasks of varying degrees of difficulty.},
 author = {Engel, Yaakov and Szabo, Peter and Volkinshtein, Dmitry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5be278a9e02bed9248a4674ff62fea2c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5be278a9e02bed9248a4674ff62fea2c-Metadata.json},
 openalex = {W2159008857},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5be278a9e02bed9248a4674ff62fea2c-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/5be278a9e02bed9248a4674ff62fea2c-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_5d75b942,
 abstract = {Tight bounds are derived on the risk of models in the ensemble generated by incremental training of an arbitrary learning algorithm. The result is based on proof techniques that are remarkably different from the standard risk analysis based on uniform convergence arguments, and improves on previous bounds published by the same authors.},
 author = {Cesa-bianchi, Nicol\`{o} and Gentile, Claudio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5d75b942ab4bd730bc2e819df9c9a4b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5d75b942ab4bd730bc2e819df9c9a4b5-Metadata.json},
 openalex = {W2046176422},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5d75b942ab4bd730bc2e819df9c9a4b5-Paper.pdf},
 publisher = {MIT Press},
 title = {Improved Risk Tail Bounds for On-Line Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/5d75b942ab4bd730bc2e819df9c9a4b5-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_5dc126b5,
 abstract = {We propose a simple information-theoretic approach to soft clustering based on maximizing the mutual information I(x,y) between the unknown cluster labels y and the training patterns x with respect to parameters of specifically constrained encoding distributions. The constraints are chosen such that patterns are likely to be clustered similarly if they lie close to specific unknown vectors in the feature space. The method may be conveniently applied to learning the optimal affinity matrix, which corresponds to learning parameters of the kernelized encoder. The procedure does not require computations of eigenvalues of the Gram matrices, which makes it potentially attractive for clustering large data sets.},
 author = {Barber, David and Agakov, Felix},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5dc126b503e374b0e08231344a7f493f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5dc126b503e374b0e08231344a7f493f-Metadata.json},
 openalex = {W2109346993},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5dc126b503e374b0e08231344a7f493f-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernelized Infomax Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/5dc126b503e374b0e08231344a7f493f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_60243f9b,
 author = {Tamosiunaite, Minija and Porr, Bernd and W\"{o}rg\"{o}tter, Florentin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/60243f9b1ac2dba11ff8131c8f4431e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/60243f9b1ac2dba11ff8131c8f4431e0-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/60243f9b1ac2dba11ff8131c8f4431e0-Paper.pdf},
 publisher = {MIT Press},
 title = {Temporally changing synaptic plasticity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/60243f9b1ac2dba11ff8131c8f4431e0-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6244b2ba,
 abstract = {Probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithms. No parametric models currently exist for modeling multi-variate correlated neural data and the high dimensional nature of the data makes fully non-parametric methods impractical. To address these problems we propose an energy-based model in which the joint probability of neural activity is represented using learned functions of the 1D marginal histograms of the data. The parameters of the model are learned using contrastive divergence and an optimization procedure for finding appropriate marginal directions. We evaluate the method using real data recorded from a population of motor cortical neurons. In particular, we model the joint probability of population spiking times and 2D hand position and show that the likelihood of test data under our model is significantly higher than under other models. These results suggest that our model captures correlations in the firing activity. Our rich probabilistic model of neural population activity is a step towards both measurement of the importance of correlations in neural coding and improved decoding of population activity.},
 author = {Wood, Frank and Roth, Stefan and Black, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6244b2ba957c48bc64582cf2bcec3d04-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6244b2ba957c48bc64582cf2bcec3d04-Metadata.json},
 openalex = {W2123601087},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6244b2ba957c48bc64582cf2bcec3d04-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling Neural Population Spiking Activity with Gibbs Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6244b2ba957c48bc64582cf2bcec3d04-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6271faad,
 abstract = {We introduce a method to automatically improve character models for a handwritten script without the use of transcriptions and using a minimum of document specific training data. We show that we can use searches for the words in a dictionary to identify portions of the document whose transcriptions are unambiguous. Using templates extracted from those regions, we retrain our character prediction model to drastically improve our search retrieval performance for words in the document.},
 author = {Edwards, Jaety and Forsyth, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6271faadeedd7626d661856b7a004e27-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6271faadeedd7626d661856b7a004e27-Metadata.json},
 openalex = {W2144166648},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6271faadeedd7626d661856b7a004e27-Paper.pdf},
 publisher = {MIT Press},
 title = {Searching for Character Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6271faadeedd7626d661856b7a004e27-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_64c31821,
 abstract = {Female crickets can locate males by phonotaxis to the mating song they produce. The behaviour and underlying physiology has been studied in some depth showing that the cricket auditory system solves this complex problem in a unique manner. We present an analogue very large scale integrated (aVLSI) circuit model of this process and show that results from testing the circuit agree with simulation and what is known from the behaviour and physiology of the cricket auditory system. The aVLSI circuitry is now being extended to use on a robot along with previously modelled neural circuitry to better understand the complete sensorimotor pathway.},
 author = {Schaik, Andre and Reeve, Richard and Jin, Craig and Hamilton, Tara},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/64c31821603ab476a318839606743bd6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/64c31821603ab476a318839606743bd6-Metadata.json},
 openalex = {W2098990046},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/64c31821603ab476a318839606743bd6-Paper.pdf},
 publisher = {MIT Press},
 title = {An aVLSI Cricket Ear Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/64c31821603ab476a318839606743bd6-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6624b6d8,
 abstract = {An analog focal-plane processor having a 128 x 128 photodiode array has been developed for directional edge filtering. It can perform 4 x 4-pixel kernel convolution for entire pixels only with 256 steps of simple analog processing. Newly developed cyclic line access and row-parallel processing scheme in conjunction with the only-nearest-neighbor interconnects architecture has enabled a very simple implementation. A proof-of-concept chip was fabricated in a 0.35-μm 2-poly 3-metal CMOS technology and the edge filtering at a rate of 200 frames/sec. has been experimentally demonstrated.},
 author = {Nakashita, Yusuke and Mita, Yoshio and Shibata, Tadashi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6624b6d8217cf71640993409df58204f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6624b6d8217cf71640993409df58204f-Metadata.json},
 openalex = {W2132989768},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6624b6d8217cf71640993409df58204f-Paper.pdf},
 publisher = {MIT Press},
 title = {An Analog Visual Pre-Processing Processor Employing Cyclic Line Access in Only-Nearest-Neighbor-Interconnects Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6624b6d8217cf71640993409df58204f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_663772ea,
 abstract = {We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semi-supervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge.},
 author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Metadata.json},
 openalex = {W2125569215},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf},
 publisher = {MIT Press},
 title = {The Curse of Highly Variable Functions for Local Kernel Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/663772ea088360f95bac3dc7ffb841be-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6775a063,
 abstract = {The computation required for Gaussian process regression with n training examples is about O(n3) during training and O(n) for each prediction. This makes Gaussian process regression too slow for large datasets. In this paper, we present a fast approximation method, based on kd-trees, that significantly reduces both the prediction and the training times of Gaussian process regression.},
 author = {Shen, Yirong and Seeger, Matthias and Ng, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6775a0635c302542da2c32aa19d86be0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6775a0635c302542da2c32aa19d86be0-Metadata.json},
 openalex = {W2115519811},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Gaussian Process Regression using KD-Trees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6b8b8e3b,
 author = {Jojic, Nebojsa and Jojic, Vladimir and Meek, Christopher and Heckerman, David and Frey, Brendan J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Paper.pdf},
 publisher = {MIT Press},
 title = {Using \textasciigrave \textasciigrave epitomes\textquotesingle \textquotesingle to model genetic diversity: Rational design of HIV vaccine cocktails},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6c990b7a,
 abstract = {Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Single-cell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision.},
 author = {Hurri, Jarmo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6c990b7aca7bc7058f5e98ea909e924b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6c990b7aca7bc7058f5e98ea909e924b-Metadata.json},
 openalex = {W2131093683},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Cue-Invariant Visual Responses},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6c990b7aca7bc7058f5e98ea909e924b-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6d19c113,
 abstract = {Given a set of points and a set of prototypes representing them, how to create a graph of the prototypes whose topology accounts for that of the points? This problem had not yet been explored in the framework of statistical learning theory. In this work, we propose a generative model based on the Delaunay graph of the prototypes and the Expectation-Maximization algorithm to learn the parameters. This work is a first step towards the construction of a topological model of a set of points grounded on statistics.},
 author = {Aupetit, Micha\"{e}l},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6d19c113404cee55b4036fce1a37c058-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6d19c113404cee55b4036fce1a37c058-Metadata.json},
 openalex = {W2144544875},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6d19c113404cee55b4036fce1a37c058-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Topology with the Generative Gaussian Graph and the EM Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6d19c113404cee55b4036fce1a37c058-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6e0e2429,
 abstract = {Brain-Computer Interface (BCI) systems create a novel communication channel from the brain to an output device by bypassing conventional motor output pathways of nerves and muscles. Therefore they could provide a new communication and control option for paralyzed patients. Modern BCI technology is essentially based on techniques for the classification of single-trial brain signals. Here we present a novel technique that allows the simultaneous optimization of a spatial and a spectral filter enhancing discriminability of multi-channel EEG single-trials. The evaluation of 60 experiments involving 22 different subjects demonstrates the superiority of the proposed algorithm. Apart from the enhanced classification, the spatial and/or the spectral filter that are determined by the algorithm can also be used for further analysis of the data, e.g., for source localization of the respective brain rhythms.},
 author = {Dornhege, Guido and Blankertz, Benjamin and Krauledat, Matthias and Losch, Florian and Curio, Gabriel and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e0e24295e8a86282cb559b860416812-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e0e24295e8a86282cb559b860416812-Metadata.json},
 openalex = {W2128044746},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e0e24295e8a86282cb559b860416812-Paper.pdf},
 publisher = {MIT Press},
 title = {Optimizing spatio-temporal filters for improving Brain-Computer Interfacing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6e0e24295e8a86282cb559b860416812-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6e3197aa,
 abstract = {We considered a gamma distribution of interspike intervals as a statistical model for neuronal spike generation. The model parameters consist of a time-dependent firing rate and a shape parameter that characterizes spiking irregularities of individual neurons. Because the environment changes with time, observed data are generated from the time-dependent firing rate, which is an unknown function. A statistical model with an unknown function is called a semiparametric model, which is one of the unsolved problem in statistics and is generally very difficult to solve. We used a novel method of estimating functions in information geometry to estimate the shape parameter without estimating the unknown function. We analytically obtained an optimal estimating function for the shape parameter independent of the functional form of the firing rate. This estimation is efficient without Fisher information loss and better than maximum likelihood estimation.},
 author = {Miura, Keiji and Okada, Masato and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e3197aae95c2ff8fcab35cb730f6a86-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e3197aae95c2ff8fcab35cb730f6a86-Metadata.json},
 openalex = {W2130094764},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e3197aae95c2ff8fcab35cb730f6a86-Paper.pdf},
 publisher = {MIT Press},
 title = {Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6e3197aae95c2ff8fcab35cb730f6a86-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6e82873a,
 abstract = {We characterize the sample complexity of active learning problems in terms of a parameter which takes into account the distribution over the input space, the specific target hypothesis, and the desired accuracy.},
 author = {Dasgupta, Sanjoy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e82873a32b95af115de1c414a1849cb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e82873a32b95af115de1c414a1849cb-Metadata.json},
 openalex = {W2115404432},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e82873a32b95af115de1c414a1849cb-Paper.pdf},
 publisher = {MIT Press},
 title = {Coarse sample complexity bounds for active learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6e82873a32b95af115de1c414a1849cb-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6f1d0705,
 abstract = {We design a new learning algorithm for the Set Covering Machine from a PAC-Bayes perspective and propose a PAC-Bayes risk bound which is minimized for classifiers achieving a non trivial margin-sparsity trade-off.},
 author = {Laviolette, Fran\c{c}ois and Marchand, Mario and Shah, Mohak},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6f1d0705c91c2145201df18a1a0c7345-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6f1d0705c91c2145201df18a1a0c7345-Metadata.json},
 openalex = {W2128274109},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf},
 publisher = {MIT Press},
 title = {A PAC-Bayes approach to the Set Covering Machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6f1d0705c91c2145201df18a1a0c7345-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_6fe13163,
 abstract = {This paper presents representation and logic for labeling contrast edges and ridges in visual scenes in terms of both surface occlusion (border ownership) and thinline objects. In natural scenes, thinline objects include sticks and wires, while in human graphical communication thin-lines include connectors, dividers, and other abstract devices. Our analysis is directed at both natural and graphical domains. The basic problem is to formulate the logic of the interactions among local image events, specifically contrast edges, ridges, junctions, and alignment relations, such as to encode the natural constraints among these events in visual scenes. In a sparse heterogeneous Markov Random Field framework, we define a set of interpretation nodes and energy/potential functions among them. The minimum energy configuration found by Loopy Belief Propagation is shown to correspond to preferred human interpretation across a wide range of prototypical examples including important illusory contour figures such as the Kanizsa Triangle, as well as more difficult examples. In practical terms, the approach delivers correct interpretations of inherently ambiguous hand-drawn box-and-connector diagrams at low computational cost.},
 author = {Saund, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6fe131632103526e3a6e8114c78eb1e1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6fe131632103526e3a6e8114c78eb1e1-Metadata.json},
 openalex = {W2152339448},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6fe131632103526e3a6e8114c78eb1e1-Paper.pdf},
 publisher = {MIT Press},
 title = {Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/6fe131632103526e3a6e8114c78eb1e1-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_73b81709,
 abstract = {We develop an approach for estimation with Gaussian Markov processes that imposes a smoothness prior while allowing for discontinuities. Instead of propagating information laterally between neighboring nodes in a graph, we study the posterior distribution of the hidden nodes as a whole—how it is perturbed by invoking discontinuities, or weakening the edges, in the graph. We show that the resulting computation amounts to feed-forward fan-in operations reminiscent of V1 neurons. Moreover, using suitable matrix preconditioners, the incurred matrix inverse and determinant can be approximated, without iteration, in the same computational style. Simulation results illustrate the merits of this approach.},
 author = {Huang, Yunsong and Jenkins, B. Keith},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/73b817090081cef1bca77232f4532c5d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/73b817090081cef1bca77232f4532c5d-Metadata.json},
 openalex = {W2122627658},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/73b817090081cef1bca77232f4532c5d-Paper.pdf},
 publisher = {MIT Press},
 title = {Non-iterative Estimation with Perturbed Gaussian Markov Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/73b817090081cef1bca77232f4532c5d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_74378afe,
 author = {Welling, Max and Gehler, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/74378afe5e8b20910cf1f939e57f0480-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/74378afe5e8b20910cf1f939e57f0480-Metadata.json},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/74378afe5e8b20910cf1f939e57f0480-Paper.pdf},
 publisher = {MIT Press},
 title = {Products of \textasciigrave \textasciigrave Edge-perts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/74378afe5e8b20910cf1f939e57f0480-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_765d5fb1,
 abstract = {We derive a Bayesian Ideal Observer (BIO) for detecting motion and solving the correspondence problem. We obtain Barlow and Tripathy's classic model as an approximation. Our psychophysical experiments show that the trends of human performance are similar to the Bayesian Ideal, but overall human performance is far worse. We investigate ways to degrade the Bayesian Ideal but show that even extreme degradations do not approach human performance. Instead we propose that humans perform motion tasks using generic, general purpose, models of motion. We perform more psychophysical experiments which are consistent with humans using a Slow-and-Smooth model and which rule out an alternative model using Slowness.},
 author = {Lu, Hongjing and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/765d5fb115a9f6a3e0b23b80a5b2e4c4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/765d5fb115a9f6a3e0b23b80a5b2e4c4-Metadata.json},
 openalex = {W2152960851},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/765d5fb115a9f6a3e0b23b80a5b2e4c4-Paper.pdf},
 publisher = {MIT Press},
 title = {Ideal Observers for Detecting Motion: Correspondence Noise},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/765d5fb115a9f6a3e0b23b80a5b2e4c4-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_780965ae,
 abstract = {There has been a surge of interest in learning non-linear manifold models to approximate high-dimensional data. Both for computational complexity reasons and for generalization capability, sparsity is a desired feature in such models. This usually means dimensionality reduction, which naturally implies estimating the intrinsic dimension, but it can also mean selecting a subset of the data to use as landmarks, which is especially important because many existing algorithms have quadratic complexity in the number of observations. This paper presents an algorithm for selecting landmarks, based on LASSO regression, which is well known to favor sparse approximations because it uses regularization with an l1 norm. As an added benefit, a continuous manifold parameterization, based on the landmarks, is also found. Experimental results with synthetic and real data illustrate the algorithm.},
 author = {Silva, Jorge and Marques, Jorge and Lemos, Jo\~{a}o},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/780965ae22ea6aee11935f3fb73da841-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/780965ae22ea6aee11935f3fb73da841-Metadata.json},
 openalex = {W2143328795},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/780965ae22ea6aee11935f3fb73da841-Paper.pdf},
 publisher = {MIT Press},
 title = {Selecting Landmark Points for Sparse Manifold Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/780965ae22ea6aee11935f3fb73da841-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_7a006957,
 abstract = {While kernel canonical correlation analysis (kernel CCA) has been applied in many problems, the asymptotic convergence of the functions estimated from a finite sample to the true functions has not yet been established. This paper gives a rigorous proof of the statistical convergence of kernel CCA and a related method (NOCCO), which provides a theoretical justification for these methods. The result also gives a sufficient condition on the decay of the regularization coefficient in the methods to ensure convergence.},
 author = {Fukumizu, Kenji and Gretton, Arthur and Bach, Francis},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7a006957be65e608e863301eb98e1808-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7a006957be65e608e863301eb98e1808-Metadata.json},
 openalex = {W2103618778},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7a006957be65e608e863301eb98e1808-Paper.pdf},
 publisher = {MIT Press},
 title = {Statistical Convergence of Kernel CCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/7a006957be65e608e863301eb98e1808-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_7e0a0209,
 abstract = {A standard method to obtain stochastic models for symbolic time series is to train state-emitting hidden Markov models (SE-HMMs) with the Baum-Welch algorithm. Based on observable operator models (OOMs), in the last few months a number of novel learning algorithms for similar purposes have been developed: (1,2) two versions of an sharpening (ES) algorithm, which iteratively improves the statistical efficiency of a sequence of OOM estimators, (3) a constrained gradient descent ML estimator for transition-emitting HMMs (TE-HMMs). We give an overview on these algorithms and compare them with SE-HMM/EM learning on synthetic and real-life data.},
 author = {Jaeger, Herbert and Zhao, Mingjie and Kolling, Andreas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7e0a0209b929d097bd3e8ef30567a5c1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7e0a0209b929d097bd3e8ef30567a5c1-Metadata.json},
 openalex = {W2129070594},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7e0a0209b929d097bd3e8ef30567a5c1-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient Estimation of OOMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/7e0a0209b929d097bd3e8ef30567a5c1-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_7f018eb7,
 abstract = {A 5-layer neuromorphic vision processor whose components communicate spike events asychronously using the address-event-representation (AER) is demonstrated. The system includes a retina chip, two convolution chips, a 2D winner-take-all chip, a delay line chip, a learning classifier chip, and a set of PCBs for computer interfacing and address space remappings. The components use a mixture of analog and digital computation and will learn to classify trajectories of a moving object. A complete experimental setup and measurements results are shown.},
 author = {Serrano-Gotarredona, R. and Oster, M. and Lichtsteiner, P. and Linares-Barranco, A. and Paz-Vicente, R. and Gomez-Rodriguez, F. and Kolle Riis, H. and Delbruck, T. and Liu, S. C. and Zahnd, S. and Whatley, A. M. and Douglas, R. and Hafliger, P. and Jimenez-Moreno, G. and Civit, A. and Serrano-Gotarredona, T. and Acosta-Jimenez, A. and Linares-Barranco, B.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7f018eb7b301a66658931cb8a93fd6e8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7f018eb7b301a66658931cb8a93fd6e8-Metadata.json},
 openalex = {W2128644883},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf},
 publisher = {MIT Press},
 title = {AER Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_7f141cf8,
 abstract = {We describe a hierarchical compositional system for detecting de- formable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corre- sponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320 × 240 images. We demonstrate the approach on detecting cat- s, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation.},
 author = {Zhu, Long and Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7f141cf8e7136ce8701dc6636c2a6fe4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7f141cf8e7136ce8701dc6636c2a6fe4-Metadata.json},
 openalex = {W1544847736},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7f141cf8e7136ce8701dc6636c2a6fe4-Paper.pdf},
 publisher = {MIT Press},
 title = {A Hierarchical Compositional System for Rapid Object Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/7f141cf8e7136ce8701dc6636c2a6fe4-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_801fd8c2,
 abstract = {We present a conditional temporal probabilistic framework for reconstructing 3D human motion in monocular video based on descriptors encoding image silhouette observations. For computational efficiency we restrict visual inference to low-dimensional kernel induced non-linear state spaces. Our methodology (kBME) combines kernel PCA-based non-linear dimensionality reduction (kPCA) and Conditional Bayesian Mixture of Experts (BME) in order to learn complex multivalued predictors between observations and model hidden states. This is necessary for accurate, inverse, visual perception inferences, where several probable, distant 3D solutions exist due to noise or the uncertainty of monocular perspective projection. Low-dimensional models are appropriate because many visual processes exhibit strong non-linear correlations in both the image observations and the target, hidden state variables. The learned predictors are temporally combined within a conditional graphical model in order to allow a principled propagation of uncertainty. We study several predictors and empirically show that the proposed algorithm positively compares with techniques based on regression, Kernel Dependency Estimation (KDE) or PCA alone, and gives results competitive to those of high-dimensional mixture predictors at a fraction of their computational cost. We show that the method successfully reconstructs the complex 3D motion of humans in real monocular video sequences.},
 author = {Sminchisescu, Cristian and Kanujia, Atul and Li, Zhiguo and Metaxas, Dimitris},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/801fd8c2a4e79c1d24a40dc735c051ae-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/801fd8c2a4e79c1d24a40dc735c051ae-Metadata.json},
 openalex = {W2137674454},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/801fd8c2a4e79c1d24a40dc735c051ae-Paper.pdf},
 publisher = {MIT Press},
 title = {Conditional Visual Tracking in Kernel Space},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/801fd8c2a4e79c1d24a40dc735c051ae-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_85690f81,
 abstract = {This paper addresses the issue of numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes. It presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods. The experiments show significant gains in computation and storage on datasets arising in image segmentation, object detection and dimensionality reduction. The paper also presents theoretical bounds on the stability of these methods.},
 author = {Freitas, Nando and Wang, Yang and Mahdaviani, Maryam and Lang, Dustin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/85690f81aadc1749175c187784afc9ee-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/85690f81aadc1749175c187784afc9ee-Metadata.json},
 openalex = {W2153508491},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/85690f81aadc1749175c187784afc9ee-Paper.pdf},
 publisher = {MIT Press},
 title = {Fast Krylov Methods for N-Body Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/85690f81aadc1749175c187784afc9ee-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_86c4ad52,
 abstract = {We present a new connectionist model for constructive, intuitionistic modal reasoning. We use ensembles of neural networks to represent intuitionistic modal theories, and show that for each intuitionistic modal program there exists a corresponding neural network ensemble that computes the program. This provides a massively parallel model for intuitionistic modal reasoning, and sets the scene for integrated reasoning, knowledge representation, and learning of intuitionistic theories in neural networks, since the networks in the ensemble can be trained by examples using standard neural learning algorithms.},
 author = {Garcez, Artur and Lamb, Luis and Gabbay, Dov M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/86c4ad52768c511046fea7b2d42b300c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/86c4ad52768c511046fea7b2d42b300c-Metadata.json},
 openalex = {W2152766352},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/86c4ad52768c511046fea7b2d42b300c-Paper.pdf},
 publisher = {MIT Press},
 title = {A Connectionist Model for Constructive Modal Reasoning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/86c4ad52768c511046fea7b2d42b300c-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_881c6efa,
 abstract = {Recurrent networks that perform a winner-take-all computation have been studied extensively. Although some of these studies include spiking networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-fire neurons which receives spike trains as inputs. We show how we can configure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the winner-take-all network on an analog VLSI array of 64 integrate-and-fire neurons which have an innate variance in their operating parameters.},
 author = {Oster, Matthias and Liu, Shih-Chii},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/881c6efa917cff1c97a74e03e15f43e8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/881c6efa917cff1c97a74e03e15f43e8-Metadata.json},
 openalex = {W2161340833},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/881c6efa917cff1c97a74e03e15f43e8-Paper.pdf},
 publisher = {MIT Press},
 title = {Spiking Inputs to a Winner-take-all Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/881c6efa917cff1c97a74e03e15f43e8-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_892c3b1c,
 abstract = {We introduce a technique for dimensionality estimation based on the notion of quantization dimension, which connects the asymptotic optimal quantization error for a probability distribution on a manifold to its intrinsic dimension. The definition of quantization dimension yields a family of estimation algorithms, whose limiting case is equivalent to a recent method based on packing numbers. Using the formalism of high-rate vector quantization, we address issues of statistical consistency and analyze the behavior of our scheme in the presence of noise.},
 author = {Raginsky, Maxim and Lazebnik, Svetlana},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/892c3b1c6dccd52936e27cbd0ff683d6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/892c3b1c6dccd52936e27cbd0ff683d6-Metadata.json},
 openalex = {W2102438258},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf},
 publisher = {MIT Press},
 title = {Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_894db62f,
 abstract = {In this paper we propose a general framework to study the generalization properties of binary classifiers trained with data which may be dependent, but are deterministically generated upon a sample of independent examples. It provides generalization bounds for binary classification and some cases of ranking problems, and clarifies the relationship between these learning tasks.},
 author = {Usunier, Nicolas and Amini, Massih R. and Gallinari, Patrick},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/894db62f7b7a6ed2f2a277dae56a017c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/894db62f7b7a6ed2f2a277dae56a017c-Metadata.json},
 openalex = {W2141732327},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/894db62f7b7a6ed2f2a277dae56a017c-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalization error bounds for classifiers trained with interdependent data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/894db62f7b7a6ed2f2a277dae56a017c-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_8b3bac12,
 abstract = {Humans make optimal perceptual decisions in noisy and ambiguous conditions. Computations underlying such optimal behavior have been shown to rely on probabilistic inference according to generative models whose structure is usually taken to be known a priori. We argue that Bayesian model selection is ideal for inferring similar and even more complex model structures from experience. We find in experiments that humans learn subtle statistical properties of visual scenes in a completely unsupervised manner. We show that these findings are well captured by Bayesian model learning within a class of models that seek to explain observed variables by independent hidden causes.},
 author = {Orb\'{a}n, Gerg\H{o} and Fiser, Jozsef and Aslin, Richard N and Lengyel, M\'{a}t\'{e}},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8b3bac12926cc1d9fb5d68783376971d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8b3bac12926cc1d9fb5d68783376971d-Metadata.json},
 openalex = {W2123562323},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8b3bac12926cc1d9fb5d68783376971d-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian model learning in human visual perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/8b3bac12926cc1d9fb5d68783376971d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_8e930496,
 abstract = {We present an efficient algorithm to actively select queries for learning the boundaries separating a function domain into regions where the function is above and below a given threshold. We develop experiment selection methods based on entropy, misclassification rate, variance, and their combinations, and show how they perform on a number of data sets. We then show how these algorithms are used to determine simultaneously valid 1 - α confidence intervals for seven cosmological parameters. Experimentation shows that the algorithm reduces the computation necessary for the parameter estimation problem by an order of magnitude.},
 author = {Bryan, Brent and Nichol, Robert C. and Genovese, Christopher R and Schneider, Jeff and Miller, Christopher J. and Wasserman, Larry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8e930496927757aac0dbd2438cb3f4f6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8e930496927757aac0dbd2438cb3f4f6-Metadata.json},
 openalex = {W2096642565},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8e930496927757aac0dbd2438cb3f4f6-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Learning For Identifying Function Threshold Boundaries},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/8e930496927757aac0dbd2438cb3f4f6-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_8e987cf1,
 abstract = {We present a model of edge and region grouping using a conditional random field built over a scale-invariant representation of images to integrate multiple cues. Our model includes potentials that capture low-level similarity, mid-level curvilinear continuity and high-level object shape. Maximum likelihood parameters for the model are learned from human labeled groundtruth on a large collection of horse images using belief propagation. Using held out test data, we quantify the information gained by incorporating generic mid-level cues and high-level shape.},
 author = {Ren, Xiaofeng and Malik, Jitendra and Fowlkes, Charless},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8e987cf1b2f1f6ffa6a43066798b4b7f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8e987cf1b2f1f6ffa6a43066798b4b7f-Metadata.json},
 openalex = {W2116773539},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8e987cf1b2f1f6ffa6a43066798b4b7f-Paper.pdf},
 publisher = {MIT Press},
 title = {Cue Integration for Figure/Ground Labeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/8e987cf1b2f1f6ffa6a43066798b4b7f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_8fc687aa,
 abstract = {We describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program. We show how neural networks can be trained to infer the motor programs required to accurately reconstruct the MNIST digits. The inferred motor programs can be used directly for digit classification, but they can also be used in other ways. By adding noise to the motor program inferred from an MNIST image we can generate a large set of very different images of the same class, thus enlarging the training set available to other methods. We can also use the motor programs as additional, highly informative outputs which reduce overfitting when training a feed-forward classifier.},
 author = {Nair, Vinod and Hinton, Geoffrey E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8fc687aa152e8199fe9e73304d407bca-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8fc687aa152e8199fe9e73304d407bca-Metadata.json},
 openalex = {W2119985666},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8fc687aa152e8199fe9e73304d407bca-Paper.pdf},
 publisher = {MIT Press},
 title = {Inferring Motor Programs from Images of Handwritten Digits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/8fc687aa152e8199fe9e73304d407bca-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_9087b0ef,
 abstract = {In previous work we presented an efficient approach to computing kernel summations which arise in many machine learning methods such as kernel density estimation. This approach, dual-tree recursion with finite-difference approximation, generalized existing methods for similar problems arising in computational physics in two ways appropriate for statistical problems: toward distribution sensitivity and general dimension, partly by avoiding series expansions. While this proved to be the fastest practical method for multivariate kernel density estimation at the optimal bandwidth, it is much less efficient at larger-than-optimal bandwidths. In this work, we explore the extent to which the dual-tree approach can be integrated with multipole-like Hermite expansions in order to achieve reasonable efficiency across all bandwidth scales, though only for low dimensionalities. In the process, we derive and demonstrate the first truly hierarchical fast Gauss transforms, effectively combining the best tools from discrete algorithms and continuous approximation theory.},
 author = {Lee, Dongryeol and Moore, Andrew and Gray, Alexander},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9087b0efc7c7acd1ef7e153678809c77-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9087b0efc7c7acd1ef7e153678809c77-Metadata.json},
 openalex = {W2154011553},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9087b0efc7c7acd1ef7e153678809c77-Paper.pdf},
 publisher = {MIT Press},
 title = {Dual-Tree Fast Gauss Transforms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/9087b0efc7c7acd1ef7e153678809c77-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_92bf5e62,
 abstract = {Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes. In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image. Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data. For visual scenes, mixture components describe the spatial structure of visual features in an object-centered coordinate frame, while transformations model the object positions in a particular image. Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler. Applied to a dataset of partially labeled street scenes, we show that the TDP's inclusion of spatial structure improves detection performance, flexibly exploiting partially labeled training images.},
 author = {Torralba, Antonio and Willsky, Alan and Sudderth, Erik and Freeman, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/92bf5e6240737e0326ea59846a83e076-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/92bf5e6240737e0326ea59846a83e076-Metadata.json},
 openalex = {W2110717342},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf},
 publisher = {MIT Press},
 title = {Describing Visual Scenes using Transformed Dirichlet Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_952c3ff9,
 abstract = {We present a competitive analysis of some non-parametric Bayesian algorithms in a worst-case online learning setting, where no probabilistic assumptions about the generation of the data are made. We consider models which use a Gaussian process prior (over the space of all functions) and provide bounds on the regret (under the log loss) for commonly used non-parametric Bayesian algorithms — including Gaussian regression and logistic regression — which show how these algorithms can perform favorably under rather general conditions. These bounds explicitly handle the infinite dimensionality of these non-parametric classes in a natural way. We also make formal connections to the minimax and minimum description length (MDL) framework. Here, we show precisely how Bayesian Gaussian regression is a minimax strategy.},
 author = {Kakade, Sham M. and Seeger, Matthias W. and Foster, Dean P.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/952c3ff98a6acdc36497d839e31aa57c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/952c3ff98a6acdc36497d839e31aa57c-Metadata.json},
 openalex = {W2151242073},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/952c3ff98a6acdc36497d839e31aa57c-Paper.pdf},
 publisher = {MIT Press},
 title = {Worst-Case Bounds for Gaussian Process Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/952c3ff98a6acdc36497d839e31aa57c-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_9719a00e,
 abstract = {We present a novel approach to the characterization of complex sensory neurons. One of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive (causing large gradients in the neural responses) or alternatively dimensions in stimulus space to which the neuronal response are invariant (defining iso-response manifolds). We formulate this problem as that of learning a geometry on stimulus space that is compatible with the neural responses: the distance between stimuli should be large when the responses they evoke are very different, and small when the responses they evoke are similar. Here we show how to successfully train such distance functions using rather limited amount of information. The data consisted of the responses of neurons in primary auditory cortex (A1) of anesthetized cats to 32 stimuli derived from natural sounds. For each neuron, a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar. The distance function was trained to fit these constraints. The resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli.},
 author = {Weiner, Inna and Hertz, Tomer and Nelken, Israel and Weinshall, Daphna},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9719a00ed0c5709d80dfef33795dcef3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9719a00ed0c5709d80dfef33795dcef3-Metadata.json},
 openalex = {W2151670934},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9719a00ed0c5709d80dfef33795dcef3-Paper.pdf},
 publisher = {MIT Press},
 title = {Analyzing Auditory Neurons by Learning Distance Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/9719a00ed0c5709d80dfef33795dcef3-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_9752d873,
 abstract = {Although variants of value iteration have been proposed for finding Nash or correlated equilibria in general-sum Markov games, these variants have not been shown to be effective in general. In this paper, we demonstrate by construction that existing variants of value iteration cannot find stationary equilibrium policies in arbitrary general-sum Markov games. Instead, we propose an alternative interpretation of the output of value iteration based on a new (non-stationary) equilibrium concept that we call We prove that value iteration identifies cyclic equilibria in a class of games in which it fails to find stationary equilibria. We also demonstrate empirically that value iteration finds cyclic equilibria in nearly all examples drawn from a random distribution of Markov games.},
 author = {Zinkevich, Martin and Greenwald, Amy and Littman, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9752d873fa71c19dc602bf2a0696f9b5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9752d873fa71c19dc602bf2a0696f9b5-Metadata.json},
 openalex = {W2145943204},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9752d873fa71c19dc602bf2a0696f9b5-Paper.pdf},
 publisher = {MIT Press},
 title = {Cyclic Equilibria in Markov Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/9752d873fa71c19dc602bf2a0696f9b5-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_98d8a23f,
 abstract = {This paper presents a new filter for online data association problems in high-dimensional spaces. The key innovation is a representation of the data association posterior in information form, in which the proximity of objects and tracks are expressed by numerical links. Updating these links requires linear time, compared to exponential time required for computing the exact posterior probabilities. The paper derives the algorithm formally and provides comparative results using data obtained by a real-world camera array and by a large-scale sensor network simulation.},
 author = {Schumitsch, Brad and Thrun, Sebastian and Bradski, Gary and Olukotun, Kunle},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/98d8a23fd60826a2a474c5b4f5811707-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/98d8a23fd60826a2a474c5b4f5811707-Metadata.json},
 openalex = {W2171744852},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/98d8a23fd60826a2a474c5b4f5811707-Paper.pdf},
 publisher = {MIT Press},
 title = {The Information-Form Data Association Filter},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/98d8a23fd60826a2a474c5b4f5811707-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_9a118833,
 abstract = {We determine the asymptotic limit of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to infinity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held fixed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classification error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the first time to be a consistent density level set estimator.},
 author = {Vert, R\'{e}gis and Vert, Jean-philippe},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9a11883317fde3aef2e2432a58c86779-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9a11883317fde3aef2e2432a58c86779-Metadata.json},
 openalex = {W2115097947},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9a11883317fde3aef2e2432a58c86779-Paper.pdf},
 publisher = {MIT Press},
 title = {Consistency of one-class SVM and related algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/9a11883317fde3aef2e2432a58c86779-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_9dc37271,
 abstract = {Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model.},
 author = {Murray, Iain and MacKay, David and Ghahramani, Zoubin and Skilling, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9dc372713683fd865d366d5d9ee810ba-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9dc372713683fd865d366d5d9ee810ba-Metadata.json},
 openalex = {W2172046899},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9dc372713683fd865d366d5d9ee810ba-Paper.pdf},
 publisher = {MIT Press},
 title = {Nested sampling for Potts models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/9dc372713683fd865d366d5d9ee810ba-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_9e82757e,
 abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [1]. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.},
 author = {Lafferty, John and Blei, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9e82757e9a1c12cb710ad680db11f6f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9e82757e9a1c12cb710ad680db11f6f1-Metadata.json},
 openalex = {W2112050062},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf},
 publisher = {MIT Press},
 title = {Correlated Topic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_a226e450,
 abstract = {We describe a neuromorphic chip that uses binary synapses with spike timing-dependent plasticity (STDP) to learn stimulated patterns of activity and to compensate for variability in excitability. Specifically, STDP preferentially potentiates (turns on) synapses that project from excitable neurons, which spike early, to lethargic neurons, which spike late. The additional excitatory synaptic current makes lethargic neurons spike earlier, thereby causing neurons that belong to the same pattern to spike in synchrony. Once learned, an entire pattern can be recalled by stimulating a subset.},
 author = {Arthur, John V. and Boahen, Kwabena},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a226e450e214f350856e2980b6e55ac9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a226e450e214f350856e2980b6e55ac9-Metadata.json},
 openalex = {W2105452346},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a226e450e214f350856e2980b6e55ac9-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning in Silicon: Timing is Everything},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/a226e450e214f350856e2980b6e55ac9-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_a36b0dcd,
 abstract = {We study the problem of maximum entropy density estimation in the presence of known sample selection bias. We propose three bias correction approaches. The first one takes advantage of unbiased sufficient statistics which can be obtained from biased samples. The second one estimates the biased distribution and then factors the bias out. The third one approximates the second by only using samples from the sampling distribution. We provide guarantees for the first two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling, where maxent has been successfully applied and where sample selection bias is a significant problem.},
 author = {Dud\'{\i}k, Miroslav and Phillips, Steven and Schapire, Robert E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a36b0dcd1e6384abc0e1867860ad3ee3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a36b0dcd1e6384abc0e1867860ad3ee3-Metadata.json},
 openalex = {W2158754421},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a36b0dcd1e6384abc0e1867860ad3ee3-Paper.pdf},
 publisher = {MIT Press},
 title = {Correcting sample selection bias in maximum entropy density estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/a36b0dcd1e6384abc0e1867860ad3ee3-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_a3788c8c,
 abstract = {This paper provides a system-level analysis of a scalable distributed sensing model for networked sensors. In our system model, a data center acquires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate distortion codes, we show that the system performance can be evaluated for any given finite R when the number of sensors L goes to infinity. The analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate R or the noise level.},
 author = {Murayama, Tatsuto and Davis, Peter},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a3788c8c64fd65c470e23e7534c3ebc8-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a3788c8c64fd65c470e23e7534c3ebc8-Metadata.json},
 openalex = {W2115864075},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a3788c8c64fd65c470e23e7534c3ebc8-Paper.pdf},
 publisher = {MIT Press},
 title = {Rate Distortion Codes in Sensor Networks: A System-level Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/a3788c8c64fd65c470e23e7534c3ebc8-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_a4666cd9,
 abstract = {While classical experiments on spike-timing dependent plasticity analyzed synaptic changes as a function of the timing of pairs of pre- and postsynaptic spikes, more recent experiments also point to the effect of spike triplets. Here we develop a mathematical framework that allows us to characterize timing based learning rules. Moreover, we identify a candidate learning rule with five variables (and 5 free parameters) that captures a variety of experimental data, including the dependence of potentiation and depression upon pre- and postsynaptic firing frequencies. The relation to the Bienenstock-Cooper-Munro rule as well as to some timing-based rules is discussed.},
 author = {Pfister, Jean-pascal and Gerstner, Wulfram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Metadata.json},
 openalex = {W2152917761},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Paper.pdf},
 publisher = {MIT Press},
 title = {Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/a4666cd9e1ab0e4abf05a0fb232f4ad3-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_a57e8915,
 abstract = {Linear implementations of the efficient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1,2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive fields in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6,7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been fixed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and find that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and filters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive fields and collectively span a broad range of spatial scales. Our work unifies several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.},
 author = {Karklin, Yan and Lewicki, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a57e8915461b83adefb011530b711704-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a57e8915461b83adefb011530b711704-Metadata.json},
 openalex = {W2171966657},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a57e8915461b83adefb011530b711704-Paper.pdf},
 publisher = {MIT Press},
 title = {Is Early Vision Optimized for Extracting Higher-order Dependencies?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/a57e8915461b83adefb011530b711704-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_a6db4ed0,
 abstract = {The problem of computing a resample estimate for the reconstruction error in PCA is reformulated as an inference problem with the help of the replica method. Using the expectation consistent (EC) approximation, the intractable inference problem can be solved efficiently using only two variational parameters. A perturbative correction to the result is computed and an alternative simplified derivation is also presented.},
 author = {Opper, Manfred},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a6db4ed04f1621a119799fd3d7545d3d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a6db4ed04f1621a119799fd3d7545d3d-Metadata.json},
 openalex = {W2126549148},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a6db4ed04f1621a119799fd3d7545d3d-Paper.pdf},
 publisher = {MIT Press},
 title = {An Approximate Inference Approach for the PCA Reconstruction Error},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/a6db4ed04f1621a119799fd3d7545d3d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_a78482ce,
 abstract = {We present a novel cochlear model implemented in analog very large scale integration (VLSI) technology that emulates nonlinear active cochlear behavior. This silicon cochlea includes outer hair cell (OHC) electromotility through active bidirectional coupling (ABC), a mechanism we proposed in which OHC motile forces, through the microanatomical organization of the organ of Corti, realize the cochlear amplifier. Our chip measurements demonstrate that frequency responses become larger and more sharply tuned when ABC is turned on; the degree of the enhancement decreases with input intensity as ABC includes saturation of OHC forces.},
 author = {Wen, Bo and Boahen, Kwabena A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a78482ce76496fcf49085f2190e675b4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a78482ce76496fcf49085f2190e675b4-Metadata.json},
 openalex = {W2151431860},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a78482ce76496fcf49085f2190e675b4-Paper.pdf},
 publisher = {MIT Press},
 title = {Active Bidirectional Coupling in a Cochlear Chip},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/a78482ce76496fcf49085f2190e675b4-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_a7a3d70c,
 abstract = {We consider the problem of localizing a set of microphones together with a set of external acoustic events (e.g., hand claps), emitted at unknown times and unknown locations. We propose a solution that approximates this problem under a far field approximation defined in the calculus of affine geometry, and that relies on singular value decomposition (SVD) to recover the affine structure of the problem. We then define low-dimensional optimization techniques for embedding the solution into Euclidean geometry, and further techniques for recovering the locations and emission times of the acoustic events. The approach is useful for the calibration of ad-hoc microphone arrays and sensor networks.},
 author = {Thrun, Sebastian},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7a3d70c6d17a73140918996d03c014f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7a3d70c6d17a73140918996d03c014f-Metadata.json},
 openalex = {W2168793017},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf},
 publisher = {MIT Press},
 title = {Affine Structure From Sound},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_a7f592ce,
 abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.},
 author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Metadata.json},
 openalex = {W2106053110},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf},
 publisher = {MIT Press},
 title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/a7f592cef8b130a6967a90617db5681b-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ac34ae1f,
 abstract = {Consider the problem of joint parameter estimation and prediction in a Markov random field: i.e., the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working in the computation-limited setting, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the wrong model even in the infinite data limit) is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product.},
 author = {Wainwright, Martin J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ac34ae1fda29b8fe781ac8d6d32a6bc7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ac34ae1fda29b8fe781ac8d6d32a6bc7-Metadata.json},
 openalex = {W2163191289},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ac34ae1fda29b8fe781ac8d6d32a6bc7-Paper.pdf},
 publisher = {MIT Press},
 title = {Estimating the wrong Markov random field: Benefits in the computation-limited setting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ac34ae1fda29b8fe781ac8d6d32a6bc7-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ad82140c,
 abstract = {In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance.},
 author = {Murillo-fuentes, Juan and Caro, Sebastian and P\'{e}rez-Cruz, Fernando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ad82140cafe816c41a9c9974e9240b7a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ad82140cafe816c41a9c9974e9240b7a-Metadata.json},
 openalex = {W2110474789},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ad82140cafe816c41a9c9974e9240b7a-Paper.pdf},
 publisher = {MIT Press},
 title = {Gaussian Processes for Multiuser Detection in CDMA receivers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ad82140cafe816c41a9c9974e9240b7a-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ad8e88c0,
 abstract = {We present an algorithm for learning a quadratic Gaussian metric (Mahalanobis distance) for use in classification tasks. Our method relies on the simple geometric intuition that a good metric is one under which points in the same class are simultaneously near each other and far from points in the other classes. We construct a convex optimization problem whose solution generates such a metric by trying to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. We show that when the metric we learn is used in simple classifiers, it yields substantial improvements over standard alternatives on a variety of problems. We also discuss how the learned metric may be used to obtain a compact low dimensional feature representation of the original input space, allowing more efficient classification with very little reduction in performance.},
 author = {Globerson, Amir and Roweis, Sam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ad8e88c0f76fa4fc8e5474384142a00a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ad8e88c0f76fa4fc8e5474384142a00a-Metadata.json},
 openalex = {W2104752854},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ad8e88c0f76fa4fc8e5474384142a00a-Paper.pdf},
 publisher = {MIT Press},
 title = {Metric Learning by Collapsing Classes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ad8e88c0f76fa4fc8e5474384142a00a-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ade55409,
 abstract = {This paper presents a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose correlations between variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlations. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. This perspective leads to a better understanding of Gaussian belief propagation and of its convergence in loopy graphs.},
 author = {Malioutov, Dmitry and Willsky, Alan and Johnson, Jason},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ade55409d1224074754035a5a937d2e0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ade55409d1224074754035a5a937d2e0-Metadata.json},
 openalex = {W2169406849},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ade55409d1224074754035a5a937d2e0-Paper.pdf},
 publisher = {MIT Press},
 title = {Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ade55409d1224074754035a5a937d2e0-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ae587cfe,
 abstract = {When trying to understand the brain, it is of fundamental importance to analyse (e.g. from EEG/MEG measurements) what parts of the cortex interact with each other in order to infer more accurate models of brain activity. Common techniques like Blind Source Separation (BSS) can estimate brain sources and single out artifacts by using the underlying assumption of source signal independence. However, physiologically interesting brain sources typically interact, so BSS will—by construction— fail to characterize them properly. Noting that there are truly interacting sources and signals that only seemingly interact due to effects of volume conduction, this work aims to contribute by distinguishing these effects. For this a new BSS technique is proposed that uses anti-symmetrized cross-correlation matrices and subsequent diagonalization. The resulting decomposition consists of the truly interacting brain sources and suppresses any spurious interaction stemming from volume conduction. Our new concept of interacting source analysis (ISA) is successfully demonstrated on MEG data.},
 author = {Nolte, Guido and Ziehe, Andreas and Meinecke, Frank and M\"{u}ller, Klaus-Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ae587cfeea5ac21a8f1c1ea51027fef0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ae587cfeea5ac21a8f1c1ea51027fef0-Metadata.json},
 openalex = {W2159463141},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ae587cfeea5ac21a8f1c1ea51027fef0-Paper.pdf},
 publisher = {MIT Press},
 title = {Analyzing Coupled Brain Sources: Distinguishing True from Spurious Interaction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ae587cfeea5ac21a8f1c1ea51027fef0-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_aee92f16,
 abstract = {We present a probabilistic generative model of entity relationships and their attributes that simultaneously discovers groups among the entities and topics among the corresponding textual attributes. Block-models of relationship data have been studied in social network analysis for some time. Here we simultaneously cluster in several modalities at once, incorporating the attributes (here, words) associated with certain relationships. Significantly, joint inference allows the discovery of topics to be guided by the emerging groups, and vice-versa. We present experimental results on two large data sets: sixteen years of bills put before the U.S. Senate, comprising their corresponding text and voting records, and thirteen years of similar data from the United Nations. We show that in comparison with traditional, separate latent-variable models for words, or Block-structures for votes, the Group-Topic model's joint inference discovers more cohesive groups and improved topics.},
 author = {Wang, Xuerui and Mohanty, Natasha and McCallum, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/aee92f16efd522b9326c25cc3237ac15-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/aee92f16efd522b9326c25cc3237ac15-Metadata.json},
 openalex = {W2137905553},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/aee92f16efd522b9326c25cc3237ac15-Paper.pdf},
 publisher = {MIT Press},
 title = {Group and Topic Discovery from Relations and Their Attributes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/aee92f16efd522b9326c25cc3237ac15-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_aeefb050,
 abstract = {The observed physiological dynamics of an infant receiving intensive care are affected by many possible factors, including interventions to the baby, the operation of the monitoring equipment and the state of health. The Factorial Switching Kalman Filter can be used to infer the presence of such factors from a sequence of observations, and to estimate the true values where these observations have been corrupted. We apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns.},
 author = {Williams, Christopher and Quinn, John and Mcintosh, Neil},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/aeefb050911334869a7a5d9e4d0e1689-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/aeefb050911334869a7a5d9e4d0e1689-Metadata.json},
 openalex = {W2145504747},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/aeefb050911334869a7a5d9e4d0e1689-Paper.pdf},
 publisher = {MIT Press},
 title = {Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/aeefb050911334869a7a5d9e4d0e1689-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b030afbb,
 abstract = {We investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity (STDP) to predict the arrival times of strong teacher inputs to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theorem, which predicts convergence of the perceptron learning rule for a simplified neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Perceptron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses.},
 author = {Legenstein, Robert and Maass, Wolfgang},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b030afbb3a8af8fb0759241c97466ee4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b030afbb3a8af8fb0759241c97466ee4-Metadata.json},
 openalex = {W2154562370},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b030afbb3a8af8fb0759241c97466ee4-Paper.pdf},
 publisher = {MIT Press},
 title = {A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b030afbb3a8af8fb0759241c97466ee4-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b0bef4c9,
 abstract = {We show that Queyranne's algorithm for minimizing symmetric submodular functions can be used for clustering with a variety of different objective functions. Two specific criteria that we consider in this paper are the single linkage and the minimum description length criteria. The first criterion tries to maximize the minimum distance between elements of different clusters, and is inherently discriminative. It is known that optimal clusterings into k clusters for any given k in polynomial time for this criterion can be computed. The second criterion seeks to minimize the description length of the clusters given a probabilistic generative model. We show that the optimal partitioning into 2 clusters, and approximate partitioning (guaranteed to be within a factor of 2 of the the optimal) for more clusters can be computed. To the best of our knowledge, this is the first time that a tractable algorithm for finding the optimal clustering with respect to the MDL criterion for 2 clusters has been given. Besides the optimality result for the MDL criterion, the chief contribution of this paper is to show that the same algorithm can be used to optimize a broad class of criteria, and hence can be used for many application specific criterion for which efficient algorithm are not known.},
 author = {Narasimhan, Mukund and Jojic, Nebojsa and Bilmes, Jeff A},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b0bef4c9a6e50d43880191492d4fc827-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b0bef4c9a6e50d43880191492d4fc827-Metadata.json},
 openalex = {W2293768704},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b0bef4c9a6e50d43880191492d4fc827-Paper.pdf},
 publisher = {MIT Press},
 title = {Q-Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b0bef4c9a6e50d43880191492d4fc827-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b1300291,
 abstract = {We consider the problem of constructing an aggregated estimator from a finite class of base functions which approximately minimizes a convex risk functional under the l1 constraint. For this purpose, we propose a stochastic procedure, the mirror descent, which performs gradient descent in the dual space. The generated estimates are additionally averaged in a recursive fashion with specific weights. Mirror descent algorithms have been developed in different contexts and they are known to be particularly efficient in high dimensional problems. Moreover their implementation is adapted to the online setting. The main result of the paper is the upper bound on the convergence rate for the generalization error.},
 author = {Juditsky, Anatoli and Nazin, Alexander and Tsybakov, Alexandre and Vayatis, Nicolas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b1300291698eadedb559786c809cc592-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b1300291698eadedb559786c809cc592-Metadata.json},
 openalex = {W2104127978},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b1300291698eadedb559786c809cc592-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalization error bounds for aggregation by mirror descent with averaging},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b1300291698eadedb559786c809cc592-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b139aeda,
 abstract = {We consider regularized least-squares (RLS) with a Gaussian kernel. We prove that if we let the Gaussian bandwidth σ → ∞ while letting the regularization parameter λ → 0, the RLS solution tends to a polynomial whose order is controlled by the rielative rates of decay of 1/ σ1 and λ: if λ = σ-(2k+1), then, as σ → ∞, the RLS solution tends to the kth order polynomial with minimal empirical error. We illustrate the result with an example.},
 author = {Lippert, Ross and Rifkin, Ryan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b139aeda1c2914e3b579aafd3ceeb1bd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b139aeda1c2914e3b579aafd3ceeb1bd-Metadata.json},
 openalex = {W2138831162},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b139aeda1c2914e3b579aafd3ceeb1bd-Paper.pdf},
 publisher = {MIT Press},
 title = {Asymptotics of Gaussian Regularized Least-Squares},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b139aeda1c2914e3b579aafd3ceeb1bd-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b19aa25f,
 abstract = {We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. This decomposition, based on a recursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores. By using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n + e), where n is the size of the network, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. We show how the proposed visualization tool allows to find specific structural fingerprints of networks.},
 author = {Alvarez-hamelin, J. and Dall\textquotesingle asta, Luca and Barrat, Alain and Vespignani, Alessandro},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b19aa25ff58940d974234b48391b9549-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b19aa25ff58940d974234b48391b9549-Metadata.json},
 openalex = {W2118136127},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b19aa25ff58940d974234b48391b9549-Paper.pdf},
 publisher = {MIT Press},
 title = {Large scale networks fingerprinting and visualization using the k-core decomposition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b19aa25ff58940d974234b48391b9549-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b2ea5e97,
 abstract = {Experimental data indicate that norepinephrine is critically involved in aspects of vigilance and attention. Previously, we considered the function of this neuromodulatory system on a time scale of minutes and longer, and suggested that it signals global uncertainty arising from gross changes in environmental contingencies. However, norepinephrine is also known to be activated phasically by familiar stimuli in well-learned tasks. Here, we extend our uncertainty-based treatment of norepinephrine to this phasic mode, proposing that it is involved in the detection and reaction to state uncertainty within a task. This role of norepinephrine can be understood through the metaphor of neural interrupts.},
 author = {Dayan, Peter and Yu, Angela J.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b2ea5e977c5fc1ccfa74171a9723dd61-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b2ea5e977c5fc1ccfa74171a9723dd61-Metadata.json},
 openalex = {W2109696716},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b2ea5e977c5fc1ccfa74171a9723dd61-Paper.pdf},
 publisher = {MIT Press},
 title = {Norepinephrine and Neural Interrupts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b2ea5e977c5fc1ccfa74171a9723dd61-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b31df16a,
 abstract = {We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge},
 author = {Roy, Benjamin and Moallemi, Ciamac C},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b31df16a88ce00fed951f24b46e08649-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b31df16a88ce00fed951f24b46e08649-Metadata.json},
 openalex = {W2295244470},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b31df16a88ce00fed951f24b46e08649-Paper.pdf},
 publisher = {MIT Press},
 title = {Consensus Propagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b31df16a88ce00fed951f24b46e08649-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b44afe91,
 abstract = {Hybrid integrated circuits, combining CMOS subsystem with nanowire crossbars and simple two-terminal nanodevices, promise to extend the exponential Moore-Law development of microelectronics into the sub-10-nm range. We are developing neuromorphic network (CrossNet) architectures for this future technology, in which neural cell bodies are implemented in CMOS, nanowires are used as axons and dendrites, while nanodevices (bistable latching switches) are used as elementary synapses. We have shown how CrossNets may be trained to perform pattern recovery and classification despite the limitations imposed by the CMOL hardware. Preliminary estimates have shown that CMOL CrossNets may be extremely dense (~107 cells per cm2) and operate approximately a million times faster than biological neural networks, at manageable power consumption. In Conclusion, we discuss in brief possible short-term and long-term applications of the emerging technology.},
 author = {Lee, Jung Hoon and Ma, Xiaolong and Likharev, Konstantin K.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b44afe91b8a427a6be2078cc89bd6f9b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b44afe91b8a427a6be2078cc89bd6f9b-Metadata.json},
 openalex = {W2115349191},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b44afe91b8a427a6be2078cc89bd6f9b-Paper.pdf},
 publisher = {MIT Press},
 title = {CMOL CrossNets: Possible Neuromorphic Nanoelectronic Circuits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b44afe91b8a427a6be2078cc89bd6f9b-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b4944963,
 abstract = {While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined.},
 author = {Sonnenburg, S\"{o}ren and R\"{a}tsch, Gunnar and Sch\"{a}fer, Christin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4944963b5c83d545c3d3022bcf03282-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4944963b5c83d545c3d3022bcf03282-Metadata.json},
 openalex = {W2154630456},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4944963b5c83d545c3d3022bcf03282-Paper.pdf},
 publisher = {MIT Press},
 title = {A General and Efficient Multiple Kernel Learning Algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b4944963b5c83d545c3d3022bcf03282-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b4a721cf,
 abstract = {This paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm. The technique is based on a probabilistic graphical model, which describes the data in terms of underlying evoked and interference sources, and explicitly models the stimulus evoked paradigm. A variational Bayesian EM algorithm infers the model from data, suppresses interference sources, and reconstructs the activity of separated individual brain sources. The new algorithm outperforms existing techniques on two real datasets, as well as on simulated data.},
 author = {Hild, Kenneth and Sekihara, Kensuke and Attias, Hagai and Nagarajan, Srikantan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4a721cfb62f5d19ec61575114d8a2d1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4a721cfb62f5d19ec61575114d8a2d1-Metadata.json},
 openalex = {W2128923036},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4a721cfb62f5d19ec61575114d8a2d1-Paper.pdf},
 publisher = {MIT Press},
 title = {Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b4a721cfb62f5d19ec61575114d8a2d1-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b4fd1d2c,
 abstract = {We show that linear generalizations of Rescorla-Wagner can perform Maximum Likelihood estimation of the parameters of all generative models for causal reasoning. Our approach involves augmenting variables to deal with conjunctions of causes, similar to the agumented model of Rescorla. Our results involve genericity assumptions on the distributions of causes. If these assumptions are violated, for example for the Cheng causal power theory, then we show that a linear Rescorla-Wagner can estimate the parameters of the model up to a nonlinear transformtion. Moreover, a nonlinear Rescorla-Wagner is able to estimate the parameters directly to within arbitrary accuracy. Previous results can be used to determine convergence and to estimate convergence rates.},
 author = {Yuille, Alan L},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4fd1d2cb085390fbbadae65e07876a7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4fd1d2cb085390fbbadae65e07876a7-Metadata.json},
 openalex = {W2160324026},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4fd1d2cb085390fbbadae65e07876a7-Paper.pdf},
 publisher = {MIT Press},
 title = {Augmented Rescorla-Wagner and Maximum Likelihood Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b4fd1d2cb085390fbbadae65e07876a7-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b5b03f06,
 abstract = {In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are wrapper techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a filter method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classification problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efficiency of our algorithm.},
 author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b5b03f06271f8917685d14cea7c6c50a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b5b03f06271f8917685d14cea7c6c50a-Metadata.json},
 openalex = {W2149620660},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b5b03f06271f8917685d14cea7c6c50a-Paper.pdf},
 publisher = {MIT Press},
 title = {Laplacian Score for Feature Selection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b5b03f06271f8917685d14cea7c6c50a-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b6617980,
 abstract = {The Variational Bayesian framework has been widely used to approximate the Bayesian learning. In various applications, it has provided computational tractability and good generalization performance. In this paper, we discuss the Variational Bayesian learning of the mixture of exponential families and provide some additional theoretical support by deriving the asymptotic form of the stochastic complexity. The stochastic complexity, which corresponds to the minimum free energy and a lower bound of the marginal likelihood, is a key quantity for model selection. It also enables us to discuss the effect of hyperparameters and the accuracy of the Variational Bayesian approach as an approximation of the true Bayesian learning.},
 author = {Watanabe, Kazuho and Watanabe, Sumio},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b6617980ce90f637e68c3ebe8b9be745-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b6617980ce90f637e68c3ebe8b9be745-Metadata.json},
 openalex = {W2099544373},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf},
 publisher = {MIT Press},
 title = {Variational Bayesian Stochastic Complexity of Mixture Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b6617980ce90f637e68c3ebe8b9be745-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_b8b9c74a,
 abstract = {We present an improvement to the DP-SLAM algorithm for simultaneous localization and mapping (SLAM) that maintains multiple hypotheses about densely populated maps (one full map per particle in a particle filter) in time that is linear in all significant algorithm parameters and takes constant (amortized) time per iteration. This means that the asymptotic complexity of the algorithm is no greater than that of a pure localization algorithm using a single map and the same number of particles. We also present a hierarchical extension of DP-SLAM that uses a two level particle filter which models drift in the particle filtering process itself. The hierarchical approach enables recovery from the inevitable drift that results from using a finite number of particles in a particle filter and permits the use of DP-SLAM in more challenging domains, while maintaining linear time asymptotic complexity.},
 author = {Eliazar, Austin I. and Parr, Ronald},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Metadata.json},
 openalex = {W2111774168},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf},
 publisher = {MIT Press},
 title = {Hierarchical Linear/Constant Time SLAM Using Particle Filters for Dense Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/b8b9c74ac526fffbeb2d39ab038d1cd7-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_bc4e356f,
 abstract = {We propose a model by which dopamine (DA) and norepinepherine (NE) combine to alternate behavior between relatively exploratory and exploitative modes. The model is developed for a target detection task for which there is extant single neuron recording data available from locus coeruleus (LC) NE neurons. An exploration-exploitation trade-off is elicited by regularly switching which of the two stimuli are rewarded. DA functions within the model to change synaptic weights according to a reinforcement learning algorithm. Exploration is mediated by the state of LC firing, with higher tonic and lower phasic activity producing greater response variability. The opposite state of LC function, with lower baseline firing rate and greater phasic responses, favors exploitative behavior. Changes in LC firing mode result from combined measures of response conflict and reward rate, where response conflict is monitored using models of anterior cingulate cortex (ACC). Increased long-term response conflict and decreased reward rate, which occurs following reward contingency switch, favors the higher tonic state of LC function and NE release. This increases exploration, and facilitates discovery of the new target.},
 author = {McClure, Samuel M. and Gilzenrat, Mark S. and Cohen, Jonathan D},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bc4e356fee1972242c8f7eabf4dff517-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bc4e356fee1972242c8f7eabf4dff517-Metadata.json},
 openalex = {W2138698304},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bc4e356fee1972242c8f7eabf4dff517-Paper.pdf},
 publisher = {MIT Press},
 title = {An exploration-exploitation model based on norepinepherine and dopamine activity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/bc4e356fee1972242c8f7eabf4dff517-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_bd135462,
 abstract = {We describe a hierarchy of motif-based kernels for multiple alignments of biological sequences, particularly suitable to process regulatory regions of genes. The kernels incorporate progressively more information, with the most complex kernel accounting for a multiple alignment of orthologous regions, the phylogenetic tree relating the species, and the prior knowledge that relevant sequence patterns occur in conserved motif blocks. These kernels can be used in the presence of a library of known transcription factor binding sites, or de novo by iterating over all k-mers of a given length. In the latter mode, a discriminative classifier built from such a kernel not only recognizes a given class of promoter regions, but as a side effect simultaneously identifies a collection of relevant, discriminative sequence motifs. We demonstrate the utility of the motif-based multiple alignment kernels by using a collection of aligned promoter regions from five yeast species to recognize classes of cell-cycle regulated genes. Supplementary data is available at http://noble.gs.washington.edu/proj/pkernel.},
 author = {Vert, Jean-philippe and Thurman, Robert and Noble, William},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bd1354624fbae3b2149878941c60df99-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bd1354624fbae3b2149878941c60df99-Metadata.json},
 openalex = {W2127952290},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bd1354624fbae3b2149878941c60df99-Paper.pdf},
 publisher = {MIT Press},
 title = {Kernels for gene regulatory regions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/bd1354624fbae3b2149878941c60df99-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_bf2fb7d1,
 abstract = {Linear text classification algorithms work by computing an inner product between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parameters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classification over the last few decades has consisted of manual efforts to identify better parameter functions. In this paper, we propose an algorithm for automatically learning this function from related classification problems. The parameter function found by our algorithm then defines a new learning algorithm for text classification, which we can apply to novel classification tasks. We find that our learned classifier outperforms existing methods on a variety of multiclass text classification tasks.},
 author = {Do, Chuong B. and Ng, Andrew Y.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bf2fb7d1825a1df3ca308ad0bf48591e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bf2fb7d1825a1df3ca308ad0bf48591e-Metadata.json},
 openalex = {W2113765418},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bf2fb7d1825a1df3ca308ad0bf48591e-Paper.pdf},
 publisher = {MIT Press},
 title = {Transfer learning for text classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/bf2fb7d1825a1df3ca308ad0bf48591e-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_c0f971d8,
 abstract = {The Perceptron algorithm, despite its simplicity, often performs well on online classification tasks. The Perceptron becomes especially effective when it is used in conjunction with kernels. However, a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis, which may grow unboundedly. In this paper we present and analyze the Forgetron algorithm for kernel-based online learning on a fixed memory budget. To our knowledge, this is the first online learning algorithm which, on one hand, maintains a strict limit on the number of examples it stores while, on the other hand, entertains a relative mistake bound. In addition to the formal results, we also present experiments with real datasets which underscore the merits of our approach.},
 author = {Dekel, Ofer and Shalev-shwartz, Shai and Singer, Yoram},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c0f971d8cd24364f2029fcb9ac7b71f5-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c0f971d8cd24364f2029fcb9ac7b71f5-Metadata.json},
 openalex = {W2103736488},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c0f971d8cd24364f2029fcb9ac7b71f5-Paper.pdf},
 publisher = {MIT Press},
 title = {The Forgetron: A Kernel-Based Perceptron on a Fixed Budget},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/c0f971d8cd24364f2029fcb9ac7b71f5-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_c3008b2c,
 abstract = {Predictive state representations (PSRs) are a method of modeling dynamical systems using only observable data, such as actions and observations, to describe their model. PSRs use predictions about the outcome of future tests to summarize the system state. The best existing techniques for discovery and learning of PSRs use a Monte Carlo approach to explicitly estimate these outcome probabilities. In this paper, we present a new algorithm for discovery and learning of PSRs that uses a gradient descent approach to compute the predictions for the current state. The algorithm takes advantage of the large amount of structure inherent in a valid prediction matrix to constrain its predictions. Furthermore, the algorithm can be used online by an agent to constantly improve its prediction quality; something that current state of the art discovery and learning algorithms are unable to do. We give empirical results to show that our constrained gradient algorithm is able to discover core tests using very small amounts of data, and with larger amounts of data can compute accurate predictions of the system dynamics.},
 author = {Mccracken, Peter and Bowling, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c3008b2c6f5370b744850a98a95b73ad-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c3008b2c6f5370b744850a98a95b73ad-Metadata.json},
 openalex = {W2163126463},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c3008b2c6f5370b744850a98a95b73ad-Paper.pdf},
 publisher = {MIT Press},
 title = {Online Discovery and Learning of Predictive State Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/c3008b2c6f5370b744850a98a95b73ad-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_c429429b,
 abstract = {There have been many graph-based approaches for semi-supervised classification. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, transformation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semi-supervised classification. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classification as an inference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classification. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classifier to classify new points. Tests on synthetic and real datasets show cases where there are significant improvements in performance over the existing approaches.},
 author = {Kapoor, Ashish and Ahn, Hyungil and Qi, Yuan and Picard, Rosalind},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c429429bf1f2af051f2021dc92a8ebea-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c429429bf1f2af051f2021dc92a8ebea-Metadata.json},
 openalex = {W2106668826},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c429429bf1f2af051f2021dc92a8ebea-Paper.pdf},
 publisher = {MIT Press},
 title = {Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/c429429bf1f2af051f2021dc92a8ebea-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_c61f571d,
 abstract = {Fusing multiple information sources can yield significant benefits to successfully accomplish learning tasks. Many studies have focussed on fusing information in supervised learning contexts. We present an approach to utilize multiple information sources in the form of similarity data for unsupervised learning. Based on similarity information, the clustering task is phrased as a non-negative matrix factorization problem of a mixture of similarity measurements. The tradeoff between the informativeness of data sources and the sparseness of their mixture is controlled by an entropy-based weighting mechanism. For the purpose of model selection, a stability-based approach is employed to ensure the selection of the most self-consistent hypothesis. The experiments demonstrate the performance of the method on toy as well as real world data sets.},
 author = {Lange, Tilman and Buhmann, Joachim},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c61f571dbd2fb949d3fe5ae1608dd48b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c61f571dbd2fb949d3fe5ae1608dd48b-Metadata.json},
 openalex = {W2106372467},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf},
 publisher = {MIT Press},
 title = {Fusion of Similarity Data in Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/c61f571dbd2fb949d3fe5ae1608dd48b-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_c68c9c82,
 abstract = {We extend a previously developed Bayesian framework for perception to account for sensory adaptation. We first note that the perceptual effects of adaptation seems inconsistent with an adjustment of the internally represented prior distribution. Instead, we postulate that adaptation increases the signal-to-noise ratio of the measurements by adapting the operational range of the measurement stage to the input range. We show that this changes the likelihood function in such a way that the Bayesian estimator model can account for reported perceptual behavior. In particular, we compare the model's predictions to human motion discrimination data and demonstrate that the model accounts for the commonly observed perceptual adaptation effects of repulsion and enhanced discriminability.},
 author = {Stocker, Alan A and Simoncelli, Eero},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c68c9c8258ea7d85472dd6fd0015f047-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c68c9c8258ea7d85472dd6fd0015f047-Metadata.json},
 openalex = {W2128188453},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c68c9c8258ea7d85472dd6fd0015f047-Paper.pdf},
 publisher = {MIT Press},
 title = {Sensory Adaptation within a Bayesian Framework for Perception},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/c68c9c8258ea7d85472dd6fd0015f047-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_c7558e9d,
 abstract = {We extend radial basis function (RBF) networks to the scenario in which multiple correlated tasks are learned simultaneously, and present the corresponding learning algorithms. We develop the algorithms for learning the network structure, in either a supervised or unsupervised manner. Training data may also be actively selected to improve the network's generalization to test data. Experimental results based on real data demonstrate the advantage of the proposed algorithms and support our conclusions.},
 author = {Liao, Xuejun and Carin, Lawrence},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c7558e9d1f956b016d1fdba7ea132378-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c7558e9d1f956b016d1fdba7ea132378-Metadata.json},
 openalex = {W2160727143},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c7558e9d1f956b016d1fdba7ea132378-Paper.pdf},
 publisher = {MIT Press},
 title = {Radial Basis Function Network for Multi-task Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/c7558e9d1f956b016d1fdba7ea132378-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_c9efe5f2,
 abstract = {We measure the ability of human observers to predict the next datum in a sequence that is generated by a simple statistical process undergoing change at random points in time. Accurate performance in this task requires the identification of changepoints. We assess individual differences between observers both empirically, and using two kinds of models: a Bayesian approach for change detection and a family of cognitively plausible fast and frugal models. Some individuals detect too many changes and hence perform sub-optimally due to excess variability. Other individuals do not detect enough changes, and perform sub-optimally because they fail to notice short-term temporal trends.},
 author = {Steyvers, Mark and Brown, Scott},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c9efe5f26cd17ba6216bbe2a7d26d490-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c9efe5f26cd17ba6216bbe2a7d26d490-Metadata.json},
 openalex = {W2151632898},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf},
 publisher = {MIT Press},
 title = {Prediction and Change Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ccd45007,
 abstract = {We introduce Gaussian process dynamical models (GPDM) for nonlinear time series analysis, with applications to learning models of human pose and motion from high-dimensionalmotion capture data. A GPDM is a latent variable model. It comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, using Gaussian process priors for both the dynamics and the observation mappings. This results in a non-parametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach, and compare four learning algorithms on human motion capture data in which each pose is 50-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces.},
 author = {Wang, Jack and Hertzmann, Aaron and Fleet, David J},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ccd45007df44dd0f12098f486e7e8a0f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ccd45007df44dd0f12098f486e7e8a0f-Metadata.json},
 openalex = {W2124609748},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ccd45007df44dd0f12098f486e7e8a0f-Paper.pdf},
 publisher = {MIT Press},
 title = {Gaussian Process Dynamical Models for Human Motion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ccd45007df44dd0f12098f486e7e8a0f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d0010a6f,
 abstract = {We present a greedy method for simultaneously performing local bandwidth selection and variable selection in nonparametric regression. The method starts with a local linear estimator with large bandwidths, and incrementally decreases the bandwidth of variables for which the gradient of the estimator with respect to bandwidth is large. The method--called rodeo (regularization of derivative expectation operator)--conducts a sequence of hypothesis tests to threshold derivatives, and is easy to implement. Under certain assumptions on the regression function and sampling density, it is shown that the rodeo applied to local linear smoothing avoids the curse of dimensionality, achieving near optimal minimax rates of convergence in the number of relevant variables, as if these variables were isolated in advance.},
 author = {Wasserman, Larry and Lafferty, John},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d0010a6f34908640a4a6da2389772a78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d0010a6f34908640a4a6da2389772a78-Metadata.json},
 openalex = {W2950953666},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d0010a6f34908640a4a6da2389772a78-Paper.pdf},
 publisher = {MIT Press},
 title = {Rodeo: Sparse Nonparametric Regression in High Dimensions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d0010a6f34908640a4a6da2389772a78-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d0bb8259,
 abstract = {We investigate the learning of the appearance of an object from a single image of it. Instead of using a large number of pictures of the object to recognize, we use a labeled reference database of pictures of other objects to learn invariance to noise and variations in pose and illumination. This acquired knowledge is then used to predict if two pictures of new objects, which do not appear on the training pictures, actually display the same object.

We propose a generic scheme called chopping to address this task. It relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object. Those splits are extended to the complete image space with a simple learning algorithm. Given two images, the responses of the split predictors are combined with a Bayesian rule into a posterior probability of similarity.

Experiments with the COIL-100 database and with a database of 150 degraded LATEX symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the similarity.},
 author = {Fleuret, Francois and Blanchard, Gilles},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d0bb8259d8fe3c7df4554dab9d7da3c9-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d0bb8259d8fe3c7df4554dab9d7da3c9-Metadata.json},
 openalex = {W2153747924},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d0bb8259d8fe3c7df4554dab9d7da3c9-Paper.pdf},
 publisher = {MIT Press},
 title = {Pattern Recognition from One Example by Chopping},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d0bb8259d8fe3c7df4554dab9d7da3c9-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d210cf37,
 abstract = {This paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials, as well as for estimating the notoriously difficult partition function of the graph. The algorithm fits into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of intermediate distributions which get closer to the desired one. While the idea of using “tempered” proposals is known, we construct a novel sequence of target distributions where, rather than dropping a global temperature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs.},
 author = {Hamze, Firas and de Freitas, Nando},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d210cf373cf002a04ec72ee395f66306-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d210cf373cf002a04ec72ee395f66306-Metadata.json},
 openalex = {W2113186741},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d210cf373cf002a04ec72ee395f66306-Paper.pdf},
 publisher = {MIT Press},
 title = {Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d210cf373cf002a04ec72ee395f66306-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d3aeec87,
 abstract = {Separation of music signals is an interesting but difficult problem. It is helpful for many other music researches such as audio content analysis. In this paper, a new music signal separation method is proposed, which is based on harmonic structure modeling. The main idea of harmonic structure modeling is that the harmonic structure of a music signal is stable, so a music signal can be represented by a harmonic structure model. Accordingly, a corresponding separation algorithm is proposed. The main idea is to learn a harmonic structure model for each music signal in the mixture, and then separate signals by using these models to distinguish harmonic structures of different signals. Experimental results show that the algorithm can separate signals and obtain not only a very high Signal-to-Noise Ratio (SNR) but also a rather good subjective audio quality.},
 author = {Zhang, Yun-gang and Zhang, Chang-shui},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d3aeec875c479e55d1cdeea161842ec6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d3aeec875c479e55d1cdeea161842ec6-Metadata.json},
 openalex = {W2171153836},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d3aeec875c479e55d1cdeea161842ec6-Paper.pdf},
 publisher = {MIT Press},
 title = {Separation of Music Signals by Harmonic Structure Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d3aeec875c479e55d1cdeea161842ec6-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d3d80b65,
 abstract = {Given a probability measure P and a reference measure μ, one is often interested in the minimum μ-measure set with P-measure at least α. Minimum volume sets of this type summarize the regions of greatest probability mass of P, and are useful for detecting anomalies and constructing confidence regions. This paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to P. Other than these samples, no other information is available regarding P, but the reference measure μ is assumed to be known. We introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classification. As in classification, we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn. Thus we obtain finite sample size performance bounds in terms of VC dimension and related quantities. We also demonstrate strong universal consistency and an oracle inequality. Estimators based on histograms and dyadic partitions illustrate the proposed rules.},
 author = {Scott, Clayton and Nowak, Robert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d3d80b656929a5bc0fa34381bf42fbdd-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d3d80b656929a5bc0fa34381bf42fbdd-Metadata.json},
 openalex = {W2139155312},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d3d80b656929a5bc0fa34381bf42fbdd-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Minimum Volume Sets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d3d80b656929a5bc0fa34381bf42fbdd-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d4784467,
 abstract = {Sparse PCA seeks approximate sparse eigenvectors whose projections capture the maximal variance of data. As a cardinality-constrained and non-convex optimization problem, it is NP-hard and is encountered in a wide range of applied fields, from bio-informatics to finance. Recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint. In contrast, we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search. Moreover, the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method. The resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive Monte Carlo evaluation trials.},
 author = {Moghaddam, Baback and Weiss, Yair and Avidan, Shai},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d47844673f2db74d78da8687d794523d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d47844673f2db74d78da8687d794523d-Metadata.json},
 openalex = {W2109675189},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d47844673f2db74d78da8687d794523d-Paper.pdf},
 publisher = {MIT Press},
 title = {Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d47844673f2db74d78da8687d794523d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d5369744,
 abstract = {We propose a fast manifold learning algorithm based on the methodology of domain decomposition. Starting with the set of sample points partitioned into two subdomains, we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain. We provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory. Numerical examples are given to illustrate the efficiency and effectiveness of the proposed methods.},
 author = {Zhang, Zhenyue and Zha, Hongyuan},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d53697441ef12a45422f6660202f9840-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d53697441ef12a45422f6660202f9840-Metadata.json},
 openalex = {W2158930984},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d53697441ef12a45422f6660202f9840-Paper.pdf},
 publisher = {MIT Press},
 title = {A Domain Decomposition Method for Fast Manifold Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d53697441ef12a45422f6660202f9840-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d58e2f07,
 abstract = {Nonnegative matrix approximation (NNMA) is a recent technique for dimensionality reduction and data analysis that yields a parts based, sparse nonnegative representation for nonnegative input data. NNMA has found a wide variety of applications, including text analysis, document clustering, face/image recognition, language modeling, speech processing and many others. Despite these numerous applications, the algorithmic development for computing the NNMA factors has been relatively deficient. This paper makes algorithmic progress by modeling and solving (using multiplicative updates) new generalized NNMA problems that minimize Bregman divergences between the input matrix and its low-rank approximation. The multiplicative update formulae in the pioneering work by Lee and Seung [11] arise as a special case of our algorithms. In addition, the paper shows how to use penalty functions for incorporating constraints other than nonnegativity into the problem. Further, some interesting extensions to the use of link functions for modeling nonlinear relationships are also discussed.},
 author = {Sra, Suvrit and Dhillon, Inderjit},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d58e2f077670f4de9cd7963c857f2534-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d58e2f077670f4de9cd7963c857f2534-Metadata.json},
 openalex = {W2168446235},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d58e2f077670f4de9cd7963c857f2534-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalized Nonnegative Matrix Approximations with Bregman Divergences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d58e2f077670f4de9cd7963c857f2534-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d71fa38b,
 abstract = {An increasing number of projects in neuroscience requires the statistical analysis of high dimensional data sets, as, for instance, in predicting behavior from neural firing or in operating artificial devices from brain recordings in brain-machine interfaces. Linear analysis techniques remain prevalent in such cases, but classical linear regression approaches are often numerically too fragile in high dimensions. In this paper, we address the question of whether EMG data collected from arm movements of monkeys can be faithfully reconstructed with linear approaches from neural activity in primary motor cortex (M1). To achieve robust data analysis, we develop a full Bayesian approach to linear regression that automatically detects and excludes irrelevant features in the data, regularizing against overfitting. In comparison with ordinary least squares, stepwise regression, partial least squares, LASSO regression and a brute force combinatorial search for the most predictive input features in the data, we demonstrate that the new Bayesian method offers a superior mixture of characteristics in terms of regularization against overfitting, computational efficiency and ease of use, demonstrating its potential as a drop-in replacement for other linear regression techniques. As neuroscientific results, our analyses demonstrate that EMG data can be well predicted from M1 neurons, further opening the path for possible real-time interfaces between brains and machines.},
 author = {Ting, Jo-anne and D\textquotesingle souza, Aaron and Yamamoto, Kenji and Yoshioka, Toshinori and Hoffman, Donna and Kakei, Shinji and Sergio, Lauren and Kalaska, John and Kawato, Mitsuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d71fa38b648d86602d14ac610f2e6194-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d71fa38b648d86602d14ac610f2e6194-Metadata.json},
 openalex = {W2102616768},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d71fa38b648d86602d14ac610f2e6194-Paper.pdf},
 publisher = {MIT Press},
 title = {Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d71fa38b648d86602d14ac610f2e6194-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_d8e1344e,
 abstract = {Given a redundant dictionary of basis vectors (or atoms), our goal is to find maximally sparse representations of signals. Previously, we have argued that a sparse Bayesian learning (SBL) framework is particularly well-suited for this task, showing that it has far fewer local minima than other Bayesian-inspired strategies. In this paper, we provide further evidence for this claim by proving a restricted equivalence condition, based on the distribution of the nonzero generating model weights, whereby the SBL solution will equal the maximally sparse representation. We also prove that if these nonzero weights are drawn from an approximate Jeffreys prior, then with probability approaching one, our equivalence condition is satisfied. Finally, we motivate the worst-case scenario for SBL and demonstrate that it is still better than the most widely used sparse representation algorithms. These include Basis Pursuit (BP), which is based on a convex relaxation of the l0 (quasi)-norm, and Orthogonal Matching Pursuit (OMP), a simple greedy strategy that iteratively selects basis vectors most aligned with the current residual.},
 author = {Rao, Bhaskar and Wipf, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Metadata.json},
 openalex = {W2124855287},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Paper.pdf},
 publisher = {MIT Press},
 title = {Comparing the Effects of Different Weight Distributions on Finding Sparse Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_db5cea26,
 abstract = {Humans are extremely adept at learning new skills by imitating the actions of others. A progression of imitative abilities has been observed in children, ranging from imitation of simple body movements to goal-based imitation based on inferring intent. In this paper, we show that the problem of goal-based imitation can be formulated as one of inferring goals and selecting actions using a learned probabilistic graphical model of the environment. We first describe algorithms for planning actions to achieve a goal state using probabilistic inference. We then describe how planning can be used to bootstrap the learning of goal-dependent policies by utilizing feedback from the environment. The resulting graphical model is then shown to be powerful enough to allow goal-based imitation. Using a simple maze navigation task, we illustrate how an agent can infer the goals of an observed teacher and imitate the teacher even when the goals are uncertain and the demonstration is incomplete.},
 author = {Verma, Deepak and Rao, Rajesh PN},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Metadata.json},
 openalex = {W2097668489},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Paper.pdf},
 publisher = {MIT Press},
 title = {Goal-Based Imitation as Probabilistic Inference over Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/db5cea26ca37aa09e5365f3e7f5dd9eb-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_dba31bb5,
 abstract = {Probabilistic temporal planning attempts to find good policies for acting in domains with concurrent durative tasks, multiple uncertain outcomes, and limited resources. These domains are typically modelled as Markov decision problems and solved using dynamic programming methods. This paper demonstrates the application of reinforcement learning — in the form of a policy-gradient method — to these domains. Our emphasis is large domains that are infeasible for dynamic programming. Our approach is to construct simple policies, or agents, for each planning task. The result is a general probabilistic temporal planner, named the Factored Policy-Gradient Planner (FPG-Planner), which can handle hundreds of tasks, optimising for probability of success, duration, and resource use.},
 author = {Aberdeen, Douglas},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dba31bb5c75992690f20c2d3b370ec7c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dba31bb5c75992690f20c2d3b370ec7c-Metadata.json},
 openalex = {W2095996877},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dba31bb5c75992690f20c2d3b370ec7c-Paper.pdf},
 publisher = {MIT Press},
 title = {Policy-Gradient Methods for Planning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/dba31bb5c75992690f20c2d3b370ec7c-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_dc20d121,
 abstract = {The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics. An efficient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results.},
 author = {Wong, K. Y. Michael and Saad, David and Gao, Zhuo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dc20d1211f3e7a99d775b26052e0163e-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dc20d1211f3e7a99d775b26052e0163e-Metadata.json},
 openalex = {W2106758131},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dc20d1211f3e7a99d775b26052e0163e-Paper.pdf},
 publisher = {MIT Press},
 title = {Message passing for task redistribution on sparse graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/dc20d1211f3e7a99d775b26052e0163e-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_dfb84a11,
 abstract = {Diffusion Tensor Magnetic Resonance Imaging (DT-MRI) is a non invasive method for brain neuronal fibers delineation. Here we show a modification for DT-MRI that allows delineation of neuronal fibers which are infiltrated by edema. We use the Muliple Tensor Variational (MTV) framework which replaces the diffusion model of DT-MRI with a multiple component model and fits it to the signal attenuation with a variational regularization mechanism. In order to reduce free water contamination we estimate the free water compartment volume fraction in each voxel, remove it, and then calculate the anisotropy of the remaining compartment. The variational framework was applied on data collected with conventional clinical parameters, containing only six diffusion directions. By using the variational framework we were able to overcome the highly ill posed fitting. The results show that we were able to find fibers that were not found by DT-MRI.},
 author = {Pasternak, Ofer and Intrator, Nathan and Sochen, Nir and Assaf, Yaniv},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dfb84a11f431c62436cfb760e30a34fe-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dfb84a11f431c62436cfb760e30a34fe-Metadata.json},
 openalex = {W2130026395},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dfb84a11f431c62436cfb760e30a34fe-Paper.pdf},
 publisher = {MIT Press},
 title = {Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/dfb84a11f431c62436cfb760e30a34fe-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e07bceab,
 abstract = {We present a computational model of human eye movements in an object class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using AdaBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated fixations, culminating with the acquisition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex non-target objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of fixations, cumulative probability of fixating the target, and scanpath distance.},
 author = {Zhang, Wei and Yang, Hyejin and Samaras, Dimitris and Zelinsky, Gregory},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e07bceab69529b0f0b43625953fbf2a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e07bceab69529b0f0b43625953fbf2a0-Metadata.json},
 openalex = {W2099179901},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e07bceab69529b0f0b43625953fbf2a0-Paper.pdf},
 publisher = {MIT Press},
 title = {A Computational Model of Eye Movements during Object Class Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e07bceab69529b0f0b43625953fbf2a0-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e27a9497,
 abstract = {We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model represents the configuration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The complexity of the model in the number of features is low, meaning our model is much more efficient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization improvements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization.},
 author = {Loeff, Nicolas and Arora, Himanshu and Sorokin, Alexander and Forsyth, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e27a949795bbe863f31c3b79a2686770-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e27a949795bbe863f31c3b79a2686770-Metadata.json},
 openalex = {W2117648589},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e27a949795bbe863f31c3b79a2686770-Paper.pdf},
 publisher = {MIT Press},
 title = {Efficient Unsupervised Learning for Localization and Detection in Object Categories},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e27a949795bbe863f31c3b79a2686770-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e2c61965,
 abstract = {A general analysis of the limiting distribution of neural network functions is performed, with emphasis on non-Gaussian limits. We show that with i.i.d. symmetric stable output weights, and more generally with weights distributed from the normal domain of attraction of a stable variable, that the neural functions converge in distribution to stable processes. Conditions are also investigated under which Gaussian limits do occur when the weights are independent but not identically distributed. Some particularly tractable classes of stable distributions are examined, and the possibility of learning with such processes.},
 author = {Der, Ricky and Lee, Daniel},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e2c61965b5e23b47b77d7c51611b6d7f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e2c61965b5e23b47b77d7c51611b6d7f-Metadata.json},
 openalex = {W2160234213},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e2c61965b5e23b47b77d7c51611b6d7f-Paper.pdf},
 publisher = {MIT Press},
 title = {Beyond Gaussian Processes: On the Distributions of Infinite Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e2c61965b5e23b47b77d7c51611b6d7f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e2f92479,
 abstract = {We present a family of approximation techniques for probabilistic graphical models, based on the use of graphical preconditioners developed in the scientific computing literature. Our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models, using non-iterative procedures that have low time complexity. As in mean field approaches, the approximations are built upon tractable subgraphs; however, we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well-studied linear systems problem of obtaining a good matrix preconditioner. Experiments are presented that compare the new approximation schemes to variational methods.},
 author = {Lafferty, John and Ravikumar, Pradeep},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e2f9247929b404b2fe98ba6f32301e3b-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e2f9247929b404b2fe98ba6f32301e3b-Metadata.json},
 openalex = {W2135978086},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e2f9247929b404b2fe98ba6f32301e3b-Paper.pdf},
 publisher = {MIT Press},
 title = {Preconditioner Approximations for Probabilistic Graphical Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e2f9247929b404b2fe98ba6f32301e3b-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e465ae46,
 abstract = {We present a simple and scalable algorithm for large-margin estimation of structured models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem and apply the extragradient method, yielding an algorithm with linear convergence using simple gradient and projection calculations. The projection step can be solved using combinatorial algorithms for min-cost quadratic flow. This makes the approach an efficient alternative to formulations based on reductions to a quadratic program (QP). We present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.},
 author = {Taskar, Ben and Lacoste-Julien, Simon and Jordan, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e465ae46b07058f4ab5e96b98f101756-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e465ae46b07058f4ab5e96b98f101756-Metadata.json},
 openalex = {W2161002641},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e465ae46b07058f4ab5e96b98f101756-Paper.pdf},
 publisher = {MIT Press},
 title = {Structured Prediction via the Extragradient Method},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e465ae46b07058f4ab5e96b98f101756-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e6af401c,
 abstract = {The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement. It has also been shown to exist in eye-motion, locomotion and was even demonstrated in motion perception and prediction. This ubiquity has fostered various attempts to uncover the origins of this empirical relationship. In these it was generally attributed either to smoothness in hand- or joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion.

We show here that white Gaussian noise also obeys this power-law. Analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise. Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing. These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise. Our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system.},
 author = {Maoz, Uri and Portugaly, Elon and Flash, Tamar and Weiss, Yair},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e6af401c28c1790eaef7d55c92ab6ab6-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e6af401c28c1790eaef7d55c92ab6ab6-Metadata.json},
 openalex = {W2137085769},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e6af401c28c1790eaef7d55c92ab6ab6-Paper.pdf},
 publisher = {MIT Press},
 title = {Noise and the two-thirds power Law},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e6af401c28c1790eaef7d55c92ab6ab6-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e6cbc650,
 abstract = {Lasso regression tends to assign zero weights to most irrelevant or redundant features, and hence is a promising technique for feature selection. Its limitation, however, is that it only offers solutions to linear models. Kernel machines with feature scaling techniques have been studied for feature selection with non-linear models. However, such approaches require to solve hard non-convex optimization problems. This paper proposes a new approach named the Feature Vector Machine (FVM). It reformulates the standard Lasso regression into a form isomorphic to SVM, and this form can be easily extended for feature selection with non-linear models by introducing kernels defined on feature vectors. FVM generates sparse solutions in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machines. Our experiments with FVM on simulated data show encouraging results in identifying the small number of dominating features that are non-linearly correlated to the response, a task the standard Lasso fails to complete.},
 author = {Li, Fan and Yang, Yiming and Xing, Eric},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e6cbc650cd5798a05dfd0f51d14cde5c-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e6cbc650cd5798a05dfd0f51d14cde5c-Metadata.json},
 openalex = {W2121308062},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e6cbc650cd5798a05dfd0f51d14cde5c-Paper.pdf},
 publisher = {MIT Press},
 title = {From Lasso regression to Feature vector machine},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e6cbc650cd5798a05dfd0f51d14cde5c-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e833e042,
 abstract = {Many real-world classification problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classification of such structured variables. In this paper, we investigate structured classification in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points.},
 author = {Altun, Y. and McAllester, D. and Belkin, M.},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e833e042f509c996b1b25324d56659fb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e833e042f509c996b1b25324d56659fb-Metadata.json},
 openalex = {W2095758845},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e833e042f509c996b1b25324d56659fb-Paper.pdf},
 publisher = {MIT Press},
 title = {Maximum Margin Semi-Supervised Learning for Structured Variables},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e833e042f509c996b1b25324d56659fb-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e8f27796,
 abstract = {We argue that when objects are characterized by many attributes, clustering them on the basis of a relatively small random subset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove a finite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. The scheme is demonstrated for collaborative filtering of users with movies rating as attributes.},
 author = {Krupka, Eyal and Tishby, Naftali},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e8f2779682fd11fa2067beffc27a9192-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e8f2779682fd11fa2067beffc27a9192-Metadata.json},
 openalex = {W2107996537},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e8f2779682fd11fa2067beffc27a9192-Paper.pdf},
 publisher = {MIT Press},
 title = {Generalization in Clustering with Unobserved Features},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e8f2779682fd11fa2067beffc27a9192-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e9245170,
 abstract = {In this paper we consider the problem of finding sets of points that conform to a given underlying model from within a dense, noisy set of observations. This problem is motivated by the task of efficiently linking faint asteroid detections, but is applicable to a range of spatial queries. We survey current tree-based approaches, showing a trade-off exists between single tree and multiple tree algorithms. To this end, we present a new type of multiple tree algorithm that uses a variable number of trees to exploit the advantages of both approaches. We empirically show that this algorithm performs well using both simulated and astronomical data.},
 author = {Kubica, Jeremy and Masiero, Joseph and Jedicke, Robert and Connolly, Andrew and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e924517087669cf201ea91bd737a4ff4-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e924517087669cf201ea91bd737a4ff4-Metadata.json},
 openalex = {W2164633392},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e924517087669cf201ea91bd737a4ff4-Paper.pdf},
 publisher = {MIT Press},
 title = {Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e924517087669cf201ea91bd737a4ff4-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_e9874147,
 abstract = {Based on a large scale spiking neuron model of the input layers 4Cα and β of macaque, we identify neural mechanisms for the observed contrast dependent receptive field size of V1 cells. We observe a rich variety of mechanisms for the phenomenon and analyze them based on the relative gain of excitatory and inhibitory synaptic inputs. We observe an average growth in the spatial extent of excitation and inhibition for low contrast, as predicted from phenomenological models. However, contrary to phenomenological models, our simulation results suggest this is neither sufficient nor necessary to explain the phenomenon.},
 author = {Wielaard, Jim and Sajda, Paul},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e98741479a7b998f88b8f8c9f0b6b6f1-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e98741479a7b998f88b8f8c9f0b6b6f1-Metadata.json},
 openalex = {W2103131930},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e98741479a7b998f88b8f8c9f0b6b6f1-Paper.pdf},
 publisher = {MIT Press},
 title = {Neural mechanisms of contrast dependent receptive field size in V1},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/e98741479a7b998f88b8f8c9f0b6b6f1-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_eb9fc349,
 abstract = {Although non-parametric tests have already been proposed for that purpose, statistical significance tests for non-standard measures (different from the classification error) are less often used in the literature. This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions. More precisely, using a very large dataset to estimate the whole population, we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size. The main result is that providing big enough evaluation sets non-parametric tests are relatively reliable in all conditions.},
 author = {Keller, Mikaela and Bengio, Samy and Wong, Siew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/eb9fc349601c69352c859c1faa287874-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/eb9fc349601c69352c859c1faa287874-Metadata.json},
 openalex = {W2101995396},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/eb9fc349601c69352c859c1faa287874-Paper.pdf},
 publisher = {MIT Press},
 title = {Benchmarking Non-Parametric Statistical Tests},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/eb9fc349601c69352c859c1faa287874-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ec0bfd00,
 abstract = {We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identically distributed (i.i.d.) but come from empirical processes of stationary β-mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classifiers resulting from a regularization achieved by restricting the 1-norm of the base classifiers' weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter.},
 author = {Lozano, Aurelie C and Kulkarni, Sanjeev and Schapire, Robert E},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec0bfd000f253eff3acb1043e1c06979-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec0bfd000f253eff3acb1043e1c06979-Metadata.json},
 openalex = {W2104290726},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec0bfd000f253eff3acb1043e1c06979-Paper.pdf},
 publisher = {MIT Press},
 title = {Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ec0bfd000f253eff3acb1043e1c06979-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ec7f3466,
 abstract = {Recent neurophysiological evidence suggests the ability to interpret biological motion is facilitated by a neuronal mirror system which maps visual inputs to the pre-motor cortex. If the common architecture and circuitry of the cortices is taken to imply a common computation across multiple perceptual and cognitive modalities, this visual-motor interaction might be expected to have a unified computational basis. Two essential tasks underlying such visual-motor cooperation are shown here to be simply expressed and directly solved as transformation-discovery inverse problems: (a) discriminating and determining the pose of a primed 3D object in a real-world scene, and (b) interpreting the 3D configuration of an articulated kinematic object in an image. The recently developed map-seeking method provides a mathematically tractable, cortically-plausible solution to these and a variety of other inverse problems which can be posed as the discovery of a composition of transformations between two patterns. The method relies on an ordering property of superpositions and on decomposition of the transformation spaces inherent in the generating processes of the problem.},
 author = {Arathorn, David},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec7f346604f518906d35ef0492709f78-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec7f346604f518906d35ef0492709f78-Metadata.json},
 openalex = {W2152680207},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec7f346604f518906d35ef0492709f78-Paper.pdf},
 publisher = {MIT Press},
 title = {A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ec7f346604f518906d35ef0492709f78-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ec8b57b0,
 abstract = {This paper explores two aspects of social network modeling. First, we generalize a successful static model of relationships into a dynamic model that accounts for friendships drifting over time. Second, we show how to make it tractable to learn such models from data, even as the number of entities n gets large. The generalized model associates each entity with a point in p -dimensional Euclidean latent space. The points can move as time progresses but large moves in latent space are improbable. Observed links between entities are more likely if the entities are close in latent space. We show how to make such a model tractable (sub-quadratic in the number of entities) by the use of appropriate kernel functions for similarity in latent space; the use of low dimensional KD-trees; a new efficient dynamic adaptation of multidimensional scaling for a first pass of approximate projection of entities into latent space; and an efficient conjugate gradient update rule for non-linear local optimization in which amortized time per entity during an update is O (log n ). We use both synthetic and real-world data on up to 11,000 entities which indicate near-linear scaling in computation time and improved performance over four alternative approaches. We also illustrate the system operating on twelve years of NIPS co-authorship data.},
 author = {Sarkar, Purnamrita and Moore, Andrew},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec8b57b0be908301f5748fb04b0714c7-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec8b57b0be908301f5748fb04b0714c7-Metadata.json},
 openalex = {W2039750798},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec8b57b0be908301f5748fb04b0714c7-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamic social network analysis using latent space models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ec8b57b0be908301f5748fb04b0714c7-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ef0d17b3,
 abstract = {The network topology of neurons in the brain exhibits an abundance of feedback connections, but the computational function of these feedback connections is largely unknown. We present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical systems with fading memory. It implies that many such systems acquire through feedback universal computational capabilities for analog computing with a non-fading memory. In particular, we show that feedback enables such systems to process time-varying input streams in diverse ways according to rules that are implemented through internal states of the dynamical system. In contrast to previous attractor-based computational models for neural networks, these flexible internal states are high-dimensional attractors of the circuit dynamics, that still allow the circuit state to absorb new information from online input streams. In this way one arrives at novel models for working memory, integration of evidence, and reward expectation in cortical circuits. We show that they are applicable to circuits of conductance-based Hodgkin-Huxley (HH) neurons with high levels of noise that reflect experimental data on in-vivo conditions.},
 author = {Maass, Wolfgang and Joshi, Prashant and Sontag, Eduardo},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ef0d17b3bdb4ee2aa741ba28c7255c53-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ef0d17b3bdb4ee2aa741ba28c7255c53-Metadata.json},
 openalex = {W2105401417},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ef0d17b3bdb4ee2aa741ba28c7255c53-Paper.pdf},
 publisher = {MIT Press},
 title = {Principles of real-time computing with feedback applied to cortical microcircuit models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ef0d17b3bdb4ee2aa741ba28c7255c53-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_ef67f7c2,
 abstract = {Knowledge of a person’s location provides important context information for many applications, ranging from services such as E911 to personal guidance systems that help cognitively-impaired individuals move safely through their community. Location information is also extremely helpful for estimating a person’s high-level activities. In this talk we show how Bayesian filtering and conditional random fields can be applied to estimate the location and activity of a person using sensors such as GPS or WiFi. The techniques track a person on graph structures that represent a street map or a skeleton of the free space in a building. We also show how to learn a user’s significant places and daily movements through the community. Our models use multiple levels of abstraction so as to bridge the gap between raw GPS measurements and high level information such as a user’s mode of transportation, her current goal, and her significant places (e.g. home or work place). Finally, we will discuss recent work on using a multi-sensor board so as to better estimate a person’s activities.},
 author = {Liao, Lin and Fox, Dieter and Kautz, Henry},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ef67f7c2d86352c2c42e19d20f881f53-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ef67f7c2d86352c2c42e19d20f881f53-Metadata.json},
 openalex = {W1565064763},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ef67f7c2d86352c2c42e19d20f881f53-Paper.pdf},
 publisher = {MIT Press},
 title = {Location-Based Activity Recognition},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/ef67f7c2d86352c2c42e19d20f881f53-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_efd7e9ed,
 abstract = {There is a long-standing controversy on the site of the cerebellar motor learning. Different theories and experimental results suggest that either the cerebellar flocculus or the brainstem learns the task and stores the memory. With a dynamical system approach, we clarify the mechanism of transferring the memory generated in the flocculus to the brainstem and that of so-called savings phenomena. The brainstem learning must comply with a sort of Hebbian rule depending on Purkinje-cell activities. In contrast to earlier numerical models, our model is simple but it accommodates explanations and predictions of experimental situations as qualitative features of trajectories in the phase space of synaptic weights, without fine parameter tuning.},
 author = {Masuda, Naoki and Amari, Shun-ichi},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/efd7e9ed0e5e694ba6df444d84dfa37d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/efd7e9ed0e5e694ba6df444d84dfa37d-Metadata.json},
 openalex = {W2135494860},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/efd7e9ed0e5e694ba6df444d84dfa37d-Paper.pdf},
 publisher = {MIT Press},
 title = {Modeling Memory Transfer and Saving in Cerebellar Motor Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/efd7e9ed0e5e694ba6df444d84dfa37d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_f076073b,
 abstract = {We consider approximate value iteration with a parameterized approximator in which the state space is partitioned and the optimal cost-to-go function over each partition is approximated by a constant. We establish performance loss bounds for policies derived from approximations associated with fixed points. These bounds identify benefits to having projection weights equal to the invariant distribution of the resulting policy. Such projection weighting leads to the same fixed points as TD(0). Our analysis also leads to the first performance loss bound for approximate value iteration with an average cost objective.},
 author = {Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f076073b2082f8741a9cd07b789c77a0-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f076073b2082f8741a9cd07b789c77a0-Metadata.json},
 openalex = {W2127487139},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f076073b2082f8741a9cd07b789c77a0-Paper.pdf},
 publisher = {MIT Press},
 title = {TD(0) Leads to Better Policies than Approximate Value Iteration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/f076073b2082f8741a9cd07b789c77a0-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_f3b7e5d3,
 abstract = {We present micropower mixed-signal VLSI hardware for real-time blind separation and localization of acoustic sources. Gradient flow representation of the traveling wave signals acquired over a miniature (1cm diameter) array of four microphones yields linearly mixed instantaneous observations of the time-differentiated sources, separated and localized by independent component analysis (ICA). The gradient flow and ICA processors each measure 3mm × 3mm in 0.5 μm CMOS, and consume 54 μW and 180 μW power, respectively, from a 3 V supply at 16 ks/s sampling rate. Experiments demonstrate perceptually clear (12dB) separation and precise localization of two speech sources presented through speakers positioned at 1.5m from the array on a conference room table. Analysis of the multipath residuals shows that they are spectrally diffuse, and void of the direct path.},
 author = {Celik, Abdullah and Stanacevic, Milutin and Cauwenberghs, Gert},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f3b7e5d3eb074cde5b76e26bc0fb5776-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f3b7e5d3eb074cde5b76e26bc0fb5776-Metadata.json},
 openalex = {W2127332917},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f3b7e5d3eb074cde5b76e26bc0fb5776-Paper.pdf},
 publisher = {MIT Press},
 title = {Gradient Flow Independent Component Analysis in Micropower VLSI},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/f3b7e5d3eb074cde5b76e26bc0fb5776-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_f499d34b,
 abstract = {We present an infinite mixture model in which each component comprises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multi-modality and overlapping output signals. The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. This allows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian specification of the effective 'gating network' for the different experts.},
 author = {Meeds, Edward and Osindero, Simon},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f499d34bd87b42948b3960b8f6b82e74-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f499d34bd87b42948b3960b8f6b82e74-Metadata.json},
 openalex = {W2111683289},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f499d34bd87b42948b3960b8f6b82e74-Paper.pdf},
 publisher = {MIT Press},
 title = {An Alternative Infinite Mixture Of Gaussian Process Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/f499d34bd87b42948b3960b8f6b82e74-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_f565bb9e,
 abstract = {We demonstrate the first fully hardware implementation of retinotopic self-organization, from photon transduction to neural map formation. A silicon retina transduces patterned illumination into correlated spike trains that drive a population of silicon growth cones to automatically wire a topographic mapping by migrating toward sources of a diffusible guidance cue that is released by postsynaptic spikes. We varied the pattern of illumination to steer growth cones projected by different retinal ganglion cell types to self-organize segregated or coordinated retinotopic maps.},
 author = {Taba, Brian and Boahen, Kwabena},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f565bb9efccaf6986443db0bf01018bc-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f565bb9efccaf6986443db0bf01018bc-Metadata.json},
 openalex = {W2138593588},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f565bb9efccaf6986443db0bf01018bc-Paper.pdf},
 publisher = {MIT Press},
 title = {Silicon growth cones map silicon retina},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/f565bb9efccaf6986443db0bf01018bc-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_f5b1b89d,
 abstract = {We present a Bayesian framework for explaining how people reason about and predict the actions of an intentional agent, based on observing its behavior. Action-understanding is cast as a problem of inverting a probabilistic generative model, which assumes that agents tend to act rationally in order to achieve their goals given the constraints of their environment. Working in a simple sprite-world domain, we show how this model can be used to infer the goal of an agent and predict how the agent will act in novel situations or when environmental constraints change. The model provides a qualitative account of several kinds of inferences that preverbal infants have been shown to perform, and also fits quantitative predictions that adult observers make in a new experiment.},
 author = {Baker, Chris and Saxe, Rebecca and Tenenbaum, Joshua},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f5b1b89d98b7286673128a5fb112cb9a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f5b1b89d98b7286673128a5fb112cb9a-Metadata.json},
 openalex = {W2119199958},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf},
 publisher = {MIT Press},
 title = {Bayesian models of human action understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_f6c9dc70,
 abstract = {We present a model that learns the influence of interacting Markov chains within a team. The proposed model is a dynamic Bayesian network (DBN) with a two-level structure: individual-level and group-level. Individual level models actions of each player, and the group-level models actions of the team as a whole. Experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model.},
 author = {Zhang, Dong and Gatica-perez, Daniel and Bengio, Samy and Roy, Deb},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f6c9dc70ecfd8f90ba8598aa2401cd1a-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f6c9dc70ecfd8f90ba8598aa2401cd1a-Metadata.json},
 openalex = {W2165061375},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f6c9dc70ecfd8f90ba8598aa2401cd1a-Paper.pdf},
 publisher = {MIT Press},
 title = {Learning Influence among Interacting Markov Chains},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/f6c9dc70ecfd8f90ba8598aa2401cd1a-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_f7552665,
 abstract = {We introduce a new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods. We develop the notion of a recognizer, a filter on actions that distorts the behavior policy to produce a related target policy with low-variance importance-sampling corrections. We also consider target policies that are deviations from the state distribution of the behavior policy, such as potential temporally abstract options, which further reduces variance. This paper introduces recognizers and their potential advantages, then develops a full algorithm for linear function approximation and proves that its updates are in the same direction as on-policy TD updates, which implies asymptotic convergence. Even though our algorithm is based on importance sampling, we prove that it requires absolutely no knowledge of the behavior policy for the case of state-aggregation function approximators.},
 author = {Precup, Doina and Paduraru, Cosmin and Koop, Anna and Sutton, Richard S and Singh, Satinder},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f75526659f31040afeb61cb7133e4e6d-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f75526659f31040afeb61cb7133e4e6d-Metadata.json},
 openalex = {W2115253045},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf},
 publisher = {MIT Press},
 title = {Off-policy Learning with Options and Recognizers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/f75526659f31040afeb61cb7133e4e6d-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_f804d211,
 abstract = {In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model fitted by a maximum a posteriori estimation procedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the suggested mapping is interval-valued, providing a set of posterior probabilities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classification, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures.},
 author = {Grandvalet, Yves and Mariethoz, Johnny and Bengio, Samy},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f804d21145597e42851fa736e221da3f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f804d21145597e42851fa736e221da3f-Metadata.json},
 openalex = {W2107190016},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f804d21145597e42851fa736e221da3f-Paper.pdf},
 publisher = {MIT Press},
 title = {A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/f804d21145597e42851fa736e221da3f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_f9fd2624,
 abstract = {We discuss a method for obtaining a subject's a priori beliefs from his/her behavior in a psychophysics context, under the assumption that the behavior is (nearly) optimal from a Bayesian perspective. The method is nonparametric in the sense that we do not assume that the prior belongs to any fixed class of distributions (e.g., Gaussian). Despite this increased generality, the method is relatively simple to implement, being based in the simplest case on a linear programming algorithm, and more generally on a straightforward maximum likelihood or maximum a posteriori formulation, which turns out to be a convex optimization problem (with no non-global local maxima) in many important cases. In addition, we develop methods for analyzing the uncertainty of these estimates. We demonstrate the accuracy of the method in a simple simulated coin-flipping setting; in particular, the method is able to precisely track the evolution of the subject's posterior distribution as more and more data are observed. We close by briefly discussing an interesting connection to recent models of neural population coding.},
 author = {Paninski, Liam},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f9fd2624beefbc7808e4e405d73f57ab-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f9fd2624beefbc7808e4e405d73f57ab-Metadata.json},
 openalex = {W2104510459},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f9fd2624beefbc7808e4e405d73f57ab-Paper.pdf},
 publisher = {MIT Press},
 title = {Nonparametric inference of prior probabilities from Bayes-optimal behavior},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/f9fd2624beefbc7808e4e405d73f57ab-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_fb3f7685,
 abstract = {We propose a mean-field approximation that dramatically reduces the computational complexity of solving stochastic dynamic games. We provide conditions that guarantee our method approximates an equilibrium as the number of agents grow. We then derive a performance bound to assess how well the approximation performs for any given number of agents. We apply our method to an important class of problems in applied microeconomics. We show with numerical experiments that we are able to greatly expand the set of economic problems that can be analyzed computationally.},
 author = {Weintraub, Gabriel Y. and Benkard, Lanier and Van Roy, Benjamin},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fb3f76858cb38e5b7fd113e0bc1c0721-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fb3f76858cb38e5b7fd113e0bc1c0721-Metadata.json},
 openalex = {W2132241089},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fb3f76858cb38e5b7fd113e0bc1c0721-Paper.pdf},
 publisher = {MIT Press},
 title = {Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/fb3f76858cb38e5b7fd113e0bc1c0721-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_fc192b0c,
 abstract = {There is experimental evidence that cortical neurons show avalanche activity with the intensity of firing events being distributed as a power-law. We present a biologically plausible extension of a neural network which exhibits a power-law avalanche distribution for a wide range of connectivity parameters.},
 author = {Levina, Anna and Herrmann, Michael},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fc192b0c0d270dbf41870a63a8c76c2f-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fc192b0c0d270dbf41870a63a8c76c2f-Metadata.json},
 openalex = {W2135427932},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fc192b0c0d270dbf41870a63a8c76c2f-Paper.pdf},
 publisher = {MIT Press},
 title = {Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/fc192b0c0d270dbf41870a63a8c76c2f-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_fd4f21f2,
 abstract = {This paper proposes an algorithm to convert a T-stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems. The optimization problem associated with the trajectory tree and random trajectory methods of Kearns, Mansour, and Ng, 2000, is solved using the Gauss-Seidel method. The algorithm breaks a multistage reinforcement learning problem into a sequence of single-stage reinforcement learning subproblems, each of which is solved via an exact reduction to a weighted-classification problem that can be solved using off-the-self methods. Thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems. It is shown that the method converges in a finite number of steps to a solution that cannot be further improved by componentwise optimization. The implication of the proposed algorithm is that a plethora of classification methods can be applied to find policies in the reinforcement learning problem.},
 author = {Blatt, Doron and Hero, Alfred},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fd4f21f2556dad0ea8b7a5c04eabebda-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fd4f21f2556dad0ea8b7a5c04eabebda-Metadata.json},
 openalex = {W2096556124},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fd4f21f2556dad0ea8b7a5c04eabebda-Paper.pdf},
 publisher = {MIT Press},
 title = {From Weighted Classification to Policy Search},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/fd4f21f2556dad0ea8b7a5c04eabebda-Abstract.html},
 volume = {18},
 year = {2005}
}

@inproceedings{NIPS2005_fdf1bc56,
 abstract = {We describe a vision-based obstacle avoidance system for off-road mobile robots. The system is trained from end to end to map raw input images to steering angles. It is trained in supervised mode to predict the steering angles provided by a human driver during training runs collected in a wide variety of terrains, weather conditions, lighting conditions, and obstacle types. The robot is a 50cm off-road truck, with two forward-pointing wireless color cameras. A remote computer processes the video and controls the robot via radio. The learning system is a large 6-layer convolutional network whose input is a single left/right pair of unprocessed low-resolution images. The robot exhibits an excellent ability to detect obstacles and navigate around them in real time at speeds of 2 m/s.},
 author = {Muller, Urs and Ben, Jan and Cosatto, Eric and Flepp, Beat and Cun, Yann},
 bib = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fdf1bc5669e8ff5ba45d02fded729feb-Bibtex.bib},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 metadata = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fdf1bc5669e8ff5ba45d02fded729feb-Metadata.json},
 openalex = {W2133233905},
 pages = {},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fdf1bc5669e8ff5ba45d02fded729feb-Paper.pdf},
 publisher = {MIT Press},
 title = {Off-Road Obstacle Avoidance through End-to-End Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/hash/fdf1bc5669e8ff5ba45d02fded729feb-Abstract.html},
 volume = {18},
 year = {2005}
}
