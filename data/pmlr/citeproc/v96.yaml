
- title: 'An Exploration of Acquisition and Mean Functions in Variational Bayesian Monte Carlo'
  abstract: 'Variational Bayesian Monte Carlo (VBMC) is a novel framework for tackling approximate posterior and model inference in models with black-box, expensive likelihoods by means of a sample-efficient approach (Acerbi, 2018). VBMC combines variational inference with Gaussian-process (GP) based, active-sampling Bayesian quadrature, using the latter to efficiently approximate the intractable integral in the variational objective. VBMC has been shown to outperform state-of-the-art inference methods for expensive likelihoods on a benchmark consisting of meaningful synthetic densities and a real model-fitting problem from computational neuroscience. In this paper, we study the performance of VBMC under variations of two key components of the framework. First, we propose and evaluate a new general family of acquisition functions for active sampling, which includes as special cases the acquisition functions used in the original work. Second, we test different mean functions for the GP surrogate, including a novel squared-exponential GP mean function. From our empirical study, we derive insights about the stability of the current VBMC algorithm, which may help inform future theoretical and applied developments of the method.'
  volume: 96
  URL: https://proceedings.mlr.press/v96/acerbi19a.html
  PDF: http://proceedings.mlr.press/v96/acerbi19a/acerbi19a.pdf
  edit: https://github.com/mlresearch//v96/edit/gh-pages/_posts/2019-01-05-acerbi19a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inference'
  publisher: 'PMLR'
  author: 
  - given: Luigi
    family: Acerbi
  editor: 
  - given: Francisco
    family: Ruiz
  - given: Cheng
    family: Zhang
  - given: Dawen
    family: Liang
  - given: Thang
    family: Bui
  page: 1-10
  id: acerbi19a
  issued:
    date-parts: 
      - 2019
      - 1
      - 5
  firstpage: 1
  lastpage: 10
  published: 2019-01-05 00:00:00 +0000
- title: 'Consistency of ELBO maximization for model selection'
  abstract: 'The Evidence Lower Bound (ELBO) is a quantity that plays a key role in variational inference. It can also be used as a criterion in model selection. However, though extremely popular in practice in the variational Bayes community, there has never been a general theoretic justication for selecting based on the ELBO. In this paper, we show that the ELBO maximization strategy has strong theoretical guarantees, and is robust to model misspeciffication while most works rely on the assumption that one model is correctly speciffied. We illustrate our theoretical results by an application to the selection of the number of principal components in probabilistic PCA.'
  volume: 96
  URL: https://proceedings.mlr.press/v96/cherief-abdellatif19a.html
  PDF: http://proceedings.mlr.press/v96/cherief-abdellatif19a/cherief-abdellatif19a.pdf
  edit: https://github.com/mlresearch//v96/edit/gh-pages/_posts/2019-01-05-cherief-abdellatif19a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inference'
  publisher: 'PMLR'
  author: 
  - given: Badr-Eddine
    family: Cherief-Abdellatif
  editor: 
  - given: Francisco
    family: Ruiz
  - given: Cheng
    family: Zhang
  - given: Dawen
    family: Liang
  - given: Thang
    family: Bui
  page: 11-31
  id: cherief-abdellatif19a
  issued:
    date-parts: 
      - 2019
      - 1
      - 5
  firstpage: 11
  lastpage: 31
  published: 2019-01-05 00:00:00 +0000
- title: 'Likelihood-free inference with emulator networks'
  abstract: 'Approximate Bayesian Computation (ABC) provides methods for Bayesian inference in simulation-based models which do not permit tractable likelihoods. We present a new ABC method which uses probabilistic neural emulator networks to learn synthetic likelihoods on simulated data - both ’local’ emulators which approximate the likelihood for specific observed data, as well as ’global’ ones which are applicable to a range of data. Simulations are chosen adaptively using an acquisition function which takes into account uncertainty about either the posterior distribution of interest, or the parameters of the emulator. Our approach does not rely on user-defined rejection thresholds or distance functions. We illustrate inference with emulator networks on synthetic examples and on a biophysical neuron model, and show that emulators allow accurate and efficient inference even on problems which are challenging for conventional ABC approaches.'
  volume: 96
  URL: https://proceedings.mlr.press/v96/lueckmann19a.html
  PDF: http://proceedings.mlr.press/v96/lueckmann19a/lueckmann19a.pdf
  edit: https://github.com/mlresearch//v96/edit/gh-pages/_posts/2019-01-05-lueckmann19a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inference'
  publisher: 'PMLR'
  author: 
  - given: Jan-Matthis
    family: Lueckmann
  - given: Giacomo
    family: Bassetto
  - given: Theofanis
    family: Karaletsos
  - given: Jakob H.
    family: Macke
  editor: 
  - given: Francisco
    family: Ruiz
  - given: Cheng
    family: Zhang
  - given: Dawen
    family: Liang
  - given: Thang
    family: Bui
  page: 32-53
  id: lueckmann19a
  issued:
    date-parts: 
      - 2019
      - 1
      - 5
  firstpage: 32
  lastpage: 53
  published: 2019-01-05 00:00:00 +0000
