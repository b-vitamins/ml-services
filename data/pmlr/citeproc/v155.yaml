
- title: 'Learning a Decision Module by Imitating Driver’s Control Behaviors'
  abstract: 'Autonomous driving systems have a pipeline of perception, decision, planning, and control. The decision module processes information from the perception module and directs the execution of downstream planning and control modules. On the other hand, the recent success of deep learning suggests that this pipeline could be replaced by end-to-end neural control policies, however, safety cannot be well guaranteed for the data-driven neural networks. In this work, we propose a hybrid framework to learn neural decisions in the classical modular pipeline through end-to-end imitation learning.  This hybrid framework can preserve the merits of the classical pipeline such as the strict enforcement of physical and logical constraints while learning complex driving decisions from data. To circumvent the ambiguous annotation of human driving decisions, our method learns high-level driving decisions by imitating low-level control behaviors. We show in the simulation experiments that our modular driving agent can generalize its driving decision and control to various complex scenarios where the rule-based programs fail. It can also generate smoother and safer driving trajectories than end-to-end neural policies. Demo and code are available at https://decisionforce.github.io/modulardecision/.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/huang21a.html
  PDF: https://proceedings.mlr.press/v155/huang21a/huang21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-huang21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Junning
    family: Huang
  - given: Sirui
    family: Xie
  - given: Jiankai
    family: Sun
  - given: Qiurui
    family: Ma
  - given: Chunxiao
    family: Liu
  - given: Dahua
    family: Lin
  - given: Bolei
    family: Zhou
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1-10
  id: huang21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1
  lastpage: 10
  published: 2021-10-04 00:00:00 +0000
- title: 'Inverting the Pose Forecasting Pipeline with SPF2: Sequential Pointcloud Forecasting for Sequential Pose Forecasting'
  abstract: 'Many autonomous systems forecast aspects of the future in order to aid decision-making. For example, self-driving vehicles and robotic manipulation systems often forecast future object poses by first detecting and tracking objects. However, this detect-then-forecast pipeline is expensive to scale, as pose forecasting algorithms typically require labeled sequences of object poses, which are costly to obtain in 3D space. Can we scale performance without requiring additional labels? We hypothesize yes, and propose inverting the detect-then-forecast pipeline. Instead of detecting, tracking and then forecasting the objects, we propose to first forecast 3D sensor data (e.g., point clouds with $100$k points) and then detect/track objects on the predicted point cloud sequences to obtain future poses, i.e., a forecast-then-detect pipeline. This inversion makes it less expensive to scale pose forecasting, as the sensor data forecasting task requires no labels. Part of this work’s focus is on the challenging first step – Sequential Pointcloud Forecasting (SPF), for which we also propose an effective approach, SPFNet. To compare our forecast-then-detect pipeline relative to the detect-then-forecast pipeline, we propose an evaluation procedure and two metrics. Through experiments on a robotic manipulation dataset and two driving datasets, we show that SPFNet is effective for the SPF task, our forecast-then-detect pipeline outperforms the detect-then-forecast approaches to which we compared, and that pose forecasting performance improves with the addition of unlabeled data. Our project website is http://www.xinshuoweng.com/projects/SPF2'
  volume: 155
  URL: https://proceedings.mlr.press/v155/weng21a.html
  PDF: https://proceedings.mlr.press/v155/weng21a/weng21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-weng21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xinshuo
    family: Weng
  - given: Jianren
    family: Wang
  - given: Sergey
    family: Levine
  - given: Kris
    family: Kitani
  - given: Nicholas
    family: Rhinehart
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 11-20
  id: weng21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 11
  lastpage: 20
  published: 2021-10-04 00:00:00 +0000
- title: 'Neuro-Symbolic Program Search for Autonomous Driving Decision Module Design'
  abstract: 'As a promising topic in cognitive robotics, neuro-symbolic modeling integrates symbolic reasoning and neural representation altogether. However, previous neuro-symbolic models usually wire their structures and the connections manually, making the underlying parameters sub-optimal. In this work, we propose the Neuro-Symbolic Program Search (NSPS) to improve the autonomous driving system design. NSPS is a novel automated search method that synthesizes the Neuro-Symbolic Programs. It can produce robust and expressive Neuro-Symbolic Programs and automatically tune the hyper-parameters. We validate NSPS in the CARLA driving simulation environment. The resulting Neuro-Symbolic Decision Programs successfully handle multiple traffic scenarios. Compared with previous neural-network-based driving and rule-based methods, our neuro-symbolic driving pipeline achieves more stable and safer behaviors in complex driving scenarios while maintaining an interpretable symbolic decision-making process.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/sun21a.html
  PDF: https://proceedings.mlr.press/v155/sun21a/sun21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-sun21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jiankai
    family: Sun
  - given: Hao
    family: Sun
  - given: Tian
    family: Han
  - given: Bolei
    family: Zhou
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 21-30
  id: sun21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 21
  lastpage: 30
  published: 2021-10-04 00:00:00 +0000
- title: 'LiRaNet: End-to-End Trajectory Prediction using Spatio-Temporal Radar Fusion'
  abstract: 'In this paper, we present LiRaNet, a novel end-to-end trajectory prediction method which utilizes radar sensor information along with widely used lidar and HD maps. Automotive radar provides rich, complementary information, allowing for longer range vehicle detection as well as instantaneous radial velocity measurements. However, there are factors that make the fusion of lidar and radar information challenging, such as the relatively low angular resolution of radar measurements, their sparsity and the lack of exact time synchronization with lidar. To overcome these challenges, we propose an efficient spatio-temporal radar feature extraction scheme which achieves state-of-the-art performance on multiple large-scale datasets. Further, by incorporating radar information, we show a 52% reduction in prediction error for objects with high acceleration and a 16% reduction in prediction error for objects at longer range.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/shah21a.html
  PDF: https://proceedings.mlr.press/v155/shah21a/shah21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-shah21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Meet
    family: Shah
  - given: Zhiling
    family: Huang
  - given: Ankit
    family: Laddha
  - given: Matthew
    family: Langford
  - given: Blake
    family: Barber
  - given: sida
    family: zhang
  - given: Carlos
    family: Vallespi-Gonzalez
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 31-48
  id: shah21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 31
  lastpage: 48
  published: 2021-10-04 00:00:00 +0000
- title: 'DROGON: A Trajectory Prediction Model based on Intention-Conditioned Behavior Reasoning'
  abstract: 'We propose a Deep RObust Goal-Oriented trajectory prediction Network (DROGON) for accurate vehicle trajectory prediction by considering behavioral intentions of vehicles in traffic scenes. Our main insight is that the behavior (i.e., motion) of drivers can be reasoned from their high level possible goals (i.e., intention) on the road. To succeed in such behavior reasoning, we build a conditional prediction model to forecast goal-oriented trajectories with the following stages: (i) relational inference where we encode relational interactions of vehicles using the perceptual context; (ii) intention estimation to compute the probability distributions of intentional goals based on the inferred relations; and (iii) behavior reasoning where we reason about the behaviors of vehicles as trajectories conditioned on the intentions. To this end, we extend the proposed framework to the pedestrian trajectory prediction task, showing the potential applicability toward general trajectory prediction.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/choi21a.html
  PDF: https://proceedings.mlr.press/v155/choi21a/choi21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-choi21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Chiho
    family: Choi
  - given: Srikanth
    family: Malla
  - given: Abhishek
    family: Patil
  - given: Joon Hee
    family: Choi
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 49-63
  id: choi21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 49
  lastpage: 63
  published: 2021-10-04 00:00:00 +0000
- title: 'CAMPs: Learning Context-Specific Abstractions for Efficient Planning in Factored MDPs'
  abstract: 'Meta-planning, or learning to guide planning from experience, is a promising approach to improving the computational cost of planning. A general meta-planning strategy is to learn to impose constraints on the states considered and actions taken by the agent. We observe that (1) imposing a constraint can induce context-specific independences that render some aspects of the domain irrelevant, and (2) an agent can take advantage of this fact by imposing constraints on its own behavior. These observations lead us to propose the context-specific abstract Markov decision process (CAMP), an abstraction of a factored MDP that affords efficient planning. We then describe how to learn constraints to impose so the CAMP optimizes a trade-off between rewards and computational cost. Our experiments consider five planners across four domains, including robotic navigation among movable obstacles (NAMO), robotic task and motion planning for sequential manipulation, and classical planning. We find planning with learned CAMPs to consistently outperform baselines, including Stilman’s NAMO-specific algorithm. Video: https://youtu.be/wTXt6djcAd4 Code: https://git.io/JTnf6'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chitnis21a.html
  PDF: https://proceedings.mlr.press/v155/chitnis21a/chitnis21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chitnis21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Rohan
    family: Chitnis
  - given: Tom
    family: Silver
  - given: Beomjoon
    family: Kim
  - given: Leslie
    family: Kaelbling
  - given: Tomas
    family: Lozano-Perez
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 64-79
  id: chitnis21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 64
  lastpage: 79
  published: 2021-10-04 00:00:00 +0000
- title: 'Augmenting GAIL with BC for sample efficient imitation learning'
  abstract: 'Imitation learning is the problem of recovering an expert policy without access to a reward signal. Behavior cloning and GAIL are two widely used methods for performing imitation learning. Behavior cloning converges in a few iterations, but doesn’t achieve peak performance due to its inherent iid assumption about the state-action distribution. GAIL addresses the issue by accounting for the temporal dependencies when performing a state distribution matching between the agent and the expert. Although GAIL is sample efficient in the number of expert trajectories required, it is still not very sample efficient in terms of the environment interactions needed for convergence of the policy. Given the complementary benefits of both methods, we present a simple and elegant method to combine both methods to enable stable and sample efficient learning. Our algorithm is very simple to implement and integrates with different policy gradient algorithms. We demonstrate the effectiveness of the algorithm in low dimensional control tasks, gridworlds and in high dimensional image-based tasks.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/jena21a.html
  PDF: https://proceedings.mlr.press/v155/jena21a/jena21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-jena21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Rohit
    family: Jena
  - given: Changliu
    family: Liu
  - given: Katia
    family: Sycara
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 80-90
  id: jena21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 80
  lastpage: 90
  published: 2021-10-04 00:00:00 +0000
- title: 'From pixels to legs: Hierarchical learning of quadruped locomotion'
  abstract: 'Legged robots navigating crowded scenes and complex terrains in the real world are required to execute dynamic leg movements while processing visual input for obstacle avoidance and path planning. We show that a quadruped robot can acquire both of these skills by means of hierarchical reinforcement learning (HRL). By virtue of their hierarchical structure, our policies learn to implicitly break down this joint problem by concurrently learning High Level (HL) and Low Level (LL) neural network policies. These two levels are connected by a low dimensional hidden layer, which we call latent command. HL receives a first-person camera view, whereas LL receives the latent command from HL and the robot’s on-board sensors to control its actuators. We train policies to walk in two different environments: a curved cliff and a maze. We show that hierarchical policies can concurrently learn to locomote and navigate in these environments, and show they are more efficient than non-hierarchical neural network policies. This architecture also allows for knowledge reuse across tasks. LL networks trained on one task can be transferred to a new task in a new environment. Finally HL, which processes camera images, can be evaluated at much lower and varying frequencies compared to LL, thus reducing computation times and bandwidth requirements.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/jain21a.html
  PDF: https://proceedings.mlr.press/v155/jain21a/jain21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-jain21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Deepali
    family: Jain
  - given: Ken
    family: Caluwaerts
  - given: Atil
    family: Iscen
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 91-102
  id: jain21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 91
  lastpage: 102
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning a Decentralized Multi-Arm Motion Planner'
  abstract: 'We present a closed-loop multi-arm motion planner that is scalable and flexible with team size. Traditional multi-arm robotic systems have relied on centralized motion planners, whose run times often scale exponentially with team size, and thus, fail to handle dynamic environments with open-loop control. In this paper, we tackle this problem with multi-agent reinforcement learning, where a shared policy network is trained to control each individual robot arm to reach its target end-effector pose given observations of its workspace state and target end-effector pose. The policy is trained using Soft Actor-Critic with expert demonstrations from a sampling-based motion planning algorithm (i.e., BiRRT). By leveraging classical planning algorithms, we can improve the learning efficiency of the reinforcement learning algorithm while retaining the fast inference time of neural networks. The resulting policy scales sub-linearly and can be deployed on multi-arm systems with variable team sizes. Thanks to the closed-loop and decentralized formulation, our approach generalizes to 5-10 multiarm systems and dynamic moving targets (>90% success rate for a 10-arm system), despite being trained on only 1-4 arm planning tasks with static targets.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ha21a.html
  PDF: https://proceedings.mlr.press/v155/ha21a/ha21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ha21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Huy
    family: Ha
  - given: Jingxi
    family: Xu
  - given: Shuran
    family: Song
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 103-114
  id: ha21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 103
  lastpage: 114
  published: 2021-10-04 00:00:00 +0000
- title: 'SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural Networks'
  abstract: 'Recent learning-based LiDAR odometry methods have demonstrated their competitiveness. However, most methods still face two substantial challenges: 1) the 2D projection representation of LiDAR data cannot effectively encode 3D structures from the point clouds; 2) the needs for a large amount of labeled data for training limit the application scope of these methods. In this paper, we propose an self-supervised LiDAR odometry method, dubbed SelfVoxeLO, to tackle these two difficulties. Specifically, we propose a 3D convolution network to process the raw LiDAR data directly, which extracts features that better encode the 3D geometric patterns. To suit our network to self-supervised learning, we design several novel loss functions that utilize the inherent properties of LiDAR point clouds. Moreover, an uncertainty-aware mechanism is incorporated in the loss functions to alleviate the interference of moving objects/noises. We evaluate our method’s performances on two large-scale datasets, ie, KITTI and Apollo-SouthBay.Our method outperforms state-of-the-art unsupervised methods by 27%-32% in terms of translational/rotational errors on the KITTI dataset and also performs well on the Apollo-SouthBay dataset. By including more unlabelled training data, our method can further improve performance comparable to the supervised methods.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/xu21a.html
  PDF: https://proceedings.mlr.press/v155/xu21a/xu21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-xu21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yan
    family: Xu
  - given: Zhaoyang
    family: Huang
  - given: Kwan-Yee
    family: Lin
  - given: Xinge
    family: Zhu
  - given: Jianping
    family: Shi
  - given: Hujun
    family: Bao
  - given: Guofeng
    family: Zhang
  - given: Hongsheng
    family: Li
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 115-125
  id: xu21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 115
  lastpage: 125
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning 3D Dynamic Scene Representations for Robot Manipulation'
  abstract: '3D scene representation for robot manipulation should capture three key object properties: permanency - objects that become occluded over time continue to exist; amodal completeness - objects have 3D occupancy, even if only partial observations are available; spatiotemporal continuity - the movement of each object is continuous over space and time. In this paper, we introduce 3D Dynamic Scene Representation (DSR), a 3D volumetric scene representation that simultaneously discovers, tracks, reconstructs objects, and predicts their dynamics while capturing all three properties. We further propose DSR-Net, which learns to aggregate visual observations over multiple interactions to gradually build and refine DSR. Our model achieves state-of-the-art performance in modeling 3D scene dynamics with DSR on both simulated and real data. Combined with model predictive control, DSR-Net enables accurate planning in downstream robotic manipulation tasks such as planar pushing. Code and data are available at dsr-net.cs.columbia.edu.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/xu21b.html
  PDF: https://proceedings.mlr.press/v155/xu21b/xu21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-xu21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zhenjia
    family: Xu
  - given: Zhanpeng
    family: He
  - given: Jiajun
    family: Wu
  - given: Shuran
    family: Song
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 126-142
  id: xu21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 126
  lastpage: 142
  published: 2021-10-04 00:00:00 +0000
- title: 'CoT-AMFlow: Adaptive Modulation Network with Co-Teaching Strategy for Unsupervised Optical Flow Estimation'
  abstract: 'The interpretation of ego motion and scene change is a fundamental task for mobile robots. Optical flow information can be employed to estimate motion in the surroundings. Recently, unsupervised optical flow estimation has become a research hotspot.  However, unsupervised approaches are often easy to be unreliable on partially occluded or texture-less regions. To deal with this problem, we propose CoT-AMFlow in this paper, an unsupervised optical flow estimation approach. In terms of the network architecture, we develop an adaptive modulation network that employs two novel module types, flow modulation modules (FMMs) and cost volume modulation modules (CMMs), to remove outliers in challenging regions. As for the training paradigm, we adopt a co-teaching strategy, where two networks simultaneously teach each other about challenging regions to further improve accuracy. Experimental results on the MPI Sintel, KITTI Flow and Middlebury Flow benchmarks demonstrate that our CoT-AMFlow outperforms all other state-of-the-art unsupervised approaches, while still running in real time. Our project page is available at https://sites.google.com/view/cot-amflow.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wang21a.html
  PDF: https://proceedings.mlr.press/v155/wang21a/wang21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wang21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Hengli
    family: Wang
  - given: Rui
    family: Fan
  - given: Ming
    family: Liu
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 143-155
  id: wang21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 143
  lastpage: 155
  published: 2021-10-04 00:00:00 +0000
- title: 'SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy Learning'
  abstract: 'We describe a policy learning approach to map visual inputs to driving controls conditioned on turning command that leverages side tasks on semantics and object affordances via a learned representation trained for driving. To learn this representation, we train a squeeze network to drive using annotations for the side task as input. This representation encodes the driving-relevant information associated with the side task while ideally throwing out side task-relevant but driving-irrelevant nuisances. We then train a mimic network to drive using only images as input and use the squeeze network’s latent representation to supervise the mimic network via a mimicking loss. Notably, we do not aim to achieve the side task nor to learn features for it; instead, we aim to learn, via the mimicking loss, a representation of the side task annotations directly useful for driving. We test our approach using the CARLA simulator. In addition, we introduce a more challenging but realistic evaluation protocol that considers a run that reaches the destination successful only if it does not violate common traffic rules.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zhao21a.html
  PDF: https://proceedings.mlr.press/v155/zhao21a/zhao21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zhao21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Albert
    family: Zhao
  - given: Tong
    family: He
  - given: Yitao
    family: Liang
  - given: Haibin
    family: Huang
  - given: Guy Van den
    family: Broeck
  - given: Stefano
    family: Soatto
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 156-175
  id: zhao21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 156
  lastpage: 175
  published: 2021-10-04 00:00:00 +0000
- title: 'Fit2Form: 3D Generative Model for Robot Gripper Form Design'
  abstract: 'The 3D shape of a robot’s end-effector plays a critical role in determining it’s functionality and overall performance. Many of today’s industrial applications rely on highly customized gripper design for a given task to ensure the system’s robustness and accuracy.  However, the process of manual hardware design is both costly and time-consuming, and the quality of the design is also dependent on the engineer’s experience and domain expertise, which can easily be out-dated or inaccurate.  The goal of this paper is to use machine learning algorithms to automate this design process and generate task-specific gripper designs that satisfy a set of pre-defined design objectives. We model the design objectives by training a Fitness network to predict their values for a pair of gripper fingers and a grasp object. This Fitness network is then used to provide training supervision to a 3D Generative network that produces a pair of 3D finger geometries for the target grasp object. Our experiments demonstrate that the proposed 3D generative design framework generates parallel jaw gripper finger shapes that achieve more stable and robust grasps as compared to other general-purpose and task-specific gripper design algorithms.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ha21b.html
  PDF: https://proceedings.mlr.press/v155/ha21b/ha21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ha21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Huy
    family: Ha
  - given: Shubham
    family: Agrawal
  - given: Shuran
    family: Song
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 176-187
  id: ha21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 176
  lastpage: 187
  published: 2021-10-04 00:00:00 +0000
- title: 'Accelerating Reinforcement Learning with Learned Skill Priors'
  abstract: 'Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One approach for leveraging prior knowledge is to transfer skills learned on prior tasks to the new task. However, as the amount of prior experience increases, the number of transferable skills grows too, making it challenging to explore the full set of available skills during downstream learning. Yet, intuitively, not all skills should be explored with equal probability; for example information about the current state can hint which skills are promising to explore. In this work, we propose to implement this intuition by learning a prior over skills. We propose a deep latent variable model that jointly learns an embedding space of skills and the skill prior from offline agent experience. We then extend common maximum-entropy RL approaches to use skill priors to guide downstream learning. We validate our approach, SPiRL (Skill-Prior RL), on complex navigation and robotic manipulation tasks and show that learned skill priors are essential for effective skill transfer from rich datasets. Videos and code are available at https://clvrai.com/spirl.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/pertsch21a.html
  PDF: https://proceedings.mlr.press/v155/pertsch21a/pertsch21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-pertsch21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Karl
    family: Pertsch
  - given: Youngwoon
    family: Lee
  - given: Joseph
    family: Lim
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 188-204
  id: pertsch21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 188
  lastpage: 204
  published: 2021-10-04 00:00:00 +0000
- title: 'Positive-Unlabeled Reward Learning'
  abstract: 'Learning reward functions from data is a promising path towards achieving scalable Reinforcement Learning (RL) for robotics. However, a major challenge in training agents from learned reward models is that the agent can learn to exploit errors in the reward model to achieve high reward behaviors that do not correspond to the intended task. These reward delusions can lead to unintended and even dangerous behaviors. On the other hand, adversarial imitation learning frameworks (Ho et al., 2016) tend to suffer the opposite problem, where the discriminator learns to trivially distinguish agent and expert behavior, resulting in reward models that produce low reward signal regardless of the input state. In this paper, we connect these two classes of reward learning methods to positive-unlabeled (PU) learning, and show that by applying a large-scale PU learning algorithm to the reward learning problem, we can address both the reward under- and over-estimation problems simultaneously. Our approach drastically improves both GAIL and supervised reward learning, without any additional assumptions.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/xu21c.html
  PDF: https://proceedings.mlr.press/v155/xu21c/xu21c.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-xu21c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Danfei
    family: Xu
  - given: Misha
    family: Denil
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 205-219
  id: xu21c
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 205
  lastpage: 219
  published: 2021-10-04 00:00:00 +0000
- title: 'GDN: A Coarse-To-Fine (C2F) Representation for End-To-End 6-DoF Grasp Detection'
  abstract: 'We proposed an end-to-end grasp detection network,  Grasp Detection Network (GDN), cooperated with a novel coarse-to-fine (C2F) grasp representation design to detect diverse and accurate 6-DoF grasps based on point clouds.   Compared to previous two-stage approaches which sample and evaluate multiple grasp candidates, our architecture is at least 20 times faster.  It is also 8% and 40% more accurate in terms of the success rate in single object scenes and the complete rate in clutter scenes, respectively. Our method shows superior results among settings with different number of views and input points.  Moreover, we propose a new AP-based metric which considers both rotation and transition errors, making it a more comprehensive evaluation tool for grasp detection models.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/jeng21a.html
  PDF: https://proceedings.mlr.press/v155/jeng21a/jeng21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-jeng21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Kuang-Yu
    family: Jeng
  - given: Yueh-Cheng
    family: Liu
  - given: Zhe Yu
    family: Liu
  - given: Jen-Wei
    family: Wang
  - given: Ya-Liang
    family: Chang
  - given: Hung-Ting
    family: Su
  - given: Winston
    family: Hsu
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 220-231
  id: jeng21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 220
  lastpage: 231
  published: 2021-10-04 00:00:00 +0000
- title: 'Action-based Representation Learning for Autonomous Driving'
  abstract: 'Human drivers produce a vast amount of data which could, in principle, be used to improve autonomous driving systems. Unfortunately, seemingly straightforward approaches for creating end-to-end driving models that map sensor data directly into driving actions are problematic in terms of interpretability, and typically have significant difficulty dealing with spurious correlations. Alternatively, we propose to use this kind of action-based driving data for learning representations. Our experiments show that an affordance-based driving model pre-trained with this approach can leverage a relatively small amount of weakly annotated imagery and outperform pure end-to-end driving models, while being more interpretable. Further, we demonstrate how this strategy outperforms previous methods based on learning inverse dynamics models as well as other methods based on heavy human supervision (ImageNet).'
  volume: 155
  URL: https://proceedings.mlr.press/v155/xiao21a.html
  PDF: https://proceedings.mlr.press/v155/xiao21a/xiao21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-xiao21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yi
    family: Xiao
  - given: Felipe
    family: Codevilla
  - given: Christopher
    family: Pal
  - given: Antonio
    family: Lopez
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 232-246
  id: xiao21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 232
  lastpage: 246
  published: 2021-10-04 00:00:00 +0000
- title: 'Task-Relevant Adversarial Imitation Learning'
  abstract: 'We show that a critical vulnerability in adversarial imitation is the tendency of discriminator networks to learn spurious associations between visual features and expert labels. When the discriminator focuses on task-irrelevant features, it does not provide an informative reward signal, leading to poor task performance. We analyze this problem in detail and propose a solution that outperforms standard Generative Adversarial Imitation Learning (GAIL). Our proposed method, Task-Relevant Adversarial Imitation Learning (TRAIL), uses constrained discriminator optimization to learn informative rewards. In comprehensive experiments, we show that TRAIL can solve challenging robotic manipulation tasks from pixels by imitating human operators without access to any task rewards, and clearly outperforms comparable baseline imitation agents, including those trained via behaviour cloning and conventional GAIL.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zolna21a.html
  PDF: https://proceedings.mlr.press/v155/zolna21a/zolna21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zolna21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Konrad
    family: Zolna
  - given: Scott
    family: Reed
  - given: Alexander
    family: Novikov
  - given: Sergio Gómez
    family: Colmenarejo
  - given: David
    family: Budden
  - given: Serkan
    family: Cabi
  - given: Misha
    family: Denil
  - given: Nando de
    family: Freitas
  - given: Ziyu
    family: Wang
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 247-263
  id: zolna21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 247
  lastpage: 263
  published: 2021-10-04 00:00:00 +0000
- title: 'SMARTS: An Open-Source Scalable Multi-Agent RL Training School for Autonomous Driving'
  abstract: 'Interaction is fundamental in autonomous driving (AD). Despite more than a decade of intensive R&D in AD, how to dynamically interact with diverse road users in various contexts still remains unsolved. Multi-agent learning has recently seen big breakthroughs and has much to offer towards solving realistic interaction in AD. However, to realize this potential we need multi-agent AD simulation of realistic interaction. To break this apparent chicken-and-egg circularity, we built an AD simulation platform called SMARTS (Scalable Multi-Agent Rl Training School), which is designed to accumulate behavior models of road users towards increasingly realistic and diverse interaction that in turn enables deeper and broader multi-agent research on interaction. In this paper, we describe the design goals of SMARTS, explain its key architectural ideas, illustrate its use for multi-agent research through experiments on concrete interaction scenarios, and introduce a set of benchmarks and metrics. As an open-source, industrial-strength platform, the future of SMARTS lies in its growth along with the multi-agent research it enables in the years to come.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zhou21a.html
  PDF: https://proceedings.mlr.press/v155/zhou21a/zhou21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zhou21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ming
    family: Zhou
  - given: Jun
    family: Luo
  - given: Julian
    family: Villella
  - given: Yaodong
    family: Yang
  - given: David
    family: Rusu
  - given: Jiayu
    family: Miao
  - given: Weinan
    family: Zhang
  - given: Montgomery
    family: Alban
  - given: IMAN
    family: FADAKAR
  - given: Zheng
    family: Chen
  - given: Chongxi
    family: Huang
  - given: Ying
    family: Wen
  - given: Kimia
    family: Hassanzadeh
  - given: Daniel
    family: Graves
  - given: Zhengbang
    family: Zhu
  - given: Yihan
    family: Ni
  - given: Nhat
    family: Nguyen
  - given: Mohamed
    family: Elsayed
  - given: Haitham
    family: Ammar
  - given: Alexander
    family: Cowen-Rivers
  - given: Sanjeevan
    family: Ahilan
  - given: Zheng
    family: Tian
  - given: Daniel
    family: Palenicek
  - given: Kasra
    family: Rezaee
  - given: Peyman
    family: Yadmellat
  - given: Kun
    family: Shao
  - given: dong
    family: chen
  - given: Baokuan
    family: Zhang
  - given: Hongbo
    family: Zhang
  - given: Jianye
    family: Hao
  - given: Wulong
    family: Liu
  - given: Jun
    family: Wang
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 264-285
  id: zhou21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 264
  lastpage: 285
  published: 2021-10-04 00:00:00 +0000
- title: 'Reconfigurable Voxels: A New Representation for LiDAR-Based Point Clouds'
  abstract: 'LiDAR is an important method for autonomous driving systems to sense the environment. The point clouds obtained by LiDAR typically exhibit sparse and irregular distribution, thus posing great challenges to the detection of 3D objects, especially those that are small and distant. To tackle this difficulty, we propose Reconfigurable Voxels, a new approach to constructing representations from 3D point clouds. Specifically, we devise a biased random walk scheme, which adaptively covers each neighborhood with a fixed number of voxels based on the local spatial distribution and produces a representation by integrating the points in the chosen neighbors. We found empirically that this approach effectively improves the stability of voxel features, especially for sparse regions. Experimental results on multiple benchmarks, including nuScenes, Lyft, and KITTI, show that this new representation can remarkably improve the detection performance for small and distant objects, without incurring noticeable overhead cost'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wang21b.html
  PDF: https://proceedings.mlr.press/v155/wang21b/wang21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wang21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tai
    family: Wang
  - given: Xinge
    family: Zhu
  - given: Dahua
    family: Lin
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 286-295
  id: wang21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 286
  lastpage: 295
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Object Manipulation Skills via Approximate State Estimation from Real Videos'
  abstract: 'Humans are adept at learning new tasks by watching a few instructional videos. On the other hand, robots that learn new actions either require a lot of effort through trial and error, or use expert demonstrations that are challenging to obtain. In this paper, we explore a method that facilitates learning object manipulation skills directly from videos. Leveraging recent advances in 2D visual recognition and differentiable rendering, we develop an optimization based method to estimate a coarse 3D state representation for the hand and the manipulated object(s) without requiring any supervision. We use these trajectories as dense rewards for an agent that learns to mimic them through reinforcement learning. We evaluate our method on simple single- and two-object actions from the Something-Something dataset. Our approach allows an agent to learn actions from single videos, while watching multiple demonstrations makes the policy more robust. We show that policies learned in a simulated environment can be easily transferred to a real robot.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/petrik21a.html
  PDF: https://proceedings.mlr.press/v155/petrik21a/petrik21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-petrik21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Vladimír
    family: Petrík
  - given: Makarand
    family: Tapaswi
  - given: Ivan
    family: Laptev
  - given: Josef
    family: Sivic
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 296-312
  id: petrik21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 296
  lastpage: 312
  published: 2021-10-04 00:00:00 +0000
- title: 'Integrating Egocentric Localization for More Realistic Point-Goal Navigation Agents'
  abstract: 'Recent work has presented embodied agents that can navigate to point-goal targets in novel indoor environments with near-perfect accuracy. However, these agents are equipped with idealized sensors for localization and take deterministic actions. This setting is practically sterile by comparison to the dirty reality of noisy sensors and actuations in the real world – wheels can slip, motion sensors have error, actuations can rebound. In this work, we take a step towards this noisy reality, developing point-goal navigation agents that rely on visual estimates of egomotion under noisy action dynamics. We find these agents outperform naive adaptions of current point-goal agents to this setting as well as those incorporating classic localization baselines. Further, our model conceptually divides learning agent dynamics or odometry (where am I?) from task-specific navigation policy (where do I want to go?). This enables a seamless adaption to changing dynamics (a different robot or floor type) by simply re-calibrating the visual odometry model – circumventing the expense of re-training of the navigation policy. Our agent was the runner-up in the PointNav track of CVPR 2020 Habitat Challenge.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/datta21a.html
  PDF: https://proceedings.mlr.press/v155/datta21a/datta21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-datta21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Samyak
    family: Datta
  - given: Oleksandr
    family: Maksymets
  - given: Judy
    family: Hoffman
  - given: Stefan
    family: Lee
  - given: Dhruv
    family: Batra
  - given: Devi
    family: Parikh
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 313-328
  id: datta21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 313
  lastpage: 328
  published: 2021-10-04 00:00:00 +0000
- title: 'PLOP: Probabilistic Polynomial Objects trajectory Prediction for autonomous driving'
  abstract: 'To navigate safely in urban environments, an autonomous vehicle (ego vehicle) must understand and anticipate its surroundings, in particular the behavior and intents of other road users (neighbors). Most of the times, multiple decision choices are acceptable for all road users (e.g., turn right or left, or different ways of avoiding an obstacle), leading to a highly uncertain and multi-modal decision space. We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework. We rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., “turn right”). Our model processes ego vehicle front-facing camera images and bird-eye view grid, computed from Lidar point clouds, with detections of past and present objects, in order to generate multiple trajectories for both ego vehicle and its neighbors. Our approach is computationally efficient and relies only on on-board sensors. We evaluate our method offline on the publicly available dataset nuScenes, achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/buhet21a.html
  PDF: https://proceedings.mlr.press/v155/buhet21a/buhet21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-buhet21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Thibault
    family: Buhet
  - given: Emilie
    family: Wirbel
  - given: Andrei
    family: Bursuc
  - given: Xavier
    family: Perrotton
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 329-338
  id: buhet21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 329
  lastpage: 338
  published: 2021-10-04 00:00:00 +0000
- title: 'Reinforcement Learning with Videos: Combining Offline Observations with Interaction'
  abstract: ' Reinforcement learning is a powerful framework for robots to acquire skills from experience, but often requires a substantial amount of online data collection. As a result, it is difficult to collect sufficiently diverse experiences that are needed for robots to generalize broadly. Videos of humans, on the other hand, are a readily available source of broad and interesting experiences. In this paper, we consider the question: can we perform reinforcement learning directly on experience collected by humans? This problem is particularly difficult, as such videos are not annotated with actions and exhibit substantial visual domain shift relative to the robot’s embodiment. To address these challenges, we propose a framework for reinforcement learning with videos (RLV). RLV learns a policy and value function using experience collected by humans in combination with data collected by robots. In our experiments, we find that RLV is able to leverage such videos to learn challenging vision-based skills with less than half as many samples as RL methods that learn from scratch.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/schmeckpeper21a.html
  PDF: https://proceedings.mlr.press/v155/schmeckpeper21a/schmeckpeper21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-schmeckpeper21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Karl
    family: Schmeckpeper
  - given: Oleh
    family: Rybkin
  - given: Kostas
    family: Daniilidis
  - given: Sergey
    family: Levine
  - given: Chelsea
    family: Finn
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 339-354
  id: schmeckpeper21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 339
  lastpage: 354
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Obstacle Representations for Neural Motion Planning'
  abstract: 'Motion planning and obstacle avoidance is a key challenge in robotics applications. While previous work succeeds to provide excellent solutions for known environments, sensor-based motion planning in new and dynamic environments remains to be  difficult. In this work we address sensor-based motion planning from a learning perspective. Motivated by recent advances in visual recognition, we argue the importance of learning appropriate representations for motion planning. We propose a new obstacle representation based on the PointNet architecture and learn it jointly with policies for obstacle avoidance. We experimentally evaluate our approach for rigid body motion planning in challenging environments and demonstrate significant improvements of the state of the art in terms of accuracy and efficiency.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/strudel21a.html
  PDF: https://proceedings.mlr.press/v155/strudel21a/strudel21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-strudel21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Robin
    family: STRUDEL
  - given: Ricardo Garcia
    family: Pinel
  - given: Justin
    family: Carpentier
  - given: Jean-Paul
    family: Laumond
  - given: Ivan
    family: Laptev
  - given: Cordelia
    family: Schmid
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 355-364
  id: strudel21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 355
  lastpage: 364
  published: 2021-10-04 00:00:00 +0000
- title: 'CLOUD: Contrastive Learning of Unsupervised Dynamics'
  abstract: 'Developing agents that can perform complex control tasks from high dimensional observations such as pixels is challenging due to difficulties in learning dynamics efficiently. In this work, we propose to learn forward and inverse dynamics in a fully unsupervised manner via contrastive estimation. Specifically, we train a forward dynamics model and an inverse dynamics model in the feature space of states and actions with data collected from random exploration. Unlike most existing deterministic models, our energy-based model takes into account the stochastic nature of agent-environment interactions. We demonstrate the efficacy of our approach across a variety of tasks including goal-directed planning and imitation from observations.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wang21c.html
  PDF: https://proceedings.mlr.press/v155/wang21c/wang21c.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wang21c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jianren
    family: Wang
  - given: Yujie
    family: Lu
  - given: Hang
    family: Zhao
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 365-376
  id: wang21c
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 365
  lastpage: 376
  published: 2021-10-04 00:00:00 +0000
- title: 'Exploratory Grasping: Asymptotically Optimal Algorithms for Grasping Challenging Polyhedral Objects'
  abstract: 'There has been significant recent work on data-driven algorithms for learning general-purpose grasping policies.  However, these policies can consistently fail to grasp challenging objects which are significantly out of the distribution of objects in the training data or which have very few high quality grasps. Motivated by such objects, we propose a novel problem setting, Exploratory Grasping, for efficiently discovering reliable grasps on an unknown polyhedral object via sequential grasping, releasing, and toppling. We formalize Exploratory Grasping as a Markov Decision Process where we assume that the robot can (1) distinguish stable poses of a polyhedral object of unknown geometry, (2) generate grasp candidates on these poses and execute them, (3) determine whether each grasp is successful, and (4) release the object into a random new pose after a grasp successor topple the object after a grasp failure.  We study the theoretical complexity of Exploratory Grasping in the context of reinforcement learning and present an efficient bandit-style algorithm, Bandits for Online Rapid Grasp ExplorationStrategy (BORGES), which leverages the structure of the problem to efficiently discover high performing grasps for each object stable pose. BORGES can be used to complement any general-purpose grasping algorithm with any grasp modality (parallel-jaw, suction, multi-fingered, etc) to learn policies for objects in which they exhibit persistent failures. Simulation experiments suggest that BORGES can significantly outperform both general-purpose grasping pipelines and two other online learning algorithms and achieves performance within 5% of the optimal policy within 1000 and 8000 timesteps on average across 46 challenging objects from the Dex-Net adversarial and EGAD! object datasets, respectively. Initial physical experiments suggesting that BORGES can improve grasp success rate on average over two challenging 3D printed objects by 45% over a Dex-Net baseline. See https://tinyurl.com/exp-grasping for supplementary material and videos.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/danielczuk21a.html
  PDF: https://proceedings.mlr.press/v155/danielczuk21a/danielczuk21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-danielczuk21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Michael
    family: Danielczuk
  - given: Ashwin
    family: Balakrishna
  - given: Daniel
    family: Brown
  - given: Ken
    family: Goldberg
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 377-393
  id: danielczuk21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 377
  lastpage: 393
  published: 2021-10-04 00:00:00 +0000
- title: 'Attention-Privileged Reinforcement Learning'
  abstract: 'Image-based Reinforcement Learning is known to suffer from poor sample efficiency and generalisation to unseen visuals such as distractors (task-independent aspects of the observation space). Visual domain randomisation encourages transfer by training over visual factors of variation that may be encountered in the target domain. This increases learning complexity, can negatively impact learning rate and performance, and requires knowledge of potential variations during deployment. In this paper, we introduce Attention-Privileged Reinforcement Learning (APRiL) which uses a self-supervised attention mechanism to significantly alleviate these drawbacks: by focusing on task-relevant aspects of the observations, attention provides robustness to distractors as well as significantly increased learning efficiency. APRiL trains two attention-augmented actor-critic agents: one purely based on image observations, available across training and transfer domains; and one with access to privileged information (such as environment states) available only during training. Experience is shared between both agents and their attention mechanisms are aligned. The image-based policy can then be deployed without access to privileged information. We experimentally demonstrate accelerated and more robust learning on a diverse set of domains, leading to improved final performance for environments both within and outside the training distribution.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/salter21a.html
  PDF: https://proceedings.mlr.press/v155/salter21a/salter21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-salter21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sasha
    family: Salter
  - given: Dushyant
    family: Rao
  - given: Markus
    family: Wulfmeier
  - given: Raia
    family: Hadsell
  - given: Ingmar
    family: Posner
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 394-408
  id: salter21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 394
  lastpage: 408
  published: 2021-10-04 00:00:00 +0000
- title: 'One Thousand and One Hours: Self-driving Motion Prediction Dataset'
  abstract: 'Motivated by the impact of large-scale datasets on ML systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labelled elements and a high-definition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/houston21a.html
  PDF: https://proceedings.mlr.press/v155/houston21a/houston21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-houston21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: John
    family: Houston
  - given: Guido
    family: Zuidhof
  - given: Luca
    family: Bergamini
  - given: Yawei
    family: Ye
  - given: Long
    family: Chen
  - given: Ashesh
    family: Jain
  - given: Sammy
    family: Omari
  - given: Vladimir
    family: Iglovikov
  - given: Peter
    family: Ondruska
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 409-418
  id: houston21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 409
  lastpage: 418
  published: 2021-10-04 00:00:00 +0000
- title: 'Recovering and Simulating Pedestrians in the Wild'
  abstract: 'Sensor simulation is a key component for testing the performance of self-driving vehicles and for data augmentation to better train perception systems. Typical approaches rely on artists to create both 3D assets and their animations to generate a new scenario. This, however, does not scale. In contrast, we propose to recover the shape and motion of pedestrians from sensor readings captured in the wild by a self-driving car driving around. Towards this goal, we formulate the problem as energy minimization in a deep structured model that exploits human shape priors, reprojection consistency with 2D poses extracted from images, and a ray-caster that encourages the reconstructed mesh to agree with the LiDAR readings. Importantly, we do not require any ground-truth 3D scans or 3D pose annotations. We then incorporate the reconstructed pedestrian assets bank in a realistic LiDAR simulation system by performing motion retargeting, and show that the simulated LiDAR data can be used to significantly reduce the amount of annotated real-world data required for visual perception tasks.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/yang21a.html
  PDF: https://proceedings.mlr.press/v155/yang21a/yang21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-yang21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ze
    family: Yang
  - given: Sivabalan
    family: Manivasagam
  - given: Ming
    family: Liang
  - given: Bin
    family: Yang
  - given: Wei-Chiu
    family: Ma
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 419-431
  id: yang21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 419
  lastpage: 431
  published: 2021-10-04 00:00:00 +0000
- title: 'SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object Manipulation'
  abstract: 'Manipulating deformable objects has long been a challenge in robotics due to its high dimensional state representation and complex dynamics. Recent success in deep reinforcement learning provides a promising direction for learning to manipulate deformable objects with data driven methods. However, existing reinforcement learning benchmarks only cover tasks with direct state observability and simple low-dimensional dynamics or with relatively simple image-based environments, such as those with rigid objects. In this paper, we present SoftGym, a set of open-source simulated benchmarks for manipulating deformable objects, with a standard OpenAI Gym API and a Python interface for creating new environments. Our benchmark will enable reproducible research in this important area. Further, we evaluate a variety of algorithms on these tasks and highlight challenges for reinforcement learning algorithms, including dealing with a state representation that has a high intrinsic dimensionality and is partially observable. The experiments and analysis indicate the strengths and limitations of existing methods in the context of deformable object manipulation that can help point the way forward for future methods development.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/lin21a.html
  PDF: https://proceedings.mlr.press/v155/lin21a/lin21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-lin21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xingyu
    family: Lin
  - given: Yufei
    family: Wang
  - given: Jake
    family: Olkin
  - given: David
    family: Held
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 432-448
  id: lin21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 432
  lastpage: 448
  published: 2021-10-04 00:00:00 +0000
- title: 'S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via Multi-View Consistency'
  abstract: 'A robot’s ability to act is fundamentally constrained by what it can perceive. Many existing approaches to visual representation learning utilize general-purpose training criteria, e.g. image reconstruction, smoothness in latent space, or usefulness for control, or else make use of large datasets annotated with specific features (bounding boxes, segmentations, etc.). However, both approaches often struggle to capture the fine-detail required for precision tasks on specific objects, e.g. grasping and mating a plug and socket. We argue that these difficulties arise from a lack of geometric structure in these models. In this work we advocate semantic 3D keypoints as a visual representation, and present a semi-supervised training objective that can allow instance or category-level keypoints to be trained to 1-5 millimeter-accuracy with minimal supervision. Furthermore, unlike local texture-based approaches, our model integrates contextual information from a large area and is therefore robust to occlusion, noise, and lack of discernible texture. We demonstrate that this ability to locate semantic keypoints enables high level scripting of human understandable behaviours. Finally we show that these keypoints provide a good way to define reward functions for reinforcement learning and area good representation for training agents.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/vecerik21a.html
  PDF: https://proceedings.mlr.press/v155/vecerik21a/vecerik21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-vecerik21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Mel
    family: Vecerik
  - given: Jean-Baptiste
    family: Regli
  - given: Oleg
    family: Sushkov
  - given: David
    family: Barker
  - given: Rugile
    family: Pevceviciute
  - given: Thomas
    family: Rothörl
  - given: Raia
    family: Hadsell
  - given: Lourdes
    family: Agapito
  - given: Jonathan
    family: Scholz
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 449-460
  id: vecerik21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 449
  lastpage: 460
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation'
  abstract: 'Segmenting unseen objects in cluttered scenes is an important skill that robots need to acquire in order to perform tasks in new environments. In this work, we propose a new method for unseen object instance segmentation by learning RGB-D feature embeddings from synthetic data. A metric learning loss function is utilized to learn to produce pixel-wise feature embeddings such that pixels from the same object are close to each other and pixels from different objects are separated in the embedding space. With the learned feature embeddings, a mean shift clustering algorithm can be applied to discover and segment unseen objects. We further improve the segmentation accuracy with a new two-stage clustering algorithm. Our method demonstrates that non-photorealistic synthetic RGB and depth images can be used to learn feature embeddings that transfer well to real-world images for unseen object instance segmentation.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/xiang21a.html
  PDF: https://proceedings.mlr.press/v155/xiang21a/xiang21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-xiang21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yu
    family: Xiang
  - given: Christopher
    family: Xie
  - given: Arsalan
    family: Mousavian
  - given: Dieter
    family: Fox
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 461-470
  id: xiang21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 461
  lastpage: 470
  published: 2021-10-04 00:00:00 +0000
- title: 'Modeling Long-horizon Tasks as Sequential Interaction Landscapes'
  abstract: 'Task planning over long-time horizons is a challenging and open problem in robotics and its complexity grows exponentially with an increasing number of subtasks. In this paper we present a deep neural network that learns dependencies and transitions across subtasks solely from a set of demonstration videos. We represent each subtasks as action symbols (e.g. move cup), and show that these symbols can be learned and predicted directly from image observations. Learning symbol sequences provides the network with additional information about the most frequent transitions and relevant dependencies between subtasks and thereby structures tasks over long-time horizons. Learning from images, on the other hand, allows the network to continuously monitor the task progress and thus to interactively adapt to changes in the environment. We evaluate our framework on two long horizon tasks: (1) block stacking of puzzle pieces being executed by humans, and (2) a robot manipulation task involving pick and place of objects and sliding a cabinet door with  a 7-DoF robot arm. We show that complex plans can be carried out when executing the robotic task and the robot can interactively adapt to changes in the environment and recover from failure cases.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/pirk21a.html
  PDF: https://proceedings.mlr.press/v155/pirk21a/pirk21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-pirk21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Soeren
    family: Pirk
  - given: Karol
    family: Hausman
  - given: Alexander
    family: Toshev
  - given: Mohi
    family: Khansari
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 471-484
  id: pirk21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 471
  lastpage: 484
  published: 2021-10-04 00:00:00 +0000
- title: 'PixL2R: Guiding Reinforcement Learning Using Natural Language by Mapping Pixels to Rewards'
  abstract: 'Reinforcement learning (RL), particularly in sparse reward settings, often requires prohibitively large numbers of interactions with the environment, thereby limiting its applicability to complex problems. To address this, several prior approaches have used natural language to guide the agent’s exploration. However, these approaches typically operate on structured representations of the environment, and/or assume some structure in the natural language commands. In this work, we propose a model that directly maps pixels to rewards, given a free-form natural language description of the task, which can then be used for policy learning. Our experiments on the Meta-World robot manipulation domain show that  language-based rewards significantly improves the sample efficiency of policy learning, both in sparse and dense reward settings.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/goyal21a.html
  PDF: https://proceedings.mlr.press/v155/goyal21a/goyal21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-goyal21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Prasoon
    family: Goyal
  - given: Scott
    family: Niekum
  - given: Raymond
    family: Mooney
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 485-497
  id: goyal21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 485
  lastpage: 497
  published: 2021-10-04 00:00:00 +0000
- title: 'Auxiliary Tasks Speed Up Learning Point Goal Navigation'
  abstract: 'PointGoal Navigation is an embodied task that requires agents to navigate to a specified point in an unseen environment. Wijmans et al. showed that this task is solvable in simulation but their method is computationally prohibitive - requiring 2.5 billion frames of experience and 180 GPU-days. We develop a method to significantly improve sample efficiency in learning PointNav using self-supervised auxiliary tasks (e.g. predicting the action taken between two egocentric observations, predicting the distance between two observations from a trajectory, etc.). We find that naively combining multiple auxiliary tasks improves sample efficiency, but only provides marginal gains beyond a point. To overcome this, we use attention to combine representations from individual auxiliary tasks. Our best agent is 5.5x faster to match the performance of the previous state-of-the-art, DD-PPO, at 40M frames, and improves on DD-PPO’s performance at 40M frames by 0.16 SPL. Our code is publicly available at github.com/joel99/habitat-pointnav-aux.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ye21a.html
  PDF: https://proceedings.mlr.press/v155/ye21a/ye21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ye21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Joel
    family: Ye
  - given: Dhruv
    family: Batra
  - given: Erik
    family: Wijmans
  - given: Abhishek
    family: Das
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 498-516
  id: ye21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 498
  lastpage: 516
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning hierarchical relationships for object-goal navigation'
  abstract: 'Direct search for objects as part of navigation poses a challenge for small items. Utilizing context in the form of object-object relationships enable hierarchical search for targets efficiently. Most of the current approaches tend to directly incorporate sensory input into a reward-based learning approach, without learning about object relationships in the natural environment, and thus generalize poorly across domains. We present Memory-utilized Joint hierarchical Object Learning for Navigation in Indoor Rooms (MJOLNIR), a target-driven navigation algorithm, which considers the inherent relationship between target objects, and the more salient contextual objects occurring in its surrounding. Extensive experiments conducted across multiple environment settings show an 82.9% and 93.5% gain over existing state-of-the-art navigation methods in terms of the success rate (SR), and success weighted by path length (SPL), respectively. We also show that our model learns to converge much faster than other algorithms, without suffering from the well-known overfitting problem. Additional details regarding the supplementary material and code are available at https://sites.google.com/eng.ucsd.edu/mjolnir.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/pal21a.html
  PDF: https://proceedings.mlr.press/v155/pal21a/pal21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-pal21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Anwesan
    family: Pal
  - given: Yiding
    family: Qiu
  - given: Henrik
    family: Christensen
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 517-528
  id: pal21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 517
  lastpage: 528
  published: 2021-10-04 00:00:00 +0000
- title: 'f-IRL: Inverse Reinforcement Learning via State Marginal Matching'
  abstract: 'Imitation learning is well-suited for robotic tasks where it is difficult to directly program the behavior or specify a cost for optimal control.  In this work, we propose a method for learning the reward function (and the corresponding policy) to match the expert state density. Our main result is the analytic gradient of any f-divergence between the agent and expert state distribution w.r.t. reward parameters. Based on the derived gradient, we present an algorithm, f-IRL, that recovers a stationary reward function from the expert density by gradient descent. We show that f-IRL can learn behaviors from a hand-designed target state density or implicitly through expert observations. Our method outperforms adversarial imitation learning methods in terms of sample efficiency and the required number of expert trajectories on IRL benchmarks. Moreover, we show that the recovered reward function can be used to quickly solve downstream tasks, and empirically demonstrate its utility on hard-to-explore tasks and for behavior transfer across changes in dynamics. Project videos and code link are available at https://sites.google.com/view/f-irl/home.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ni21a.html
  PDF: https://proceedings.mlr.press/v155/ni21a/ni21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ni21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tianwei
    family: Ni
  - given: Harshit
    family: Sikchi
  - given: Yufei
    family: Wang
  - given: Tejus
    family: Gupta
  - given: Lisa
    family: Lee
  - given: Ben
    family: Eysenbach
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 529-551
  id: ni21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 529
  lastpage: 551
  published: 2021-10-04 00:00:00 +0000
- title: 'Iterative Semi-parametric Dynamics Model Learning For Autonomous Racing'
  abstract: 'Accurately modeling robot dynamics is crucial to safe and efficient motion control. In this paper, we develop and apply an iterative learning semi-parametric model, with a neural network, to the task of autonomous racing with a Model Predictive  Controller (MPC). We present a novel non-linear semi-parametric dynamics model where we represent the known dynamics with a parametric model, and a neural network captures the unknown dynamics. We show that our model can learn more accurately than a purely parametric model and generalize better than a purely non-parametric model, making it ideal for real-world applications where collecting data from the full state space is not feasible. We present a system where the model is bootstrapped on pre-recorded data and then updated iteratively at run time. Then we apply our iterative learning approach to the simulated problem of autonomous racing and show that it can safely adapt to modified dynamics online and even achieve better performance than models trained on data from manual driving.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/georgiev21a.html
  PDF: https://proceedings.mlr.press/v155/georgiev21a/georgiev21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-georgiev21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ignat
    family: Georgiev
  - given: Christoforos
    family: Chatzikomis
  - given: Timo
    family: Voelkl
  - given: Joshua
    family: Smith
  - given: Michael
    family: Mistry
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 552-563
  id: georgiev21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 552
  lastpage: 563
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Predictive Representations for Deformable Objects Using Contrastive Estimation'
  abstract: 'Using visual model-based learning for deformable object manipulation is challenging due to difficulties in learning plannable visual representations along with complex dynamic models. In this work, we propose a new learning framework that jointly optimizes both the visual representation model and the dynamics model using contrastive estimation. Using simulation data collected by randomly perturbing deformable objects on a table, we learn latent dynamics models for these objects in an offline fashion. Then, using the learned models, we use simple model-based planning to solve challenging deformable object manipulation tasks such as spreading ropes and cloths. Experimentally, we show substantial improvements in performance over standard model-based learning techniques across our rope and cloth manipulation suite. Finally, we transfer our visual manipulation policies trained on data purely collected in simulation to a real PR2 robot through domain randomization.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/yan21a.html
  PDF: https://proceedings.mlr.press/v155/yan21a/yan21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-yan21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Wilson
    family: Yan
  - given: Ashwin
    family: Vangipuram
  - given: Pieter
    family: Abbeel
  - given: Lerrel
    family: Pinto
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 564-574
  id: yan21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 564
  lastpage: 574
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Latent Representations to Influence Multi-Agent Interaction'
  abstract: 'Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent’s behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent’s policy, where the ego agent identifies the relationship between its behavior and the other agent’s future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/xie21a.html
  PDF: https://proceedings.mlr.press/v155/xie21a/xie21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-xie21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Annie
    family: Xie
  - given: Dylan
    family: Losey
  - given: Ryan
    family: Tolsma
  - given: Chelsea
    family: Finn
  - given: Dorsa
    family: Sadigh
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 575-588
  id: xie21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 575
  lastpage: 588
  published: 2021-10-04 00:00:00 +0000
- title: 'Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments'
  abstract: 'Deep reinforcement learning (RL) agents are able to learn contact-rich manipulation tasks by maximizing a reward signal, but require large amounts of experience, especially in environments with many obstacles that complicate exploration. In contrast, motion planners use explicit models of the agent and environment to plan collision-free paths to faraway goals, but suffer from inaccurate models in tasks that require contacts with the environment. To combine the benefits of both approaches, we propose motion planner augmented RL (MoPA-RL) which augments the action space of an RL agent with the long-horizon planning capabilities of motion planners. Based on the magnitude of the action, our approach smoothly transitions between directly executing the action and invoking a motion planner. We evaluate our approach on various simulated manipulation tasks and compare it to alternative action spaces in terms of learning efficiency and safety. The experiments demonstrate that MoPA-RL increases learning efficiency, leads to a faster exploration, and results in safer policies that avoid collisions with the environment. Videos and code are available at https://clvrai.com/mopa-rl.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/yamada21a.html
  PDF: https://proceedings.mlr.press/v155/yamada21a/yamada21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-yamada21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jun
    family: Yamada
  - given: Youngwoon
    family: Lee
  - given: Gautam
    family: Salhotra
  - given: Karl
    family: Pertsch
  - given: Max
    family: Pflueger
  - given: Gaurav
    family: Sukhatme
  - given: Joseph
    family: Lim
  - given: Peter
    family: Englert
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 589-603
  id: yamada21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 589
  lastpage: 603
  published: 2021-10-04 00:00:00 +0000
- title: 'The EMPATHIC Framework for Task Learning from Implicit Human Feedback'
  abstract: 'Reactions such as gestures, facial expressions, and vocalizations are an abundant, naturally occurring channel of information that humans provide during interactions. A robot or other agent could leverage an understanding of such implicit human feedback to improve its task performance at no cost to the human. This approach contrasts with common agent teaching methods based on demonstrations, critiques, or other guidance that need to be attentively and intentionally provided. In this paper, we first define the general problem of learning from implicit human feedback and then propose to address this problem through a novel data-driven framework, EMPATHIC. This two-stage method consists of (1) mapping implicit human feedback to relevant task statistics such as reward, optimality, and advantage; and (2) using such a mapping to learn a task. We instantiate the first stage and three second-stage evaluations of the learned mapping. To do so, we collect a dataset of human facial reactions while subjects observe an agent execute a sub-optimal policy for a prescribed training task. We train a deep neural network on this data and demonstrate its ability to (1) infer relative reward ranking of events in the training task from prerecorded human facial reactions; (2) improve the policy of an agent in the training task using live human facial reactions; and (3) transfer to a novel domain in which it evaluates robot manipulation trajectories.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/cui21a.html
  PDF: https://proceedings.mlr.press/v155/cui21a/cui21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-cui21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuchen
    family: Cui
  - given: Qiping
    family: Zhang
  - given: Brad
    family: Knox
  - given: Alessandro
    family: Allievi
  - given: Peter
    family: Stone
  - given: Scott
    family: Niekum
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 604-626
  id: cui21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 604
  lastpage: 626
  published: 2021-10-04 00:00:00 +0000
- title: 'Range Conditioned Dilated Convolutions for Scale Invariant 3D Object Detection'
  abstract: 'This paper presents a novel 3D object detection framework that processes LiDAR data directly on its native representation: range images. Benefiting from the compactness of range images, 2D convolutions can efficiently process dense LiDAR data of the scene. To overcome scale sensitivity in this perspective view, a novel range-conditioned dilation (RCD) layer is proposed to dynamically adjust a continuous dilation rate as a function of the measured range. Furthermore, localized soft range gating combined with a 3D box-refinement stage improves robustness in occluded areas, and produces overall more accurate bounding box predictions. On the public large-scale Waymo Open Dataset, our method sets a new baseline for range-based 3D detection, outperforming multiview and voxel-based methods over all ranges with unparalleled performance at long range detection.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/bewley21a.html
  PDF: https://proceedings.mlr.press/v155/bewley21a/bewley21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-bewley21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Alex
    family: Bewley
  - given: Pei
    family: Sun
  - given: Thomas
    family: Mensink
  - given: Dragomir
    family: Anguelov
  - given: Cristian
    family: Sminchisescu
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 627-641
  id: bewley21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 627
  lastpage: 641
  published: 2021-10-04 00:00:00 +0000
- title: 'High Acceleration Reinforcement Learning for Real-World Juggling with Binary Rewards'
  abstract: 'Robots that can learn in the physical world will be important to enable robots to escape their stiff and pre-programmed movements.  For dynamic high-acceleration tasks, such as juggling, learning in the real-world is particularly challenging  as  one  must  push  the  limits  of  the  robot  and  its  actuation  without harming the system, amplifying the necessity of sample efficiency and safety for robot learning algorithms.  In contrast to prior work which mainly focuses on the learning algorithm, we propose a learning system, that directly incorporates these requirements in the design of the policy representation,  initialization,  and optimization.  We demonstrate that this system enables the high-speed Barrett WAM manipulator to learn juggling two balls from 56 minutes of experience with a binary reward signal and finally juggles continuously for up to 33 minutes or about 4500 repeated catches. The videos documenting the learning process and the evaluation can be found at https://sites.google.com/view/jugglingbot'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ploeger21a.html
  PDF: https://proceedings.mlr.press/v155/ploeger21a/ploeger21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ploeger21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Kai
    family: Ploeger
  - given: Michael
    family: Lutter
  - given: Jan
    family: Peters
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 642-653
  id: ploeger21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 642
  lastpage: 653
  published: 2021-10-04 00:00:00 +0000
- title: 'Guaranteeing Safety of Learned Perception Modules via Measurement-Robust Control Barrier Functions'
  abstract: 'Modern nonlinear control theory seeks to develop feedback controllers that endow systems with properties such as safety and stability. The guarantees ensured by these controllers often rely on accurate estimates of the system state for determining control actions. In practice, measurement model uncertainty can lead to error in state estimates that degrades these guarantees. In this paper, we seek to unify techniques from control theory and machine learning to synthesize controllers that achieve safety in the presence of measurement model uncertainty. We define the notion of a Measurement-Robust Control Barrier Function (MR-CBF) as a tool for determining safe control inputs when facing measurement model uncertainty. Furthermore, MR-CBFs are used to inform sampling methodologies for learning-based perception systems and quantify tolerable error in the resulting learned models. We demonstrate the efficacy of MR-CBFs in achieving safety with measurement model uncertainty on a simulated Segway system.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/dean21a.html
  PDF: https://proceedings.mlr.press/v155/dean21a/dean21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-dean21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sarah
    family: Dean
  - given: Andrew
    family: Taylor
  - given: Ryan
    family: Cosner
  - given: Benjamin
    family: Recht
  - given: Aaron
    family: Ames
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 654-670
  id: dean21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 654
  lastpage: 670
  published: 2021-10-04 00:00:00 +0000
- title: 'Sim-to-Real Transfer for Vision-and-Language Navigation'
  abstract: 'We study the challenging problem of releasing a robot in a previously unseen environment, and having it follow unconstrained natural language navigation instructions. Recent work on the task of Vision-and-Language Navigation (VLN) has achieved significant progress in simulation. To assess the implications of this work for robotics, we transfer a VLN agent trained in simulation to a physical robot. To bridge the gap between the high-level discrete action space learned by the VLN agent, and the robot’s low-level continuous action space, we propose a subgoal model to identify nearby waypoints, and use domain randomization to mitigate visual domain differences. For accurate sim and real comparisons in parallel environments, we annotate a 325m2 office space with 1.3km of navigation instructions, and create a digitized replica in simulation. We find that sim-to-real transfer to an environment not seen in training is successful if an occupancy map and navigation graph can be collected and annotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more challenging in the hardest setting with no prior mapping at all (success rate of 22.5%).'
  volume: 155
  URL: https://proceedings.mlr.press/v155/anderson21a.html
  PDF: https://proceedings.mlr.press/v155/anderson21a/anderson21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-anderson21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Peter
    family: Anderson
  - given: Ayush
    family: Shrivastava
  - given: Joanne
    family: Truong
  - given: Arjun
    family: Majumdar
  - given: Devi
    family: Parikh
  - given: Dhruv
    family: Batra
  - given: Stefan
    family: Lee
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 671-681
  id: anderson21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 671
  lastpage: 681
  published: 2021-10-04 00:00:00 +0000
- title: 'Interactive Imitation Learning in State-Space'
  abstract: 'Imitation Learning techniques enable programming the behaviour of agents through demonstrations rather than manual engineering. However, they are limited by the quality of available demonstration data. Interactive Imitation Learning techniques can improve the efficacy of learning since they involve teachers providing feedback while the agent executes its task. In this work, we propose a novel Interactive Learning technique that uses human feedback in state-space to train and improve agent behaviour (as opposed to alternative methods that use feedback in action-space). Our method titled Teaching Imitative Policies in State-space (TIPS) enables providing guidance to the agent in terms of ‘changing its state’ which is often more intuitive for a human demonstrator. Through continuous improvement via corrective feedback, agents trained by non-expert demonstrators using TIPS outperformed the demonstrator and conventional Imitation Learning agents.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/jauhri21a.html
  PDF: https://proceedings.mlr.press/v155/jauhri21a/jauhri21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-jauhri21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Snehal
    family: Jauhri
  - given: Carlos
    family: Celemin
  - given: Jens
    family: Kober
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 682-692
  id: jauhri21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 682
  lastpage: 692
  published: 2021-10-04 00:00:00 +0000
- title: 'Keypoints into the Future: Self-Supervised Correspondence in Model-Based Reinforcement Learning'
  abstract: 'Predictive models have been at the core of many robotic systems, from quadrotors to walking robots. However, it has been challenging to develop and apply such models to practical robotic manipulation due to high-dimensional sensory observations such as images. Previous approaches to learning models in the context of robotic manipulation have either learned whole image dynamics or used autoencoders to learn dynamics in a low-dimensional latent state. In this work, we introduce model-based prediction with self-supervised visual correspondence learning, and show that not only is this indeed possible, but demonstrate that these types of predictive models show compelling performance improvements over alternative methods for vision-based RL with autoencoder-type vision training. Through simulation experiments, we demonstrate that our models provide better generalization precision, particularly in 3D scenes, scenes involving occlusion, and in category-generalization. Additionally, we validate that our method effectively transfers to the real world through hardware experiments. https://sites.google.com/view/keypointsintothefuture'
  volume: 155
  URL: https://proceedings.mlr.press/v155/manuelli21a.html
  PDF: https://proceedings.mlr.press/v155/manuelli21a/manuelli21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-manuelli21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Lucas
    family: Manuelli
  - given: Yunzhu
    family: Li
  - given: Pete
    family: Florence
  - given: Russ
    family: Tedrake
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 693-710
  id: manuelli21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 693
  lastpage: 710
  published: 2021-10-04 00:00:00 +0000
- title: 'Model-based Reinforcement Learning for Decentralized Multiagent Rendezvous'
  abstract: 'Collaboration requires agents to align their goals on the fly. Underlying the human ability to align goals with other agents is their ability to predict the intentions of others and actively update their own plans. We propose hierarchical predictive planning (HPP), a model-based reinforcement learning method for decentralized multiagent rendezvous. Starting with pretrained, single-agent point to point navigation policies  and using noisy, high-dimensional sensor inputs like lidar, we first learn via self-supervision motion predictions of all agents on the team. Next, HPP uses the prediction models to propose and evaluate navigation subgoals for completing the rendezvous task without explicit communication among agents. We evaluate HPP in a suite of unseen environments, with increasing complexity and numbers of obstacles. We show that HPP outperforms alternative reinforcement learning, path planning, and heuristic-based baselines on challenging, unseen environments. Experiments in the real world demonstrate successful transfer of the prediction models from sim to real world without any additional fine-tuning. Altogether, HPP removes the need for a centralized operator in multiagent systems by combining model-based RL and inference methods, enabling agents to dynamically align plans.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wang21d.html
  PDF: https://proceedings.mlr.press/v155/wang21d/wang21d.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wang21d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Rose
    family: Wang
  - given: J. Chase
    family: Kew
  - given: Dennis
    family: Lee
  - given: Tsang-Wei
    family: Lee
  - given: Tingnan
    family: Zhang
  - given: Brian
    family: Ichter
  - given: Jie
    family: Tan
  - given: Aleksandra
    family: Faust
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 711-725
  id: wang21d
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 711
  lastpage: 725
  published: 2021-10-04 00:00:00 +0000
- title: 'Transporter Networks: Rearranging the Visual World for Robotic Manipulation'
  abstract: 'Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input – which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code will be made available at https://transporternets.github.io'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zeng21a.html
  PDF: https://proceedings.mlr.press/v155/zeng21a/zeng21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zeng21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Andy
    family: Zeng
  - given: Pete
    family: Florence
  - given: Jonathan
    family: Tompson
  - given: Stefan
    family: Welker
  - given: Jonathan
    family: Chien
  - given: Maria
    family: Attarian
  - given: Travis
    family: Armstrong
  - given: Ivan
    family: Krasin
  - given: Dan
    family: Duong
  - given: Vikas
    family: Sindhwani
  - given: Johnny
    family: Lee
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 726-747
  id: zeng21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 726
  lastpage: 747
  published: 2021-10-04 00:00:00 +0000
- title: 'Assisted Perception: Optimizing Observations to Communicate State'
  abstract: 'We aim to help users estimate the state of the world in tasks like robotic teleoperation and navigation with visual impairment, where user may have systematic biases that lead to suboptimal behavior: they might struggle to process observations from multiple sensors simultaneously, receive delayed observations, or underestimate distances to obstacles. While we cannot directly change the user’s internal beliefs or their internal state estimation process, our insight is that we can still assist them by modifying the user’s observations. Instead of showing the user their true observations, ***we synthesize new observations that lead to more accurate internal state estimates when processed by the user***. We refer to this method as assistive state estimation (ASE): an automated assistant uses the true observations to infer the state of the world, then generates a modified observation for the user to consume (e.g., through an augmented reality interface), and optimizes the modification to induce the user’s new beliefs to match the assistant’s current beliefs. To predict the effect of the modified observation on the user’s beliefs, ASE learns a model of the user’s state estimation process: after each task completion, it searches for a model that would have led to beliefs that explain the user’s actions. We evaluate ASE in a user study with 12 participants who each perform four tasks: two tasks with known user biases – bandwidth-limited image classification and a driving video game with observation delay – and two with unknown biases that our method has to learn – guided 2D navigation and a lunar lander teleoperation video game. ASE’s general-purpose approach to synthesizing informative observations enables a different assistance strategy to emerge in each domain, such as quickly revealing informative pixels to speed up image classification, using a dynamics model to undo observation delay in driving, identifying nearby landmarks for navigation, and exaggerating a visual indicator of tilt in the lander game. The results show that ASE substantially improves the task performance of users with bandwidth constraints, observation delay, and other unknown biases.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/reddy21a.html
  PDF: https://proceedings.mlr.press/v155/reddy21a/reddy21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-reddy21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Siddharth
    family: Reddy
  - given: Sergey
    family: Levine
  - given: Anca
    family: Dragan
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 748-764
  id: reddy21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 748
  lastpage: 764
  published: 2021-10-04 00:00:00 +0000
- title: 'Action-Conditional Recurrent Kalman Networks For Forward and Inverse Dynamics Learning'
  abstract: 'Estimating accurate forward and inverse dynamics models is a crucial component of model-based control for sophisticated robots such as robots driven by hydraulics, artificial muscles, or robots dealing with different contact situations. Analytic models to such processes are often unavailable or inaccurate due to complex hysteresis effects, unmodelled friction and stiction phenomena, and unknown effects during contact situations. A promising approach is to obtain spatio-temporal models in a data-driven way using recurrent neural networks, as they can overcome those issues. However, such models often do not meet accuracy demands sufficiently, degenerate in performance for the required high sampling frequencies and cannot provide uncertainty estimates.  We adopt a recent probabilistic recurrent neural network architecture, called Recurrent Kalman Networks (RKNs), to model learning by conditioning its transition dynamics on the control actions. RKNs outperform standard recurrent networks such as LSTMs on many state estimation tasks.Inspired by Kalman filters, the RKN provides an elegant way to achieve action conditioning within its recurrent cell by leveraging additive interactions between the current latent state and the action variables. We present two architectures, one for forward model learning and one for inverse model learning. Both architectures significantly outperform existing model learning frameworks as well as analytical models in terms of prediction performance on a variety of real robot dynamics models.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/shaj21a.html
  PDF: https://proceedings.mlr.press/v155/shaj21a/shaj21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-shaj21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Vaisakh
    family: Shaj
  - given: Philipp
    family: Becker
  - given: Dieter
    family: Büchler
  - given: Harit
    family: Pandya
  - given: Niels van
    family: Duijkeren
  - given: C. James
    family: Taylor
  - given: Marc
    family: Hanheide
  - given: Gerhard
    family: Neumann
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 765-781
  id: shaj21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 765
  lastpage: 781
  published: 2021-10-04 00:00:00 +0000
- title: 'Untangling Dense Knots by Learning Task-Relevant Keypoints'
  abstract: 'Untangling ropes, wires, and cables is a challenging task for robots due to the high-dimensional configuration space, visual homogeneity, self-occlusions, and complex dynamics. We consider dense (tight) knots that lack space between self-intersections and present an iterative approach that uses learned geometric structure in configurations. We instantiate this into an algorithm, HULK: Hierarchical Untangling from Learned Keypoints, which combines learning-based perception with a geometric planner into a policy that guides a bilateral robot to untangle knots. To evaluate the policy, we perform experiments both in a novel simulation environment modelling cables with varied knot types and textures and in a physical system using the da Vinci surgical robot. We find that HULK is able to untangle cables with dense figure-eight and overhand knots and generalize to varied textures and appearances. We compare two variants of HULK to three baselines and observe that HULK achieves 43.3% higher success rates on a physical system compared to the next best baseline. HULK successfully untangles a cable from a dense initial configuration containing up to two overhand and figure-eight knots in 97.9% of 378 simulation experiments with an average of 12.1 actions per trial. In physical experiments, HULK achieves 61.7% untangling success, averaging 8.48 actions per trial. Supplementary material, code, and videos can be found at https://tinyurl.com/y3a88ycu.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/grannen21a.html
  PDF: https://proceedings.mlr.press/v155/grannen21a/grannen21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-grannen21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jennifer
    family: Grannen
  - given: Priya
    family: Sundaresan
  - given: Brijen
    family: Thananjeyan
  - given: Jeffrey
    family: Ichnowski
  - given: Ashwin
    family: Balakrishna
  - given: Vainavi
    family: Viswanath
  - given: Michael
    family: Laskey
  - given: Joseph
    family: Gonzalez
  - given: Ken
    family: Goldberg
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 782-800
  id: grannen21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 782
  lastpage: 800
  published: 2021-10-04 00:00:00 +0000
- title: 'Safe Policy Learning for Continuous Control'
  abstract: 'We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through near-safe policies, i.e., policies that keep the agent in desirable situations, both during training and at convergence. We formulate these problems as {\em constrained} Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while enforcing near-constraint satisfaction for every policy update by projecting either the policy parameter or the selected action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, in practice our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world robot obstacle-avoidance problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chow21a.html
  PDF: https://proceedings.mlr.press/v155/chow21a/chow21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chow21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yinlam
    family: Chow
  - given: Ofir
    family: Nachum
  - given: Aleksandra
    family: Faust
  - given: Edgar
    family: Dueñez-Guzman
  - given: Mohammad
    family: Ghavamzadeh
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 801-821
  id: chow21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 801
  lastpage: 821
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning to Compose Hierarchical Object-Centric Controllers for Robotic Manipulation'
  abstract: 'Manipulation tasks can often be decomposed into multiple subtasks performed in parallel, e.g., sliding an object to a goal pose while maintaining contact with a table.  Individual subtasks can be achieved by task-axis controllers defined relative to the objects being manipulated, and a set of object-centric controllers can be combined in an hierarchy. In prior works, such combinations are defined manually or learned from demonstrations. By contrast, we propose using reinforcement learning to dynamically compose hierarchical object-centric controllers for manipulation tasks. Experiments in both simulation and real world show how the proposed approach leads to improved sample efficiency, zero-shot generalization to novel test environments, and simulation-to-reality transfer without fine-tuning.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/sharma21a.html
  PDF: https://proceedings.mlr.press/v155/sharma21a/sharma21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-sharma21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Mohit
    family: Sharma
  - given: Jacky
    family: Liang
  - given: Jialiang
    family: Zhao
  - given: Alex
    family: Lagrassa
  - given: Oliver
    family: Kroemer
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 822-844
  id: sharma21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 822
  lastpage: 844
  published: 2021-10-04 00:00:00 +0000
- title: 'Relational Learning for Skill Preconditions'
  abstract: 'To determine if a skill can be executed in any given environment, a robot needs to learn the preconditions for the skill. As robots begin to operate in dynamic and unstructured environments, these precondition models will need to generalize to variable number of objects with different shapes and sizes. In this work, we focus on learning precondition models for manipulation skills in unconstrained environments. Our work is motivated by the intuition that many complex manipulation tasks, with multiple objects, can be simplified by focusing on less complex pairwise object relations. We propose an object-relation model that learns continuous representations for these pairwise object relations.  Our object-relation model is trained completely in simulation, and once learned, is used by a separate precondition model to predict skill preconditions for real world tasks. We evaluate our precondition model on 3 different manipulation tasks: sweeping, cutting, and unstacking.  We show that our approach leads to significant improvements in predicting preconditions for all 3 tasks, across objects of different shapes and sizes.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/sharma21b.html
  PDF: https://proceedings.mlr.press/v155/sharma21b/sharma21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-sharma21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Mohit
    family: Sharma
  - given: Oliver
    family: Kroemer
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 845-861
  id: sharma21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 845
  lastpage: 861
  published: 2021-10-04 00:00:00 +0000
- title: 'Social-VRNN: One-Shot Multi-modal Trajectory Prediction for Interacting Pedestrians'
  abstract: 'Prediction of human motions is key for safe navigation of autonomous robots among humans. In cluttered environments, several motion hypotheses may exist for a pedestrian, due to its interactions with the environment and other pedestrians. Previous works for estimating multiple motion hypotheses require a large number of samples which limits their applicability in real-time motion planning. In this paper, we present a variational learning approach for interaction-aware and multi-modal trajectory prediction based on deep generative neural networks. Our approach can achieve faster convergence and requires significantly fewer samples comparing to state-of-the-art methods. Experimental results on real and simulation data show that our model can effectively learn to infer different trajectories. We compare our method with three baseline approaches and present performance results demonstrating that our generative model can achieve higher accuracy for trajectory prediction by producing diverse trajectories.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/brito21a.html
  PDF: https://proceedings.mlr.press/v155/brito21a/brito21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-brito21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Bruno Ferreira de
    family: Brito
  - given: Hai
    family: Zhu
  - given: Wei
    family: Pan
  - given: Javier
    family: Alonso-Mora
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 862-872
  id: brito21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 862
  lastpage: 872
  published: 2021-10-04 00:00:00 +0000
- title: 'MuGNet: Multi-Resolution Graph Neural Network for Segmenting Large-Scale Pointclouds'
  abstract: 'In this paper, we propose a multi-resolution deep-learning architecture to segment dense large-scale pointclouds semantically. Dense pointcloud data require a computationally expensive feature encoding process before semantic segmentation. Previous work has used different approaches to drastically downsample from the original pointcloud to utilize common computing hardware. While these approaches can relieve the computation burden to some extent, they are still limited in their processing capability for multiple scans. We present MuGNet, a memory-efficient, end-to-end graph neural network framework to perform semantic segmentation on large-scale pointclouds. We reduce the computation demand by utilizing a graph neural network on the preformed pointcloud graphs and retain the segmentation’s precision with a bidirectional network that fuses feature embedding at different resolutions. Our framework has been validated on benchmark datasets, including Stanford Large-Scale 3D Indoor Spaces Dataset(S3DIS) and Virtual KITTI Dataset. We demonstrate that our framework can process up to 45 room scans at once on a single 11 GB GPU while still surpassing other graph-based solutions for segmentation on S3DIS with an 88.5% (+3%) overall accuracy and 69.8% (+7.7%) mIOU accuracy.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/xie21b.html
  PDF: https://proceedings.mlr.press/v155/xie21b/xie21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-xie21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Liuyue
    family: Xie
  - given: Tomotake
    family: Furuhata
  - given: Kenji
    family: Shimada
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 873-882
  id: xie21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 873
  lastpage: 882
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning a Contact-Adaptive Controller for Robust, Efficient Legged Locomotion'
  abstract: 'We present a hierarchical framework that combines model-based control and reinforcement learning (RL) to synthesize robust controllers for a quadruped (the Unitree Laikago). The system consists of a high-level controller that learns to choose from a set of primitives in response to changes in the environment and a low-level controller that utilizes an established control method to robustly execute the primitives. Our framework learns a controller that can adapt to challenging environmental changes on the fly, including novel scenarios not seen during training. The learned controller is up to 85 percent more energy efficient and is more robust compared to baseline methods. We also deploy the controller on a physical robot without any randomization or adaptation scheme.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/da21a.html
  PDF: https://proceedings.mlr.press/v155/da21a/da21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-da21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xingye
    family: Da
  - given: Zhaoming
    family: Xie
  - given: David
    family: Hoeller
  - given: Byron
    family: Boots
  - given: Anima
    family: Anandkumar
  - given: Yuke
    family: Zhu
  - given: Buck
    family: Babich
  - given: Animesh
    family: Garg
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 883-894
  id: da21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 883
  lastpage: 894
  published: 2021-10-04 00:00:00 +0000
- title: 'TNT: Target-driven Trajectory Prediction'
  abstract: 'Predicting the future behavior of moving agents is essential for real world applications. It is challenging as the intent of the agent and the corresponding behavior is unknown and intrinsically multimodal. Our key insight is that for prediction within a moderate time horizon, the future modes can be effectively captured by a set of target states. This leads to our target-driven trajectory prediction (TNT) framework. TNT has three stages which are trained end-to-end. It first predicts an agent’s potential target states T steps into the future, by encoding its interactions with the environment and the other agents. TNT then generates trajectory state sequences conditioned on targets. A final stage estimates trajectory likelihoods and a final compact set of trajectory predictions is selected. This is in contrast to previous work which models agent intents as latent variables, and relies on test-time sampling to generate diverse trajectories. We benchmark TNT on trajectory prediction of vehicles and pedestrians, where we achieve better than state-of-the-art performance on Argoverse Forecasting, INTERACTION, Stanford Drone and an in-house Pedestrian-at-Intersection dataset.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zhao21b.html
  PDF: https://proceedings.mlr.press/v155/zhao21b/zhao21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zhao21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Hang
    family: Zhao
  - given: Jiyang
    family: Gao
  - given: Tian
    family: Lan
  - given: Chen
    family: Sun
  - given: Ben
    family: Sapp
  - given: Balakrishnan
    family: Varadarajan
  - given: Yue
    family: Shen
  - given: Yi
    family: Shen
  - given: Yuning
    family: Chai
  - given: Cordelia
    family: Schmid
  - given: Congcong
    family: Li
  - given: Dragomir
    family: Anguelov
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 895-904
  id: zhao21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 895
  lastpage: 904
  published: 2021-10-04 00:00:00 +0000
- title: 'Planning Paths Through Unknown Space by Imagining What Lies Therein'
  abstract: 'This paper presents a novel framework for planning paths in maps containing unknown spaces, such as from occlusions. Our approach takes as input a semantically-annotated point cloud, and leverages an image inpainting neural network to generate a reasonable model of unknown space as free or occupied. Our validation campaign shows that it is possible to greatly increase the performance of standard pathfinding algorithms which adopt the general optimistic assumption of treating unknown space as free.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/han21a.html
  PDF: https://proceedings.mlr.press/v155/han21a/han21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-han21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yutao
    family: Han
  - given: Jacopo
    family: Banfi
  - given: Mark
    family: Campbell
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 905-914
  id: han21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 905
  lastpage: 914
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Dexterous Manipulation from Suboptimal Experts'
  abstract: 'Learning dexterous manipulation in high-dimensional state-action spaces is an important open challenge with exploration presenting a major bottleneck. Although in many cases the learning process could be guided by demonstrations or other suboptimal experts, current RL algorithms for continuous action spaces often fail to effectively utilize combinations of highly off-policy expert data and on-policy exploration data. As a solution, we introduce Relative Entropy Q-Learning (REQ), a simple policy iteration algorithm that combines ideas from successful offline and conventional RL algorithms. It represents the optimal policy via importance sampling from a learned prior and is well-suited to take advantage of mixed data distributions. We demonstrate experimentally that REQ outperforms several strong baselines on robotic manipulation tasks for which suboptimal experts are available. We show how suboptimal experts can be constructed effectively by composing simple waypoint tracking controllers, and we also show how learned primitives can be combined with waypoint controllers to obtain reference behaviors to bootstrap a complex manipulation task on a simulated bimanual robot with human-like hands. Finally, we show that REQ is also effective for general off-policy RL, offline RL, and RL from demonstrations.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/jeong21a.html
  PDF: https://proceedings.mlr.press/v155/jeong21a/jeong21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-jeong21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Rae
    family: Jeong
  - given: Jost Tobias
    family: Springenberg
  - given: Jackie
    family: Kay
  - given: Dan
    family: Zheng
  - given: Alexandre
    family: Galashov
  - given: Nicolas
    family: Heess
  - given: Francesco
    family: Nori
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 915-934
  id: jeong21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 915
  lastpage: 934
  published: 2021-10-04 00:00:00 +0000
- title: 'STReSSD: Sim-To-Real from Sound for Stochastic Dynamics'
  abstract: 'Sound is an information-rich medium that captures dynamic physical events. This work presents STReSSD, a framework that uses sound to bridge the simulation-to-reality gap for stochastic dynamics, demonstrated for the canonical case of a bouncing ball. A physically-motivated noise model is presented to capture stochastic behavior of the balls upon collision with the environment. A likelihood-free Bayesian inference framework is used to infer the parameters of the noise model, as well as a material property called the coefficient of restitution, from audio observations.  The same inference framework and the calibrated stochastic simulator are then used to learn a probabilistic model of ball dynamics.  The predictive capabilities of the dynamics model are tested in two robotic experiments. First, open-loop predictions anticipate probabilistic success of bouncing a ball into a cup. The second experiment integrates audio perception with a robotic arm to track and deflect a bouncing ball in real-time. We envision that this work is a step towards integrating audio-based inference for dynamic robotic tasks.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/matl21a.html
  PDF: https://proceedings.mlr.press/v155/matl21a/matl21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-matl21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Carolyn
    family: Matl
  - given: Yashraj
    family: Narang
  - given: Dieter
    family: Fox
  - given: Ruzena
    family: Bajcsy
  - given: Fabio
    family: Ramos
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 935-958
  id: matl21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 935
  lastpage: 958
  published: 2021-10-04 00:00:00 +0000
- title: 'Contrastive Variational Reinforcement Learning for Complex Observations'
  abstract: 'Deep reinforcement learning (DRL) has achieved significant success in various robot tasks: manipulation, navigation, etc. However, complex visual observations in natural environments remains a major challenge. This paper presents Contrastive Variational Reinforcement Learning (CVRL), a model-based method that tackles complex visual observations in  DRL.  CVRL learns a contrastive variational model by maximizing the mutual information between latent states and observations discriminatively, through contrastive learning. It avoids modeling the complex observation space unnecessarily, as the commonly used generative observation model often does,  and is significantly more robust. CVRL achieves comparable performance with state-of-the-art model-based DRL methods on standard Mujoco tasks. It significantly outperforms them on Natural Mujoco tasks and a robot box-pushing task with complex observations, e.g., dynamic shadows. The CVRL code is available publicly at https://github.com/Yusufma03/CVRL.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ma21a.html
  PDF: https://proceedings.mlr.press/v155/ma21a/ma21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ma21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xiao
    family: Ma
  - given: SIWEI
    family: CHEN
  - given: David
    family: Hsu
  - given: Wee Sun
    family: Lee
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 959-972
  id: ma21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 959
  lastpage: 972
  published: 2021-10-04 00:00:00 +0000
- title: 'Universal Embeddings for Spatio-Temporal Tagging of Self-Driving Logs'
  abstract: 'In this paper, we tackle the problem of spatio-temporal tagging of self-driving scenes from raw sensor data. Our approach learns a universal embedding for all tags, enabling efficient tagging of many attributes and faster learning of new attributes with limited data. Importantly, the embedding is spatio-temporally aware, allowing the model to naturally output spatio-temporal tag values. Values can then be pooled over arbitrary regions, in order to, for example, compute the pedestrian density in front of the SDV, or determine if a car is blocking another car at a 4-way intersection. We demonstrate the effectiveness of our approach on a new large scale self-driving dataset, SDVScenes, containing 15 attributes relating to vehicle and pedestrian density, the actions of each actor, the speed of each actor, interactions between actors, and the topology of the road map.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/segal21a.html
  PDF: https://proceedings.mlr.press/v155/segal21a/segal21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-segal21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sean
    family: Segal
  - given: Eric
    family: Kee
  - given: Wenjie
    family: Luo
  - given: Abbas
    family: Sadat
  - given: Ersin
    family: Yumer
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 973-983
  id: segal21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 973
  lastpage: 983
  published: 2021-10-04 00:00:00 +0000
- title: 'Asynchronous Deep Model Reference Adaptive Control'
  abstract: 'In this paper, we present Asynchronous implementation of Deep Neural Network-based Model Reference Adaptive Control (DMRAC). We evaluate this new neuro-adaptive control architecture through flight tests on a small quadcopter. We demonstrate that a single DMRAC controller can handle significant nonlinearities due to severe system faults and deliberate wind disturbances while executing high-bandwidth attitude control. We also show that the architecture has long-term learning abilities across different flight regimes, and can generalize to fly different flight trajectories than those on which it was trained. These results demonstrating the efficacy of this architecture for high bandwidth closed-loop attitude control of unstable and nonlinear robots operating in adverse situations. To achieve these results, we designed a software+communication architecture to ensure online real-time inference of the deep network on a high-bandwidth computation-limited platform. We expect that this architecture will benefit other deep learning in the closed-loop experiments on robots.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/joshi21a.html
  PDF: https://proceedings.mlr.press/v155/joshi21a/joshi21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-joshi21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Girish
    family: Joshi
  - given: Jasvir
    family: Virdi
  - given: Girish
    family: Chowdhary
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 984-1000
  id: joshi21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 984
  lastpage: 1000
  published: 2021-10-04 00:00:00 +0000
- title: 'Probably Approximately Correct Vision-Based Planning using Motion Primitives'
  abstract: 'This paper presents an approach for learning vision-based planners that provably generalize to novel environments (i.e., environments unseen during training). We leverage the Probably Approximately Correct (PAC)-Bayes framework to obtain an upper bound on the expected cost of policies across all environments. Minimizing the PAC-Bayes upper bound thus trains policies that are accompanied by a certificate of performance on novel environments. The training pipeline we propose provides strong generalization guarantees for deep neural network policies by (a) obtaining a good prior distribution on the space of policies using Evolutionary Strategies (ES) followed by (b) formulating the PAC-Bayes optimization as an efficiently-solvable parametric convex optimization problem. We demonstrate the efficacy of our approach for producing strong generalization guarantees for learned vision-based motion planners through two simulated examples: (1) an Unmanned Aerial Vehicle (UAV) navigating obstacle fields with an onboard vision sensor, and (2) a dynamic quadrupedal robot traversing rough terrains with proprioceptive and exteroceptive sensors.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/veer21a.html
  PDF: https://proceedings.mlr.press/v155/veer21a/veer21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-veer21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sushant
    family: Veer
  - given: Anirudha
    family: Majumdar
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1001-1014
  id: veer21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1001
  lastpage: 1014
  published: 2021-10-04 00:00:00 +0000
- title: 'Tactile Object Pose Estimation from the First Touch with Geometric Contact Rendering'
  abstract: 'In this paper, we present an approach to tactile pose estimation from the first touch for known objects. First, we create an object-agnostic map from real tactile observations to contact shapes. Next, for a new object with known geometry, we learn a tailored perception model completely in simulation. To do so, we simulate the contact shapes that a dense set of object poses would produce on the sensor. Then, given a new contact shape obtained from the sensor output, we match it against the pre-computed set using the object-specific embedding learned purely in simulation using contrastive learning. This results in a perception model that can localize objects from a single tactile observation. It also allows reasoning over pose distributions and including additional pose constraints coming from other perception systems or multiple contacts. We provide quantitative results for four objects. Our approach provides high accuracy pose estimations from distinctive tactile observations while regressing pose distributions to account for those contact shapes that could result from different object poses. We further extend and test our approach in multi-contact scenarios where several tactile sensors are simultaneously in contact with the object.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/villalonga21a.html
  PDF: https://proceedings.mlr.press/v155/villalonga21a/villalonga21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-villalonga21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Maria Bauza
    family: Villalonga
  - given: Alberto
    family: Rodriguez
  - given: Bryan
    family: Lim
  - given: Eric
    family: Valls
  - given: Theo
    family: Sechopoulos
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1015-1029
  id: villalonga21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1015
  lastpage: 1029
  published: 2021-10-04 00:00:00 +0000
- title: 'ROLL: Visual Self-Supervised Reinforcement Learning with Object Reasoning'
  abstract: 'Current image-based reinforcement learning (RL) algorithms typically operate on the whole image without performing object-level reasoning.  This leads to inefficient goal sampling and ineffective reward functions. In this paper, we improve upon previous visual self-supervised RL by incorporating object-level reasoning and occlusion reasoning. Specifically, we use unknown object segmentation to ignore distractors in the scene for better reward computation and goal generation; we further enable occlusion reasoning by employing a novel auxiliary loss and training scheme. We demonstrate that our proposed algorithm, ROLL (Reinforcement learning with Object Level Learning), learns dramatically faster and achieves better final performance compared with previous methods in several simulated visual control tasks. Project video and code are available at https://sites.google.com/andrew.cmu.edu/roll.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wang21e.html
  PDF: https://proceedings.mlr.press/v155/wang21e/wang21e.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wang21e.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yufei
    family: Wang
  - given: Narasimhan
    family: Gautham
  - given: Xingyu
    family: Lin
  - given: Brian
    family: Okorn
  - given: David
    family: Held
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1030-1048
  id: wang21e
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1030
  lastpage: 1048
  published: 2021-10-04 00:00:00 +0000
- title: 'Sample-efficient Cross-Entropy Method for Real-time Planning'
  abstract: 'Trajectory optimizers for model-based reinforcement learning, such as the  Cross-Entropy Method (CEM), can yield compelling results even in high-dimensional control tasks and sparse-reward environments. However, their sampling inefficiency prevents them from being used for real-time planning and control. We propose an improved version of the CEM algorithm for fast planning, with novel additions including temporally-correlated actions and memory, requiring 2.7-22x less samples and yielding a performance increase of 1.2-10x in high-dimensional control problems.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/pinneri21a.html
  PDF: https://proceedings.mlr.press/v155/pinneri21a/pinneri21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-pinneri21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Cristina
    family: Pinneri
  - given: Shambhuraj
    family: Sawant
  - given: Sebastian
    family: Blaes
  - given: Jan
    family: Achterhold
  - given: Joerg
    family: Stueckler
  - given: Michal
    family: Rolinek
  - given: Georg
    family: Martius
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1049-1065
  id: pinneri21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1049
  lastpage: 1065
  published: 2021-10-04 00:00:00 +0000
- title: 'Sim2Real Transfer for Deep Reinforcement Learning with Stochastic State Transition Delays'
  abstract: 'Deep Reinforcement Learning (RL) has demonstrated to be useful for a wide variety of robotics applications. To address sample efficiency and safety during training, it is common to train Deep RL policies in a simulator and then deploy to the real world, a process called Sim2Real transfer.  For robotics applications, the deployment heterogeneities and runtime compute stochasticity results in variable timing characteristics of sensor sampling rates and end-to-end delays from sensing to actuation. Prior works have used the technique of domain randomization to enable the successful transfer of policies across domains having different state transition delays. We show that variation in sampling rates and policy execution time leads to degradation in Deep RL policy performance, and that domain randomization is insufficient to overcome this limitation. We propose the Time-in-State RL (TSRL) approach, which includes delays and sampling rate as additional agent observations at training time to improve the robustness of Deep RL policies. We demonstrate the efficacy of TSRL on HalfCheetah, Ant, and car robot in simulation and on a real robot using a 1/18th scale car.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/sandha21a.html
  PDF: https://proceedings.mlr.press/v155/sandha21a/sandha21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-sandha21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sandeep Singh
    family: Sandha
  - given: Luis
    family: Garcia
  - given: Bharathan
    family: Balaji
  - given: Fatima
    family: Anwar
  - given: Mani
    family: Srivastava
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1066-1083
  id: sandha21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1066
  lastpage: 1083
  published: 2021-10-04 00:00:00 +0000
- title: 'Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion'
  abstract: 'Modern Reinforcement Learning (RL) algorithms promise to solve difficult motor control problems directly from raw sensory inputs. Their attraction is due in part to the fact that they can represent a general class of methods that allow to learn a solution with a reasonably set reward and minimal prior knowledge, even in situations where it is difficult or expensive for a human expert. For RL to truly make good on this promise, however, we need algorithms and learning setups that can work across a broad range of problems with minimal problem specific adjustments or engineering. In this paper, we study this idea of generality in the locomotion domain. We develop a learning framework that can learn sophisticated locomotion behaviour for a wide spectrum of legged robots, such as bipeds, tripeds, quadrupeds and hexapods, including wheeled variants. Our learning framework relies on a data-efficient, off-policy multi-task RL algorithm and a small set of reward functions that are semantically identical across robots. To underline the general applicability of the method, we keep the hyper-parameter settings and reward definitions constant across experiments and rely exclusively on on-board sensing. For nine different types of robots, including a real-world quadruped robot, we demonstrate that the same algorithm can rapidly learn diverse and reusable locomotion skills without any platform specific adjustments or additional instrumentation of the learning setup.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/hafner21a.html
  PDF: https://proceedings.mlr.press/v155/hafner21a/hafner21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-hafner21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Roland
    family: Hafner
  - given: Tim
    family: Hertweck
  - given: Philipp
    family: Kloeppner
  - given: Michael
    family: Bloesch
  - given: Michael
    family: Neunert
  - given: Markus
    family: Wulfmeier
  - given: Saran
    family: Tunyasuvunakool
  - given: Nicolas
    family: Heess
  - given: Martin
    family: Riedmiller
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1084-1099
  id: hafner21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1084
  lastpage: 1099
  published: 2021-10-04 00:00:00 +0000
- title: 'IV-SLAM: Introspective Vision for Simultaneous Localization and Mapping'
  abstract: 'Existing solutions to visual simultaneous localization and mapping (V-SLAM) assume that errors in feature extraction and matching are independent and identically distributed (i.i.d), but this assumption is known to not be true – features extracted from low-contrast regions of images exhibit wider error distributions than features from sharp corners. Furthermore, V-SLAM algorithms are prone to catastrophic tracking failures when sensed images include challenging conditions such as specular reflections, lens flare, or shadows of dynamic objects. To address such failures, previous work has focused on building more robust visual frontends, to filter out challenging features. In this paper, we present introspective vision for SLAM (IV-SLAM), a fundamentally different approach for addressing these challenges. IV-SLAM explicitly models the noise process of reprojection errors from visual features to be context-dependent, and hence non-i.i.d. We introduce an autonomously supervised approach for IV-SLAM to collect training data to learn such a context-aware noise model. Using this learned noise model, IV-SLAM guides feature extraction to select more features from parts of the image that are likely to result in lower noise, and further incorporate the learned noise model into the joint maximum likelihood estimation, thus making it robust to the aforementioned types of errors. We present empirical results to demonstrate that IV-SLAM 1) is able to accurately predict sources of error in input images, 2) reduces tracking error compared to V-SLAM, and 3) increases the mean distance between tracking failures by more than 70% on challenging real robot data compared to V-SLAM.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/rabiee21a.html
  PDF: https://proceedings.mlr.press/v155/rabiee21a/rabiee21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-rabiee21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sadegh
    family: Rabiee
  - given: Joydeep
    family: Biswas
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1100-1109
  id: rabiee21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1100
  lastpage: 1109
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning to Walk in the Real World with Minimal Human Effort'
  abstract: 'Reliable and stable locomotion has been one of the most fundamental challenges for legged robots. Deep reinforcement learning (deep RL) has emerged as a promising method for developing such control policies autonomously. In this paper, we develop a system for learning legged locomotion policies with deep RL in the real world with minimal human effort. The key difficulties for on-robot learning systems are automatic data collection and safety. We overcome these two challenges by developing a multi-task learning procedure and a safety-constrained RL framework. We tested our system on the task of learning to walk on three different terrains: flat ground, a soft mattress, and a doormat with crevices. Our system can automatically and efficiently learn locomotion skills on a Minitaur robot with little human intervention.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ha21c.html
  PDF: https://proceedings.mlr.press/v155/ha21c/ha21c.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ha21c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sehoon
    family: Ha
  - given: Peng
    family: Xu
  - given: Zhenyu
    family: Tan
  - given: Sergey
    family: Levine
  - given: Jie
    family: Tan
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1110-1120
  id: ha21c
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1110
  lastpage: 1120
  published: 2021-10-04 00:00:00 +0000
- title: 'Generation of Realistic Images for Learning in Simulation using FeatureGAN'
  abstract: 'This paper presents FeatureGan, a methodology to train image translators (generators) using an unpaired image training set. FeatureGan is based on the use of Generative Adversarial Networks (GAN) and has three main novel components: (i) the use of a feature loss to ensure alignment between the input and the generated image, (ii) the use of a feature pyramid discriminator, which uses a tensor composed of features at different levels of abstraction generated by a pre-trained network, and (iii) the introduction of a per class loss to improve the results in the simulation-to-reality task. The main advantage of the proposed methodology when compared to classical approaches is a more stable training process, which includes a higher resilience to common GAN problems such as mode collapse, as well as better and more consistent results. FeatureGan is also fast to train, easy to replicate, and especially suited to be used in simulation-to-reality applications where the generated realistic images allow to close the visual simulation-to-reality gap. As a proof of concept, we show the application of the proposed methodology in soccer robotics, where realistic images are generated in a soccer robotics simulator, and robot and ball detectors are trained using these images and then tested in reality. The same methodology is used to generate realistic images from images rendered in a video game. The realistic images are then used to train a semantic segmentation network.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/cruz21a.html
  PDF: https://proceedings.mlr.press/v155/cruz21a/cruz21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-cruz21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Nicolas
    family: Cruz
  - given: Javier
    family: Ruiz-del-Solar
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1121-1136
  id: cruz21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1121
  lastpage: 1136
  published: 2021-10-04 00:00:00 +0000
- title: 'Incremental learning of EMG-based control commands using Gaussian Processes'
  abstract: 'Myoelectric control is the process of controlling a prosthesis or an assistive robot by using electrical signals of the muscles. Pattern recognition in myoelectric control is a challenging field, since the underlying distribution of the signal is likely to change during the application. Covariate shifts, including changes of the arm position or different levels of muscular activation, often lead to significant instability of the control signal. This work tries to overcome these challenges by enhancing a myoelectric human machine interface through the use of the sparse Gaussian Process (sGP) approximation Variational Free Energy and by the introduction of a novel adaptive model based on an unsupervised incremental learning approach. The novel adaptive model integrates an interclass and intraclass distance to improve prediction stability under challenging conditions. Furthermore, it demonstrates the successful incorporation of incremental updates which is shown to lead to a significantly increased performance and higher  stability of the predictions in an online user study.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/schiel21a.html
  PDF: https://proceedings.mlr.press/v155/schiel21a/schiel21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-schiel21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Felix
    family: Schiel
  - given: Annette
    family: Hagengruber
  - given: Jörn
    family: Vogel
  - given: Rudolph
    family: Triebel
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1137-1146
  id: schiel21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1137
  lastpage: 1146
  published: 2021-10-04 00:00:00 +0000
- title: 'Flightmare: A Flexible Quadrotor Simulator'
  abstract: 'State-of-the-art quadrotor simulators have a rigid and highly-specialized structure: either are they really fast, physically accurate, or photo-realistic. In this work, we propose a paradigm shift in the development of simulators: moving the trade-off between accuracy and speed from the developers to the end-users. We use this idea to develop a flexible quadrotor simulator: Flightmare. In this work, we propose a novel quadrotor simulator: Flightmare.  Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. Those two components are totally decoupled and can run independently of each other. This makes our simulator extremely fast: rendering achieves speeds of up to 230 Hz, while physics simulation of up to 200,000 Hz on a laptop. In addition, Flightmare comes with several desirable features: (i) a large multi-modal sensor suite, including an interface to extract the 3D point-cloud of the scene; (ii) an API for reinforcement learning which can simulate hundreds of quadrotors in parallel; and (iii) integration with a virtual-reality headset for interaction with the simulated environment. We demonstrate the flexibility of Flightmare by using it for two different robotic tasks: quadrotor control using deep reinforcement learning and collision-free path planning in a complex 3D environment.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/song21a.html
  PDF: https://proceedings.mlr.press/v155/song21a/song21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-song21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yunlong
    family: Song
  - given: Selim
    family: Naji
  - given: Elia
    family: Kaufmann
  - given: Antonio
    family: Loquercio
  - given: Davide
    family: Scaramuzza
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1147-1157
  id: song21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1147
  lastpage: 1157
  published: 2021-10-04 00:00:00 +0000
- title: 'Hardware as Policy: Mechanical and Computational Co-Optimization using Deep Reinforcement Learning'
  abstract: 'Deep Reinforcement Learning (RL) has shown great success in learning complex control policies for a variety of applications in robotics. However, in most such cases, the hardware of the robot has been considered immutable, modeled as part of the environment.  In this study, we explore the problem of learning hardware and control parameters together in a unified RL framework. To achieve this, we propose to model the robot body as a “hardware policy”, analogous to and optimized jointly with its computational counterpart. We show that, by modeling such hardware policies as auto-differentiable computational graphs, the ensuing optimization problem can be solved efficiently by gradient-based algorithms from the Policy Optimization family.  We present two such design examples:  a toy mass-spring problem, and a real-world problem of designing an underactuated hand.  We compare our method against traditional co-optimization approaches, and also demonstrate its effectiveness by building a physical prototype based on the learned hardware parameters.  Videos and more details are available at https://roamlab.github.io/hwasp/.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chen21a.html
  PDF: https://proceedings.mlr.press/v155/chen21a/chen21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chen21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tianjian
    family: Chen
  - given: Zhanpeng
    family: He
  - given: Matei
    family: Ciocarlie
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1158-1173
  id: chen21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1158
  lastpage: 1173
  published: 2021-10-04 00:00:00 +0000
- title: 'StrObe: Streaming Object Detection from LiDAR Packets'
  abstract: 'Many modern robotics systems employ LiDAR as their main sensing modality due to its geometrical richness. Rolling shutter LIDARs are particularly common, in which a sweep is built through the accumulation of points over an entire revolution of the sensor, thus producing a 360 point cloud of the scene. Modern perception algorithms wait for the full sweep to be built before processing the data, which introduces an additional latency of up to 100ms. As a consequence, by the time an output is produced, it no longer accurately reflects the state of the world. This poses a big challenge, as robotics applications require minimal reaction times, such that maneuvers can be quickly planned in the event of a safety-critical situation. In this paper we propose StrObe, a novel approach that minimizes latency by ingesting packets of LiDAR and emitting a stream of detections without waiting for the full sweep to be built. StrObe reuses computations from previous points and iteratively updates the spatial memory of the scene as new evidence comes in, resulting in latency reduced accurate perception. We demonstrate the effectiveness of our approach on a large scale dataset, showing that our approach far outperforms the state-of-the-art when latency is taken into account while still matching the performance in the traditional setting.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/frossard21a.html
  PDF: https://proceedings.mlr.press/v155/frossard21a/frossard21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-frossard21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Davi
    family: Frossard
  - given: Shun Da
    family: Suo
  - given: Sergio
    family: Casas
  - given: James
    family: Tu
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1174-1183
  id: frossard21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1174
  lastpage: 1183
  published: 2021-10-04 00:00:00 +0000
- title: 'Towards Robotic Assembly by Predicting Robust, Precise and Task-oriented Grasps'
  abstract: 'Robust task-oriented grasp planning is vital for autonomous robotic precision assembly tasks. Knowledge of the objects’ geometry and preconditions of the target task should be incorporated when determining the proper grasp to execute. However, several factors contribute to the challenges of realizing these grasps such as noise when controlling the robot, unknown object properties, and difficulties modeling complex object-object interactions. We propose a method that decomposes this problem and optimizes for grasp robustness, precision, and task performance by learning three cascaded networks. We evaluate our method in simulation on three common assembly tasks: inserting gears onto pegs, aligning brackets into corners, and inserting shapes into slots. Our policies are trained using a curriculum based on large-scale self-supervised grasp simulations with procedurally generated objects. Finally, we evaluate the performance of the first two tasks with a real robot where our method achieves 4.28mm error for bracket insertion and 1.44mm error for gear insertion.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zhao21c.html
  PDF: https://proceedings.mlr.press/v155/zhao21c/zhao21c.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zhao21c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jialiang
    family: Zhao
  - given: Daniel
    family: Troniak
  - given: Oliver
    family: Kroemer
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1184-1194
  id: zhao21c
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1184
  lastpage: 1194
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning to Communicate and Correct Pose Errors'
  abstract: 'Learned communication makes multi-agent systems more effective by aggregating distributed information. However, it also exposes individual agents to the threat of erroneous messages they receive. In this paper, we study the setting proposed in V2VNet, where nearby self-driving vehicles jointly perform object detection and motion forecasting in a cooperative manner. Despite a huge performance boost when the agents solve the task together, the gain is quickly diminished in the presence of pose noise since the communication relies on spatial transformations. Hence, we propose a novel neural reasoning framework that learns to communicate, to estimate errors, and finally, to reach a consensus about those errors. Experiments confirm that our proposed framework significantly improves the robustness of multi-agent self-driving perception systems under realistic and severe localization noise.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/vadivelu21a.html
  PDF: https://proceedings.mlr.press/v155/vadivelu21a/vadivelu21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-vadivelu21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Nicholas
    family: Vadivelu
  - given: Mengye
    family: Ren
  - given: James
    family: Tu
  - given: Jingkang
    family: Wang
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1195-1210
  id: vadivelu21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1195
  lastpage: 1210
  published: 2021-10-04 00:00:00 +0000
- title: 'Visual Localization and Mapping with Hybrid SFA'
  abstract: 'Visual localization is a crucial requirement in mobile robotics, field and service robotics, and self-driving cars. Recently, unsupervised learning with Slow Feature Analysis (SFA) has shown to produce spatial representations that enable localization from holistic images. The approach is faster and much less complex than state-of-the-art monocular visual SLAM methods while achieving similar localization performance in small-scale environments. However, the holistic approach’s performance drops significantly for highly complex, large-scale environments due to scene variations occurring during a training phase. Instead of using holistic images, an alternative is to perform localization relative to unique regions present in a scene. Therefore, in this paper, we add a new component to the SFA localization pipeline that leverages state-of-the-art CNN to identify unique image regions. Hence we propose a hybrid approach that first learns such regions with a pre-trained CNN and then uses SFA for unsupervised pose estimation relative to each region. We present the experimental results from an autonomous robot in two different outdoor environments of varying complexity and size. The experiments show the proposed hybrid approach outperforms holistic SFA w.r.t localization accuracy in both environments, but benefits are more pronounced in the large-scale environment.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/haris21a.html
  PDF: https://proceedings.mlr.press/v155/haris21a/haris21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-haris21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Muhammad
    family: Haris
  - given: Mathias
    family: Franzius
  - given: Ute
    family: Bauer-Wersing
  - given: Sai Krishna Kaushik
    family: Karanam
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1211-1220
  id: haris21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1211
  lastpage: 1220
  published: 2021-10-04 00:00:00 +0000
- title: 'EXI-Net: EXplicitly/Implicitly Conditioned Network for Multiple Environment Sim-to-Real Transfer'
  abstract: 'Sim-to-real transfer is attractive for robot learning, as it avoids the high cost of collecting data with real robots, but transferring agents from simulation to the real world is challenging. Previous studies have presented promising methods to solve this problem, but they may fail when a wider range of dynamics has to be considered. In this study, we propose a network architecture with explicit and implicit dynamics parameters for sim-to-real transfer from multiple environments. Using this method, we can simultaneously estimate the dynamics of the real world and optimize the action in various kinds of environments. The core novelty lies in the simultaneous dynamics estimation and action optimization, as well as the use of explicit (physically quantifiable) and implicit (latent) dynamics parameters to condition the network input. We apply our method to the object pushing task and verify its effectiveness by comparing it with previous methods and real-world experiments.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/murooka21a.html
  PDF: https://proceedings.mlr.press/v155/murooka21a/murooka21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-murooka21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Takayuki
    family: Murooka
  - given: Masashi
    family: Hamaya
  - given: Felix von
    family: Drigalski
  - given: Kazutoshi
    family: Tanaka
  - given: Yoshihisa
    family: Ijiri
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1221-1230
  id: murooka21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1221
  lastpage: 1230
  published: 2021-10-04 00:00:00 +0000
- title: 'Self-Supervised Object-in-Gripper Segmentation from Robotic Motions'
  abstract: 'Accurate object segmentation is a crucial task in the context of robotic manipulation. However, creating sufficient annotated training data for neural networks is particularly time consuming and often requires manual labeling. To this end, we propose a simple, yet robust solution for learning to segment unknown objects grasped by a robot. Specifically, we exploit motion and temporal cues in RGB video sequences. Using optical flow estimation we first learn to predict segmentation masks of our given manipulator. Then, these annotations are used in combination with motion cues to automatically distinguish between background, manipulator and unknown, grasped object. In contrast to existing systems our approach is fully self-supervised and independent of precise camera calibration, 3D models or potentially imperfect depth data. We perform a thorough comparison with alternative baselines and approaches from literature. The object masks and views are shown to be suitable training data for segmentation networks that generalize to novel environments and also allow for watertight 3D reconstruction.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/boerdijk21a.html
  PDF: https://proceedings.mlr.press/v155/boerdijk21a/boerdijk21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-boerdijk21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Wout
    family: Boerdijk
  - given: Martin
    family: Sundermeyer
  - given: Maximilian
    family: Durner
  - given: Rudolph
    family: Triebel
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1231-1245
  id: boerdijk21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1231
  lastpage: 1245
  published: 2021-10-04 00:00:00 +0000
- title: 'MELD: Meta-Reinforcement Learning from Images via Latent State Models'
  abstract: 'Meta-reinforcement learning algorithms can enable autonomous agents, such as robots, to quickly acquire new behaviors by leveraging prior experience in a set of related training tasks. However, the onerous data requirements of meta-training compounded with the challenge of learning from sensory inputs such as images have made meta-RL challenging to apply to real robotic systems. Latent state models, which learn compact state representations from a sequence of observations, can accelerate representation learning from visual inputs. In this paper, we leverage the perspective of meta-learning as task inference to show that latent state models can {\em also} perform meta-learning given an appropriately defined observation space. Building on this insight, we develop meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that performs inference in a latent state model to quickly acquire new skills given observations and rewards. MELD outperforms prior meta-RL methods on several simulated image-based robotic control problems, and enables a real WidowX robotic arm to insert an Ethernet cable into new locations given a sparse task completion signal after only $8$ hours of real world meta-training. To our knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic control setting from images.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zhao21d.html
  PDF: https://proceedings.mlr.press/v155/zhao21d/zhao21d.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zhao21d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zihao
    family: Zhao
  - given: Anusha
    family: Nagabandi
  - given: Kate
    family: Rakelly
  - given: Chelsea
    family: Finn
  - given: Sergey
    family: Levine
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1246-1261
  id: zhao21d
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1246
  lastpage: 1261
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning from Suboptimal Demonstration via Self-Supervised Reward Regression'
  abstract: 'Learning from Demonstration (LfD) seeks to democratize robotics by enabling non-roboticist end-users to teach robots to perform a task by providing a human demonstration. However, modern LfD techniques, e.g. inverse reinforcement learning (IRL), assume users provide at least stochastically optimal demonstrations. This assumption fails to hold in most real-world scenarios. Recent attempts to learn from sub-optimal demonstration leverage pairwise rankings and following the Luce-Shepard rule. However, we show these approaches make incorrect assumptions and thus suffer from brittle, degraded performance. We overcome these limitations in developing a novel approach that bootstraps off suboptimal demonstrations to synthesize optimality-parameterized data to train an idealized reward function. We empirically validate we learn an idealized reward function with  0.95 correlation with ground-truth reward versus   0.75 for prior work. We can then train policies achieving  200% improvement over the suboptimal demonstration and  90% improvement over prior work. We present a physical demonstration of teaching a robot a topspin strike in table tennis that achieves 32% faster returns and 40% more topspin than user demonstration.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chen21b.html
  PDF: https://proceedings.mlr.press/v155/chen21b/chen21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chen21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Letian
    family: Chen
  - given: Rohan
    family: Paleja
  - given: Matthew
    family: Gombolay
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1262-1277
  id: chen21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1262
  lastpage: 1277
  published: 2021-10-04 00:00:00 +0000
- title: 'Stein Variational Model Predictive Control'
  abstract: 'Decision making under uncertainty is critical to real-world, autonomous systems. Model Predictive Control (MPC) methods have demonstrated favorable performance in practice, but remain limited when dealing with complex probability distributions. In this paper, we propose a generalization of MPC that represents a multitude of solutions as posterior distributions. By casting MPC as a Bayesian inference problem, we employ variational methods for posterior computation, naturally encoding the complexity and multi-modality of the decision making problem. We propose a Stein variational gradient descent method to estimate the posterior over control parameters, given a cost function and a sequence of state observations. We show that this framework leads to successful planning in challenging, non-convex optimal control problems.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/lambert21a.html
  PDF: https://proceedings.mlr.press/v155/lambert21a/lambert21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-lambert21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Alexander
    family: Lambert
  - given: Fabio
    family: Ramos
  - given: Byron
    family: Boots
  - given: Dieter
    family: Fox
  - given: Adam
    family: Fishman
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1278-1297
  id: lambert21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1278
  lastpage: 1297
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Interactively to Resolve Ambiguity in Reference Frame Selection'
  abstract: 'In Learning from Demonstrations, ambiguities can lead to bad generalization of the learned policy. This paper proposes a framework called Learning Interactively to Resolve Ambiguity (LIRA), that recognizes ambiguous situations, in which more than one action have similar probabilities, avoids a random action selection, and uses the human feedback for solving them. The aim is to improve the user experience, the learning performance and safety. LIRA is tested in the selection of the right goal of Movement Primitives (MP) out of a candidate list if multiple contradictory generalizations of the demonstration(s) are possible. The framework is validated on different pick and place operations on a Emika-Franka Robot. A user study showed a significant reduction on the task load of the user, compared to a system that does not allow interactive resolution of ambiguities.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/franzese21a.html
  PDF: https://proceedings.mlr.press/v155/franzese21a/franzese21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-franzese21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Giovanni
    family: Franzese
  - given: Carlos
    family: Celemin
  - given: Jens
    family: Kober
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1298-1311
  id: franzese21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1298
  lastpage: 1311
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Trajectories for Visual-Inertial System Calibration via Model-based Heuristic Deep Reinforcement Learning'
  abstract: 'Visual-inertial systems rely on precise calibrations of both camera intrinsics and inter-sensor extrinsics, which typically require manually performing complex motions in front of a calibration target. In this work we present a novel approach to obtain favorable trajectories for visual-inertial system calibration, using model-based deep reinforcement learning. Our key contribution is to model the calibration process as a Markov decision process and then use model-based deep reinforcement learning with particle swarm optimization to establish a sequence of calibration trajectories to be performed by a robot arm. Our experiments show that while maintaining similar or shorter path lengths, the trajectories generated by our learned policy result in lower calibration errors compared to random or handcrafted trajectories. The code is publicly available.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chen21c.html
  PDF: https://proceedings.mlr.press/v155/chen21c/chen21c.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chen21c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Le
    family: Chen
  - given: Yunke
    family: Ao
  - given: Florian
    family: Tschopp
  - given: Andrei
    family: Cramariuc
  - given: Michel
    family: Breyer
  - given: Jen Jen
    family: Chung
  - given: Roland
    family: Siegwart
  - given: Cesar
    family: Cadena
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1312-1325
  id: chen21c
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1312
  lastpage: 1325
  published: 2021-10-04 00:00:00 +0000
- title: 'A User’s Guide to Calibrating Robotic Simulators'
  abstract: 'Simulators are a critical component of modern robotics research. Strategies for both perception and decision making can be studied in simulation first, before deployed to real world systems, saving on time and costs. Despite significant progress on the development of sim-to-real algorithms, the analysis of  different methods is still conducted in an ad-hoc manner without a consistent set of tests and metrics for comparison. This paper intends to fill this gap and proposes a set of benchmarks and a framework for the study of various algorithms aimed to transfer models and policies learnt in simulation to the real world. We conduct experiments on a wide range of well known simulated environments to characterise and offer insights into the performance of different algorithms. Our analysis can be useful for practitioners working in this area and can help make informed choices about the behaviour and main properties of sim-to-real algorithms.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/mehta21a.html
  PDF: https://proceedings.mlr.press/v155/mehta21a/mehta21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-mehta21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Bhairav
    family: Mehta
  - given: Ankur
    family: Handa
  - given: Dieter
    family: Fox
  - given: Fabio
    family: Ramos
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1326-1340
  id: mehta21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1326
  lastpage: 1340
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Stability Certificates from Data'
  abstract: 'Many existing tools in nonlinear control theory for establishing stability or safety of a dynamical system can be distilled to the construction of a certificate function which guarantees a desired property. However, algorithms for synthesizing certificate functions typically require a closed-form analytical expression of the underlying dynamics, which rules out their use on many modern robotic platforms. To circumvent this issue, we develop algorithms for learning certificate functions only from trajectory data. We establish bounds on the generalization error – the probability that a certificate will not certify a new, unseen trajectory – when learning from trajectories, and we convert such generalization error bounds into global stability guarantees. We demonstrate empirically that certificates for complex dynamics can be efficiently learned, and that the learned certificates can be used for downstream tasks such as adaptive control.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/boffi21a.html
  PDF: https://proceedings.mlr.press/v155/boffi21a/boffi21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-boffi21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Nicholas
    family: Boffi
  - given: Stephen
    family: Tu
  - given: Nikolai
    family: Matni
  - given: Jean-Jacques
    family: Slotine
  - given: Vikas
    family: Sindhwani
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1341-1350
  id: boffi21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1341
  lastpage: 1350
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Hybrid Control Barrier Functions from Data'
  abstract: 'Motivated by the lack of systematic tools to obtain safe control laws for hybrid systems, we propose an optimization-based framework for learning certifiably safe control laws from data. In particular, we assume a setting in which the system dynamics are known and in which data exhibiting safe system behavior is available. We propose hybrid control barrier functions for hybrid systems as a means to synthesize safe control inputs. Based on this notion, we present an optimization-based framework to learn such hybrid control barrier functions from data. Importantly, we identify sufficient conditions on the data such that feasibility of the optimization problem ensures correctness of the learned hybrid control barrier functions, and hence the safety of the system. We illustrate our findings in two simulations studies, including a compass gait walker.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/lindemann21a.html
  PDF: https://proceedings.mlr.press/v155/lindemann21a/lindemann21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-lindemann21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Lars
    family: Lindemann
  - given: Haimin
    family: Hu
  - given: Alexander
    family: Robey
  - given: Hanwen
    family: Zhang
  - given: Dimos
    family: Dimarogonas
  - given: Stephen
    family: Tu
  - given: Nikolai
    family: Matni
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1351-1370
  id: lindemann21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1351
  lastpage: 1370
  published: 2021-10-04 00:00:00 +0000
- title: 'Map-Adaptive Goal-Based Trajectory Prediction'
  abstract: 'We present a new method for multi-modal, long-term vehicle trajectory prediction. Our approach relies on using lane centerlines captured in rich maps of the environment to generate a set of proposed goal paths for each vehicle. Using these paths – which are generated at run time and therefore dynamically adapt to the scene – as spatial anchors, we predict a set of goal-based trajectories along with a categorical distribution over the goals. This approach allows us to directly model the goal-directed behavior of traffic actors, which unlocks the potential for more accurate long-term prediction. Our experimental results on both a large-scale internal driving dataset and on the public nuScenes dataset show that our model outperforms state-of-the-art approaches for vehicle trajectory prediction over a 6-second horizon. We also empirically demonstrate that our model is better able to generalize to road scenes from a completely new city than existing methods.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zhang21a.html
  PDF: https://proceedings.mlr.press/v155/zhang21a/zhang21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zhang21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Lingyao
    family: Zhang
  - given: Po-Hsun
    family: Su
  - given: Jerrick
    family: Hoang
  - given: Galen Clark
    family: Haynes
  - given: Micol
    family: Marchetti-Bowick
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1371-1383
  id: zhang21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1371
  lastpage: 1383
  published: 2021-10-04 00:00:00 +0000
- title: 'The RobotSlang Benchmark: Dialog-guided Robot Localization and Navigation'
  abstract: 'Autonomous robot systems for applications from search and rescue to assistive guidance should be able to engage in natural language dialog with people. To study such cooperative communication, we introduce Robot Simultaneous Localization and Mapping with Natural Language (RobotSlang), a benchmark of 169 natural language dialogs between a human Driver controlling a robot and a human Commander providing guidance towards navigation goals. In each trial, the pair first cooperates to localize the robot on a global map visible to the Commander, then the Driver follows Commander instructions to move the robot to a sequence of target objects. We introduce a Localization from Dialog History (LDH) and a Navigation from Dialog History (NDH) task where a learned agent is given dialog and visual observations from the robot platform as input and must localize in the global map or navigate towards the next target object, respectively. RobotSlang is comprised of nearly 5k utterances and over 1k minutes of robot camera and control streams. We present an initial model for the NDH task, and show that an agent trained in simulation can follow the RobotSlang dialog-based navigation instructions for controlling a physical robot platform. Code and data are available at https://umrobotslang.github.io/.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/banerjee21a.html
  PDF: https://proceedings.mlr.press/v155/banerjee21a/banerjee21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-banerjee21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Shurjo
    family: Banerjee
  - given: Jesse
    family: Thomason
  - given: Jason
    family: Corso
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1384-1393
  id: banerjee21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1384
  lastpage: 1393
  published: 2021-10-04 00:00:00 +0000
- title: 'The Emergence of Adversarial Communication in Multi-Agent Reinforcement Learning'
  abstract: 'Many real-world problems require the coordination of multiple autonomous agents. Recent work has shown the promise of Graph Neural Networks (GNNs) to learn explicit communication strategies that enable complex multi-agent coordination. These works use models of cooperative multi-agent systems whereby agents strive to achieve a shared global goal. When considering agents with self-interested local objectives, the standard design choice is to model these as separate learning systems (albeit sharing the same environment). Such a design choice, however, precludes the existence of a single, differentiable communication channel, and consequently prohibits the learning of inter-agent communication strategies. In this work, we address this gap by presenting a learning model that accommodates individual non-shared rewards and a differentiable communication channel that is common among all agents. We focus on the case where agents have self-interested objectives, and develop a learning algorithm that elicits the emergence of adversarial communications. We perform experiments on multi-agent coverage and path planning problems, and employ a post-hoc interpretability technique to visualize the messages that agents communicate to each other. We show how a single self-interested agent is capable of learning highly manipulative communication strategies that allows it to significantly outperform a cooperative team of agents.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/blumenkamp21a.html
  PDF: https://proceedings.mlr.press/v155/blumenkamp21a/blumenkamp21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-blumenkamp21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jan
    family: Blumenkamp
  - given: Amanda
    family: Prorok
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1394-1414
  id: blumenkamp21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1394
  lastpage: 1414
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning rich touch representations through cross-modal self-supervision'
  abstract: 'The sense of touch is fundamental in several manipulation tasks, but rarely used in robot manipulation. In this work we tackle the problem of learning rich touch features from cross-modal self-supervision. We evaluate them identifying objects and their properties in a few-shot classification setting. Two new datasets are introduced using a simulated anthropomorphic robotic hand equipped with tactile sensors on both synthetic and daily life objects. Several self-supervised learning methods are benchmarked on these datasets, by evaluating few-shot classification on unseen objects and poses. Our experiments indicate that cross-modal self-supervision effectively improves touch representation, and in turn has great potential to enhance robot manipulation skills.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zambelli21a.html
  PDF: https://proceedings.mlr.press/v155/zambelli21a/zambelli21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zambelli21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Martina
    family: Zambelli
  - given: Yusuf
    family: Aytar
  - given: Francesco
    family: Visin
  - given: Yuxiang
    family: Zhou
  - given: Raia
    family: Hadsell
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1415-1425
  id: zambelli21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1415
  lastpage: 1425
  published: 2021-10-04 00:00:00 +0000
- title: 'Generalization Guarantees for Imitation Learning'
  abstract: 'Control policies from imitation learning can often fail to generalize to novel environments due to imperfect demonstrations or the inability of imitation learning algorithms to accurately infer the expert’s policies. In this paper, we present rigorous generalization guarantees for imitation learning by leveraging the Probably Approximately Correct (PAC)-Bayes framework to provide upper bounds on the expected cost of policies in novel environments. We propose a two-stage training method where a latent policy distribution is first embedded with multi-modal expert behavior using a conditional variational autoencoder, and then “fine-tuned” in new training environments to explicitly optimize the generalization bound. We demonstrate strong generalization bounds and their tightness relative to empirical performance in simulation for (i) grasping diverse mugs, (ii) planar pushing with visual feedback, and (iii) vision-based indoor navigation, as well as through hardware experiments for the two manipulation tasks.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ren21a.html
  PDF: https://proceedings.mlr.press/v155/ren21a/ren21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ren21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Allen
    family: Ren
  - given: Sushant
    family: Veer
  - given: Anirudha
    family: Majumdar
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1426-1442
  id: ren21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1426
  lastpage: 1442
  published: 2021-10-04 00:00:00 +0000
- title: 'Multi-Modal Anomaly Detection for Unstructured and Uncertain Environments'
  abstract: 'To achieve high-levels of autonomy, modern robots require the ability to detect and recover from anomalies and failures with minimal human supervision. Multi-modal sensor signals could provide more information for such anomaly detection tasks; however, the fusion of high-dimensional and heterogeneous sensor modalities remains a challenging problem. We propose a deep learning neural network: supervised variational autoencoder (SVAE), for failure identification in unstructured and uncertain environments. Our model leverages the representational power of VAE to extract robust features from high-dimensional inputs for supervised learning tasks. The training objective unifies the generative model and the discriminative model, thus making the learning a one-stage procedure. Our experiments on real field robot data demonstrate superior failure identification performance than baseline methods, and that our model learns interpretable representations.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ji21a.html
  PDF: https://proceedings.mlr.press/v155/ji21a/ji21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ji21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tianchen
    family: Ji
  - given: Sri Theja
    family: Vuppala
  - given: Girish
    family: Chowdhary
  - given: Katherine
    family: Driggs-Campbell
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1443-1455
  id: ji21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1443
  lastpage: 1455
  published: 2021-10-04 00:00:00 +0000
- title: 'Generative adversarial training of product of policies for robust and adaptive movement primitives'
  abstract: 'In learning from demonstrations, many generative models of trajectories make simplifying assumptions of independence. Correctness is sacrificed in the name of tractability and speed of the learning phase. The ignored dependencies, which are often the kinematic and dynamic constraints of the system, are then only restored when synthesizing the motion, which introduces possibly heavy distortions. In this work, we propose to use those approximate trajectory distributions as close-to-optimal discriminators in the popular generative adversarial framework to stabilize and accelerate the learning procedure. The two problems of adaptability and robustness are addressed with our method. In order to adapt the motions to varying contexts, we propose to use a product of Gaussian policies defined in several parametrized task spaces. Robustness to perturbations and varying dynamics is ensured with the use of stochastic gradient descent and ensemble methods to learn the stochastic dynamics. Two experiments are performed on a 7-DoF manipulator to validate the approach.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/pignat21a.html
  PDF: https://proceedings.mlr.press/v155/pignat21a/pignat21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-pignat21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Emmanuel
    family: Pignat
  - given: Hakan
    family: Girgin
  - given: Sylvain
    family: Calinon
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1456-1470
  id: pignat21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1456
  lastpage: 1470
  published: 2021-10-04 00:00:00 +0000
- title: 'Robot Action Selection Learning via Layered Dimension Informed Program Synthesis'
  abstract: 'Abstract: Action selection policies (ASPs), used to compose low-level robot skills into complex high-level tasks are commonly represented as neural networks (NNs) in the state of the art. Such a paradigm, while very effective, suffers from a few key problems: 1) NNs are opaque to the user and hence not amenable to verification, 2) they require significant amounts of training data, and 3) they are hard to repair when the domain changes. We present two key insights about ASPs for robotics. First, ASPs need to reason about physically meaningful quantities derived from the state of the world, and second, there exists a layered structure for composing these policies. Leveraging these insights, we introduce layered dimension-informed program synthesis (LDIPS) - by reasoning about the physical dimensions of state variables, and dimensional constraints on operators, LDIPS directly synthesizes ASPs in a human-interpretable domain-specific language that is amenable to program repair. We present empirical results to demonstrate that LDIPS 1) can synthesize effective ASPs for robot soccer and autonomous driving domains, 2) requires two orders of magnitude fewer training examples than a comparable NN representation, and 3) can repair the synthesized ASPs with only a small number of corrections when transferring from simulation to real robots.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/holtz21a.html
  PDF: https://proceedings.mlr.press/v155/holtz21a/holtz21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-holtz21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jarrett
    family: Holtz
  - given: Arjun
    family: Guha
  - given: Joydeep
    family: Biswas
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1471-1480
  id: holtz21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1471
  lastpage: 1480
  published: 2021-10-04 00:00:00 +0000
- title: 'Policy learning in SE(3) action spaces'
  abstract: 'In the spatial action representation, the action space spans the space of target poses for robot motion commands, i.e. SE(2) or SE(3). This approach has been used to solve challenging robotic manipulation problems and shows promise. However, the method is often limited to a three dimensional action space and short horizon tasks. This paper proposes ASRSE3, a new method for handling higher dimensional spatial action spaces that transforms an original MDP with high dimensional action space into a new MDP with reduced action space and augmented state space. We also propose SDQfD, a variation of DQfD designed for large action spaces. ASRSE3 and SDQfD are evaluated in the context of a set of challenging block construction tasks. We show that both methods outperform standard baselines and can be used in practice on real robotics systems.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wang21f.html
  PDF: https://proceedings.mlr.press/v155/wang21f/wang21f.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wang21f.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Dian
    family: Wang
  - given: Colin
    family: Kohler
  - given: Robert
    family: Platt
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1481-1497
  id: wang21f
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1481
  lastpage: 1497
  published: 2021-10-04 00:00:00 +0000
- title: 'Amodal 3D Reconstruction for Robotic Manipulation via Stability and Connectivity'
  abstract: 'Learning-based 3D object reconstruction enables single- or few-shot estimation of 3D object models. For robotics,  this holds the potential to allow model-based methods to rapidly adapt to novel objects and scenes. Existing 3D reconstruction techniques optimize for visual reconstruction fidelity, typically measured by chamfer distance or voxel IOU. We find that when applied to realistic, cluttered robotics environments, these systems produce reconstructions with low physical realism, resulting in poor task performance when used for model-based control.  We propose ARM, an amodal 3D reconstruction system that introduces (1) a stability prior over object shapes, (2) a connectivity prior, and (3) a multi-channel input representation that allows for reasoning over relationships between groups of objects.  By using these priors over the physical properties of objects, our system improves reconstruction quality not just by standard visual metrics, but also performance of model-based control on a variety of robotics manipulation tasks in challenging, cluttered environments.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/agnew21a.html
  PDF: https://proceedings.mlr.press/v155/agnew21a/agnew21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-agnew21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: William
    family: Agnew
  - given: Christopher
    family: Xie
  - given: Aaron
    family: Walsman
  - given: Octavian
    family: Murad
  - given: Yubo
    family: Wang
  - given: Pedro
    family: Domingos)
  - given: Siddhartha
    family: Srinivasa
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1498-1508
  id: agnew21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1498
  lastpage: 1508
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning an Expert Skill-Space for Replanning Dynamic Quadruped Locomotion over Obstacles'
  abstract: 'Function approximators are increasingly being considered as a tool for generating robot motions that are temporally extended and express foresight about the scenario at hand. While these longer behaviors are often necessary or beneficial, they also induce multimodality in the decision space, which complicates the training of a regression model on expert data. Motivated by the problem of quadrupedal locomotion over obstacles, we apply an approach that disentangles modal variation from task-to-solution regression by using a conditional variational autoencoder. The resulting decoder is a regression model that outputs trajectories based on the task and a real-valued latent mode vector representing a style of behavior. With the task consisting of robot-relative descriptions of the state, the goal, and nearby obstacles, this model is suitable for receding-horizon generation of structured dynamic motion. We test this approach, along with a trajectory library baseline method, for producing sustained locomotion plans that use a generalized gait. Both options strongly bias planned footholds away from obstacle regions, while the multimodal regressor is far less susceptible to violating kinematic constraints. We conclude by identifying further prospective benefits of the continuous latent mode representation, along with targets for future integration into a hardware-deployable pipeline including perception and control.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/surovik21a.html
  PDF: https://proceedings.mlr.press/v155/surovik21a/surovik21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-surovik21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: David
    family: Surovik
  - given: Oliwier
    family: Melon
  - given: Mathieu
    family: Geisert
  - given: Maurice
    family: Fallon
  - given: Ioannis
    family: Havoutis
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1509-1518
  id: surovik21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1509
  lastpage: 1518
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Certified Control Using Contraction Metric'
  abstract: 'In this paper, we solve the problem of finding a certified control policy that drives a robot from any given initial state and under any bounded disturbance to the desired reference trajectory, with guarantees on the convergence or bounds on the tracking error. Such a controller is crucial in safe motion planning. We leverage the advanced theory in Control Contraction Metric and design a learning framework based on neural networks to co-synthesize the contraction metric and the controller for control-affine systems. We further provide methods to validate the convergence and bounded error guarantees. We demonstrate the performance of our method using a suite of challenging robotic models, including models with learned dynamics as neural networks. We compare our approach with leading methods using sum-of-squares programming, reinforcement learning, and model predictive control. Results show that our methods indeed can handle a broader class of systems with less tracking error and faster execution speed. Code is available at https://github.com/sundw2014/C3M.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/sun21b.html
  PDF: https://proceedings.mlr.press/v155/sun21b/sun21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-sun21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Dawei
    family: Sun
  - given: Susmit
    family: Jha
  - given: Chuchu
    family: Fan
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1519-1539
  id: sun21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1519
  lastpage: 1539
  published: 2021-10-04 00:00:00 +0000
- title: 'Same Object, Different Grasps: Data and Semantic Knowledge for Task-Oriented Grasping'
  abstract: 'Despite the enormous progress and generalization in robotic grasping in recent years, existing methods have yet to scale and generalize task-oriented grasping to the same extent. This is largely due to the scale of the datasets both in terms of the number of objects and tasks studied. We address these concerns with the TaskGrasp dataset which is more diverse both in terms of objects and tasks, and an order of magnitude larger than previous datasets. The dataset contains 250K task-oriented grasps for 56 tasks and 191 objects along with their RGB-D information. We take advantage of this new breadth and diversity in the data and present the GCNGrasp framework which uses the semantic knowledge of objects and tasks encoded in a knowledge graph to generalize to new object instances, classes and even new tasks. Our framework shows a significant improvement of around 12% on held-out settings compared to baseline methods which do not use semantics. We demonstrate that our dataset and model are applicable for the real world by executing task-oriented grasps on a real robot on unknown objects. Code, data and supplementary video could be found at https://github.com/adithyamurali/TaskGrasp.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/murali21a.html
  PDF: https://proceedings.mlr.press/v155/murali21a/murali21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-murali21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Adithyavairavan
    family: Murali
  - given: Weiyu
    family: Liu
  - given: Kenneth
    family: Marino
  - given: Sonia
    family: Chernova
  - given: Abhinav
    family: Gupta
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1540-1557
  id: murali21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1540
  lastpage: 1557
  published: 2021-10-04 00:00:00 +0000
- title: 'DIRL: Domain-Invariant Representation Learning for Sim-to-Real Transfer'
  abstract: 'Generating large-scale synthetic data in simulation is a feasible alternative to collecting/labelling real data for training vision-based deep learning models, albeit the modelling inaccuracies do not generalize to the physical world. In this paper, we present a domain-invariant representation learning (DIRL) algorithm to adapt deep models to the physical environment with a small amount of real data. Existing approaches that only mitigate the covariate shift by aligning the marginal distributions across the domains and assume the conditional distributions to be domain-invariant can lead to ambiguous transfer in real scenarios. We propose to jointly align the marginal (input domains) and the conditional (output labels) distributions to mitigate the covariate and the conditional shift across the domains with adversarial learning, and combine it with a triplet distribution loss to make the conditional distributions disjoint in the shared feature space. Experiments on digit domains yield state-of-the-art performance on challenging benchmarks, while sim-to-real transfer of object recognition for vision-based decluttering with a mobile robot improves from $26.8 %$ to $91.0 %$, resulting in $86.5 %$ grasping accuracy of a wide variety of objects. Code and supplementary details are available at: https://sites.google.com/view/dirl'
  volume: 155
  URL: https://proceedings.mlr.press/v155/tanwani21a.html
  PDF: https://proceedings.mlr.press/v155/tanwani21a/tanwani21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-tanwani21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ajay
    family: Tanwani
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1558-1571
  id: tanwani21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1558
  lastpage: 1571
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Hierarchical Task Networks with Preferences from Unannotated Demonstrations'
  abstract: 'We address the problem of learning Hierarchical Task Networks (HTNs) from unannotated task demonstrations, while retaining action execution preferences present in the demonstration data.  We show that the problem of learning a complex HTN structure can be made analogous to the problem of series/parallel reduction of resistor networks, a foundational concept in Electrical Engineering.  Based on this finding, we present the CircuitHTN algorithm, which constructs an action graph representing the demonstrations, and then reduces the graph following rules for reducing combination electrical circuits. Evaluation on real-world household kitchen tasks shows that CircuitHTN outperforms prior work in task structure and preference learning, successfully handling large data sets and exhibiting similar action selection preferences as the demonstrations.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chen21d.html
  PDF: https://proceedings.mlr.press/v155/chen21d/chen21d.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chen21d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Kevin
    family: Chen
  - given: Nithin Shrivatsav
    family: Srikanth
  - given: David
    family: Kent
  - given: Harish
    family: Ravichandar
  - given: Sonia
    family: Chernova
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1572-1581
  id: chen21d
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1572
  lastpage: 1581
  published: 2021-10-04 00:00:00 +0000
- title: 'A Long Horizon Planning Framework for Manipulating Rigid Pointcloud Objects'
  abstract: 'We present a framework for solving long-horizon planning problems involving manipulation of rigid objects that operates directly from a point-cloud observation. Our method plans in the space of object subgoals and frees the planner from reasoning about robot-object interaction dynamics. We show that for rigid-bodies, this abstraction can be realized using low-level manipulation skills that maintain sticking-contact with the object and represent subgoals as 3D transformations. To enable generalization to unseen objects and improve planning performance, we propose a novel way of representing subgoals for rigid-body manipulation and a graph-attention based neural network architecture for processing point-cloud inputs. We experimentally validate these choices using simulated and real-world experiments on the YuMi robot. Results demonstrate that our method can successfully manipulate new objects into target configurations requiring long-term planning. Overall, our framework realizes the best of the worlds of task-and-motion planning (TAMP) and learning-based approaches. Project website: https://anthonysimeonov.github.io/rpo-planning-framework/.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/simeonov21a.html
  PDF: https://proceedings.mlr.press/v155/simeonov21a/simeonov21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-simeonov21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Anthony
    family: Simeonov
  - given: Yilun
    family: Du
  - given: Beomjoon
    family: Kim
  - given: Francois
    family: Hogan
  - given: Joshua
    family: Tenenbaum
  - given: Pulkit
    family: Agrawal
  - given: Alberto
    family: Rodriguez
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1582-1601
  id: simeonov21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1582
  lastpage: 1601
  published: 2021-10-04 00:00:00 +0000
- title: 'Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter'
  abstract: 'General robot grasping in clutter requires the ability to synthesize grasps that work for previously unseen objects and that are also robust to physical interactions, such as collisions with other objects in the scene. In this work, we design and train a network that predicts 6 DOF grasps from 3D scene information gathered from an on-board sensor such as a wrist-mounted depth camera. Our proposed Volumetric Grasping Network (VGN) accepts a Truncated Signed Distance Function (TSDF) representation of the scene and directly outputs the predicted grasp quality and the associated gripper orientation and opening width for each voxel in the queried 3D volume. We show that our approach can plan grasps in only 10 ms and is able to clear 92% of the objects in real-world clutter removal experiments without the need for explicit collision checking. The real-time capability opens up the possibility for closed-loop grasp planning, allowing robots to handle disturbances, recover from errors and provide increased robustness.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/breyer21a.html
  PDF: https://proceedings.mlr.press/v155/breyer21a/breyer21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-breyer21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Michel
    family: Breyer
  - given: Jen Jen
    family: Chung
  - given: Lionel
    family: Ott
  - given: Roland
    family: Siegwart
  - given: Juan
    family: Nieto
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1602-1611
  id: breyer21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1602
  lastpage: 1611
  published: 2021-10-04 00:00:00 +0000
- title: 'Uncertainty-Aware Constraint Learning for Adaptive Safe Motion Planning from Demonstrations'
  abstract: 'We present a method for learning to satisfy uncertain constraints from demonstrations. Our method uses robust optimization to obtain a belief over the potentially infinite set of possible constraints consistent with the demonstrations, and then uses this belief to plan trajectories that trade off performance with satisfying the possible constraints. We use these trajectories in a closed-loop policy that executes and replans using belief updates, which incorporate data gathered during execution. We derive guarantees on the accuracy of our constraint belief and probabilistic guarantees on plan safety. We present results on a 7-DOF arm and 12D quadrotor, showing our method can learn to satisfy high-dimensional (up to 30D) uncertain constraints and outperforms baselines in safety and efficiency.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chou21a.html
  PDF: https://proceedings.mlr.press/v155/chou21a/chou21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chou21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Glen
    family: Chou
  - given: Dmitry
    family: Berenson
  - given: Necmiye
    family: Ozay
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1612-1639
  id: chou21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1612
  lastpage: 1639
  published: 2021-10-04 00:00:00 +0000
- title: 'Belief-Grounded Networks for Accelerated Robot Learning under Partial Observability'
  abstract: 'Many important robotics problems are partially observable where a single visual or force-feedback measurement is insufficient to reconstruct the state. Standard approaches involve learning a policy over beliefs or observation-action histories.    However, both of these have drawbacks; it is expensive to track the belief online, and it is hard to learn policies directly over histories. We propose a method for policy learning under partial observability called the Belief-Grounded Network (BGN) in which an auxiliary belief-reconstruction loss incentivizes a neural network to concisely summarize its input history. Since the resulting policy is a function of the history rather than the belief, it can be executed easily at runtime. We compare BGN against several baselines on classic benchmark tasks as well as three novel robotic force-feedback tasks. BGN outperforms all other tested methods and its learned policies work well when transferred onto a physical robot.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/nguyen21a.html
  PDF: https://proceedings.mlr.press/v155/nguyen21a/nguyen21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-nguyen21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Hai
    family: Nguyen
  - given: Brett
    family: Daley
  - given: Xinchao
    family: Song
  - given: Christopher
    family: Amato
  - given: Robert
    family: Platt
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1640-1653
  id: nguyen21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1640
  lastpage: 1653
  published: 2021-10-04 00:00:00 +0000
- title: 'Time-Bounded Mission Planning in Time-Varying Domains with Semi-MDPs and Gaussian Processes'
  abstract: 'Uncertain, time-varying dynamic environments are ubiquitous in real world robotics. We propose an online planning framework to address time-bounded missions under time-varying dynamics, where those dynamics affect the duration and outcome of actions. We pose such problems as semi-Markov decision processes, where actions have a duration distributed according to an a priori unknown time-varying function. Our approach maintains a belief over this function, and time is propagated through a discrete search tree that efficiently maintains a subset of reachable states. We show improved mission performance on a marine vehicle simulator acting under real-world spatio-temporal ocean currents, and demonstrate the ability to solve co-safe linear temporal logic problems, which are more complex than the reachability problems tackled in previous approaches.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/duckworth21a.html
  PDF: https://proceedings.mlr.press/v155/duckworth21a/duckworth21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-duckworth21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Paul
    family: Duckworth
  - given: Bruno
    family: Lacerda
  - given: Nick
    family: Hawes
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1654-1668
  id: duckworth21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1654
  lastpage: 1668
  published: 2021-10-04 00:00:00 +0000
- title: '3D-OES: Viewpoint-Invariant Object-Factorized Environment Simulators'
  abstract: 'We propose an action-conditioned dynamics model that predicts scene changes caused by object and agent interactions in a viewpoint-invariant 3D neural scene representation space, inferred from RGB-D videos. In this 3D feature space, objects do not interfere with one another and their appearance persists over time and across viewpoints. This permits our model to predict future scenes long in the future by simply “moving” 3D object features based on cumulative object motion predictions.  Object motion predictions are computed by a graph neural network that operates over the object features extracted from the 3D neural scene representation. Our model’s simulations can be decoded by a neural renderer into 2D image views from any desired viewpoint, which aids the interpretability of our latent 3D simulation space.  We show our model generalizes well its predictions across varying number and appearances of interacting objects as well as across camera viewpoints, outperforming existing 2D and 3D dynamics models. We further demonstrate sim-to-real transfer of the learnt dynamics by applying our model trained solely in simulation to model-based control for pushing objects to desired locations under clutter on a real robotic setup.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/tung21a.html
  PDF: https://proceedings.mlr.press/v155/tung21a/tung21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-tung21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Hsiao-Yu
    family: Tung
  - given: Zhou
    family: Xian
  - given: Mihir
    family: Prabhudesai
  - given: Shamit
    family: Lal
  - given: Katerina
    family: Fragkiadaki
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1669-1683
  id: tung21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1669
  lastpage: 1683
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Object-conditioned Exploration using Distributed Soft Actor Critic'
  abstract: 'Object navigation is defined as navigating to an object of a given label in a complex, unexplored environment. In its general form, this problem poses several challenges for Robotics: semantic exploration of unknown environments in search of an object and low-level control. In this work we study object-guided exploration and low-level control, and present an end-to-end trained navigation policy achieving a success rate of 0.68 and SPL of 0.58 on unseen, visually complex scans of real homes. We propose a highly scalable implementation of an off-policy Reinforcement Learning algorithm, distributed Soft Actor Critic, which allows the system to utilize 98M experience steps in 24 hours on 8 GPUs. Our system learns to control a differential drive mobile base in simulation from a stack of high dimensional observations commonly used on robotic platforms. The learned policy is capable of object-guided exploratory behaviors and low-level control learned from pure experiences in realistic environments.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wahid21a.html
  PDF: https://proceedings.mlr.press/v155/wahid21a/wahid21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wahid21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ayzaan
    family: Wahid
  - given: Austin
    family: Stone
  - given: Kevin
    family: Chen
  - given: Brian
    family: Ichter
  - given: Alexander
    family: Toshev
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1684-1695
  id: wahid21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1684
  lastpage: 1695
  published: 2021-10-04 00:00:00 +0000
- title: 'Fast robust peg-in-hole insertion with continuous visual servoing'
  abstract: 'This paper demonstrates a visual servoing method which is robust towards uncertainties related to system calibration and grasping, while significantly reducing the peg-in-hole time compared to classical methods and recent attempts based on deep learning. The proposed visual servoing method is based on peg and hole point estimates from a deep neural network in a multi-cam setup, where the model is trained on purely synthetic data. Empirical results show that the learnt model generalizes to the real world, allowing for higher success rates and lower cycle times than existing approaches.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/haugaard21a.html
  PDF: https://proceedings.mlr.press/v155/haugaard21a/haugaard21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-haugaard21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Rasmus
    family: Haugaard
  - given: Jeppe
    family: Langaa
  - given: Christoffer
    family: Sloth
  - given: Anders
    family: Buch
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1696-1705
  id: haugaard21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1696
  lastpage: 1705
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning a natural-language to LTL executable semantic parser for grounded robotics'
  abstract: 'Children acquire their native language with apparent ease by observing how language is used in context and attempting to use it themselves. They do so without laborious annotations, negative examples, or even direct corrections. We take a step toward robots that can do the same by training a grounded semantic parser, which discovers latent linguistic representations that can be used for the execution of  natural-language commands. In particular, we focus on the difficult domain of commands with a temporal aspect, whose semantics we capture with Linear Temporal Logic, LTL. Our parser is trained with pairs of sentences and executions as well as an executor. At training time, the parser hypothesizes a meaning representation for the input as a formula in LTL. Three competing pressures allow the parser to discover  meaning from language. First, any hypothesized meaning for a sentence must be permissive enough to reflect all the annotated execution trajectories. Second, the executor — a pretrained end-to-end LTL planner — must find that the observed trajectories are  likely executions of the meaning. Finally, a generator, which reconstructs the original input, encourages the model to find representations that conserve knowledge about the command. Together these ensure that the meaning is neither too general nor too specific. Our model generalizes well, being able to parse and execute both machine-generated and human-generated commands, with near-equal accuracy, despite the fact that the human-generated sentences are much more varied and complex with an open lexicon. The approach presented here is not specific to LTL: it can be applied to any domain where sentence meanings can be hypothesized and  an executor can verify these meanings, thus opening the door to many applications for robotic agents.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wang21g.html
  PDF: https://proceedings.mlr.press/v155/wang21g/wang21g.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wang21g.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Christopher
    family: Wang
  - given: Candace
    family: Ross
  - given: Yen-Ling
    family: Kuo
  - given: Boris
    family: Katz
  - given: Andrei
    family: Barbu
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1706-1718
  id: wang21g
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1706
  lastpage: 1718
  published: 2021-10-04 00:00:00 +0000
- title: 'PLAS: Latent Action Space for Offline Reinforcement Learning'
  abstract: 'The goal of offline reinforcement learning is to learn a policy from a fixed dataset, without further interactions with the environment. This setting will be an increasingly more important paradigm for real-world applications of reinforcement learning such as robotics, in which data collection is slow and potentially dangerous. Existing off-policy algorithms have limited performance on static datasets due to extrapolation errors from out-of-distribution actions. This leads to the challenge of constraining the policy to select actions within the support of the dataset during training. We propose to simply learn the Policy in the Latent Action Space (PLAS) such that this requirement is naturally satisfied. We evaluate our method on continuous control benchmarks in simulation and a deformable object manipulation task with a physical robot. We demonstrate that our method provides competitive performance consistently across various continuous control tasks and different types of datasets, outperforming existing offline reinforcement learning methods with explicit constraints.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/zhou21b.html
  PDF: https://proceedings.mlr.press/v155/zhou21b/zhou21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-zhou21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Wenxuan
    family: Zhou
  - given: Sujay
    family: Bajracharya
  - given: David
    family: Held
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1719-1735
  id: zhou21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1719
  lastpage: 1735
  published: 2021-10-04 00:00:00 +0000
- title: 'Unsupervised Metric Relocalization Using Transform Consistency Loss'
  abstract: 'Training networks to perform metric relocalization traditionally requires accurate image correspondences. In practice, these are obtained by restricting domain coverage, employing additional sensors, or capturing large multi-view datasets. We instead propose a self-supervised solution, which exploits a key insight: localizing a query image within a map should yield the same absolute pose, regardless of the reference image used for registration. Guided by this intuition, we derive a novel transform consistency loss. Using this loss function, we train a deep neural network to infer dense feature and saliency maps to perform robust metric relocalization in dynamic environments.  We evaluate our framework on synthetic and real-world data, showing our approach outperforms other supervised methods when a limited amount of ground-truth information is available.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/kasper21a.html
  PDF: https://proceedings.mlr.press/v155/kasper21a/kasper21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-kasper21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Mike
    family: Kasper
  - given: Fernando
    family: Nobre
  - given: Christoffer
    family: Heckman
  - given: Nima
    family: Keivan
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1736-1745
  id: kasper21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1736
  lastpage: 1745
  published: 2021-10-04 00:00:00 +0000
- title: 'MultiPoint: Cross-spectral registration of thermal and optical aerial imagery'
  abstract: 'While optical cameras are ubiquitous in robotics, some robots can sense the world in several sections of the electromagnetic spectrum simultaneously, which can extend their capabilities in fundamental ways. For instance, many fixed-wing UAVs carry both optical and thermal imaging cameras, potentially allowing them to detect temperature difference-induced atmospheric updrafts, map their locations, and adjust their flight path accordingly to increase their time aloft. A key step for unlocking the potential offered by multi-spectral data is generating consistent, multi-spectral maps of the environment. In this work, we introduce MultiPoint, a novel data-driven method for generating interest points and associated descriptors for registering optical and thermal image pairs without knowledge of the relative camera viewpoints. Existing pixel-based alignment methods are accurate but too slow to work in near-real time, while feature-based methods such as SuperPoint are fast but produce poor-quality cross-spectral matches due to interest point instability in thermal images. MultiPoint capitalizes on the strengths of both approaches. An offline mutual information-based procedure is used to align cross-spectral image pairs from a training set, which are then processed by our generalized multi-spectral homographic adaptation stage to generate highly repeatable interest points that are invariant across viewpoint changes in both spectra. These are used to train a MultiPoint deep neural network by exposing  this model to both same-spectrum and cross-spectral image pairs. This model is then deployed for fast and accurate online interest point detection. We show that MultiPoint outperforms existing techniques for feature-based image alignment using a dataset of real-world thermal-optical imagery captured by a UAV during flights in different conditions and release this dataset, the first of its kind.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/achermann21a.html
  PDF: https://proceedings.mlr.press/v155/achermann21a/achermann21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-achermann21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Florian
    family: Achermann
  - given: Andrey
    family: Kolobov
  - given: Debadeepta
    family: Dey
  - given: Timo
    family: Hinzmann
  - given: Jen Jen
    family: Chung
  - given: Roland
    family: Siegwart
  - given: Nicholas
    family: Lawrance
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1746-1760
  id: achermann21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1746
  lastpage: 1760
  published: 2021-10-04 00:00:00 +0000
- title: 'TartanVO: A Generalizable Learning-based VO'
  abstract: 'We present the first learning-based visual odometry (VO) model, which generalizes to multiple datasets and real-world scenarios and outperforms geometry-based methods in challenging scenes. We achieve this by leveraging the SLAM dataset TartanAir, which provides a large amount of diverse synthetic data in challenging environments. Furthermore, to make our VO model generalize across datasets, we propose an up-to-scale loss function and incorporate the camera intrinsic parameters into the model. Experiments show that a single model, TartanVO, trained only on synthetic data, without any finetuning, can be generalized to real-world datasets such as KITTI and EuRoC, demonstrating significant advantages over the geometry-based methods on challenging trajectories. Our code is available at https://github.com/castacks/tartanvo.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wang21h.html
  PDF: https://proceedings.mlr.press/v155/wang21h/wang21h.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wang21h.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Wenshan
    family: Wang
  - given: Yaoyu
    family: Hu
  - given: Sebastian
    family: Scherer
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1761-1772
  id: wang21h
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1761
  lastpage: 1772
  published: 2021-10-04 00:00:00 +0000
- title: 'Soft Multicopter Control Using Neural Dynamics Identification'
  abstract: 'We propose a data-driven method to automatically generate feedback controllers for soft multicopters featuring deformable materials, non-conventional geometries, and asymmetric rotor layouts, to deliver compliant deformation and agile locomotion. Our approach coordinates two sub-systems: a physics-inspired network ensemble that simulates the soft drone dynamics and a custom LQR control loop enhanced by a novel online-relinearization scheme to control the neural dynamics. Harnessing the insights from deformation mechanics, we design a decomposed state formulation whose modularity and compactness facilitate the dynamics learning while its measurability readies it for real-world adaptation. Our method is painless to implement, and requires only conventional, low-cost gadgets for fabrication. In a high-fidelity simulation environment, we demonstrate the efficacy of our approach by controlling a variety of customized soft multicopters to perform hovering, target reaching, velocity tracking, and active deformation.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/deng21a.html
  PDF: https://proceedings.mlr.press/v155/deng21a/deng21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-deng21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yitong
    family: Deng
  - given: Yaorui
    family: Zhang
  - given: Xingzhe
    family: He
  - given: Shuqi
    family: Yang
  - given: Yunjin
    family: Tong
  - given: Michael
    family: Zhang
  - given: Daniel
    family: DiPietro
  - given: Bo
    family: Zhu
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1773-1782
  id: deng21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1773
  lastpage: 1782
  published: 2021-10-04 00:00:00 +0000
- title: 'Safe Optimal Control Using Stochastic Barrier Functions and Deep Forward-Backward SDEs'
  abstract: 'This paper introduces a new formulation for stochastic optimal control and stochastic  dynamic  optimization that ensures safety with respect to state and control constraints.  The proposed  methodology brings together concepts such as  Forward-Backward Stochastic Differential Equations,  Stochastic  Barrier Functions, Differentiable Convex Optimization and Deep Learning.  Using the aforementioned concepts, a Neural Network architecture is designed for safe trajectory optimization in which learning can be performed in  an end-to-end fashion. Simulations are performed on three systems to show the efficacy of the proposed methodology.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/pereira21a.html
  PDF: https://proceedings.mlr.press/v155/pereira21a/pereira21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-pereira21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Marcus
    family: Pereira
  - given: Ziyi
    family: Wang
  - given: Ioannis
    family: Exarchos
  - given: Evangelos
    family: Theodorou
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1783-1801
  id: pereira21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1783
  lastpage: 1801
  published: 2021-10-04 00:00:00 +0000
- title: 'Diverse Plausible Shape Completions from Ambiguous Depth Images'
  abstract: 'We propose PSSNet, a network architecture for generating diverse plausible 3D reconstructions from a single 2.5D depth image. Existing methods tend to produce only small variations on a single shape, even when multiple shapes are consistent with an observation. To obtain diversity we alter a Variational Auto Encoder by providing a learned shape bounding box feature as side information during training. Since these features are known during training, we are able to add a supervised loss to the encoder and noiseless values to the decoder. To evaluate, we sample a set of completions from a network, construct a set of plausible shape matches for each test observation, and compare using our plausible diversity metric defined over sets of shapes. We perform experiments using Shapenet mugs and partially-occluded YCB objects and find that our method performs comparably in datasets with little ambiguity, and outperforms existing methods when many shapes plausibly fit an observed depth image. We demonstrate one use for PSSNet on a physical robot when grasping objects in occlusion and clutter.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/saund21a.html
  PDF: https://proceedings.mlr.press/v155/saund21a/saund21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-saund21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Bradley
    family: Saund
  - given: Dmitry
    family: Berenson
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1802-1813
  id: saund21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1802
  lastpage: 1813
  published: 2021-10-04 00:00:00 +0000
- title: 'Multiagent Rollout and Policy Iteration for POMDP with Application to Multi-Robot Repair Problems'
  abstract: 'In this paper we consider infinite horizon discounted dynamic programming problems with finite state and control spaces, partial state observations, and a multiagent structure. We discuss and compare algorithms that simultaneously or sequentially optimize the agents’ controls by using multistep lookahead, truncated rollout with a known base policy, and a terminal cost function approximation. Our methods specifically address the computational challenges of partially observable multiagent problems. In particular: 1) We consider rollout algorithms that dramatically reduce required computation while preserving the key cost improvement property of the standard rollout method. The per-step computational requirements for our methods are on the order of $O(Cm)$ as compared with $O(C^m)$ for standard rollout, where $C$ is the maximum cardinality of the constraint set for the control component of each agent, and $m$ is the number of agents. 2) We show that our methods can be applied to challenging problems with a graph structure, including a class of robot repair problems whereby multiple robots collaboratively inspect and repair a system under partial information. 3) We provide a simulation study that compares our methods with existing methods, and demonstrate that our methods can handle larger and more complex partially observable multiagent problems (state space size $10^{37}$ and control space size $10^{7}$, respectively). In particular, we verify experimentally that our multiagent rollout methods perform nearly as well as standard rollout for problems with few agents, and produce satisfactory policies for problems with a larger number of agents that are intractable by standard rollout and other state of the art methods. Finally, we incorporate our multiagent rollout algorithms as building blocks in an approximate policy iteration scheme, where successive rollout policies are approximated by using neural network classifiers. While this scheme requires a strictly off-line implementation, it works well in our computational experiments and produces additional significant performance improvement over the single online rollout iteration method.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/bhattacharya21a.html
  PDF: https://proceedings.mlr.press/v155/bhattacharya21a/bhattacharya21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-bhattacharya21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sushmita
    family: Bhattacharya
  - given: Siva
    family: Kailas
  - given: Sahil
    family: Badyal
  - given: Stephanie
    family: Gil
  - given: Dimitri
    family: Bertsekas
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1814-1828
  id: bhattacharya21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1814
  lastpage: 1828
  published: 2021-10-04 00:00:00 +0000
- title: 'Few-shot Object Grounding and Mapping for Natural Language Robot Instruction Following'
  abstract: 'We study the problem of learning a robot policy to follow natural language instructions that can be easily extended to reason about new objects. We introduce a few-shot language-conditioned object grounding method trained from augmented reality data that uses exemplars to identify objects and align them to their mentions in instructions. We present a learned map representation that encodes object locations and their instructed use, and construct it from our few-shot grounding output. We integrate this mapping approach into an instruction-following policy, thereby allowing it to  reason about previously unseen objects at test-time by simply adding exemplars. We evaluate on the task of learning to map raw observations and instructions to continuous control of a physical quadcopter. Our approach significantly outperforms the prior state of the art in the presence of new objects, even when the prior approach observes all objects during training.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/blukis21a.html
  PDF: https://proceedings.mlr.press/v155/blukis21a/blukis21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-blukis21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Valts
    family: Blukis
  - given: Ross
    family: Knepper
  - given: Yoav
    family: Artzi
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1829-1854
  id: blukis21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1829
  lastpage: 1854
  published: 2021-10-04 00:00:00 +0000
- title: 'Deep Latent Competition: Learning to Race Using Visual Control Policies in Latent Space'
  abstract: 'Learning competitive behaviors in multi-agent settings such as racing requires long-term reasoning about potential adversarial interactions. This paper presents Deep Latent Competition (DLC), a novel reinforcement learning algorithm that learns competitive visual control policies through self-play in imagination. The DLC agent imagines multi-agent interaction sequences in the compact latent space of a learned world model that combines a joint transition function with opponent viewpoint prediction. Imagined self-play reduces costly sample generation in the real world, while the latent representation enables planning to scale gracefully with observation dimensionality.  We demonstrate the effectiveness of our algorithm in learning competitive behaviors on a novel multi-agent racing benchmark that requires planning from image observations. Code and videos available at https://sites.google.com/view/deep-latent-competition.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/schwarting21a.html
  PDF: https://proceedings.mlr.press/v155/schwarting21a/schwarting21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-schwarting21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Wilko
    family: Schwarting
  - given: Tim
    family: Seyde
  - given: Igor
    family: Gilitschenski
  - given: Lucas
    family: Liebenwein
  - given: Ryan
    family: Sander
  - given: Sertac
    family: Karaman
  - given: Daniela
    family: Rus
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1855-1870
  id: schwarting21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1855
  lastpage: 1870
  published: 2021-10-04 00:00:00 +0000
- title: 'TriFinger: An Open-Source Robot for Learning Dexterity'
  abstract: 'Dexterous object manipulation is still an open problem in robotics, despite the rapid progress in machine learning during the past decade. We argue that a key issue which has hindered progress is the high cost of experimentation on real systems, in terms of both time and money. We address this problem by proposing a novel open-source robotic platform, consisting of hardware and software, to drastically reduce the cost of experimentation. The hardware is inexpensive yet highly dynamic, robust, and capable of complex contact interaction with external objects. The software allows for 1-kilohertz real-time control and performs safety checks to prevent the hardware from breaking. These properties enable the platform to run without human supervision. In addition, we provide easy-to-use C++ and Python interfaces. We illustrate the potential of the proposed platform by performing an object-manipulation task using an optimal-control algorithm and training a learning-based method directly on the real system.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/wuthrich21a.html
  PDF: https://proceedings.mlr.press/v155/wuthrich21a/wuthrich21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-wuthrich21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Manuel
    family: Wuthrich
  - given: Felix
    family: Widmaier
  - given: Felix
    family: Grimminger
  - given: Shruti
    family: Joshi
  - given: Vaibhav
    family: Agrawal
  - given: Bilal
    family: Hammoud
  - given: Majid
    family: Khadiv
  - given: Miroslav
    family: Bogdanovic
  - given: Vincent
    family: Berenz
  - given: Julian
    family: Viereck
  - given: Maximilien
    family: Naveau
  - given: Ludovic
    family: Righetti
  - given: Bernhard
    family: Schölkopf
  - given: Stefan
    family: Bauer
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1871-1882
  id: wuthrich21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1871
  lastpage: 1882
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning to Improve Multi-Robot Hallway Navigation'
  abstract: 'As multi-robot applications become more prevalent, it becomes necessary to develop navigation systems which allow autonomous mobile robots to efficiently and safely pass each other in confined spaces. Existing navigation systems, such as the widely used ROS Navigation Stack, usually produce safe, collision free paths in static environments. However, these systems are not perfect, and when multiple mobile robots simultaneously navigate in narrow spaces, collisions and turnarounds are not uncommon. Fine-tuning and enhancing such navigation stacks is not as simple as it looks since they are made up of multiple layers of code, and there exists a tradeoff between optimizing for efficiency, i.e. minimizing time to destination (TTD) vs. optimizing for safety, i.e. minimizing collisions, with each objective leading to a different combination of parameter values. In this paper we develop a methodology to improve existing navigation stacks with regards to both objectives, without tuning their parameters, while preserving their inherent safety control properties. Our proposed approach is a decentralized learning-based approach that is geared toward real world robotic deployment, by requiring little computing resources. It is agnostic of the underlying navigation stack and can adapt to different types of environmental layouts (i.e., hallway structures).'
  volume: 155
  URL: https://proceedings.mlr.press/v155/park21a.html
  PDF: https://proceedings.mlr.press/v155/park21a/park21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-park21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jin Soo
    family: Park
  - given: Brian
    family: Tsang
  - given: Harel
    family: Yedidsion
  - given: Garrett
    family: Warnell
  - given: Daehyun
    family: Kyoung
  - given: Peter
    family: Stone
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1883-1895
  id: park21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1883
  lastpage: 1895
  published: 2021-10-04 00:00:00 +0000
- title: 'ACNMP: Skill Transfer and Task Extrapolation through Learning from Demonstration and Reinforcement Learning via Representation Sharing'
  abstract: 'To equip robots with dexterous skills, an effective approach is to first transfer the desired skill via Learning from Demonstration (LfD), then let the robot improve it by self-exploration via Reinforcement Learning (RL). In this paper, we propose a novel LfD+RL framework, namely Adaptive Conditional Neural Movement Primitives (ACNMP), that allows efficient policy improvement in novel environments and effective skill transfer between different agents. This is achieved through exploiting the latent representation learned by the underlying Conditional Neural Process (CNP) model, and simultaneous training of the model with supervised learning (SL) for acquiring the demonstrated trajectories and via RL for new trajectory discovery. Through simulation experiments, we show that (i) ACNMP enables the system to extrapolate to situations where pure LfD fails; (ii) Simultaneous training of the system through SL and RL  preserves the shape of demonstrations while adapting to novel situations due to the shared representations used by both learners; (iii) ACNMP enables order-of-magnitude sample-efficient RL in extrapolation of reaching tasks compared to the existing approaches; (iv) ACNMPs can be used to implement skill transfer between robots having different morphology, with competitive learning speeds and importantly with less number of assumptions compared to the state-of-the-art approaches. Finally, we show the real-world suitability of ACNMPs through real robot experiments that involve obstacle avoidance, pick and place and pouring actions.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/akbulut21a.html
  PDF: https://proceedings.mlr.press/v155/akbulut21a/akbulut21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-akbulut21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Mete
    family: Akbulut
  - given: Erhan
    family: Oztop
  - given: Muhammet Yunus
    family: Seker
  - given: Hh
    family: X
  - given: Ahmet
    family: Tekden
  - given: Emre
    family: Ugur
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1896-1907
  id: akbulut21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1896
  lastpage: 1907
  published: 2021-10-04 00:00:00 +0000
- title: 'Unsupervised Monocular Depth Learning in Dynamic Scenes'
  abstract: 'We present a method for jointly training the estimation of depth, ego-motion, and a dense 3D translation field of objects relative to the scene, with monocular photometric consistency being the sole source of supervision. We show that this apparently heavily underdetermined problem can be regularized by imposing the following prior knowledge about 3D translation fields: they are sparse, since most of the scene is static, and they tend to be piecewise constant for rigid moving objects. We show that this regularization alone is sufficient to train monocular depth prediction models that exceed the accuracy achieved in prior work for dynamic scenes, including methods that require semantic input.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/li21a.html
  PDF: https://proceedings.mlr.press/v155/li21a/li21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-li21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Hanhan
    family: Li
  - given: Ariel
    family: Gordon
  - given: Hang
    family: Zhao
  - given: Vincent
    family: Casser
  - given: Anelia
    family: Angelova
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1908-1917
  id: li21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1908
  lastpage: 1917
  published: 2021-10-04 00:00:00 +0000
- title: 'BayesRace: Learning to race autonomously using prior experience'
  abstract: 'Autonomous race cars require perception, estimation, planning, and control modules which work together asynchronously while driving at the limit of a vehicle’s handling capability. A fundamental challenge encountered in designing these software components lies in predicting the vehicle’s future state (e.g. position, orientation, and speed) with high accuracy. The root cause is the difficulty in identifying vehicle model parameters that capture the effects of lateral tire slip. We present a model-based planning and control framework for autonomous racing that significantly reduces the effort required in system identification and control design. Our approach alleviates the gap induced by simulation-based controller design by learning from on-board sensor measurements. A major focus of this work is empirical, thus, we demonstrate our contributions by experiments on validated 1:43 and 1:10 scale autonomous racing simulations.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/jain21b.html
  PDF: https://proceedings.mlr.press/v155/jain21b/jain21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-jain21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Achin
    family: Jain
  - given: Matthew
    family: O’Kelly
  - given: Pratik
    family: Chaudhari
  - given: Manfred
    family: Morari
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1918-1929
  id: jain21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1918
  lastpage: 1929
  published: 2021-10-04 00:00:00 +0000
- title: 'Model-Based Inverse Reinforcement Learning from Visual Demonstrations'
  abstract: 'Scaling model-based inverse reinforcement learning (IRL) to real robotic manipulation tasks with unknown dynamics remains an open problem. The key challenges lie in learning good dynamics models, developing algorithms that scale to high-dimensional state-spaces and being able to learn from both visual and proprioceptive demonstrations. In this work, we present a gradient-based inverse reinforcement learning framework that utilizes a pre-trained visual dynamics model to learn cost functions when given only visual human demonstrations. The learned cost functions are then used to reproduce the demonstrated behavior via visual model predictive control. We evaluate our framework on hardware on two basic object manipulation tasks.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/das21a.html
  PDF: https://proceedings.mlr.press/v155/das21a/das21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-das21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Neha
    family: Das
  - given: Sarah
    family: Bechtle
  - given: Todor
    family: Davchev
  - given: Dinesh
    family: Jayaraman
  - given: Akshara
    family: Rai
  - given: Franziska
    family: Meier
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1930-1942
  id: das21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1930
  lastpage: 1942
  published: 2021-10-04 00:00:00 +0000
- title: 'Deep Reactive Planning in Dynamic Environments'
  abstract: 'The main novelty of the proposed approach is that it allows a robot to learn an end-to-end policy which can adapt to changes in the environment during execution. While goal conditioning of policies has been studied in the RL literature, such approaches are not easily extended to settings where the robot’s goal can change during execution. This is something that humans are naturally able to do. However, it is difficult for robots to learn such reflexes (i.e., to naturally respond to dynamic environments), especially when the goal location is not explicitly provided to the robot, and instead needs to be perceived through a vision sensor. In the current work, we present a method that can achieve such behavior by combining traditional kinematic planning, deep learning, and deep reinforcement learning in a synergistic fashion to generalize to arbitrary environments. We demonstrate the proposed approach for several reaching and pick-and-place tasks in simulation, as well as on a real system of a 6-DoF industrial manipulator.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ota21a.html
  PDF: https://proceedings.mlr.press/v155/ota21a/ota21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ota21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Kei
    family: Ota
  - given: Devesh
    family: Jha
  - given: Tadashi
    family: Onishi
  - given: Asako
    family: Kanezaki
  - given: Yusuke
    family: Yoshiyasu
  - given: Yoko
    family: Sasaki
  - given: Toshisada
    family: Mariyama
  - given: Daniel
    family: Nikovski
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1943-1957
  id: ota21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1943
  lastpage: 1957
  published: 2021-10-04 00:00:00 +0000
- title: 'Reactive motion planning with probabilisticsafety guarantees'
  abstract: 'Motion planning in environments with multiple agents is critical to many important autonomous applications such as autonomous vehicles and assistive robots. This paper considers the problem of motion planning, where the controlled agent shares the environment with multiple uncontrolled agents. First, a predictive model of the uncontrolled agents is trained to predict all possible trajectories within a short horizon based on the scenario. The prediction is then fed to a motion planning module based on model predictive control. We proved generalization bound for the predictive model using three different methods, post-bloating, support vector machine (SVM), and conformal analysis, all capable of generating stochastic guarantees of the correctness of the predictor. The proposed approach is demonstrated in simulation in a scenario emulating autonomous highway driving.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chen21e.html
  PDF: https://proceedings.mlr.press/v155/chen21e/chen21e.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chen21e.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuxiao
    family: Chen
  - given: Ugo
    family: Rosolia
  - given: Chuchu
    family: Fan
  - given: Aaron
    family: Ames
  - given: Richard
    family: Murray
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1958-1970
  id: chen21e
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1958
  lastpage: 1970
  published: 2021-10-04 00:00:00 +0000
- title: 'Hierarchical Robot Navigation in Novel Environments using Rough 2-D Maps'
  abstract: 'In robot navigation, generalizing quickly to unseen environments is essential.  Hierarchical methods inspired by human navigation have been proposed, typically consisting of a high-level landmark proposer and a low-level controller. However, these methods either require precise high-level information to be given in advance, or need to construct such guidance from extensive interaction with the environment. In this work, we propose an approach that leverages a rough 2-D map of the environment to navigate in novel environments without requiring further learning. In particular, we introduce a dynamic topological map that can be initialized from the rough 2-D map along with a high-level planning approach for proposing reachable 2-D map patches of the intermediate landmarks between the start and goal locations. To use proposed 2-D patches, we train a deep generative model to generate intermediate landmarks in observation space which are used as subgoals by low-level goal-conditioned reinforcement learning. Importantly, because the low-level controller is only trained with local behaviors (e.g. go across the intersection, turn left at a corner) on existing environments, this framework allows us to generalize to novel environments given only a rough 2-D map, without requiring further learning. Experimental results demonstrate the effectiveness of the proposed framework in both seen and novel environments.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/xu21d.html
  PDF: https://proceedings.mlr.press/v155/xu21d/xu21d.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-xu21d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Chengguang
    family: Xu
  - given: Christopher
    family: Amato
  - given: Lawson
    family: Wong
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1971-1991
  id: xu21d
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1971
  lastpage: 1991
  published: 2021-10-04 00:00:00 +0000
- title: 'Visual Imitation Made Easy'
  abstract: 'Visual imitation learning provides a framework for learning complex manipulation behaviors by leveraging human demonstrations. However, current interfaces for imitation such as kinesthetic teaching or teleoperation prohibitively restrict our ability to efficiently collect large-scale data in the wild. Obtaining such diverse demonstration data is paramount for the generalization of learned skills to novel scenarios. In this work, we present an alternate interface for imitation that simplifies the data collection process while allowing for easy transfer to robots. We use commercially available reacher-grabber assistive tools both as a data collection device and as the robot’s end-effector. To extract action information from these visual demonstrations, we use off-the-shelf Structure from Motion (SfM) techniques in addition to training a finger detection network. We experimentally evaluate on two challenging tasks: non-prehensile pushing and prehensile stacking, with 1000 diverse demonstrations for each task. For both tasks, we use standard behavior cloning to learn executable policies from the previously collected offline demonstrations. To improve learning performance, we employ a variety of data augmentations and provide an extensive analysis of its effects. Finally, we demonstrate the utility of our interface by evaluating on real robotic scenarios with previously unseen objects and achieve a 87% success rate on pushing and a 62% success rate on stacking. Robot videos are available at our  project website: https://sites.google.com/view/visual-imitation-made-easy .'
  volume: 155
  URL: https://proceedings.mlr.press/v155/young21a.html
  PDF: https://proceedings.mlr.press/v155/young21a/young21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-young21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sarah
    family: Young
  - given: Dhiraj
    family: Gandhi
  - given: Shubham
    family: Tulsiani
  - given: Abhinav
    family: Gupta
  - given: Pieter
    family: Abbeel
  - given: Lerrel
    family: Pinto
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 1992-2005
  id: young21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 1992
  lastpage: 2005
  published: 2021-10-04 00:00:00 +0000
- title: 'DeepMPCVS: Deep Model Predictive Control for Visual Servoing'
  abstract: 'The simplicity of the visual servoing approach makes it an attractive option for tasks dealing with vision-based control of robots in many real-world applications. However, attaining precise alignment for unseen environments pose a challenge to existing visual servoing approaches. While classical approaches assume a perfect world, the recent data-driven approaches face issues when generalizing to novel environments. In this paper, we aim to combine the best of both worlds. We present a deep model predictive visual servoing framework that can achieve precise alignment with optimal trajectories and can generalize to novel environments. Our framework consists of a deep network for optical flow predictions, which are used along with a predictive model to forecast future optical flow. For generating an optimal set of velocities we present a control network that can be trained on-the-fly without any supervision. Through extensive simulations on photo-realistic indoor settings of the popular Habitat framework, we show significant performance gain due to the proposed formulation vis-a-vis recent state of the art methods. Specifically, we show vastly improved performance in trajectory length and faster convergence over recent approaches.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/katara21a.html
  PDF: https://proceedings.mlr.press/v155/katara21a/katara21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-katara21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Pushkal
    family: Katara
  - given: Harish
    family: YVS
  - given: Harit
    family: Pandya
  - given: Abhinav
    family: Gupta
  - given: AadilMehdi
    family: Sanchawala
  - given: Gourav
    family: Kumar
  - given: Brojeshwar
    family: Bhowmick
  - given: Madhava
    family: Krishna
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2006-2015
  id: katara21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2006
  lastpage: 2015
  published: 2021-10-04 00:00:00 +0000
- title: 'Deep Reinforcement Learning with Population-Coded Spiking Neural Network for Continuous Control'
  abstract: 'The energy-efficient control of mobile robots has become crucial as the complexity of their real-world applications increasingly involves high-dimensional observation and action spaces, which cannot be offset by their limited on-board resources. An emerging non-Von Neumann model of intelligence, where spiking neural networks (SNNs) are executed on neuromorphic processors, is now considered as an energy-efficient and robust alternative to the state-of-the-art real-time robotic controllers for low dimensional control tasks. The challenge now for this new computing paradigm is to scale so that it can keep up with real-world applications. To do so, SNNs need to overcome the inherent limitations of their training, namely the limited ability of their spiking neurons to represent information and the lack of effective learning algorithms. Here, we propose a population-coded spiking actor network (PopSAN) that was trained in conjunction with a deep critic network using deep reinforcement learning (DRL). The population coding scheme, which is prevalent across brain networks, dramatically increased the representation capacity of the network and the hybrid learning combined the training advantages of deep networks with the energy-efficient inference of spiking networks. To show that our approach can be used for general-purpose spike-based reinforcement learning, we demonstrated its integration with a wide spectrum of policy-gradient based DRL methods covering both on-policy and off-policy DRL algorithms. We deployed the trained PopSAN on Intel’s Loihi neuromorphic chip and benchmarked our method against the mainstream DRL algorithms for continuous control. To allow for a fair comparison among all methods, we validated them on OpenAI gym tasks. Our Loihi-run PopSAN consumed 140 times less energy per inference when compared against the deep actor network on Jetson TX2, and achieved the same level of performance. Our results demonstrate the overall efficiency of neuromorphic controllers and suggest the hybrid reinforcement learning approach as an alternative to deep learning, when both energy-efficiency and robustness are important.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/tang21a.html
  PDF: https://proceedings.mlr.press/v155/tang21a/tang21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-tang21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Guangzhi
    family: Tang
  - given: Neelesh
    family: Kumar
  - given: Raymond
    family: Yoo
  - given: Konstantinos
    family: Michmizos
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2016-2029
  id: tang21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2016
  lastpage: 2029
  published: 2021-10-04 00:00:00 +0000
- title: 'Tolerance-Guided Policy Learning for Adaptable and Transferrable Delicate Industrial Insertion'
  abstract: 'Policy learning for delicate industrial insertion tasks (e.g., PC board assembly) is challenging. This paper considers two major problems: how to learn a diversified policy (instead of just one average policy) that can efficiently handle different workpieces with minimum amount of training data, and how to handle defects of workpieces during insertion. To address the problems, we propose tolerance-guided policy learning. To encourage transferability of the learned policy to different workpieces, we add a task embedding to the policy’s input space using the insertion tolerance. Then we train the policy using generative adversarial imitation learning with reward shaping (RS-GAIL) on a variety of representative situations. To encourage adaptability of the learned policy to handle defects, we build a probabilistic inference model that can output the best inserting pose based on failed insertions using the tolerance model. The best inserting pose is then used as a reference to the learned policy. This proposed method is validated on a sequence of IC socket insertion tasks in simulation. The results show that 1) RS-GAIL can efficiently learn optimal policies under sparse rewards; 2) the tolerance embedding can enhance the transferability of the learned policy; 3) the probabilistic inference makes the policy robust to defects on the workpieces.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/niu21a.html
  PDF: https://proceedings.mlr.press/v155/niu21a/niu21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-niu21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Boshen
    family: Niu
  - given: Chenxi
    family: Wang
  - given: Changliu
    family: Liu
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2030-2039
  id: niu21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2030
  lastpage: 2039
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Vision-based Reactive Policies for Obstacle Avoidance'
  abstract: 'In this paper, we address the problem of vision-based obstacle avoidance for robotic manipulators. This topic poses challenges for both perception and motion generation. While most work in the field aims at improving one of those aspects, we provide a unified framework for approaching this problem. The main goal of this framework is to connect perception and motion by identifying the relationship between the visual input and the corresponding motion representation. To this end, we propose a method for learning reactive obstacle avoidance policies. We evaluate our method on goal-reaching tasks for single and multiple obstacles scenarios. We show the ability of the proposed method to efficiently learn stable obstacle avoidance strategies at a high success rate while maintaining closed-loop responsiveness required for critical applications like human-robot interaction.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/aljalbout21a.html
  PDF: https://proceedings.mlr.press/v155/aljalbout21a/aljalbout21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-aljalbout21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Elie
    family: Aljalbout
  - given: Ji
    family: Chen
  - given: Konstantin
    family: Ritt
  - given: Maximilian
    family: Ulmer
  - given: Sami
    family: Haddadin
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2040-2054
  id: aljalbout21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2040
  lastpage: 2054
  published: 2021-10-04 00:00:00 +0000
- title: 'Sampling-based Reachability Analysis: A Random Set Theory Approach with Adversarial Sampling'
  abstract: 'Reachability analysis is at the core of many applications, from neural network verification, to safe trajectory planning of uncertain systems. However, this problem is notoriously challenging, and current approaches tend to be either too restrictive, too slow, too conservative, or approximate and therefore lack guarantees. In this paper, we propose a simple yet effective sampling-based approach to perform reachability analysis for arbitrary dynamical systems. Our key novel idea consists of using random set theory to give a rigorous interpretation of our method, and prove that it returns sets which are guaranteed to converge to the convex hull of the true reachable sets. Additionally, we leverage recent work on robust deep learning and propose a new adversarial sampling approach to robustify our algorithm and accelerate its convergence. We demonstrate that our method is faster and less conservative than prior work, present results for approximate reachability analysis of neural networks and robust trajectory optimization of high-dimensional uncertain nonlinear systems, and discuss future applications.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/lew21a.html
  PDF: https://proceedings.mlr.press/v155/lew21a/lew21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-lew21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Thomas
    family: Lew
  - given: Marco
    family: Pavone
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2055-2070
  id: lew21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2055
  lastpage: 2070
  published: 2021-10-04 00:00:00 +0000
- title: 'Transformers for One-Shot Visual Imitation'
  abstract: 'Humans are able to seamlessly visually imitate others, by inferring their intentions and using past experience to achieve the same end goal. In other words, we can parse complex semantic knowledge from raw video and efficiently translate that into concrete motor control. Is it possible to give a robot this same capability? Prior research in robot imitation learning has created agents which can acquire diverse skills from expert human operators. However, expanding these techniques to work with a single positive example during test time is still an open challenge. Apart from control, the difficulty stems from mismatches between the demonstrator and robot domains. For example, objects may be placed in different locations (e.g. kitchen layouts are different in every house). Additionally, the demonstration may come from an agent with different morphology and physical appearance (e.g. human), so one-to-one action correspondences are not available. This paper investigates techniques which allow robots to partially bridge these domain gaps, using their past experience. A neural network is trained to mimic ground truth robot actions given context video from another agent, and must generalize to unseen task instances when prompted with new videos during test time. We hypothesize that our policy representations must be both context driven and dynamics aware in order to perform these tasks. These assumptions are baked into the neural network using the Transformers attention mechanism and a self-supervised inverse dynamics loss. Finally, we experimentally determine that our method accomplishes a 2x improvement in terms of task success rate over prior baselines in a suite of one-shot manipulation tasks.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/dasari21a.html
  PDF: https://proceedings.mlr.press/v155/dasari21a/dasari21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-dasari21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sudeep
    family: Dasari
  - given: Abhinav
    family: Gupta
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2071-2084
  id: dasari21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2071
  lastpage: 2084
  published: 2021-10-04 00:00:00 +0000
- title: 'Self-Supervised 3D Keypoint Learning for Ego-Motion Estimation'
  abstract: 'Detecting and matching robust viewpoint-invariant keypoints is critical for visual SLAM and Structure-from-Motion. State-of-the-art learning-based methods generate training samples via homography adaptation to create 2D synthetic views with known keypoint matches from a single image. This approach does not, however, generalize to non-planar 3D scenes with illumination variations commonly seen in real-world videos. In this work, we propose self-supervised learning depth-aware keypoints from unlabeled videos directly. We jointly learn keypoint and depth estimation networks by combining appearance and geometric matching via a differentiable structure-from-motion module based on Procrustean residual pose correction. We show how our self-supervised keypoints can be trivially incorporated into state-of-the-art visual odometry frameworks for robust and accurate ego-motion estimation of autonomous vehicles in real-world conditions.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/tang21b.html
  PDF: https://proceedings.mlr.press/v155/tang21b/tang21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-tang21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jiexiong
    family: Tang
  - given: Rares
    family: Ambrus
  - given: Vitor
    family: Guizilini
  - given: Sudeep
    family: Pillai
  - given: Hanme
    family: Kim
  - given: Patric
    family: Jensfelt
  - given: Adrien
    family: Gaidon
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2085-2103
  id: tang21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2085
  lastpage: 2103
  published: 2021-10-04 00:00:00 +0000
- title: 'Self-Supervised Learning of Scene-Graph Representations for Robotic Sequential Manipulation Planning'
  abstract: 'We present a self-supervised representation learning approach for visual reasoning and integrate it into a nonlinear program formulation for motion optimization to tackle sequential manipulation tasks. Such problems have usually been addressed by combined task and motion planning approaches, for which spatial relations and logical rules that rely on symbolic representations have to be predefined by the user. We propose to learn relational structures by leveraging visual perception to alleviate the resulting knowledge acquisition bottleneck. In particular, we learn constructing {\em scene-graphs}, that represent objects (“red box"), and their spatial relationships (‘yellow cylinder on red box"). This representation allows us to plan high-level discrete decisions effectively using graph search algorithms. We integrate the visual reasoning module with a nonlinear optimization method for robot motion planning and verify its feasibility on the classic blocks-world domain. Our proposed framework successfully finds the sequence of actions and enables the robot to execute feasible motion plans to realize the given tasks.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/nguyen21b.html
  PDF: https://proceedings.mlr.press/v155/nguyen21b/nguyen21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-nguyen21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Son
    family: Nguyen
  - given: Ozgur
    family: Oguz
  - given: Valentin
    family: Hartmann
  - given: Marc
    family: Toussaint
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2104-2119
  id: nguyen21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2104
  lastpage: 2119
  published: 2021-10-04 00:00:00 +0000
- title: 'Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning'
  abstract: 'One of the great promises of robot learning systems is that they will be able to learn from their mistakes and continuously adapt to ever-changing environments. Despite this potential, most of the robot learning systems today produce static policies that are not further adapted during deployment, because the algorithms which produce those policies are not designed for continual adaptation. We present an adaptation method, and empirical evidence that it supports a robot learning framework for continual adaption. We show that this very simple method-fine-tuning off-policy reinforcement learning using offline datasets–is robust to changes in background, object shape and appearance, lighting conditions, and robot morphology. We demonstrate how to adapt vision-based robotic manipulation policies to new variations using less than 0.2% of the data necessary to learn the task from scratch. Furthermore, we demonstrate that this robustness holds in an episodic continual learning setting. We also show that pre-training via RL is essential: training from scratch or adapting from super vised ImageNet features are both unsuccessful with such small amounts of data. Our empirical conclusions are consistently supported by experiments on simulated manipulation tasks, and by 60 unique fine-tuning experiments on a real robotic grasping system pre-trained on 580,000 grasps. For video results and an overview of the methods and experiments in this study, see the project website at \url{https://ryanjulian.me/continual-fine-tuning}.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/julian21a.html
  PDF: https://proceedings.mlr.press/v155/julian21a/julian21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-julian21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ryan
    family: Julian
  - given: Benjamin
    family: Swanson
  - given: Gaurav
    family: Sukhatme
  - given: Sergey
    family: Levine
  - given: Chelsea
    family: Finn
  - given: Karol
    family: Hausman
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2120-2136
  id: julian21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2120
  lastpage: 2136
  published: 2021-10-04 00:00:00 +0000
- title: 'Explicitly Encouraging Low Fractional Dimensional Trajectories Via Reinforcement Learning'
  abstract: 'A key limitation in using various modern methods of machine learning in developing feedback control policies is the lack of appropriate methodologies to analyze their long-term dynamics, in terms of making any sort of guarantees (even statistically) about robustness.  The central reasons for this are largely due to the so-called curse of dimensionality, combined with the black-box nature of the resulting control policies themselves. This paper aims at the first of these issues. Although the full state space of a system may be quite large in dimensionality, it is a common feature of most model-based control methods that the resulting closed-loop systems demonstrate dominant dynamics that are rapidly driven to some lower-dimensional sub-space within. In this work we argue that the dimensionality of this subspace is captured by tools from fractal geometry, namely various notions of a fractional dimension. We then show that the dimensionality of trajectories induced by model free reinforcement learning agents can be influenced adding a post processing function to the agents reward signal. We verify that the dimensionality reduction is robust to noise being added to the system and show that that the modified agents are more actually more robust to noise and push disturbances in general for the systems we examined.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/gillen21a.html
  PDF: https://proceedings.mlr.press/v155/gillen21a/gillen21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-gillen21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sean
    family: Gillen
  - given: Katie
    family: Byl
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2137-2147
  id: gillen21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2137
  lastpage: 2147
  published: 2021-10-04 00:00:00 +0000
- title: 'S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds'
  abstract: 'With the increasing reliance of self-driving and similar robotic systems on robust 3D vision, the processing of LiDAR scans with deep convolutional neural networks has become a trend in academia and industry alike. Prior attempts on the challenging Semantic Scene Completion task - which entails the inference of dense 3D structure and associated semantic labels from “sparse” representations - have been, to a degree, successful in small indoor scenes when provided with dense point clouds or dense depth maps often fused with semantic segmentation maps from RGB images. However, the performance of these systems drop drastically when applied to large outdoor scenes characterized by dynamic and exponentially sparser conditions. Likewise, processing of the entire sparse volume becomes infeasible due to memory limitations and workarounds introduce computational inefficiency as practitioners are forced to divide the overall volume into multiple equal segments and infer on each individually, rendering real-time performance impossible. In this work, we formulate a method that subsumes the sparsity of large-scale environments and present S3CNet, a sparse convolution based neural network that predicts the semantically completed scene from a single, unified LiDAR point cloud. We show that our proposed method outperforms all counterparts on the 3D task, achieving state-of-the art results on the SemanticKITTI benchmark. Furthermore, we propose a 2D variant of S3CNet with a multi-view fusion strategy to complement our 3D network, providing robustness to occlusions and extreme sparsity in distant regions. We conduct experiments for the 2D semantic scene completion task and compare the results of our sparse 2D network against several leading LiDAR segmentation models adapted for bird’s eye view segmentation on two open-source datasets.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/cheng21a.html
  PDF: https://proceedings.mlr.press/v155/cheng21a/cheng21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-cheng21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ran
    family: Cheng
  - given: Christopher
    family: Agia
  - given: Yuan
    family: Ren
  - given: Xinhai
    family: Li
  - given: Liu
    family: Bingbing
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2148-2161
  id: cheng21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2148
  lastpage: 2161
  published: 2021-10-04 00:00:00 +0000
- title: 'Chaining Behaviors from Data with Model-Free Reinforcement Learning'
  abstract: 'Reinforcement learning has been applied to a wide variety of robotics problems, but most of such applications involve collecting data from scratch for each new task. Since the amount of robot data we can collect for any single task is limited by time and cost considerations, the learned behavior is typically narrow: the policy can only execute the task in a handful of scenarios that it was trained on. What if there was a way to incorporate a large amount of prior data, either from previously solved tasks or from unsupervised or undirected environment interaction, to extend and generalize learned behaviors? While most prior work on extending robotic skills using pre-collected data focuses on building explicit hierarchies or skill decompositions, we show in this paper that we can reuse prior data to extend new skills simply through model-free reinforcement learning via dynamic programming. We show that even when the prior data does not actually succeed at solving the new task, it can still be utilized for learning a better policy, by providing the agent with a broader understanding of the mechanics of its environment. We demonstrate the effectiveness of such an approach by chaining together several behaviors seen in prior datasets for solving a new task, with our hardest experimental setting involving composing four robotic skills in a row: picking, placing, drawer opening, and grasping, where a +1/0 sparse reward is provided only on task completion. We train our policies in an end-to-end fashion, mapping high-dimensional image observations to low-level robot control commands, and present results in both simulated and real world domains.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/singh21a.html
  PDF: https://proceedings.mlr.press/v155/singh21a/singh21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-singh21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Avi
    family: Singh
  - given: Albert
    family: Yu
  - given: Jonathan
    family: Yang
  - given: Jesse
    family: Zhang
  - given: Aviral
    family: Kumar
  - given: Sergey
    family: Levine
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2162-2177
  id: singh21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2162
  lastpage: 2177
  published: 2021-10-04 00:00:00 +0000
- title: 'Differentiable Logic Layer for Rule Guided Trajectory Prediction'
  abstract: 'In this work, we propose a method for integration of temporal logic formulas into a neural network. Our main contribution is a new logic optimization layer that uses differentiable optimization on the formulas’ robustness function. This allows incorporating traffic rules into deep learning based trajectory prediction approaches. In the forward pass, an initial prediction from a base predictor is used to initialize and guide the robustness optimization process. Backpropagation through the logic layer allows for simultaneously adjusting the parameters of the rules and the initial prediction network. The integration of a logic layer affords both improved predictions, as well as quantification rule satisfaction and violation during predictor execution. As such, it can serve as a parametric safety- envelope for black box behavior models. We demonstrate how integrating traffic rules improves the predictor performance using real traffic data from the NuScenes dataset.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/li21b.html
  PDF: https://proceedings.mlr.press/v155/li21b/li21b.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-li21b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xiao
    family: Li
  - given: Guy
    family: Rosman
  - given: Igor
    family: Gilitschenski
  - given: Jonathan
    family: DeCastro
  - given: Cristian-Ioan
    family: Vasile
  - given: Sertac
    family: Karaman
  - given: Daniela
    family: Rus
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2178-2194
  id: li21b
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2178
  lastpage: 2194
  published: 2021-10-04 00:00:00 +0000
- title: 'Attentional Separation-and-Aggregation Network for Self-supervised Depth-Pose Learning in Dynamic Scenes'
  abstract: 'Learning depth and ego-motion from unlabeled videos via self-supervision from epipolar projection can improve the robustness and accuracy of the 3D perception and localization of vision-based robots. However, the rigid projection computed by ego-motion cannot represent all scene points, such as points on moving objects, leading to false guidance in these regions. To address this problem, we propose an Attentional Separation-and-Aggregation Network (ASANet), which can learn to distinguish and extract the scene’s static and dynamic characteristics via the attention mechanism. We further propose a novel MotionNet with an ASANet as the encoder, followed by two separate decoders, to estimate the camera’s ego-motion and the scene’s dynamic motion field. Then, we introduce an auto-selecting approach to detect the moving objects for dynamic-aware learning automatically. Empirical experiments demonstrate that our method can achieve the state-of-the-art performance on the KITTI benchmark.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/gao21a.html
  PDF: https://proceedings.mlr.press/v155/gao21a/gao21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-gao21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Feng
    family: Gao
  - given: Jincheng
    family: Yu
  - given: Hao
    family: Shen
  - given: Yu
    family: Wang
  - given: Huazhong
    family: Yang
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2195-2205
  id: gao21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2195
  lastpage: 2205
  published: 2021-10-04 00:00:00 +0000
- title: 'Harnessing Distribution Ratio Estimators for Learning Agents with Quality and Diversity'
  abstract: 'Quality-Diversity (QD) is a concept from Neuroevolution with some intriguing applications to Reinforcement Learning. It facilitates learning a population of agents where each member is optimized to simultaneously accumulate high task-returns and exhibit behavioral diversity compared to other members. In this paper, we build on a recent kernel-based method for training a QD policy ensemble with Stein variational gradient descent. With kernels based on $f$-divergence between the stationary distributions of policies, we convert the problem to that of efficient estimation of the ratio of these stationary distributions. We then study various distribution ratio estimators used previously for off-policy evaluation and imitation and re-purpose them to compute the gradients for policies in an ensemble such that the resultant population is diverse and of high-quality.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/gangwani21a.html
  PDF: https://proceedings.mlr.press/v155/gangwani21a/gangwani21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-gangwani21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tanmay
    family: Gangwani
  - given: Jian
    family: Peng
  - given: Yuan
    family: Zhou
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2206-2215
  id: gangwani21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2206
  lastpage: 2215
  published: 2021-10-04 00:00:00 +0000
- title: 'Multimodal Trajectory Prediction via Topological Invariance for Navigation at Uncontrolled Intersections'
  abstract: 'We focus on decentralized navigation among multiple non-communicating rational agents at {\em uncontrolled} intersections, i.e., street intersections without traffic signs or signals. Avoiding collisions in such domains relies on the ability of agents to predict each others’ intentions reliably, and react quickly. Multiagent trajectory prediction is NP-hard whereas the sample complexity of existing data-driven approaches limits their applicability. Our key insight is that the geometric structure of the intersection and the incentive of agents to move efficiently and avoid collisions (rationality) reduces the space of likely behaviors, effectively relaxing the problem of trajectory prediction. In this paper, we collapse the space of multiagent trajectories at an intersection into a set of modes representing different classes of multiagent behavior, formalized using a notion of topological invariance. Based on this formalism, we design Multiple Topologies Prediction (MTP), a data-driven trajectory-prediction mechanism that reconstructs trajectory representations of high-likelihood modes in multiagent intersection scenes. We show that MTP outperforms a state-of-the-art multimodal trajectory prediction baseline (MFP) in terms of prediction accuracy by 78.24% on a challenging simulated dataset. Finally, we show that MTP enables our optimization-based planner, MTPnav, to achieve collision-free and time-efficient navigation across a variety of challenging intersection scenarios on the CARLA simulator.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/roh21a.html
  PDF: https://proceedings.mlr.press/v155/roh21a/roh21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-roh21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Junha
    family: Roh
  - given: Christoforos
    family: Mavrogiannis
  - given: Rishabh
    family: Madan
  - given: Dieter
    family: Fox
  - given: Siddhartha
    family: Srinivasa
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2216-2227
  id: roh21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2216
  lastpage: 2227
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning from Demonstrations using Signal Temporal Logic'
  abstract: 'Learning-from-demonstrations is an emerging paradigm to obtain effective robot control policies for complex tasks via reinforcement learning without the need to explicitly design reward functions. However, it is susceptible to imperfections in demonstrations and also raises concerns of safety and interpretability in the learned control policies. To address these issues, we use Signal Temporal Logic to evaluate and rank the quality of demonstrations. Temporal logic-based specifications allow us to create non-Markovian rewards, and also define interesting causal dependencies between tasks such as sequential task specifications. We validate our approach through experiments on discrete-world and OpenAI Gym environments, and show that our approach outperforms the state-of-the-art Maximum Causal Entropy Inverse Reinforcement Learning.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/puranic21a.html
  PDF: https://proceedings.mlr.press/v155/puranic21a/puranic21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-puranic21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Aniruddh
    family: Puranic
  - given: Jyotirmoy
    family: Deshmukh
  - given: Stefanos
    family: Nikolaidis
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2228-2242
  id: puranic21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2228
  lastpage: 2242
  published: 2021-10-04 00:00:00 +0000
- title: 'MATS: An Interpretable Trajectory Forecasting Representation for Planning and Control'
  abstract: 'Reasoning about human motion is a core component of modern human-robot interactive systems. In particular, one of the main uses of behavior prediction in autonomous systems is to inform robot motion planning and control. However, a majority of planning and control algorithms reason about system dynamics rather than the predicted agent tracklets (i.e., ordered sets of waypoints) that are commonly output by trajectory forecasting methods, which can hinder their integration. Towards this end, we propose Mixtures of Affine Time-varying Systems (MATS) as an output representation for trajectory forecasting that is more amenable to downstream planning and control use. Our approach leverages successful ideas from probabilistic trajectory forecasting works to learn dynamical system representations that are well-studied in the planning and control literature. We integrate our predictions with a proposed multimodal planning methodology and demonstrate significant computational efficiency improvements on a large-scale autonomous driving dataset.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/ivanovic21a.html
  PDF: https://proceedings.mlr.press/v155/ivanovic21a/ivanovic21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-ivanovic21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Boris
    family: Ivanovic
  - given: Amine
    family: Elhafsi
  - given: Guy
    family: Rosman
  - given: Adrien
    family: Gaidon
  - given: Marco
    family: Pavone
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2243-2256
  id: ivanovic21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2243
  lastpage: 2256
  published: 2021-10-04 00:00:00 +0000
- title: 'Robust Quadrupedal Locomotion on Sloped Terrains: A Linear Policy Approach'
  abstract: 'In this paper, with a view toward fast deployment of locomotion gaits in low-cost hardware, we use a linear policy for realizing end-foot trajectories in the quadruped robot, Stoch 2. In particular, the parameters of the end-foot trajectories are shaped via a linear feedback policy that takes the torso orientation and the terrain slope as inputs. The corresponding desired joint angles are obtained via an inverse kinematics solver and tracked via a PID control law. Augmented Random Search, a model-free and a gradient-free learning algorithm is used to train this linear policy. Simulation results show that the resulting walking is robust to terrain slope variations and external pushes. This methodology is not only computationally light-weight but also uses minimal sensing and actuation capabilities in the robot, thereby justifying the approach.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/paigwar21a.html
  PDF: https://proceedings.mlr.press/v155/paigwar21a/paigwar21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-paigwar21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Kartik
    family: Paigwar
  - given: Lokesh
    family: Krishna
  - given: sashank
    family: tirumala
  - given: naman
    family: khetan
  - given: aditya
    family: varma
  - given: ashish
    family: joglekar
  - given: Shalabh
    family: Bhatnagar
  - given: Ashitava
    family: Ghosal
  - given: Bharadwaj
    family: Amrutur
  - given: Shishir
    family: Kolathaya
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2257-2267
  id: paigwar21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2257
  lastpage: 2267
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Predictive Models for Ergonomic Control of Prosthetic Devices'
  abstract: 'We present Model-Predictive Interaction Primitives – a robot learning framework for assistive motion in human-machine collaboration tasks which explicitly accounts for biomechanical impact on the human musculoskeletal system. First, we extend Interaction Primitives to enable predictive biomechanics: the prediction of future biomechanical states of a human partner conditioned on current observations and intended robot control signals. In turn, we leverage this capability within a model-predictive control strategy to identify the future ergonomic and biomechanical ramifications of potential robot actions. Optimal control trajectories are selected so as to minimize future physical impact on the human musculoskeletal system. We empirically demonstrate that our approach minimizes knee or muscle forces via generated control actions selected according to biomechanical cost functions. Experiments are performed in synthetic and real-world experiments involving powered prosthetic devices.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/clark21a.html
  PDF: https://proceedings.mlr.press/v155/clark21a/clark21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-clark21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: GEOFFEY
    family: CLARK
  - given: Joseph
    family: Campbell
  - given: Heni Ben
    family: Amor
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2268-2278
  id: clark21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2268
  lastpage: 2278
  published: 2021-10-04 00:00:00 +0000
- title: 'ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations'
  abstract: 'Common methods for learning robot dynamics assume motion is continuous, causing unrealistic model predictions for systems undergoing discontinuous impact and stiction behavior. In this work, we resolve this conflict with a smooth, implicit encoding of the structure inherent to contact-induced discontinuities. Our method, ContactNets, learns parameterizations of inter-body signed distance and contact-frame Jacobians, a representation that is compatible with many simulation, control, and planning environments for robotics. We furthermore circumvent the need to differentiate through stiff or non-smooth dynamics with a novel loss function inspired by the principles of complementarity and maximum dissipation. Our method can predict realistic impact, non-penetration, and stiction when trained on 60 seconds of real-world data.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/pfrommer21a.html
  PDF: https://proceedings.mlr.press/v155/pfrommer21a/pfrommer21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-pfrommer21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Samuel
    family: Pfrommer
  - given: Mathew
    family: Halm
  - given: Michael
    family: Posa
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2279-2291
  id: pfrommer21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2279
  lastpage: 2291
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Equality Constraints for Motion Planning on Manifolds'
  abstract: 'Constrained robot motion planning is a widely used technique to solve complex robot tasks. We consider the problem of learning representations of constraints from demonstrations with a deep neural network, which we call Equality Constraint Manifold Neural Network (ECoMaNN). The key idea is to learn a level-set function of the constraint suitable for integration into a constrained sampling-based motion planner. Learning proceeds by aligning subspaces in the network with subspaces of the data. We combine both learned constraints and analytically described constraints into the planner and use a projection-based strategy to find valid points. We evaluate ECoMaNN on its representation capabilities of constraint manifolds, the impact of its individual loss terms, and the motions produced when incorporated into a planner.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/sutanto21a.html
  PDF: https://proceedings.mlr.press/v155/sutanto21a/sutanto21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-sutanto21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Giovanni
    family: Sutanto
  - given: Isabel Rayas
    family: Fernández
  - given: Peter
    family: Englert
  - given: Ragesh Kumar
    family: Ramachandran
  - given: Gaurav
    family: Sukhatme
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2292-2305
  id: sutanto21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2292
  lastpage: 2305
  published: 2021-10-04 00:00:00 +0000
- title: 'Multi-Level Structure vs. End-to-End-Learning in High-Performance Tactile Robotic Manipulation'
  abstract: 'In this paper we apply a multi-level structure to robotic manipulation learning. It consists of a hybrid dynamical system we denote skill and a parameter learning layer that leverages the underlying structure to simplify the problem at hand. For the learning layer we introduce a novel algorithm based on the idea of learning to partition the parameter solution space to quickly and efficiently find good and robust solutions to complex manipulation problems. In a benchmark comparison we show a significant performance increase compared with other black-box optimization algorithms such as HiREPS and particle swarm optimization. Furthermore, we validate and compare our approach on a very hard real-world manipulation problem, namely inserting a key into a lock, with state-of-the-art deep reinforcement learning.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/voigt21a.html
  PDF: https://proceedings.mlr.press/v155/voigt21a/voigt21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-voigt21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Florian
    family: Voigt
  - given: Lars
    family: Johannsmeier
  - given: Sami
    family: Haddadin
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2306-2316
  id: voigt21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2306
  lastpage: 2316
  published: 2021-10-04 00:00:00 +0000
- title: 'Learning Arbitrary-Goal Fabric Folding with One Hour of Real Robot Experience'
  abstract: 'Manipulating deformable objects, such as fabric, is a long standing problem in robotics, with state estimation and control posing a significant challenge for traditional methods. In this paper, we show that it is possible to learn fabric folding skills in only an hour of self-supervised real robot experience, without human supervision or simulation. Our approach relies on fully convolutional networks and the manipulation of visual inputs to exploit learned features, allowing us to create an expressive goal-conditioned pick and place policy that can be trained efficiently with real world robot data only. Folding skills are learned with only a sparse reward function and thus do not require reward function engineering, merely an image of the goal configuration. We demonstrate our method on a set of towel-folding tasks, and show that our approach is able to discover sequential folding strategies, purely from trial-and-error. We achieve state-of-the-art results without the need for demonstrations or simulation, used in prior approaches.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/lee21a.html
  PDF: https://proceedings.mlr.press/v155/lee21a/lee21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-lee21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Robert
    family: Lee
  - given: Daniel
    family: Ward
  - given: Vibhavari
    family: Dasagi
  - given: Akansel
    family: Cosgun
  - given: Juxi
    family: Leitner
  - given: Peter
    family: Corke
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2317-2327
  id: lee21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2317
  lastpage: 2327
  published: 2021-10-04 00:00:00 +0000
- title: 'Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation'
  abstract: 'Vision-based robotics often factors the control loop into separate components for perception and control. Conventional perception components usually extract hand-engineered features from the visual input that are then used by the control component in an explicit manner. In contrast, recent advances in deep RL make it possible to learn these features end-to-end during training, but the final result is often brittle, fails unexpectedly under minuscule visual shifts, and comes with a high sample complexity cost. In this work, we study the effects of using mid-level visual representations asynchronously trained for traditional computer vision objectives as a generic and easy-to-decode perceptual state in an end-to-end RL framework. We show that the invariances provided by the mid-level representations aid generalization, improve sample complexity, and lead to a higher final performance. Compared to the alternative approaches for incorporating invariances, such as domain randomization, using asynchronously trained mid-level representations scale better to harder problems and larger domain shifts, and consequently, successfully trains policies for tasks where domain randomization or learning-from-scratch failed. Our experimental findings are reported on manipulation and navigation tasks using real robots as well as simulations.'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chen21f.html
  PDF: https://proceedings.mlr.press/v155/chen21f/chen21f.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chen21f.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Bryan
    family: Chen
  - given: Alexander
    family: Sax
  - given: Francis
    family: Lewis
  - given: Iro
    family: Armeni
  - given: Silvio
    family: Savarese
  - given: Amir
    family: Zamir
  - given: Jitendra
    family: Malik
  - given: Lerrel
    family: Pinto
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2328-2346
  id: chen21f
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2328
  lastpage: 2346
  published: 2021-10-04 00:00:00 +0000
- title: 'Towards Autonomous Eye Surgery by Combining Deep Imitation Learning with Optimal Control'
  abstract: 'During retinal microsurgery, precise manipulation of the delicate retinal tissue is required for positive surgical outcome. However, accurate manipulation and navigation of surgical tools remain difficult due to a constrained workspace and the top-down view during the surgery, which limits the surgeon’s ability to estimate depth. To alleviate such difficulty, we propose to automate the tool-navigation task by learning to predict relative goal position on the retinal surface from the current tool-tip position. Given an estimated target on the retina, we generate an optimal trajectory leading to the predicted goal while imposing safety-related physical constraints aimed to minimize tissue damage. As an extended task, we generate goal predictions to various points across the retina to localize eye geometry and further generate safe trajectories within the estimated confines. Through experiments in both simulation and with several eye phantoms, we demonstrate that our framework can permit navigation to various points on the retina within 0.089mm and 0.118mm in xy error which is less than the human’s surgeon mean tremor at the tool-tip of 0.180mm. All safety constraints were fulfilled and the algorithm was robust to previously unseen eyes as well as unseen objects in the scene. Live video demonstration is available here: https://youtu.be/n5j5jCCelXk'
  volume: 155
  URL: https://proceedings.mlr.press/v155/kim21a.html
  PDF: https://proceedings.mlr.press/v155/kim21a/kim21a.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-kim21a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ji Woong
    family: Kim
  - given: Peiyao
    family: Zhang
  - given: Peter
    family: Gehlbach
  - given: Iulian
    family: Iordachita
  - given: Marin
    family: Kobilarov
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2347-2358
  id: kim21a
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2347
  lastpage: 2358
  published: 2021-10-04 00:00:00 +0000
- title: 'Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements Matching'
  abstract: 'The crucial step for localization is to match the current observation to the map. When the two sensor modalities are significantly different, matching becomes challenging. In this paper, we present an end-to-end deep phase correlation network (DPCN) to match heterogeneous sensor measurements. In DPCN, the primary component is a differentiable correlation-based estimator that back-propagates the pose error to learnable feature extractors, which addresses the problem that there are no direct common features for supervision. In addition, it eliminates the exhaustive evaluation in some previous methods, improving efficiency. With the interpretable modeling, the network is light-weighted and promising for better generalization. We evaluate the system on both the simulation data and Aero-Ground Dataset which consists of heterogeneous sensor images and aerial images acquired by satellites or aerial robots. The results show that our method is able to match the heterogeneous sensor measurements, outperforming the comparative traditional phase correlation and other learning-based methods. Code is available at https://github.com/jessychen1016/DPCN .'
  volume: 155
  URL: https://proceedings.mlr.press/v155/chen21g.html
  PDF: https://proceedings.mlr.press/v155/chen21g/chen21g.pdf
  edit: https://github.com/mlresearch//v155/edit/gh-pages/_posts/2021-10-04-chen21g.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 2020 Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zexi
    family: Chen
  - given: Xuecheng
    family: Xu
  - given: Yue
    family: Wang
  - given: Rong
    family: Xiong
  editor: 
  - given: Jens
    family: Kober
  - given: Fabio
    family: Ramos
  - given: Claire
    family: Tomlin
  page: 2359-2375
  id: chen21g
  issued:
    date-parts: 
      - 2021
      - 10
      - 4
  firstpage: 2359
  lastpage: 2375
  published: 2021-10-04 00:00:00 +0000
