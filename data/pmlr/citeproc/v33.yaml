
- title: 'Preface'
  abstract: 'Preface to AISTATS 2014'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kaski14.html
  PDF: http://proceedings.mlr.press/v33/kaski14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kaski14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: i-iv
  id: kaski14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: i
  lastpage: iv
  published: 2014-04-02 00:00:00 +0000
- title: 'Decontamination of Mutually Contaminated Models'
  abstract: 'A variety of machine learning problems are characterized by   data sets that are drawn from multiple different convex   combinations of a fixed set of base distributions.  We call this a mutual contamination model.  In such problems, it is often of interest to   recover these base distributions, or otherwise discern   their properties. This work focuses on the problem of   classification with multiclass label noise, in a general   setting where the noise proportions are unknown and the   true class distributions are nonseparable and potentially   quite complex. We develop a procedure for decontamination   of the contaminated models from data, which then   facilitates the design of a consistent discrimination rule. Our   approach relies on a novel method for estimating the error   when projecting one distribution onto a convex combination   of others, where the projection is with respect to an   information divergence known as the separation distance.   Under sufficient conditions on the amount of noise and   purity of the base distributions, this projection procedure   successfully recovers the underlying class distributions. Connections to   novelty detection, topic modeling, and other learning problems are also   discussed.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/blanchard14.html
  PDF: http://proceedings.mlr.press/v33/blanchard14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-blanchard14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Gilles
    family: Blanchard
  - given: Clayton
    family: Scott
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1-9
  id: blanchard14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1
  lastpage: 9
  published: 2014-04-02 00:00:00 +0000
- title: 'Distributed optimization of deeply nested systems'
  abstract: 'Intelligent processing of complex signals such as images is often performed by a hierarchy of nonlinear processing layers, such as a deep net or an object recognition cascade. Joint estimation of the parameters of all the layers is a difficult nonconvex optimization. We describe a general strategy to learn the parameters and, to some extent, the architecture of nested systems, which we call the method of auxiliary coordinates (MAC). This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. The constrained problem may be solved with penalty-based methods using alternating optimization over the parameters and the auxiliary coordinates. MAC has provable convergence, is easy to implement reusing existing algorithms for single layers, can be parallelized trivially and massively, applies even when parameter derivatives are not available or not desirable, can perform some model selection on the fly, and is competitive with state-of-the-art nonlinear optimizers even in the serial computation setting, often providing reasonable models within a few iterations.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/carreira-perpinan14.html
  PDF: http://proceedings.mlr.press/v33/carreira-perpinan14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-carreira-perpinan14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Miguel
    family: Carreira-Perpinan
  - given: Weiran
    family: Wang
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 10-19
  id: carreira-perpinan14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 10
  lastpage: 19
  published: 2014-04-02 00:00:00 +0000
- title: 'Analysis of Empirical MAP and Empirical Partially Bayes: Can They be Alternatives to Variational Bayes?'
  abstract: 'Variational Bayesian (VB) learning is known to be a   promising   approximation to Bayesian learning  with computational efficiency.  However, in some applications,  e.g., large-scale collaborative filtering and tensor factorization,  VB is still computationally too costly.  In such cases,   looser approximations  such as MAP estimation and partially Bayesian (PB) learning,  where a part of the parameters are point-estimated,  seem attractive.  In this paper,   we theoretically investigate the behavior of the MAP and the PB solutions of matrix factorization.  A notable finding is that the global solutions of MAP and PB in the empirical Bayesian scenario,  where the hyperparameters are also estimated from observation,  are trivial and useless,  while  their local solutions behave similarly to the global solution of VB.  This suggests that empirical MAP and empirical PB with local search can be alternatives to empirical VB  equipped with the useful automatic relevance determination property.  Experiments support our theory.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/nakajima14.html
  PDF: http://proceedings.mlr.press/v33/nakajima14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-nakajima14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Shinichi
    family: Nakajima
  - given: Masashi
    family: Sugiyama
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 20-28
  id: nakajima14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 20
  lastpage: 28
  published: 2014-04-02 00:00:00 +0000
- title: 'Improved Bounds for Online Learning Over the Permutahedron and Other Ranking Polytopes'
  abstract: 'Consider the following game: There is a fixed set V of n items.  At each step an adversary chooses a score function  s_t:V\mapsto[0,1], a learner outputs a  ranking of V, and then s_t is revealed.  The learner’s loss is the sum over v∈V, of s_t(v) times v’s  position (0th, 1st, 2nd, ...) in the ranking.  This problem captures, for example, online systems that iteratively present ranked lists of items to users, who then respond by choosing one (or more) sought items.  The loss measures the users’ burden, which increases the further the sought items are from the top.  It also captures a version of online rank aggregation.      We present an algorithm of expected regret  O(n\sqrtOPT + n^2), where OPT is the loss of the best (single) ranking in hindsight.   This improves the previously best known  algorithm of Suehiro et. al (2012) by saving a factor of Ω(\sqrt\log n).  We also reduce the per-step running time  from O(n^2) to O(n\log n).   We provide matching lower bounds.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/ailon14.html
  PDF: http://proceedings.mlr.press/v33/ailon14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-ailon14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Nir
    family: Ailon
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 29-37
  id: ailon14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 29
  lastpage: 37
  published: 2014-04-02 00:00:00 +0000
- title: 'Information-Theoretic Characterization of Sparse Recovery'
  abstract: 'We formulate sparse support recovery as a salient set identification problem and use information-theoretic analyses to characterize the recovery performance and sample complexity. We consider a very general framework where we are not restricted to linear models or specific distributions. We state non-asymptotic bounds on recovery probability and a tight mutual information formula for sample complexity. We evaluate our bounds for applications such as sparse linear regression and explicitly characterize effects of correlation or noisy features on recovery performance. We show improvements upon previous work and identify gaps between the performance of recovery algorithms and fundamental information. This illustrates a trade-off between computational complexity and sample complexity, contrasting the recovery of the support as a discrete object with signal estimation approaches.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/aksoylar14.html
  PDF: http://proceedings.mlr.press/v33/aksoylar14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-aksoylar14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Cem
    family: Aksoylar
  - given: Venkatesh
    family: Saligrama
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 38-46
  id: aksoylar14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 38
  lastpage: 46
  published: 2014-04-02 00:00:00 +0000
- title: 'Hybrid Discriminative-Generative Approach with Gaussian Processes'
  abstract: 'Machine learning practitioners are often faced with a choice between a discriminative and a generative approach to modelling. Here, we present a model based on a hybrid approach that breaks down some of the barriers between the discriminative and generative points of view, allowing continuous dimensionality reduction of hybrid discrete-continuous data, discriminative classification with missing inputs and manifold learning informed by class labels.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/andradepacheco14.html
  PDF: http://proceedings.mlr.press/v33/andradepacheco14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-andradepacheco14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Ricardo
    family: Andrade Pacheco
  - given: James
    family: Hensman
  - given: Max
    family: Zwiessele
  - given: Neil D.
    family: Lawrence
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 47-56
  id: andradepacheco14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 47
  lastpage: 56
  published: 2014-04-02 00:00:00 +0000
- title: 'Average Case Analysis of High-Dimensional Block-Sparse Recovery and Regression for Arbitrary Designs'
  abstract: 'This paper studies conditions for high-dimensional inference when the set of observations is given by a linear combination of a small number of groups of columns of a design matrix, termed the “block-sparse” case. In this regard, it first specifies conditions on the design matrix under which most of its block submatrices are well conditioned. It then leverages this result for average-case analysis of high-dimensional block-sparse recovery and regression. In contrast to earlier works, the results of this paper are fundamentally different because (i) they provide conditions on arbitrary designs that can be explicitly computed in polynomial time, (ii) the provided conditions translate into near-optimal scaling of the number of observations with the number of active blocks of the design matrix, and (iii) they suggest that the spectral norm, rather than the column/block coherences, of the design matrix fundamentally limits the performance of computational methods in high-dimensional settings.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/bajwa14.html
  PDF: http://proceedings.mlr.press/v33/bajwa14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-bajwa14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Waheed
    family: Bajwa
  - given: Marco
    family: Duarte
  - given: Robert
    family: Calderbank
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 57-67
  id: bajwa14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 57
  lastpage: 67
  published: 2014-04-02 00:00:00 +0000
- title: 'A New Perspective on Learning Linear Separators with Large L_qL_p Margins'
  abstract: 'We give theoretical and empirical results that provide new insights into large margin learning. We prove a bound on the generalization error of learning linear separators with large L_qL_p margins (where L_q and L_p are dual norms) for any finite p \ge 1. The bound leads to a simple data-dependent sufficient condition for fast learning in addition to extending and improving upon previous results. We also provide the first study that shows the benefits of taking advantage of margins with p < 2 over margins with p \ge 2. Our experiments confirm that our theoretical results are relevant in practice.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/balcan14.html
  PDF: http://proceedings.mlr.press/v33/balcan14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-balcan14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Maria-Florina
    family: Balcan
  - given: Christopher
    family: Berlind
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 68-76
  id: balcan14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 68
  lastpage: 76
  published: 2014-04-02 00:00:00 +0000
- title: 'A Non-parametric Conditional Factor Regression Model for Multi-Dimensional Input and Response'
  abstract: 'In this paper, we propose a  non-parametric conditional factor regression (NCFR) model for domains with multi-dimensional input and response. NCFR enhances linear regression in two ways: a) introducing low-dimensional latent factors leading to dimensionality reduction and b) integrating the Indian Buffet Process as prior for the latent layer to dynamically derive an optimal number of sparse factors. Thanks to IBP’s enhancements to the latent factors, NCFR can significantly avoid over-fitting even in the case of a very small sample size compared to the dimensionality. Experimental results on three diverse datasets comparing NCRF to a few baseline alternatives give evidence of its robust learning, remarkable predictive performance, good mixing and computational efficiency.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/bargi14.html
  PDF: http://proceedings.mlr.press/v33/bargi14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-bargi14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Ava
    family: Bargi
  - given: Richard Yi
    family: Xu
  - given: Zoubin
    family: Ghahramani
  - given: Massimo
    family: Piccardi
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 77-85
  id: bargi14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 77
  lastpage: 85
  published: 2014-04-02 00:00:00 +0000
- title: 'Learning Optimal Bounded Treewidth Bayesian Networks via Maximum Satisfiability'
  abstract: 'Bayesian network structure learning is the well-known computationally hard problem of finding a directed acyclic graph structure that optimally describes given data. A learned structure can then be used for probabilistic inference. While exact inference in Bayesian networks is in general NP-hard, it is tractable in networks with low treewidth. This provides good motivations for developing algorithms for the NP-hard problem of learning optimal bounded treewidth Bayesian networks (BTW-BNSL). In this work, we develop a novel score-based approach to BTW-BNSL, based on casting BTW-BNSL as weighted partial Maximum satisfiability. We demonstrate empirically that the approach scales notably better than a recent exact dynamic programming algorithm for BTW-BNSL.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/berg14.html
  PDF: http://proceedings.mlr.press/v33/berg14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-berg14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Jeremias
    family: Berg
  - given: Matti
    family: Järvisalo
  - given: Brandon
    family: Malone
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 86-95
  id: berg14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 86
  lastpage: 95
  published: 2014-04-02 00:00:00 +0000
- title: 'Online Passive-Aggressive Algorithms for Non-Negative Matrix Factorization and Completion'
  abstract: 'Stochastic Gradient Descent (SGD) is a popular online algorithm for large-scale matrix factorization.  However, SGD can often be difficult to use for practitioners, because its performance is very sensitive to the choice of the learning rate parameter.  In this paper, we present non-negative  passive-aggressive (NN-PA), a family of online algorithms for non-negative matrix factorization (NMF).  Our algorithms are scalable, easy to implement and do not require the tedious tuning of a learning rate parameter. We demonstrate the effectiveness of our algorithms on three large-scale matrix completion problems and analyze them in the regret bound model.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/blondel14.html
  PDF: http://proceedings.mlr.press/v33/blondel14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-blondel14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Mathieu
    family: Blondel
  - given: Yotaro
    family: Kubo
  - given: Ueda
    family: Naonori
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 96-104
  id: blondel14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 96
  lastpage: 104
  published: 2014-04-02 00:00:00 +0000
- title: 'PAC-Bayesian Theory for Transductive Learning'
  abstract: 'We propose a PAC-Bayesian analysis of the transductive learning setting, introduced by Vapnik [2008], by proposing a family of new bounds on the generalization error. Some of them are derived from their counterpart in the inductive setting, and others are new. We also compare their behavior.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/begin14.html
  PDF: http://proceedings.mlr.press/v33/begin14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-begin14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Luc
    family: Bégin
  - given: Pascal
    family: Germain
  - given: François
    family: Laviolette
  - given: Jean-Francis
    family: Roy
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 105-113
  id: begin14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 105
  lastpage: 113
  published: 2014-04-02 00:00:00 +0000
- title: 'Random Bayesian networks with bounded indegree'
  abstract: 'Bayesian networks (BN) are an extensively used graphical model for representing a probability distribution in artificial intelligence, data mining, and machine learning. In this paper, we propose a simple model for large random BNs with bounded indegree, that is, large directed acyclic graphs (DAG) where the edges appear at random and each node has at most a given number of parents. Using this model, we can study useful asymptotic properties of large BNs and BN algorithms with basic combinatorics tools. We estimate the expected size of a BN, the expected size increase of moralization, the expected size of the Markov blanket, and the maximum size of a minimal d-separator. We also provide an upper bound on the average time complexity of an algorithm for finding a minimal d-separator. In addition, the estimates are evaluated against BNs learned from real world data.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/chen14a.html
  PDF: http://proceedings.mlr.press/v33/chen14a.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-chen14a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Eunice Yuh-Jie
    family: Chen
  - given: Judea
    family: Pearl
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 114-121
  id: chen14a
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 114
  lastpage: 121
  published: 2014-04-02 00:00:00 +0000
- title: 'Efficient Low-Rank Stochastic Gradient Descent Methods for Solving Semidefinite Programs'
  abstract: 'We propose a low-rank stochastic gradient descent (LR-SGD) method for solving a class of semidefinite programming (SDP) problems. LR-SGD has clear computational advantages over the standard SGD peers as its iterative projection step (a SDP problem) can be solved in an efficient manner. Specifically, LR-SGD constructs a low-rank stochastic gradient  and computes an optimal solution to the projection step via analyzing the low-rank structure of its stochastic gradient. Moreover, our theoretical analysis shows the universal existence of arbitrary low-rank stochastic gradients which in turn validates the rationale of  the LR-SGD method. Since LR-SGD is a SGD based method, it achieves the optimal convergence rates of the standard SGD methods. The presented experimental results demonstrate the efficiency and effectiveness of the LR-SGD method.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/chen14b.html
  PDF: http://proceedings.mlr.press/v33/chen14b.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-chen14b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Jianhui
    family: Chen
  - given: Tianbao
    family: Yang
  - given: Shenghuo
    family: Zhu
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 122-130
  id: chen14b
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 122
  lastpage: 130
  published: 2014-04-02 00:00:00 +0000
- title: 'Characterizing EVOI-Sufficient k-Response Query Sets in Decision Problems'
  abstract: 'In finite decision problems where an agent can query its human user to obtain information about its environment before acting, a query’s usefulness is in terms of its Expected Value of Information (EVOI). The usefulness of a query set is similarly measured in terms of the EVOI of the queries it contains. When the only constraint on what queries can be asked is that they have exactly k possible responses (with k \ge 2), we show that the set of k-response decision queries (which ask the user to select his/her preferred decision given a choice of k decisions) is EVOI-Sufficient, meaning that no single k-response query can have higher EVOI than the best single k-response decision query for any decision problem. When multiple queries can be asked before acting, we provide a negative result that shows the set of  depth-n query trees constructed from k-response decision queries is not EVOI-Sufficient. However, we also provide a positive result that the set of depth-n query trees constructed from k-response decision-set queries, which ask the user to select from among k sets of decisions as to which set contains the best decision,  is EVOI-Sufficient. We conclude with a discussion and analysis of algorithms that draws on a connection to other recent work on decision-theoretic knowledge elicitation.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/cohn14.html
  PDF: http://proceedings.mlr.press/v33/cohn14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-cohn14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Robert
    family: Cohn
  - given: Satinder
    family: Singh
  - given: Edmund
    family: Durfee
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 131-139
  id: cohn14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 131
  lastpage: 139
  published: 2014-04-02 00:00:00 +0000
- title: 'Doubly Aggressive Selective Sampling Algorithms for Classification'
  abstract: 'Online selective sampling algorithms learn to perform binary    classification, and additionally they decided whether to ask, or    query, for a label of any given example. We introduce two stochastic    linear algorithms and analyze them in the worst-case mistake-bound    framework. Even though stochastic, for some inputs, our algorithms    query with probability 1 and make an update even if there is    no mistake, yet the margin is small, hence they are doubly      aggressive. We prove bounds in the worst-case settings, which    may be lower than previous bounds in some settings. Experiments with    33 document classification datasets, some with 100Ks examples,    show the superiority of doubly-aggressive algorithms both in    performance and number of queries.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/crammer14.html
  PDF: http://proceedings.mlr.press/v33/crammer14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-crammer14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Koby
    family: Crammer
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 140-148
  id: crammer14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 140
  lastpage: 148
  published: 2014-04-02 00:00:00 +0000
- title: 'Sparse Bayesian Variable Selection for the Identification of Antigenic Variability in the Foot-and-Mouth Disease Virus'
  abstract: 'Vaccines created from closely related viruses are vital for offering protection against newly emerging strains. For Foot-and-Mouth disease virus (FMDV), where multiple serotypes co-circulate, testing large numbers of vaccines can be infeasible. Therefore the development of an in silico predictor of cross-protection between strains is important to help optimise vaccine choice. Here we describe a novel sparse Bayesian variable selection model using spike and slab priors which is able to predict antigenic variability and identify sites which are important for the neutralisation of the virus. We are able to identify multiple residues which are known to be key indicators of antigenic variability. Many of these were not identified previously using Frequentist mixed-effects models and still cannot be found when an L1 penalty is used. We further explore how the Markov chain Monte Carlo (MCMC) proposal method for the inclusion of variables can offer significant reductions in computational requirements, both for spike and slab priors in general, and our hierarchical Bayesian model in particular.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/davies14.html
  PDF: http://proceedings.mlr.press/v33/davies14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-davies14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Vinny
    family: Davies
  - given: Richard
    family: Reeve
  - given: William
    family: Harvey
  - given: Francois
    family: Maree
  - given: Dirk
    family: Husmeier
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 149-158
  id: davies14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 149
  lastpage: 158
  published: 2014-04-02 00:00:00 +0000
- title: 'Sparsity and the Truncated $l^2$-norm'
  abstract: 'Sparsity is a fundamental topic in high-dimensional data analysis.  Perhaps the most common measures of sparsity are the $l^p$-norms, for $p < 2$.  In this paper, we study an alternative measure of sparsity, the truncated $l^2$-norm, which is related to other $l^p$-norms, but appears to have some unique and useful properties.  Focusing on the n-dimensional Gaussian location model, we derive exact asymptotic minimax results for estimation over truncated $l^2$-balls, which complement existing results for $l^p$-balls.  We then propose simple new adaptive thresholding estimators that are inspired by the truncated $l^2$-norm and are adaptive asymptotic minimax over $l^p$-balls ($p < 2$), as well as truncated $l^2$-balls.  Finally, we derive lower bounds on the Bayes risk of an estimator, in terms of the parameter’s truncated $l^2$-norm.  These bounds provide necessary conditions for Bayes risk consistency in certain problems that are relevant for high-dimensional Bayesian modeling.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/dicker14.html
  PDF: http://proceedings.mlr.press/v33/dicker14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-dicker14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Lee
    family: Dicker
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 159-166
  id: dicker14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 159
  lastpage: 166
  published: 2014-04-02 00:00:00 +0000
- title: 'Efficient Distributed Topic Modeling with Provable Guarantees'
  abstract: 'Topic modeling for large-scale distributed web-collections requires distributed techniques that account for both computational and communication costs. We consider topic modeling under the separability assumption and develop novel computationally efficient methods that provably achieve the statistical performance of the state-of-the-art centralized approaches while requiring insignificant communication between the distributed document collections. We achieve tradeoffs between communication and computation without actually transmitting the documents. Our scheme is based on exploiting the geometry of normalized word-word co-occurrence matrix and viewing each row of this matrix as a vector in a high-dimensional space. We relate the solid angle subtended by extreme points of the convex hull of these vectors to topic identities and construct distributed schemes to identify topics.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/ding14a.html
  PDF: http://proceedings.mlr.press/v33/ding14a.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-ding14a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Weicong
    family: Ding
  - given: Mohammad
    family: Rohban
  - given: Prakash
    family: Ishwar
  - given: Venkatesh
    family: Saligrama
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 167-175
  id: ding14a
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 167
  lastpage: 175
  published: 2014-04-02 00:00:00 +0000
- title: 'Pan-sharpening with a Bayesian nonparametric dictionary learning model'
  abstract: 'Pan-sharpening, a method for constructing high resolution images from low resolution observations, has recently been explored from the perspective of compressed sensing and sparse representation theory. We present a new pan-sharpening algorithm that uses a Bayesian nonparametric dictionary learning model to give an underlying sparse representation for image reconstruction. In contrast to existing dictionary learning methods, the proposed method infers parameters such as dictionary size, patch sparsity and noise variances. In addition, our regularization includes image constraints such as a total variation penalization term and a new gradient penalization on the reconstructed PAN image. Our method does not require high resolution multiband images for dictionary learning, which are unavailable in practice, but rather the dictionary is learned directly on the reconstructed image as part of the inversion process. We present experiments on several images to validate our method and compare with several other well-known approaches.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/ding14b.html
  PDF: http://proceedings.mlr.press/v33/ding14b.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-ding14b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Xinghao
    family: Ding
  - given: Yiyong
    family: Jiang
  - given: Yue
    family: Huang
  - given: John
    family: Paisley
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 176-184
  id: ding14b
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 176
  lastpage: 184
  published: 2014-04-02 00:00:00 +0000
- title: 'Approximate Slice Sampling for Bayesian Posterior Inference'
  abstract: 'In this paper, we advance the theory of large scale Bayesian posterior inference by introducing a new approximate slice sampler that uses only small mini-batches of data in every iteration. While this introduces a bias in the stationary distribution, the computational savings allow us to draw more samples in a given amount of time and reduce sampling variance. We empirically verify on three  different models that the approximate slice sampling algorithm can significantly outperform a traditional slice sampler if we are allowed only a fixed amount of computing time for our simulations.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/dubois14.html
  PDF: http://proceedings.mlr.press/v33/dubois14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-dubois14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Christopher
    family: DuBois
  - given: Anoop
    family: Korattikara
  - given: Max
    family: Welling
  - given: Padhraic
    family: Smyth
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 185-193
  id: dubois14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 185
  lastpage: 193
  published: 2014-04-02 00:00:00 +0000
- title: 'Bayesian Logistic Gaussian Process Models for Dynamic Networks'
  abstract: 'Time-varying adjacency matrices encoding the presence or absence of a relation among entities are available in many research fields. Motivated by an application to studying dynamic networks among sports teams, we propose a Bayesian nonparametric model.  The proposed approach uses a logistic mapping from the probability matrix, encoding link probabilities between each team, to an embedded latent relational space.  Within this latent space, we incorporate a dictionary of Gaussian process (GP) latent trajectories characterizing changes over time in each team, while allowing learning of the number of latent dimensions through a specially tailored prior for the GP covariance.  The model is provably flexible and borrows strength across the network and over time. We provide simulation experiments and an application to the Italian soccer Championship.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/durante14.html
  PDF: http://proceedings.mlr.press/v33/durante14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-durante14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Daniele
    family: Durante
  - given: David
    family: Dunson
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 194-201
  id: durante14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 194
  lastpage: 201
  published: 2014-04-02 00:00:00 +0000
- title: 'Avoiding pathologies in very deep networks'
  abstract: 'Choosing appropriate architectures and regularization strategies of deep networks is crucial to good predictive performance.  To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions.  Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network.  We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit.  We propose an alternate network architecture which does not suffer from this pathology.  We also examine deep covariance functions, obtained by composing infinitely many feature transforms.  Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/duvenaud14.html
  PDF: http://proceedings.mlr.press/v33/duvenaud14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-duvenaud14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: David
    family: Duvenaud
  - given: Oren
    family: Rippel
  - given: Ryan
    family: Adams
  - given: Zoubin
    family: Ghahramani
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 202-210
  id: duvenaud14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 202
  lastpage: 210
  published: 2014-04-02 00:00:00 +0000
- title: 'Efficient Inference for Complex Queries on Complex Distributions'
  abstract: 'We consider problems of approximate inference in which the query of interest is given by a complex formula (such as a formula in disjunctive formal form (DNF)) over a joint distribution given by a graphical model. We give a general reduction showing that (approximate) marginal inference for a class of distributions yields approximate inference for DNF queries, and extend our techniques to accommodate even more complex queries, and dense graphical models with variational inference, under certain conditions. Our results unify and generalize classical inference techniques (which are generally restricted to simple marginal queries) and approximate counting methods such as those introduced by Karp, Luby and Madras (which are generally restricted to product distributions).'
  volume: 33
  URL: https://proceedings.mlr.press/v33/dworkin14.html
  PDF: http://proceedings.mlr.press/v33/dworkin14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-dworkin14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Lili
    family: Dworkin
  - given: Michael
    family: Kearns
  - given: Lirong
    family: Xia
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 211-219
  id: dworkin14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 211
  lastpage: 219
  published: 2014-04-02 00:00:00 +0000
- title: 'Bayesian Switching Interaction Analysis Under Uncertainty'
  abstract: 'We introduce a Bayesian discrete-time framework for switching-interaction analysis under uncertainty, in which latent interactions, switching pattern and signal states and dynamics are inferred from noisy (and possibly missing) observations of these signals. We propose reasoning over full posterior distribution of these latent variables as a means of combating and characterizing uncertainty. This approach also allows for answering a variety of questions probabilistically, which is suitable for exploratory pattern discovery and post-analysis by human experts. This framework is based on a fully-Bayesian learning of the structure of a switching dynamic Bayesian network (DBN) and utilizes a state-space approach to allow for noisy observations and missing data. It generalizes the autoregressive switching interaction model of Siracusa et al., which does not allow observation noise, and the switching linear dynamic system model of Fox et al., which does not infer interactions among signals. Posterior samples are obtained via a Gibbs sampling procedure, which is particularly efficient in the case of linear Gaussian dynamics and observation models. We demonstrate the utility of our framework on a controlled human-generated data, and a real-world climate data.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/dzunic14.html
  PDF: http://proceedings.mlr.press/v33/dzunic14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-dzunic14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Zoran
    family: Dzunic
  - given: John
    family: Fisher III
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 220-228
  id: dzunic14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 220
  lastpage: 228
  published: 2014-04-02 00:00:00 +0000
- title: 'Robust learning of inhomogeneous PMMs'
  abstract: 'Inhomogeneous parsimonious Markov models have recently been introduced for modeling symbolic sequences, with a main application being DNA sequence analysis.  Structure and parameter learning of these models has been proposed using a Bayesian approach, which entails the practically challenging choice of the prior distribution.  Cross validation is a possible way of tuning the prior hyperparameters towards a specific task such as prediction or classification, but it is overly time-consuming.  On this account, robust learning methods, which do not require explicit prior specification and – in the absence of prior knowledge – no hyperparameter tuning, are of interest.  In this work, we empirically investigate the performance of robust alternatives for structure and parameter learning that extend the practical applicability of inhomogeneous parsimonious Markov models to more complex settings than before.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/eggeling14.html
  PDF: http://proceedings.mlr.press/v33/eggeling14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-eggeling14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Ralf
    family: Eggeling
  - given: Teemu
    family: Roos
  - given: Petri
    family: Myllymäki
  - given: Ivo
    family: Grosse
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 229-237
  id: eggeling14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 229
  lastpage: 237
  published: 2014-04-02 00:00:00 +0000
- title: 'Fully-Automatic Bayesian Piecewise Sparse Linear Models'
  abstract: 'Piecewise linear models (PLMs) have been widely used in many enterprise machine learning problems, which assign linear experts to individual partitions on feature spaces and express whole models as patches of local experts. This paper addresses simultaneous model selection issues of PLMs; partition structure determination and feature selection of individual experts. Our contributions are mainly three-fold. First, we extend factorized asymptotic Bayesian (FAB) inference for hierarchical mixtures of experts (probabilistic PLMs). FAB inference offers penalty terms w.r.t. partition and expert complexities, and enable us to resolve the model selection issue. Second, we propose posterior optimization which significantly improves predictive accuracy. Roughly speaking, our new posterior optimization mitigates accuracy degradation due to a gap between marginal log-likelihood maximization and predictive accuracy. Third, we present an application of energy demand forecasting as well as benchmark comparisons. The experiments show our capability of acquiring compact and highly-accurate models.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/eto14.html
  PDF: http://proceedings.mlr.press/v33/eto14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-eto14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Riki
    family: Eto
  - given: Ryohei
    family: Fujimaki
  - given: Satoshi
    family: Morinaga
  - given: Hiroshi
    family: Tamano
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 238-246
  id: eto14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 238
  lastpage: 246
  published: 2014-04-02 00:00:00 +0000
- title: 'Learning with Maximum A-Posteriori Perturbation Models'
  abstract: 'Perturbation models are families of distributions induced from perturbations. They combine randomization of the parameters with maximization to draw unbiased samples. Unlike Gibbs’ distributions, a perturbation model defined on the basis of low order statistics still gives rise to high order dependencies. In this paper, we analyze, extend and seek to estimate such dependencies from data. In particular, we shift the modelling focus from the parameters of the Gibbs’ distribution used as a base model to the space of perturbations. We estimate dependent perturbations over the parameters using a hard-EM approach, cast in the form of inverse convex programs. Each inverse program confines the randomization to the parameter polytope responsible for generating the observed answer. We illustrate the method on several computer vision problems.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/gane14.html
  PDF: http://proceedings.mlr.press/v33/gane14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-gane14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Andreea
    family: Gane
  - given: Tamir
    family: Hazan
  - given: Tommi
    family: Jaakkola
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 247-256
  id: gane14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 247
  lastpage: 256
  published: 2014-04-02 00:00:00 +0000
- title: 'Sketching the Support of a Probability Measure'
  abstract: 'We want to sketch the support of a probability measure on Euclidean    space from samples that have been drawn from the measure. This    problem is closely related to certain manifold learning problems,    where one assumes that the sample points are drawn from a manifold    that is embedded in Euclidean space. Here we propose to sketch the    support of the probability measure (that does not need to be a    manifold) by some gradient flow complex, or more precisely by its    Hasse diagram. The gradient flow is defined with respect to the    distance function to the sample points. We prove that a gradient    flow complex (that can be computed) is homotopy equivalent to the    support of the measure for sufficiently dense samplings, and    demonstrate the feasibility of our approach on real world data sets.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/giesen14.html
  PDF: http://proceedings.mlr.press/v33/giesen14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-giesen14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Joachim
    family: Giesen
  - given: Soeren
    family: Laue
  - given: Lars
    family: Kuehne
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 257-265
  id: giesen14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 257
  lastpage: 265
  published: 2014-04-02 00:00:00 +0000
- title: 'Robust Stochastic Principal Component Analysis'
  abstract: 'We consider the problem of finding lower dimensional subspaces in the presence of outliers and noise in the online setting. In particular, we extend previous batch formulations of robust PCA to the stochastic setting with minimal storage requirements and runtime complexity. We introduce three novel stochastic approximation algorithms for robust PCA that are extensions of standard algorithms for PCA - the stochastic power method, incremental PCA and online PCA using matrix-exponentiated-gradient (MEG) updates. For robust online PCA we also give a sub-linear convergence guarantee. Our numerical results demonstrate the superiority of the the robust online method over the other robust stochastic methods and the advantage of robust methods over their non-robust counterparts in the presence of outliers in artificial and real scenarios.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/goes14.html
  PDF: http://proceedings.mlr.press/v33/goes14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-goes14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: John
    family: Goes
  - given: Teng
    family: Zhang
  - given: Raman
    family: Arora
  - given: Gilad
    family: Lerman
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 266-274
  id: goes14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 266
  lastpage: 274
  published: 2014-04-02 00:00:00 +0000
- title: 'Bayesian Nonparametric Poisson Factorization for Recommendation Systems'
  abstract: 'We develop a Bayesian nonparametric Poisson factorization model for recommendation systems. Poisson factorization implicitly models each user’s limited budget of attention (or money) that allows consumption of only a small subset of the available items.  In our Bayesian nonparametric variant, the number of latent components is theoretically unbounded and effectively estimated when computing a posterior with observed user behavior data.  To approximate the posterior, we develop an efficient variational inference algorithm. It adapts the dimensionality of the latent components to the data, only requires iteration over the user/item pairs that have been rated, and has computational complexity on the same order as for a parametric model with fixed dimensionality.  We studied our model and algorithm with large real-world data sets of user-movie preferences.  Our model eases the computational burden of searching for the number of latent components and gives better predictive performance than its parametric counterpart.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/gopalan14.html
  PDF: http://proceedings.mlr.press/v33/gopalan14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-gopalan14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Prem
    family: Gopalan
  - given: Francisco J.
    family: Ruiz
  - given: Rajesh
    family: Ranganath
  - given: David
    family: Blei
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 275-283
  id: gopalan14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 275
  lastpage: 283
  published: 2014-04-02 00:00:00 +0000
- title: 'Efficiently Enforcing Diversity in Multi-Output Structured Prediction'
  abstract: 'This paper proposes a novel method for efficiently generating multiple diverse predictions for structured prediction problems. Existing methods like SDPPs or DivMBest work by making a series of predictions where each prediction is made after considering the predictions that came before it. Such approaches are inherently sequential and computationally expensive. In contrast, our method, Diverse Multiple Choice Learning, learns a set of models to make multiple independent, yet diverse, predictions at testtime. We achieve this by including a diversity encouraging term in the loss function used for training the models. This approach encourages diversity in the predictions while preserving computational efficiency at test-time. Experimental results on a number of challenging problems show that our method learns models that not only predict more diverse results than competing methods, but are also able to generalize better and produce results with high test accuracy.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/guzman-rivera14.html
  PDF: http://proceedings.mlr.press/v33/guzman-rivera14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-guzman-rivera14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Abner
    family: Guzman-Rivera
  - given: Pushmeet
    family: Kohli
  - given: Dhruv
    family: Batra
  - given: Rob
    family: Rutenbar
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 284-292
  id: guzman-rivera14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 284
  lastpage: 292
  published: 2014-04-02 00:00:00 +0000
- title: 'Learning and Evaluation in Presence of Non-i.i.d. Label Noise'
  abstract: 'In many real-world applications, the simplified assumption of independent and identically distributed noise breaks down, and labels can have structured, systematic noise. For example, in brain-computer interface applications, training data is often the result of lengthy experimental sessions, where the attention levels of participants can change over the course of the experiment. In such application cases, structured label noise will cause problems because most machine learning methods assume independent and identically distributed label noise. In this paper, we present a novel methodology for learning and evaluation in presence of systematic label noise. The core of which is a novel extension of support vector data description / one-class SVM that can incorporate latent variables. Controlled simulations on synthetic data and a real-world EEG  experiment with 20 subjects from the domain of brain-computer-interfacing show that our method achieves accuracies that go beyond the state of the art.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/gornitz14.html
  PDF: http://proceedings.mlr.press/v33/gornitz14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-gornitz14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Nico
    family: Görnitz
  - given: Anne
    family: Porbadnigk
  - given: Alexander
    family: Binder
  - given: Claudia
    family: Sannelli
  - given: Mikio
    family: Braun
  - given: Klaus-Robert
    family: Mueller
  - given: Marius
    family: Kloft
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 293-302
  id: gornitz14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 293
  lastpage: 302
  published: 2014-04-02 00:00:00 +0000
- title: 'Analytic Long-Term Forecasting with Periodic Gaussian Processes'
  abstract: 'Gaussian processes are a state-of-the-art method for learning    models from data. Data with an underlying periodic structure appears    in many areas, e.g., in climatology or robotics.  It is often    important to predict the long-term evolution of such a time series,    and to take the inherent periodicity explicitly into account.  In a    Gaussian process, periodicity can be accounted for by an appropriate kernel    choice. However, the standard periodic kernel does not allow for    analytic long-term forecasting, which requires to map distributions    through the Gaussian process.  To address this shortcoming, we re-parametrize the periodic kernel,    which, in combination with a double approximation, allows for    analytic long-term forecasting of a periodic state evolution with    Gaussian processes.  Our model allows for probabilistic long-term forecasting of periodic    processes, which can be valuable in Bayesian decision making,    optimal control, reinforcement learning, and robotics.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/hajighassemi14.html
  PDF: http://proceedings.mlr.press/v33/hajighassemi14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-hajighassemi14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Nooshin
    family: HajiGhassemi
  - given: Marc
    family: Deisenroth
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 303-311
  id: hajighassemi14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 303
  lastpage: 311
  published: 2014-04-02 00:00:00 +0000
- title: 'On Estimating Causal Effects based on Supplemental Variables'
  abstract: 'This paper considers the problem of estimating causal effects of a treatment on a response using supplementary variables.   Under the assumption that a treatment is associated with a response through a univariate supplementary variable in the framework of linear regression models, Cox (1960) showed that the estimation accuracy of the regression coefficient of the treatment on the response in the single linear regression model can be improved by using the recursive linear regression model based on the supplementary variable from the viewpoint of the asymptotic variance.   However, such assumptions may not hold in many practical situations.   In this paper, we consider the situation where a treatment is associated with a response through a set of supplementary variables in both linear and discrete models.   Then, we show that the estimation accuracy of the causal effect can be improved by using the supplementary variables.   Different from Cox (1960), the results of this paper are derived without the assumption of Gaussian error terms in linear models or dichotomous variables in discrete models.   The results of this paper help us to obtain the reliable evaluation of causal effects from observed data.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/hayashi14.html
  PDF: http://proceedings.mlr.press/v33/hayashi14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-hayashi14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Takahiro
    family: Hayashi
  - given: Manabu
    family: Kuroki
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 312-319
  id: hayashi14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 312
  lastpage: 319
  published: 2014-04-02 00:00:00 +0000
- title: 'Non-Asymptotic Analysis of Relational Learning with One Network'
  abstract: 'This theoretical paper is concerned with a rigorous non-asymptotic analysis of relational learning applied to a single network. Under suitable and intuitive conditions on features and clique dependencies over the network, we present the first probably approximately correct (PAC) bound for maximum likelihood estimation (MLE). To our best knowledge, this is the first sample complexity result of this problem. We propose a novel combinational approach to analyze complex dependencies of relational data, which is crucial to our non-asymptotic analysis. The consistency of MLE under our conditions is also proved as the consequence of our sample complexity bound. Finally, our combinational method for analyzing dependent data can be easily generalized to treat other generalized maximum likelihood estimators for relational learning.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/he14a.html
  PDF: http://proceedings.mlr.press/v33/he14a.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-he14a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Peng
    family: He
  - given: Changshui
    family: Zhang
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 320-327
  id: he14a
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 320
  lastpage: 327
  published: 2014-04-02 00:00:00 +0000
- title: 'Exploiting the Limits of Structure Learning via Inherent Symmetry'
  abstract: 'This theoretical paper is concerned with the structure learning limit for Gaussian Markov random fields from i.i.d. samples. The common strategy is applying the Fano method to a family of restricted ensembles. The efficiency of this method, however, depends crucially on selected restricted ensembles. To break through this limitation, we analyze the whole graph ensemble from high-dimensional geometric and group-theoretical perspectives. The key ingredients of our approach are the geometric property of concentration matrices and the invariance of orthogonal group actions on the symmetric Kullback-Leibler divergence. We then establish the connection of the learning limit and eigenvalues of concentration matrices, which leads to a sharper structure learning limit. To our best knowledge, this is the first paper to consider the structure learning problem via inherent symmetries of the whole ensemble. Finally, our approach can be applicable to other graphical structure learning problems.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/he14b.html
  PDF: http://proceedings.mlr.press/v33/he14b.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-he14b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Peng
    family: He
  - given: Changshui
    family: Zhang
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 328-337
  id: he14b
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 328
  lastpage: 337
  published: 2014-04-02 00:00:00 +0000
- title: 'A Statistical Model for Event Sequence Data'
  abstract: 'The identification of recurring patterns within a sequence of events is an important task in behavior research. In this paper, we consider a general probabilistic framework for identifying such patterns, by distinguishing between events that belong to a pattern and events that occur as part of background processes. The event processes, both for background events and events that are part of recurring patterns, are modeled as competing renewal processes. Using this framework, we develop an inference procedure to detect the sequences present in observed data. Our method is compared to a current approach used within the ethology literature on both simulated data and data collected to study the impact of fragmented and unpredictable maternal behavior on cognitive development of children.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/heins14.html
  PDF: http://proceedings.mlr.press/v33/heins14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-heins14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Kevin
    family: Heins
  - given: Hal
    family: Stern
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 338-346
  id: heins14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 338
  lastpage: 346
  published: 2014-04-02 00:00:00 +0000
- title: 'Probabilistic Solutions to Differential Equations and their Application to Riemannian Statistics'
  abstract: 'We study a probabilistic numerical method for the solution of both boundary and initial value problems that returns a joint Gaussian process posterior over the solution. Such methods have concrete value in the statistics on Riemannian manifolds, where non-analytic ordinary differential equations are involved in virtually all computations. The probabilistic formulation permits marginalising the uncertainty of the numerical solution such that statistics are less sensitive to inaccuracies. This leads to new Riemannian algorithms for mean value computations and principal geodesic analysis. Marginalisation also means results can be less precise than point estimates, enabling a noticeable speed-up over the state of the art. Our approach is an argument for a wider point that uncertainty caused by numerical calculations should be tracked throughout the pipeline of machine learning algorithms.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/hennig14.html
  PDF: http://proceedings.mlr.press/v33/hennig14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-hennig14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Philipp
    family: Hennig
  - given: Søren
    family: Hauberg
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 347-355
  id: hennig14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 347
  lastpage: 355
  published: 2014-04-02 00:00:00 +0000
- title: 'Tilted Variational Bayes'
  abstract: 'We present a novel method for approximate inference. Using some of the constructs from expectation propagation (EP), we derive a lower bound of the marginal likelihood in a similar fashion to variational Bayes (VB). The method combines some of the benefits of VB and EP: it can be used with light-tailed likelihoods (where traditional VB fails), and it provides a lower bound on the marginal likelihood. We apply the method to Gaussian process classification, a situation where the Kullback-Leibler divergence minimized in traditional VB can be infinite, and to robust Gaussian process regression, where the inference process is dramatically simplified in comparison to EP.    Code to reproduce all the experiments can be found at github.com/SheffieldML/TVB.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/hensman14.html
  PDF: http://proceedings.mlr.press/v33/hensman14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-hensman14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: James
    family: Hensman
  - given: Max
    family: Zwiessele
  - given: Neil D.
    family: Lawrence
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 356-364
  id: hensman14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 356
  lastpage: 364
  published: 2014-04-02 00:00:00 +0000
- title: 'On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning'
  abstract: 'We address the problem of finding the maximizer of a nonlinear function that can only be evaluated, subject to noise, at a finite number of query locations. Further, we will assume that there is a constraint on the total number of permitted function evaluations. We introduce a Bayesian approach for this problem and show that it empirically outperforms both the existing frequentist counterpart and other Bayesian optimization methods. The Bayesian approach places emphasis on detailed modelling, including the modelling of correlations among the arms. As a result, it can perform well in situations where the number of arms is much larger than the number of allowed function evaluation, whereas the frequentist counterpart is inapplicable. This feature enables us to develop and deploy practical applications, such as automatic machine learning toolboxes. The paper presents comprehensive comparisons of the proposed approach with many Bayesian and bandit optimization techniques, the first comparison of many of these methods in the literature.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/hoffman14.html
  PDF: http://proceedings.mlr.press/v33/hoffman14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-hoffman14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Matthew
    family: Hoffman
  - given: Bobak
    family: Shahriari
  - given: Nando
    family: Freitas
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 365-374
  id: hoffman14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 365
  lastpage: 374
  published: 2014-04-02 00:00:00 +0000
- title: 'Optimality of Thompson Sampling for Gaussian Bandits Depends on Priors'
  abstract: 'In stochastic bandit problems, a Bayesian policy called Thompson sampling (TS) has recently attracted much attention for its excellent empirical performance. However, the theoretical analysis of this policy is difficult and its asymptotic optimality is only proved for one-parameter models. In this paper we discuss the optimality of TS for the model of normal distributions with unknown means and variances as one of the most fundamental examples of multiparameter models. First we prove that the expected regret of TS with the uniform prior achieves the theoretical bound, which is the first result to show that the asymptotic bound is achievable for the normal distribution model. Next we prove that TS with Jeffreys prior and reference prior cannot achieve the theoretical bound. Therefore choice of priors is important for TS and non-informative priors are sometimes risky in cases of multiparameter models.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/honda14.html
  PDF: http://proceedings.mlr.press/v33/honda14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-honda14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Junya
    family: Honda
  - given: Akimichi
    family: Takemura
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 375-383
  id: honda14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 375
  lastpage: 383
  published: 2014-04-02 00:00:00 +0000
- title: 'Tight Bounds for the Expected Risk of Linear Classifiers and PAC-Bayes Finite-Sample Guarantees'
  abstract: 'We analyze the expected risk of linear classifiers for a fixed weight vector in the “minimax” setting. That is, we analyze the worst-case risk among all data distributions with a given mean and covariance. We provide a simpler proof of the tight polynomial-tail bound for general random variables. For sub-Gaussian random variables, we derive a novel tight exponential-tail bound. We also provide new PAC-Bayes finite-sample guarantees when training data is available. Our “minimax” generalization bounds are dimensionality-independent and \mathcalO(\sqrt1/m) for m samples.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/honorio14.html
  PDF: http://proceedings.mlr.press/v33/honorio14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-honorio14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Jean
    family: Honorio
  - given: Tommi
    family: Jaakkola
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 384-392
  id: honorio14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 384
  lastpage: 392
  published: 2014-04-02 00:00:00 +0000
- title: 'Latent Gaussian Models for Topic Modeling'
  abstract: 'A new approach is proposed for topic modeling, in which the latent matrix factorization employs Gaussian priors, rather than the Dirichlet-class priors widely used in such models. The use of a latent-Gaussian model permits simple and efficient approximate Bayesian posterior inference, via the Laplace approximation. On multiple datasets, the proposed approach is demonstrated to yield results as accurate as state-of-the-art approaches based on Dirichlet constructions, at a small fraction of the computation. The framework is general enough to jointly model text and binary data, here demonstrated to produce accurate and fast results for joint analysis of voting rolls and the associated legislative text. Further, it is demonstrated how the technique may be scaled up to massive data, with encouraging performance relative to alternative methods.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/hu14.html
  PDF: http://proceedings.mlr.press/v33/hu14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-hu14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Changwei
    family: Hu
  - given: Eunsu
    family: Ryu
  - given: David
    family: Carlson
  - given: Yingjian
    family: Wang
  - given: Lawrence
    family: Carin
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 393-401
  id: hu14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 393
  lastpage: 401
  published: 2014-04-02 00:00:00 +0000
- title: 'A Finite-Sample Generalization Bound for Semiparametric Regression: Partially Linear Models'
  abstract: 'In this paper we provide generalization bounds for semiparametric regression with the so-called partially linear models where the regression function is written as the sum of a linear parametric and a nonlinear, nonparametric function, the latter taken from a some set \mathcalH with finite entropy-integral. The problem is technically challenging because the parametric part is unconstrained and the model is underdetermined, while the response is allowed to be unbounded with subgaussian tails. Under natural regularity conditions, we bound the generalization error as a function of the metric entropy of \mathcalH and the dimension of the linear model. Our main tool is a ratio-type concentration inequality for increments of empirical processes, based on which we are able to give an exponential tail bound on the size of the parametric component. We also provide a comparison to alternatives of this technique and discuss why and when the unconstrained parametric part in the model may cause a problem in terms of the expected risk. We also explain by means of a specific example why this problem cannot be detected using the results of classical asymptotic analysis often seen in the statistics literature.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/huang14.html
  PDF: http://proceedings.mlr.press/v33/huang14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-huang14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Ruitong
    family: Huang
  - given: Csaba
    family: Szepesvari
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 402-410
  id: huang14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 402
  lastpage: 410
  published: 2014-04-02 00:00:00 +0000
- title: 'Global Optimization Methods for Extended Fisher Discriminant Analysis'
  abstract: 'The Fisher discriminant analysis (FDA) is a common technique for binary classification. A parametrized extension, which we call the extended FDA, has been introduced from the viewpoint of robust optimization. In this work, we first give a new probabilistic interpretation of the extended FDA. We then develop algorithms for solving an optimization problem that arises from the extended FDA: computing the distance between a point and the surface of an ellipsoid. We solve this problem via the KKT points, which we show are obtained by solving a generalized eigenvalue problem. We speed up the algorithm by taking advantage of the matrix structure and proving that a globally optimal solution is a KKT point with the smallest Lagrange multiplier, which can be computed efficiently as the leftmost eigenvalue. Numerical experiments illustrate the efficiency and effectiveness of the extended FDA model combined with our algorithm.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/iwata14.html
  PDF: http://proceedings.mlr.press/v33/iwata14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-iwata14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Satoru
    family: Iwata
  - given: Yuji
    family: Nakatsukasa
  - given: Akiko
    family: Takeda
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 411-419
  id: iwata14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 411
  lastpage: 419
  published: 2014-04-02 00:00:00 +0000
- title: 'High-Dimensional Density Ratio Estimation with Extensions to Approximate Likelihood Computation'
  abstract: 'The ratio between two probability density functions is an important component of various tasks, including selection bias correction, novelty detection and classification.  Recently, several estimators of this ratio have been proposed. Most of these methods fail if the sample space is high-dimensional, and hence require a dimension reduction step, the result of which can be a significant loss of information. Here we propose a simple-to-implement, fully nonparametric density ratio estimator that expands the ratio in terms of the eigenfunctions of a kernel-based operator; these functions reflect the underlying geometry of the data (e.g., submanifold structure), often leading to better estimates without an explicit dimension reduction step.  We show how our general framework can be extended to address another important problem, the estimation of a likelihood function in situations where that function cannot be well-approximated by an analytical form. One is often faced with this situation when performing statistical inference with data from the sciences, due the complexity of the data and of the processes that generated those data. We emphasize applications where using existing likelihood-free methods of inference would be challenging due to the high dimensionality of the sample space, but where our spectral series method yields a reasonable estimate of the likelihood function. We provide theoretical guarantees and illustrate the effectiveness of our proposed method with numerical experiments.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/izbicki14.html
  PDF: http://proceedings.mlr.press/v33/izbicki14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-izbicki14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Rafael
    family: Izbicki
  - given: Ann
    family: Lee
  - given: Chad
    family: Schafer
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 420-429
  id: izbicki14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 420
  lastpage: 429
  published: 2014-04-02 00:00:00 +0000
- title: 'Near Optimal Bayesian Active Learning for Decision Making'
  abstract: 'How should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible.     We identify necessary and sufficient conditions for correctly identifying a decision region that contains all hypotheses consistent with observations. We develop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove that is competitive with the intractable optimal policy. Our efficient implementation of the algorithm relies on computing subsets of the complete homogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on two practical applications: approximate comparison-based learning and active localization using a robot manipulator.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/javdani14.html
  PDF: http://proceedings.mlr.press/v33/javdani14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-javdani14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Shervin
    family: Javdani
  - given: Yuxin
    family: Chen
  - given: Amin
    family: Karbasi
  - given: Andreas
    family: Krause
  - given: Drew
    family: Bagnell
  - given: Siddhartha
    family: Srinivasa
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 430-438
  id: javdani14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 430
  lastpage: 438
  published: 2014-04-02 00:00:00 +0000
- title: 'A Level-set Hit-and-run Sampler for Quasi-Concave Distributions'
  abstract: 'We develop a new sampling strategy that uses the hit-and-run algorithm within level sets of a target density. Our method can be applied to any quasi-concave density, which covers a broad class of models. Standard sampling methods often perform poorly on densities that are high-dimensional or multi-modal. Our level set sampler performs well in high-dimensional settings, which we illustrate on a spike-and-slab mixture model. We also extend our method to exponentially-tilted quasi-concave densities, which arise in Bayesian models consisting of a log-concave likelihood and quasi-concave prior density. We illustrate our exponentially-tilted level-set sampler on a Cauchy-normal model where our sampler is better able to handle a high-dimensional and multi-modal posterior distribution compared to Gibbs sampling and Hamiltonian Monte Carlo.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/jensen14.html
  PDF: http://proceedings.mlr.press/v33/jensen14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-jensen14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Shane
    family: Jensen
  - given: Dean
    family: Foster
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 439-447
  id: jensen14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 439
  lastpage: 447
  published: 2014-04-02 00:00:00 +0000
- title: 'New Bounds on Compressive Linear Least Squares Regression'
  abstract: 'In this paper we provide a new analysis of compressive least squares regression that removes a spurious log N factor from previous bounds, where N is the number of training points. Our new bound has a clear interpretation and reveals meaningful structural properties of the linear regression problem that makes it solvable effectively in a small dimensional random subspace. In addition, the main part of our analysis does not require the compressive matrix to have the Johnson-Lindenstrauss property, or the RIP property. Instead, we only require its entries to be drawn i.i.d. from a 0-mean symmetric distribution with finite first four moments.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kaban14.html
  PDF: http://proceedings.mlr.press/v33/kaban14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kaban14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Ata
    family: Kaban
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 448-456
  id: kaban14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 448
  lastpage: 456
  published: 2014-04-02 00:00:00 +0000
- title: 'Recovering Distributions from Gaussian RKHS Embeddings'
  abstract: 'Recent advances of kernel methods have yielded a framework for nonparametric statistical inference called RKHS embeddings, in which all probability distributions are represented as elements in a reproducing kernel Hilbert space, namely kernel means. In this paper, we consider the recovery of the information of a distribution from an estimate of the kernel mean, when a Gaussian kernel is used. To this end, we theoretically analyze the properties of a consistent estimator of a kernel mean, which is represented as a weighted sum of feature vectors. First, we prove that the weighted average of a function in a Besov space, whose weights and samples are given by the kernel mean estimator, converges to the expectation of the function. As corollaries, we show that the moments and the probability measures on intervals can be recovered from an estimate of the kernel mean. We also prove that a consistent estimator of the density of a distribution can be defined using a kernel mean estimator. This result confirms that we can in fact completely recover the information of distributions from RKHS embeddings.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kanagawa14.html
  PDF: http://proceedings.mlr.press/v33/kanagawa14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kanagawa14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Motonobu
    family: Kanagawa
  - given: Kenji
    family: Fukumizu
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 457-465
  id: kanagawa14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 457
  lastpage: 465
  published: 2014-04-02 00:00:00 +0000
- title: 'Collaborative Ranking for Local Preferences'
  abstract: 'For many collaborative ranking tasks, we have access to relative preferences among subsets of items, but not to global preferences among all items. To address this, we introduce a matrix factorization framework called Collaborative Local Ranking (CLR). We justify CLR by proving a bound on its generalization error, the first such bound for collaborative ranking that we know of. We then derive a simple alternating minimization algorithm and prove that it converges in sublinear time. Lastly, we apply CLR to a novel venue recommendation task and demonstrate that it outperforms state-of-the-art collaborative ranking methods on real-world data sets.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kapicioglu14.html
  PDF: http://proceedings.mlr.press/v33/kapicioglu14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kapicioglu14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Berk
    family: Kapicioglu
  - given: David
    family: Rosenberg
  - given: Robert
    family: Schapire
  - given: Tony
    family: Jebara
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 466-474
  id: kapicioglu14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 466
  lastpage: 474
  published: 2014-04-02 00:00:00 +0000
- title: 'Scalable Collaborative Bayesian Preference Learning'
  abstract: 'Learning about users’ utilities from preference, discrete choice or implicit feedback data is of integral importance in e-commerce, targeted advertising and web search. Due to the sparsity and diffuse nature of data, Bayesian approaches  hold much promise, yet most prior work does not scale up to realistic data sizes. We shed light on why inference for such settings is computationally   difficult for standard machine learning methods, most of which focus on predicting explicit ratings only. To simplify the difficulty, we present a novel expectation maximization algorithm, driven by expectation propagation approximate inference, which scales to very large datasets without requiring strong factorization assumptions. Our  utility model uses both latent bilinear collaborative filtering and non-parametric Gaussian process (GP) regression. In experiments on large real-world  datasets, our method gives substantially better results than either matrix factorization or GPs in isolation, and converges significantly faster.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/khan14.html
  PDF: http://proceedings.mlr.press/v33/khan14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-khan14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Mohammad Emtiyaz
    family: Khan
  - given: Young Jun
    family: Ko
  - given: Matthias
    family: Seeger
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 475-483
  id: khan14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 475
  lastpage: 483
  published: 2014-04-02 00:00:00 +0000
- title: 'A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data'
  abstract: 'We investigate a Gaussian latent variable model for semi-supervised learning of linear large margin classifiers.  The model’s latent variables encode the signed distance of examples to the separating hyperplane, and we constrain these variables, for both labeled and unlabeled examples, to ensure that the classes are separated by a large margin.  Our approach is based on similar intuitions as semi-supervised support vector machines (S3VMs), but these intuitions are formalized in a probabilistic framework.  Within this framework we are able to derive an especially simple Expectation-Maximization (EM) algorithm for learning.  The algorithm alternates between applying Bayes rule to “fill in” the latent variables (the E-step) and performing an unconstrained least-squares regression to update the weight vector (the M-step).  For the best results it is necessary to constrain the unlabeled data to have a similar ratio of positive to negative examples as the labeled data.  Within our model this constraint renders exact inference intractable, but we show that a Lyapunov central limit theorem (for sums of independent, but non-identical random variables) provides an excellent approximation to the true posterior distribution.  We perform experiments on large-scale text classification and find that our model significantly outperforms existing implementations of S3VMs.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kim14a.html
  PDF: http://proceedings.mlr.press/v33/kim14a.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kim14a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Do-kyum
    family: Kim
  - given: Matthew
    family: Der
  - given: Lawrence
    family: Saul
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 484-492
  id: kim14a
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 484
  lastpage: 492
  published: 2014-04-02 00:00:00 +0000
- title: 'Scalable Variational Bayesian Matrix Factorization with Side Information'
  abstract: 'Bayesian matrix factorization (BMF) is a popular method for collaborative prediction, because of its robustness to overfitting as well as of being free from cross-validation for fine tuning of regularization parameters. In practice, however, due to its cubic time complexity with respect to the rank of factor matrices, existing variational inference algorithms for BMF are not well suited to web-scale datasets where billions of ratings provided by millions of users are available. The time complexity even increases when the side information, such as user binary implicit feedback or item content information, is incorporated into variational Bayesian matrix factorization (VBMF). For instance, a state of the arts in VBMF with side information, is to place Gaussian priors on user and item factor matrices, where mean of each prior is regressed on the corresponding side information. Since this approach introduces additional cubic time complexity with respect to the size of feature vectors, the use of rich side information in a form of high-dimensional feature vector is prohibited. In this paper, we present a scalable inference for VBMF with side information, the complexity of which is linear in the rank K of factor matrices. Moreover, the algorithm can be easily parallelized on multi-core systems. Experiments on large-scale datasets demonstrate the useful behavior of our algorithm such as scalability, fast learning, and prediction accuracy.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kim14b.html
  PDF: http://proceedings.mlr.press/v33/kim14b.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kim14b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Yong-Deok
    family: Kim
  - given: Seungjin
    family: Choi
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 493-502
  id: kim14b
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 493
  lastpage: 502
  published: 2014-04-02 00:00:00 +0000
- title: 'Algebraic Reconstruction Bounds and Explicit Inversion for Phase Retrieval at the Identifiability Threshold'
  abstract: 'We study phase retrieval from magnitude measurements of an unknown signal as an algebraic estimation problem. Indeed, phase retrieval from rank-one and more general linear measurements can be treated in an algebraic way. It is verified that a certain number of generic rank-one or generic linear measurements are sufficient to enable signal reconstruction for generic signals, and slightly more generic measurements yield reconstructability for all signals. Our results solve few open problems stated in the recent literature. Furthermore, we show how the algebraic estimation problem can be solved by a closed-form algebraic estimation technique, termed ideal regression, providing non-asymptotic success guarantees.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kiraly14.html
  PDF: http://proceedings.mlr.press/v33/kiraly14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kiraly14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Franz
    family: Király
  - given: Martin
    family: Ehler
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 503-511
  id: kiraly14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 503
  lastpage: 511
  published: 2014-04-02 00:00:00 +0000
- title: 'Visual Boundary Prediction:  A Deep Neural Prediction Network and Quality Dissection'
  abstract: 'This paper investigates visual boundary detection, i.e. prediction   of the presence of a boundary at a given image location. We develop a   novel neurally-inspired deep architecture for the task. Notable   aspects of our work are (i) the use of “covariance features”  [Ranzato and Hinton, 2010] which depend on the \emphsquared response   of a filter to the input image, and (ii) the integration of image   information from multiple scales and semantic levels via multiple   streams of interlinked, layered, and non-linear “deep”   processing. Our results on the Berkeley Segmentation Data Set   500 (BSDS500) show comparable or better performance to the   top-performing methods  [Arbelaez et al., 2011, Ren and Bo, 2012, Lim et al., 2013, Dollár  and Zitnick, 2013]  with effective inference times. We also propose novel quantitative assessment techniques for improved method understanding and comparison. We  carefully dissect the performance of our architecture,  feature-types used and training methods, providing clear signals for model understanding and development.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kivinen14.html
  PDF: http://proceedings.mlr.press/v33/kivinen14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kivinen14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Jyri
    family: Kivinen
  - given: Chris
    family: Williams
  - given: Nicolas
    family: Heess
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 512-521
  id: kivinen14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 512
  lastpage: 521
  published: 2014-04-02 00:00:00 +0000
- title: 'Low-Rank Spectral Learning'
  abstract: 'Spectral learning methods have recently been proposed as  alternatives to slow, non-convex optimization algorithms like EM for  a variety of probabilistic models in which hidden information must  be inferred by the learner.  These methods are typically controlled  by a rank hyperparameter that sets the complexity of the model; when  the model rank matches the true rank of the process generating the  data, the resulting predictions are provably consistent and admit  finite sample convergence bounds.  However, in practice we usually  do not know the true rank, and, in any event, from a computational  and statistical standpoint it is likely to be prohibitively large.  It is therefore of great practical interest to understand the  behavior of low-rank spectral learning, where the model rank is   less than the true rank.  Counterintuitively, we show that even   when the singular values omitted by lowering the rank are   arbitrarily small, the resulting prediction errors can in fact be  arbitrarily large.  We identify two distinct possible causes for  this bad behavior, and illustrate them with simple examples.  We  then show that these two causes are essentially complete: assuming  that they do not occur, we can prove that the prediction error is  bounded in terms of the magnitudes of the omitted singular values.  We argue that the assumptions necessary for this result are  relatively realistic, making low-rank spectral learning a viable  option for many applications.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kulesza14.html
  PDF: http://proceedings.mlr.press/v33/kulesza14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kulesza14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Alex
    family: Kulesza
  - given: N. Raj
    family: Rao
  - given: Satinder
    family: Singh
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 522-530
  id: kulesza14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 522
  lastpage: 530
  published: 2014-04-02 00:00:00 +0000
- title: 'Fugue: Slow-Worker-Agnostic Distributed Learning for Big Models on Big Data'
  abstract: 'We present a scheme for fast, distributed learning on  big (i.e. high-dimensional) models applied to big datasets.  Unlike algorithms that focus on distributed learning in either the big data or big model setting  (but not both), our scheme partitions both the data and model variables  simultaneously. This not only leads to faster learning on distributed clusters,  but also enables machine learning applications where both data  and model are too large to fit within the memory of a single machine. Furthermore, our scheme  allows worker machines to perform additional updates while waiting for slow workers to finish,  which provides users with a tunable synchronization strategy that can  be set based on learning needs and cluster conditions.  We prove the correctness of such strategies, as well as provide  bounds on the variance of the model variables under our scheme.  Finally, we present empirical results for latent space models such  as topic models, which demonstrate that our method  scales well with large data and model sizes, while beating  learning strategies that fail to take both data and model partitioning into account.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kumar14.html
  PDF: http://proceedings.mlr.press/v33/kumar14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kumar14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Abhimanu
    family: Kumar
  - given: Alex
    family: Beutel
  - given: Qirong
    family: Ho
  - given: Eric
    family: Xing
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 531-539
  id: kumar14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 531
  lastpage: 539
  published: 2014-04-02 00:00:00 +0000
- title: 'Computational Education using Latent Structured Prediction'
  abstract: 'Computational education offers an important add-on to conventional teaching. To provide optimal learning conditions, accurate representation of students’ current skills and adaptation to newly acquired knowledge are essential. To obtain sufficient representational power we investigate suitability of general graphical models and discuss adaptation by learning parameters of a log-linear distribution. For interpretability we propose to constrain the parameter space a-priori by leveraging domain knowledge. We show the benefits of general graphical models and of regularizing the parameter space by evaluation of our models on data collected from a computational education software for children having difficulties in learning mathematics.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/kaser14.html
  PDF: http://proceedings.mlr.press/v33/kaser14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-kaser14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Tanja
    family: Käser
  - given: Alexander
    family: Schwing
  - given: Tamir
    family: Hazan
  - given: Markus
    family: Gross
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 540-548
  id: kaser14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 540
  lastpage: 548
  published: 2014-04-02 00:00:00 +0000
- title: 'Towards building a Crowd-Sourced Sky Map'
  abstract: 'We describe a system that builds a high dynamic-range and wide-angle image of the night sky by combining a large set of input images.  The method makes use of pixel-rank information in the individual input images to improve a “consensus” pixel rank in the combined image.  Because it only makes use of ranks and the complexity of the algorithm is linear in the number of images, the method is useful for large sets of uncalibrated images that might have undergone unknown non-linear tone mapping transformations for visualization or aesthetic reasons.  We apply the method to images of the night sky (of unknown provenance) discovered on the Web.  The method permits discovery of astronomical objects or features that are not visible in any of the input images taken individually.  More importantly, however, it permits scientific exploitation of a huge source of astronomical images that would not be available to astronomical research without our automatic system.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/lang14.html
  PDF: http://proceedings.mlr.press/v33/lang14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-lang14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Dustin
    family: Lang
  - given: David
    family: Hogg
  - given: Bernhard
    family: Schölkopf
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 549-557
  id: lang14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 549
  lastpage: 557
  published: 2014-04-02 00:00:00 +0000
- title: 'Incremental Tree-Based Inference with Dependent Normalized Random Measures'
  abstract: 'Normalized random measures (NRMs) form a broad class of discrete random measures that are used as priors for Bayesian nonparametric models. Dependent normalized random measures (DNRMs) introduce dependencies in a set of NRMs, to facilitate the handling of data where the assumption of exchangeability is violated. Various methods have been developed to construct DNRMs; of particular interest is mixed normalized random measures (MNRMs), where DNRM is represented as a mixture of underlying shared normalized random measures. Emphasis in existing works is placed on the construction methods of DNRMs, but there is a little work on efficient inference for DNRMs. In this paper, we present a tree-based inference method for MNRM mixture models, extending Bayesian hierarchical clustering (BHC) which was originally developed as a deterministic approximate inference for Dirichlet process mixture (DPM) models. We also present an incremental inference for MNRM mixture models, building a tree incrementally in the sense that the tree structure is partially updated whenever a new data point comes in. The tree, when constructed in such a way, allows us to efficiently perform tree-consistent MAP inference in MRNM mixture models, determining a most probable tree-consistent partition, as well as to compute a marginal likelihood approximately. Numerical experiments on both synthetic and real-world datasets demonstrate the usefulness of our algorithm, compared to MCMC methods.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/lee14.html
  PDF: http://proceedings.mlr.press/v33/lee14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-lee14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Juho
    family: Lee
  - given: Seungjin
    family: Choi
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 558-566
  id: lee14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 558
  lastpage: 566
  published: 2014-04-02 00:00:00 +0000
- title: 'Jointly Informative Feature Selection'
  abstract: 'We propose several novel criteria for the selection of groups of jointly informative continuous features in the context of classification. Our approach is based on combining a Gaussian modeling of the feature responses, with derived upper bounds on their mutual information with the class label and their joint entropy.    We further propose specific algorithmic implementations of these criteria which reduce the computational complexity of the algorithms  by up to two-orders of magnitude, making these strategies tractable in practice.    Experiments on multiple computer-vision data-bases, and using several types of classifiers, show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/lefakis14.html
  PDF: http://proceedings.mlr.press/v33/lefakis14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-lefakis14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Leonidas
    family: Lefakis
  - given: Francois
    family: Fleuret
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 567-575
  id: lefakis14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 567
  lastpage: 575
  published: 2014-04-02 00:00:00 +0000
- title: 'Learning Heterogeneous Hidden Markov Random Fields'
  abstract: 'Hidden Markov random fields (HMRFs) are conventionally assumed to be homogeneous in the sense that the potential functions are invariant across different sites. However in some biological applications, it is desirable to make HMRFs heterogeneous, especially when there exists some background knowledge about how the potential functions vary. We formally define heterogeneous HMRFs and propose an EM algorithm whose M-step combines a contrastive divergence learner with a kernel smoothing step to incorporate the background knowledge. Simulations show that our algorithm is effective for learning heterogeneous HMRFs and outperforms alternative binning methods. We learn a heterogeneous HMRF in a real-world study.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/liu14.html
  PDF: http://proceedings.mlr.press/v33/liu14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-liu14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Jie
    family: Liu
  - given: Chunming
    family: Zhang
  - given: Elizabeth
    family: Burnside
  - given: David
    family: Page
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 576-584
  id: liu14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 576
  lastpage: 584
  published: 2014-04-02 00:00:00 +0000
- title: 'PAC-Bayesian Collective Stability'
  abstract: 'Recent results have shown that the generalization error of structured predictors decreases with both the number of examples and the size of each example, provided the data distribution has weak dependence and the predictor exhibits a smoothness property called collective stability. These results use an especially strong definition of collective stability that must hold uniformly over all inputs and all hypotheses in the class. We investigate whether weaker definitions of collective stability suffice. Using the PAC-Bayes framework, which is particularly amenable to our new definitions, we prove that generalization is indeed possible when uniform collective stability happens with high probability over draws of predictors (and inputs). We then derive a generalization bound for a class of structured predictors with variably convex inference, which suggests a novel learning objective that optimizes collective stability.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/london14.html
  PDF: http://proceedings.mlr.press/v33/london14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-london14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Ben
    family: London
  - given: Bert
    family: Huang
  - given: Ben
    family: Taskar
  - given: Lise
    family: Getoor
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 585-594
  id: london14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 585
  lastpage: 594
  published: 2014-04-02 00:00:00 +0000
- title: 'Active Area Search via Bayesian Quadrature'
  abstract: 'The selection of data collection locations is a problem that has received significant research attention from classical design of experiments to various recent active learning algorithms.  Typical objectives are to map an unknown function, optimize it, or find level sets in it.  Each of these objectives focuses on an assessment of individual points.  The introduction of set kernels has led to algorithms that instead consider labels assigned to sets of data points.  In this paper we combine these two concepts and consider the problem of choosing data collection locations when the goal is to identify regions whose set of collected data would be labeled positively by a set classifier.  We present an algorithm for the case where the positive class is defined in terms of a region’s average function value being above some threshold with high probability, a problem we call active area search. To this end, we model the latent function using a Gaussian process and use Bayesian quadrature to estimate its integral on predefined regions.  Our method is the first which directly solves the active area search problem.  In experiments it outperforms previous algorithms that were developed for other active search goals.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/ma14.html
  PDF: http://proceedings.mlr.press/v33/ma14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-ma14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Yifei
    family: Ma
  - given: Roman
    family: Garnett
  - given: Jeff
    family: Schneider
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 595-603
  id: ma14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 595
  lastpage: 603
  published: 2014-04-02 00:00:00 +0000
- title: 'Active Boundary Annotation using Random MAP Perturbations'
  abstract: 'We address the problem of efficiently annotating labels of objects when they are structured. Often the distribution over labels can be described using a joint potential function over the labels for which sampling is provably hard but efficient maximum a-posteriori (MAP) solvers exist. In this setting we develop novel entropy bounds that are based on the expected amount of perturbation to the potential function that is needed to change MAP decisions. By reasoning about the entropy reduction and cost tradeoff, our algorithm actively selects the next annotation task. As an example of our framework we propose a boundary refinement task which can used to obtain pixel-accurate image boundaries much faster than traditional tools by focussing on parts of the image for refinement in a multi-scale manner.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/maji14.html
  PDF: http://proceedings.mlr.press/v33/maji14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-maji14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Subhransu
    family: Maji
  - given: Tamir
    family: Hazan
  - given: Tommi
    family: Jaakkola
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 604-613
  id: maji14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 604
  lastpage: 613
  published: 2014-04-02 00:00:00 +0000
- title: 'Interpretable Sparse High-Order Boltzmann Machines'
  abstract: 'Fully-observable high-order Boltzmann Machines are capable of identifying explicit high-order feature interactions theoretically. However, they have never been used in practice due to their prohibitively high computational cost for inference and learning. In this paper, we propose an efficient approach for learning a fully observable high-order Boltzmann Machine based on sparse learning and contrastive divergence, resulting in an interpretable Sparse High-order Boltzmann Machine, denoted as SHBM. Experimental results on synthetic datasets and a real dataset demonstrate that SHBM can produce higher pseudo-log-likelihood and better reconstructions on test data than the state-of-the-art methods. In addition, we apply SHBM to a challenging bioinformatics problem of discovering complex Transcription Factor interactions. Compared to conventional Boltzmann Machine and directed Bayesian Network, SHBM can identify much more biologically meaningful interactions that are supported by recent biological studies. To the best of our knowledge, SHBM is the first working Boltzmann Machine with explicit high-order feature interactions applied to real-world problems.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/min14.html
  PDF: http://proceedings.mlr.press/v33/min14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-min14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Martin Renqiang
    family: Min
  - given: Xia
    family: Ning
  - given: Chao
    family: Cheng
  - given: Mark
    family: Gerstein
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 614-622
  id: min14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 614
  lastpage: 622
  published: 2014-04-02 00:00:00 +0000
- title: 'Efficient Lifting of MAP LP Relaxations Using k-Locality'
  abstract: 'Inference in large scale graphical models is an important task in many domains, and in particular for probabilistic relational models (e.g,. Markov logic networks). Such models often exhibit considerable symmetry, and it is a challenge to devise algorithms that exploit this symmetry to speed up inference.   Here we address this task in the context of the MAP inference problem and its linear programming relaxations. We show that symmetry in these problems  can be discovered using an elegant algorithm known as the k-dimensional Weisfeiler-Lehman (k-WL) algorithm. We run k-WL on the original graphical model, and not on the far larger graph of the linear program (LP) as proposed in earlier work in the field. Furthermore, the algorithm is polynomial and thus far more practical than other previous approaches which rely on orbit partitions that are GI complete to find. The fact that k-WL can be used in this manner follows from the recently introduced notion of k-local LPs and their relation to Sherali Adams relaxations of graph automorphisms. Finally, for relational models such as Markov logic networks, the benefits of our approach are even more dramatic, as we can discover symmetries in the original domain graph, as opposed to running lifting on the much larger grounded model.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/mladenov14.html
  PDF: http://proceedings.mlr.press/v33/mladenov14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-mladenov14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Martin
    family: Mladenov
  - given: Kristian
    family: Kersting
  - given: Amir
    family: Globerson
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 623-632
  id: mladenov14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 623
  lastpage: 632
  published: 2014-04-02 00:00:00 +0000
- title: 'A Geometric Algorithm for Scalable Multiple Kernel Learning'
  abstract: 'We present a geometric formulation of the Multiple Kernel Learning (MKL) problem.   To do so, we reinterpret the problem of learning kernel weights as searching for a kernel that maximizes the minimum (kernel) distance between two convex polytopes.   This interpretation combined with additional structural insights from our geometric formulation allows us to reduce the MKL problem to a simple optimization routine that yields provable convergence as well as quality guarantees. As a result our method scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with an uniform unweighted combination of kernels.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/moeller14.html
  PDF: http://proceedings.mlr.press/v33/moeller14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-moeller14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: John
    family: Moeller
  - given: Parasaran
    family: Raman
  - given: Suresh
    family: Venkatasubramanian
  - given: Avishek
    family: Saha
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 633-642
  id: moeller14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 633
  lastpage: 642
  published: 2014-04-02 00:00:00 +0000
- title: 'On the Testability of Models with Missing Data'
  abstract: 'Graphical models that depict the process by which data are lost are helpful in recovering information from missing data. We address the question of whether any such model can be submitted to a statistical test given that the data available are corrupted by missingness. We present sufficient conditions for testability in missing data applications and note the impediments for testability when data are contaminated by missing entries. Our results strengthen the available tests for MCAR and MAR and further provide tests in the category of MNAR. Furthermore, we provide sufficient conditions to detect the existence of dependence between a variable and its missingness mechanism. We use our results to show that model sensitivity persists in    almost all models typically categorized as MNAR.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/mohan14.html
  PDF: http://proceedings.mlr.press/v33/mohan14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-mohan14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Karthika
    family: Mohan
  - given: Judea
    family: Pearl
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 643-650
  id: mohan14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 643
  lastpage: 650
  published: 2014-04-02 00:00:00 +0000
- title: 'Selective Sampling with Drift'
  abstract: 'Recently there has been much work on selective sampling, an online active learning setting, in which algorithms work in rounds. On each round an algorithm receives an input and makes a prediction. Then, it can decide whether to query a label, and if so to update its model, otherwise the input is discarded. Most of this work is focused on the stationary case, where it is assumed that there is a fixed target model, and the performance of the algorithm is compared to a fixed model. However, in many real-world applications, such as spam prediction, the best target function may drift over time, or have shifts from time to time. We develop a novel selective sampling algorithm for the drifting setting, analyze it under no assumptions on the mechanism generating the sequence of instances, and derive new mistake bounds that depend on the amount of drift in the problem. Simulations on synthetic and real-world datasets demonstrate the superiority of our algorithms as a selective sampling algorithm in the drifting setting.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/moroshko14.html
  PDF: http://proceedings.mlr.press/v33/moroshko14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-moroshko14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Edward
    family: Moroshko
  - given: Koby
    family: Crammer
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 651-659
  id: moroshko14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 651
  lastpage: 659
  published: 2014-04-02 00:00:00 +0000
- title: 'The Dependent Dirichlet Process Mixture of Objects for Detection-free Tracking and Object Modeling'
  abstract: 'This paper explores how to find, track, and learn models of arbitrary objects in a video without a predefined method for object detection.  We present a model that localizes objects via unsupervised tracking while learning a representation of each object, avoiding the need for pre-built detectors. Our model uses a dependent Dirichlet process mixture to capture the uncertainty in the number and appearance of objects and requires only spatial and color video data that can be efficiently extracted via frame differencing. We give two inference algorithms for use in both online and offline settings, and use them to perform accurate detection-free tracking on multiple real videos.  We demonstrate our method in difficult detection scenarios involving occlusions and appearance shifts, on videos containing a large number of objects, and on a recent human-tracking benchmark where we show performance comparable to state of the art detector-based methods.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/neiswanger14.html
  PDF: http://proceedings.mlr.press/v33/neiswanger14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-neiswanger14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Willie
    family: Neiswanger
  - given: Frank
    family: Wood
  - given: Eric
    family: Xing
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 660-668
  id: neiswanger14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 660
  lastpage: 668
  published: 2014-04-02 00:00:00 +0000
- title: 'Bias Reduction and Metric Learning for Nearest-Neighbor Estimation of Kullback-Leibler Divergence'
  abstract: 'Asymptotically unbiased nearest-neighbor estimators for K-L divergence have recently been proposed and demonstrated in a number of applications.  With small sample sizes, however, these nonparametric methods typically suffer from high estimation bias due to the non-local statistics of empirical nearest-neighbor information.  In this paper, we show that this non-local bias can be mitigated by changing the distance metric, and we propose a method for learning an optimal Mahalanobis-type metric based on global information provided by approximate parametric models of the underlying densities. In both simulations and experiments, we demonstrate that this interplay between parametric models and nonparametric estimation methods significantly improves the accuracy of the nearest-neighbor K-L divergence estimator.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/noh14.html
  PDF: http://proceedings.mlr.press/v33/noh14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-noh14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Yung-Kyun
    family: Noh
  - given: Masashi
    family: Sugiyama
  - given: Song
    family: Liu
  - given: Marthinus C.
    family: Plessis
  - given: Frank Chongwoo
    family: Park
  - given: Daniel D.
    family: Lee
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 669-677
  id: noh14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 669
  lastpage: 677
  published: 2014-04-02 00:00:00 +0000
- title: 'Robust Forward Algorithms via PAC-Bayes and Laplace Distributions'
  abstract: 'Laplace random variables are commonly used  to model extreme noise in many fields, while  systems trained to deal with such noises are  often characterized by robustness properties.  We introduce new learning algorithms that  minimize objectives derived directly from  PAC-Bayes bounds, incorporating Laplace  distributions. The resulting algorithms are  regulated by the Huber loss function and  are robust to noise, as the Laplace distribution  integrated large deviation of parameters.  We analyze the convexity properties  of the objective, and propose a few bounds  which are fully convex, two of which jointly  convex in the mean and standard-deviation  under certain conditions. We derive new forward  algorithms analogous to recent boosting  algorithms, providing novel relations between  boosting and PAC-Bayes analysis. Experiments  show that our algorithms outperforms  AdaBoost, L1-LogBoost, and RobustBoost  in a wide range of input noise.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/noy14.html
  PDF: http://proceedings.mlr.press/v33/noy14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-noy14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Asaf
    family: Noy
  - given: Koby
    family: Crammer
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 678-686
  id: noy14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 678
  lastpage: 686
  published: 2014-04-02 00:00:00 +0000
- title: 'Joint Structure Learning of Multiple Non-Exchangeable Networks'
  abstract: 'Several methods have recently been developed for joint structure learning of multiple (related) graphical models or networks. These methods treat individual networks as exchangeable, such that each pair of networks are equally encouraged to have similar structures. However, in many practical applications, exchangeability in this sense does not hold, as some pairs of networks may be more closely related than others, for example due to group and sub-group structures in the data. Here we present a novel Bayesian formulation that generalises joint structure learning beyond the exchangeable case. Moreover (i) a novel default prior over the joint structure space is proposed that requires no user input; (ii) latent networks are permitted; (iii) for time series data and dynamic Bayesian networks, an efficient, exact algorithm is provided. We present empirical results on non-exchangeable populations, including a real example from cancer biology, where cell-line specific networks are related according to known genomic features.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/oates14.html
  PDF: http://proceedings.mlr.press/v33/oates14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-oates14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Chris
    family: Oates
  - given: Sach
    family: Mukherjee
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 687-695
  id: oates14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 687
  lastpage: 695
  published: 2014-04-02 00:00:00 +0000
- title: 'Scaling Nonparametric Bayesian Inference via Subsample-Annealing'
  abstract: 'We describe an adaptation of the simulated annealing algorithm to nonparametric clustering and related probabilistic models. This new algorithm learns nonparametric latent structure over a growing and constantly churning subsample of training data, where the portion of data subsampled can be interpreted as the inverse temperature β(t) in an annealing schedule. Gibbs sampling at high temperature (i.e., with a very small subsample) can more quickly explore sketches of the final latent state by (a) making longer jumps around latent space (as in block Gibbs) and (b) lowering energy barriers (as in simulated annealing). We prove subsample annealing speeds up mixing time N^2 →N in a simple clustering model and \exp(N) →N in another class of models, where N is data size. Empirically subsample-annealing outperforms naive Gibbs sampling in accuracy-per-wallclock time, and can scale to larger datasets and deeper hierarchical models. We demonstrate improved inference on million-row subsamples of US Census data and network log data and a 307-row hospital rating dataset, using a Pitman-Yor generalization of the Cross Categorization model.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/obermeyer14.html
  PDF: http://proceedings.mlr.press/v33/obermeyer14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-obermeyer14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Fritz
    family: Obermeyer
  - given: Jonathan
    family: Glidden
  - given: Eric
    family: Jonas
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 696-705
  id: obermeyer14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 696
  lastpage: 705
  published: 2014-04-02 00:00:00 +0000
- title: 'Fast Distribution To Real Regression'
  abstract: 'We study the problem of distribution to real regression, where one aims to regress a mapping f that takes in a distribution input covariate P∈\mathcalI (for a non-parametric family of distributions \mathcalI) and outputs a real-valued response Y=f(P) + ε.   This setting was recently studied in Pózcos et al. (2013), where the “Kernel-Kernel” estimator was introduced and shown to have a polynomial rate of convergence.   However, evaluating a new prediction with the Kernel-Kernel estimator scales as Ω(N). This causes the difficult situation where a large amount of data may be necessary for a low estimation risk, but the computation cost of estimation becomes infeasible when the data-set is too large. To this end, we propose the Double-Basis estimator, which looks to alleviate this big data problem in two ways: first, the Double-Basis estimator is shown to have a computation complexity that is independent of the number of of instances N when evaluating new predictions after training; secondly, the Double-Basis estimator is shown to have a fast rate of convergence for a general class of mappings f∈\mathcalF.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/oliva14a.html
  PDF: http://proceedings.mlr.press/v33/oliva14a.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-oliva14a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Junier
    family: Oliva
  - given: Willie
    family: Neiswanger
  - given: Barnabas
    family: Poczos
  - given: Jeff
    family: Schneider
  - given: Eric
    family: Xing
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 706-714
  id: oliva14a
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 706
  lastpage: 714
  published: 2014-04-02 00:00:00 +0000
- title: 'FuSSO: Functional Shrinkage and Selection Operator'
  abstract: 'We present the FuSSO, a functional analogue to the LASSO, that efficiently finds a sparse set of functional input covariates to regress a real-valued response against. The FuSSO does so in a semi-parametric fashion, making no parametric assumptions about the nature of input functional covariates and assuming a linear form to the mapping of functional covariates to the response. We provide a statistical backing for use of the FuSSO via proof of asymptotic sparsistency under various conditions. Furthermore, we observe good results on both synthetic and real-world data.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/oliva14b.html
  PDF: http://proceedings.mlr.press/v33/oliva14b.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-oliva14b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Junier
    family: Oliva
  - given: Barnabas
    family: Poczos
  - given: Timothy
    family: Verstynen
  - given: Aarti
    family: Singh
  - given: Jeff
    family: Schneider
  - given: Fang-Cheng
    family: Yeh
  - given: Wen-Yih
    family: Tseng
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 715-723
  id: oliva14b
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 715
  lastpage: 723
  published: 2014-04-02 00:00:00 +0000
- title: 'To go deep or wide in learning?'
  abstract: 'To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multi-layered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/pandey14.html
  PDF: http://proceedings.mlr.press/v33/pandey14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-pandey14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Gaurav
    family: Pandey
  - given: Ambedkar
    family: Dukkipati
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 724-732
  id: pandey14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 724
  lastpage: 732
  published: 2014-04-02 00:00:00 +0000
- title: 'LAMORE: A Stable, Scalable Approach to Latent Vector Autoregressive Modeling of Categorical Time Series'
  abstract: 'Latent vector autoregressive models for categorical time series have a wide range of potential applications from marketing research to healthcare analytics. However, a brute-force particle filter implementation of the Expectation-Maximization (EM) algorithm often fails to estimate the maximum likelihood parameters due to the Monte Carlo approximation of the E-step and multiple local optima of the log-likelihood function. This paper proposes two auxiliary techniques that help stabilize and calibrate the estimated parameters. These two techniques, namely \textitasymptotic mean regularization and \textitlow-resolution augmentation, do not require any additional parameter tuning, and can be implemented by modifying the brute-force EM algorithm. Experiments with simulated data show that the proposed techniques effectively stabilize the parameter estimation process. Also, experimental results using Medicare and MIMIC-II datasets illustrate various potential applications of the proposed model and methods.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/park14.html
  PDF: http://proceedings.mlr.press/v33/park14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-park14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Yubin
    family: Park
  - given: Carlos
    family: Carvalho
  - given: Joydeep
    family: Ghosh
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 733-742
  id: park14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 733
  lastpage: 742
  published: 2014-04-02 00:00:00 +0000
- title: 'Spoofing Large Probability Mass Functions to Improve Sampling Times and Reduce Memory Costs'
  abstract: 'Sampling from a probability mass function (PMF) has many applications in modern computing.  This paper presents a novel lossy compression method intended for large (O(10^5)) dense PMFs that speeds up the sampling process and guarantees high fidelity sampling.  This compression method closely approximates an input PMF P with another PMF Q that is easy to store and sample from.  All samples are drawn from Q as opposed to the original input distribution P.  We say that Q “spoofs” P while this switch is difficult to detect with a statistical test.  The lifetime of Q is the sample size required to detect the switch from P to Q.  We show how to compute a single PMF’s lifetime and present numeric examples demonstrating compression rates ranging from 62% to 75% when the input PMF is not sorted and 88% to 99% when the input is already sorted.  These examples have speed ups ranging from 1.47 to 2.82 compared to binary search sampling.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/parker14.html
  PDF: http://proceedings.mlr.press/v33/parker14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-parker14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Jon
    family: Parker
  - given: Hans
    family: Engler
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 743-750
  id: parker14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 743
  lastpage: 750
  published: 2014-04-02 00:00:00 +0000
- title: 'Learning Bounded Tree-width Bayesian Networks using Integer Linear Programming'
  abstract: 'In many applications one wants to compute conditional probabilities given a Bayesian network. This inference problem is NP-hard in general but becomes tractable when the network has low tree-width. Since the inference problem is common in many application areas, we provide a practical algorithm for learning bounded tree-width Bayesian networks. We cast this problem  as an integer linear program (ILP). The program can be solved by an anytime algorithm which provides upper bounds to assess the quality of the found solutions.  A key component of our program is a novel integer linear formulation for bounding tree-width of a graph. Our tests clearly indicate that our approach works in practice, as our implementation was able to find an optimal or nearly optimal network for most of the data sets.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/parviainen14.html
  PDF: http://proceedings.mlr.press/v33/parviainen14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-parviainen14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Pekka
    family: Parviainen
  - given: Hossein
    family: Shahrabi Farahani
  - given: Jens
    family: Lagergren
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 751-759
  id: parviainen14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 751
  lastpage: 759
  published: 2014-04-02 00:00:00 +0000
- title: 'An Efficient Algorithm for Large Scale Compressive Feature Learning'
  abstract: 'This paper focuses on large-scale unsupervised feature selection from text. We expand upon the recently proposed Compressive Feature Learning (CFL) framework, a method that uses dictionary-based compression to select a K-gram representation for a document corpus. We show that CFL is NP-Complete and provide a novel and efficient approximation algorithm based on a homotopy that transforms a convex relaxation of CFL into the original problem. Our algorithm allows CFL to scale to corpuses comprised of millions of documents because each step is linear in the corpus length and highly parallelizable. We use it to extract features from the BeerAdvocate dataset, a corpus of over 1.5 million beer reviews spanning 10 years. CFL uses two orders of magnitude fewer features than the full trigram space. It beats a standard unigram model in a number of prediction tasks and achieves nearly twice the accuracy on an author identification task.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/paskov14.html
  PDF: http://proceedings.mlr.press/v33/paskov14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-paskov14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Hristo
    family: Paskov
  - given: John
    family: Mitchell
  - given: Trevor
    family: Hastie
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 760-768
  id: paskov14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 760
  lastpage: 768
  published: 2014-04-02 00:00:00 +0000
- title: 'Expectation Propagation for Likelihoods Depending on an Inner Product of Two Multivariate Random Variables'
  abstract: 'We describe how a deterministic Gaussian posterior approximation can be constructed using expectation propagation (EP) for models, where the likelihood function depends on an inner product of two multivariate random variables. The family of applicable models includes a wide variety of important linear latent variable models used in statistical machine learning, such as principal component and factor analysis, their linear extensions, and errors-in-variables regression. The EP computations are facilitated by an integral transformation of the Dirac delta function, which allows transforming the multidimensional integrals over the two multivariate random variables into an analytically tractable form up to one-dimensional analytically intractable integrals that can be efficiently computed numerically. We study the resulting posterior approximations in sparse principal component analysis with Gaussian and probit likelihoods. Comparisons to Gibbs sampling and variational inference are presented.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/peltola14.html
  PDF: http://proceedings.mlr.press/v33/peltola14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-peltola14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Tomi
    family: Peltola
  - given: Pasi
    family: Jylänki
  - given: Aki
    family: Vehtari
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 769-777
  id: peltola14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 769
  lastpage: 777
  published: 2014-04-02 00:00:00 +0000
- title: 'An inclusion optimal algorithm for chain graph structure learning'
  abstract: 'This paper presents and proves an extension of Meek’s conjecture to chain graphs under the Lauritzen-Wermuth-Frydenberg interpretation. The proof of the conjecture leads to the development of a structure learning algorithm that finds an inclusion optimal chain graph for any given probability distribution satisfying the composition property. Finally, the new algorithm is experimentally evaluated.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/pena14.html
  PDF: http://proceedings.mlr.press/v33/pena14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-pena14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Jose
    family: Peña
  - given: Dag
    family: Sonntag
  - given: Jens
    family: Nielsen
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 778-786
  id: pena14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 778
  lastpage: 786
  published: 2014-04-02 00:00:00 +0000
- title: 'A Stepwise uncertainty reduction approach to constrained global optimization'
  abstract: 'Using statistical emulators to guide sequential evaluations of complex computer experiments is now  a well-established practice. When a model provides multiple outputs, a typical objective is to optimize one of the outputs   with constraints (for instance, a threshold not to exceed) on the values of the other outputs.   We propose here a new optimization strategy based on the stepwise uncertainty reduction paradigm,   which offers an efficient trade-off between exploration and local search near the boundaries.  The strategy is illustrated on numerical examples.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/picheny14.html
  PDF: http://proceedings.mlr.press/v33/picheny14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-picheny14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Victor
    family: Picheny
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 787-795
  id: picheny14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 787
  lastpage: 795
  published: 2014-04-02 00:00:00 +0000
- title: 'Connected Sub-graph Detection'
  abstract: 'We characterize the family of connected subgraphs in terms of linear matrix inequalities (LMI) with additional integrality constraints. We then show that convex relaxations of the integral LMI lead to parameterization of all weighted connected subgraphs. These developments allow for optimizing arbitrary graph functionals under connectivity constraints. For concreteness we consider the connected sub-graph detection problem that arises in a number of applications including network intrusion, disease outbreaks, and video surveillance. In these applications feature vectors are associated with nodes and edges of a graph. The problem is to decide whether or not the null hypothesis is true based on the measured features. For simplicity we consider the elevated mean problem wherein feature values at various nodes are distributed IID under the null hypothesis. The non-null (positive) hypothesis is distinguished from the null hypothesis by the fact that feature values on some unknown connected sub-graph has elevated mean.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/qian14.html
  PDF: http://proceedings.mlr.press/v33/qian14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-qian14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Jing
    family: Qian
  - given: Venkatesh
    family: Saligrama
  - given: Yuting
    family: Chen
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 796-804
  id: qian14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 796
  lastpage: 804
  published: 2014-04-02 00:00:00 +0000
- title: 'An Analysis of Active Learning with Uniform Feature Noise'
  abstract: 'In active learning, the user sequentially chooses values for feature X and an oracle returns the corresponding label Y. In this paper, we consider the effect of feature noise in active learning, which could arise either because X itself is being measured, or it is corrupted in transmission to the oracle, or the oracle returns the label of a noisy version of the query point. In statistics, feature noise is known as“errors in variables” and has been studied extensively in non-active settings. However, the effect of feature noise in active learning has not been studied before. We consider the well-known Berkson errors-in-variables model with additive uniform noise of width σ.    Our simple but revealing setting is that of one-dimensional binary classification setting where the goal is to learn a threshold (point where the probability of a + label crosses half). We deal with regression functions that are antisymmetric in a region of size σaround the threshold and also satisfy Tsybakov’s margin condition around the threshold. We prove minimax lower and upper bounds which demonstrate that when σis smaller than the minimiax active/passive noiseless error derived in Castro & Nowak (2007), then noise has no effect on the rates and one achieves the same noiseless rates. For larger σ, the \textitunflattening of the regression function on convolution with uniform noise, along with its local antisymmetry around the threshold, together yield a behaviour where noise \textitappears to be beneficial. Our key result is that active learning can buy significant improvement over a passive strategy even in the presence of feature noise.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/ramdas14.html
  PDF: http://proceedings.mlr.press/v33/ramdas14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-ramdas14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Aaditya
    family: Ramdas
  - given: Barnabas
    family: Poczos
  - given: Aarti
    family: Singh
  - given: Larry
    family: Wasserman
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 805-813
  id: ramdas14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 805
  lastpage: 813
  published: 2014-04-02 00:00:00 +0000
- title: 'Black Box Variational Inference'
  abstract: 'Variational inference has become a widely used method to approximate posteriors in complex latent variables models.  However, deriving a variational inference algorithm generally requires significant model-specific analysis. These efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand.  In this paper, we present a “black box” variational inference algorithm, one that can be quickly applied to many models with little additional derivation.  Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution.  We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations.  We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/ranganath14.html
  PDF: http://proceedings.mlr.press/v33/ranganath14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-ranganath14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Rajesh
    family: Ranganath
  - given: Sean
    family: Gerrish
  - given: David
    family: Blei
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 814-822
  id: ranganath14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 814
  lastpage: 822
  published: 2014-04-02 00:00:00 +0000
- title: 'Cluster Canonical Correlation Analysis'
  abstract: 'In this paper we present cluster canonical correlation analysis (cluster-CCA) for joint dimensionality reduction of two sets of data points.  Unlike the standard pairwise correspondence between the  data points, in our problem each set is partitioned into multiple clusters or classes, where the class labels define correspondences between the sets. Cluster-CCA is able to learn discriminant low dimensional representations that maximize the correlation between the two sets while segregating the different classes on the learned space.  Furthermore, we present a kernel extension, kernel cluster canonical correlation analysis (cluster-KCCA) that extends cluster-CCA to account for non-linear relationships.  Cluster-(K)CCA is shown to be computationally efficient, the complexity being similar to standard (K)CCA. By means of experimental evaluation on benchmark datasets, cluster-(K)CCA is shown to achieve state of the art performance for cross-modal retrieval tasks.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/rasiwasia14.html
  PDF: http://proceedings.mlr.press/v33/rasiwasia14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-rasiwasia14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Nikhil
    family: Rasiwasia
  - given: Dhruv
    family: Mahajan
  - given: Vijay
    family: Mahadevan
  - given: Gaurav
    family: Aggarwal
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 823-831
  id: rasiwasia14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 823
  lastpage: 831
  published: 2014-04-02 00:00:00 +0000
- title: 'Sequential crowdsourced labeling as an epsilon-greedy exploration in a Markov Decision Process'
  abstract: 'Crowdsourcing marketplaces are widely used for curating large annotated datasets by collecting labels from multiple annotators. In such scenarios one has to balance the tradeoff between the accuracy of the collected labels, the cost of acquiring these labels, and the time taken to finish the labeling task. With the goal of reducing the labeling cost, we introduce the notion of sequential crowdsourced labeling, where instead of asking for all the labels in one shot we acquire labels from annotators sequentially one at a time. We model it as an epsilon-greedy exploration in a Markov Decision Process with a Bayesian decision theoretic utility function that incorporates accuracy, cost and time. Experimental results confirm that the proposed sequential labeling procedure can achieve similar accuracy at roughly half the labeling cost and at any stage in the labeling process the algorithm achieves a higher accuracy compared to randomly asking for the next label.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/raykar14.html
  PDF: http://proceedings.mlr.press/v33/raykar14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-raykar14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Vikas
    family: Raykar
  - given: Priyanka
    family: Agrawal
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 832-840
  id: raykar14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 832
  lastpage: 840
  published: 2014-04-02 00:00:00 +0000
- title: 'Learning Structured Models with the AUC Loss and Its Generalizations'
  abstract: 'Many problems involve the prediction of multiple, possibly dependent labels. The structured output prediction framework builds predictors that take these dependencies into account and use them to improve accuracy. In many such tasks, performance is evaluated by the Area Under the ROC Curve (AUC). While a framework for optimizing the AUC loss for unstructured models exists, it does not naturally extend to structured models.    In this work, we propose a representation and learning formulation for optimizing structured models over the AUC loss, show how our approach generalizes the unstructured case, and provide algorithms for solving the resulting inference and learning problems. We also explore several new variants of the AUC measure which naturally arise from our formulation. Finally, we empirically show the utility of our approach in several domains.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/rosenfeld14.html
  PDF: http://proceedings.mlr.press/v33/rosenfeld14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-rosenfeld14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Nir
    family: Rosenfeld
  - given: Ofer
    family: Meshi
  - given: Danny
    family: Tarlow
  - given: Amir
    family: Globerson
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 841-849
  id: rosenfeld14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 841
  lastpage: 849
  published: 2014-04-02 00:00:00 +0000
- title: 'Class Proportion Estimation with Application to Multiclass Anomaly Rejection'
  abstract: 'This work addresses two classification problems that fall under the heading of domain adaptation, wherein the distributions of training and testing examples differ. The first problem studied is that of class proportion estimation, which is the problem of estimating the class proportions in an unlabeled testing data set given labeled examples of each class. Compared to previous work on this problem, our approach has the novel feature that it does not require labeled training data from one of the classes. This property allows us to address the second domain adaptation problem, namely, multiclass anomaly rejection. Here, the goal is to design a classifier that has the option of assigning a “reject” label, indicating that the instance did not arise from a class present in the training data. We establish consistent learning strategies for both of these domain adaptation problems, which to our knowledge are the first of their kind. We also implement the class proportion estimation technique and demonstrate its performance on several benchmark data sets.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/sanderson14.html
  PDF: http://proceedings.mlr.press/v33/sanderson14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-sanderson14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Tyler
    family: Sanderson
  - given: Clayton
    family: Scott
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 850-858
  id: sanderson14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 850
  lastpage: 858
  published: 2014-04-02 00:00:00 +0000
- title: 'Lifted MAP Inference for Markov Logic Networks'
  abstract: 'In this paper, we present a new approach for lifted MAP inference in Markov Logic Networks (MLNs). Our approach is based on the following key result that we prove in the paper: if an MLN has no shared terms then MAP inference over it can be reduced to MAP inference over a Markov network having the following properties: (i) the number of random variables in the Markov network is equal to the number of first-order atoms in the MLN; and (ii) the domain size of each variable in the Markov network is equal to the number of groundings of the corresponding first-order atom. We show that inference over this Markov network is exponentially more efficient than ground inference, namely inference over the Markov network obtained by grounding all first-order atoms in the MLN. We improve this result further by showing that if non-shared MLNs contain no self joins, namely every atom appears at most once in each of its formulas, then all variables in the corresponding Markov network need only be bi-valued.  Our approach is quite general and can be easily applied to an arbitrary MLN by simply grounding all of its shared terms. The key feature of our approach is that because we reduce lifted inference to propositional inference, we can use any propositional MAP inference algorithm for performing lifted MAP inference. Within our approach, we experimented with two propositional MAP inference algorithms: Gurobi and MaxWalkSAT. Our experiments on several benchmark MLNs clearly demonstrate our approach is superior to ground MAP inference in terms of scalability and solution quality.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/sarkhel14.html
  PDF: http://proceedings.mlr.press/v33/sarkhel14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-sarkhel14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Somdeb
    family: Sarkhel
  - given: Deepak
    family: Venugopal
  - given: Parag
    family: Singla
  - given: Vibhav
    family: Gogate
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 859-867
  id: sarkhel14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 859
  lastpage: 867
  published: 2014-04-02 00:00:00 +0000
- title: 'Estimating Dependency Structures for non-Gaussian Components with Linear and Energy Correlations'
  abstract: 'The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the ICA components. It would be very useful to estimate the dependency structure from data. However, most models have concentrated on higher-order correlations such as energy correlations, neglecting linear correlations. Linear correlations might be a strong and informative form of a dependency for some real data sets, but they are usually completely removed by ICA and related methods, and not analyzed at all. In this paper, we propose a probabilistic model of non-Gaussian components which are allowed to have both linear and energy correlations. The dependency structure of the components is explicitly parametrized by a parameter matrix, which defines an undirected graphical model over the latent components. Furthermore, the estimation of the parameter matrix is shown to be particularly simple because using score matching, the objective function is a quadratic form. Using artificial data, we demonstrate that the proposed method is able to estimate non-Gaussian components and their dependency structures, as it is designed to do. When applied to natural images and outputs of simulated complex cells in the primary visual cortex, novel dependencies between the estimated features are discovered.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/sasaki14.html
  PDF: http://proceedings.mlr.press/v33/sasaki14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-sasaki14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Hiroaki
    family: Sasaki
  - given: Michael
    family: Gutmann
  - given: Hayaru
    family: Shouno
  - given: Aapo
    family: Hyvarinen
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 868-876
  id: sasaki14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 868
  lastpage: 876
  published: 2014-04-02 00:00:00 +0000
- title: 'Student-t Processes as Alternatives to Gaussian Processes'
  abstract: 'We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model.  We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process – a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels – but has enhanced flexibility, and a predictive covariance that, unlike a Gaussian process, explicitly depends on the values of training observations.  We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/shah14.html
  PDF: http://proceedings.mlr.press/v33/shah14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-shah14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Amar
    family: Shah
  - given: Andrew
    family: Wilson
  - given: Zoubin
    family: Ghahramani
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 877-885
  id: shah14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 877
  lastpage: 885
  published: 2014-04-02 00:00:00 +0000
- title: 'In Defense of Minhash over Simhash'
  abstract: 'MinHash and SimHash are the two widely adopted Locality Sensitive Hashing (LSH) algorithms for large-scale data processing applications. Deciding which LSH to use for a particular problem at hand is an important question, which has no clear answer in the existing  literature. In this study, we provide a theoretical answer (validated by experiments) that MinHash virtually always outperforms  SimHash when the data are binary, as common in practice such as search.      The collision probability of MinHash is a function of  \em resemblance similarity (\mathcalR), while the collision probability of SimHash is a function of  \em cosine  similarity (\mathcalS). To provide a common basis for  comparison,  we evaluate  retrieval results in terms of \mathcalS for both MinHash and SimHash. This evaluation is valid  as we can prove that MinHash is  a valid LSH with respect to \mathcalS, by using a  general inequality  \mathcalS^2≤\mathcalR≤\frac\mathcalS2-\mathcalS. Our \textbfworst case  analysis  can show that MinHash significantly outperforms SimHash in  \textbfhigh similarity region.        Interestingly, our intensive experiments reveal that MinHash is also substantially better than SimHash even in datasets where most of the data points are not too similar to each other. This is partly because, in practical data, often \mathcalR≥\frac\mathcalSz-\mathcalS holds where z is only slightly larger than 2 (e.g., z≤2.1). Our \textbfrestricted worst case analysis by assuming \frac\mathcalSz-\mathcalS≤\mathcalR≤\frac\mathcalS2-\mathcalS shows that MinHash indeed significantly outperforms SimHash even in \textbflow similarity region.        We believe the results in this paper  will provide valuable guidelines for search in practice,  especially when the data are sparse.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/shrivastava14.html
  PDF: http://proceedings.mlr.press/v33/shrivastava14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-shrivastava14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Anshumali
    family: Shrivastava
  - given: Ping
    family: Li
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 886-894
  id: shrivastava14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 886
  lastpage: 894
  published: 2014-04-02 00:00:00 +0000
- title: 'Loopy Belief Propagation in the Presence of Determinism'
  abstract: 'It is well known that loopy Belief propagation (LBP) performs poorly on probabilistic graphical models (PGMs) with determinism. In this paper, we propose a new method for remedying this problem. The key idea in our method is finding a reparameterization of the graphical model such that LBP, when run on the reparameterization, is likely to have better convergence properties than LBP on the original graphical model. We propose several schemes for finding such reparameterizations, all of which leverage unique properties of zeros as well as research on LBP convergence done over the last decade. Our experimental evaluation on a variety of PGMs clearly demonstrates the promise of our method – it often yields accuracy and convergence time improvements of an order of magnitude or more over LBP.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/smith14.html
  PDF: http://proceedings.mlr.press/v33/smith14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-smith14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: David
    family: Smith
  - given: Vibhav
    family: Gogate
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 895-903
  id: smith14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 895
  lastpage: 903
  published: 2014-04-02 00:00:00 +0000
- title: 'Explicit Link Between Periodic Covariance Functions and State Space Models'
  abstract: 'This paper shows how periodic covariance functions in Gaussian process regression can be reformulated as state space models, which can be solved with classical Kalman filtering theory. This reduces the problematic cubic complexity of Gaussian process regression in the number of time steps into linear time complexity. The representation is based on expanding periodic covariance functions into a series of stochastic resonators. The explicit representation of the canonical periodic covariance function is written out and the expansion is shown to uniformly converge to the exact covariance function with a known convergence rate. The framework is generalized to quasi-periodic covariance functions by introducing damping terms in the system and applied to two sets of real data. The approach could be easily extended to non-stationary and spatio-temporal variants.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/solin14.html
  PDF: http://proceedings.mlr.press/v33/solin14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-solin14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Arno
    family: Solin
  - given: Simo
    family: Särkkä
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 904-912
  id: solin14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 904
  lastpage: 912
  published: 2014-04-02 00:00:00 +0000
- title: 'Bat Call Identification with Gaussian Process Multinomial Probit Regression and a Dynamic Time Warping Kernel'
  abstract: 'We study the problem of identifying bat species from echolocation calls in order to build automated bioacoustic monitoring algorithms. We employ the Dynamic Time Warping algorithm which has been successfully applied for bird flight calls identification and show that classification performance is superior to hand crafted call shape parameters used in previous research. This highlights that generic bioacoustic software with good classification rates can be constructed with little domain knowledge. We conduct a study with field data of 21 bat species from the north and central Mexico using a multinomial probit regression model with Gaussian process prior and a full EP approximation of the posterior of latent function values. Results indicate high classification accuracy across almost all classes while misclassification rate across families of species is low highlighting the common evolutionary path of echolocation in bats.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/stathopoulos14.html
  PDF: http://proceedings.mlr.press/v33/stathopoulos14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-stathopoulos14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Vassilios
    family: Stathopoulos
  - given: Veronica
    family: Zamora-Gutierrez
  - given: Kate
    family: Jones
  - given: Mark
    family: Girolami
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 913-921
  id: stathopoulos14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 913
  lastpage: 921
  published: 2014-04-02 00:00:00 +0000
- title: 'SMERED: A Bayesian Approach to Graphical Record Linkage and De-duplication'
  abstract: 'We propose a novel unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files.  Our key innovation is to represent the pattern of links between records as a bipartite graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records.  This flexible new representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate k-way posterior probabilities of matches across records, and propagate the uncertainty of record linkage into later analyses. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain  Monte Carlo algorithm, which overcomes many obstacles encountered by previously proposed methods of record linkage, despite the high dimensional parameter space. We assess our results on real and simulated data.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/steorts14.html
  PDF: http://proceedings.mlr.press/v33/steorts14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-steorts14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Rebecca
    family: Steorts
  - given: Rob
    family: Hall
  - given: Stephen
    family: Fienberg
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 922-930
  id: steorts14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 922
  lastpage: 930
  published: 2014-04-02 00:00:00 +0000
- title: ' Adaptive Variable Clustering in Gaussian Graphical Models'
  abstract: 'Gaussian graphical models (GGMs) are widely-used to describe the relationship between random variables. In many real-world applications, GGMs have a block structure in the sense that the variables can be clustered into groups so that inter-group correlation is much weaker than intra-group correlation. We present a novel nonparametric Bayesian generative model for such a block-structured GGM and an efficient inference algorithm to find the clustering of variables in this GGM by combining a Gibbs sampler and a split-merge Metropolis-Hastings algorithm. Experimental results show that our method performs well on both synthetic and real data. In particular, our method outperforms generic clustering algorithms and can automatically identify the true number of clusters.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/sun14.html
  PDF: http://proceedings.mlr.press/v33/sun14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-sun14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Siqi
    family: Sun
  - given: Yuancheng
    family: Zhu
  - given: Jinbo
    family: Xu
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 931-939
  id: sun14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 931
  lastpage: 939
  published: 2014-04-02 00:00:00 +0000
- title: 'Scaling Graph-based Semi Supervised Learning to Large Number of Labels Using Count-Min Sketch'
  abstract: 'Graph-based Semi-supervised learning (SSL) algorithms have been successfully used in a large number of applications. These methods classify initially unlabeled nodes by propagating label information over the structure of graph starting from seed nodes. Graph-based SSL algorithms usually scale linearly with the number of distinct labels (m), and require O(m) space on each node. Unfortunately, there exist many applications of practical significance with very large m over large graphs, demanding better space and time complexity. In this paper, we propose MAD-Sketch, a novel graph-based SSL algorithm which compactly stores label distribution on each node using Count-min Sketch, a randomized data structure. We present theoretical analysis showing that under mild conditions, MAD-Sketch can reduce space complexity at each node from O(m) to O(\log(m)), and achieve similar savings in time complexity as well. We support our analysis through experiments on multiple real world datasets. We observe that MAD-Sketch achieves similar performance as existing state-of-the-art graph-based SSL algorithms, while requiring smaller memory footprint and at the same time achieving up to 10x speedup. We find that MAD-Sketch is able to scale to datasets with one million labels, which is beyond the scope of existing graph-based SSL algorithms.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/talukdar14.html
  PDF: http://proceedings.mlr.press/v33/talukdar14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-talukdar14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Partha
    family: Talukdar
  - given: William
    family: Cohen
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 940-947
  id: talukdar14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 940
  lastpage: 947
  published: 2014-04-02 00:00:00 +0000
- title: 'Path Thresholding: Asymptotically Tuning-Free High-Dimensional Sparse Regression'
  abstract: 'In this paper, we address the challenging problem of selecting tuning parameters for high-dimensional sparse regression.  We propose a simple and computationally efficient method, called path thresholding PaTh, that transforms any tuning parameter-dependent sparse regression algorithm into an asymptotically tuning-free sparse regression algorithm.  More specifically, we prove that, as the problem size becomes large (in the number of variables and in the number of observations), PaTh performs accurate sparse regression, under appropriate conditions, without specifying a tuning parameter.  In finite-dimensional settings, we demonstrate that PaTh can alleviate the computational burden of model selection algorithms by significantly reducing the search space of tuning parameters.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/vats14a.html
  PDF: http://proceedings.mlr.press/v33/vats14a.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-vats14a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Divyanshu
    family: Vats
  - given: Richard
    family: Baraniuk
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 948-957
  id: vats14a
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 948
  lastpage: 957
  published: 2014-04-02 00:00:00 +0000
- title: 'Active Learning for Undirected Graphical Model Selection'
  abstract: 'This paper studies graphical model selection, i.e., the problem of estimating a graph of statistical relationships among a collection of random variables.  Conventional graphical model selection algorithms are passive, i.e., they require all the measurements to have been collected before processing begins.  We propose an active learning algorithm that uses junction tree representations to adapt future measurements based on the information gathered from prior measurements.  We prove that, under certain conditions, our active learning algorithm requires fewer scalar measurements than any passive algorithm to reliably estimate a graph.  A range of numerical results validate our theory and demonstrates the benefits of active learning.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/vats14b.html
  PDF: http://proceedings.mlr.press/v33/vats14b.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-vats14b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Divyanshu
    family: Vats
  - given: Robert
    family: Nowak
  - given: Richard
    family: Baraniuk
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 958-967
  id: vats14b
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 958
  lastpage: 967
  published: 2014-04-02 00:00:00 +0000
- title: 'Linear-time training of nonlinear low-dimensional embeddings'
  abstract: 'Nonlinear embeddings such as stochastic neighbor embedding or the elastic embedding achieve better results than spectral methods but require an expensive, nonconvex optimization, where the objective function and gradient are quadratic on the sample size. We address this bottleneck by formulating the optimization as an N-body problem and using fast multipole methods (FMMs) to approximate the gradient in linear time. We study the effect, in theory and experiment, of approximating gradients in the optimization and show that the expected error is related to the mean curvature of the objective function, and that gradually increasing the accuracy level in the FMM over iterations leads to a faster training. When combined with standard optimizers, such as gradient descent or L-BFGS, the resulting algorithm beats the \mathcalO(N \log N) Barnes-Hut method and achieves reasonable embeddings for one million points in around three hours’ runtime.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/vladymyrov14.html
  PDF: http://proceedings.mlr.press/v33/vladymyrov14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-vladymyrov14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Max
    family: Vladymyrov
  - given: Miguel
    family: Carreira-Perpinan
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 968-977
  id: vladymyrov14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 968
  lastpage: 977
  published: 2014-04-02 00:00:00 +0000
- title: 'Gaussian Copula Precision Estimation with Missing Values'
  abstract: 'We consider the problem of estimating sparse precision matrix of Gaussian copula distributions using samples with missing values in high dimensions.  Existing approaches, primarily designed for Gaussian distributions, suggest using plugin estimators by disregarding the missing values.  In this paper, we propose double plugin Gaussian (DoPinG) copula estimators to estimate the sparse precision matrix corresponding to \emphnon-paranormal distributions.  DoPinG uses two plugin procedures and consists of three steps: (1) estimate nonparametric correlations based on observed values,  including Kendall’s tau and Spearman’s rho; (2) estimate the non-paranormal correlation matrix; (3) plug into existing sparse precision estimators.   We prove that DoPinG copula estimators consistently estimate the non-paranormal correlation matrix at a rate of O(\frac1(1-δ)\sqrt\frac\log pn), where δis the probability of missing values.   We provide experimental results to illustrate the effect of sample size and percentage of missing data on the model performance. Experimental results show that DoPinG is significantly better than estimators like mGlasso, which are primarily designed for Gaussian data.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/wang14a.html
  PDF: http://proceedings.mlr.press/v33/wang14a.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-wang14a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Huahua
    family: Wang
  - given: Farideh
    family: Fazayeli
  - given: Soumyadeep
    family: Chatterjee
  - given: Arindam
    family: Banerjee
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 978-986
  id: wang14a
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 978
  lastpage: 986
  published: 2014-04-02 00:00:00 +0000
- title: 'An LP for Sequential Learning Under Budgets'
  abstract: 'We present a convex framework to learn sequential decisions and apply this to the problem of learning under a budget. We consider the structure proposed [1], where sensor measurements are acquired in a sequence. The goal after acquiring each new measurement is to make a decision whether to stop and classify or to pay the cost of using the next sensor in the sequence. We introduce a novel formulation of an empirical risk objective for the multi stage sequential decision problem. This objective naturally lends itself to a non-convex multilinear formulation. Nevertheless, we derive a novel perspective that leads to a tight convex objective. This is accomplished by expressing the empirical risk in terms of linear superposition of indicator functions. We then derive an LP formulation by utilizing hinge loss surrogates. Our LP achieves or exceeds the empirical performance as the non-convex alternating algorithm that requires a large number of random initializations. Consequently, the LP has the advantage of guaranteed convergence, global optimality, repeatability and computation efficiency.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/wang14b.html
  PDF: http://proceedings.mlr.press/v33/wang14b.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-wang14b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Joseph
    family: Wang
  - given: Kirill
    family: Trapeznikov
  - given: Venkatesh
    family: Saligrama
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 987-995
  id: wang14b
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 987
  lastpage: 995
  published: 2014-04-02 00:00:00 +0000
- title: 'Efficient Algorithms and Error Analysis for the Modified Nystrom Method'
  abstract: 'Many kernel methods suffer from high time and space complexities and are thus prohibitive in big-data applications. To tackle the computational challenge, the Nyström method has been extensively used to reduce time and space complexities by sacrificing some accuracy. The Nyström method speedups computation by constructing an approximation of the kernel matrix using only a few  columns of the matrix. Recently, a variant of the Nyström method called the modified Nyström method has demonstrated significant improvement over the standard Nyström method in approximation accuracy, both theoretically and empirically.      In this paper, we propose two algorithms that make the modified Nyström method practical. First, we devise a  simple column selection algorithm with a provable error bound. Our algorithm is more  efficient and easier to implement than and nearly as accurate as the state-of-the-art algorithm. Second, with the selected columns at hand, we propose an algorithm that computes the approximation in lower time complexity than the approach in the previous work. Furthermore, we prove that the modified Nyström method is exact under certain conditions, and we establish a lower error bound for the modified Nyström method.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/wang14c.html
  PDF: http://proceedings.mlr.press/v33/wang14c.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-wang14c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Shusen
    family: Wang
  - given: Zhihua
    family: Zhang
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 996-1004
  id: wang14c
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 996
  lastpage: 1004
  published: 2014-04-02 00:00:00 +0000
- title: 'Bayesian Multi-Scale Optimistic Optimization'
  abstract: 'Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks, as well as a novel application to automate information extraction, demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/wang14d.html
  PDF: http://proceedings.mlr.press/v33/wang14d.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-wang14d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Ziyu
    family: Wang
  - given: Babak
    family: Shakibi
  - given: Lin
    family: Jin
  - given: Nando
    family: Freitas
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1005-1014
  id: wang14d
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1005
  lastpage: 1014
  published: 2014-04-02 00:00:00 +0000
- title: 'Accelerating ABC methods using Gaussian processes'
  abstract: 'Approximate Bayesian computation (ABC)  methods are used to approximate  posterior distributions  using simulation rather than likelihood calculations. We introduce Gaussian process (GP) accelerated ABC, which we show can significantly reduce the number of simulations required. As  computational resource is usually the main determinant of accuracy in ABC, GP-accelerated methods can thus enable  more accurate inference in some models.  GP models of the unknown log-likelihood function are  used to exploit continuity and smoothness,  reducing the required computation. We use a sequence of models  that increase in accuracy, using intermediate models to rule out regions of the parameter space as implausible. The methods will not be suitable for all problems, but when they can be used, can result in significant computational savings. For the Ricker model, we are able to achieve accurate approximations to the posterior distribution using a factor of 100 fewer simulator evaluations than comparable Monte Carlo approaches, and for a population genetics model we are able to approximate the exact posterior for the first time.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/wilkinson14.html
  PDF: http://proceedings.mlr.press/v33/wilkinson14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-wilkinson14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Richard
    family: Wilkinson
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1015-1023
  id: wilkinson14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1015
  lastpage: 1023
  published: 2014-04-02 00:00:00 +0000
- title: 'A New Approach to Probabilistic Programming Inference'
  abstract: 'We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is easy to implement and to parallelize, applies to Turing-complete probabilistic programming languages, and supports accurate inference in models that make use of complex control flow, including stochastic recursion, as well as primitives from nonparametric Bayesian statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings samplers.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/wood14.html
  PDF: http://proceedings.mlr.press/v33/wood14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-wood14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Frank
    family: Wood
  - given: Jan Willem
    family: Meent
  - given: Vikash
    family: Mansinghka
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1024-1032
  id: wood14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1024
  lastpage: 1032
  published: 2014-04-02 00:00:00 +0000
- title: 'Dynamic Resource Allocation for Optimizing Population Diffusion'
  abstract: 'This paper addresses adaptive conservation planning, where the objective is to maximize the population spread of a species by allocating limited resources over time to conserve land parcels. This problem is characterized by having highly stochastic exogenous events (population spread), a large action branching factor (number of allocation options) and state space, and the need to reason about numeric resources. Together these characteristics render most existing AI planning techniques ineffective. The main contribution of this paper is to design and evaluate an online planner for this problem based on Hindsight Optimization (HOP), a technique that has shown promise in other stochastic planning problems. Unfortunately, standard implementations of HOP scale linearly with the number of actions in a domain, which is not feasible for conservation problems such as ours. Thus, we develop a new approach for computing HOP policies based on mixed-integer programming and dual decomposition. Our experiments on synthetic and real-world scenarios show that this approach is effective and scalable compared to existing alternatives.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/xue14.html
  PDF: http://proceedings.mlr.press/v33/xue14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-xue14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Shan
    family: Xue
  - given: Alan
    family: Fern
  - given: Daniel
    family: Sheldon
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1033-1041
  id: xue14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1033
  lastpage: 1041
  published: 2014-04-02 00:00:00 +0000
- title: 'Mixed Graphical Models via Exponential Families'
  abstract: 'Markov Random Fields, or undirected graphical models are widely used to model high-dimensional multivariate data. Classical instances of these models, such as Gaussian Graphical and Ising Models, as well as recent extensions to graphical models specified by univariate exponential families, assume all variables arise from the same distribution. Complex data from high-throughput genomics and social networking for example, often contain discrete, count, and continuous variables measured on the same set of samples. To model such heterogeneous data, we develop a \emphnovel class of mixed graphical models by specifying that each node-conditional distribution is a member of a possibly different univariate exponential family. We study several instances of our model, and propose scalable M-estimators for recovering the underlying network structure. Simulations as well as an application to learning mixed genomic networks from next generation sequencing and mutation data demonstrate the versatility of our methods.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/yang14a.html
  PDF: http://proceedings.mlr.press/v33/yang14a.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-yang14a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Eunho
    family: Yang
  - given: Yulia
    family: Baker
  - given: Pradeep
    family: Ravikumar
  - given: Genevera
    family: Allen
  - given: Zhandong
    family: Liu
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1042-1050
  id: yang14a
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1042
  lastpage: 1050
  published: 2014-04-02 00:00:00 +0000
- title: 'Context Aware Group Nearest Shrunken Centroids in Large-Scale Genomic Studies'
  abstract: 'Recent genomic studies have identified genes related to specific phenotypes.  In addition to marginal association analysis for individual genes, analyzing gene pathways (functionally related sets of genes) may yield additional valuable insights.  We have devised an approach to phenotype classification from gene expression profiling.  Our method named “group Nearest Shrunken Centroids (gNSC)” is an enhancement of the Nearest Shrunken Centroids (NSC) which is a popular and scalable method to analyze big data. While fully utilizing the variable structure of gene pathways, gNSC shares comparable computational speed as NSC if the group size is small.  Comparing with NSC, gNSC improves the power of classification by utilizing the gene pathway information. In practice, we investigate the performance of gNSC on one of the largest microarray datasets aggregated from the internet. We show the effectiveness of our method by comparing the misclassification rate of gNSC with that of NSC.  Additionally, we present a novel application of NSC/gNSC on context analysis of association between pathways  and certain medical words.  Some newest biological findings are rediscovered.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/yang14b.html
  PDF: http://proceedings.mlr.press/v33/yang14b.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-yang14b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Juemin
    family: Yang
  - given: Fang
    family: Han
  - given: Rafael
    family: Irizarry
  - given: Han
    family: Liu
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1051-1059
  id: yang14b
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1051
  lastpage: 1059
  published: 2014-04-02 00:00:00 +0000
- title: 'Nonparametric estimation and testing of exchangeable graph models'
  abstract: 'Exchangeable graph models (ExGM) are a nonparametric approach to modeling network data that subsumes a number of popular models. The key object that defines an ExGM is often referred to as a graphon, or graph kernel. Here, we make three contributions to advance the theory of estimation of graphons. We determine conditions under which a unique canonical representation for a graphon exists and it is identifiable. We propose a 3-step procedure to estimate the canonical graphon of any ExGM that satisfies these conditions. We then focus on a specific estimator, built using the proposed 3-step procedure, which combines probability matrix estimation by Universal Singular Value Thresholding (USVT) and empirical degree sorting of the observed adjacency matrix. We prove that this estimator is consistent. We illustrate how the proposed theory and methods can be used to develop hypothesis testing procedures for models of network data.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/yang14c.html
  PDF: http://proceedings.mlr.press/v33/yang14c.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-yang14c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Justin
    family: Yang
  - given: Christina
    family: Han
  - given: Edoardo
    family: Airoldi
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1060-1067
  id: yang14c
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1060
  lastpage: 1067
  published: 2014-04-02 00:00:00 +0000
- title: 'Generating Efficient MCMC Kernels from Probabilistic Programs'
  abstract: 'Universal probabilistic programming languages (such as Church) trade performance for abstraction: any model can be represented compactly as an arbitrary stochastic computation, but costly online analyses are required for inference. We present a technique that recovers hand-coded levels of performance from a universal probabilistic language, for the Metropolis-Hastings (MH) MCMC inference algorithm. It takes a Church program as input and traces its execution to remove computation overhead. It then analyzes the trace for each proposal, using slicing, to identify the minimal computation needed to evaluate the MH acceptance probability. Generated incremental code is much faster than a baseline implementation (up to 600x) and usually as fast as hand-coded MH kernels.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/yang14d.html
  PDF: http://proceedings.mlr.press/v33/yang14d.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-yang14d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Lingfeng
    family: Yang
  - given: Patrick
    family: Hanrahan
  - given: Noah
    family: Goodman
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1068-1076
  id: yang14d
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1068
  lastpage: 1076
  published: 2014-04-02 00:00:00 +0000
- title: 'Efficient Transfer Learning Method for Automatic Hyperparameter Tuning'
  abstract: 'We propose a fast and effective algorithm for automatic hyperparameter tuning that can generalize across datasets. Our method is an instance of sequential model-based optimization (SMBO) that transfers information by constructing a common response surface for all datasets, similar to Bardenet et al. (2013). The time complexity of reconstructing the response surface at every SMBO iteration in our method is linear in the number of trials (significantly less than previous work with comparable performance), allowing the method to realistically scale to many more datasets. Specifically, we use deviations from the per-dataset mean as the response values. We empirically show the superiority of our method on a large number of synthetic and real-world datasets for tuning hyperparameters of logistic regression and ensembles of classifiers.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/yogatama14.html
  PDF: http://proceedings.mlr.press/v33/yogatama14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-yogatama14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Dani
    family: Yogatama
  - given: Gideon
    family: Mann
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1077-1085
  id: yogatama14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1077
  lastpage: 1085
  published: 2014-04-02 00:00:00 +0000
- title: 'Accelerated Stochastic Gradient Method for Composite Regularization'
  abstract: 'Regularized risk minimization often involves nonsmooth optimization. This can be particularly challenging when the regularizer is a sum of simpler regularizers, as in the overlapping group lasso. Very recently, this is alleviated by using the proximal average, in which an implicitly nonsmooth function is employed to approximate the composite regularizer. In this paper, we propose a novel extension with accelerated gradient method for stochastic optimization. On both general convex and strongly convex problems, the resultant approximation errors reduce at a faster rate than methods based on stochastic smoothing and ADMM. This is also verified experimentally on a number of synthetic and real-world data sets.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/zhong14.html
  PDF: http://proceedings.mlr.press/v33/zhong14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-zhong14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Wenliang
    family: Zhong
  - given: James
    family: Kwok
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1086-1094
  id: zhong14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1086
  lastpage: 1094
  published: 2014-04-02 00:00:00 +0000
- title: 'Heterogeneous Domain Adaptation for Multiple Classes'
  abstract: 'In this paper, we present an efficient Multi-class  Heterogeneous Domain Adaptation (HDA)  method, where data from the source and target domains are  represented by heterogeneous features with different dimensions.  Specifically, we propose to reconstruct a sparse feature  transformation matrix to map the features of multiple classes from  the source domain to the target domain. We  cast this  learning task as a compressed sensing problem, where each classifier  can be deemed as a measurement sensor. Based on compressive  sensing theory, the estimation error of the transformation matrix  decreases with the increasing number of classifiers. Therefore, to  guarantee the reconstruction performance, we construct  sufficiently many binary classifiers based on the error correcting output  correcting. Extensive experiments are conducted on  both toy data and three real-world HDA applications to verify the  superiority of our proposed method over existing state-of-the-art  HDA methods in terms of prediction accuracy.'
  volume: 33
  URL: https://proceedings.mlr.press/v33/zhou14.html
  PDF: http://proceedings.mlr.press/v33/zhou14.pdf
  edit: https://github.com/mlresearch//v33/edit/gh-pages/_posts/2014-04-02-zhou14.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics'
  publisher: 'PMLR'
  author: 
  - given: Joey Tianyi
    family: Zhou
  - given: Ivor
    family: W.Tsang
  - given: Sinno Jialin
    family: Pan
  - given: Mingkui
    family: Tan
  editor: 
  - given: Samuel
    family: Kaski
  - given: Jukka
    family: Corander
  address: Reykjavik, Iceland
  page: 1095-1103
  id: zhou14
  issued:
    date-parts: 
      - 2014
      - 4
      - 2
  firstpage: 1095
  lastpage: 1103
  published: 2014-04-02 00:00:00 +0000
