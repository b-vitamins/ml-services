
- title: 'Asian Conference on Machine Learning: Preface'
  abstract: 'Preface to ACML 2023.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/yanikoglu24a.html
  PDF: https://proceedings.mlr.press/v222/yanikoglu24a/yanikoglu24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-yanikoglu24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: i-ii
  id: yanikoglu24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: i
  lastpage: ii
  published: 2024-02-27 00:00:00 +0000
- title: 'Mitigating Bias: Enhancing Image Classification by Improving Model Explanations'
  abstract: 'Deep learning models have demonstrated remarkable capabilities in learning complex patterns and concepts from training data. However, recent findings indicate that these models tend to rely heavily on simple and easily discernible features present in the background of images rather than the main concepts or objects they are intended to classify. This phenomenon poses a challenge to image classifiers as the crucial elements of interest in images may be overshadowed. In this paper, we propose a novel approach to address this issue and improve the learning of main concepts by image classifiers. Our central idea revolves around concurrently guiding the model’s attention toward the foreground during the classification task. By emphasizing the foreground, which encapsulates the primary objects of interest, we aim to shift the focus of the model away from the dominant influence of the background. To accomplish this, we introduce a mechanism that encourages the model to allocate sufficient attention to the foreground. We investigate various strategies, including modifying the loss function or incorporating additional architectural components, to enable the classifier to effectively capture the primary concept within an image. Additionally, we explore the impact of different foreground attention mechanisms on model performance and provide insights into their effectiveness. Through extensive experimentation on benchmark datasets, we demonstrate the efficacy of our proposed approach in improving the classification accuracy of image classifiers. Our findings highlight the importance of foreground attention in enhancing model understanding and representation of the main concepts within images. The results of this study contribute to advancing the field of image classification and provide valuable insights for developing more robust and accurate deep-learning models.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ahmadi24a.html
  PDF: https://proceedings.mlr.press/v222/ahmadi24a/ahmadi24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ahmadi24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Raha
    family: Ahmadi
  - given: Mohammad Javad
    family: Rajabi
  - given: Mohammad
    family: Khalooie
  - given: Mohammad
    family: Sabokrou
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1-14
  id: ahmadi24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1
  lastpage: 14
  published: 2024-02-27 00:00:00 +0000
- title: 'Degree-based stratification of nodes in Graph Neural Networks'
  abstract: 'Despite much research, Graph Neural Networks (GNNs) still do not display the favorable scaling properties of other deep neural networks such as Convolutional Neural Networks and Transformers. Previous work has identified issues such as oversmoothing of the latent representation and have suggested solutions such as skip connections and sophisticated normalization schemes. Here, we propose a different approach that is based on a stratification of the graph nodes. We provide motivation that the nodes in a graph can be stratified into those with a low degree and those with a high degree and that the two groups are likely to behave differently. Based on this motivation, we modify the Graph Neural Network (GNN) architecture so that the weight matrices are learned, separately, for the nodes in each group. This simple-to-implement modification seems to improve performance across datasets and GNN methods. To verify that this increase in performance is not only due to the added capacity, we also perform the same modification for random splits of the nodes, which does not lead to any improvement.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ali24a.html
  PDF: https://proceedings.mlr.press/v222/ali24a/ali24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ali24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Ameen
    family: Ali
  - given: Lior
    family: Wolf
  - given: Hakan
    family: Cevikalp
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 15-27
  id: ali24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 15
  lastpage: 27
  published: 2024-02-27 00:00:00 +0000
- title: 'Variance Reduced Online Gradient Descent for Kernelized Pairwise Learning with Limited Memory'
  abstract: 'Pairwise learning is essential in machine learning, especially for problems involving loss functions defined on pairs of training examples. Online gradient descent (OGD) algorithms have been proposed to handle online pairwise learning, where data arrives sequentially. However, the pairwise nature of the problem makes scalability challenging, as the gradient computation for a new sample involves all past samples. Recent advancements in OGD algorithms have aimed to reduce the complexity of calculating online gradients, achieving complexities less than $O(T)$ and even as low as $O(1)$. However, these approaches are primarily limited to linear models and have induced variance. In this study, we propose a limited memory OGD algorithm that extends to kernel online pairwise learning while improving the sublinear regret. Specifically, we establish a clear connection between the variance of online gradients and the regret, and construct online gradients using the most recent stratified samples with a limited buffer of size of $s$ representing all past data, which have a complexity of $O(sT)$ and employs $O(\sqrt{T}\log{T})$ random Fourier features for kernel approximation. Importantly, our theoretical results demonstrate that the variance-reduced online gradients lead to an improved sublinear regret bound. The experiments on real-world datasets demonstrate the superiority of our algorithm over both kernelized and linear online pairwise learning algorithms.The code is available at https://github.com/halquabeh/ACML-2023-FPOGD-Code.git.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/alquabeh24a.html
  PDF: https://proceedings.mlr.press/v222/alquabeh24a/alquabeh24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-alquabeh24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Hilal
    family: AlQuabeh
  - given: Bhaskar
    family: Mukhoty
  - given: Bin
    family: Gu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 28-43
  id: alquabeh24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 28
  lastpage: 43
  published: 2024-02-27 00:00:00 +0000
- title: 'Ted: Tree delineation with reduced dimensions using entropy and deep learning'
  abstract: 'Trees play a very important role in maintaining the ecosystem. To automate the process of extracting trees from Remote Sensing data and counting thereby, we have used Cartosat-2S merged products @ 0.6m as input. Vegetation indices such as NDVI, and EVI can retrieve the Vegetation class which contains trees and their look-alikes (such as grass, fields, and shrubs) having the same spectral signatures \em{i.e.}, high reflectance in NIR band, and high absorption in Red band. Extraction of only Trees is not possible with these indices. In this paper we present a novel method for tree delineation from its look-alikes using AIML, concentrating more on preparation of input datasets efficiently. Based on the Spectral Separability analysis, only Red and NIR bands from the satellite imagery are utilized in the proposed method to separate Vegetation from the background classes (such as water, bare soil, terrain, and built-up). In addition to the two bands from satellite imagery entropy layer is computed from the NIR band and utilized as the third band to delineate trees from their look-alikes. Deep Neural Networks have the capability of learning complex patterns that can separate trees and their look-alikes. However, the performance is boosted when the entropy layer is added to the input image. The proposed method showed better performance when utilizing 3 bands Red, NIR, and Entropy bands compared to 4 bands i.e. Red, Green, Blue, and NIR bands. The proposed method obtains a precision of 96%, a recall of 90%, and an F1-score of 93% even with a relatively smaller training dataset. The study is performed on the data collected from various locations of the Indian state Rajasthan.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/anjani24a.html
  PDF: https://proceedings.mlr.press/v222/anjani24a/anjani24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-anjani24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: RN
    family: Anjani
  - given: CH
    family: Sarvani
  - given: K
    family: Kalyan Deep
  - given: P
    family: Aravinda Kumar
  - given: Sitiraju
    family: Srinivasa Rao
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 44-57
  id: anjani24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 44
  lastpage: 57
  published: 2024-02-27 00:00:00 +0000
- title: 'A Pragmatic Look at Deep Imitation Learning'
  abstract: 'The introduction of the generative adversarial imitation learning (GAIL) algorithm has spurred the development of scalable imitation learning approaches using deep neural networks. Many of the algorithms that followed used a similar procedure, combining on-policy actor-critic algorithms with inverse reinforcement learning. More recently there have been an even larger breadth of approaches, most of which use off-policy algorithms. However, with the breadth of algorithms, everything from datasets to base reinforcement learning algorithms to evaluation settings can vary, making it difficult to fairly compare them. In this work we re-implement 6 different IL algorithms, updating 3 of them to be off-policy, base them on a common off-policy algorithm (SAC), and evaluate them on a widely-used expert trajectory dataset (D4RL) for the most common benchmark (MuJoCo). After giving all algorithms the same hyperparameter optimisation budget, we compare their results for a range of expert trajectories. In summary, GAIL, with all of its improvements, consistently performs well across a range of sample sizes, AdRIL is a simple contender that performs well with one important hyperparameter to tune, and behavioural cloning remains a strong baseline when data is more plentiful.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/arulkumaran24a.html
  PDF: https://proceedings.mlr.press/v222/arulkumaran24a/arulkumaran24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-arulkumaran24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Kai
    family: Arulkumaran
  - given: Dan
    family: Ogawa Lillrank
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 58-73
  id: arulkumaran24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 58
  lastpage: 73
  published: 2024-02-27 00:00:00 +0000
- title: 'Equilibrium Point Learning'
  abstract: 'We present a novel approach, Equilibrium Point Learning (EPL), for training the deep equilibrium model (DEQ). In this method, the equilibrium point of the DEQ serves as the learnable parameters. Notably, the DEQ parameters encapsulate the learning algorithm itself and remain fixed. Consequently, by exploring the parameter space, we can discover a more efficient learning algorithm without relying on conventional techniques such as backpropagation or Q-learning. In this paper, we adopt an evolutionary approach inspired by biological neurons to evolve the DEQ model parameters. Initially, we examine the physical dynamics of neurons at the molecular level and translate them into a dynamical system representation. Subsequently, we formulate a deep implicit layer that is mathematically proven to possess an equilibrium point. The energy function of the implicit layer is defined using a quadratic form augmented with entropy and momentum terms. Given the resemblance between the dynamics of the deep implicit layer and the principles of physics and chemistry, it can effectively capture the biomodel of systems biology and the neural model of spiking neural networks (SNNs). This equivalence enables us to define the implicit layer of the DEQ, allowing for seamless integration with existing artificial neural networks (ANNs). Finally, we employ HyperNEAT to evolve the parameters of the dynamical system. Through our experiments, we observe a consistent improvement in learning efficiency, with each successive generation exhibiting a 0.2% increase in learning speed per generation.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/baik24a.html
  PDF: https://proceedings.mlr.press/v222/baik24a/baik24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-baik24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Dowoo
    family: Baik
  - given: Ji Won
    family: Yoon
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 74-89
  id: baik24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 74
  lastpage: 89
  published: 2024-02-27 00:00:00 +0000
- title: 'Show Me How It’s Done: The Role of Explanations in Fine-Tuning Language Models'
  abstract: 'Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model’s parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model. In essence, our findings suggest that fine-tuning with explanations significantly bolsters the performance of large language models.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ballout24a.html
  PDF: https://proceedings.mlr.press/v222/ballout24a/ballout24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ballout24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Mohamad
    family: Ballout
  - given: Ulf
    family: Krumnack
  - given: Gunther
    family: Heidemann
  - given: Kai-Uwe
    family: Kühnberger
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 90-105
  id: ballout24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 90
  lastpage: 105
  published: 2024-02-27 00:00:00 +0000
- title: 'ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning'
  abstract: 'Diffusion models are generative models that have shown significant advantages compared to other generative models in terms of higher generation quality and more stable training. However, the computational need for training diffusion models is considerably increased. In this work, we incorporate prototype learning into diffusion models to achieve high generation quality faster than the original diffusion model. Instead of randomly initialized class embeddings, we use separately learned class prototypes as the conditioning information to guide the diffusion process. We observe that our method, called ProtoDiffusion, achieves better performance in the early stages of training compared to the baseline method, signifying that using the learned prototypes shortens the training time. We demonstrate the performance of ProtoDiffusion using various datasets and experimental settings, achieving the best performance in shorter times across all settings.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/baykal24a.html
  PDF: https://proceedings.mlr.press/v222/baykal24a/baykal24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-baykal24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Gulcin
    family: Baykal
  - given: Halil Faruk
    family: Karagoz
  - given: Taha
    family: Binhuraib
  - given: Gozde
    family: Unal
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 106-120
  id: baykal24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 106
  lastpage: 120
  published: 2024-02-27 00:00:00 +0000
- title: 'Deep Reinforcement Learning for Two-sided Online Bipartite Matching in Collaborative Order Picking'
  abstract: 'As a growing number of warehouse operators are moving from human-only to Collaborative human-robot Order Picking solutions, more efficient picker routing policies are needed, since the complexity of coordinating multiple actors in the system increases significantly. The objective of these policies is to match human pickers and robot carriers to fulfill picking tasks, optimizing pick-rate and total tardiness of the orders. In this paper, we propose to formulate the order picking routing problem as a more general combinatorial optimization problem known as Two-sided Online Bipartite Matching. We present an end-to-end Deep Reinforcement Learning approach to optimize a combination of pick-rate and order tardiness, and to deal with the uncertainty of real-world warehouse environments. To extract and exploit spatial information from the environment, we devise three different Graph Neural Network architectures and empirically evaluate them on several scenarios of growing complexity in a simulation environment we developed. We show that all proposed methods significantly outperform greedy and more sophisticated heuristics, as well as non-GNN-based DRL approaches. Moreover, our methods exhibit good transferability properties, even when scaling up test problem instances to more than forty times the size of the ones the models were trained on. Code is available at: \url{https://github.com/lbegnardi/DRL-TOBM-CPR}.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/begnardi24a.html
  PDF: https://proceedings.mlr.press/v222/begnardi24a/begnardi24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-begnardi24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Luca
    family: Begnardi
  - given: Hendrik
    family: Baier
  - given: Willem
    prefix: van
    family: Jaarsveld
  - given: Yingqian
    family: Zhang
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 121-136
  id: begnardi24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 121
  lastpage: 136
  published: 2024-02-27 00:00:00 +0000
- title: 'ASAP: Attention-Based State Space Abstraction for Policy Summarization'
  abstract: 'Deep reinforcement learning (RL) has shown remarkable performance, but end-users do not understand how the system solves tasks due to the black-box nature of neural networks. Many methods from explainable machine learning have been adapted to RL. However, they do not focus on the unique challenges of explaining actions’ short-term and long-term consequences. This work introduces a new perspective to understanding RL policies by clustering states into abstract states utilizing attention maps, giving a bird’s-eye view of the policy’s behavior. We learn the attention maps iteratively together with the clustering of states by masking the input features to estimate their importance. In contrast to previous works that have uninterpretable abstract states and/or clustering objectives using state values that are non-human intuitive, we only leverage attention maps in the clustering. The policy only indirectly affects the clustering via attention maps. This allows us to give global explanations from the view of feature attention, a quantity a human can relate to given interpretable features. The experiments demonstrate that our method provides faithful abstractions by capturing state semantics, policy behavior, and feature attention. Furthermore, we show that our attention maps can mask state features without affecting policy performance.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/bekkemoen24a.html
  PDF: https://proceedings.mlr.press/v222/bekkemoen24a/bekkemoen24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-bekkemoen24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Yanzhe
    family: Bekkemoen
  - given: Helge
    family: Langseth
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 137-152
  id: bekkemoen24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 137
  lastpage: 152
  published: 2024-02-27 00:00:00 +0000
- title: 'Federated Learning with Uncertainty via Distilled Predictive Distributions'
  abstract: 'Most existing federated learning methods are unable to estimate model/predictive uncertainty since the client models are trained using the standard loss function minimization approach which ignores such uncertainties. In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the model parameters at each client as it leads to more accurate predictions and also because reliable estimates of uncertainty can be used for tasks, such as out-of-distribution (OOD) detection, and sequential decision-making tasks, such as active learning. We present a framework for federated learning with uncertainty where, in each round, each client infers the posterior distribution over its parameters as well as the posterior predictive distribution (PPD), distills the PPD into a single deep neural network, and sends this network to the server. Unlike some of the recent Bayesian approaches to federated learning, our approach does not require sending the whole posterior distribution of the parameters from each client to the server but only the PPD in the distilled form as a deep neural network. In addition, when making predictions at test time, it does not require computationally expensive Monte-Carlo averaging over the posterior distribution because our approach always maintains the PPD in form a single deep neural network. Moreover, our approach does not make any restrictive assumptions, such as the form of the clients’ posterior distributions, or of their PPDs. We evaluate our approach on classification in federated setting, as well as active learning and OOD detection in federated settings, on which our approach outperforms various existing federated learning baselines.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/bhatt24a.html
  PDF: https://proceedings.mlr.press/v222/bhatt24a/bhatt24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-bhatt24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Shrey
    family: Bhatt
  - given: Aishwarya
    family: Gupta
  - given: Piyush
    family: Rai
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 153-168
  id: bhatt24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 153
  lastpage: 168
  published: 2024-02-27 00:00:00 +0000
- title: 'Diffusion-based Visual Representation Learning for Medical Question Answering'
  abstract: 'Medical visual question answering (Med-VQA) aims to correctly answer the medical question based on the given image. One of the major challenges is the scarcity of large professional labeled datasets for training, which poses obstacles to feature extraction, especially for medical images. To overcome it, we propose a method to learn transferable visual representation based on conditional denoising diffusion probabilistic model(conditional DDPM).Specifically, we collate a large amount of unlabeled radiological images and train a conditional DDPM with the paradigm of auto-encoder to obtain a model which can extract high-level semantic information from medical images.The pre-trained model can be used as a well initialized visual feature extractor and can be easily adapt to any Med-VQA systems. We build our Med-VQA system follow the state-of-the-art Med-VQA architecture and replace the visual extractor with our pre-trained model.Our proposal method outperforms the state-of-the-art Med-VQA method on VQA-RAD and achieves comparable result on SLAKE.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/bian24a.html
  PDF: https://proceedings.mlr.press/v222/bian24a/bian24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-bian24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Dexin
    family: Bian
  - given: Xiaoru
    family: Wang
  - given: Meifang
    family: Li
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 169-184
  id: bian24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 169
  lastpage: 184
  published: 2024-02-27 00:00:00 +0000
- title: 'Lost and Found: How Self-Supervised Learning Helps GPS Coordinates Find Their Way'
  abstract: 'GPS coordinates are a fundamental aspect of location-based applications, yet prior methods for representing them do not fully capture the intricate relationships between different locations. In this paper, we propose a novel map-based approach to embedding GPS coordinates using self-supervised learning. Unlike most prior studies that directly embed GPS coordinates to a latent space, we leverage a map-based approach, allowing embeddings to capture geographical and economic features. Namely, we use a student-teacher architecture, where a student network is trained to mimic the outputs of the teacher, using two different augmented versions of the same input. To capture the rich underlying semantics of GPS coordinates, we further leverage auxiliary tasks including \textit{geo} prediction, high-level reconstruction, and intermediate clustering. The intermediate clustering loss facilitates learning features at different levels of granularity, while the high-level reconstruction loss encourages “local-to-global” correspondences. We evaluate our method on a large-scale dataset of GPS coordinates and demonstrate that it outperforms several baseline methods in terms of the quality of the learned embeddings. Moreover, we show the usefulness of our embeddings in various downstream tasks, such as predicting land price, land cover type, or water quality indice.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/bougie24a.html
  PDF: https://proceedings.mlr.press/v222/bougie24a/bougie24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-bougie24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Nicolas
    family: Bougie
  - given: Daria
    family: Vazhenina
  - given: Narimasa
    family: Watanabe
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 185-200
  id: bougie24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 185
  lastpage: 200
  published: 2024-02-27 00:00:00 +0000
- title: 'BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning'
  abstract: 'This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/cagatan24a.html
  PDF: https://proceedings.mlr.press/v222/cagatan24a/cagatan24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-cagatan24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Omer Veysel
    family: Cagatan
  - given: Baris
    family: Akgun
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 201-216
  id: cagatan24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 201
  lastpage: 216
  published: 2024-02-27 00:00:00 +0000
- title: 'Deep Uniformly Distributed Centers on a Hypersphere for Open Set Recognition'
  abstract: 'This study introduces a new approach for open set recognition, wherein we propose a novel method utilizing uniformly distributed centers on a hypersphere. Each class in the proposed method is represented by a center, and these centers and features of the deep learning architecture are jointly learned from the training data in an end-to-end fashion. We ensure that the centers lie on the boundary of a hypersphere whose center is positioned at the origin. The class-specific samples are compelled by the proposed loss function to be closer to their respective centers. In open set recognition scenarios, an additional loss term is employed to separate the background samples from the known class centers. The assignment of test samples to classes is based on the Euclidean distances calculated from the learned class centers. Experimental results show that the proposed method yields the state-of-the-art accuracies on open set recognition datasets.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/cevikalp24a.html
  PDF: https://proceedings.mlr.press/v222/cevikalp24a/cevikalp24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-cevikalp24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Hakan
    family: Cevikalp
  - given: Hasan Serhan
    family: Yavuz
  - given: Hasan
    family: Saribas
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 217-230
  id: cevikalp24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 217
  lastpage: 230
  published: 2024-02-27 00:00:00 +0000
- title: 'Understanding More Knowledge Makes the Transformer Perform Better in Document-level Relation Extraction'
  abstract: 'Relation extraction plays a vital role in knowledge graph construction. In contrast with the traditional relation extraction on a single sentence, extracting relations from multiple sentences as a whole will harvest more valuable and richer knowledge. Recently, the Transformer-based pre-trained language models (TPLMs) are widely adopted to tackle document-level relation extraction (DocRE). Graph-based methods, aiming to acquire knowledge between entities to form entity-level relation graphs, have facilitated the rapid development of DocRE by infusing their proposed models with the knowledge. However, beyond entity-level knowledge, we discover many other kinds of knowledge that can aid humans to extract relations. It remains unclear whether and in which way they can be adopted to improve the performance of the Transformer, which affects the maximum performance gain of Transformer-based methods. In this paper, we propose a novel weighted multi-channel Transformer (WMCT) to infuse unlimited kinds of knowledge into the vanilla Transformer. Based on WMCT, we also explore five kinds of knowledge to enhance both its reasoning ability and expressive power. Our extensive experimental results demonstrate that: (1) more knowledge makes the performance of the Transformer better and (2) more informative knowledge leads to more performance gain. We appeal to future Transformer-based work to consider exploring more informative knowledge to improve the performance of the Transformer.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/haotian24a.html
  PDF: https://proceedings.mlr.press/v222/haotian24a/haotian24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-haotian24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Chen
    family: Haotian
  - given: Chen
    family: Yijiang
  - given: Zhou
    family: Xiangdong
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 231-246
  id: haotian24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 231
  lastpage: 246
  published: 2024-02-27 00:00:00 +0000
- title: 'Optimal Nonlinearities Improve Generalization Performance of Random Features'
  abstract: 'Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the "parameters" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than widely-used nonlinear functions such as ReLU. Furthermore, we illustrate that the proposed nonlinearities also mitigate the so-called double descent phenomenon, which is known as the non-monotonic generalization performance regarding the sample size and the model size.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/demir24a.html
  PDF: https://proceedings.mlr.press/v222/demir24a/demir24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-demir24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Samet
    family: Demir
  - given: Zafer
    family: Doğan
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 247-262
  id: demir24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 247
  lastpage: 262
  published: 2024-02-27 00:00:00 +0000
- title: 'Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training'
  abstract: 'Modern DNN-based recommendation systems rely on training-derived embeddings of sparse features. Input sparsity makes obtaining high-quality embeddings for rarely-occurring categories harder as their representations are updated infrequently. We demonstrate a training-time technique to produce superior embeddings via effective cross-category learning and theoretically explain its surprising effectiveness. The scheme, termed the multi-layer embeddings training (MLET), trains embeddings using factorization of the embedding layer, with an inner dimension higher than the target embedding dimension. For inference efficiency, MLET converts the trained two-layer embedding into a single-layer one thus keeping inference-time model size unchanged. Empirical superiority of MLET is puzzling as its search space is not larger than that of the single-layer embedding. The strong dependence of MLET on the inner dimension is even more surprising. We develop a theory that explains both of these behaviors by showing that MLET creates an adaptive update mechanism modulated by the singular vectors of embeddings. When tested on multiple state-of-the-art recommendation models for click-through rate (CTR) prediction tasks, MLET consistently produces better models, especially for rare items. At constant model quality, MLET allows embedding dimension, and model size, reduction by up to 16x, and 5.8x on average, across the models.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/deng24a.html
  PDF: https://proceedings.mlr.press/v222/deng24a/deng24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-deng24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Zihao
    family: Deng
  - given: Benjamin
    family: Ghaemmaghami
  - given: Ashish Kumar
    family: Singh
  - given: Benjamin
    family: Cho
  - given: Leo
    family: Orshansky
  - given: Mattan
    family: Erez
  - given: Michael
    family: Orshansky
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 263-278
  id: deng24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 263
  lastpage: 278
  published: 2024-02-27 00:00:00 +0000
- title: 'Hyper-Label-Graph: Modeling Branch-Level Dependencies of Labels for Hierarchical Multi-Label Text Classification'
  abstract: 'In the task of Hierarchical Multi-label Text Classification (HTMC), there exist multiple multivariate relations between labels, particularly the semantic dependencies within label branches of the hierarchy. However, existing methods struggle to fully exploit these potential multivariate dependencies since they can only model binary relationships at best. In this paper, we address this limitation by focusing on leveraging semantic dependencies among labels within branches and propose a Hyper-Label-Graph Model (HLGM). Specifically, we first construct a label hypergraph based on the taxonomy hierarchy and utilize a hypergraph attention mechanism to learn branch-level multivariate dependencies among labels. Furthermore, the model employs a label-text fusion module to generate label-level text representations, facilitating the comprehensive integration of semantic features between text and labels. Additionally, we introduce a hierarchical triplet loss to enhance the ability to distinguish labels within the hyperedge structure. We validate the effectiveness of the proposed model on three benchmark datasets, and the experimental results demonstrate that HLGM outperforms competitive GNN-based baselines.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/deng24b.html
  PDF: https://proceedings.mlr.press/v222/deng24b/deng24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-deng24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Wenmin
    family: Deng
  - given: Jing
    family: Zhang
  - given: Peng
    family: Zhang
  - given: Yitong
    family: Yao
  - given: Hui
    family: Gao
  - given: Yurui
    family: Zhang
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 279-294
  id: deng24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 279
  lastpage: 294
  published: 2024-02-27 00:00:00 +0000
- title: 'Temporal Shift - Multi-Objective Loss Function for Improved Anomaly Fall Detection'
  abstract: 'Falls are a major cause of injuries and deaths among older adults worldwide. Accurate fall detection can help reduce potential injuries and additional health complications. Different types of video modalities can be used in a home setting to detect falls, including RGB, Infrared, and Thermal cameras. Anomaly detection frameworks using autoencoders and their variants can be used for fall detection due to the data imbalance that arises from the rarity and diversity of falls. However, the use of reconstruction error in autoencoders can limit the application of networks’ structures that propagate information. In this paper, we propose a new multi-objective loss function called Temporal Shift, which aims to predict both future and reconstructed frames within a window of sequential frames. The proposed loss function is evaluated on a semi-naturalistic fall detection dataset containing multiple camera modalities. The autoencoders were trained on normal activities of daily living (ADL) performed by older adults and tested on ADLs and falls performed by young adults. Temporal shift shows significant improvement to a baseline 3D Convolutional autoencoder, an attention U-Net CAE, and a multi-modal neural network. The greatest improvement was observed in an attention U-Net model improving by $0.20$ AUC ROC for a single camera when compared to reconstruction alone. With significant improvement across different models, this approach has the potential to be widely adopted and improve anomaly detection capabilities in other settings besides fall detection.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/denkovski24a.html
  PDF: https://proceedings.mlr.press/v222/denkovski24a/denkovski24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-denkovski24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Stefan
    family: Denkovski
  - given: Shehroz S
    family: Khan
  - given: Alex
    family: Mihailidis
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 295-310
  id: denkovski24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 295
  lastpage: 310
  published: 2024-02-27 00:00:00 +0000
- title: 'Better Loss Landscape Visualization for Deep Neural Networks with Trajectory Information'
  abstract: 'The loss landscape of neural networks is a valuable perspective for studying the trainability, generalization, and robustness of networks, and hence its visualization has been extensively studied. Essentially, visualization methods project the parameter space into a low-dimensional subspace, resulting in a substantial loss of network parameter information. The key is to identify the direction of loss reduction in the visualized loss landscape. However, the existing methods generally focus on one simple point, make it challenging to properly capture the main properties of the landscape. An obvious and important problem is that regardless of whether the center point is the convergence or not, the current methods may depict it a local optimal point in the visualization. To address this issue, we propose a visualization method that relies on the whole training process not a single solution, better reflecting the actual training loss.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ding24a.html
  PDF: https://proceedings.mlr.press/v222/ding24a/ding24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ding24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Ruiqi
    family: Ding
  - given: Tao
    family: Li
  - given: Xiaolin
    family: Huang
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 311-326
  id: ding24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 311
  lastpage: 326
  published: 2024-02-27 00:00:00 +0000
- title: 'Pedestrian Cross Forecasting with Hybrid Feature Fusion'
  abstract: 'Forecasting the crossing intention of pedestrians is an essential task for the safe driving of Autonomous Vehicles (AVs) in the real world. Pedestrians’ behaviors are usually influenced by their surroundings in traffic scenes. Recent works based on vision-based neural networks extract key information from images to perform prediction. However, in the driving environment, there exists much critical information, such as the social and scene interaction in the driving area, the location and distance between the ego car and target pedestrian, and the motion of all targets. How properly exploring and utilizing the above implicit interactions will promote the development of Autonomous Vehicles. In this chapter, two novel attributes, the pedestrian’s location on the road or sidewalk, and the relative distance from the target pedestrian to the ego-car, which are derived from the semantic map and depth map combined with bounding boxes, are introduced. A hybrid prediction network based on multi-modal is proposed to capture the interactions between all the features and predict pedestrian crossing intention. Evaluated by two public pedestrian crossing datasets, PIE and JAAD, the proposed hybrid framework outperforms the state-of-the-art by about an accuracy of 3%.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/dong24a.html
  PDF: https://proceedings.mlr.press/v222/dong24a/dong24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-dong24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Meng
    family: Dong
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 327-342
  id: dong24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 327
  lastpage: 342
  published: 2024-02-27 00:00:00 +0000
- title: 'Deep Representation Learning for Prediction of Temporal Event Sets in the Continuous Time Domain'
  abstract: 'Temporal Point Processes (TPP) play an important role in predicting or forecasting events. Although these problems have been studied extensively, predicting multiple simultaneously occurring events can be challenging. For instance, more often than not, a patient gets admitted to a hospital with multiple conditions at a time. Similarly people buy more than one stock and multiple news breaks out at the same time. Moreover, these events do not occur at discrete time intervals, and forecasting event sets in the continuous time domain remains an open problem. Naïve approaches for extending the existing TPP models for solving this problem lead to dealing with an exponentially large number of events or ignoring set dependencies among events. In this work, we propose a scalable and efficient approach based on TPPs to solve this problem. Our proposed approach incorporates contextual event embeddings, temporal information, and domain features to model the temporal event sets. We demonstrate the effectiveness of our approach through extensive experiments on multiple datasets, showing that our model outperforms existing methods in terms of prediction metrics and computational efficiency. To the best of our knowledge, this is the first work that solves the problem of predicting event set intensities in the continuous time domain by using TPPs.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/dutta24a.html
  PDF: https://proceedings.mlr.press/v222/dutta24a/dutta24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-dutta24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Parag
    family: Dutta
  - given: Kawin
    family: Mayilvaghanan
  - given: Pratyaksha
    family: Sinha
  - given: Ambedkar
    family: Dukkipati
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 343-358
  id: dutta24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 343
  lastpage: 358
  published: 2024-02-27 00:00:00 +0000
- title: 'Robust Image Classification via Using Multiple Diversity Losses'
  abstract: 'Many research works focus on the robustness of convolutional neural networks (CNNs) on image classification. Diversity loss has been demonstrated to be an effective method to boost robustness. However, the existing diversity losses did not fully consider the strong correlation between regional features when filters are locally activated. They focused on improving filter responses constraint with classification loss. However, diversity loss has deeper optimization space. We explore the combinations of different filter diversity losses and feature diversity losses. We enhance the orthogonality between pair-wise filters to make them more diverse and penalize irrelevance between regional response mappings. We make multiple combinations and propose several methods on improving orthogonality, which have different adaptations for different datasets and network models. We evaluate their effectiveness in experiment. Our combinations could improve the efficiency of robust image recognition.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/fang24a.html
  PDF: https://proceedings.mlr.press/v222/fang24a/fang24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-fang24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Yi
    family: Fang
  - given: Wen-Hao
    family: Zheng
  - given: Qihui
    family: Wang
  - given: Xiao-Xin
    family: Li
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 359-373
  id: fang24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 359
  lastpage: 373
  published: 2024-02-27 00:00:00 +0000
- title: 'Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices'
  abstract: 'The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate multiscale information from diverse data sources. MM-NNs extract and fuse features from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose \textit{Harmonic-NAS}, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. \textit{Harmonic-NAS} involves a two-tier optimization approach for the unimodal backbone architectures and fusion strategy and operators. By incorporating the hardware dimension into the optimization, evaluation results on various devices and multimodal datasets have demonstrated the superiority of \textit{Harmonic-NAS} over state-of-the-art approaches achieving up to $\sim \textbf{10.9%} accuracy improvement, $\sim$\textbf{1.91x} latency reduction, and $\sim$\textbf{2.14x} energy efficiency gain.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ghebriout24a.html
  PDF: https://proceedings.mlr.press/v222/ghebriout24a/ghebriout24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ghebriout24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Mohamed Imed Eddine
    family: Ghebriout
  - given: Halima
    family: Bouzidi
  - given: Smail
    family: Niar
  - given: Hamza
    family: Ouarnoughi
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 374-389
  id: ghebriout24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 374
  lastpage: 389
  published: 2024-02-27 00:00:00 +0000
- title: 'State Value Generation with Prompt Learning and Self-Training for Low-Resource Dialogue State Tracking'
  abstract: 'Recently, low-resource dialogue state tracking (DST) has received increasing attention. First obtaining state values then based on values to generate slot types has made great progress in this task. However, obtaining state values is still an under-studied problem. Existing extraction-based approaches cannot capture values that require the understanding of context and are not generalizable either. To address these issues, we propose a novel State VAlue Generation based framework (SVAG), decomposing DST into state value generation and domain slot generation. Specifically, we propose to generate state values and use self-training to further improve state value generation. Moreover, we design an estimator aiming at detecting incomplete generation and incorrect generation for pseudo-labeled data selection during self-training. Experimental results on the MultiWOZ 2.1 dataset show that our method which has only less than 1 billion parameters achieves state-of-the-art performance under the data ratio settings of 5%, 10%, and 25% when limited to models under 100 billion parameters. Compared to models with more than 100 billion parameters, SVAG still reaches competitive results.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/gu24a.html
  PDF: https://proceedings.mlr.press/v222/gu24a/gu24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-gu24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Ming
    family: Gu
  - given: Yan
    family: Yang
  - given: Chengcai
    family: Chen
  - given: Zhou
    family: Yu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 390-405
  id: gu24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 390
  lastpage: 405
  published: 2024-02-27 00:00:00 +0000
- title: 'Self Weighted Multiplex Modularity Maximization for Multiview Clustering'
  abstract: 'In response to the challenge of representing data from multiple sources, researchers have proposed the use of multiplex graphs as a solution. Multiplex graphs are particularly useful for representing multi-view data, where each layer represents a specific type of interaction. Pillar community detection of multiplex graphs is a clustering application that computes groups of vertices across all layers. Modularity maximization is a popular technique for graph clustering, which has been generalized to multiplex graphs. However, this generalization did not consider the importance of each layer in pillar clustering. This paper presents a new technique called Self Weighted Multiplex Modularity (SWMM), which optimizes the weights associated with each layer and the partition that maximizes the multiplex modularity. The paper proposes two optimization methods, iterative and direct, and demonstrates the effectiveness and robustness of the technique in accurately retrieving clusters even when data is highly missing.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/henka24a.html
  PDF: https://proceedings.mlr.press/v222/henka24a/henka24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-henka24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Noureddine
    family: Henka
  - given: Mohamad
    family: Assaad
  - given: Sami
    family: Tazi
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 406-421
  id: henka24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 406
  lastpage: 421
  published: 2024-02-27 00:00:00 +0000
- title: 'How GAN Generators can Invert Networks in Real-Time'
  abstract: 'In this work, we propose a fast and accurate method to reconstruct activations of classification and semantic segmentation networks by stitching them with a GAN generator utilizing a 1x1 convolution. We test our approach on images of animals from the AFHQ wild dataset, ImageNet1K, and real-world digital pathology scans of stained tissue samples. Our results show comparable performance to established gradient descent methods but with a processing time that is two orders of magnitude faster, making this approach promising for practical applications.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/herdt24a.html
  PDF: https://proceedings.mlr.press/v222/herdt24a/herdt24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-herdt24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Rudolf
    family: Herdt
  - given: Maximilian
    family: Schmidt
  - given: Daniel
    family: Otero Baguer
  - given: Jean
    family: Le’Clerc Arrastia
  - given: Peter
    family: Maaß
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 422-437
  id: herdt24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 422
  lastpage: 437
  published: 2024-02-27 00:00:00 +0000
- title: 'Towards Human-Like RL: Taming Non-Naturalistic Behavior in Deep RL via Adaptive Behavioral Costs in 3D Games'
  abstract: 'In this paper, we propose a new approach called Adaptive Behavioral Costs in Reinforcement Learning (ABC-RL) for training a human-like agent with competitive strength. While deep reinforcement learning agents have recently achieved superhuman performance in various video games, some of these unconstrained agents may exhibit actions, such as shaking and spinning, that are not typically observed in human behavior, resulting in peculiar gameplay experiences. To behave like humans and retain similar performance, ABC-RL augments behavioral limitations as cost signals in reinforcement learning with dynamically adjusted weights. Unlike traditional constrained policy optimization, we propose a new formulation that minimizes the behavioral costs subject to a constraint of the value function. By leveraging the augmented Lagrangian, our approach is an approximation of the Lagrangian adjustment, which handles the trade-off between the performance and the human-like behavior. Through experiments conducted on 3D games in DMLab-30 and Unity ML-Agents Toolkit, we demonstrate that ABC-RL achieves the same performance level while significantly reducing instances of shaking and spinning. These findings underscore the effectiveness of our proposed approach in promoting more natural and human-like behavior during gameplay.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ho24a.html
  PDF: https://proceedings.mlr.press/v222/ho24a/ho24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ho24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Kuo-Hao
    family: Ho
  - given: Ping-Chun
    family: Hsieh
  - given: Chiu-Chou
    family: Lin
  - given: You-Ren
    family: Lou
  - given: Feng-Jian
    family: Wang
  - given: I-Chen
    family: Wu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 438-453
  id: ho24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 438
  lastpage: 453
  published: 2024-02-27 00:00:00 +0000
- title: 'Outlier Robust Adversarial Training'
  abstract: 'Supervised learning models are challenged by the intrinsic complexities of training data such as outliers and minority subpopulations and intentional attacks at inference time with adversarial samples. While traditional robust learning methods and the recent adversarial training approaches are designed to handle each of the two challenges, to date, no work has been done to develop models that are robust with regard to the low-quality training data and the potential adversarial attack at inference time simultaneously. It is for this reason that we introduce \underline{O}utlier \underline{R}obust \underline{A}dversarial \underline{T}raining (ORAT) in this work. ORAT is based on a bi-level optimization formulation of adversarial training with a robust rank-based loss function. Theoretically, we show that the learning objective of ORAT satisfies the $\mathcal{H}$-consistency in binary classification, which establishes it as a proper surrogate to adversarial 0/1 loss. Furthermore, we analyze its generalization ability and provide uniform convergence rates in high probability. ORAT can be optimized with a simple algorithm. Experimental evaluations on three benchmark datasets demonstrate the effectiveness and robustness of ORAT in handling outliers and adversarial attacks. Our code is available at \url{https://github.com/discovershu/ORAT}.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/hu24a.html
  PDF: https://proceedings.mlr.press/v222/hu24a/hu24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-hu24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Shu
    family: Hu
  - given: Zhenhuan
    family: Yang
  - given: Xin
    family: Wang
  - given: Yiming
    family: Ying
  - given: Siwei
    family: Lyu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 454-469
  id: hu24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 454
  lastpage: 469
  published: 2024-02-27 00:00:00 +0000
- title: 'Temporal RPN Learning for Weakly-Supervised Temporal Action Localization'
  abstract: 'Weakly-Supervised Temporal Action Localization (WSTAL) aims to train an action instance localization model from untrimmed videos with only video-level labels, similar to the Object Detection (OD) task. Existing Top-k MIL-based WSTAL methods cannot flexibly define the learning space, which limits the model’s learning efficiency and performance. Faster R-CNN is a classic two-stage object detection architecture with an efficient Region Proposal Network. This paper successfully migrates the Faster R-CNN liked two-stage architecture to the WSTAL task: first to build a T-RPN and integrate it with the traditional WSTAL framework; and then to propose a pseudo label generation mechanism to enable the T-RPN learning without temporal annotations. Our new framework has achieved breakthrough performances on THUMOS-14 and ActivityNet-v1.2 datasets, and comprehensive ablation experiments have verified the effectiveness of the innovations. Code will be available at: \href{https://github.com/ZJUHJ/TRPN}{https://github.com/ZJUHJ/TRPN}.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/huang24a.html
  PDF: https://proceedings.mlr.press/v222/huang24a/huang24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-huang24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Jing
    family: Huang
  - given: Ming
    family: Kong
  - given: Luyuan
    family: Chen
  - given: Tian
    family: Liang
  - given: Qiang
    family: Zhu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 470-485
  id: huang24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 470
  lastpage: 485
  published: 2024-02-27 00:00:00 +0000
- title: 'Probing Traffic Trend Forecasting via Spatial-Temporal Aware Learning-Graph Attention'
  abstract: 'Traffic forecasting plays an extremely important role in many applications such as intelligent transportation and smart cities. However, due to the hidden and complex dynamic spatio-temporal correlations and heterogeneity, achieving high-precision traffic prediction is a challenging task. This paper proposes a new spatio-temporal aware learning graph neural network (STALGNN) for traffic prediction. First, a temporal-aware graph generation module is designed to exploit the spatial-temporal features that the spatial graph may not be able to present. Then, a spatio-temporal joint module is designed to more effectively capture local spatio-temporal correlations. Next, a multi-scale gated convolutions module is proposed to capture gloable dynamic spatio-temporal correlations. Furthermore, STALGNN further learns explicit  spatio-temporal correlations through integrated attention mechanisms and stacked graph convolutional networks to handle long-term prediction. Extensive experiments on several real traffic datasets show that the proposed method can achieve the superior performance compared with other baselines.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/huang24b.html
  PDF: https://proceedings.mlr.press/v222/huang24b/huang24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-huang24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Xinyuan
    family: Huang
  - given: Qianqian
    family: Ren
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 486-501
  id: huang24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 486
  lastpage: 501
  published: 2024-02-27 00:00:00 +0000
- title: 'Reinforcement Learning for Solving Stochastic Vehicle Routing Problem'
  abstract: 'This study addresses a gap in the utilization of Reinforcement Learning (RL) and Machine Learning (ML) techniques in solving the Stochastic Vehicle Routing Problem (SVRP) that involves the challenging task of optimizing vehicle routes under uncertain conditions. We propose a novel end-to-end framework that comprehensively addresses the key sources of stochasticity in SVRP and utilizes an RL agent with a simple yet effective architecture and a tailored training method. Through comparative analysis, our proposed model demonstrates superior performance compared to a widely adopted state-of-the-art metaheuristic, achieving a significant 3.43% reduction in travel costs. Furthermore, the model exhibits robustness across diverse SVRP settings, highlighting its adaptability and ability to learn optimal routing strategies in varying environments. The publicly available implementation of our framework serves as a valuable resource for future research endeavors aimed at advancing RL-based solutions for SVRP.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/iklassov24a.html
  PDF: https://proceedings.mlr.press/v222/iklassov24a/iklassov24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-iklassov24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Zangir
    family: Iklassov
  - given: Ikboljon
    family: Sobirov
  - given: Ruben
    family: Solozabal
  - given: Martin
    family: Takáč
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 502-517
  id: iklassov24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 502
  lastpage: 517
  published: 2024-02-27 00:00:00 +0000
- title: 'Mem-Rec: Memory Efficient Recommendation System using Alternative Representation'
  abstract: 'Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI models to provide high-quality personalized recommendations. Training data used for modern recommendation systems commonly includes categorical features taking on tens-of-millions of possible distinct values. These categorical tokens are typically assigned learned vector representations, that are stored in large embedding tables, on the order of 100s of GB. Storing and accessing these tables represent a substantial performance burden. Our work proposes MEM-REC, a novel alternative representation approach for embedding tables. MEM-REC leverages Bloom filters and hashing methods to encode categorical features using two cache-friendly embedding tables. The first table (token embedding) contains raw embeddings (i.e. learned vector representation), and the second table (weight embedding), which is much smaller, contains weights to scale these raw embeddings to provide better discriminative capability to each data point. We provide a detailed architecture, design and analysis of MEM-REC addressing trade-offs in accuracy and computation requirements. In comparison with state-of-the-art techniques MEM-REC can not only maintain the recommendation quality and significantly reduce the memory footprint for commercial scale recommendation models but can also improve the embedding latency. In particular, based on our results, MEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by $2900\times$ and performs up to $3.4\times$ faster embeddings while achieving the same AUC as that of the full uncompressed model.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/jha24a.html
  PDF: https://proceedings.mlr.press/v222/jha24a/jha24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-jha24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Gopi Krishna
    family: Jha
  - given: Anthony
    family: Thomas
  - given: Nilesh
    family: Jain
  - given: Sameh
    family: Gobriel
  - given: Tajana
    family: Rosing
  - given: Ravi
    family: Iyer
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 518-533
  id: jha24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 518
  lastpage: 533
  published: 2024-02-27 00:00:00 +0000
- title: 'Adaptive Riemannian stochastic gradient descent and reparameterization for Gaussian mixture model fitting'
  abstract: 'Recent advances in manifold optimization for the Gaussian mixture model (GMM) have gained increasing interest. In this work, instead of directly addressing the manifold optimization on covariance matrices of GMM, we consider the GMM fitting as an optimization of the density function over a statistical manifold and seek the natural gradient to speed up the optimization process. We present an upper bound for the Kullback–Leibler (KL) divergence between two GMMs and obtain simple closed-form expressions for the natural gradients. With the natural gradients, we then apply the Riemannian stochastic gradient descent (RSGD) algorithm to optimize covariance matrices on a symmetric and positive definite (SPD) matrix manifold. We further propose a Riemannian Adam (RAdam) algorithm that extends the momentum method and adaptive learning in the Euclidean space to the SPD manifold space. Extensive simulations show that the proposed algorithms scale well to high-dimensional large-scale datasets and outperform expectation maximization (EM) algorithms in fitted log-likelihood.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ji24a.html
  PDF: https://proceedings.mlr.press/v222/ji24a/ji24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ji24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Chunlin
    family: Ji
  - given: Yuhao
    family: Fu
  - given: Ping
    family: He
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 534-549
  id: ji24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 534
  lastpage: 549
  published: 2024-02-27 00:00:00 +0000
- title: 'Deep Kernel Regression with Finite Learnable Kernels'
  abstract: 'In this work, we study kernel regression that integrates with a modern deep neural network (DNN). The DNN projects the input into an embedding space, meanwhile a set of representative points is constructed in this embedding space. We build the regression using a finite set of kernels defined on the embedding space, where the DNN weights, the regression coefficients, and kernel hyperparameters are all learnable. We extend the model by introducing a location attention strategy and the multiple kernel technique. We provide effective ways to obtain representative points. The proposed model can be trained with an end-to-end learning algorithm with simple implementation. Simulation studies show that the proposed deep kernel regression is well scalable to large datasets and comparable to or superior to recent deep kernel models in various regression problems.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ji24b.html
  PDF: https://proceedings.mlr.press/v222/ji24b/ji24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ji24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Chunlin
    family: Ji
  - given: Yuhao
    family: Fu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 550-565
  id: ji24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 550
  lastpage: 565
  published: 2024-02-27 00:00:00 +0000
- title: 'Prototypical Model with Information-Theoretic Loss Functions for Generalized Zero-Shot Learning'
  abstract: 'Generalized zero-shot learning (GZSL) is still a technical challenge of deep learning. To preserve the semantic relation between source and target classes when only trained with data from source classes, we address the quantification of the knowledge transfer from an information-theoretic viewpoint. We use the prototypical model and format the variables of concern as a probability vector. Taking advantage of the probability vector representation, information measurements can be effectively evaluated with simple closed forms. We propose two information-theoretic loss functions: a mutual information loss to bridge seen data and target classes; an uncertainty-aware entropy constraint loss to prevent overfitting when using seen data to learn the embedding of target classes. Simulation shows that, as a deterministic model, our proposed method obtains state-of-the-art results on GZSL benchmark datasets. We achieve 21% − 64% improvements over the baseline model – deep calibration network (DCN) and demonstrate that a deterministic model can perform as well as generative ones. Furthermore, the proposed method is compatible with generative models and can noticeably improve their performance.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ji24c.html
  PDF: https://proceedings.mlr.press/v222/ji24c/ji24c.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ji24c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Chunlin
    family: Ji
  - given: Zhan
    family: Xiong
  - given: Meiying
    family: Zhang
  - given: Huiwen
    family: Yang
  - given: Feng
    family: Chen
  - given: Hanchun
    family: Shen
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 566-581
  id: ji24c
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 566
  lastpage: 581
  published: 2024-02-27 00:00:00 +0000
- title: 'Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation'
  abstract: 'Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for closeness and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limitations in the literature. We formulate an iterative algorithm to compute provably robust CEs and prove its convergence, soundness and completeness. Through a comparative experiment involving six baselines, five of which target robustness, we show that PROPLACE achieves state-of-the-art performances against metrics on three evaluation aspects.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/jiang24a.html
  PDF: https://proceedings.mlr.press/v222/jiang24a/jiang24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-jiang24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Junqi
    family: Jiang
  - given: Jianglin
    family: Lan
  - given: Francesco
    family: Leofante
  - given: Antonio
    family: Rago
  - given: Francesca
    family: Toni
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 582-597
  id: jiang24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 582
  lastpage: 597
  published: 2024-02-27 00:00:00 +0000
- title: 'SART-Res-UNet: Fan Beam CT Image Reconstruction from Limited Projections using attention-enabled residual U-Net'
  abstract: 'CT scans significantly improve analytical competencies but uses X-Rays which will produce ionizing radiation that bring higher radiation to the living tissues. Thus, optimization of CT radiation dose has a significant concern to lower the health risks. Many manufacturers have done a greater contribution by developing technologies to reduce dosage by maintaining image quality by adding noise reduction filters, automatic exposure control, and using many iterative reconstruction algorithms. Image reconstruction algorithms play a vital role in maintaining or improving image quality in reduced-dose CT. The present research work combines the state-of-the-art reconstruction technique Simultaneous Algebraic Reconstruction Technique (SART) with a Residual U-Net network to generate images from limited number of sinograms. The proposed model is trained using sinograms corresponding head and neck and head CT images of 10 patients. The proposed model predicted superior diagnostic quality images with max PSNR of 70.23 and Structural Similarity Index Measure (SSIM) of 0.99. Thus the proposed model, SART-Res-Unet, ensures a very low radiation exposure to a patient during the repeated CT imaging sequence, which is an inevitable part of radiotherapy.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/jinka24a.html
  PDF: https://proceedings.mlr.press/v222/jinka24a/jinka24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-jinka24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Harika
    family: Jinka
  - given: Jyothsna
    family: Shaji
  - given: Sangeeth
    family: John
  - given: Sreeraj R
    family: Menon
  - given: Amalu
    family: Pradeep
  - given: Jayaraj
    family: P B
  - given: Pournami
    family: P N
  - given: Niyas
    family: Puzhakkal
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 598-613
  id: jinka24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 598
  lastpage: 613
  published: 2024-02-27 00:00:00 +0000
- title: 'Ada$^2$NPT: An Adaptive Nearest Proxies Triplet Loss for Attribute-Aware Face Recognition with Adaptively Compacted Feature Learning'
  abstract: 'Attribute-aware face recognition has gained increasing attention in recent years due to its potential to improve the robustness of face recognition systems. However, this may raise concerns about potential biases and privacy issues. To alleviate this, some studies involve complex designs to obtain independent ID and attribute features and fuse them based on the application scenario (for better accuracy or fairness). In this paper, we obviate their complex design and demonstrate that the Nearest neighbours Proxy Triplet (NPT) loss has an intrinsic capability for feature disentanglement. To further enhance the effectiveness of NPT, we propose a novel margin-based loss, namely Adaptive-rank NPT, which naturally separates the identity and attribute features. While a margin-based loss ensures inter-class separability, it imposes no constraints on intra-class compactness. The samples that meet the inter-class margin will not contribute to network training. To mitigate this issue, we propose an adaptive distance measurement to promote the compactness of the learned features, resulting in the final Ada$^2$NPT loss. The experimental results obtained on several benchmarks demonstrate the superiority and merits of the proposed loss function over the state-of-the-art losses in terms of accuracy and fairness.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ju24a.html
  PDF: https://proceedings.mlr.press/v222/ju24a/ju24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ju24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Lei
    family: Ju
  - given: Zhanhua
    family: Feng
  - given: Muhammad
    family: Awais
  - given: Josef
    family: Kittler
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 614-629
  id: ju24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 614
  lastpage: 629
  published: 2024-02-27 00:00:00 +0000
- title: 'Cross-Domain Relation Adaptation'
  abstract: 'We consider the challenge of establishing relationships between samples in distinct domains, A and B, using supervised data that captures the intrinsic relationships within each domain. In other words, we present a semi-supervised setting in which there are no labeled mixed-domain pairs of samples. Our method is derived based on a generalization bound and incorporates supervised terms for each domain, a domain confusion term on the learned features, and a consistency term for domain-specific relationships when considering mixed-domain sample pairs. Our findings showcase the efficacy of our approach in two disparate domains: (i) Predicting protein-protein interactions between viruses and hosts by modeling genetic sequences. (ii) Forecasting link connections within citation graphs using graph neural networks.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/kessler24a.html
  PDF: https://proceedings.mlr.press/v222/kessler24a/kessler24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-kessler24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Ido
    family: Kessler
  - given: Omri
    family: Lifshitz
  - given: Sagie
    family: Benaim
  - given: Lior
    family: Wolf
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 630-645
  id: kessler24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 630
  lastpage: 645
  published: 2024-02-27 00:00:00 +0000
- title: 'Thompson Exploration with Best Challenger Rule in Best Arm Identification'
  abstract: 'This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K \geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to asymptotically optimal policies in terms of sample complexity while requiring less computation cost. In addition, we highlight the advantages of our policy by comparing it to the concept of $\beta$-optimality, a relaxed notion of asymptotic optimality commonly considered in the analysis of a class of policies including the proposed one.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/lee24a.html
  PDF: https://proceedings.mlr.press/v222/lee24a/lee24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-lee24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Jongyeong
    family: Lee
  - given: Junya
    family: Honda
  - given: Masashi
    family: Sugiyama
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 646-661
  id: lee24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 646
  lastpage: 661
  published: 2024-02-27 00:00:00 +0000
- title: 'Facto-CNN: Memory-Efficient CNN Training with Low-rank Tensor Factorization and Lossy Tensor Compression'
  abstract: 'Convolutional neural networks (CNNs) are becoming deeper and wider to achieve higher accuracy and lower loss, significantly expanding the computational resources. Especially, training CNN models extensively consumes memory mainly due to storing intermediate feature maps generated in the forward-propagation for calculating the gradient in the backpropagation. The memory usage of the CNN model training escalates with the increase in batch size and the complexity of the model. Therefore, a lightweight training method is essential, especially when the computational resources are limited. In this paper, we propose a CNN training mechanism called Facto-CNN, leveraging low-rank tensor factorization and lossy tensor compression to reduce the memory usage required in training the CNN models. Facto-CNN factorizes the weight tensors of convolutional and fully-connected layers and then only updates one of the factorized tensors for each layer, dramatically reducing the feature map size stored in the memory. To further reduce memory consumption, Facto-CNN compresses the feature maps with a simple lossy compression technique that exploits the value similarity in the feature maps. Our experimental evaluation demonstrates that Facto-CNN reduces the memory usage for storing the feature maps by 68-93% with a trivial accuracy degradation when training the CNN models.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/lee24b.html
  PDF: https://proceedings.mlr.press/v222/lee24b/lee24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-lee24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Seungtae
    family: Lee
  - given: Jonghwan
    family: Ko
  - given: Seokin
    family: Hong
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 662-677
  id: lee24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 662
  lastpage: 677
  published: 2024-02-27 00:00:00 +0000
- title: 'SANGEA: Scalable and Attributed Network Generation'
  abstract: 'The topic of synthetic graph generators (SGGs) has recently received much attention due to the wave of the latest breakthroughs in generative modelling. However, many state-of-the-art SGGs do not scale well with the graph size. Indeed, in the generation process, all the possible edges for a fixed number of nodes must often be considered, which scales in $\mathcal{O}(N^2)$, with $N$ being the number of nodes in the graph. For this reason, many state-of-the-art SGGs are not applicable to large graphs. In this paper, we present SANGEA, a sizeable synthetic graph generation framework which extends the applicability of any SGG to large graphs. By first splitting the large graph into communities, SANGEA trains one SGG per community, then links the community graphs back together to create a synthetic large graph. Our experiments show that the graphs generated by SANGEA have high similarity to the original graph, in terms of both topology and node feature distribution. Additionally, these generated graphs achieve high utility on downstream tasks such as link prediction. Finally, we provide a privacy assessment of the generated graphs to show that, even though they have excellent utility, they also achieve reasonable privacy scores.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/lemaire24a.html
  PDF: https://proceedings.mlr.press/v222/lemaire24a/lemaire24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-lemaire24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Valentin
    family: Lemaire
  - given: Youssef
    family: Achenchabe
  - given: Lucas
    family: Ody
  - given: Houssem Eddine
    family: Souid
  - given: Gianmarco
    family: Aversano
  - given: Nicolas
    family: Posocco
  - given: Sabri
    family: Skhiri
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 678-693
  id: lemaire24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 678
  lastpage: 693
  published: 2024-02-27 00:00:00 +0000
- title: 'FusionU-Net: U-Net with Enhanced Skip Connection for Pathology Image Segmentation'
  abstract: ' In recent years, U-Net and its variants have been widely used in pathology image segmentation tasks. One of the key designs of U-Net is the use of skip connections between the encoder and decoder, which helps to recover detailed information after upsampling. While most variations of U-Net adopt the original skip connection design, there is semantic gap between the encoder and decoder that can negatively impact model performance. Therefore, it is important to reduce this semantic gap before conducting skip connection. To address this issue, we propose a new segmentation network called FusionU-Net, which is based on U-Net structure and incorporates a fusion module to exchange information between different skip connections to reduce semantic gaps. Unlike the other fusion modules in existing networks, ours is based on a two-round fusion design that fully considers the local relevance between adjacent encoder layer outputs and the need for bi-directional information exchange across multiple layers. We conducted extensive experiments on multiple pathology image datasets to evaluate our model and found that FusionU-Net achieves better performance compared to other competing methods. We argue our fusion module is more effective than the designs of existing networks, and it could be easily embedded into other networks to further enhance the model performance. Our code is available at: https://github.com/Zongyi-Lee/FusionU-Net'
  volume: 222
  URL: https://proceedings.mlr.press/v222/li24a.html
  PDF: https://proceedings.mlr.press/v222/li24a/li24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-li24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Zongyi
    family: Li
  - given: Hongbing
    family: Lyu
  - given: Jun
    family: Wang
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 694-706
  id: li24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 694
  lastpage: 706
  published: 2024-02-27 00:00:00 +0000
- title: 'Detecting and Repairing Deviated Outputs of Compressed Models'
  abstract: 'With the rapid development of deep learning and its pervasive usage on various low-power and resource-constrained devices, model compression methods are increasingly used to reduce the model size and computation cost. Despite the overall high test accuracy of the compressed models, our observation shows that an original model and its compressed version (e.g., via quantization) can have deviated prediction outputs on the same inputs. These behavior deviations on compressed models are undesirable, given that the compressed models may be used in reliability-critical scenarios such as automated manufacturing and robotics systems. Inspired by software engineering practices, this paper proposes CompD, a differential testing (DT)-based framework for detecting and repairing prediction deviations on compressed models and their plaintext versions. CompD treats original/compressed models as “black-box,” thus offering an efficient method orthogonal to specific different compression schemes. Furthermore, CompD can leverage deviation-triggering inputs to finetune the compressed models, largely “repairing” their defects. Evaluations show that CompD can effectively test and repair common models compressed by different schemes.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/li24b.html
  PDF: https://proceedings.mlr.press/v222/li24b/li24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-li24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Yichen
    family: Li
  - given: Qi
    family: Pang
  - given: Dongwei
    family: Xiao
  - given: Zhibo
    family: Liu
  - given: Shuai
    family: Wang
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 707-722
  id: li24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 707
  lastpage: 722
  published: 2024-02-27 00:00:00 +0000
- title: 'Attributed Graph Subspace Clustering with Graph-Boosting'
  abstract: 'Attributed graph clustering groups nodes into disjoint categories with graph convolutional networks and has exhibited promising performance in various applications. However, there are two issues preventing the performance from being improved  further. First, the relationships between distant nodes are generally overlooked due to the sparsity of graphs. Second, the graph convolutional networks with few layers are sensitive to noises. To address these issues, we propose Attributed Graph Subspace clustering with Graph-Boosting (AGSGB). Specifically, to deal with the first issue, an auxiliary graph is built from the feature matrix to establish the distant relationships. And to address the second issue, a subspace clustering module, famous for its robustness to noise, is introduced. Based on the auxiliary graph and the subspace clustering module, a graph enhance module and a graph refine module are constructed, together with the graph autoencoder constituting the final clustering model. By using the given graph and the refined graph built by the graph refine module, a dual guidance supervisor is designed to train the clustering model. Finally, the clustering result can be obtained by the subspace clustering module. Extensive experimental results on five public benchmark datasets validate the effectiveness of the proposed method.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/li24c.html
  PDF: https://proceedings.mlr.press/v222/li24c/li24c.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-li24c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Wang
    family: Li
  - given: En
    family: Zhu
  - given: Siwei
    family: Wang
  - given: Xifeng
    family: Guo
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 723-738
  id: li24c
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 723
  lastpage: 738
  published: 2024-02-27 00:00:00 +0000
- title: 'K-Truss Based Temporal Graph Convolutional Network for Dynamic Graphs'
  abstract: 'Learning latent representations of nodes in graphs is important for many real-world applications, such as recommender systems, traffic prediction and fraud detection. Most of the existing research on graph representation learning has focused on static graphs. However, many real-world graphs are dynamic and their structures change over time, which makes learning dynamic node representations challenging. We propose a novel k-truss based temporal graph convolutional network named TTGCN to learn potential node representations on dynamic graphs. Specifically, TTGCN utilizes a novel truss-based graph convolutional layer named TrussGCN to capture the topology and hierarchical structure information of graphs, and combines it with a temporal evolution module to capture complex temporal dependencies. We conduct link prediction experiments on five different dynamic graph datasets. Experimental results demonstrate the superiority of TTGCN for dynamic graph embedding, as it consistently outperforms several state-of-the-art baselines in the link prediction task. In addition, our ablation experiments demonstrate the effectiveness of adopting TrussGCN in a dynamic graph embedding method.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/li24d.html
  PDF: https://proceedings.mlr.press/v222/li24d/li24d.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-li24d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Hongxi
    family: Li
  - given: Zuxuan
    family: Zhang
  - given: Dengzhe
    family: Liang
  - given: Yuncheng
    family: Jiang
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 739-754
  id: li24d
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 739
  lastpage: 754
  published: 2024-02-27 00:00:00 +0000
- title: 'Efficient Medical Images Text Detection with Vision-Language Pre-training Approach'
  abstract: 'Text detection in medical images is a critical task, essential for automating the extraction of valuable information from diverse healthcare documents. Conventional text detection methods, predominantly based on segmentation, encounter substantial challenges when confronted with text-rich images, extreme aspect ratios, and multi-oriented text. In response to these complexities, this paper introduces an innovative text detection system aimed at enhancing its efficacy. Our proposed system comprises two fundamental components: the Efficient Feature Enhancement Module (EFEM) and the Multi-Scale Feature Fusion Module (MSFM), both serving as integral elements of the segmentation head. The EFEM incorporates a spatial attention mechanism to improve segmentation performance by introducing multi-level information. The MSFM merges features from the EFEM at different depths and scales to generate final segmentation features. In conjunction with our segmentation methodology, our post-processing module employs a differentiable binarization technique, facilitating adaptive threshold adjustment to enhance text detection precision. To further bolster accuracy and robustness, we introduce the integration of a vision-language pre-training model. Through extensive pretraining on large-scale visual language understanding tasks, this model amasses a wealth of rich visual and semantic representations. When seamlessly integrated with the segmentation module, the pretraining model effectively leverages its potent representation capabilities. Our proposed model undergoes rigorous evaluation on medical text image datasets, consistently demonstrating exceptional performance. Benchmark experiments reaffirm its efficacy.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/li24e.html
  PDF: https://proceedings.mlr.press/v222/li24e/li24e.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-li24e.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Tianyang
    family: Li
  - given: Jinxu
    family: Bai
  - given: Qingzhu
    family: Wang
  - given: Hanwen
    family: Xu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 755-770
  id: li24e
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 755
  lastpage: 770
  published: 2024-02-27 00:00:00 +0000
- title: 'Evolutionary Neural Architecture Search for Multivariate Time Series Forecasting'
  abstract: 'Multivariate time series forecasting is of great importance in a diverse range of domains. In recent years, a variety of spatial-temporal graph neural networks (STGNNs) have been proposed to address this task and achieved promising results. However, these networks are typically handcrafted and require extensive human expertise. Additionally, the temporal and spatial dependencies hidden within different scenarios vary, making it difficult for them to adapt to different scenarios. In this paper, we propose an evolutionary neural architecture search framework, entitled EMTSF, for automated STGNN design. Specifically, we employ fine-grained neural architecture search into both the spatial convolution module and the temporal convolution module. For the spatial convolution search space, various feature filtering and neighbor aggregation operations are employed to find the most suitable message-passing mechanism for modeling the spatial dependencies. For the temporal convolution search space, gated temporal convolutions with different kernel sizes are utilized to best capture temporal dependencies with various ranges. The spatial convolution module and temporal convolution module are jointly optimized with the proposed evolutionary search algorithm to heuristically identify the optimal STGNN architecture. Extensive experiments on four commonly used benchmark datasets show EMTSF achieves promising performance compared with the state-of-the-art methods, which demonstrates the effectiveness of the proposed framework.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/liang24a.html
  PDF: https://proceedings.mlr.press/v222/liang24a/liang24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-liang24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Zixuan
    family: Liang
  - given: Yanan
    family: Sun
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 771-786
  id: liang24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 771
  lastpage: 786
  published: 2024-02-27 00:00:00 +0000
- title: 'Dynamic Offset Metric on Heterogeneous Information Networks for Cold-start Recommendation'
  abstract: 'The cold-start problem poses a significant challenge in recommendation systems, particularly when interaction data is scarce. While meta-learning has shown promise in few-shot classification, its application to cold-start recommendations has mostly involved simple transplantations of generic approaches. The effectiveness of metric learning, a powerful meta-learning method, is hindered by differences in problem definition when applied to rating prediction. Heterogeneous information networks (HINs), as high-order graph structures, can capture valuable semantic information even in data-starved conditions. Efficient utilization of HINs can alleviate the cold-start dilemma. However, in the cold-start domain, there is a lack of dynamic node-level and semantic-level feature fusion schemes, resulting in the underutilization of complex information. This study addresses these issues by combining metric learning and HINs, proposing OMHIN (Dynamic Offset Metric approach to Heterogeneous Information Networks). Our approach transforms a direct similarity metric into an indirect metric to enhance model robustness. By flexibly applying one-dimensional convolution, OMHIN effectively integrates rich information from HINs while minimizing noise introduction. Experimental results on two datasets demonstrate that OMHIN achieves state-of-the-art performance in various scenarios, particularly in complex and challenging situations. It is especially suitable for sequence cold-start recommendations.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/liu24a.html
  PDF: https://proceedings.mlr.press/v222/liu24a/liu24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-liu24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Mingshi
    family: Liu
  - given: Xiaoru
    family: Wang
  - given: Zhihong
    family: Yu
  - given: Fu
    family: Li
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 787-802
  id: liu24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 787
  lastpage: 802
  published: 2024-02-27 00:00:00 +0000
- title: 'Overcoming catastrophic forgetting with classifier expander'
  abstract: 'It is essential for models to gradually adapt to the world’s increasing complexity, and we can use models more effectively if they keep up with the times. However, the technique of continuous learning (CL) has an issue with catastrophic forgetting, and effective continuous learning methods can only be attained by effectively limiting forgetting and learning new tasks. In this study, we offer the Classifier Expander(CE) method, which combines the regularization-based and replay-based approaches. By undergoing two stages of training, it fulfills the aforementioned standards. The training content for the new task is limited to the portion of the network relevant to that task in the first stage, which uses the replay approach to reduce the forgetting problem. This strategy minimizes disruption to the old task while facilitating efficient learning of the new one. Utilizing all of the data available, the second stage retrains the network and sufficiently trains the classifier to balance the learning performance of the old and new tasks. Our method regularly outperforms previous CL methods on the CIFAR-100 and CUB-200 datasets, obtaining an average improvement of 2.94% on the class-incremental learning and 1.16% on the task-incremental learning compared to the best method currently available. Our code is available at https://github.com/EmbraceTomorrow/CE.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/liu24b.html
  PDF: https://proceedings.mlr.press/v222/liu24b/liu24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-liu24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Xinchen
    family: Liu
  - given: Hongbo
    family: Wang
  - given: Yingjian
    family: Tian
  - given: Linyao
    family: Xie
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 803-817
  id: liu24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 803
  lastpage: 817
  published: 2024-02-27 00:00:00 +0000
- title: 'KURL: A Knowledge-Guided Reinforcement Learning Model for Active Object Tracking'
  abstract: 'Recent studies have shown that active object tracking algorithms based on deep reinforcement learning have the difficulty of model training while achieving favorable tracking outcomes. In addition, current active object tracking methods are not suitable for air-to-ground object tracking scenarios in high-altitude environments, such as air search and rescue. Therefore, we proposed a Knowledge-gUided Reinforcement learning (KURL) model for active object tracking, which includes two embedded knowledge-guided models (i.e., the state recognition model and the world model), together with a reinforcement learning module. The state recognition model utilizes the correlation between the observed states and image quality (as measured by object recognition probability) as prior knowledge to guide reinforcement learning algorithm to improve the observed image quality. The reinforcement learning module actively controls the Pan-Tilt-Zoom (PTZ) camera to achieve stable tracking. Additionally, a world model is proposed to replace the traditional Unreal Engine (UE) simulator for model training, which significantly enhancing the training efficiency (about ten times). The results indicate that the KURL model can significantly enhance the image quality, stability and robustness of tracking, compared with other methods in similar tasks.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/liu24c.html
  PDF: https://proceedings.mlr.press/v222/liu24c/liu24c.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-liu24c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Xin
    family: Liu
  - given: Jie
    family: Tan
  - given: Xiaoguang
    family: Ren
  - given: Weiya
    family: Ren
  - given: Huadong
    family: Dai
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 818-833
  id: liu24c
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 818
  lastpage: 833
  published: 2024-02-27 00:00:00 +0000
- title: 'Multi-objective Adaptive Dynamics Attention Model to Solve Multi-objective Vehicle Routing Problem'
  abstract: 'Multi-objective combinatorial optimization problems (MOCOP) are commonly encountered in everyday life. However, finding the optimal solution through traditional exact and heuristic algorithms can be time-consuming due to its NP-hard nature. Fortunately, deep reinforcement learning (DRL) has shown promise in solving complex combinatorial optimization problems (COP). In this paper, we introduce a new Multi-objective Adaptive Dynamics Attention Model (MOADAM) that aims to better approximate the whole Pareto set. We modify the encoder and decoder of the model to better utilize dynamic information, and we also design a new weight sampling method to improve the model’s performance for extreme solutions. Our experimental results demonstrate that our proposed model outperforms the current state-of-the-art algorithm in terms of solution quality on multi-objective vehicle routing problems with capacity constraints (MOCVRP).'
  volume: 222
  URL: https://proceedings.mlr.press/v222/luo24a.html
  PDF: https://proceedings.mlr.press/v222/luo24a/luo24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-luo24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Guang
    family: Luo
  - given: Jianping
    family: Luo
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 834-849
  id: luo24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 834
  lastpage: 849
  published: 2024-02-27 00:00:00 +0000
- title: 'TFAN: Temporal-Feature correlations Attention-based Network for Urban Air Quality Prediction using Data Fusion technology'
  abstract: 'Air pollution raises a detrimental impact on human health and natural environment. Accurate prediction of air quality is crucial for effective pollution control and mitigation strategies. Numerous existing methods for analyzing the variation tendency of a specific air component primarily focus on its temporal and spatial information, neglecting the potential interactions between different attributes within the same time interval. In this paper, we propose a Temporal-Feature correlations Attention-based deep learning Network (TFAN), which incorporates data fusion technology. TFAN focuses on capturing temporal dependencies, feature correlations, and the potential relationship between temporal-feature through the Attention mechanism, and the data fusion method allows for a comprehensive consideration of multiple factors on prediction. Experimental results conducted using real-world data from Beijing City demonstrate that TFAN outperforms various baseline models in prediction accuracy for multiple pollutants by 10+%.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ma24a.html
  PDF: https://proceedings.mlr.press/v222/ma24a/ma24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ma24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Siyuan
    family: Ma
  - given: Fan
    family: Zhang
  - given: Wanli
    family: Hou
  - given: Yarui
    family: Li
  - given: Wei
    family: Song
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 850-865
  id: ma24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 850
  lastpage: 865
  published: 2024-02-27 00:00:00 +0000
- title: 'VMLC: Statistical Process Control for Image Classification in Manufacturing'
  abstract: 'Through ground-breaking advances in Machine Learning its real-world applications have become commonplace in many areas over the past decade. Deep and complex models are able to solve difficult tasks with super-human precision. But for manufacturing quality control, in theory a ideal match for these methods, the step from proof-of-concept towards live deployment is often not feasible. One major obstacle is the unreliability of Machine Learning predictions when confronted with data diverging from the known characteristics. While overall accuracy is high, wrong results may be returned with no indication of their uncertainty. In manufacturing, where scarce errors mean great damages, additional safety measures are required. In this work, I present Visual Machine Learning Control (VMLC), an approach developed upon a real world visual quality control system that operates in a high throughput manufacturing line. Instead of applying sole classification or anomaly detection, both is done in combination. A scalar metric derived from an Auto-Encoder reconstruction error measures the compliance of captured images with the training data the system is trained on. This metric is integrated into the widely used framework of industrial Statistical Process Control (SPC), significantly increasing robustness through meaningful control limits and enabling active learning. The system is evaluated on a large dataset of real-world industrial welding images.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/mascha24a.html
  PDF: https://proceedings.mlr.press/v222/mascha24a/mascha24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-mascha24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Philipp
    family: Mascha
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 866-881
  id: mascha24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 866
  lastpage: 881
  published: 2024-02-27 00:00:00 +0000
- title: 'A Mixed-Precision Quantization Method without Accuracy Degradation Using Semilayers'
  abstract: 'Reducing the memory usage and computational complexity of high-performance deep neural networks while minimizing degradation of accuracy is a key issue in implementing these models on edge devices. To address this issue, partial quantization methods have been proposed to partially reduce the weight parameters of neural network models. However, the accuracy of existing methods degrades rapidly with increasing compression ratio. Although retraining can compensate for this issue to some extent, it is computationally very expensive. In this study, we propose a mixed-precision quantization algorithm without retraining or degradation in accuracy. In the proposed method, first, the difference between values after and before quantization losses of each channel in the layers of the pretrained model is calculated for all channels. Next, the layers are divided into two groups called semilayers according to whether the loss difference is positive or negative. The priorities for quantization in the semilayers are determined based on the Kulback-Leibler divergence derived from the probability distribution of the softmax output after and before quantization. The same process is repeated as a mixed-precision quantization while gradually decreasing the bitwidth, for example, with 8-, 6-, and 4-bit quantizations, and so forth. The results of an experimental evaluation show that the proposed method successfully compressed a ResNet-18 model by 81.44%, a ResNet-34 model by 84.25%, and a ResNet-50 model by 80.39% on image classification tasks using the ImageNet dataset, and a ResNet-18 model by 80.56% on image classification tasks using the CIFAR-10 dataset, with no degradation of the inference accuracy of the pretrained models.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/matsumoto24a.html
  PDF: https://proceedings.mlr.press/v222/matsumoto24a/matsumoto24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-matsumoto24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Kengo
    family: Matsumoto
  - given: Tomoya
    family: Matsuda
  - given: Atsuki
    family: Inoue
  - given: Hiroshi
    family: Kawaguchi
  - given: Yasufumi
    family: Sakai
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 882-894
  id: matsumoto24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 882
  lastpage: 894
  published: 2024-02-27 00:00:00 +0000
- title: 'Maximization of Minimum Weighted Hamming Distance between Set Pairs'
  abstract: '\emph{Finding diverse solutions} to combinatorial optimization problems is beneficial for a deeper understanding of complicated real-world problems and for simpler and more practical mathematical modeling. For this purpose, it is desirable that every solution is far away from one another and that solutions can be found in time not depending polynomially on the size of the family of feasible solutions. In this paper, we investigate the problem of finding diverse sets in the sense of maximizing the \emph{minimum} of \emph{weighted Hamming distance} between \emph{set pairs}. Under a particular assumption, we provide an algorithm that gives diverse sets of almost $\mu /2$-approximation in expectation in the sense of maximization of the minimum of the expected value, where $\mu \in [0, 1]$ is a parameter on a subroutine. We further give a hardness result that any approximation ratio better than $2/3$ is impossible in polynomial time under the assumption of $\mathrm{P} \neq \mathrm{NP}$.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/matsuoka24a.html
  PDF: https://proceedings.mlr.press/v222/matsuoka24a/matsuoka24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-matsuoka24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Tatsuya
    family: Matsuoka
  - given: Shinji
    family: Ito
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 895-910
  id: matsuoka24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 895
  lastpage: 910
  published: 2024-02-27 00:00:00 +0000
- title: 'Q-Match: Self-Supervised Learning by Matching Distributions Induced by a Queue'
  abstract: 'In semi-supervised learning, student-teacher distribution matching has been successful in improving performance of models using unlabeled data in conjunction with few labeled samples. In this paper, we aim to replicate that success in the self-supervised setup where we do not have access to any labeled data during pre-training. We introduce our algorithm, Q-Match, and show it is possible to induce the student-teacher distributions without any knowledge of downstream classes by using a queue of embeddings of samples from the unlabeled dataset. We focus our study on tabular datasets and show that Q-Match outperforms previous self-supervised learning techniques when measuring downstream classification performance. Furthermore, we show that our method is sample efficient–in terms of both the labels required for downstream training and the amount of unlabeled data required for pre-training–and scales well to the sizes of both the labeled and unlabeled data.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/mulc24a.html
  PDF: https://proceedings.mlr.press/v222/mulc24a/mulc24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-mulc24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Thomas
    family: Mulc
  - given: Debidatta
    family: Dwibedi
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 911-926
  id: mulc24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 911
  lastpage: 926
  published: 2024-02-27 00:00:00 +0000
- title: 'Free Energy of Bayesian Convolutional Neural Network with Skip Connection'
  abstract: 'Since the success of Residual Network(ResNet), many of architectures of Convolutional Neural Networks(CNNs) have adopted skip connection. While the generalization performance of CNN with skip connection has been explained within the framework of Ensemble Learning, the dependency on the number of parameters has not been revealed. In this paper, we show that Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning. Bayesian Free Energy is the negative log marginal likelihood which is equivalent to Stochastic Complexity or Minimum Description Length (MDL) used for evaluating model complexity. The upper bound of free energy of Bayesian CNN with skip connection does not depend on the oveparametrization and, the generalization error of Bayesian CNN has similar property.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/nagayasu24a.html
  PDF: https://proceedings.mlr.press/v222/nagayasu24a/nagayasu24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-nagayasu24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Shuya
    family: Nagayasu
  - given: Sumio
    family: Watanabe
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 927-942
  id: nagayasu24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 927
  lastpage: 942
  published: 2024-02-27 00:00:00 +0000
- title: 'Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee'
  abstract: 'A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing an acquisition function that is defined as a measure of confidence of the function being higher or lower than the given threshold. A theoretical analysis for the convergence of the algorithm to an accurate solution is provided. On multiple synthetic and real-world datasets, our algorithm successfully outperforms state-of-the-art methods.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ngo24a.html
  PDF: https://proceedings.mlr.press/v222/ngo24a/ngo24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ngo24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Giang
    family: Ngo
  - given: Dang
    family: Nguyen
  - given: Dat
    family: Phan-Trong
  - given: Sunil
    family: Gupta
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 943-958
  id: ngo24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 943
  lastpage: 958
  published: 2024-02-27 00:00:00 +0000
- title: 'Empirical Study of Federated Unlearning: Efficiency and Effectiveness'
  abstract: 'The right to be forgotten (RTBF) is a concept that pertains to an individual’s right to request the removal or deletion of their personal information when it is no longer necessary, relevant, or accurate for the purposes for which it was initially collected. Machine Learning (ML) models often rely on large, diverse datasets for optimal performance. Hence, when an individual exercises the RTBF, it can impact the ML model’s performance and accuracy. In the context of Federated Learning (FL), where a server trains a model across multiple decentralized devices without moving data away from clients, implementing the RTBF in FL presents some unique challenges compared to traditional ML approaches. For instance, the decentralized nature makes it challenging to identify and remove specific user data from the model. Although various unlearning methods have been proposed in the literature, they have not been well investigated from the efficiency perspective. To fill this gap, this paper presents an empirical study to investigate the impacts of various unlearning methods. Our experiments are designed in diverse scenarios involving multiple communication and unlearning rounds using three datasets, MNIST, CIFAR-10, and CIFAR-100. We utilize backdoor attack and Cosine Similarity to assess the effectiveness of each unlearning method. The findings and insights from this research can be integrated into FL systems to enhance their overall performance and effectiveness. Our research codes are available on GitHub at \url{https://github.com/sail-research/fed-unlearn}.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/nguyen24a.html
  PDF: https://proceedings.mlr.press/v222/nguyen24a/nguyen24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-nguyen24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Thai-Hung
    family: Nguyen
  - given: Hong-Phuc
    family: Vu
  - given: Dung Thuy
    family: Nguyen
  - given: Tuan Minh
    family: Nguyen
  - given: Khoa D
    family: Doan
  - given: Kok-Seng
    family: Wong
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 959-974
  id: nguyen24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 959
  lastpage: 974
  published: 2024-02-27 00:00:00 +0000
- title: 'The Importance of Anti-Aliasing in Tiny Object Detection'
  abstract: 'Tiny object detection has gained considerable attention in the research community owing to the frequent occurrence of tiny objects in numerous critical real-world scenarios. However, convolutional neural networks (CNNs) used as the backbone for object detection architectures typically neglect Nyquist’s sampling theorem during down-sampling operations, resulting in aliasing and degraded performance. This is likely to be a particular issue for tiny objects that occupy very few pixels and therefore have high spatial frequency features. This paper applied an existing approach WaveCNet for anti-aliasing to tiny object detection. WaveCNet addresses aliasing by replacing standard down-sampling processes in CNNs with Wavelet Pooling (WaveletPool) layers, effectively suppressing aliasing. We modify the original WaveCNet to apply WaveletPool in a consistent way in both pathways of the residual blocks in ResNets. Additionally, we also propose a bottom-heavy version of the backbone, which further improves the performance of tiny object detection while also reducing the required number of parameters by almost half. Experimental results on the TinyPerson, WiderFace, and DOTA datasets demonstrate the importance of anti-aliasing in tiny object detection and the effectiveness of the proposed method which achieves new state-of-the-art results on all three datasets. Codes and experiment results are released at \url{https://github.com/freshn/Anti-aliasing-Tiny-Object-Detection.git}.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/ning24a.html
  PDF: https://proceedings.mlr.press/v222/ning24a/ning24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-ning24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Jinlai
    family: Ning
  - given: Michael
    family: Spratling
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 975-990
  id: ning24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 975
  lastpage: 990
  published: 2024-02-27 00:00:00 +0000
- title: 'Transformed Gaussian Processes for Characterizing a Model’s Discrepancy'
  abstract: 'Mathematical models of observational phenomena are at the core of experimental sciences. By learning the parameters of such models from typically noisy observations, we can interpret and predict the phenomena under investigation. This process, however, assumes that the model itself is correct and that we are only uncertain of its parameters. In practice, this is rarely true, but rather the model is a simplification of the actual generative process. One proposed remedy is a post hoc investigation of how the model differs from reality, by explicitly modeling the discrepancy between the two. In this paper, we use transformed Gaussian processes as flexible models for this. Our formulation relaxes the assumption on the correctness of the model by assuming it is only correct in expectation, and it directly supports both additive and multiplicative corrections, treated separately in the literature, using suitable transformations. We demonstrate the approach in two example cases: modeling human growth (relation age-height) and modeling the risk attitude (relation reward-utility). The former provides a simple example, while the second case highlights the importance of the transformations in obtaining meaningful information about the discrepancy.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/nioche24a.html
  PDF: https://proceedings.mlr.press/v222/nioche24a/nioche24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-nioche24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Aurélien
    family: Nioche
  - given: Ville
    family: Tanskanen
  - given: Marcelo
    family: Hartmann
  - given: Arto
    family: Klami
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 991-1006
  id: nioche24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 991
  lastpage: 1006
  published: 2024-02-27 00:00:00 +0000
- title: 'Frequency-dependent Image Reconstruction Error for Micro Defect Detection'
  abstract: 'Micro defects, such as casting pores in industrial products, have been detected by human visual inspection using X-ray CT images and image processing tools. Automatic detection of micro defects is challenging for anomaly detection methods using image reconstruction errors and nearest neighbor distances because these metrics are dominated by low-frequency information and are insensitive to minor defects. Although recent methods achieve high anomaly detection performances, their detection abilities are insufficient for micro defects. To overcome these problems, we propose to extend a state-of-the-art anomaly detection method by introducing frequency-dependent losses to capture reconstruction errors appearing around micro defects and frequency-dependent data augmentation to improve the sensitivity against the errors. We demonstrate the effectiveness of the proposed method through experiments with MVTec AD dataset especially on the detection of micro defects.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/nomura24a.html
  PDF: https://proceedings.mlr.press/v222/nomura24a/nomura24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-nomura24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuhei
    family: Nomura
  - given: Hirotaka
    family: Hachiya
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1007-1022
  id: nomura24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1007
  lastpage: 1022
  published: 2024-02-27 00:00:00 +0000
- title: 'Selective Nonparametric Regression via Testing'
  abstract: 'Prediction with the possibility of abstention (or selective prediction) is an important problem for error-critical machine learning applications. While well-studied in the classification setup, selective approaches to regression are much less developed. In this work, we consider the nonparametric heteroskedastic regression problem and develop an abstention procedure via testing the hypothesis on the value of the conditional variance at a given point. Unlike existing methods, the proposed one allows to account not only for the value of the variance itself but also for the uncertainty of the corresponding variance predictor. We prove non-asymptotic bounds on the risk of the resulting estimator and show the existence of several different convergence regimes. Theoretical analysis is illustrated with a series of experiments on simulated and real-world data.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/noskov24a.html
  PDF: https://proceedings.mlr.press/v222/noskov24a/noskov24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-noskov24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Fedor
    family: Noskov
  - given: Alexander
    family: Fishkov
  - given: Maxim
    family: Panov
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1023-1038
  id: noskov24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1023
  lastpage: 1038
  published: 2024-02-27 00:00:00 +0000
- title: 'Domain Generalization with Interpolation Robustness'
  abstract: 'Domain generalization (DG) uses multiple source (training) domains to learn a model that generalizes well to unseen domains. Existing approaches to DG need more scrutiny over (i) the ability to imagine data beyond the source domains and (ii) the ability to cope with the scarcity of training data. To address these shortcomings, we propose a novel framework - \emph{interpolation robustness}, where we view each training domain as a point on a domain manifold and learn class-specific representations that are domain invariant across all interpolations between domains. We use this representation to propose a generic domain generalization approach that can be seamlessly combined with many state-of-the-art methods in DG. Through extensive experiments, we show that our approach can enhance the performance of several methods in the conventional and the limited training data setting.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/palakkadavath24a.html
  PDF: https://proceedings.mlr.press/v222/palakkadavath24a/palakkadavath24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-palakkadavath24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Ragja
    family: Palakkadavath
  - given: Thanh
    family: Nguyen-Tang
  - given: Hung
    family: Le
  - given: Svetha
    family: Venkatesh
  - given: Sunil
    family: Gupta
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1039-1054
  id: palakkadavath24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1039
  lastpage: 1054
  published: 2024-02-27 00:00:00 +0000
- title: 'Enhancing Model Generalization of Cervical Fluid-Based Cell Detection through Causal Feature Extraction:A Novel Method'
  abstract: 'Cervical cancer is the most common gynecologic malignancy, and in clinical practice, cervical cancer is best treated if it is detected at an early stage. Thinprep Cytologic Test (TCT) is the best early detection method for cervical cancer as determined by the WHO. As the coverage of early detection of cervical cancer increases, the number of samples in hospitals increases annually, and the pressure on the pathologists to read the cytological images increases, which easily leads to an increase in the rate of misdiagnosis and missed diagnosis. Therefore, automatic detection of abnormal cells in cervical cytology images of cervical fluid using deep learning techniques has become a hot research topic today. However, existing deep learning models for cell detection often collect a single data source from a medical institution for construction. Different medical institutions have different equipment and staining methods, and the accuracy, magnification, and staining results of the images obtained will be different. As a result, the application performance of the model in different medical institution data is not good, and there is a problem of domain shift. To address these problems, this paper proposes a method for cervical fluid-based cell detection based on causal feature extraction. The method is based on the one-stage detection model RetinaNet, and incorporates causal autoencoder to learn the invariant causal feature representation from data. It reduces the impact of task-irrelevant feature representations, reduces the variability of feature distributions in different datasets, and effectively solves the domain shift problem. The addition of deformable convolution and attention mechanism enhances the feature extraction capability for foreground categories with variable shapes in cervical fluid-based pathology images. This reduces the impact of possible strong correlation between background features and goal cells, and reduces the interference of the foreground categories by fading and lack of brightness in the staining. The generalization ability of the model is improved, which makes the model better applicable to different medical institutions. The experimental results show that the method in this paper not only improves the accuracy of the model detection, but also verifies its good generalization effect on different datasets.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/pan24a.html
  PDF: https://proceedings.mlr.press/v222/pan24a/pan24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-pan24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Qiao
    family: Pan
  - given: Bin
    family: Yang
  - given: Dehua
    family: Chen
  - given: Mei
    family: Wang
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1055-1070
  id: pan24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1055
  lastpage: 1070
  published: 2024-02-27 00:00:00 +0000
- title: 'Simple and Efficient Vision Backbone Adapter for Image Semantic Segmentation'
  abstract: 'Utilizing a pretrained vision backbone to finetune a model for semantic segmentation is common practice in computer vision. However, there are few works intending to enlarge the semantic context learning capacity by incorporating a segmentation adapter into the backbone. Thus, in this paper, we present a simple but efficient segmentation adapter, termed as SegAdapter, which can be plugged into the pretrained vision backbone to improve the performance of existing models for image semantic segmentation. We summarize SegAdapter with three attractive advantages: 1) SegAdapter is a plug-and-play module demonstrating strong adaptability in CNN and Transformer based models such as ConvNext and Segformer, 2) SegAdapter applies a light-weight High-order Spatial Attention (HSA) to make use of intermediate features from the pretrained backbone which extends the model depth and produces auxiliary segmentation maps for model enhancement, 3) SegAdapter builds a powerful vision backbone by incorporating the semantic context into each stage which takes on some of the functions of the segmentation head. So, SegAdapter augmented model can be used in simple designed decode head to avoid heavy computational cost. By plugging multiple SegAdapter layers into different vision backbones, we construct a series of SegAdapter-based segmentation models. We show through the extensive experiments that SegAdapter can be used with mainstream backbones like CNN and Transformer to improve mIoU performance in a large margin while introducing minimal additional parameters and FLOPs.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/peng24a.html
  PDF: https://proceedings.mlr.press/v222/peng24a/peng24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-peng24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Dingjie
    family: Peng
  - given: Wataru
    family: Kameyama
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1071-1086
  id: peng24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1071
  lastpage: 1086
  published: 2024-02-27 00:00:00 +0000
- title: 'The Fine Print on Tempered Posteriors'
  abstract: 'We conduct a detailed investigation of tempered posteriors and uncover a number of crucial and previously undiscussed points. Contrary to previous results, we first show that for realistic models and datasets and the tightly controlled case of the Laplace approximation to the posterior, stochasticity does not in general improve test accuracy. The coldest temperature is often optimal. One might think that Bayesian models with some stochasticity can at least obtain improvements in terms of calibration. However, we show empirically that when gains are obtained this comes at the cost of degradation in test accuracy. We then discuss how targeting Frequentist metrics using Bayesian models provides a simple explanation of the need for a temperature parameter $\lambda$ in the optimization objective. Contrary to prior works, we finally show through a PAC-Bayesian analysis that the temperature $\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/pitas24a.html
  PDF: https://proceedings.mlr.press/v222/pitas24a/pitas24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-pitas24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Konstantinos
    family: Pitas
  - given: Julyan
    family: Arbel
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1087-1102
  id: pitas24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1087
  lastpage: 1102
  published: 2024-02-27 00:00:00 +0000
- title: 'Folded Hamiltonian Monte Carlo for Bayesian Generative Adversarial Networks'
  abstract: 'Probabilistic modelling on Generative Adversarial Networks (GANs) within the Bayesian framework has shown success in estimating the complex distribution in literature. In this paper, we develop a Bayesian formulation for unsupervised and semi-supervised GAN learning. Specifically, we propose Folded Hamiltonian Monte Carlo (F-HMC) methods within this framework to learn the distributions over the parameters of the generators and discriminators. We show that the F-HMC efficiently approximates multi-modal and high dimensional data when combined with Bayesian GANs. Its composition improves run time and test error in generating diverse samples. Experimental results with high-dimensional synthetic multi-modal data and natural image benchmarks, including CIFAR-10, SVHN and ImageNet, show that F-HMC outperforms the state-of-the-art methods in terms of test error, run times per epoch, inception score and Frechet Inception Distance scores.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/pourshahrokhi24a.html
  PDF: https://proceedings.mlr.press/v222/pourshahrokhi24a/pourshahrokhi24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-pourshahrokhi24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Narges
    family: Pourshahrokhi
  - given: Yunpeng
    family: Li
  - given: Samaneh
    family: Kouchaki
  - given: Payam
    family: Barnaghi
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1103-1118
  id: pourshahrokhi24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1103
  lastpage: 1118
  published: 2024-02-27 00:00:00 +0000
- title: 'Multi-behavior Session-based Recommendation via Graph Reinforcement Learning'
  abstract: 'Multi-behavior session-based recommendation (MBSBR) is a critical task in e-commerce and online advertising. By modeling these multiple behaviors, models can better capture the user intent and make more effective recommendations. However, existing models face the challenge of incompletely differentiating between different behavior types, which hinders their ability to fully capture the different tendencies exhibited by each behavior. In addition, most existing multi-behavior methods focus only on predicting a single target behavior and fail to achieve a unified model for predicting the next user-item interaction across multiple behavior types. To address these limitations, we introduce reinforcement learning to the multi-behavior session-based recommendation task and propose a novel approach called the multi-behavior graph reinforcement learning network (MB-GRL). Specifically, we use a graph neural network to encode item transition information from the session graph. Then, we use an attention network to obtain a session representation and generate recommendations based on it. At the same time, we also apply Deep Q-Network (DQN) as a regularizer to improve the recommendation performance for certain behavior types. Experiments on various public benchmark datasets show that MB-GRL outperforms other models for multi-behavior session-based recommendation.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/qin24a.html
  PDF: https://proceedings.mlr.press/v222/qin24a/qin24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-qin24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Shuo
    family: Qin
  - given: Feng
    family: Lin
  - given: Lingxiao
    family: Xu
  - given: Bowen
    family: Deng
  - given: Siwen
    family: Li
  - given: Fangcheng
    family: Yang
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1119-1134
  id: qin24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1119
  lastpage: 1134
  published: 2024-02-27 00:00:00 +0000
- title: 'Remote Wildfire Detection using Multispectral Satellite Imagery and Vision Transformers'
  abstract: 'Wildfires pose a significant and recurring challenge in North America, impacting both human and natural environments. The size and severity of wildfires in the region have been increasing in recent years, making it a pressing concern for communities, ecosystems, and the economy. The accurate and timely detection of active wildfires in remote areas is crucial for effective wildfire management and mitigation efforts. In this research paper, we propose a robust approach for detecting active wildfires using multispectral satellite imagery by leveraging vision transformers and a vast repository of landsat-$8$ satellite data with a $30$m spatial resolution in North America. Our methodology involves experimenting with vision transformers and deep convolutional neural networks for wildfire detection in multispectral satellite images. We compare the capabilities of these two architecture families in detecting wildfires within the multispectral satellite imagery. Furthermore, we propose a novel u-shape vision transformer that effectively captures spatial dependencies and learns meaningful representations from multispectral images, enabling precise discrimination between wildfire and non-wildfire regions. To evaluate the performance of our approach, we conducted experiments on a comprehensive dataset of wildfire incidents. The results demonstrate the effectiveness of the proposed method in accurately detecting active wildfires with an \textit{Dice Score or F$1$} of $%90.05$ and \textit{Recall} of $%89.61$ . Overall, our research presents a promising approach for leveraging vision transformers for multispectral satellite imagery to detect remote wildfires.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/rad24a.html
  PDF: https://proceedings.mlr.press/v222/rad24a/rad24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-rad24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Ryan
    family: Rad
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1135-1150
  id: rad24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1135
  lastpage: 1150
  published: 2024-02-27 00:00:00 +0000
- title: 'Distilling Influences to Mitigate Prediction Churn in Graph Neural Networks'
  abstract: 'Models with similar performances exhibit significant disagreement in the predictions of individual samples, referred to as prediction churn. Our work explores this phenomenon in graph neural networks by investigating differences between models differing only in their initializations in their utilized features for predictions. We propose a novel metric called Influence Difference (ID) to quantify the variation in reasons used by nodes across models by comparing their influence distribution. Additionally, we consider the differences between nodes with a stable and an unstable prediction, positing that both equally utilize different reasons and thus provide a meaningful gradient signal to closely match two models even when the predictions for nodes are similar. Based on our analysis, we propose to minimize this ID in Knowledge Distillation, a domain where a new model should closely match an established one. As an efficient approximation, we introduce DropDistillation (DD) that matches the output for a graph perturbed by edge deletions. Our empirical evaluation of six benchmark datasets for node classification validates the differences in utilized features. DD outperforms previous methods regarding prediction stability and overall performance in all considered Knowledge Distillation experiments.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/roth24a.html
  PDF: https://proceedings.mlr.press/v222/roth24a/roth24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-roth24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Andreas
    family: Roth
  - given: Thomas
    family: Liebig
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1151-1166
  id: roth24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1151
  lastpage: 1166
  published: 2024-02-27 00:00:00 +0000
- title: 'Logarithmic regret in communicating MDPs: Leveraging known dynamics with bandits'
  abstract: 'We study regret minimization in an average-reward and communicating Markov Decision Process (MDP) with known dynamics, but unknown reward function. Although learning in such MDPs is a priori easier than in fully unknown ones, they are still largely challenging as they include as special cases large classes of problems such as combinatorial semi-bandits. Leveraging the knowledge on transition function in regret minimization, in a statistically efficient way, appears largely unexplored. As it is conjectured that achieving exact optimality in generic MDPs is NP-hard, even with known transitions, we focus on a computationally efficient relaxation, at the cost of achieving order-optimal logarithmic regret instead of exact optimality. We contribute to filling this gap by introducing a novel algorithm based on the popular Indexed Minimum Empirical Divergence strategy for bandits. A key component of the proposed algorithm is a carefully designed stopping criterion leveraging the recurrent classes induced by stationary policies. We derive a non-asymptotic, problem-dependent, and logarithmic regret bound for this algorithm, which relies on a novel regret decomposition leveraging the structure. We further provide an efficient implementation and experiments illustrating its promising empirical performance.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/saber24a.html
  PDF: https://proceedings.mlr.press/v222/saber24a/saber24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-saber24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Hassan
    family: Saber
  - given: Fabien
    family: Pesquerel
  - given: Odalric-Ambrym
    family: Maillard
  - given: Mohammad Sadegh
    family: Talebi
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1167-1182
  id: saber24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1167
  lastpage: 1182
  published: 2024-02-27 00:00:00 +0000
- title: 'Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings'
  abstract: 'Unsupervised learning allows us to leverage unlabelled data, which has become abundantly available, and to create embeddings that are usable on a variety of downstream tasks. However, the typical lack of interpretability of unsupervised representation learning has become a limiting factor with regard to recent transparent-AI regulations. In this paper, we study graph representation learning and we show that data augmentation that preserves semantics can be learned and used to produce interpretations. Our framework, which we named INGENIOUS, creates inherently interpretable embeddings and eliminates the need for costly additional post-hoc analysis. We also introduce additional metrics addressing the lack of formalism and metrics in the understudied area of unsupervised-representation-learning interpretability. Our results are supported by an experimental study applied to both graph-level and node-level tasks and show that interpretable embeddings provide state-of-the-art performance on subsequent downstream tasks.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/scafarto24a.html
  PDF: https://proceedings.mlr.press/v222/scafarto24a/scafarto24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-scafarto24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Gregory
    family: Scafarto
  - given: Madalina
    family: Ciortan
  - given: Simon
    family: Tihon
  - given: Quentin
    family: Ferre
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1183-1198
  id: scafarto24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1183
  lastpage: 1198
  published: 2024-02-27 00:00:00 +0000
- title: 'A New Perspective On the Expressive Equivalence Between Graph Convolution and Attention Models'
  abstract: 'Graph neural networks (GNNs) have demonstrated impressive achievements in diverse graph tasks, and research on their expressive power has experienced significant growth in recent years. The well-known Weisfeiler and Lehman (WL) isomorphism test has been widely used to assess GNNs’ ability to distinguish graph structures. However, despite being considered less expressive than other GNNs in graph-level tasks based on the WL test, two prominent GNN models, namely graph convolution networks (GCN) and attention-based graph networks (GAT), still exhibit strong performance in node-level classification tasks. In this paper, we present a comprehensive analysis of their expressive power using a novel evaluation metric: the number of linear regions. We demonstrate that by enhancing GCN with refined graph Ricci curvature, our proposed high-rank graph convolution network (HRGCN) can match or even surpass the prediction advantage of attention models. Thus, the two models exhibit equivalent node-level expressive powers. This fresh perspective highlights the evaluation of GNNs’ expressive power in node-level classifications rather than solely at the graph level. Experimental results showcase that the proposed HRGCN model outperforms the state-of-the-art in various classification and prediction tasks.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/shi24a.html
  PDF: https://proceedings.mlr.press/v222/shi24a/shi24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-shi24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Dai
    family: Shi
  - given: Zhiqi
    family: Shao
  - given: Andi
    family: Han
  - given: Yi
    family: Guo
  - given: Gao
    family: Junbin
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1199-1214
  id: shi24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1199
  lastpage: 1214
  published: 2024-02-27 00:00:00 +0000
- title: 'Edit-A-Video: Single Video Editing with Object-Aware Consistency'
  abstract: 'With advancements in text-to-image (TTI) models, text-to-video (TTV) models have recently been introduced. Motivated by approaches on TTV models adapting from diffusion-based TTI models, we suggest the text-guided video editing framework given only a pretrained TTI model and a single <text, video> pair, which we term Edit-A-Video. The framework consists of two stages: (1) inflating the 2D model into the 3D model by appending temporal modules and tuning on the source video (2) inverting the source video into the noise and editing with target text through attention map injection. Each stage enables the temporal modeling and preservation of semantic attributes of the source video. One of the key challenges for video editing is a background inconsistency problem, where the regions unrelated to the edit suffer from undesirable and inconsistent temporal alterations. To mitigate this issue, we also introduce a novel mask blending method, termed as temporal-consistent blending (TC Blending). We improve previous mask blending methods to reflect the temporal consistency, ensuring that the area where the editing is applied exhibits smooth transition while also achieving spatio-temporal consistency of the unedited regions. We present extensive experimental results over various types of text and videos, and demonstrate the superiority of the proposed method compared to baselines in terms of background consistency, text alignment, and video editing quality. Our samples are available on https://editavideo.github.io.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/shin24a.html
  PDF: https://proceedings.mlr.press/v222/shin24a/shin24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-shin24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Chaehun
    family: Shin
  - given: Heeseung
    family: Kim
  - given: Che Hyun
    family: Lee
  - given: Sang-gil
    family: Lee
  - given: Sungroh
    family: Yoon
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1215-1230
  id: shin24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1215
  lastpage: 1230
  published: 2024-02-27 00:00:00 +0000
- title: 'Advancing Deep Metric Learning With Adversarial Robustness'
  abstract: 'Deep Metric Learning (DML) is a prominent subfield of machine learning with extensive practical applications in learning visual similarities. However, DML systems are vulnerable to input distributions during inference that differ from the training data, such as adversarial examples (AXs). In this paper, we introduce MDProp, a framework that enhances the clean data performance and adversarial robustness of DML models by generating novel Multi-Targeted AXs and Unadversarial Examples, in addition to conventional single-targeted AXs, in the feature space. To handle the input distribution shift caused by the generated novel input distributions, MDProp scales the separate batch normalization layer strategy. Our comprehensive experimental analysis demonstrates that MDProp outperforms current state-of-the-art convolutional neural networks by up to 2.95% in terms of R@1 scores for clean data, while simultaneously improving adversarial robustness by up to 2.12 times. Additionally, MDProp achieves state-of-the-art results in data-scarce setting while utilizing only half of the training data. Implementation is available at \url{https://github.com/intherejeet/MDProp}.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/singh24a.html
  PDF: https://proceedings.mlr.press/v222/singh24a/singh24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-singh24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Inderjeet
    family: Singh
  - given: Kazuya
    family: Kakizaki
  - given: Toshinori
    family: Araki
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1231-1246
  id: singh24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1231
  lastpage: 1246
  published: 2024-02-27 00:00:00 +0000
- title: 'Learning to Terminate in Object Navigation'
  abstract: 'This paper tackles the critical challenge of object navigation in autonomous navigation systems, particularly focusing on the problem of target approach and episode termination in environments with long optimal episode length in Deep Reinforcement Learning (DRL) based methods. While effective in environment exploration and object localization, conventional DRL methods often struggle with optimal path planning and termination recognition due to a lack of depth information. To overcome these limitations, we propose a novel approach, namely the Depth-Inference Termination Agent (DITA), which incorporates a supervised model called the Judge Model to implicitly infer object-wise depth and decide termination jointly with reinforcement learning. We train our judge model along with reinforcement learning in parallel and supervise the former efficiently by reward signal. Our evaluation shows the method is demonstrating superior performance, we achieve a 9.3% gain on success rate than our baseline method across all room types and gain 51.2% improvements on long episodes environment while maintaining slightly better Success Weighted by Path Length (SPL). Code and resources, visualization are available at: \url{https://github.com/HuskyKingdom/DITA_acml2023}'
  volume: 222
  URL: https://proceedings.mlr.press/v222/song24a.html
  PDF: https://proceedings.mlr.press/v222/song24a/song24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-song24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuhang
    family: Song
  - given: Anh
    family: Nguyen
  - given: Chun-Yi
    family: Lee
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1247-1262
  id: song24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1247
  lastpage: 1262
  published: 2024-02-27 00:00:00 +0000
- title: 'Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate Card Transformer'
  abstract: 'Amazon ships billions of packages to its customers annually within the United States. Shipping cost of these packages are used on the day of shipping (day 0) to estimate profitability of sales. Downstream systems utilize these days 0 profitability estimates to make financial decisions, such as pricing strategies and delisting loss-making products. However, obtaining accurate shipping cost estimates on day 0 is complex for reasons like delay in carrier invoicing or fixed cost components getting recorded at monthly cadence. Inaccurate shipping cost estimates can lead to bad decision, such as pricing items too low or high, or promoting the wrong product to the customers. Current solutions for estimating shipping costs on day 0 rely on tree-based models that require extensive manual engineering efforts. In this study, we propose a novel architecture called the Rate Card Transformer (RCT) that uses self-attention to encode all package shipping information such as package attributes, carrier information and route plan. Unlike other transformer-based tabular models, RCT has the ability to encode a variable list of one-to-many relations of a shipment, allowing it to capture more information about a shipment. For example, RCT can encode properties of all products in a package. Our results demonstrate that cost predictions made by the RCT have 28.82% less error compared to tree-based GBDT model. Moreover, the RCT outperforms the state-of-the-art transformer-based tabular model, FTTransformer, by 6.08%. We also illustrate that the RCT learns a generalized manifold of the rate card that can improve the performance of tree-based models.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/sreekar24a.html
  PDF: https://proceedings.mlr.press/v222/sreekar24a/sreekar24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-sreekar24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: P Aditya
    family: Sreekar
  - given: Sahil
    family: Verma
  - given: Varun
    family: Madhavan
  - given: Abhishek
    family: Persad
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1263-1275
  id: sreekar24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1263
  lastpage: 1275
  published: 2024-02-27 00:00:00 +0000
- title: 'Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy'
  abstract: 'Regularization plays a crucial role in machine learning models, especially for deep neural networks. The existing regularization techniques mainly rely on the i.i.d. assumption and only consider the knowledge from the current sample, without the leverage of the neighboring relationship between samples. In this work, we propose a general regularizer called \textbf{Patch-level Neighborhood Interpolation (Pani)} that conducts a non-local representation in the computation of networks. Our proposal explicitly constructs patch-level graphs in different layers and then linearly interpolates neighborhood patch features, serving as a general and effective regularization strategy. Further, we customize our approach into two kinds of popular regularization methods, namely Virtual Adversarial Training (VAT) and MixUp as well as its variants. The first derived \textbf{Pani VAT} presents a novel way to construct non-local adversarial smoothness by employing patch-level interpolated perturbations. The second derived \textbf{Pani MixUp} method extends the MixUp, and achieves superiority over MixUp and competitive performance over state-of-the-art variants of MixUp method with a significant advantage in computational efficiency. Extensive experiments have verified the effectiveness of our Pani approach in both supervised and semi-supervised settings.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/sun24a.html
  PDF: https://proceedings.mlr.press/v222/sun24a/sun24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-sun24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Ke
    family: Sun
  - given: Bing
    family: Yu
  - given: Zhouchen
    family: Lin
  - given: Zhanxing
    family: Zhu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1276-1291
  id: sun24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1276
  lastpage: 1291
  published: 2024-02-27 00:00:00 +0000
- title: 'Meta-forests: Domain generalization on random forests with meta-learning'
  abstract: 'Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called "meta-forests", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study. Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/sun24b.html
  PDF: https://proceedings.mlr.press/v222/sun24b/sun24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-sun24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuyang
    family: Sun
  - given: Panagiotis
    family: Kosmas
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1292-1307
  id: sun24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1292
  lastpage: 1307
  published: 2024-02-27 00:00:00 +0000
- title: 'Hybrid Convolution Method for Graph Classification Using Hierarchical Topology Feature'
  abstract: 'Graph classification is a crucial task in the field of graph learning with numerous practical applications. Typically, the first step is to construct vertex features by the statistical information of the graph. Existing graph neural networks often adopt the one-hot degree encoding strategy to construct vertex features. Then, these features are fed into a linear layer, which outputs a low-dimensional real vector serving as the initial vertex representation for the graph model. However, the conventional approach of constructing vertex features may not be optimal. Intuitively, the method of constructing vertex features can have significant impact on the effectiveness of model. Hence, the construction of informative vertex features from the graph and the design of an efficient graph model to process these features pose great challenges. In this paper, we propose a novel method for constructing hierarchical topology vertex features and designing a hybrid convolution method to handle these features. Experimental results on public graph datasets of Social Networks, Small Molecules, and Bioinformatics demonstrate the superior performance of our method compared to baselines.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/sun24c.html
  PDF: https://proceedings.mlr.press/v222/sun24c/sun24c.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-sun24c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Jiangfeng
    family: Sun
  - given: Xinyue
    family: Lin
  - given: Fangyu
    family: Hao
  - given: Meina
    family: Song
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1308-1320
  id: sun24c
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1308
  lastpage: 1320
  published: 2024-02-27 00:00:00 +0000
- title: 'Intractability of Learning the Discrete Logarithm with Gradient-Based Methods'
  abstract: 'The discrete logarithm problem is a fundamental challenge in number theory with significant implications for cryptographic protocols. In this paper, we investigate the limitations of gradient-based methods for learning the parity bit of the discrete logarithm in finite cyclic groups of prime order. Our main result, supported by theoretical analysis and empirical verification, reveals the concentration of the gradient of the loss function around a fixed point, independent of the logarithm’s base used. This concentration property leads to a restricted ability to learn the parity bit efficiently using gradient-based methods, irrespective of the complexity of the network architecture being trained. Our proof relies on Boas-Bellman inequality in inner product spaces and it involves establishing approximate orthogonality of discrete logarithm’s parity bit functions through the spectral norm of certain matrices. Empirical experiments using a neural network-based approach further verify the limitations of gradient-based learning, demonstrating the decreasing success rate in predicting the parity bit as the group order increases.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/takhanov24a.html
  PDF: https://proceedings.mlr.press/v222/takhanov24a/takhanov24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-takhanov24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Rustem
    family: Takhanov
  - given: Maxat
    family: Tezekbayev
  - given: Artur
    family: Pak
  - given: Arman
    family: Bolatov
  - given: Zhibek
    family: Kadyrsizova
  - given: Zhenisbek
    family: Assylbekov
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1321-1336
  id: takhanov24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1321
  lastpage: 1336
  published: 2024-02-27 00:00:00 +0000
- title: 'Single Image Super-resolution Based On Non-subsampled Shearlet Transform'
  abstract: 'With the development of deep learning, breakthroughs in single image super-resolution have been achieved. However, most existing methods are limited to using only spatial domain information or only frequency domain information, and the rich information of the image in the frequency domain space is not fully utilized, so it is still difficult to recover satisfactory texture details. In this paper, we propose a method to fuse the frequency domain and spatial domain information. Our method uses a two-branch network to extract the spatial domain information and the frequency domain information separately and uses a fusion module to fuse the different information in the two domains. We also use the Non-Subsampled Shearlet Transform (NSST) to preserve the texture directionality well, and design two NSST-based directional texture enhancement modules, which are embedded in different parts of the network, to enhance the recovery of texture details in the image reconstruction process. Quantitative and qualitative experimental results show that the method outperforms advanced single-image super-resolution methods in recovering images.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/tan24a.html
  PDF: https://proceedings.mlr.press/v222/tan24a/tan24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-tan24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Ming
    family: Tan
  - given: Liang
    family: Chen
  - given: Xuan
    family: Wu
  - given: Yi
    family: Wu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1337-1352
  id: tan24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1337
  lastpage: 1352
  published: 2024-02-27 00:00:00 +0000
- title: 'Decouple then Combine: A Simple and Effective Framework for Fraud Transaction Detection'
  abstract: 'With the popularity of electronic mobile and online payment, the demand for detecting financial fraudulent transactions is increasing. Although numerous efforts are devoted to tackling this problem, there are still two key challenges that are not well resolved, \emph{i.e.}, the class imbalance ratio of test samples are extremely larger than that of training samples and amount of detected fraudulent transactions do not be considered. In this paper, we propose a simple and effective framework composed of majority and minority branches to address the above issues. The input samples of majority and minority branches come from vanilla and re-adjusted distribution, respectively. Parameters of each branch are optimized individually, by which the representation learning for majority and minority samples are decoupled. Besides, an extra loss re-weighted by amount is added in the majority branch to improve the recall amount of detected fraudulent transactions. Theoretical results show that under the proposed framework, minimizing the empirical risk is guaranteed to achieve small generalization risk on more imbalanced data with high probability. Experiments on real-world datasets from Tencent Wechat payments demonstrate that our framework achieves superior performance than competitive methods in terms of both number and money of detected fraudulent transactions.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/tang24a.html
  PDF: https://proceedings.mlr.press/v222/tang24a/tang24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-tang24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Pengwei
    family: Tang
  - given: Huayi
    family: Tang
  - given: Wenhan
    family: Wang
  - given: Hanjing
    family: Su
  - given: Yong
    family: Liu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1353-1368
  id: tang24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1353
  lastpage: 1368
  published: 2024-02-27 00:00:00 +0000
- title: 'Early Diagnosis of Alzheimer through Swin-Transformer-Based Deep Learning Framework using Sparse Diffusion Measures'
  abstract: 'Alzheimer disease is one of the most common neuro-degenerative diseases, with an estimated 6.2 million cases in the United States. This research article investigates the potential of Transformer-based deep learning techniques to accelerate the processing of diffusion tensor imaging (DTI) measures and improve the early diagnosis of Alzheimer disease (AD) using sparse data. Diffusion Weighted Imaging (DWI) is a time-consuming process, with each diffusion direction taking between 2-5 minutes, and at least 40 diffusion directions are needed for routine clinical diagnosis, which needs scanning duration exceeding 3 hours for each patient. By leveraging the attention mechanism, our proposed model generates quantitative measures of fractional anisotropy (FA), axial diffusivity (AxD), and mean diffusivity (MD) using 5 and 21 diffusion directions, making it useful for clinical diagnosis through reduced scanning time of more than half. Our experimental results on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our proposed model outperforms the traditional linear least square method, achieving accurate quantitative measurement of FA, AxD, and MD scores for early diagnosis of AD patients from healthy controls using sparse diffusion directions. Our analysis highlights the potential of Swin-Transformer attention-based deep learning framework to improve the early diagnosis and treatment of Alzheimer’s disease.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/tiwari24a.html
  PDF: https://proceedings.mlr.press/v222/tiwari24a/tiwari24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-tiwari24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Abhishek
    family: Tiwari
  - given: Ananya
    family: Singhal
  - given: Saurabh J.
    family: Shigwan
  - given: Rajeev Kumar
    family: Singh
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1369-1384
  id: tiwari24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1369
  lastpage: 1384
  published: 2024-02-27 00:00:00 +0000
- title: 'Towards Better Explanations for Object Detection'
  abstract: 'Recent advances in Artificial Intelligence (AI) technology have promoted their use in almost every field. The growing complexity of deep neural networks (DNNs) makes it increasingly difficult and important to explain the inner workings and decisions of the network. However, most current techniques for explaining DNNs focus mainly on interpreting classification tasks. This paper proposes a method to explain the decision for any object detection model called D-CLOSE. To closely track the model’s behavior, we used multiple levels of segmentation on the image and a process to combine them. We performed tests on the MS-COCO dataset with the YOLOX model, which shows that our method outperforms D-RISE and can give a better quality and less noise explanation.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/truong24a.html
  PDF: https://proceedings.mlr.press/v222/truong24a/truong24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-truong24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Van Binh
    family: Truong
  - given: Truong Thanh Hung
    family: Nguyen
  - given: Vo Thanh Khang
    family: Nguyen
  - given: Quoc Khanh
    family: Nguyen
  - given: Quoc Hung
    family: Cao
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1385-1400
  id: truong24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1385
  lastpage: 1400
  published: 2024-02-27 00:00:00 +0000
- title: 'Graph Structure Learning via Lottery Hypothesis at Scale'
  abstract: 'Graph Neural Networks (GNNs) are commonly applied to analyze real-world graph-structured data. However, GNNs are sensitive to the given graph structure, which cast importance on graph structure learning to find optimal graph structures and representations. Previous methods have been restricted from large graphs due to high computational complexity. Lottery ticket hypothesis suggests that there exists a subnetwork that has comparable or better performance with proto-networks, which has been transferred to suit for pruning GNNs recently. There are few studies that address lottery ticket hypothesis’s performance on defense in graphs. In this paper, we propose a scalable graph structure learning method leveraging lottery (ticket) hypothesis : GSL-LH. Our experiments show that GSL-LH can outperform its backbone model without attack and show better robustness against attack, achieving state-of-the-art performances in regular-size graphs compared to other graph structure learning methods without feature augmentation. In large graphs, GSL-LH can have comparable results with state-of-the-art defense methods other than graph structure learning, while bringing some insights into explanation of robustness.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/yuxin24a.html
  PDF: https://proceedings.mlr.press/v222/yuxin24a/yuxin24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-yuxin24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Wang
    family: Yuxin
  - given: Hu
    family: Xiannian
  - given: Xie
    family: Jiaqing
  - given: Yin
    family: Zhangyue
  - given: Zhou
    family: Yunhua
  - given: Qiu
    family: Xipeng
  - given: Huang
    family: Xuanjing
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1401-1416
  id: yuxin24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1401
  lastpage: 1416
  published: 2024-02-27 00:00:00 +0000
- title: 'Robust Blind Watermarking Framework for Hybrid Networks Combining CNN and Transformer'
  abstract: 'As an essential means of copyright protection, the deep learning-based robust watermarking method is being studied extensively. Its framework consists of three main parts: the encoder, the noise layer and the decoder. But practically all of the schemes are directed at the encoder rather than the decoder. And the whole network is structured by shallow Convolutional Neural Networks (CNNs) for primary feature extraction, while CNNs capture local information and do not model non-local information in watermarked images well. To solve this problem, we consider the use of Transformer networks with a spatially self-attention mechanism. We propose to construct a novel decoder network by combining Transformer and CNNs, which can not only enriches local feature information but also enhances the ability to explore global representations. Meanwhile, to embed secret messages more perfectly, we design a multi-scale attentional feature fusion module to achieve an efficient aggregation of cover image features and secret message features, resulting in the encoded images with rich hybrid features. In addition, perceptual loss is introduced to better evaluate the visual quality of the watermarked images. Extensive experimental results show that our proposed method achieves better results in terms of imperceptibility and robustness compared with existing State-Of-The-Art (SOTA) methods.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/wang24a.html
  PDF: https://proceedings.mlr.press/v222/wang24a/wang24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-wang24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Baowei
    family: Wang
  - given: Ziwei
    family: Song
  - given: Yufeng
    family: Wu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1417-1432
  id: wang24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1417
  lastpage: 1432
  published: 2024-02-27 00:00:00 +0000
- title: 'A Simple and General Binarization Method for Image Restoration Neural Networks'
  abstract: 'With the advancement of deep learning techniques, image restoration (IR) performance has improved significantly. However, these techniques often come with high computational costs, which pose challenges in meeting the processing latency requirements of resource-constrained hardware in edge computer vision systems. To address this issue, we propose a simple binarization technique and an efficient training strategy called Gentle Approximation Method (GAM) to extend the application of binary neural networks (BNNs) to various IR tasks, including low-light image enhancement, deraining, denoising, and super-resolution. Our results demonstrate the effectiveness of our method in binarizing full-precision deep neural networks. By binarizing these networks, we achieve a significant reduction in computational and memory demands while maintaining satisfactory performance. For instance, in the denoising task, the FLOPs can be reduced to only 3% of the original network while preserving most of the performance.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/wang24b.html
  PDF: https://proceedings.mlr.press/v222/wang24b/wang24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-wang24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Mengxue
    family: Wang
  - given: Yue
    family: Zhang
  - given: Xiaodong
    family: Zhang
  - given: Run
    family: Min
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1433-1448
  id: wang24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1433
  lastpage: 1448
  published: 2024-02-27 00:00:00 +0000
- title: 'A Partially Observable Monte Carlo Planning Algorithm Based on Path Modification'
  abstract: 'Balancing exploration and exploitation has long been recognized as an important theme in the online planning algorithms for POMDP problems. Explorative actions on one hand prevent the planning from falling into the suboptimal dilemma, while hindering the convergence of the planning procedure on the other hand. Therefore, it is meaningful to maintain the exploration as well as taking a step forward towards exploitation. Note that there is a deviation between the action selection criteria in the planning procedure and in the execution procedure, which inspires us to build a bridge between these two criteria to accelerate the convergence. A Partially Observable Monte Carlo Planning algorithm based on Path Modification (POMCP-PM) is presented in the paper, which modifies the backtracing paths by considering the two criteria simultaneously when updating the values of parent nodes. The algorithm is general as the Upper Confidence Bound Apply to Tree (UCT) algorithm used to select actions can be easily replaced by other criteria. Experimental results demonstrate that POMCP-PM outperforms POMCP with varying numbers of simulations on several scenarios with different scales.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/wang24c.html
  PDF: https://proceedings.mlr.press/v222/wang24c/wang24c.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-wang24c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Qingya
    family: Wang
  - given: Feng
    family: Liu
  - given: Bin
    family: Luo
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1449-1462
  id: wang24c
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1449
  lastpage: 1462
  published: 2024-02-27 00:00:00 +0000
- title: 'Estimation of Counterfactual Interventions under Uncertainties'
  abstract: 'Counterfactual analysis is intuitively performed by humans on a daily basis eg. ”What should I have done differently to get the loan approved?”. Such counterfactual questions also steer the formulation of scientific hypotheses. More formally it provides insights about potential improvements of a system by inferring the effects of hypothetical interventions into a past observation of the system’s behaviour which plays a prominent role in a variety of industrial applications. Due to the hypothetical nature of such analysis, counterfactual distributions are inherently ambiguous. This ambiguity is particularly challenging in continuous settings in which a continuum of explanations exist for the same observation. In this paper, we address this problem by following a hierarchical Bayesian approach which explicitly models such uncertainty. In particular, we derive counterfactual distributions for a Bayesian Warped Gaussian Process thereby allowing for non-Gaussian distributions and non-additive noise. We illustrate the properties of our approach on a synthetic and on a semi-synthetic example and show its performance when used within an algorithmic recourse downstream task.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/weilbach24a.html
  PDF: https://proceedings.mlr.press/v222/weilbach24a/weilbach24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-weilbach24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Juliane
    family: Weilbach
  - given: Sebastian
    family: Gerwinn
  - given: Melih
    family: Kandemir
  - given: Martin
    family: Fraenzle
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1463-1478
  id: weilbach24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1463
  lastpage: 1478
  published: 2024-02-27 00:00:00 +0000
- title: 'A Novel Counterfactual Data Augmentation Method for Aspect-Based Sentiment Analysis'
  abstract: 'Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation task, which analyzes the emotional polarity of the evaluation aspects. Generally, the emotional polarity of an aspect exists in the corresponding opinion expression, whose diversity has great impact on model’s performance. To mitigate this problem, we propose a novel and simple counterfactual data augmentation method to generate opinion expressions with reversed sentiment polarity. In particular, the integrated gradients are calculated to locate and mask the opinion expression. Then, a prompt combined with the reverse expression polarity is added to the original text, and a Pre-trained language model (PLM), T5, is finally was employed to predict the masks. The experimental results shows the proposed counterfactual data augmentation method performs better than current augmentation methods on three ABSA datasets, i.e. Laptop, Restaurant, and MAMS.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/wu24a.html
  PDF: https://proceedings.mlr.press/v222/wu24a/wu24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-wu24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Dongming
    family: Wu
  - given: Lulu
    family: Wen
  - given: Chao
    family: Chen
  - given: Zhaoshu
    family: Shi
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1479-1493
  id: wu24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1479
  lastpage: 1493
  published: 2024-02-27 00:00:00 +0000
- title: 'Can Infinitely Wide Deep Nets Help Small-data Multi-label Learning?'
  abstract: 'In Multi-label Learning (MLL), kernel methods and deep neural networks (DNNs) are two typical families of approaches. Recent theory discovers an interesting connection between infinitely wide DNNs and neural tangent kernel (NTK) based methods. Further, recent work has shown the promising performance of NTK-based methods in \emph{small-data single-labeled tasks}. Then, a natural question arises: can infinitely wide DNNs help small-data multi-label learning? To answer this question, in this paper, we present to utilize infinitely wide DNNs for the MLL task. Specifically, we propose an NTK-based kernel method for MLL, which aims to minimize Hamming and ranking loss simultaneously. Moreover, to efficiently train the model, we use the Nystr{ö}m method, which has rarely been used in MLL. Further, we give rigorous theoretical analyses on learning guarantees of the proposed algorithm w.r.t. these two measures. Finally, empirical results on small-scale datasets illustrate its superior performance along with efficiency over several related baselines.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/wu24b.html
  PDF: https://proceedings.mlr.press/v222/wu24b/wu24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-wu24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Guoqiang
    family: Wu
  - given: Jun
    family: Zhu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1494-1509
  id: wu24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1494
  lastpage: 1509
  published: 2024-02-27 00:00:00 +0000
- title: 'Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples'
  abstract: 'Semi-supervised learning (SSL) is a promising approach for training deep classification models using labeled and unlabeled datasets. However, existing SSL methods rely on a large unlabeled dataset, which may not always be available in many real-world applications due to legal constraints (e.g., GDPR). In this paper, we investigate the research question: \textit{Can we train SSL models without real unlabeled datasets?} Instead of using real unlabeled datasets, we propose an SSL method using synthetic datasets generated from generative foundation models trained on datasets containing millions of samples in diverse domains (e.g., ImageNet). Our main concepts are to identify synthetic samples that emulate unlabeled samples from generative foundation models and to train classifiers using these synthetic samples. To achieve this, our method is formulated as an alternating optimization problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers using real labeled and synthetic unlabeled samples. For (i), we propose a meta-learning objective that optimizes latent variables to generate samples that resemble real labeled samples and minimize the validation loss. For (ii), we propose a simple unsupervised loss function that regularizes the feature extractors of classifiers to maximize the performance improvement obtained from synthetic samples. We confirm that our method outperforms baselines using generative foundation models on SSL. We also demonstrate that our methods outperform SSL using real unlabeled datasets in scenarios with extremely small amounts of labeled datasets. This suggests that synthetic samples have the potential to provide improvement gains more efficiently than real unlabeled data.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/yamaguchi24a.html
  PDF: https://proceedings.mlr.press/v222/yamaguchi24a/yamaguchi24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-yamaguchi24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Shin’ya
    family: Yamaguchi
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1510-1525
  id: yamaguchi24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1510
  lastpage: 1525
  published: 2024-02-27 00:00:00 +0000
- title: 'GWQ: Group-Wise Quantization Framework for Neural Networks'
  abstract: 'As the most commonly used quantization techniques for deep neural networks, the int-only quantization methods use scale factor to linearly approximate the weights or activation of each layer. However, when passing activation data between layers, such int-only quantization methods require extra Scale Factor Conversion (SFC) operations, resulting in computational overhead. In this paper, we propose a Group-Wise Quantization framework, called GWQ, to reduce computational consumption during the activation data pass process by allowing multiple layers share one scale factor in SFC operations. Specifically, in the GWQ framework, we propose two algorithms for network layers grouping and model training. For the grouping of network layers, we propose a grouping algorithm based on the similarity of data numerical distribution. Then, the network layers divided into the same group will be quantified using the same common scale factor to reduce the computational consumption. Considering the additional performance loss caused by sharing scale factors among multiple layers, we propose a training algorithm to optimize these shared scale factors and model parameters, by designing a learnable power-of-two scaling parameter for each layer. Extensive experiments demonstrate that the proposed GWQ framework is able to effectively reduce the computational burden during inference, while maintaining model performance with negligible impact.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/yang24a.html
  PDF: https://proceedings.mlr.press/v222/yang24a/yang24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-yang24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Jiaming
    family: Yang
  - given: Chenwei
    family: Tang
  - given: Caiyang
    family: Yu
  - given: Jiancheng
    family: Lv
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1526-1541
  id: yang24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1526
  lastpage: 1541
  published: 2024-02-27 00:00:00 +0000
- title: 'Long-Range Graph U-Nets: Node and Edge Clustering Pooling Model For Stroke Classification in Online Handwritten Documents'
  abstract: 'Stroke classification is a crucial step for applications with online handwritten input. It is a challenging task due to the variations in writing style, complex structure, long contextual semantic dependence of written content and etc. In this work, we propose a method called Long-Range Graph U-Nets, which involves using a novel node and edge clustering graph pooling layer in the encoder block and a multi-level feature fusion strategy. Such operations guide the model to leverage both temporal and spatial contextual information, establish long-range semantic dependencies, and effectively reduce redundant information caused by local instances of the same category. Extensive experiments conducted on publicly available online handwritten document datasets, demonstrate that our proposed method outperforms previous methods by a significant margin, particularly in the List category, and achieves state-of-the-art performance.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/yao24a.html
  PDF: https://proceedings.mlr.press/v222/yao24a/yao24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-yao24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Muwu
    family: Yao
  - given: Shuang
    family: She
  - given: Jinrong
    family: Li
  - given: Jianmin
    family: Lin
  - given: Ming
    family: Yang
  - given: Hongxing
    family: Peng
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1542-1557
  id: yao24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1542
  lastpage: 1557
  published: 2024-02-27 00:00:00 +0000
- title: 'Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency'
  abstract: 'Spiking Neural Networks (SNNs) that operate in an event-driven manner and employ binary spike representation have recently emerged as promising candidates for energy-efficient computing. However, a cost bottleneck arises in obtaining high-performance SNNs: training a SNN model requires a large number of time steps in addition to the usual learning iterations, hence this limits their energy efficiency. This paper proposes a general training framework that enhances feature learning and activation efficiency within a limited time step, providing a new solution for more energy-efficient SNNs.  Our framework allows SNN neurons to learn robust spike feature from different receptive fields and update neuron states by utilizing both current stimuli and recurrence information transmitted from other neurons. This setting continuously complements information within a single time step. Additionally, we propose a projection function to merge these two stimuli to smoothly optimize neuron weights (spike firing threshold and activation). We evaluate the proposal for both convolution and recurrent models. Our experimental results indicate state-of-the-art visual classification tasks, including CIFAR10, CIFAR100, and TinyImageNet. Our framework achieves 72.41% and 72.31% top-1 accuracy with only 1 time step on CIFAR100 for CNNs and RNNs, respectively. Our method reduces 10X and 3X joule energy than a standard ANN and SNN, respectively, on CIFAR10, without additional time steps.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/yao24b.html
  PDF: https://proceedings.mlr.press/v222/yao24b/yao24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-yao24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Yunpeng
    family: Yao
  - given: Man
    family: Wu
  - given: Zheng
    family: Chen
  - given: Renyuan
    family: Zhang
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1558-1573
  id: yao24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1558
  lastpage: 1573
  published: 2024-02-27 00:00:00 +0000
- title: 'DENL: Diverse Ensemble and Noisy Logits for Improved Robustness of Neural Networks'
  abstract: 'Neural Networks (NN) are increasingly used for image classification in medical, transportation, and security devices. However, recent studies have revealed neural networks’ vulnerability against adversarial examples generated by adding small perturbations to images. These malicious samples are imperceptible by human eyes, but can give rise to misclassification by NN models. Defensive distillation is a defence mechanism in which the NN’s output probabilities are scaled to a user-defined range and used as labels to train a new model less sensitive to input perturbations. Despite initial success, defensive distillation was defeated by state-of-the-art attacks. A proposed countermeasure was to add noise in the inference time to hamper the adversarial attack which also decreased the model accuracy. In this paper, we address this limitation by proposing a two-phase training methodology to defend against adversarial attacks. In the first phase, we train architecturally diversified models individually using the cross-entropy loss function. In the second phase, we train the ensemble using a diversity-promoting loss function. Our experimental results show that our training methodology and noise addition in the inference time improved our ensemble’s resistance against adversarial attacks, while maintaining reasonable accuracy, compared to the state-of-the-art methods.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/yazdani24a.html
  PDF: https://proceedings.mlr.press/v222/yazdani24a/yazdani24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-yazdani24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Mina
    family: Yazdani
  - given: Hamed
    family: Karimi
  - given: Reza
    family: Samavi
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1574-1589
  id: yazdani24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1574
  lastpage: 1589
  published: 2024-02-27 00:00:00 +0000
- title: 'A Multi-Surrogate Assisted Salp Swarm Feature Selection Algorithm with Multi-Population Adaptive Generation Strategy for Classification'
  abstract: 'The salp swarm algorithm(SSA) has been successfully used to solve the feature selection problem due to its fast convergence and simple structure. However, existing SSA-based methods still suffer from the issue of low classification accuracy due to the problem of getting trapped in local optima. Therefore, this paper proposes a novel feature selection method for classification based on SSA, which can continuously generate new sub-populations to improve the search environment of the main population. Specifically, a flip-prohibition(F-P) operator is first proposed to help the main population, which may currently fall into a local optimum, find a new and more promising region. A multi-surrogate technique is suggested to evaluate the region to determine the position of sub-populations, which can reduce the high computational cost. In addition, a population initialization method is developed according to the importance of features and the dimensionality of the dataset. Finally, a communication mechanism is presented to enable different sub-populations to learn from each other. By comparing the proposed method with other 6 feature selection methods on 16 datasets, we demonstrate that the proposed method has better classification ability and can select a smaller feature subset in most cases.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/yu24a.html
  PDF: https://proceedings.mlr.press/v222/yu24a/yu24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-yu24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Zikang
    family: Yu
  - given: Hongbin
    family: Dong
  - given: Tianyu
    family: Guo
  - given: Bingxu
    family: Zhao
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1590-1605
  id: yu24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1590
  lastpage: 1605
  published: 2024-02-27 00:00:00 +0000
- title: 'Cell Variational Information Bottleneck Network'
  abstract: 'In this work, we propose “Cell Variational Information Bottleneck Network (cellVIB)”, a convolutional neural network using information bottleneck mechanism, which can be combined with the latest feedforward network architecture in an end-to-end training method. Our Cell Variational Information Bottleneck Network is constructed by stacking VIB cells, which generate feature maps with uncertainty. As layers going deeper, the regularization effect will gradually increase, instead of directly adding excessive regular constraints to the output layer of the model as in Deep VIB. In each VIB cell, the feedforward process learns an independent mean term and a standard deviation term, and predicts the Gaussian distribution based on them. The feedback process is based on reparameterization trick for effective training. This work performs an extensive analysis on MNIST dataset to verify the effectiveness of each VIB cells mentioned above, and provides an insightful analysis on how the VIB cells affect mutual information. Experiments conducted on CIFAR-10 also prove that our network is robust against noisy labels during training and against corrupted images during testing. Then, we validate our method on PACS dataset, whose results show that the VIB cells can significantly improve the generalization performance of the basic model. Finally, in a more complex representation learning task, face recognition, our network structure has also achieved very competitive results.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zhai24a.html
  PDF: https://proceedings.mlr.press/v222/zhai24a/zhai24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zhai24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Zhonghua
    family: Zhai
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1606-1621
  id: zhai24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1606
  lastpage: 1621
  published: 2024-02-27 00:00:00 +0000
- title: 'Graph Contrastive Learning with Group Whitening'
  abstract: 'Graph neural networks (GNNs) have demonstrated their great power in learning graph-structured data. Due to the limitations of expensive labeled data, contrastive learning has been applied in graph domain. We propose GWGCL, a graph contrastive learning method based on feature group whitening to achieve two key properties of contrastive learning: alignment and uniformity. GWGCL achieves the alignment by ensuring consistency between positive samples. There is no need for negative samples to participate, but rather to achieve the uniformity between samples through whitening. Because whitening has the effect of feature divergence, it avoids the collapse of all sample representations to a single point, which is called dimensional collapse. Moreover, GWGCL can achieve better results and higher efficiency without the need for asymmetric networks, projection layers, stopping gradients and complex loss function. Through extensive experiments, GWGCL performs competitively on node classification and graph classification tasks across ten common graph datasets.The code is in: https://github.com/MR9812/GWGCL.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zhang24a.html
  PDF: https://proceedings.mlr.press/v222/zhang24a/zhang24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zhang24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Chunhui
    family: Zhang
  - given: Rui
    family: Miao
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1622-1637
  id: zhang24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1622
  lastpage: 1637
  published: 2024-02-27 00:00:00 +0000
- title: 'Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise'
  abstract: 'This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\sqrt{\bar{\alpha}}=\cos(\eta)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary differential equation (ODE). In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be used effectively. The second contribution is to directly estimate both the image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which enables more stable calculations of the update step in the inverse diffusion steps, as accurate estimation of both the image and noise are crucial at different stages of the process. Together with these changes, our model achieves faster generation, with the ability to converge on high-quality images more quickly, and higher quality of the generated images, as measured by metrics such as Fr{é}chet Inception Distance (FID), spatial Fr{é}chet Inception Distance (sFID), precision, and recall.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zhang24b.html
  PDF: https://proceedings.mlr.press/v222/zhang24b/zhang24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zhang24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Zhenkai
    family: Zhang
  - given: Krista A.
    family: Ehinger
  - given: Tom
    family: Drummond
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1638-1653
  id: zhang24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1638
  lastpage: 1653
  published: 2024-02-27 00:00:00 +0000
- title: 'Self-supervised Example Difficulty Balancing for Local Descriptor Learning'
  abstract: 'In scenarios where there is an imbalance between positive and negative examples, hard example mining strategies have been shown to improve recognition performance by assisting models in distinguishing subtle differences between positive and negative examples. However, overly strict mining strategies may introduce false negative examples, while implementing the mining strategy can disrupt the difficulty distribution of examples in the real dataset and cause overfitting on difficult examples in the model. Therefore, in this paper, we explore how to balance the difficulty of mined examples in order to obtain and exploit high-quality negative examples, and try to solve the problem in terms of both loss function and training strategy. The proposed balance loss provides an effective discriminant for the quality of negative examples by incorporating a self-supervised approach into the loss function, employing dynamic gradient modulation to achieve finer adjustment for examples of different difficulties. The proposed annealing training strategy constrains the difficulty of negative examples drawn from mining and uses examples of decreasing difficulty to mitigate the overfitting issue of hard negative examples in training. Extensive experiments demonstrate that our new sparse descriptors outperform previously established state-of-the-art sparse descriptors.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zhang24c.html
  PDF: https://proceedings.mlr.press/v222/zhang24c/zhang24c.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zhang24c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Jiahan
    family: Zhang
  - given: Dayong
    family: Tian
  - given: Tianyang
    family: Wu
  - given: Yiqing
    family: Cao
  - given: Yaoqi
    family: Du
  - given: Yiwen
    family: Wei
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1654-1669
  id: zhang24c
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1654
  lastpage: 1669
  published: 2024-02-27 00:00:00 +0000
- title: 'Deep Traffic Benchmark: Aerial Perception and Driven Behavior Dataset'
  abstract: 'Predicting human driving behavior has always been an important area of autonomous driving research. Existing data on autonomous driving in this area is limited in both perspective and duration. For example, vehicles may block each other on the road, although data from vehicles behind them is useful for research. In addition, driving in this area is constrained by the road environment, and the host vehicle cannot observe the designated area for an extended period of time. To investigate the potential relationship between human driving behavior and traffic conditions, we provide a drone-collected video dataset, Deep Traffic, that includes: (1) aerial footage from a vertical perspective, (2) image and annotation capture for training vehicle destination detection and semantic segmentation model, (3) high-definition map data of the captured area, (4) development scripts for various features. Deep Traffic is the largest and most comprehensive dataset to date, covering both urban and high-speed areas. We believe that this benchmark dataset will greatly facilitate the development of drones to monitor traffic flow and study human driver behavior, and that the capacity of the traffic system is of great importance. All datasets and pre-training results can be downloaded from github project.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zhang24d.html
  PDF: https://proceedings.mlr.press/v222/zhang24d/zhang24d.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zhang24d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Guoxing
    family: Zhang
  - given: Qiuping
    family: Li
  - given: Yiming
    family: Liu
  - given: Zhanpeng
    family: Wang
  - given: Yuanqi
    family: Chen
  - given: Wenrui
    family: Cai
  - given: Weiye
    family: Zhang
  - given: Bingting
    family: Guo
  - given: Zhi
    family: Zeng
  - given: Jiasong
    family: Zhu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1670-1682
  id: zhang24d
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1670
  lastpage: 1682
  published: 2024-02-27 00:00:00 +0000
- title: 'Unleashing the Power of High-pass Filtering in Continuous Graph Neural Networks'
  abstract: 'Recent Continuous Graph Neural Networks (CGNNs) have attracted great attention due to its merits of infinite depth without oversmoothing. However, most of the existing CGNNs perform low-pass filtering in nature, as they are derived from discrete Laplacian-smoothing based graph neural networks (GNNs). While prior research has shown the promising results of high-pass filtering for node representation learning, particularly on heterophilous graphs, there remains a need to extend it to continuous domain and explore the synergy between two filtering channels. In this paper, by leveraging low-pass and high-pass filtering, we propose a novel dual-channel continuous graph neural network architecture to address this gap. In particular, we introduce a dimension masking method to coordinate the contribution of all low and high pass filtered feature dimensions to node classification. Our aim is to deepen the understanding of the link between high and low filters, unraveling their distinct roles in learning node representations. To evaluate the effectiveness of our framework, we conduct extensive experiments focusing on the node classification task of heterophilous graphs. Our results demonstrate the competitive performance of our approach, showcasing its robustness to oversmoothing.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zhang24e.html
  PDF: https://proceedings.mlr.press/v222/zhang24e/zhang24e.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zhang24e.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Acong
    family: Zhang
  - given: Ping
    family: Li
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1683-1698
  id: zhang24e
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1683
  lastpage: 1698
  published: 2024-02-27 00:00:00 +0000
- title: 'Revisiting Structured Dropout'
  abstract: 'Large neural networks are often overparameterised and prone to overfitting, Dropout is a widely used regularization technique to combat overfitting and improve model generalization. However, unstructured Dropout is not always effective for specific network architectures and this has led to the formation of multiple structured Dropout approaches to improve model performance and, sometimes, reduce the computational resources required for inference. In this work, we revisit structured Dropout comparing different Dropout approaches on natural language processing and computer vision tasks for multiple state-of-the-art networks. Additionally, we devise an approach to structured Dropout we call \textbf{\emph{ProbDropBlock}} which drops contiguous blocks from feature maps with a probability given by the normalized feature salience values. We find that, with a simple scheduling strategy, the proposed approach to structured Dropout consistently improves model performance compared to baselines and other Dropout approaches on a diverse range of tasks and models. In particular, we show \textbf{\emph{ProbDropBlock}} improves RoBERTa finetuning on MNLI by $0.22%$, and training of ResNet50 on ImageNet by $0.28%$.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zhao24a.html
  PDF: https://proceedings.mlr.press/v222/zhao24a/zhao24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zhao24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Yiren
    family: Zhao
  - given: Oluwatomisin
    family: Dada
  - given: Robert
    family: Mullins
  - given: Xitong
    family: Gao
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1699-1714
  id: zhao24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1699
  lastpage: 1714
  published: 2024-02-27 00:00:00 +0000
- title: 'Automatic Segmentation of Aortic and Mitral Valves for Heart Surgical Planning of Hypertrophic Obstructive Cardiomyopathy'
  abstract: 'Hypertrophic obstructive cardiomyopathy (HOCM) is a leading cause of sudden cardiac death in young people. Septal myectomy surgery has been recognized as the gold standard for non-pharmacological therapy of HOCM, in which aortic and mitral valves are critical regions for surgical planning.Currently, manual segmentation of aortic and mitral valves is widely performed in clinical practice to construct 3D models used for HOCM surgical planning. Such a process, however, is time-consuming and costly. In this paper, we integrate anatomical prior knowledge into deep learning for automatic segmentation of aortic and mitral valves.In particular, a two-stage method is proposed: we first obtain the region of interest (RoI) from a CT image, where heart segmentation is then performed. The spatial relationship between heart substructures is utilized to identify a valve region that contains the aortic and mitral valves. Unlike typical two-stage methods, we feed the refined segmentation of the left ventricle, left atrium, and aorta as additional input for the valve segmentation. By incorporating this anatomical prior knowledge, deep neural networks (DNNs) can leverage the surrounding anatomical structures to improve valve segmentation. We collected a dataset of 27 CT images from patients with a medical history of septal myectomy surgery.Experimental results show that our method achieves an average Dice score of 71.2% and an improvement of 4.2% over existing methods.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zheng24a.html
  PDF: https://proceedings.mlr.press/v222/zheng24a/zheng24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zheng24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Limin
    family: Zheng
  - given: Hongyu
    family: Chen
  - given: Lu
    family: Qing
  - given: Jian
    family: Zhuang
  - given: Bo
    family: Meng
  - given: Xiaowei
    family: Xu
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1715-1730
  id: zheng24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1715
  lastpage: 1730
  published: 2024-02-27 00:00:00 +0000
- title: 'Faster Target Encirclement with Utilization of Obstacles via Multi-Agent Reinforcement Learning'
  abstract: 'Multi-agent encirclement refers to controlling multiple agents to restrict the movement of a target and surround it with a specific formation. However, two challenges remain: encirclement in obstacle scenarios and encirclement of a faster target. In obstacle scenarios, we propose the utilization of obstacles for facilitating encirclement and introduce the concept of contributing angle to quantify the contribution of agents and obstacles, which enables agents to effectively utilize obstacles while mitigating the credit assignment problem. To address the challenge of encircling a faster target, we propose a two-stage encirclement method inspired by lions’ hunting strategy, effectively preventing target escape. We design the reward function based on the contributing angle and the lion encirclement method, integrating it with the Multi-Agent Deep Deterministic Policy Gradient (MADDPG). The simulation results demonstrate that our method can utilize obstacles to complete encirclement and has a higher success rate. In some conditions with insufficient numbers of agents, our methods can still accomplish the task. Ablation experiments are conducted to verify the effectiveness of the contributing angle and the lion encirclement method respectively.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zheng24b.html
  PDF: https://proceedings.mlr.press/v222/zheng24b/zheng24b.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zheng24b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuxi
    family: Zheng
  - given: Yongjun
    family: Zhang
  - given: Chenran
    family: Zhao
  - given: Huanhuan
    family: Yang
  - given: Tongyue
    family: Li
  - given: Qianying
    family: Ouyang
  - given: Ying
    family: Chen
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1731-1746
  id: zheng24b
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1731
  lastpage: 1746
  published: 2024-02-27 00:00:00 +0000
- title: 'A Corrected Expected Improvement Acquisition Function Under Noisy Observations'
  abstract: 'Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless settings. We show that our method achieves a sublinear convergence rate on the cumulative regret bound under heteroscedastic observation noise. Our empirical results demonstrate that our proposed acquisition function can outperform EI in the presence of noisy observations on benchmark functions for black-box optimization, as well as on parameter search for neural network model compression.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zhou24a.html
  PDF: https://proceedings.mlr.press/v222/zhou24a/zhou24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zhou24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Han
    family: Zhou
  - given: Xingchen
    family: Ma
  - given: Matthew B
    family: Blaschko
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1747-1762
  id: zhou24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1747
  lastpage: 1762
  published: 2024-02-27 00:00:00 +0000
- title: 'FasterVoxelPose+: Fast and Accurate Voxel-based 3D Human Pose Estimation by Depth-wise Projection Decay'
  abstract: 'In terms of multi-person multi-view 3D pose estimation, voxel-based methods gain promising accuracy by directly manipulating features in 3D space. Since their high computational cost prevents them from practical applications, Faster VoxelPose was proposed to address this complication by re-projecting the 3D feature volume onto coordinate planes, which greatly improved the efficiency of the model. However, it suffers from an obvious performance drop, especially when there are fewer cameras. In this paper, we propose a more accurate real-time 3D pose estimation method, FasterVoxelPose+, to address the above problem. We have made two improvements to the previous methods. First, we propose a novel method for constructing voxel feature volume called Depth-wise Projection Decay (DPD). It introduces extra depth information to the projection to alleviate depth ambiguity. Second, we design an Encoder-Decoder Network for processing the re-projected voxel features to further push up the performance of the model. Our method obtains 17.42mm MPJPE on Panoptic with real-time speed and can be easily used in other voxel-based models.'
  volume: 222
  URL: https://proceedings.mlr.press/v222/zhuang24a.html
  PDF: https://proceedings.mlr.press/v222/zhuang24a/zhuang24a.pdf
  edit: https://github.com/mlresearch//v222/edit/gh-pages/_posts/2024-02-27-zhuang24a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of the 15th Asian Conference on Machine Learning'
  publisher: 'PMLR'
  author: 
  - given: Zonghuang
    family: Zhuang
  - given: Yue
    family: Zhou
  editor: 
  - given: Berrin
    family: Yanıkoğlu
  - given: Wray
    family: Buntine
  page: 1763-1778
  id: zhuang24a
  issued:
    date-parts: 
      - 2024
      - 2
      - 27
  firstpage: 1763
  lastpage: 1778
  published: 2024-02-27 00:00:00 +0000
