@proceedings{AMBN2017,
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Antti Hyttinen and Joe Suzuki and Brandon Malone},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 volume = {73}
}

@inproceedings{pmlr-v73-buntine17a,
 abstract = {Various authors have highlighted inadequacies of BDeu type scores and this problem is
shared in parameter estimation. Basically, Laplace estimates work poorly, at least because
setting the prior concentration is challenging. In 1997, Freidman et al suggested a simple
backoff approach for Bayesian network classifiers (BNCs). Backoff methods dominate in
in n-gram language models, with modified Kneser-Ney smoothing, being the best known,
and a Bayesian variant exists in the form of Pitman-Yor process language models from
Teh in 2006. In this talk we will present some results on using backoff methods for Bayes
network classifiers and Bayesian networks generally. For BNCs at least, the improvements
are dramatic and alleviate some of the issues of choosing too dense a network.
},
 author = {Buntine, Wray},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2757101043},
 pages = {3--3},
 pdf = {http://proceedings.mlr.press/v73/buntine17a/buntine17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Backoff methods for estimating parameters of a Bayesian network},
 url = {https://proceedings.mlr.press/v73/buntine17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-chan17a,
 abstract = {Arithmetic circuits have been used as tractable representations of probability distributions, either generated from models such as Bayesian networks, sum-product networks and Probability Sentential Decision Diagrams, or directly from data. An interesting question is how we can incorporate uncertain evidence, which specifies that the marginal probabilities of a variable has to undergo certain changes, directly into an arithmetic circuit and then perform reasoning on it to compute the probability distribution after incorporating this uncertain evidence. In this paper, we show that we can incorporate uncertain evidence on a variable by setting indicators of this variable in the arithmetic circuit to non-negative values based on the likelihood ratios in Pearl's method of virtual evidence and the current marginal probabilities of this variable. For tractable computation of these marginal probabilities, the arithmetic circuit has to satisfy the properties of decomposability and smoothness, and we show that an algorithm using a downward pass can compute these marginal probabilities for all single variables. We show a procedure of how to incorporate virtual evidence, including multiple pieces of virtual evidence.},
 author = {Chan, Hei},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2805558265},
 pages = {105--116},
 pdf = {http://proceedings.mlr.press/v73/chan17a/chan17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Incorporating Uncertain Evidence Into Arithmetic Circuits Representing Probability Distributions},
 url = {https://proceedings.mlr.press/v73/chan17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-giso-dal17a,
 abstract = {Bayesian networks (BN) are a popular representation for reasoning under uncertainty. The computational complexity of inference, however, hinders its applicability to many real-world domains that in principle can be modeled by BNs. Inference methods based on <i>Weighted Model Counting</i> (WMC) reduce the cost of inference by exploiting patterns exhibited by the probabilities associated with BN nodes. However, these methods require a computationally intensive compilation step in search of these patterns, limiting the number of BNs that are eligible based on their size. In this paper, we aim to extend WMC methods in general by proposing a scalable, compilation framework that is language agnostic, which solves this problem by partitioning BNs and compiling them as a set of smaller sub-problems. This reduces the cost of compilation and allows state-of-the-art innovations in WMC to be applied to a much larger range of Bayesian networks.},
 author = {Dal, Giso H. and Michels, Steffen and Lucas, Peter J. F.},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2756633154},
 pages = {141--152},
 pdf = {http://proceedings.mlr.press/v73/giso-dal17a/giso-dal17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Reducing the Cost of Probabilistic Knowledge Compilation},
 url = {https://proceedings.mlr.press/v73/giso-dal17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-guo17a,
 abstract = {To improve the learning accuracy of parameters in a Bayesian network (BN) from limited
data, domain knowledge is often incorporated into the learning process as parameter con-
straints. Maximum a posteriori (MAP) based methods that use both data and constraints
have been studied extensively. Among those methods, the qualitatively maximum a pos-
teriori (QMAP) method exhibits high learning performance. In the QMAP method, when
the data are limited, estimation from the data often fails to satisfy all the parameter con-
straints, which makes the overall QMAP estimation unreliable. To ensure that the QMAP
estimation does not violate any given parameter constraint and further improve the learn-
ing accuracy, in this paper, we propose a qualitatively maximum a posteriori correction
(QMAP-C) estimation algorithm, which regulates QMAP estimation by replacing the data
estimation with a further constrained estimation. Experiments show that the proposed al-
gorithm outperforms most of the existing parameter learning methods when the parameter
constraints are correct.},
 author = {Guo, Zhigao and Gao, Xiaoguang and Di, Ruohai},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2757644707},
 pages = {93--104},
 pdf = {http://proceedings.mlr.press/v73/guo17a/guo17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Bayesian Network Parameters with Domain Knowledge and Insufficient Data.},
 url = {https://proceedings.mlr.press/v73/guo17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-halloran17a,
 abstract = {In the past two decades, the field of proteomics has seen explosive growth, largely due
to the development of tandem mass spectrometry (MS/MS). With a complex biological
sample as input, a typical MS/MS experiment quickly produces a large (often numbering
in the hundreds-of-thousands) collection of spectra representative of the proteins present
in the original complex sample. A majority of widely used methods to search and identify
MS/MS spectra use scoring functions which rely on static, hand-selected parameters rather
than affording the ability to learn parameters and adapt to the widely varying characteristics
of MS/MS data. In this talk, we discuss recent work utilizing dynamic Bayesian networks
(DBNs) to identify MS/MS spectra. In particular, we discuss a recently proposed DBN for
Rapid Identification of Peptides (DRIP) which, in contrast to popular scoring functions,
allows efficient generative and discriminative learning of parameters to achieve state-of-theart
spectrum-identification accuracy. Furthermore, facilitated by DRIPâs generative nature,
we present current innovations leveraging DBNs to significantly enhance many other aspects
of MS/MS analysis, such as improving downstream discriminative classification via detailed
feature extraction and speeding up identification runtime using trellises and approximate
inference.
},
 author = {Halloran, John T.},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2759341858},
 pages = {6--6},
 pdf = {http://proceedings.mlr.press/v73/halloran17a/halloran17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Analyzing Tandem Mass Spectra: A Graphical Models Perspective.},
 url = {https://proceedings.mlr.press/v73/halloran17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-jitta17a,
 abstract = {  Cross-domain object matching refers to the task of inferring unknown
alignment between objects in two data collections that do not have a
shared data representation. In recent years several methods have
been proposed for solving the special case that assumes each object
is to be paired with exactly one object, resulting
in a constrained optimization problem over permutations. A related
problem formulation of cluster matching seeks to match a cluster of
objects in one data set to a cluster of objects in the other set,
which can be considered as many-to-many extension of cross-domain
object matching and can be solved without explicit constraints.  In
this work we study the intermediate region between these two special
cases, presenting a range of Bayesian inference algorithms that work
also for few-to-few cross-domain object matching problems where
constrained optimization is necessary but the optimization domain is
broader than just permutations. },
 author = {Jitta, Aditya and Klami, Arto},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2760341199},
 pages = {176--187},
 pdf = {http://proceedings.mlr.press/v73/jitta17a/jitta17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Few-to-few Cross-domain Object Matching},
 url = {https://proceedings.mlr.press/v73/jitta17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-kei-amii17a,
 abstract = {  In this paper, we analyze the size of decision diagrams (DD) representing the set of all parse trees of a context-free grammar (CFG).
CFG is widely used in the field of natural language processing and bioinformatics to estimate the hidden structures of sequence data.
A decision diagram is a data structure that represents a Boolean function in a concise form. By using DDs to represent the set of all parse trees, we can efficiently perform many useful operations over the parse trees, such as finding trees that satisfy additional constraints and finding the best parse tree.
Since the time complexity of these operations depends on DD size, selecting an appropriate DD variant is important.
Experiments on a simple CFG show that the Zero-suppressed Sentential Decision Diagram (ZSDD) is better than other DDs; we also give theoretical upper bounds on ZSDD size.},
 author = {Nishino, Masaaki and Amii, Kei and Yamamoto, Akihiro},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2759749471},
 pages = {153--164},
 pdf = {http://proceedings.mlr.press/v73/kei-amii17a/kei-amii17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On the Sizes of Decision Diagrams Representing the Set of All Parse Trees of a Context-free Grammar},
 url = {https://proceedings.mlr.press/v73/kei-amii17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-lee17a,
 abstract = {Bayesian networks are a widely used graphical model with diverse applications in knowledge discovery, classification,
and decision making. Learning a Bayesian network from discrete data can be cast as a combinatorial optimization problem and
thus solved using optimization techniques---the well-known \emph{score-and-search} approach. An important consideration
when applying a score-and-search method for Bayesian network structure learning (BNSL) is its anytime behavior; i.e., how
does the quality of the solution found improve as a function of the amount of time given to the algorithm. Previous studies
of the anytime behavior of methods for BNSL are limited by the scale of the instances used in the evaluation and evaluate
only algorithms that do not scale to larger instances. In this paper, we perform an extensive evaluation of the
anytime behavior of the current state-of-the-art algorithms for BNSL. Our benchmark instances range from small (instances
with fewer than 20 random variables) to massive (instances with more than 1,500 random variables).  We find that a local search
algorithm based on memetic search dominates the performance of other state-of-the-art algorithms when considering anytime behavior.},
 author = {Lee, Colin and van Beek, Peter},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2757513240},
 pages = {69--80},
 pdf = {http://proceedings.mlr.press/v73/lee17a/lee17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An Experimental Analysis of Anytime Algorithms for Bayesian Network Structure Learning.},
 url = {https://proceedings.mlr.press/v73/lee17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-natori17a,
 abstract = {We have already proposed a constraint-based learning Bayesian network method using Bayes factor. Since a conditional independence test using Bayes factor has consistency, the learning method improves the learning accuracy of the traditional constraint-based learning methods. Additionally, the method is expected to learn larger network structures than the traditional methods do because it greatly improves computational efficiency. However, its expected benefits have not been demonstrated empirically. This report describes some experiments related to the learning of large network structures. Results show that the proposed method can learn surprisingly huge networks with thousands of variables. },
 author = {Natori, Kazuki and Uto, Masaki and Ueno, Maomi},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2758592880},
 pages = {57--68},
 pdf = {http://proceedings.mlr.press/v73/natori17a/natori17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Consistent Learning Bayesian Networks with Thousands of Variables},
 url = {https://proceedings.mlr.press/v73/natori17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-pena17a,
 abstract = {Alternative acyclic directed mixed graphs (ADMGs) are graphs that may allow causal effect identification in scenarios where Pearl's original ADMGs may not, and vice versa. Therefore, they complement each other. In this paper, we introduce a sound algorithm for identifying arbitrary causal effects from alternative ADMGs. Moreover, we show that the algorithm is complete for identifying the causal effect of a single random variable on the rest. We also show that the algorithm follows from a calculus similar to Pearl's <i>do</i>-calculus. },
 author = {PeÃ±a, Jose M.},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2760168969},
 pages = {21--32},
 pdf = {http://proceedings.mlr.press/v73/pena17a/pena17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Effect Identification in Alternative Acyclic Directed Mixed Graphs},
 url = {https://proceedings.mlr.press/v73/pena17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-pena17b,
 abstract = {Andersson-Madigan-Perlman chain graphs were originally introduced to represent independence models. They have recently been shown to be suitable for representing causal models with additive noise. In this paper, we present an algorithm for learning causal chain graphs. The algorithm builds on the ideas by \citet{Hoyeretal.2009}, i.e. it exploits the nonlinearities in the data to identify the direction of the causal relationships. We also report experimental results on real-world data.},
 author = {PeÃ±a, Jose M.},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2759677871},
 pages = {33--44},
 pdf = {http://proceedings.mlr.press/v73/pena17b/pena17b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Causal AMP Chain Graphs.},
 url = {https://proceedings.mlr.press/v73/pena17b.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-sato17a,
 abstract = {Learning probability by probabilistic modeling is a major task in statistical machine learning
and it has traditionally been supported by maximum likelihood estimation applied to
generative models or by a local maximizer applied to discriminative models. In this talk, we
introduce a third approach, an innovative one that learns probability by comparing probabilistic
events. In our approach, we give the ranking of probabilistic events and the system
learns a probability distribution so that the ranking is well respected. We implemented
this approach in PRISM, a logic-based probabilistic programming language, and conducted
learning experiments with real data for models described by PRISM programs.},
 author = {Sato, Taisuke},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 pages = {5--5},
 pdf = {http://proceedings.mlr.press/v73/sato17a/sato17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning probability by comparison},
 url = {https://proceedings.mlr.press/v73/sato17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-scanagatta17a,
 abstract = {We present a novel approach for score-based structure learning of Bayesian network, which couples an existing ordering-based algorithm for structure optimization with a novel operator for exploring the neighborhood of a given order in the space of the orderings. Our approach achieves state-of-the-art performances in data sets containing thousands of variables. },
 author = {Scanagatta, Mauro and Corani, Giorgio and Zaffalon, Marco},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2758544309},
 pages = {45--56},
 pdf = {http://proceedings.mlr.press/v73/scanagatta17a/scanagatta17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improved Local Search in Bayesian Networks Structure Learning},
 url = {https://proceedings.mlr.press/v73/scanagatta17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-scutari17a,
 abstract = {A classic approach for learning Bayesian networks from data is to identify a maximum a posteriori (MAP) network structure. In the case of discrete Bayesian networks, MAP networks are selected by maximising one of several possible Bayesian–Dirichlet (BD) scores; the most famous is the Bayesian–Dirichlet equivalent uniform (BDeu) score from Heckerman et al. (Mach Learn 20(3):197–243, 1995). The key properties of BDeu arise from its uniform prior over the parameters of each local distribution in the network, which makes structure learning computationally efficient; it does not require the elicitation of prior knowledge from experts; and it satisfies score equivalence. In this paper we will review the derivation and the properties of BD scores, and of BDeu in particular, and we will link them to the corresponding entropy estimates to study them from an information theoretic perspective. To this end, we will work in the context of the foundational work of Giffin and Caticha (Proceedings of the 27th international workshop on Bayesian inference and maximum entropy methods in science and engineering, pp 74–84, 2007), who showed that Bayesian inference can be framed as a particular case of the maximum relative entropy principle. We will use this connection to show that BDeu should not be used for structure learning from sparse data, since it violates the maximum relative entropy principle; and that it is also problematic from a more classic Bayesian model selection perspective, because it produces Bayes factors that are sensitive to the value of its only hyperparameter. Using a large simulation study, we found in our previous work [Scutari in J Mach Learn Res (Proc Track PGM 2016) 52:438–448, 2016] that the Bayesian–Dirichlet sparse (BDs) score seems to provide better accuracy in structure learning; in this paper we further show that BDs does not suffer from the issues above, and we recommend to use it for sparse data instead of BDeu. Finally, will show that these issues are in fact different aspects of the same problem and a consequence of the distributional assumptions of the prior.},
 author = {Scutari, Marco},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2963204549},
 pages = {8--20},
 pdf = {http://proceedings.mlr.press/v73/scutari17a/scutari17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Dirichlet Bayesian network scores and the maximum relative entropy principle},
 url = {https://proceedings.mlr.press/v73/scutari17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-shan-gao17a,
 abstract = {Compiling Bayesian Networks (BNs) into secondary structures to implement efficient exact inference is a hot topic in probabilistic modeling. 
One class of algorithms to compile BNs is to transform the BNs into junction tree structures utilizing the conditional dependency in the network. 
Performing message passing on the junction tree structure, we can calculate marginal probabilities for any variables in the network efficiently. 
However, the message passing algorithm does not consider the local structure in the network. Since the ability to exploit local structure to avoid redundant calculations has a significant impact on exact inference,
in this article, we propose a fast message passing algorithm by exploiting local structure using Zero-suppressed Binary Decision Diagrams (ZDDs).
We convert all the components used in message passing algorithm into Multi-linear Functions (MLFs), and then compile them into compact representation using ZDDs.
We show that message passing on ZDDs can work more efficient than the conventional message passing algorithm on junction tree structures on some benchmark networks although it may be too memory consuming for some larger instances.
},
 author = {Ishihata, Masakazu and Gao, Shan and Minato, Shin-Ichi},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2757091925},
 pages = {117--128},
 pdf = {http://proceedings.mlr.press/v73/shan-gao17a/shan-gao17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Fast Message Passing Algorithm Using ZDD-Based Local Structure Compilation},
 url = {https://proceedings.mlr.press/v73/shan-gao17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-silander17a,
 abstract = {The BDeu scoring criterion for learning Bayesian network structures is known to be very
sensitive to the equivalent sample size hyper-parameter. Recently some authors have suggested
alternative Bayesian scoring criteria that appear to behave better than BDeu. So
is the problem solved? We will review the problem and suggested solutions and present
empirical assessment of the current situation.
},
 author = {Silander, Tomi},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2758640356},
 pages = {7--7},
 pdf = {http://proceedings.mlr.press/v73/silander17a/silander17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Hyperparameter sensitivity revisited},
 url = {https://proceedings.mlr.press/v73/silander17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-suzuki17a,
 abstract = {Preface},
 author = {Suzuki, Joe and Hyttinen, Antti and Malone, Brandon},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2756551969},
 pages = {1--2},
 pdf = {http://proceedings.mlr.press/v73/suzuki17a/suzuki17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Advanced Methodologies for Bayesian Networks 2017: Preface.},
 url = {https://proceedings.mlr.press/v73/suzuki17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-takahashi17a,
 abstract = {We propose \textit{restricted quasi Bayesian networks} as an efficient prototyping tool for designing computational models of individual cortical areas of the brain.
Restricted quasi Bayesian networks are simplified Bayesian networks
that only distinguish probability value 0 from other values.
Using our tool, it is possible to concentrate on the essential part of model design and efficiently construct prototypes.
We demonstrate that restricted quasi Bayesian networks actually work well as a prototyping tool by implementing a syntactic parser for an ambiguous English sentence.},
 author = {Takahashi, Naoto and Ichisugi, Yuuji},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2759305255},
 pages = {188--199},
 pdf = {http://proceedings.mlr.press/v73/takahashi17a/takahashi17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Restricted Quasi Bayesian Networks as a Prototyping Tool for Computational Models of Individual Cortical Areas},
 url = {https://proceedings.mlr.press/v73/takahashi17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-teruji-sugaya17a,
 abstract = {In this paper, we propose a new method to compile $s$-$t$ simple paths on a graph using a new compilation method called merging frontier based search. Recently, Nishino et al. proposed a top-down construction algorithm, which compiles $s$-$t$ simple paths into a Zero-suppressed SDD (ZSDD),  and they showed that this method is more efficient than simpath by Knuth.
However, since the method of Nishino et al. uses ZSDD as a tractable representation, it requires complicated steps for compilation. In this paper, we propose z-st-d-DNNF, which is a super set of ZSDD. By using this method instead of ZSDD, we show that more efficient $s$-$t$ simple paths compilation can be realized. },
 author = {Yasuda, Norihito and Sugaya, Teruji and Minato, Shin-Ichi},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2760607190},
 pages = {129--140},
 pdf = {http://proceedings.mlr.press/v73/teruji-sugaya17a/teruji-sugaya17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Fast Compilation of s-t Paths on a Graph for Counting and Enumeration.},
 url = {https://proceedings.mlr.press/v73/teruji-sugaya17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-yamazaki17a,
 abstract = {The structure learning is one of the main concerns in studies of the Bayesian networks.
In the present paper, we consider the network consisting of both observable and hidden nodes,
and propose a method to investigate the existence of a hidden node between two observable nodes,
which is the model selection problem between the networks with and without the middle hidden node.
When the network includes a hidden node, it has been known that there are singularities in the parameter space,
and the Fisher information matrix is not positive definite.
Then, the many conventional criteria for the structure learning based on the Laplace approximation do not work.
The proposed method is based on the Bayesian clustering,
and its asymptotic property justifies the result; the redundant labels are eliminated and the simplest structure is detected
even if there are singularities.},
 author = {Yamazaki, Keisuke and Motomura, Yoichi},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2805699900},
 pages = {165--175},
 pdf = {http://proceedings.mlr.press/v73/yamazaki17a/yamazaki17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Hidden Node Detection between Two Observable Nodes Based on Bayesian Clustering},
 url = {https://proceedings.mlr.press/v73/yamazaki17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-zhang17a,
 abstract = {Can we find the causal direction between two variables? How can we make optimal predictions
in the presence of distribution shift? We are often faced with such causal modeling
or prediction problems. Recently, with the rapid accumulation of huge volumes of data,
both causal discovery, i.e., learning causal information from purely observational data, and
machine learning are seeing exciting opportunities as well as great challenges. This talk will
be focused on recent advances in causal discovery and how causal information facilitates
understanding and solving certain problems of learning from heterogeneous data. In particular,
I will talk about basic approaches to causal discovery and address practical issues
in causal discovery, including nonstationarity or heterogeneity of the data and existence of
measurement error. Finally, I will discuss why and how underlying causal knowledge helps
in learning from heterogeneous data when the i.i.d. assumption is dropped, with transfer
learning? as a particular example.
},
 author = {Zhang, Kun},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 pages = {4--4},
 pdf = {http://proceedings.mlr.press/v73/zhang17a/zhang17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Learning and Machine Learning},
 url = {https://proceedings.mlr.press/v73/zhang17a.html},
 volume = {73},
 year = {2017}
}

@inproceedings{pmlr-v73-zhou17a,
 abstract = {Probabilistic graphical models, e.g., Markov network and Bayesian network have been well studied in the past two decades. However, it is still difficult to learn a reliable network structure, especially with limited data. Recent works found multi-task learning can improve the robustness of the learned networks by leveraging data from related tasks. In this paper, we focus on the estimation of Direct Acyclic Graph (DAG) of Bayesian network. Most existing multi-task or transfer learning algorithms for Bayesian network use the DAG relatedness as an inductive bias in the optimization of multiple structures. More specifically, some works firstly find shared hidden structures among related tasks, and then treat them as the structure penalties in the learning step. However, current works omit the setting that the shared hidden structure comes from different parts of different DAGs. Thus, in this paper, the Non-negative Matrix Factorization (NMF) is employed to learn a parts-based representation to mediate this problem. Theoretically, we show the plausibility of our approach. Empirically, we show that compared to single task learning, multi-task learning is better able to positively identify true edges with synthetic data and real-world landmine data. },
 author = {Zhou, Yun and Wang, Jiang and Zhu, Cheng and Zhang, Weiming},
 booktitle = {Proceedings of The 3rd International Workshop on Advanced Methodologies for Bayesian Networks},
 editor = {Hyttinen, Antti and Suzuki, Joe and Malone, Brandon},
 month = {20--22 Sep},
 openalex = {W2807454078},
 pages = {81--92},
 pdf = {http://proceedings.mlr.press/v73/zhou17a/zhou17a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multiple DAGs Learning with Non-negative Matrix Factorization.},
 url = {https://proceedings.mlr.press/v73/zhou17a.html},
 volume = {73},
 year = {2017}
}
