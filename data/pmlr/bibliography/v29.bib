@proceedings{ACML2013,
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Cheng Soon Ong and Tu Bao Ho},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the 5th Asian Conference on Machine Learning},
 volume = {29}
}

@inproceedings{pmlr-v29-Audiffren13,
 abstract = {We study the stability properties of nonlinear multi-task regression in reproducing Hilbert spaces with operator-valued kernels. Such kernels, a.k.a. multi-task kernels, are appropriate for learning prob- lems with nonscalar outputs like multi-task learning and structured out- put prediction. We show that multi-task kernel regression algorithms are uniformly stable in the general case of infinite-dimensional output spaces. We then derive under mild assumption on the kernel generaliza- tion bounds of such algorithms, and we show their consistency even with non Hilbert-Schmidt operator-valued kernels . We demonstrate how to apply the results to various multi-task kernel regression methods such as vector-valued SVR and functional ridge regression.},
 address = {Australian National University, Canberra, Australia},
 author = {Audiffren, Julien and Kadri, Hachem},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W1533423624},
 pages = {1--16},
 pdf = {http://proceedings.mlr.press/v29/Audiffren13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Stability of Multi-Task Kernel Regression Algorithms},
 url = {https://proceedings.mlr.press/v29/Audiffren13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Daswani13,
 abstract = {We extend the Q-learning algorithm from the Markov Decision Process setting to problems where observations are non-Markov and do not reveal the full state of the world i.e. to POMDPs. We do this in a natural manner by adding ‘0 regularisation to the pathwise squared Q-learning objective function and then optimise this over both a choice of map from history to states and the resulting MDP parameters. The optimisation procedure involves a stochastic search over the map class nested with classical Q-learning of the parameters. This algorithm ts perfectly into the feature reinforcement learning framework, which chooses maps based on a cost criteria. The cost criterion used so far for feature reinforcement learning has been model-based and aimed at predicting future states and rewards. Instead we directly predict the return, which is what is needed for choosing optimal actions. Our Q-learning criteria also lends itself immediately to a function approximation setting where features are chosen based on the history. This algorithm is somewhat similar to the recent line of work on lasso temporal dierence learning which aims at nding a small feature set with which one can perform policy evaluation. The distinction is that we aim directly for learning the Q-function of the optimal policy and we use ‘0 instead of ‘1 regularisation. We perform an experimental evaluation on classical benchmark domains and nd improvement in convergence speed as well as in economy of the state representation. We also compare against MC-AIXI on the large Pocman domain and achieve competitive performance in average reward. We use less than half the CPU time and 36 times less memory. Overall, our algorithm hQL provides a better combination of computational, memory and data eciency than existing algorithms in this setting.},
 address = {Australian National University, Canberra, Australia},
 author = {Daswani, Mayank and Sunehag, Peter and Hutter, Marcus},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2144655553},
 pages = {213--228},
 pdf = {http://proceedings.mlr.press/v29/Daswani13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Q-learning for history-based reinforcement learning},
 url = {https://proceedings.mlr.press/v29/Daswani13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Durrant13,
 abstract = {We examine the performance of an ensemble of randomly-projected Fisher Linear Discriminant classifiers, focusing on the case when there are fewer training observations than data dimensions. Our ensemble is learned from a sequence of randomly-projected representations of the original high dimensional data and therefore for this approach data can be collected, stored and processed in such a compressed form. The specific form and simplicity of this ensemble permits a direct and much more detailed analysis than existing generic tools in previous works.  In particular, we are able to derive the exact form of the generalization error of our ensemble, conditional on the training set, and based on this  we give theoretical guarantees which directly link the performance of the ensemble to that of the corresponding linear discriminant learned in the full data space. To the best of our knowledge these are the first theoretical results to prove such an explicit link for any classifier and classifier ensemble pair. Furthermore we show that the randomly-projected ensemble is equivalent to implementing a sophisticated regularization scheme to the linear discriminant learned in the original data space  and this prevents overfitting in conditions of small sample size where pseudo-inverse FLD learned in the data space is provably poor.},
 address = {Australian National University, Canberra, Australia},
 author = {Durrant, Robert and Kaban, Ata},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2175444967},
 pages = {17--32},
 pdf = {http://proceedings.mlr.press/v29/Durrant13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Random Projections as Regularizers: Learning a Linear Discriminant Ensemble from Fewer Observations than Dimensions},
 url = {https://proceedings.mlr.press/v29/Durrant13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Eyck13,
 abstract = {Monte Carlo tree search (MCTS) is a sampling and simulation based technique for searching in large search spaces containing both decision nodes and probabilistic events. This technique has recently become popular due to its successful application to games, e.g. Poker Van den Broeck et al. (2009) and Go Coulom (2006); Chaslot et al. (2006); Gelly and Silver (2012)). Such games have known rules and the alternation between self-moves and non-deterministic events or opponent moves can be used to prune uninteresting branches. In this paper we study a real-world setting where the processes in the domain have a high degree of uncertainty and the need for longer-term planning implies a sequence of (planning) decisions without any intermediate feedback. Fortunately, unlike the combinatorial complexity in strategic games, many real-world environments can be approximated by ecient algorithms on a short term. This paper proposes an MCTS variant using a new type of prior information based on estimating the eects of part of the world and explores its application to the problem of hospital planning, where machine learning algorithms can be used to predict the length of stay of patients for each of the dierent stages of their recovery.},
 address = {Australian National University, Canberra, Australia},
 author = {Eyck, Jelle Van and Ramon, Jan and Guiza, Fabian and MeyFroidt, Geert and Bruynooghe, Maurice and Berghe, Greet Van den},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2141388186},
 pages = {33--47},
 pdf = {http://proceedings.mlr.press/v29/Eyck13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Guided Monte Carlo Tree Search for Planning in Learned Environments},
 url = {https://proceedings.mlr.press/v29/Eyck13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Fornoni13,
 abstract = {Kernelized Support Vector Machines (SVM) have gained the status of o-the-shelf classiers, able to deliver state of the art performance on almost any problem. Still, their practical use is constrained by their computational and memory complexity, which grows super-linearly with the number of training samples. In order to retain the low training and testing complexity of linear classiers and the exibility of non linear ones, a growing, promising alternative is represented by methods that learn non-linear classiers through local combinations of linear ones. In this paper we propose a new multi class local classier,},
 address = {Australian National University, Canberra, Australia},
 author = {Fornoni, Marco and Caputo, Barbara and Orabona, Francesco},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2135670105},
 pages = {229--244},
 pdf = {http://proceedings.mlr.press/v29/Fornoni13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multiclass Latent Locally Linear Support Vector Machines},
 url = {https://proceedings.mlr.press/v29/Fornoni13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Forouzan13,
 abstract = {Maximum a posteriori (MAP) inference is one of the fundamental inference tasks in graphical models. MAP inference is in general NP-hard, making approximate methods of interest for many problems. One successful class of approximate inference algorithms is based on linear programming (LP) relaxations. The augmented Lagrangian method can be used to overcome a lack of strict convexity in LP relaxations, and the Alternating Direction Method of Multipliers (ADMM) provides an elegant algorithm for finding the saddle point of the augmented Lagrangian. Here we present an ADMM-based algorithm to solve the primal form of the MAPLP whose closed form updates are based on a linear approximation technique. Our technique gives efficient, closed form updates that converge to the global optimum of the LP relaxation. We compare our algorithm to two existing ADMM-based MAP-LP methods, showing that our technique is faster on general, non-binary or non-pairwise models.},
 address = {Australian National University, Canberra, Australia},
 author = {Forouzan, Sholeh and Ihler, Alexander},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2130300788},
 pages = {48--61},
 pdf = {http://proceedings.mlr.press/v29/Forouzan13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Linear Approximation to ADMM for MAP inference},
 url = {https://proceedings.mlr.press/v29/Forouzan13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Galichet13,
 abstract = {Motivated by applications in energy management, this paper presents the Multi-Armed Risk-Aware Bandit (MaRaB) algorithm. With the goal of limiting the exploration of risky arms, MaRaB takes as arm quality its conditional value at risk. When the user-supplied risk level goes to 0, the arm quality tends toward the essential infimum of the arm distribution density, and MaRaB tends toward the MIN multi-armed bandit algorithm, aimed at the arm with maximal minimal value. As a first contribution, this paper presents a theoretical analysis of the MIN algorithm under mild assumptions, establishing its robustness comparatively to UCB. The analysis is  supported by extensive experimental validation of MIN and MaRaB compared to UCB and  state-of-art risk-aware MAB algorithms on  artificial and real-world problems. },
 address = {Australian National University, Canberra, Australia},
 author = {Galichet, Nicolas and Sebag, MichÃ¨le and Teytaud, Olivier},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 pages = {245--260},
 pdf = {http://proceedings.mlr.press/v29/Galichet13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Exploration vs Exploitation vs Safety: Risk-Aware Multi-Armed Bandits},
 url = {https://proceedings.mlr.press/v29/Galichet13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Gieseke13,
 abstract = {Maximum margin clustering can be regarded as the direct extension of support vector machines to unsupervised learning scenarios. The goal is to partition unlabeled data into two classes such that a subsequent application of a support vector machine would yield the overall best result (with respect to the optimization problem associated with support vector machines). While being very appealing from a conceptual point of view, the combinatorial nature of the induced optimization problem renders a direct application of this concept difficult. In order to obtain efficient optimization schemes, various surrogates of the original problem definition have been proposed in the literature. In this work, we consider one of these variants, called unsupervised regularized least-squares classification, which is based on the square loss, and develop polynomial upper runtime bounds for the induced combinatorial optimization task. In particular, we show that forn patterns and kernel matrix of fixed rank r (with given eigendecomposition), one can obtain an optimal solution inO(n r ) time for r 2 and inO(n r 1 ) time for r 3. The algorithmic framework is based on an interesting connection to the field of quadratic zero-one programming and permits the computation of exact solutions for the more general case of non-linear kernel functions in polynomial time.},
 address = {Australian National University, Canberra, Australia},
 author = {Gieseke, Fabian and Pahikkala, Tapio and Igel, Christian},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2168304473},
 pages = {62--71},
 pdf = {http://proceedings.mlr.press/v29/Gieseke13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Polynomial Runtime Bounds for Fixed-Rank Unsupervised Least-Squares Classification},
 url = {https://proceedings.mlr.press/v29/Gieseke13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Glasmachers13,
 abstract = {Coordinate descent (CD) algorithms have become the method of choice for solving a number of machine learning tasks. They are particularly popular for training linear models, includ- ing linear support vector machine classication, LASSO regression, and logistic regression. We propose an extension of the CD algorithm, called the adaptive coordinate frequencies (ACF) method. This modied CD scheme does not treat all coordinates equally, in that it does not pick all coordinates equally often for optimization. Instead the relative frequen- cies of coordinates are subject to online adaptation. The resulting optimization scheme can result in signicant speed-ups. We demonstrate the usefulness of our approach on a number of large scale machine learning problems.},
 address = {Australian National University, Canberra, Australia},
 author = {Glasmachers, Tobias and Dogan, Urun},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2117229703},
 pages = {72--86},
 pdf = {http://proceedings.mlr.press/v29/Glasmachers13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Accelerated Coordinate Descent with Adaptive Coordinate Frequencies},
 url = {https://proceedings.mlr.press/v29/Glasmachers13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Jiang13,
 abstract = {Recommender systems are often based on collaborative ltering. Previous researches on collaborative ltering mainly focus on one single recommender or formulating hybrid with dierent approaches. In consideration of the problems of sparsity, recommender error rate, sample weight update, and potential, we adapt AdaBoost and propose two novel boosting frameworks for collaborative ltering. Each of the frameworks combines multiple homogeneous recommenders, which are based on the same collaborative ltering algorithm with dierent sample weights. We use seven popular collaborative ltering algorithms to evaluate the two frameworks with two MovieLens datasets of dierent scale. Experimental result shows the proposed frameworks improve the performance of collaborative ltering.},
 address = {Australian National University, Canberra, Australia},
 author = {Jiang, Xiaotian and Niu, Zhendong and Guo, Jiamin and Mustafa, Ghulam and Lin, Zihan and Chen, Baomi and Zhou, Qian},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2163864220},
 pages = {87--99},
 pdf = {http://proceedings.mlr.press/v29/Jiang13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Novel Boosting Frameworks to Improve the Performance of Collaborative Filtering},
 url = {https://proceedings.mlr.press/v29/Jiang13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Kadri13,
 abstract = {We study the problem of learning from multiple views using kernel methods in a supervised setting. We approach this problem from a multi-task learning point of view and illustrate how to capture the interesting multimodal structure of the data using multi-task kernels. Our analysis shows that the multi-task perspective oers the exibility to design more ecient multiple-source learning algorithms, and hence the ability to exploit multiple descriptions of the data. In particular, we formulate the multimodal learning framework using vector-valued reproducing kernel Hilbert spaces, and we derive specic multi-task kernels that can operate over multiple modalities. Finally, we analyze the vector-valued regularized least squares algorithm in this context, and demonstrate its potential in a series of experiments with a real-world multimodal data set.},
 address = {Australian National University, Canberra, Australia},
 author = {Kadri, Hachem and Ayache, Stephane and Capponi, CÃ©cile and KoÃ§o, Sokol and DupÃ©, FranÃ§ois-Xavier and Morvant, Emilie},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2136653245},
 pages = {261--276},
 pdf = {http://proceedings.mlr.press/v29/Kadri13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The Multi-Task Learning View of Multimodal Data},
 url = {https://proceedings.mlr.press/v29/Kadri13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Koco13,
 abstract = {In imbalanced multi-class classication problems, the misclassication rate as an error measure may not be a relevant choice. Several methods have been developed where the performance measure retained richer information than the mere misclassication rate: misclassication costs, ROC-based information, etc. Following this idea of dealing with alternate measures of performance, we propose to address imbalanced classication problems by using a new measure to be optimized: the norm of the confusion matrix. Indeed, recent results show that using the norm of the confusion matrix as an error measure can be quite interesting due to the ne-grain informations contained in the matrix, especially in the case of imbalanced classes. Our rst contribution then consists in showing that optimizing criterion based on the confusion matrix gives rise to a common background for cost-sensitive methods aimed at dealing with imbalanced classes learning problems. As our second contribution, we propose an extension of a recent multi-class boosting method | namely AdaBoost.MM | to the imbalanced class problem, by greedily minimizing the empirical norm of the confusion matrix. A theoretical analysis of the properties of the proposed method is presented, while experimental results illustrate the behavior of the algorithm and show the relevancy of the approach compared to other methods.},
 address = {Australian National University, Canberra, Australia},
 author = {KoÃ§o, Sokol and Capponi, CÃ©cile},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W96499654},
 pages = {277--292},
 pdf = {http://proceedings.mlr.press/v29/Koco13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On multi-class classification through the minimization of the confusion matrix norm},
 url = {https://proceedings.mlr.press/v29/Koco13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Komiyama13,
 abstract = {We investigate a stochastic multi-armed bandit problem in which the forecaster’s choice is restricted. In this problem, rounds are divided into lock-up periods and the forecaster must select the same arm throughout a period. While there has been much work on finding optimal algorithms for the stochastic multi-armed bandit problem, their use under restricted conditions is not obvious. We extend the application ranges of these algorithms by proposing their natural conversion from ones for the stochastic bandit problem (indexbased algorithms and greedy algorithms) to ones for the multi-armed bandit problem with lock-up periods. We prove that the regret of the converted algorithms is O(log T +Lmax), where T is the total number of rounds and Lmax is the maximum size of the lock-up periods. The regret is preferable, except for the case when the maximum size of the lock-up periods is large. For these cases, we propose a meta-algorithm that results in a smaller regret by using a empirical best arm for large periods. We empirically compare and discuss these algorithms.},
 address = {Australian National University, Canberra, Australia},
 author = {Komiyama, Junpei and Sato, Issei and Nakagawa, Hiroshi},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W42040492},
 pages = {100--115},
 pdf = {http://proceedings.mlr.press/v29/Komiyama13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-armed Bandit Problem with Lock-up Periods},
 url = {https://proceedings.mlr.press/v29/Komiyama13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Le13,
 abstract = {Learning distances that are specically designed to compare histograms in the probability simplex has recently attracted the attention of the community. Learning such distances is important because most machine learning problems involve bags of features rather than simple vectors. Ample empirical evidence suggests that the Euclidean distance in general and Mahalanobis metric learning in particular may not be suitable to quantify distances between points in the simplex. We propose in this paper a new contribution to address this problem by generalizing a family of embeddings proposed by Aitchison (1982) to map the probability simplex onto a suitable Euclidean space. We provide algorithms to estimate the parameters of such maps, and show that these algorithms lead to representations that outperform alternative approaches to compare histograms.},
 address = {Australian National University, Canberra, Australia},
 author = {Le, Tam and Cuturi, Marco},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2141014758},
 pages = {293--308},
 pdf = {http://proceedings.mlr.press/v29/Le13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Generalized Aitchison Embeddings for Histograms},
 url = {https://proceedings.mlr.press/v29/Le13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Liu13,
 abstract = {Many problems in machine learning and other fields can be (re)formulated as linearly constrained separable convex programs. In most of the cases, there are multiple blocks of variables. However, the traditional alternating direction method (ADM) and its linearized version (LADM, obtained by linearizing the quadratic penalty term) are for the two-block case and cannot be naively generalized to solve the multi-block case. So there is great demand on extending the ADM based methods for the multi-block case. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-block separable convex programs efficiently. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the sufficient and necessary conditions for global convergence. We further propose a simple optimality measure and reveal the convergence rate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, with refined parameter estimation we devise a practical version of LADMPSAP for faster convergence. Finally, we generalize LADMPSAP to handle programs with more difficult objective functions by linearizing part of the objective function as well. LADMPSAP is particularly suitable for sparse representation and low-rank recovery problems because its subproblems have closed form solutions and the sparsity and low-rankness of the iterates can be preserved during the iteration. It is also highly parallelizable and hence fits for parallel or distributed computing. Numerical experiments testify to the advantages of LADMPSAP in speed and numerical accuracy.},
 address = {Australian National University, Canberra, Australia},
 author = {Liu, Risheng and Lin, Zhouchen and Su, Zhixun},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2071631699},
 pages = {116--132},
 pdf = {http://proceedings.mlr.press/v29/Liu13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning},
 url = {https://proceedings.mlr.press/v29/Liu13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Louche13,
 abstract = {We tackle the problem of learning linear classifiers from noisy datasets in a multiclass setting. The two-class version of this problem was studied a few years ago where the proposed approaches to combat the noise revolve around a Perceptron learning scheme fed with peculiar examples computed through a weighted average of points from the noisy training set. We propose to build upon these approaches and we introduce a new algorithm called Unconfused Multiclass additive Algorithm (U MA) which may be seen as a generalization to the multiclass setting of the previous approaches. In order to characterize the noise we use the confusion matrix as a multiclass extension of the classification noise studied in the aforementioned literature. Theoretically well-founded, U MA furthermore displays very good empirical noise robustness, as evidenced by numerical simulations conducted on both synthetic and real data.},
 address = {Australian National University, Canberra, Australia},
 author = {Louche, Ugo and Ralaivola, Liva},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2104970751},
 pages = {309--324},
 pdf = {http://proceedings.mlr.press/v29/Louche13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Unconfused ultraconservative multiclass algorithms},
 url = {https://proceedings.mlr.press/v29/Louche13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Lu13,
 abstract = {Collaborative Filtering (CF) is one of the most successful learning techniques in building real-world recommender systems. Traditional CF algorithms are often based on batch machine learning methods which suer from several critical drawbacks, e.g., extremely expensive model retraining cost whenever new samples arrive, unable to capture the latest change of user preferences over time, and high cost and slow reaction to new users or products extension. Such limitations make batch learning based CF methods unsuitable for real-world online applications where data often arrives sequentially and user preferences may change dynamically and rapidly. To address these limitations, we investigate online collaborative ltering techniques for building live recommender systems where the CF model can evolve on-the-y over time. Unlike the regular rst order CF algorithms},
 address = {Australian National University, Canberra, Australia},
 author = {Lu, Jing and Hoi, Steven and Wang, Jialei},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2121806078},
 pages = {325--340},
 pdf = {http://proceedings.mlr.press/v29/Lu13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Second Order Online Collaborative Filtering},
 url = {https://proceedings.mlr.press/v29/Lu13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Moustafa13,
 abstract = {Manifold learning algorithms rely on a neighbourhood graph to provide an estimate of the data’s local topology. Unfortunately, current methods for estimating local topology assume local Euclidean geometry and locally uniform data density, which often leads to poor data embeddings. We address these shortcomings by proposing a framework that combines local learning with parametric density estimation for local topology estimation. Given a data set D X , we rst estimate a new metric space ( X;dX) that characterizes the varying sample density ofX in X, then use (X;dX) as a new (pilot) input space for the graph construction step of the manifold learning process. The proposed framework results in signicantly improved embeddings, which we demonstrated objectively by assessing clustering accuracy.},
 address = {Australian National University, Canberra, Australia},
 author = {Moustafa, Karim Abou- and Schuurmans, Dale and Ferrie, Frank},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2151427378},
 pages = {341--356},
 pdf = {http://proceedings.mlr.press/v29/Moustafa13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning a Metric Space for Neighbourhood Topology Estimation: Application to Manifold Learning},
 url = {https://proceedings.mlr.press/v29/Moustafa13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Neumann13,
 abstract = {Exploiting autocorrelation for node-label prediction in networked data has led to great success. However, when dealing with sparsely labeled networks, common in present-day tasks, the autocorrelation assumption is dicult to exploit. Taking a step beyond, we propose the coinciding walk kernel (cwk), a novel kernel leveraging label-structure similarity ‐ the idea that nodes with similarly arranged labels in their local neighbourhoods are likely to have the same label ‐ for learning problems on partially labeled graphs. Inspired by the success of random walk based schemes for the construction of graph kernels, cwk is defined in terms of the probability that the labels encountered during parallel random walks coincide. In addition to its intuitive probabilistic interpretation, coinciding walk kernels outperform existing kernel- and walk-based methods on the task of node-label prediction in sparsely labeled graphs with high label-structure similarity. We also show that computing cwks is faster than many state-of-the-art kernels on graphs. We evaluate cwks on several realworld networks, including cocitation and coauthor graphs, as well as a graph of interlinked populated places extracted from the dbpedia knowledge base.},
 address = {Australian National University, Canberra, Australia},
 author = {Neumann, Marion and Garnett, Roman and Kersting, Kristian},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2160840604},
 pages = {357--372},
 pdf = {http://proceedings.mlr.press/v29/Neumann13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Coinciding Walk Kernels: Parallel Absorbing Random Walks for Learning with Graphs and Few Labels},
 url = {https://proceedings.mlr.press/v29/Neumann13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Nguyen13,
 abstract = {The success of any machine learning system depends critically on eective representations of data. In many cases, especially those in vision, it is desirable that a representation scheme uncovers the parts-based, additive nature of the data. Of current representation learning schemes, restricted Boltzmann machines (RBMs) have proved to be highly eective in unsupervised settings. However, when it comes to parts-based discovery, RBMs do not usually produce satisfactory results. We enhance such capacity of RBMs by introducing nonnegativity into the model weights, resulting in a variant called nonnegative restricted Boltzmann machine (NRBM). The NRBM produces not only controllable decomposition of data into interpretable parts but also oers a way to estimate the intrinsic nonlinear dimensionality of data. We demonstrate the capacity of our model on well-known datasets of handwritten digits, faces and documents. The decomposition quality on images is comparable with or better than what produced by the nonnegative matrix factorisation (NMF), and the thematic features uncovered from text are qualitatively interpretable in a similar manner to that of the latent Dirichlet allocation (LDA). However, the learnt features, when used for classication,},
 address = {Australian National University, Canberra, Australia},
 author = {Nguyen, Tu Dinh and Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W166926187},
 pages = {133--148},
 pdf = {http://proceedings.mlr.press/v29/Nguyen13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Parts-based Representations with Nonnegative Restricted Boltzmann Machine},
 url = {https://proceedings.mlr.press/v29/Nguyen13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Ohara13,
 abstract = {We address a problem of efficiently estimating the influence of a node in information diffusion over a social network. Since the information diffusion is a stochastic process, the influence degree of a node is quantified by the expectation, which is usually obtained by very time consuming many runs of simulation. Our contribution is that we proposed a framework for predictive simulation based on the leave-N-out cross validation technique that well approximates the error from the unknown ground truth for two target problems: one to estimate the influence degree of each node, and the other to identify top-K influential nodes. The method we proposed for the first problem estimates the approximation error of the influence degree of each node, and the method for the second problem estimates the precision of the derived top-K nodes, both without knowing the true influence degree. We experimentally evaluate the proposed methods using the three real world networks, and show that they can serve as a good measure to solve the target problems with far fewer runs of simulation ensuring the accuracy if N is appropriately chosen, and that estimating the top-K nodes is easier than estimating the influence degree, which means one can identify the influential nodes without knowing exactly their influence degree. },
 address = {Australian National University, Canberra, Australia},
 author = {Ohara, Kouzou and Saito, Kazumi and Kimura, Masahiro and Motoda, Hiroshi},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2939114988},
 pages = {149--164},
 pdf = {http://proceedings.mlr.press/v29/Ohara13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Resampling-based predictive simulation framework of stochastic diffusion model for identifying top-K influential nodes},
 url = {https://proceedings.mlr.press/v29/Ohara13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Ong13,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 address = {Australian National University, Canberra, Australia},
 author = {Ong, Cheng Soon and Ho, Tu Bao},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W4240465921},
 pages = {1--17},
 pdf = {http://proceedings.mlr.press/v29/Ong13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v29/Ong13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Peltonen13,
 abstract = {In visual data exploration with scatter plots, no single plot is sufficient to analyze complicated high-dimensional data sets. Given numerous visualizations created with different features or methods, meta-visualization is needed to analyze the visualizations together. We solve \emphhow to arrange numerous visualizations onto a meta-visualization display, so that their similarities and differences can be analyzed. We introduce a machine learning approach to optimize the meta-visualization, based on an information retrieval perspective: two visualizations are similar if the analyst would retrieve similar neighborhoods between data samples from either visualization. Based on the approach, we introduce a nonlinear embedding method for meta-visualization: it optimizes locations of visualizations on a display, so that visualizations giving similar information about data are close to each other.},
 address = {Australian National University, Canberra, Australia},
 author = {Peltonen, Jaakko and Lin, Ziyuan},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W19427831},
 pages = {165--180},
 pdf = {http://proceedings.mlr.press/v29/Peltonen13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Information Retrieval Perspective to Meta-visualization},
 url = {https://proceedings.mlr.press/v29/Peltonen13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Premachandra13,
 abstract = {Prediction markets which trade on contracts representing unknown future outcomes are designed specically to aggregate expert predictions via the market price. While there are some existing machine learning interpretations for the market price and connections to Bayesian updating under the equilibrium analysis of such markets, there is less of an understanding of what the instantaneous price in sequentially traded markets means. In this paper we show that the prices generated in sequentially traded prediction markets are stochastic approximations to the price given by an equilibrium analysis. We do so by showing the equilibrium price is a solution to a stochastic optimisation problem which is solved by stochastic mirror descent (SMD) by a class of sequential pricing mechanisms. This connection leads us to propose a scheme called \mini-trading which introduces a parameter related to the learning rate in SMD. We prove several properties of this scheme and show that it can improve the stability of prices in sequentially traded prediction markets.},
 address = {Australian National University, Canberra, Australia},
 author = {Premachandra, Mindika and Reid, Mark},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2129192811},
 pages = {373--387},
 pdf = {http://proceedings.mlr.press/v29/Premachandra13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Aggregating Predictions via Sequential Mini-Trading},
 url = {https://proceedings.mlr.press/v29/Premachandra13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Shen13,
 abstract = {Bipartite ranking is a fundamental ranking problem that learns to order relevant instances ahead of irrelevant ones. The pair-wise approach for bi-partite ranking construct a quadratic number of pairs to solve the problem, which is infeasible for large-scale data sets. The point-wise approach, albeit more efficient, often results in inferior performance. That is, it is difficult to conduct bipartite ranking accurately and efficiently at the same time. In this paper, we develop a novel active sampling scheme within the pair-wise approach to conduct bipartite ranking efficiently. The scheme is inspired from active learning and can reach a competitive ranking performance while focusing only on a small subset of the many pairs during training. Moreover, we propose a general Combined Ranking and Classification (CRC) framework to accurately conduct bipartite ranking. The framework unifies point-wise and pair-wise approaches and is simply based on the idea of treating each instance point as a pseudo-pair. Experiments on 14 real-word large-scale data sets demonstrate that the proposed algorithm of Active Sampling within CRC, when coupled with a linear Support Vector Machine, usually outperforms state-of-the-art point-wise and pair-wise ranking approaches in terms of both accuracy and efficiency.},
 address = {Australian National University, Canberra, Australia},
 author = {Shen, Wei-Yuan and Lin, Hsuan-Tien},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2118363113},
 pages = {388--403},
 pdf = {http://proceedings.mlr.press/v29/Shen13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Active Sampling of Pairs and Points for Large-scale Linear Bipartite Ranking},
 url = {https://proceedings.mlr.press/v29/Shen13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Su13,
 abstract = {We present new methods for multilabel classification, relying on ensemble learning on a collection of random output graphs imposed on the multilabel, and a kernel-based structured output learner as the base classifier. For ensemble learning, differences among the output graphs provide the required base classifier diversity and lead to improved performance in the increasing size of the ensemble. We study different methods of forming the ensemble prediction, including majority voting and two methods that perform inferences over the graph structures before or after combining the base models into the ensemble. We put forward a theoretical explanation of the behaviour of multilabel ensembles in terms of the diversity and coherence of microlabel predictions, generalizing previous work on single target ensembles. We compare our methods on a set of heterogeneous multilabel benchmark problems against the state-of-the-art machine learning approaches, including multilabel AdaBoost, convex multitask feature learning, as well as single target learning approaches represented by Bagging and SVM. In our experiments, the random graph ensembles are very competitive and robust, ranking first or second on most of the datasets. Overall, our results show that our proposed random graph ensembles are viable alternatives to flat multilabel and multitask learners.},
 address = {Australian National University, Canberra, Australia},
 author = {Su, Hongyu and Rousu, Juho},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2115386194},
 pages = {404--418},
 pdf = {http://proceedings.mlr.press/v29/Su13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multilabel classification through random graph ensembles},
 url = {https://proceedings.mlr.press/v29/Su13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Tran13,
 abstract = {The predictive accuracy of a learning algorithm can be split into specicity and sensitivity, amongst other decompositions. Sensitivity, also known as completeness, is the ratio of true positives to the total number of positive examples, while specicity is the ratio of true negative to the total negative examples. In top-down learning methods of inductive logic programming, there is generally a bias towards sensitivity, since the learning starts from the most general rule (everything is positive) and specialises by excluding some of the negative examples. While this is often useful, it is not always the best choice: for example, in novelty detection, where the negative examples are rare and often varied, they may well be ignored by the learning. In this paper we introduce a method that attempts to remove the bias towards sensitivity by fortifying the model by computing and then including in the model some descriptions of the negative data even if they are considered redundant by the normal learning algorithm. We demonstrate the method on a set of standard datasets for description logic learning and show that the predictive accuracy increases.},
 address = {Australian National University, Canberra, Australia},
 author = {Tran, An and Dietrich, Jens and Guesgen, Hans and Marsland, Stephen},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2140088893},
 pages = {419--434},
 pdf = {http://proceedings.mlr.press/v29/Tran13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improving Predictive Specificity of Description Logic Learners by Fortification},
 url = {https://proceedings.mlr.press/v29/Tran13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Vanck13,
 abstract = {The concept of covariate shift in supervised data analysis describes a dierence between the training and test distribution while the conditional distribution remains the same. To improve the prediction performance one can address such a change by using individual weights for each training datapoint, which emphasizes the training points close to the test data set so that these get a higher signicance. We propose a new method for calculating such weights by minimizing a Fourier series approximation of distance measures, in particular we consider the total variation distance, the Euclidean distance and Kullback-Leibler divergence. To be able to use the Fourier approach for higher dimensional data, we employ the so-called hyperbolic cross approximation. Results show that the new approach can compete with the latest methods and that on real life data an improved performance can be obtained.},
 address = {Australian National University, Canberra, Australia},
 author = {Vanck, Thomas and Garcke, Jochen},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2147767721},
 pages = {435--450},
 pdf = {http://proceedings.mlr.press/v29/Vanck13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Using Hyperbolic Cross Approximation to measure and compensate Covariate Shift},
 url = {https://proceedings.mlr.press/v29/Vanck13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Wang13a,
 abstract = {We present locally-linear learning machines (L3M) for multi-class classification. We formulate a global convex risk function to jointly learn linear feature space partitions and region-specific linear classifiers. L3M’s features such as: (1) discriminative power similar to Kernel SVMs and Adaboost; (2) tight control on generalization error; (3) low training time cost due to on-line training; (4) low test-time costs due to local linearity; are all potentially well-suited for “big-data” applications. We derive tight convex surrogates for the empirical risk function associated with space partitioning classifiers. These empirical risk functions are non-convex since they involve products of indicator functions. We obtain a global convex surrogate by first embedding empirical risk loss as an extremal point of an optimization problem and then convexifying this resulting problem. Using the proposed convex formulation, we demonstrate improvement in classification performance, test and training time relative to common discriminative learning methods on challenging multiclass data sets.},
 address = {Australian National University, Canberra, Australia},
 author = {Wang, Joseph and Saligrama, Venkatesh},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2154907969},
 pages = {451--466},
 pdf = {http://proceedings.mlr.press/v29/Wang13a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Locally-Linear Learning Machines (L3M)},
 url = {https://proceedings.mlr.press/v29/Wang13a.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Wang13b,
 abstract = {Co-training is a famous semi-supervised learning paradigm exploiting unlabeled data with two views. Most previous theoretical analyses on co-training are based on the assumption that each of the views is sucient to correctly predict the label. However, this assumption can hardly be met in real applications due to feature corruption or various feature noise. In this paper, we present the theoretical analysis on co-training when neither view is sucient. We dene the diversity between the two views with respect to the condence of prediction and prove that if the two views have large diversity, co-training is able to improve the learning performance by exploiting unlabeled data even with insucient views. We also discuss the relationship between view insuciency and diversity, and give some implications for understanding of the dierence between co-training and co-regularization.},
 address = {Australian National University, Canberra, Australia},
 author = {Wang, Wei and Zhou, Zhi-Hua},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2166984637},
 pages = {467--482},
 pdf = {http://proceedings.mlr.press/v29/Wang13b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Co-Training with Insufficient Views},
 url = {https://proceedings.mlr.press/v29/Wang13b.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Watanabe13,
 abstract = {The normalized maximum likelihood model achieves the minimax coding (log-loss) regret for data of xed sample size n. However, it is a batch strategy, i.e., it requires that n be known in advance. Furthermore, it is computationally infeasible for most statistical models, and several computationally feasible alternative strategies have been devised. We characterize the achievability of asymptotic minimaxity by batch strategies (i.e., strategies that depend on n) as well as online strategies (i.e., strategies independent of n). On one hand, we conjecture that for a large class of models, no online strategy can be asymptotically minimax. We prove that this holds under a slightly stronger denition of asymptotic minimaxity. Our numerical experiments support the conjecture about non-achievability by so called last-step minimax algorithms, which are independent of n. On the other hand, we show that in the multinomial model, a Bayes mixture dened by the conjugate Dirichlet prior with a simple dependency on n achieves asymptotic minimaxity for all sequences, thus providing a simpler asymptotic minimax strategy compared to earlier work by Xie and Barron. The numerical results also demonstrate superior nite-sample behavior by a},
 address = {Australian National University, Canberra, Australia},
 author = {Watanabe, Kazuho and Roos, Teemu and MyllymÃ¤ki, Petri},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W67589879},
 pages = {181--196},
 pdf = {http://proceedings.mlr.press/v29/Watanabe13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Achievability of Asymptotic Minimax Regret in Online and Batch Prediction},
 url = {https://proceedings.mlr.press/v29/Watanabe13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Wirth13,
 abstract = {Reinforcement learning algorithms are usually hard to use for non expert users. It is required to consider several aspects like the denition of state-, action- and reward-space as well as the algorithms hyperparameters. Preference based approaches try to address these problems by omitting the requirement for exact rewards, replacing them with preferences over solutions. Some algorithms have been proposed within this framework, but they are usually requiring parameterized policies which is again a hinderance for their application. Monte Carlo based approaches do not have this restriction and are also model free. Hence, we present a new preference-based reinforcement learning algorithm, utilizing Monte Carlo estimates. The main idea is to estimate the relativeQ-value of two actions for the same state within a every-visit framework. This means, preferences are used to estimate the Q-value of state-action pairs within a trajectory, based on the feedback concerning the complete trajectory. The algorithm is evaluated on three common benchmark problems, namely mountain car, inverted pendulum and acrobot, showing its advantage over a closely related algorithm which is also using estimates for intermediate states, but based on a probability theorem. In comparison to SARSA( ), EPMC converges somewhat slower, but computes policies that are almost as good or better.},
 address = {Australian National University, Canberra, Australia},
 author = {Wirth, Christian and FÃ¼rnkranz, Johannes},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2101589629},
 pages = {483--497},
 pdf = {http://proceedings.mlr.press/v29/Wirth13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {EPMC: Every Visit Preference Monte Carlo for Reinforcement Learning},
 url = {https://proceedings.mlr.press/v29/Wirth13.html},
 volume = {29},
 year = {2013}
}

@inproceedings{pmlr-v29-Wu13,
 abstract = {The problem of multi-label classification has attracted great interests in the last decade. Multi-label classification refers to the problems where an example that is represented by a single instance can be assigned to more than one category. Until now, most of the researches on multi-label classification have focused on supervised settings whose assumption is that large amount of labeled training data is available. Unfortunately, labeling training example is expensive and time-consuming, especially when it has more than one label. However, in many cases abundant unlabeled data is easy to obtain. Current attempts toward exploiting unlabeled data for multi-label classification work under the transductive setting, which aim at making predictions on existing unlabeled data while can not generalize to new unseen data. In this paper, the problem of inductive semi-supervised multi-label classification is studied, where a new approach named iMLCU, i.e. inductive Multi-Label Classification with Unlabeled data, is proposed. We formulate the inductive semi-supervised multi-label learning as an optimization problem of learning linear models and ConCave Convex Procedure (CCCP) is applied to optimize the non-convex optimization problem. Empirical studies on twelve diversified real-word multi-label learning tasks clearly validate the superiority of iMLCU against the other well-established multi-label learning approaches.},
 address = {Australian National University, Canberra, Australia},
 author = {Wu, Le and Zhang, Min-Ling},
 booktitle = {Proceedings of the 5th Asian Conference on Machine Learning},
 editor = {Ong, Cheng Soon and Ho, Tu Bao},
 month = {13--15 Nov},
 openalex = {W2157552837},
 pages = {197--212},
 pdf = {http://proceedings.mlr.press/v29/Wu13.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-Label Classification with Unlabeled Data: An Inductive Approach},
 url = {https://proceedings.mlr.press/v29/Wu13.html},
 volume = {29},
 year = {2013}
}
