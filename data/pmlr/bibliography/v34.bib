@proceedings{ICGI2014,
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Alexander Clark and Makoto Kanazawa and Ryo Yoshinaka},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The 12th International Conference on Grammatical Inference},
 volume = {34}
}

@inproceedings{pmlr-v34-beros14a,
 abstract = {We prove the existence of a canonical form for semi-deterministic transducers with sets of pairwise incomparable output strings. Based on this, we develop an algorithm which learns semi-deterministic transducers given access to translation queries. We also prove that there is no learning algorithm for semi-deterministic transducers that uses only domain knowledge.},
 address = {Kyoto, Japan},
 author = {Beros, Achilles and Higuera, Colin},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2963940995},
 pages = {33--48},
 pdf = {http://proceedings.mlr.press/v34/beros14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Canonical Semi-Deterministic Transducer},
 url = {https://proceedings.mlr.press/v34/beros14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-clark14a,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 address = {Kyoto, Japan},
 author = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W4240465921},
 pages = {1--2},
 pdf = {http://proceedings.mlr.press/v34/clark14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v34/clark14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-coste14a,
 abstract = {Based on Harris’s substitutability criterion, the recent denitions of classes of substitutable languages have led to interesting polynomial learnability results for expressive formal languages. These classes are also promising for practical applications: in natural language analysis, because denitions have strong linguisitic support, but also in biology for modeling protein families, as suggested in our previous study introducing the class of local substitutable languages. But turning recent theoretical advances into practice badly needs truly practical algorithms. We present here an ecient},
 address = {Kyoto, Japan},
 author = {Coste, FranÃ§ois and Garet, GaÃ«lle and Nicolas, Jacques},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2162613012},
 pages = {49--63},
 pdf = {http://proceedings.mlr.press/v34/coste14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A bottom-up efficient algorithm learning substitutable languages from positive examples},
 url = {https://proceedings.mlr.press/v34/coste14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-gybels14a,
 abstract = {Spectral methods propose new and elegant solutions in probabilistic grammatical inference. We propose two ways to improve them. We show how a linear representation, or equivalently a weighted automata, output by the spectral learning algorithm can be taken as an initial point for the Baum Welch algorithm, in order to increase the likelihood of the observation data. Secondly, we show how the inference problem can naturally be expressed in the framework of Structured Low-Rank Approximation. Both ideas are tested on a benchmark extracted from the PAutomaC challenge.},
 address = {Kyoto, Japan},
 author = {Gybels, Mattias and Denis, FranÃ§ois and Habrard, Amaury},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W150331267},
 pages = {64--78},
 pdf = {http://proceedings.mlr.press/v34/gybels14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Some improvements of the spectral learning approach for probabilistic grammatical inference},
 url = {https://proceedings.mlr.press/v34/gybels14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-isberner14a,
 abstract = {Counterexample analysis has emerged as one of the key challenges in Angluin-style active automata learning. Rivest and Schapire (1993) showed for the L algorithm that a single sux of the counterexample was sucient to ensure progress. This sux can be obtained in a binary search fashion, requiring (log m) membership queries for a counterexample of length m. Correctly implementing this algorithm can be quite tricky, and its correctness sometimes even has been disputed. In this paper, we establish an abstract framework for counterexample analysis, which basically reduces the problem of nding a sux to nding distinct neighboring elements in a 0 =1 sequence, where the rst element is 0 and the last element is 1. We demonstrate the conciseness and simplicity of our framework by using it to present new counterexample analysis algorithms, which, while maintaining the worst-case complexity of O(logm), perform signicantly},
 address = {Kyoto, Japan},
 author = {Isberner, Malte and Steffen, Bernhard},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2137239337},
 pages = {79--93},
 pdf = {http://proceedings.mlr.press/v34/isberner14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An Abstract Framework for Counterexample Analysis in Active Automata Learning},
 url = {https://proceedings.mlr.press/v34/isberner14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-jardine14a,
 abstract = {In this paper, we present a new algorithm that can identify in polynomial time and data using positive examples any class of subsequential functions that share a particular nitestate structure. While this structure is given to the learner a priori, it allows for the exact learning of partial functions, and both the time and data complexity of the algorithm are linear. We demonstrate the algorithm on examples from natural language phonology and morphology in which the needed structure has been argued to be plausibly known in advance. A procedure for making any subsequential transducer onward without changing its structure is also presented.},
 address = {Kyoto, Japan},
 author = {Jardine, Adam and Chandlee, Jane and Eyraud, RÃ©mi and Heinz, Jeffrey},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2157269328},
 pages = {94--108},
 pdf = {http://proceedings.mlr.press/v34/jardine14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Very efficient learning of structured classes of subsequential functions from positive data},
 url = {https://proceedings.mlr.press/v34/jardine14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-khalili14a,
 abstract = {In applications where abstract models of reactive systems are to be inferred, one important challenge is that the behavior of such systems can be inherently nondeterministic. To cope with this challenge, we developed an algorithm to infer nondeterministic computation models in the form of Mealy machines. We introduce our approach and provide extensive experimental results to assess its potential in the identication of black-box reactive systems. The experiments involve both articially-gener ated abstract Mealy machines, and the identication of a TFTP server model starting from a publicly-available implementation.},
 address = {Kyoto, Japan},
 author = {Khalili, Ali and Tacchella, Armando},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2138076011},
 pages = {109--123},
 pdf = {http://proceedings.mlr.press/v34/khalili14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Nondeterministic Mealy Machines},
 url = {https://proceedings.mlr.press/v34/khalili14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-rabusseau14a,
 abstract = {This paper investigates the use of linear representations of trees (i.e. mappings from the set of trees into a nite dimensional vector space which are induced by rational series on trees) in the context of structured data learning. We argue that this representation space can be more appealing than the space of trees to handle machine learning problems involving trees. Focusing on a tree series maximization problem, we rst analyze its complexity to motivate the use of approximation techniques. We then show how a tree series can be extended to the continuous representation space, we propose an adaptive Metropolis-Hastings algorithm to solve the maximization problem in this space, and we establish convergence guarantees. Finally, we provide some experiments comparing our algorithm with an implementation of the Metropolis-Hastings algorithm in the space of trees.},
 address = {Kyoto, Japan},
 author = {Rabusseau, Guillaume and Denis, FranÃ§ois},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W39589503},
 pages = {124--138},
 pdf = {http://proceedings.mlr.press/v34/rabusseau14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Maximizing a Tree Series in the Representation Space},
 url = {https://proceedings.mlr.press/v34/rabusseau14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-sakamoto14a,
 abstract = {A grammatical inference algorithm tries to nd as a small grammar as possible representing a potentially innite sequence of strings. Here, let us consider a simple restriction: the input is a nite sequence or it might be a singleton set. Then the restricted problem is called the grammar compression to nd the smallest CFG generating just the input. In the last decade many researchers have tackled this problem because of its scalable applications, e.g., expansion of data storage capacity, speeding-up information retrieval, DNA sequencing, frequent pattern mining, and similarity search. We would review the history of grammar compression and its wide applications together with an important future work. The study of grammar compression has begun with the bad news: the smallest CFG problem is NP-hard. Hence, the rst question is: Can we get a near-optimal solution in a polynomial time? (Is there a reasonable approximation algorithm?) And the next question is: Can we minimize the costs of time and space? (Does a linear time algorithm exist within an optimal working space?) The recent results produced by the research community answer armatively the questions. We introduce several important results and typical applications to a huge text collection. On the other hand, the shrinkage of the advantage of grammar compression is caused by the data explosion, since there is no working space for storing the whole data supplied from data stream. The last question is: How can we handle the stream data? For this question, we propose the framework of stream grammar compression for the next generation and its attractive application to fast data transmission.},
 address = {Kyoto, Japan},
 author = {Sakamoto, Hiroshi},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2165258139},
 pages = {3--20},
 pdf = {http://proceedings.mlr.press/v34/sakamoto14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Grammar Compression: Grammatical Inference by Compression and Its Application to Real Data},
 url = {https://proceedings.mlr.press/v34/sakamoto14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-scicluna14a,
 abstract = {Recently, dierent theoretical learning results have been found for a variety of context-free grammar subclasses through the use of distributional learning (Clark, 2010b). However, these results are still not extended to probabilistic grammars. In this work, we give a practical algorithm, with some proven properties, that learns a subclass of probabilistic grammars from positive data. A minimum satisability solver is used to direct the search towards small grammars. Experiments on typical context-free languages and articial natural language grammars give positive results.},
 address = {Kyoto, Japan},
 author = {Scicluna, James and de la Higuera, Colin},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W53452579},
 pages = {139--152},
 pdf = {http://proceedings.mlr.press/v34/scicluna14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Grammatical Inference of some Probabilistic Context-Free Grammars  from Positive Data using Minimum Satisfiability},
 url = {https://proceedings.mlr.press/v34/scicluna14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-shibata14a,
 abstract = {Motivated by the idea of applying nonparametric Bayesian models to dual approaches for distributional learning, we dene ( k;l)-context-sensitive probabilistic context-free grammars (PCFGs) using hierarchical Pitman-Yor processes (PYPs). The data sparseness problem that occurs when inferring context-sensitive probabilities for rules is handled by the smoothing eect of hierarchical PYPs. Many possible denitions or constructions of PYP hierarchies can be used to represent the context sensitivity of derivations of CFGs in Chomsky normal form. In this study, we use a denition},
 address = {Kyoto, Japan},
 author = {Shibata, Chihiro},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2126657435},
 pages = {153--166},
 pdf = {http://proceedings.mlr.press/v34/shibata14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Inferring (k,l)-context-sensitive probabilistic context-free grammars using hierarchical Pitman-Yor processes},
 url = {https://proceedings.mlr.press/v34/shibata14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-smetsers14a,
 abstract = {In Angluin’s L algorithm a learner constructs a sequence of hypotheses in order to learn a regular language. Each hypothesis is consistent with a larger set of observations and is described by a bigger model. From a behavioral perspective, however, a hypothesis is not always better than the previous one, in the sense that the minimal length of a counterexample that distinguishes a hypothesis from the target language may decrease. We present a simple modication of the L algorithm that ensures that for subsequent hypotheses the minimal length of a counterexample never decreases, which implies that the distance to the target language never increases in a corresponding ultrametric. Preliminary experimental evidence suggests that our algorithm speeds up learning in practical applications by reducing the number of equivalence queries.},
 address = {Kyoto, Japan},
 author = {Smetsers, Rick and Volpato, Michele and Vaandrager, Frits and Verwer, Sicco},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W51171243},
 pages = {167--181},
 pdf = {http://proceedings.mlr.press/v34/smetsers14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Bigger is Not Always Better: on the Quality of Hypotheses in Active Automata Learning},
 url = {https://proceedings.mlr.press/v34/smetsers14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-stabler14a,
 abstract = {Recent computational, mathematical work on learnability extends to classes of languages that plausibly include the human languages, but there is nevertheless a gulf between this work and linguistic theory. The languages of the two elds seem almost completely disjoint and incommensurable. This paper shows that this has happened, at least in part, because the recent advances in learnability have been misdescribed in two important respects. First, they have been described as resting on ‘empiricist’ conceptions of language, when actually, in fundamental respects that are made precise here, they are equally compatible with the ‘rationalist’, ‘nativist’ traditions in linguistic theory. Second, the recent mathematical proposals have sometimes been presented as if they not only advance but complete the account of human language acquisition, taking the rather dramatic dierence between what current mathematical models can achieve and what current linguistic theories tell us as an indication that current linguistic theories are quite generally mistaken. This paper compares the two perspectives and takes some rst steps toward a unied theory, aiming},
 address = {Kyoto, Japan},
 author = {Stabler, Edward},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2161234683},
 pages = {21--32},
 pdf = {http://proceedings.mlr.press/v34/stabler14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Towards a rationalist theory of language acquisition},
 url = {https://proceedings.mlr.press/v34/stabler14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-tajima14a,
 abstract = {In this paper, we show a special example distribution on which the learner can guess a correct simple deterministic grammar in polynomial time from membership queries and random examples. At rst, we show a learning algorithm of simple deterministic languages from membership and equivalence queries. This algorithm is not a polynomial time algorithm but, assuming a special example distribution, we can modify it to the polynomial time probabilistic learning algorithm.},
 address = {Kyoto, Japan},
 author = {Tajima, Yasuhiro and Kikui, Genichiro},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W177469043},
 pages = {182--192},
 pdf = {http://proceedings.mlr.press/v34/tajima14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An example distribution for probabilistic query learning of simple deterministic languages},
 url = {https://proceedings.mlr.press/v34/tajima14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-vanzaanen14a,
 abstract = {Grammatical inference deals with learning of grammars describing languages. Formal grammatical inference aims at identifying families of languages that have a shared property, which can be used to prove ecient learnability of the families formally. In contrast, in empirical grammatical inference research, practical systems are developed that are applied to languages. The eectiveness of these systems is measured by comparing the learned grammar against a Gold standard which indicates the ground truth. From successful empirical learnability results, either shared properties may be identied, leading to further formal learnability results, or modications to the systems may be made, improving practical results. Proper evaluation of empirical systems is, therefore, essential. Here, we evaluate and compare existing state-of-the-art context-free grammar learning systems (and novel systems based on combinations of existing phases) in a standardized evaluation environment (on a corpus of plain natural language sentences), illustrating future directions for empirical grammatical inference research.},
 address = {Kyoto, Japan},
 author = {Zaanen, Menno and Noord, Nanne},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2171910792},
 pages = {193--206},
 pdf = {http://proceedings.mlr.press/v34/vanzaanen14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Evaluation of selection in context-free grammar learning systems},
 url = {https://proceedings.mlr.press/v34/vanzaanen14a.html},
 volume = {34},
 year = {2014}
}

@inproceedings{pmlr-v34-wieczorek14a,
 abstract = {In this paper a new algorithm for the induction of a Directed Acyclic Word Graph (DAWG) is proposed. A DAWG can serve as a very ecient data structure for lexicon representation and fast string matching, and have a variety of applications. Similar structures are being investigated in the theory of formal languages and grammatical inference, namely deterministic and nondeterministic nite automata (DFA and NFA, respectively). Since a DAWG is acyclic the proposed method is suited for problems where the target language does not necessarily have to be innite. The experiments have been performed for a dataset from the domain of bioinformatics, and our results are compared with those obtained using the current state-of-the-art methods in heuristic DFA induction.},
 address = {Kyoto, Japan},
 author = {Wieczorek, Wojciech and Unold, Olgierd},
 booktitle = {The 12th International Conference on Grammatical Inference},
 editor = {Clark, Alexander and Kanazawa, Makoto and Yoshinaka, Ryo},
 month = {17--19 Sep},
 openalex = {W2113676955},
 pages = {207--217},
 pdf = {http://proceedings.mlr.press/v34/wieczorek14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Induction of Directed Acyclic Word Graph in a Bioinformatics Task},
 url = {https://proceedings.mlr.press/v34/wieczorek14a.html},
 volume = {34},
 year = {2014}
}
