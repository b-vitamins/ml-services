@proceedings{AFCP2022,
 booktitle = {Proceedings of the Workshop on Algorithmic Fairness through the Lens of Causality and Privacy},
 editor = {Awa Dieng and Miriam Rateike and Golnoosh Farnadi and Ferdinando Fioretto and Matt Kusner and Jessica Schrouff},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the Workshop on Algorithmic Fairness through the Lens of Causality and Privacy},
 volume = {214}
}

@inproceedings{pmlr-v214-binkyte23a,
 abstract = {It is crucial to consider the social and ethical consequences of AI and ML based decisions for the safe and acceptable use of these emerging technologies. Fairness, in particular, guarantees that the ML decisions do not result in discrimination against individuals or minorities. Identifying and measuring reliably fairness/discrimination is better achieved using causality which considers the causal relation, beyond mere association, between the sensitive attribute (e.g. gender, race, religion, etc.) and the decision (e.g. job hiring, loan granting, etc.). The big impediment to the use of causality to address fairness, however, is the unavailability of the causal model (typically represented as a causal graph). Existing causal approaches to fairness in the literature do not address this problem and assume that the causal model is available. In this paper, we do not make such assumption and we review the major algorithms to discover causal relations from observable data. This study focuses on causal discovery and its impact on fairness. In particular, we show how different causal discovery approaches may result in different causal models and, most importantly, how even slight differences between causal models can have significant impact on fairness/discrimination conclusions. These results are consolidated by empirical analysis using synthetic and standard fairness benchmark datasets. The main goal of this study is to highlight the importance of the causal discovery step to appropriately address fairness using causality.},
 author = {Binkyt\.{e}, R\={u}ta and Makhlouf, Karima and Pinz\'{o}n, Carlos and Zhioua, Sami and Palamidessi, Catuscia},
 booktitle = {Proceedings of the Workshop on Algorithmic Fairness through the Lens of Causality and Privacy},
 editor = {Dieng, Awa and Rateike, Miriam and Farnadi, Golnoosh and Fioretto, Ferdinando and Kusner, Matt and Schrouff, Jessica},
 month = {03 Dec},
 openalex = {W4282983520},
 pages = {7--22},
 pdf = {https://proceedings.mlr.press/v214/binkyte23a/binkyte23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Discovery for Fairness},
 url = {https://proceedings.mlr.press/v214/binkyte23a.html},
 volume = {214},
 year = {2023}
}

@inproceedings{pmlr-v214-dieng23a,
 author = {Dieng, Awa and Rateike, Miriam and Farnadi, Golnoosh and Fioretto, Ferdinando and Kusner, Matt and Schrouff, Jessica},
 booktitle = {Proceedings of the Workshop on Algorithmic Fairness through the Lens of Causality and Privacy},
 editor = {Dieng, Awa and Rateike, Miriam and Farnadi, Golnoosh and Fioretto, Ferdinando and Kusner, Matt and Schrouff, Jessica},
 month = {03 Dec},
 pages = {1--6},
 pdf = {https://proceedings.mlr.press/v214/dieng23a/dieng23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Algorithmic Fairness through the Lens of Causality and Privacy (AFCP) 2022},
 url = {https://proceedings.mlr.press/v214/dieng23a.html},
 volume = {214},
 year = {2023}
}

@inproceedings{pmlr-v214-friedberg23a,
 abstract = {We study a new privacy model where users belong to certain sensitive groups and we would like to conduct statistical inference on whether there is significant differences in outcomes between the various groups. In particular we do not consider the outcome of users to be sensitive, rather only the membership to certain groups. This is in contrast to previous work that has considered locally private statistical tests, where outcomes and groups are jointly privatized, as well as private A/B testing where the groups are considered public (control and treatment groups) while the outcomes are privatized. We cover several different settings of hypothesis tests after group membership has been privatized amongst the samples, including binary and real valued outcomes. We adopt the generalized $\chi^2$ testing framework used in other works on hypothesis testing in different privacy models, which allows us to cover $Z$-tests, $\chi^2$ tests for independence, t-tests, and ANOVA tests with a single unified approach. When considering two groups, we derive confidence intervals for the true difference in means and show traditional approaches for computing confidence intervals miss the true difference when privacy is introduced. For more than two groups, we consider several mechanisms for privatizing the group membership, showing that we can improve statistical power over the traditional tests that ignore the noise due to privacy. We also consider the application to private A/B testing to determine whether there is a significant change in the difference in means across sensitive groups between the control and treatment.},
 author = {Friedberg, Rina and Rogers, Ryan},
 booktitle = {Proceedings of the Workshop on Algorithmic Fairness through the Lens of Causality and Privacy},
 editor = {Dieng, Awa and Rateike, Miriam and Farnadi, Golnoosh and Fioretto, Ferdinando and Kusner, Matt and Schrouff, Jessica},
 month = {03 Dec},
 openalex = {W4292420236},
 pages = {23--66},
 pdf = {https://proceedings.mlr.press/v214/friedberg23a/friedberg23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Privacy Aware Experimentation over Sensitive Groups: A General Chi Square Approach},
 url = {https://proceedings.mlr.press/v214/friedberg23a.html},
 volume = {214},
 year = {2023}
}

@inproceedings{pmlr-v214-juarez23a,
 abstract = {As in traditional machine learning models, models trained with federated learning may exhibit disparate performance across demographic groups. Model holders must identify these disparities to mitigate undue harm to the groups. However, measuring a modelâs performance in a group requires access to information about group membership which, for privacy reasons, often has limited availability. We propose novel locally differentially private mechanisms to measure differences in performance across groups while protecting the privacy of group membership. To analyze the effectiveness of the mechanisms, we bound their error in estimating a disparity when optimized for a given privacy budget. Our results show that the error rapidly decreases for realistic numbers of participating clients, demonstrating that, contrary to what prior work suggested, protecting privacy is not necessarily in conflict with identifying performance disparities of federated models.},
 author = {Juarez, Marc and Korolova, Aleksandra},
 booktitle = {Proceedings of the Workshop on Algorithmic Fairness through the Lens of Causality and Privacy},
 editor = {Dieng, Awa and Rateike, Miriam and Farnadi, Golnoosh and Fioretto, Ferdinando and Kusner, Matt and Schrouff, Jessica},
 month = {03 Dec},
 pages = {67--85},
 pdf = {https://proceedings.mlr.press/v214/juarez23a/juarez23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {âYou Canât Fix What You Canât Measureâ: Privately Measuring Demographic Performance Disparities in Federated Learning},
 url = {https://proceedings.mlr.press/v214/juarez23a.html},
 volume = {214},
 year = {2023}
}

@inproceedings{pmlr-v214-lowy23a,
 abstract = {Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals' health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term "stochastic" refers to the fact that our proposed algorithm converges even when minibatches of data are used at each iteration (i.e. stochastic optimization). Our framework is flexible enough to permit different fairness notions, including demographic parity and equalized odds. In addition, our algorithm can be applied to non-binary classification tasks with multiple (non-binary) sensitive attributes. As a byproduct of our convergence analysis, we provide the first utility guarantee for a DP algorithm for solving nonconvex-strongly concave min-max problems. Our numerical experiments show that the proposed algorithm consistently offers significant performance gains over the state-of-the-art baselines, and can be applied to larger scale problems with non-binary target/sensitive attributes.},
 author = {Lowy, Andrew and Gupta, Devansh and Razaviyayn, Meisam},
 booktitle = {Proceedings of the Workshop on Algorithmic Fairness through the Lens of Causality and Privacy},
 editor = {Dieng, Awa and Rateike, Miriam and Farnadi, Golnoosh and Fioretto, Ferdinando and Kusner, Matt and Schrouff, Jessica},
 month = {03 Dec},
 openalex = {W4306802263},
 pages = {86--119},
 pdf = {https://proceedings.mlr.press/v214/lowy23a/lowy23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Stochastic Differentially Private and Fair Learning},
 url = {https://proceedings.mlr.press/v214/lowy23a.html},
 volume = {214},
 year = {2023}
}
