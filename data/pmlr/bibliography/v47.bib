@inproceedings{pmlr-v47-lemmerich14a,
 abstract = {Statistically sound validation of results plays an important role in modern data mining. In this context, it has been advocated to disregard patterns that cannot be automatically conrmed as statistically valid by the available data. In this short position paper, we argue against a mandatory automatic signicance ltering of results.},
 address = {Nancy, France},
 author = {Lemmerich, Florian and Puppe, Frank},
 booktitle = {Proceedings of the Workshop on Statistically Sound Data Mining at ECML/PKDD},
 editor = {HÃ¤mÃ¤lÃ¤inen, Wilhelmiina and Petitjean, FranÃ§ois and Webb, I.},
 month = {15 Sep},
 openalex = {W2475102546},
 pages = {21--27},
 pdf = {http://proceedings.mlr.press/v47/lemmerich14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Critical View on Automatic Significance-Filtering in Pattern Mining},
 url = {https://proceedings.mlr.press/v47/lemmerich14a.html},
 volume = {47},
 year = {2015}
}

@inproceedings{pmlr-v47-preface,
 abstract = {These proceedings contain the papers accepted to the 1st ECML/PKDD Workshop on Statistically Sound Data Mining, which took place at the French Institute for Computer Science (INRIA) in Nancy, at the opening of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD) on the 15th of September 2014.},
 address = {Nancy, France},
 author = {HÃ¤mÃ¤lÃ¤inen, Wilhelmiina and Petitjean, FranÃ§ois and Webb, Geoffrey I.},
 booktitle = {Proceedings of the Workshop on Statistically Sound Data Mining at ECML/PKDD},
 editor = {HÃ¤mÃ¤lÃ¤inen, Wilhelmiina and Petitjean, FranÃ§ois and Webb, I.},
 month = {15 Sep},
 openalex = {W2255823589},
 pages = {1--2},
 pdf = {http://proceedings.mlr.press/v47/preface.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface to the 1st ECML/PKDD workshop on Statistically Sound Data Mining},
 url = {https://proceedings.mlr.press/v47/preface.html},
 volume = {47},
 year = {2015}
}

@inproceedings{pmlr-v47-sese14a,
 abstract = {Genome-wide association studies (GWAS) have been widely used for understanding the associations of single-nucleotide polymorphisms (SNPs) with a disease. GWAS data are often combined with known biological networks, and they have been analyzed using graphmining techniques toward a systems understanding of the biological changes caused by the SNPs. To determine which subgraphs are associated with the disease, a statistical test on each subgraph needs to be conducted. However, no statistically signicant results were found because multiple testing correction causes an extremely small corrected signicance level. We introduce a method called gLAMP to enumerate subgraphs having statistically signicant associations with a diagnosis. gLAMP integrates the Limitless Arity Multipletesting Procedure (LAMP) with a graph-mining algorithm called COmmon Itemset Network mining (COIN). LAMP gives us the smallest possible Bonferroni factor, and COIN provides us with ecient enumeration of testable subgraphs. Theoretical results of their combination show the potential to enumerate subgraphs statistically signicantly associ},
 address = {Nancy, France},
 author = {Sese, Jun and Terada, Aika and Saito, Yuki and Tsuda, Koji},
 booktitle = {Proceedings of the Workshop on Statistically Sound Data Mining at ECML/PKDD},
 editor = {HÃ¤mÃ¤lÃ¤inen, Wilhelmiina and Petitjean, FranÃ§ois and Webb, I.},
 month = {15 Sep},
 openalex = {W2341843224},
 pages = {29--36},
 pdf = {http://proceedings.mlr.press/v47/sese14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Statistically significant subgraphs for genome-wide association study},
 url = {https://proceedings.mlr.press/v47/sese14a.html},
 volume = {47},
 year = {2015}
}

@inproceedings{pmlr-v47-vanwinckelen14a,
 abstract = {Machine learning is largely an experimental science, of which the evaluation of predictive models is an important aspect. These days, cross-validation is the most widely used method for this task. There are, however, a number of important points that should be taken into account when using this methodology. First, one should clearly state what they are trying to estimate. Namely, a distinction should be made between the evaluation of a model learned on a single dataset, and that of a learner trained on a random sample from a given data population. Each of these two questions requires a dierent statistical approach and should not be confused with each other. While this has been noted before, the literature on this topic is generally not very accessible. This paper tries to give an understandable overview of the statistical aspects of these two evaluation tasks. We also pose that because of the often limited availability of data, and the diculty of selecting an appropriate statistical test, it is in some cases perhaps better to abstain from statistical testing, and instead focus on an interpretation of the immediate results.},
 address = {Nancy, France},
 author = {Vanwinckelen, Gitte and Blockeel, Hendrik},
 booktitle = {Proceedings of the Workshop on Statistically Sound Data Mining at ECML/PKDD},
 editor = {HÃ¤mÃ¤lÃ¤inen, Wilhelmiina and Petitjean, FranÃ§ois and Webb, I.},
 month = {15 Sep},
 openalex = {W2209840346},
 pages = {3--20},
 pdf = {http://proceedings.mlr.press/v47/vanwinckelen14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Look before you leap: Some insights into learner evaluation with cross-validation},
 url = {https://proceedings.mlr.press/v47/vanwinckelen14a.html},
 volume = {47},
 year = {2015}
}

@inproceedings{pmlr-v47-wang14a,
 abstract = {Most analysis of U-statistics assumes that data points are independent or stationary. However, when we analyze network data, these two assumptions do not hold any more. We rst dene the problem of weighted U-statistics on networked data by extending previous work. We analyze their variance using Hoeding’s decomposition and also give exponential concentration inequalities. Two eciently solvable linear programs are proposed to nd estimators with minimum worst-case variance or with tighter concentration inequalities.},
 address = {Nancy, France},
 author = {Wang, Yuyi and Pelekis, Christos and Ramon, Jan},
 booktitle = {Proceedings of the Workshop on Statistically Sound Data Mining at ECML/PKDD},
 editor = {HÃ¤mÃ¤lÃ¤inen, Wilhelmiina and Petitjean, FranÃ§ois and Webb, I.},
 month = {15 Sep},
 openalex = {W2237437562},
 pages = {37--48},
 pdf = {http://proceedings.mlr.press/v47/wang14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {U-statistics on network-structured data with kernels of degree larger than one},
 url = {https://proceedings.mlr.press/v47/wang14a.html},
 volume = {47},
 year = {2015}
}

@proceedings{SSDM2014,
 booktitle = {Proceedings of the Workshop on Statistically Sound Data Mining at ECML/PKDD},
 editor = {Wilhelmiina HÃ¤mÃ¤lÃ¤inen and FranÃ§ois Petitjean and I. Webb},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the Workshop on Statistically Sound Data Mining at ECML/PKDD},
 volume = {47}
}
