@comment{@Proceedings{KDD-Cup 20092009,
  title =     {Proceedings of KDD-Cup 2009 Competition},
  booktitle = {Proceedings of KDD-Cup 2009 Competition},
  editor =    {Gideon Dror and Mar BoullÃ© and Isabelle Guyon and Vincent Lemaire and David Vogel},
  publisher = {PMLR},
  series =    {Proceedings of Machine Learning Research},
  volume =    7
}}

@inproceedings{pmlr-v7-busa09,
 abstract = {This paper explores how multi-armed bandits (MABs) can be applied to accelerate AdaBoost. AdaBoost constructs a strong classifier in a stepwise fashion by adding simple base classifiers to a pool and using their weighted vote to determine the final classification. We model this stepwise base classifier selection as a sequential decision problem, and optimize it with MABs. Each arm represents a subset of the base classifier set. The MAB gradually learns the of the subsets, and selects one of the subsets in each iteration. ADABOOST then searches only this subset instead of optimizing the base classifier over the whole space. The reward is defined as a function of the accuracy of the base classifier. We investigate how the well-known UCB algorithm can be applied in the case of boosted stumps, trees, and products of base classifiers. The KDD Cup 2009 was a large-scale learning task with a limited training time, thus this challenge offered us a good opportunity to test the utility of our approach. During the challenge our best results came in the Up-selling task where our model was within 1% of the best AUC rate. After more thorough post-challenge validation the algorithm performed as well as the best challenge submission on the small data set in two of the three tasks.},
 address = {New York, New York, USA},
 author = {Busa-Fekete, RÃ³bert and KÃ©gl, BalÃ¡zs},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 openalex = {W3968525},
 pages = {111--122},
 pdf = {http://proceedings.mlr.press/v7/busa09/busa09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Accelerating AdaBoost using UCB},
 url = {https://proceedings.mlr.press/v7/busa09.html},
 volume = {7},
 year = {2009}
}

@inproceedings{pmlr-v7-doetsch09,
 abstract = {In this work, we describe our approach to the Small Challenge of the KDD cup 2009, a classification task with incomplete data. Preprocessing, feature extraction and model selection are documented in detail. We suggest a criterion based on the number of missing values to select a suitable imputation method for each feature. Logistic Model Trees (LMT) are extended with a split criterion optimizing the Area under the ROC Curve (AUC), which was the requested evaluation criterion. By stacking boosted decision stumps and LMT we achieved the best result for the Small Challenge without making use of additional data from other feature sets, resulting in an AUC score of 0.8081. We also present results of an AUC optimizing model combination that scored only slightly worse with an AUC score of 0.8074.},
 address = {New York, New York, USA},
 author = {Doetsch, Patrick and Buck, Christian and Golik, Pavlo and Hoppe, Niklas and Kramp, Michael and Laudenberg, Johannes and OberdÃ¶rfer, Christian and Steingrube, Pascal and Forster, Jens and Mauser, Arne},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 openalex = {W2099542185},
 pages = {77--88},
 pdf = {http://proceedings.mlr.press/v7/doetsch09/doetsch09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Logistic Model Trees with AUC split criterion for the KDD cup 2009 small challenge},
 url = {https://proceedings.mlr.press/v7/doetsch09.html},
 volume = {7},
 year = {2009}
}

@inproceedings{pmlr-v7-guyon09,
 abstract = {We organized the KDD cup 2009 around a marketing problem with the goal of identifying data mining techniques capable of rapidly building predictive models and scoring new entries on a large database. Customer Relationship Management (CRM) is a key element of modern marketing strategies. The KDD Cup 2009 offered the opportunity to work on large marketing databases from the French Telecom company Orange to predict the propensity of customers to switch provider (churn), buy new products or services (appetency), or buy upgrades or add-ons proposed to them to make the sale more profitable (up-selling). The challenge started on March 10, 2009 and ended on May 11, 2009. This challenge attracted over 450 participants from 46 countries. We attribute the popularity of the challenge to several factors: (1) A generic problem relevant to the Industry (a classification problem), but presenting a number of scientific and technical challenges of practical interest including: a large number of training examples (50,000) with a large number of missing values (about 60%) and a large number of features (15,000), unbalanced class proportions (fewer than 10% of the examples of the positive class), noisy data, presence of categorical variables with many different values. (2) Prizes (Orange offered 10,000 Euros in prizes). (3) A well designed protocol and web site (we benefitted from past experience). (4) An effective advertising campaign using mailings and a teleconference to answer potential participants questions. The results of the challenge were discussed at the KDD conference (June 28, 2009). The principal conclusions are that ensemble methods are very effective and that ensemble of decision trees offer off-the-shelf solutions to problems with large numbers of samples and attributes, mixed types of variables, and lots of missing values. The data and the platform of the challenge remain available for research and educational purposes at http://www.kddcup-orange.com/.},
 address = {New York, New York, USA},
 author = {Guyon, Isabelle and Lemaire, Vincent and BoullÃ©, Marc and Dror, Gideon and Vogel, David},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 openalex = {W2129114758},
 pages = {1--22},
 pdf = {http://proceedings.mlr.press/v7/guyon09/guyon09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Analysis of the KDD cup 2009: fast scoring on a large orange customer database},
 url = {https://proceedings.mlr.press/v7/guyon09.html},
 volume = {7},
 year = {2009}
}

@inproceedings{pmlr-v7-kurucz09,
 abstract = {We describe the method used in our final submission to KDD Cup 2009 as well as a selection of promising directions that are generally believed to work well but did not justify our expectations. Our final method consists of a combination of a LogitBoost and an ADTree classifier with a feature selection method that, as shaped by the experiments we have conducted, have turned out to be very different from those described in some well-cited surveys. Some methods that failed include distance, information and dependence measures for feature selection as well as combination of classifiers over a partitioned feature set. As another main lesson learned, alternating decision trees and LogitBoost outperformed most classifiers for most feature subsets of the KDD Cup 2009 data.},
 address = {New York, New York, USA},
 author = {Kurucz, MiklÃ³s and SiklÃ³si, DÃ¡vid and BÃ­rÃ³, IstvÃ¡n and Csizsek, PÃ©ter and Fekete, Zsolt and Iwatt, RÃ³bert and Kiss, TamÃ¡s and SzabÃ³, Adrienn},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 openalex = {W111219783},
 pages = {65--75},
 pdf = {http://proceedings.mlr.press/v7/kurucz09/kurucz09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {KDD Cup 2009 @ Budapest: feature partitioning and boosting},
 url = {https://proceedings.mlr.press/v7/kurucz09.html},
 volume = {7},
 year = {2009}
}

@inproceedings{pmlr-v7-lo09,
 abstract = {This paper describes our ensemble of three classifiers for the KDD Cup 2009 challenge. First, we transform the three binary classification tasks into a joint multi-class classification problem, and solve an l1-regularized maximum entropy model under the LIBLINEAR framework. Second, we propose a heterogeneous base learner, which is capable of handling different types of features and missing values, and use AdaBoost to improve the base learner. Finally, we adopt a selective naÃ¯ve Bayes classifier that automatically groups categorical features and discretizes numerical ones. The parameters are tuned using crossvalidation results rather than the 10% test results on the competition website. Based on the observation that the three positive labels are exclusive, we conduct a post-processing step using the linear SVM to jointly adjust the prediction scores of each classifier on the three tasks. Then, we average these prediction scores with careful validation to get the final outputs. Our final average AUC on the whole test set is 0.8461, which ranks third place in the slow track of KDD Cup 2009.},
 address = {New York, New York, USA},
 author = {Lo, Hung-Yi and Chang, Kai-Wei and Chen, Shang-Tse and Chiang, Tsung-Hsien and Ferng, Chun- Sung and Hsieh, Cho-Jui and Ko, Yi-Kuang and Kuo, Tsung-Ting and Lai, Hung-Che and Lin, Ken-Yi and Wang, Chia-Hsuan and Yu, Hsiang-Fu and Lin, Chih-Jen and Lin, Hsuan-Tien and Lin, Shou-de},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 pages = {57--64},
 pdf = {http://proceedings.mlr.press/v7/lo09/lo09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An Ensemble of Three Classifiers for KDD Cup 2009: Expanded Linear Model, Heterogeneous Boosting, and Selective Naive Bayes},
 url = {https://proceedings.mlr.press/v7/lo09.html},
 volume = {7},
 year = {2009}
}

@inproceedings{pmlr-v7-miller09,
 abstract = {We discuss the challenges of the 2009 KDD Cup along with our ideas and methodologies for modelling the problem. The main stages included aggressive nonparametric feature selection, careful treatment of categorical variables and tuning a gradient boosting machine under Bernoulli loss with trees.},
 address = {New York, New York, USA},
 author = {Miller, Hugh and Clarke, Sandy and Lane, Stephen and Lonie, Andrew and Lazaridis, David and Petrovski, Slave and Jones, Owen},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 openalex = {W2149781057},
 pages = {45--55},
 pdf = {http://proceedings.mlr.press/v7/miller09/miller09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Predicting customer behaviour: the University of Melbourne's KDD Cup report},
 url = {https://proceedings.mlr.press/v7/miller09.html},
 volume = {7},
 year = {2009}
}

@inproceedings{pmlr-v7-niculescu09,
 abstract = {We describe our wining solution for the KDD Cup Orange Challenge.},
 address = {New York, New York, USA},
 author = {Niculescu-Mizil, Alexandru and Perlich, Claudia and Swirszcz, Grzegorz and Sindhwani, Vikas and Liu, Yan and Melville, Prem and Wang, Dong and Xiao, Jing and Hu, Jianying and Singh, Moninder and Shang, Wei Xiong and Zhu, Yan Feng},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 openalex = {W174542576},
 pages = {23--34},
 pdf = {http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Winning the KDD Cup Orange Challenge with ensemble selection},
 url = {https://proceedings.mlr.press/v7/niculescu09.html},
 volume = {7},
 year = {2009}
}

@inproceedings{pmlr-v7-nikulin09,
 abstract = {With imbalanced data a classifier built using all of the data has the tendency to ignore the minority class. To overcome this problem, we propose to use an ensemble classifier constructed on the basis of a large number of relatively small and balanced subsets, where representatives from both patterns are to be selected randomly. As an outcome, the system produces the matrix of linear regression coefficients whose rows represent the random sub- sets and the columns represent the features. Based on this matrix, we make an assessment of how stable the influence of a particular feature is. It is proposed to keep in the model only features with stable influence. The final model represents an average of the base-learners, which is not necessarily a linear regression. Proper data pre-processing is very important for the effectiveness of the whole system, and it is proposed to reduce the original data to the most simple binary sparse format, which is particularly convenient for the construction of decision trees. As a result, any particular feature will be represented by several binary variables or bins, which are absolutely equivalent in terms of data structure. This property is very important and may be used for feature selection. The proposed method exploits not only contributions of particular variables to the base-learners, but also the diversity of such contributions. Test results against KDD-2009 competition datasets are presented.},
 address = {New York, New York, USA},
 author = {Nikulin, Vladimir and McLachlan, Geoffrey J.},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 openalex = {W2987205337},
 pages = {89--100},
 pdf = {http://proceedings.mlr.press/v7/nikulin09/nikulin09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Classification of imbalanced marketing data with balanced random sets},
 url = {https://proceedings.mlr.press/v7/nikulin09.html},
 volume = {7},
 year = {2009}
}

@inproceedings{pmlr-v7-sorokina09,
 abstract = {This paper describes a field trial for a recently developed ensemble called Additive Groves on KDD Cup'09 competition. Additive Groves were applied to three tasks provided at the competition using the data set. On one of the three tasks, appetency, we achieved the best result among participants who similarly worked with the small dataset only. Postcompetition analysis showed that less successfull result on another task, churn, was partially due to insufficient preprocessing of nominal attributes.

Code for Additive Groves is publicly available as a part of TreeExtra package. Another part of this package provides an important preprocessing technique also used for this competition entry, feature evaluation through bagging with multiple counts.},
 address = {New York, New York, USA},
 author = {Sorokina, Daria},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 openalex = {W258036788},
 pages = {101--109},
 pdf = {http://proceedings.mlr.press/v7/sorokina09/sorokina09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Application of Additive Groves ensemble with multiple counts feature evaluation to KDD Cup'09 small data set},
 url = {https://proceedings.mlr.press/v7/sorokina09.html},
 volume = {7},
 year = {2009}
}

@inproceedings{pmlr-v7-xie09,
 abstract = {We present the ideas and methodologies that we used to address the KDD Cup 2009 challenge on rank-ordering the probability of churn, appetency and up-selling of wireless customers. We choose stochastic gradient boosting tree (TreeNet®) as our main classifier to handle this large unbalanced dataset. In order to further improve the robustness and accuracy of our results, we bag a series of boosted tree models together as our final submission. Through our exploration we conclude that the most critical factors to achieve our results are effective variable preprocessing and selection, proper imbalanced data handling as well as the combination of bagging and boosting techniques.},
 address = {New York, New York, USA},
 author = {Xie, Jianjun and Rojkova, Viktoria and Pal, Siddharth and Coggeshall, Stephen},
 booktitle = {Proceedings of KDD-Cup 2009 Competition},
 editor = {Dror, Gideon and BoullÃ©, Mar and Guyon, Isabelle and Lemaire, Vincent and Vogel, David},
 month = {28 Jun},
 openalex = {W2168278308},
 pages = {35--43},
 pdf = {http://proceedings.mlr.press/v7/xie09/xie09.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A combination of boosting and bagging for KDD Cup 2009 - fast scoring on a large database},
 url = {https://proceedings.mlr.press/v7/xie09.html},
 volume = {7},
 year = {2009}
}
