@proceedings{ICBINB2021,
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Melanie F. Pradier and Aaron Schein and Stephanie Hyland and Francisco J. R. Ruiz and Jessica Z. Forde},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 volume = {163}
}

@inproceedings{pmlr-v163-adila22a,
 abstract = {Despite machine learning models' success in Natural Language Processing (NLP) tasks, predictions from these models frequently fail on out-of-distribution (OOD) samples. Prior works have focused on developing state-of-the-art methods for detecting OOD. The fundamental question of how OOD samples differ from in-distribution samples remains unanswered. This paper explores how data dynamics in training models can be used to understand the fundamental differences between OOD and in-distribution samples in extensive detail. We found that syntactic characteristics of the data samples that the model consistently predicts incorrectly in both OOD and in-distribution cases directly contradict each other. In addition, we observed preliminary evidence supporting the hypothesis that models are more likely to latch on trivial syntactic heuristics (e.g., overlap of words between two sentences) when making predictions on OOD samples. We hope our preliminary study accelerates the data-centric analysis on various machine learning phenomena.},
 author = {Adila, Dyah and Kang, Dongyeop},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 openalex = {W3216502854},
 pages = {1--8},
 pdf = {https://proceedings.mlr.press/v163/adila22a/adila22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Understanding Out-of-distribution: A Perspective of Data Dynamics},
 url = {https://proceedings.mlr.press/v163/adila22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-blaas22a,
 abstract = {Image augmentations applied during training are crucial for the generalization performance of image classifiers. Therefore, a large body of research has focused on finding the optimal augmentation policy for a given task. Yet, RandAugment [2], a simple random augmentation policy, has recently been shown to outperform existing sophisticated policies. Only Adversarial AutoAugment (AdvAA) [11], an approach based on the idea of adversarial training, has shown to be better than RandAugment. In this paper, we show that random augmentations are still competitive compared to an optimal adversarial approach, as well as to simple curricula, and conjecture that the success of AdvAA is due to the stochasticity of the policy controller network, which introduces a mild form of curriculum.},
 author = {Blaas, Arno and Suau, Xavier and Ramapuram, Jason and Apostoloff, Nicholas and Zappella, Luca},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 openalex = {W3216881825},
 pages = {9--14},
 pdf = {https://proceedings.mlr.press/v163/blaas22a/blaas22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Challenges of Adversarial Image Augmentations},
 url = {https://proceedings.mlr.press/v163/blaas22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-borji22a,
 abstract = {Humans rely heavily on shape information to recognize objects. Conversely, convolutional neural networks (CNNs) are biased more towards texture. This fact is perhaps the main reason why CNNs are susceptible to adversarial examples. Here, we explore how shape bias can be incorporated into CNNs to improve their robustness. Two algorithms are proposed, based on the observation that edges are invariant to moderate imperceptible perturbations. In the first one, a classifier is adversarially trained on images with the edge map as an additional channel. At inference time, the edge map is recomputed and concatenated to the image. In the second algorithm, a conditional GAN is trained to translate the edge maps, from clean and/or perturbed images, into clean images. The inference is done over the generated image corresponding to the inputâs edge map. A large number of experiments with more than 10 data sets demonstrate the effectiveness of the proposed algorithms against FGSM, $\ell_{\infty}$ PGD, Carlini-Wagner, Boundary, and adaptive attacks. Further, we show that edge information can a) benefit other adversarial training methods, b) be even more effective in conjunction with background subtraction, c) be used to defend against poisoning attacks, and d) make CNNs more robust against natural image corruptions such as motion blur, impulse noise, and JPEG compression, than CNNs trained solely on RGB images. From a broader perspective, our study suggests that CNNs do not adequately account for image structures and operations that are crucial for robustness. The code is available at: https://github.com/aliborji/ShapeDefense.git},
 author = {Borji, Ali},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 pages = {15--20},
 pdf = {https://proceedings.mlr.press/v163/borji22a/borji22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Shape Defense},
 url = {https://proceedings.mlr.press/v163/borji22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-caterini22a,
 abstract = {Deep generative models trained by maximum likelihood remain very popular methods for reasoning about data probabilistically. However, it has been observed that they can assign higher likelihoods to out-of-distribution (OOD) data than in-distribution data, thus calling into question the meaning of these likelihood values. In this work we provide a novel perspective on this phenomenon, decomposing the average likelihood into a KL divergence term and an entropy term. We argue that the latter can explain the curious OOD behaviour mentioned above, suppressing likelihood values on datasets with higher entropy. Although our idea is simple, we have not seen it explored yet in the literature. This analysis provides further explanation for the success of OOD detection methods based on likelihood ratios, as the problematic entropy term cancels out in expectation. Finally, we discuss how this observation relates to recent success in OOD detection with manifold-supported models, for which the above decomposition does not hold directly.},
 author = {Caterini, Anthony L. and Loaiza-Ganem, Gabriel},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 openalex = {W4380985042},
 pages = {21--26},
 pdf = {https://proceedings.mlr.press/v163/caterini22a/caterini22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Entropic Issues in Likelihood-Based OOD Detection},
 url = {https://proceedings.mlr.press/v163/caterini22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-chatelain22a,
 abstract = {Recent work has identified simple empirical scaling laws for language models, linking compute budget, dataset size, model size, and autoregressive modeling loss. The validity of these simple power laws across orders of magnitude in model scale provides compelling evidence that larger models are also more capable models. However, scaling up models under the constraints of hardware and infrastructure is no easy feat, and rapidly becomes a hard and expensive engineering problem. We investigate ways to tentatively cheat scaling laws, and train larger models for cheaper. We emulate an increase in effective parameters, using efficient approximations: either by doping the models with frozen random parameters, or by using fast structured transforms in place of dense linear layers. We find that the scaling relationship between test loss and compute depends only on the actual number of trainable parameters; scaling laws cannot be deceived by spurious parameters.},
 author = {Chatelain, Am\'{e}lie and Djeghri, Amine and Hesslow, Daniel and Launay, Julien},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 openalex = {W3201950162},
 pages = {27--32},
 pdf = {https://proceedings.mlr.press/v163/chatelain22a/chatelain22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Is the Number of Trainable Parameters All That Actually Matters},
 url = {https://proceedings.mlr.press/v163/chatelain22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-eastwood22a,
 abstract = {To adapt to changes in real-world data distributions, neural networks must update their parameters. We argue that unit-level surprise should be useful for: (i) determining which few parameters should update to adapt quickly; and (ii) learning a modularization such that few modules need be adapted to transfer. We empirically validate (i) in simple settings and reflect on the challenges and opportunities of realizing both (i) and (ii) in more general settings.},
 author = {Eastwood, Cian and Mason, Ian and Williams, Christopher K. I.},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 pages = {33--40},
 pdf = {https://proceedings.mlr.press/v163/eastwood22a/eastwood22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Unit-level surprise in neural networks},
 url = {https://proceedings.mlr.press/v163/eastwood22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-hayou22a,
 abstract = {Recent work by Jacot et al. (2018) has shown that training a neural network of any kind with gradient descent is strongly related to kernel gradient descent in function space with respect to the Neural Tangent Kernel (NTK). Empirical results in (Lee et al., 2019) demonstrated high performance of a linearized version of training using the so-called NTK regime. In this paper, we show that the large depth limit of this regime is unexpectedly trivial, and we fully characterize the convergence rate to this trivial regime.},
 author = {Hayou, Soufiane and Doucet, Arnaud and Rousseau, Judith},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 pages = {41--47},
 pdf = {https://proceedings.mlr.press/v163/hayou22a/hayou22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The Curse of Depth in Kernel Regime},
 url = {https://proceedings.mlr.press/v163/hayou22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-kasturi22a,
 abstract = {A well-known but rarely used approach to text categorization uses conditional entropy estimates computed using data compression tools. Text affinity scores derived from compressed sizes can be used for classification and ranking tasks, but their success depends on the compression tools used. We use the Zstandard compressor and strengthen these ideas in several ways, calling the resulting language-agnostic technique Zest. In applications, this approach simplifies configuration, avoiding careful feature extraction and large ML models. Our ablation studies confirm the value of individual enhancements we introduce. We show that Zest complements and can compete with language-specific multidimensional content embeddings in production, but cannot outperform other counting methods on public datasets.},
 author = {Kasturi, Nitya and Markov, Igor L.},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 openalex = {W4286963606},
 pages = {48--53},
 pdf = {https://proceedings.mlr.press/v163/kasturi22a/kasturi22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Text Ranking and Classification using Data Compression},
 url = {https://proceedings.mlr.press/v163/kasturi22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-kelz22a,
 abstract = {We cast the combinatorial problem of polyphonic piano transcription as a two stage process. A nonlinear denoising stage maps spectrogram representations of arbitrary piano music with unknown timbral characteristics onto a canonical spectrogram representation with known timbral characteristics. A subsequent linear demixing stage aims to exploit the knowledge about the canonical timbral characteristics. The idea behind this two stage process is to try to elegantly sidestep any musical bias inherent in the training dataset that is easily picked up by a single stage, nonlinear (neural) transcription system (with large capacity). The two stage process tries not to force the nonlinear system to solve a combinatorial problem, which is more amenable to being solved by a linear decomposition method that has the superposition property. Using the simplest setup we could think of, we obtain (rather mixed (pun intended)) results on a standard polyphonic piano transcription dataset - the two stage process still suffers from generalization problems after the first stage, which the second stage is unable to compensate.},
 author = {Kelz, Rainer and Widmer, Gerhard},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 pages = {54--58},
 pdf = {https://proceedings.mlr.press/v163/kelz22a/kelz22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Nonlinear Denoising, Linear Demixing},
 url = {https://proceedings.mlr.press/v163/kelz22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-murray22a,
 abstract = {Farquhar et al. [2021] show that correcting for active learning bias with underparameterised models leads to improved downstream performance. For overparameterised models such as NNs, however, correction leads either to decreased or unchanged performance. They suggest that this is due to an "overfitting bias" which offsets the active learning bias. We show that depth uncertainty networks operate in a low overfitting regime, much like underparameterised models. They should therefore see an increase in performance with bias correction. Surprisingly, they do not. We propose that this negative result, as well as the results Farquhar et al. [2021], can be explained via the lens of the bias-variance decomposition of generalisation error.},
 author = {Murray, Chelsea and Allingham, James U. and Antor\'{a}n, Javier and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 openalex = {W4200630763},
 pages = {59--63},
 pdf = {https://proceedings.mlr.press/v163/murray22a/murray22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Addressing Bias in Active Learning with Depth Uncertainty Networks... or Not},
 url = {https://proceedings.mlr.press/v163/murray22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-ness22a,
 abstract = {Batch Normalizaiton (BN) is a normalization method for deep neural networks that has been shown to accelerate training. While the effectiveness of BN is undisputed, the explanation of its effectiveness is still being studied. The original BN paper attributes the success of BN to reducing internal covariate shift, so we take this a step further and explicitly enforce a Gaussian distribution on hidden layer activations. This approach proves to be ineffective, demonstrating further that reducing internal covariate shift is not important for successful layer normalization.},
 author = {Ness, Mike Van and Udell, Madeleine},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 pages = {64--68},
 pdf = {https://proceedings.mlr.press/v163/ness22a/ness22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {{CDF} Normalization for Controlling the Distribution of Hidden Nodes},
 url = {https://proceedings.mlr.press/v163/ness22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-pirozelli22a,
 abstract = {âBeautyâ is a highly disputed word in philosophy and art. It also appears frequently in scientific debates. But what is the role of beauty in science, and how can it be useful to AI? In this paper, we argue that scientific progress depends on the diversity of the judgment of scientists, something that is only possible because multiple aspects are involved in the evaluation of theories. Particularly important within these criteria are those related to aesthetic considerations, such as simplicity, consistency, broadness, and fertility. We claim that AI should be less focused on accuracy and related metrics, and instead should aim at integrating epistemic measures related to these aesthetic concepts.},
 author = {Pirozelli, Paulo and Cortese, Jo\~{a}o F. N.},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 pages = {69--74},
 pdf = {https://proceedings.mlr.press/v163/pirozelli22a/pirozelli22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The Beauty Everywhere: How Aesthetic Criteria Contribute to the Development of {AI}},
 url = {https://proceedings.mlr.press/v163/pirozelli22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-rohde22a,
 abstract = {It is often argued that causal inference is a step that follows probabilistic estimation in a two step procedure, with a separate statistical estimation and causal inference step and each step is governed by its own principles. We have argued to the contrary that Bayesian decision theory is perfectly adequate to do causal inference in a single step using nothing more than Bayesian conditioning. If true this formulation greatly simplifies causal inference. We outline this beautifully simple idea and discuss why some object to it.},
 author = {Rohde, David},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 pages = {75--79},
 pdf = {https://proceedings.mlr.press/v163/rohde22a/rohde22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Inference, is just Inference: A beautifully simple idea that not everyone accepts},
 url = {https://proceedings.mlr.press/v163/rohde22a.html},
 volume = {163},
 year = {2022}
}

@inproceedings{pmlr-v163-wang22a,
 abstract = {We consider the problem of probabilistic forecasting over categories with graph structure, where the dynamics at a vertex depends on its local connectivity structure. We present GOPHER, a method that combines the inductive bias of graph neural networks with neural ODEs to capture the intrinsic local continuous-time dynamics of our probabilistic forecasts. We study the benefits of these two inductive biases by comparing against baseline models that help disentangle the benefits of each. We find that capturing the graph structure is crucial for accurate in-domain probabilistic predictions and more sample efficient models. Surprisingly, our experiments demonstrate that the continuous time evolution inductive bias brings little to no benefit despite reflecting the true probability dynamics.},
 author = {Wang, Ke A. and Maddix, Danielle and Wang, Yuyang},
 booktitle = {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
 editor = {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
 month = {13 Dec},
 openalex = {W4226096409},
 pages = {80--85},
 pdf = {https://proceedings.mlr.press/v163/wang22a/wang22a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {GOPHER: Categorical probabilistic forecasting with graph structure via local continuous-time dynamics},
 url = {https://proceedings.mlr.press/v163/wang22a.html},
 volume = {163},
 year = {2022}
}
