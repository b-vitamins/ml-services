@proceedings{AISTATS2009,
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {David Dyk and Max Welling},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 volume = {5}
}

@inproceedings{pmlr-v5-ackerman09a,
 abstract = {We investigate measures of the clusterability of data sets. Namely, ways to dene how ‘strong’ or ‘conclusive’ is the clustering structure of a given data set. We address this issue with generality, aiming for conclusions that apply regardless of any particular clustering algorithm or any specic data generation model. We survey several notions of clusterability that have been discussed in the literature, as well as propose a new notion of data clusterability. Our comparison of these notions reveals that, although they all attempt to evaluate the same intuitive property, they are pairwise inconsistent. Our analysis discovers an interesting phenomenon; Although most of the common clustering tasks are NP-hard, nding a closeto-optimal clustering for well clusterable data sets is easy (computationally). We prove instances of this general claim with respect to the various clusterability notions that we discuss. Finally, we investigate how hard it is to determine the clusterability value of a given data set. In most cases, it turns out that this is an NP-hard problem.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Ackerman, Margareta and Ben-David, Shai},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2162148987},
 pages = {1--8},
 pdf = {http://proceedings.mlr.press/v5/ackerman09a/ackerman09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Clusterability: A Theoretical Study},
 url = {https://proceedings.mlr.press/v5/ackerman09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-alvarez09a,
 abstract = {Purely data driven approaches for machine learning present diculties when data is scarce relative to the complexity of the model or when the model is forced to extrapolate. On the other hand, purely mechanistic approaches need to identify and specify all the interactions in the problem at hand (which may not be feasible) and still leave the issue of how to parameterize the system. In this paper, we present a hybrid approach using Gaussian processes and dierential equations to combine data driven modelling with a physical model of the system. We show how dierent, physically-inspired, kernel functions can be developed through sensible, simple, mechanistic assumptions about the underlying system. The versatility of our approach is illustrated with three case studies from computational biology, motion capture and geostatistics.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Ãlvarez, Mauricio and Luengo, David and Lawrence, Neil D.},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1490533726},
 pages = {9--16},
 pdf = {http://proceedings.mlr.press/v5/alvarez09a/alvarez09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Latent Force Models},
 url = {https://proceedings.mlr.press/v5/alvarez09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-armagan09a,
 abstract = {Here we obtain approximate Bayes inferences through variational methods when an exponential power family type prior is specified for the regression coefficients to mimic the characteristics of the Bridge regression. We accomplish this through hierarchical modeling of such priors. Although the mixing distribution is not explicitly stated for scale normal mixtures, we obtain the required moments only to attain the variational distributions for the regression coefficients. By choosing specific values of hyper-parameters (tuning parameters) present in the model, we can mimic the model selection performance of best subset selection in sparse underlying settings. The fundamental difference between MAP, maximum a posteriori, estimation and the proposed method is that, here we can obtain approximate inferences besides a point estimator. We also empirically analyze the frequentist properties of the estimator obtained. Results suggest that the proposed method yields an estimator that performs significantly better in sparse underlying setups than the existing state-of-the-art procedures in both n > p and p > n scenarios.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Armagan, Artin},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1945766733},
 pages = {17--24},
 pdf = {http://proceedings.mlr.press/v5/armagan09a/armagan09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variational Bridge Regression},
 url = {https://proceedings.mlr.press/v5/armagan09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-ben-david09a,
 abstract = {We define a novel, basic, unsupervised learning problem – learning hyperplane passing through the origin with the lowest probability density. Namely, given a random sample generated by some unknown probability distribution, the task is to find a hyperplane passing through the origin with smallest integral of the probability density on the hyperplane. This task is relevant to several problems in machine learning, such as semisupervised learning and clustering stability. We investigate the question of existence of a universally consistent algorithm for this problem. We propose two natural learning paradigms and prove that, on input random samples generated i.i.d. by any distribution, they are guaranteed to converge to the optimal separator for that distribution. We complement this result by showing that no learning algorithm for our task can achieve learning rates that are independent of the data generating distribution.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Ben-David, Shai and Lu, Tyler and Pal, David and Sotakova, Miroslava},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1491144097},
 pages = {25--32},
 pdf = {http://proceedings.mlr.press/v5/ben-david09a/ben-david09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Low Density Separators},
 url = {https://proceedings.mlr.press/v5/ben-david09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-bo09a,
 abstract = {We present a probabilistic structured prediction method for learning input-output dependencies where correlations between outputs are modeled as low-dimensional manifolds constrained by both geometric, distance preserving output relations, and predictive power of inputs. Technically this reduces to learning a probabilistic, input conditional model, over latent (manifold) and output variables using an alternation scheme. In one round, we optimize the parameters of an input-driven manifold predictor using latent targets given by preimages (conditional expectations) of the current manifold-to-output model. In the next round, we use the distribution given by the manifold predictor in order to maximize the probability of the outputs with an additional, implicit geometry preserving constraint on the manifold. The resulting Supervised Spectral Latent Variable Model (SSLVM) combines the properties of probabilistic geometric manifold learning (accommodates geometric constraints corresponding to any spectral embedding method including PCA, ISOMAP or Laplacian Eigenmaps), with the additional supervisory information to further constrain it for predictive tasks. We demonstrate the superiority of the method over baseline PPCA + regression frameworks and show its potential in difficult real-world computer vision benchmarks designed for the reconstruction of three-dimensional human poses from monocular image sequences. Appearing in Proceedings of the 12 International Conference on Artificial Intelligence and Statistics (AISTATS) 2009, Clearwater Beach, Florida, USA. Volume 5 of JMLR: W&CP 5. Copyright 2009 by the authors.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Bo, Liefeng and Sminchisescu, Cristian},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W132289796},
 pages = {33--40},
 pdf = {http://proceedings.mlr.press/v5/bo09a/bo09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Supervised Spectral Latent Variable Models},
 url = {https://proceedings.mlr.press/v5/bo09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-bravo09a,
 abstract = {We present a novel method for estimating tree-structured covariance matrices directly from observed continuous data. Specifically, we estimate a covariance matrix from observations of p continuous random variables encoding a stochastic process over a tree with p leaves. A representation of these classes of matrices as linear combinations of rank-one matrices indicating object partitions is used to formulate estimation as instances of well-studied numerical optimization problems.In particular, our estimates are based on projection, where the covariance estimate is the nearest tree-structured covariance matrix to an observed sample covariance matrix. The problem is posed as a linear or quadratic mixed-integer program (MIP) where a setting of the integer variables in the MIP specifies a set of tree topologies of the structured covariance matrix. We solve these problems to optimality using efficient and robust existing MIP solvers.We present a case study in phylogenetic analysis of gene expression and a simulation study comparing our method to distance-based tree estimating procedures.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Bravo, Hector Corrada and Wright, Stephen and Eng, Kevin and Keles, Sunduz and Wahba, Grace},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2157402621},
 pages = {41--48},
 pdf = {http://proceedings.mlr.press/v5/bravo09a/bravo09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Estimating Tree-Structured Covariance Matrices via Mixed-Integer Programming.},
 url = {https://proceedings.mlr.press/v5/bravo09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-brown09a,
 abstract = {Feature Filters are among the simplest and fastest approaches to feature selection. A filter defines a statistical criterion, used to rank features on how useful they are expected to be for classification. The highest ranking features are retained, and the lowest ranking can be discarded. A common approach is to use the Mutual Information between the feature and class label. This area has seen a recent flurry of activity, resulting in a confusing variety of heuristic criteria all based on mutual information, and a lack of a principled way to understand or relate them. The contribution of this paper is a unifying theoretical understanding of such filters. In contrast to current methods which manually construct filter criteria with particular properties, we show how to naturally derive a space of possible ranking criteria. We will show that several recent contributions in the feature selection literature are points within this continuous space, and that there exist many points that have never been explored.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Brown, Gavin},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2115247131},
 pages = {49--56},
 pdf = {http://proceedings.mlr.press/v5/brown09a/brown09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A New Perspective for Information Theoretic Feature Selection},
 url = {https://proceedings.mlr.press/v5/brown09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-busetto09a,
 abstract = {We consider the problem of optimal experimental design in structure identification. Whereas standard approaches simply minimize Shannon’s entropy of the estimated parameter posterior, we show how to select between alternative model configurations, too. Our method specifies the intervention that makes an experiment capable of determining whether or not a particular configuration hypothesis is correct. This is performed by a novel clustering technique in approximated Bayesian parameter estimation for non-linear dynamical systems. The computation of the perturbation that minimizes the effective number of clusters in the belief state is constrained by the increase of the expected Kullback-Leibler divergence between the parameter prior and the posterior. This enables the disambiguation of persisting alternative explanations in cases where standard design systematically fails. Its applicability is illustrated with a biochemical Goodwin model, showing correct identification between multiple kinetic structures. We expect that our approach will prove useful especially for complex structures with reduced observability and multimodal posteriors.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Busetto, Alberto Giovanni and Buhmann, Joachim},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2531148845},
 pages = {57--64},
 pdf = {http://proceedings.mlr.press/v5/busetto09a/busetto09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Structure Identification by Optimized Interventions},
 url = {https://proceedings.mlr.press/v5/busetto09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-canini09a,
 abstract = {Inference algorithms for topic models are typically designed to be run over an entire collection of documents after they have been observed. However, in many applications of these models, the collection grows over time, making it infeasible to run batch algorithms repeatedly. This problem can be addressed by using online algorithms, which update estimates of the topics as each document is observed. We introduce two related RaoBlackwellized online inference algorithms for the latent Dirichlet allocation (LDA) model – incremental Gibbs samplers and particle filters – and compare their runtime and performance to that of existing algorithms.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Canini, Kevin and Shi, Lei and Griffiths, Thomas},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W159230833},
 pages = {65--72},
 pdf = {http://proceedings.mlr.press/v5/canini09a/canini09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Online Inference of Topics with Latent Dirichlet Allocation},
 url = {https://proceedings.mlr.press/v5/canini09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-carvalho09a,
 abstract = {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 pages = {73--80},
 pdf = {http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Handling Sparsity via the Horseshoe},
 url = {https://proceedings.mlr.press/v5/carvalho09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-chang09a,
 abstract = {We develop the relational topic model (RTM), a model of documents and the links between them. For each pair of documents, the RTM models their link as a binary random variable that is conditioned on their contents. The model can be used to summarize a network of documents, predict links between them, and predict words within them. We derive efficient inference and learning algorithms based on variational methods and evaluate the predictive performance of the RTM for large networks of scientific abstracts and web documents.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Chang, Jonathan and Blei, David},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2123549998},
 pages = {81--88},
 pdf = {http://proceedings.mlr.press/v5/chang09a/chang09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Relational Topic Models for Document Networks},
 url = {https://proceedings.mlr.press/v5/chang09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-chu09a,
 abstract = {In multiway data, each sample is measured by multiple sets of correlated attributes. We develop a probabilistic framework for modeling structural dependency from partially observed multi-dimensional array data, known as pTucker. Latent components associated with individual array dimensions are jointly retrieved while the core tensor is integrated out. The resulting algorithm is capable of handling large-scale data sets. We verify the usefulness of this approach by comparing against classical models on applications to modeling amino acid fluorescence, collaborative filtering and a number of benchmark multiway array data.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Chu, Wei and Ghahramani, Zoubin},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2096602764},
 pages = {89--96},
 pdf = {http://proceedings.mlr.press/v5/chu09a/chu09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Probabilistic models for incomplete multi-dimensional arrays},
 url = {https://proceedings.mlr.press/v5/chu09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-clemencon09a,
 abstract = {The purpose of this paper is to investigate the properties of partitioning scoring rules in the bipartite ranking setup. We focus on ranking rules based on scoring functions. General sufcient conditions for the AUC consistency of scoring functions that are constant on cells of a partition of the feature space are provided. Rate bounds are obtained for cubic histogram scoring rules under mild smoothness assumptions on the regression function. In this setup, it is shown how to penalize the empirical AUC criterion in order to select a scoring rule nearly as good as the one that can be built when the degree of smoothness of the regression function is known.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Clemencon, Stephan and Vayatis, Nicolas},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2101769891},
 pages = {97--104},
 pdf = {http://proceedings.mlr.press/v5/clemencon09a/clemencon09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On Partitioning Rules for Bipartite Ranking},
 url = {https://proceedings.mlr.press/v5/clemencon09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-crammer09a,
 abstract = {We introduce Gaussian Margin Machines (GMMs), which maintain a Gaussian distribution over weight vectors for binary classification. The learning algorithm for these machines seeks the least informative distribution that will classify the training data correctly with high probability. One formulation can be expressed as a convex constrained optimization problem whose solution can be represented linearly in terms of training instances and their inner and outer products, supporting kernelization. The algorithm admits a natural PAC-Bayesian justification and is shown to minimize a quantity directly related to a PAC-Bayesian generalization bound. A preliminary evaluation on handwriting recognition data shows that our algorithm improves on SVMs for the same task, achieving lower test error and lower test error variance.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Crammer, Koby and Mohri, Mehryar and Pereira, Fernando},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2103452263},
 pages = {105--112},
 pdf = {http://proceedings.mlr.press/v5/crammer09a/crammer09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Gaussian Margin Machines},
 url = {https://proceedings.mlr.press/v5/crammer09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-dafna09a,
 abstract = {Structure learning algorithms usually focus on the compactness of the learned model. However, for general compact models, both exact and approximate inference are still NP-hard. Therefore, the focus only on compactness leads to learning models that require approximate inference techniques, thus reducing their prediction quality. In this paper, we propose a method for learning an attractive class of models: bounded-treewidth junction trees, which permit both compact representation of probability distributions and efficient exact inference. Using Bethe approximation of the likelihood, we transform the problem of finding a good junction tree separator into a minimum cut problem on a weighted graph. Using the graph cut intuition, we present an efficient algorithm with theoretical guarantees for finding good separators, which we recursively apply to obtain a thin junction tree. Our extensive empirical evaluation demonstrates the benefit of applying exact inference using our models to answer queries. We also extend our technique to learning low tree-width conditional random fields, and demonstrate significant improvements over state of the art block-L1 regularization techniques.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Dafna, Shahaf and Guestrin, Carlos},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2136316075},
 pages = {113--120},
 pdf = {http://proceedings.mlr.press/v5/dafna09a/dafna09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Thin Junction Trees via Graph Cuts},
 url = {https://proceedings.mlr.press/v5/dafna09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-diethe09a,
 abstract = {We derive a novel sparse version of Kernel Fisher Discriminant Analysis (KFDA) using an approach based on Matching Pursuit (MP). We call this algorithm Matching Pursuit Kernel Fisher Discriminant Analysis (MPKFDA). We provide generalisation error bounds analogous to those constructed for the Robust Minimax algorithm together with a sample compression bounding technique. We present experimental results on real world datasets, which show that MPKFDA is competitive with the KFDA and the SVM on UCI datasets, and additional experiments that show that the MPKFDA on average outperforms KFDA and SVM in extremely high dimensional settings.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Diethe, Tom and Hussain, Zakria and Hardoon, David and Shawe-Taylor, John},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2528514897},
 pages = {121--128},
 pdf = {http://proceedings.mlr.press/v5/diethe09a/diethe09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Matching pursuit kernel fisher discriminant analysis},
 url = {https://proceedings.mlr.press/v5/diethe09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-dillon09a,
 abstract = {Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. We prove the consistency of the estimators, provide formulas for their asymptotic variance and computational complexity, and discuss experimental results in the context of Boltzmann machines and conditional random fields. The theoretical and experimental studies demonstrate the effectiveness of the estimators in achieving a predefined balance between computational complexity and statistical accuracy.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Dillon, Joshua and Lebanon, Guy},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1945981599},
 pages = {129--136},
 pdf = {http://proceedings.mlr.press/v5/dillon09a/dillon09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Statistical and Computational Tradeoffs in Stochastic Composite Likelihood},
 url = {https://proceedings.mlr.press/v5/dillon09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-doshi09a,
 abstract = {The Indian Buet Process (IBP) is a nonparametric prior for latent feature models in which observations are influenced by a combination of hidden features. For example, images may be composed of several objects and sounds may consist of several notes. Latent feature models seek to infer these unobserved features from a set of observations; the IBP provides a principled prior in situations where the number of hidden features is unknown. Current inference methods for the IBP have all relied on sampling. While these methods are guaranteed to be accurate in the limit, samplers for the IBP tend to mix slowly in practice. We develop a deterministic variational method for inference in the IBP based on a truncated stick-breaking approximation, provide theoretical bounds on the truncation error, and evaluate our method in several data regimes.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Doshi, Finale and Miller, Kurt and Gael, Jurgen Van and Teh, Yee Whye},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2126658427},
 pages = {137--144},
 pdf = {http://proceedings.mlr.press/v5/doshi09a/doshi09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variational Inference for the Indian Buffet Process},
 url = {https://proceedings.mlr.press/v5/doshi09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-eaton09a,
 abstract = {In this paper we propose an algorithm for  approximate inference on graphical models  based on belief propagation (BP). Our algorithm  is an approximate version of Cutset  Conditioning, in which a set of variables is  instantiated to make the rest of the graph  singly connected. We relax the constraint  of single-connectedness, and select variables  one at a time for conditioning, running belief  propagation after each selection. We consider  the problem of determining the best variable  to clamp at each level of recursion, and  propose a fast heuristic which applies backpropagation  to the BP updates. We demonstrate  that the heuristic performs better than  selecting variables at random, and give experimental  results which show that it performs  competitively with existing approximate inference  algorithms.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Eaton, Frederik and Ghahramani, Zoubin},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W136290230},
 pages = {145--152},
 pdf = {http://proceedings.mlr.press/v5/eaton09a/eaton09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Choosing a Variable to Clamp},
 url = {https://proceedings.mlr.press/v5/eaton09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-erhan09a,
 abstract = {Whereas theoretical work suggests that deep architectures might be more e cient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pretraining. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this di cult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments confirm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive e ect of pre-training in terms of optimization and its role as a regularizer. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Erhan, Dumitru and Manzagol, Pierre-Antoine and Bengio, Yoshua and Bengio, Samy and Vincent, Pascal},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2117499988},
 pages = {153--160},
 pdf = {http://proceedings.mlr.press/v5/erhan09a/erhan09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training},
 url = {https://proceedings.mlr.press/v5/erhan09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-givoni09a,
 abstract = {Recently, anity propagation (AP) was introduced as an unsupervised learning algorithm for exemplar based clustering. Here we extend the AP model to account for semisupervised clustering. AP, which is formulated as inference in a factor-graph, can be naturally extended to account for ‘instancelevel’ constraints: pairs of data points that cannot belong to the same cluster (cannotlink), or must belong to the same cluster (must-link). We present a semi-supervised AP algorithm (SSAP) that can use instancelevel constraints to guide the clustering. We demonstrate the applicability of SSAP to interactive image segmentation by using SSAP to cluster superpixels while taking into account user instructions regarding which superpixels belong to the same object. We demonstrate SSAP can achieve better performance compared to other semi-supervised methods.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Givoni, Inmar and Frey, Brendan},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2105979014},
 pages = {161--168},
 pdf = {http://proceedings.mlr.press/v5/givoni09a/givoni09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Semi-Supervised Affinity Propagation with Instance-Level Constraints},
 url = {https://proceedings.mlr.press/v5/givoni09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-goldberg09a,
 abstract = {We study semi-supervised learning when the data consists of multiple intersecting manifolds. We give a finite sample analysis to quantify the potential gain of using unlabeled data in this multi-manifold setting. We then propose a semi-supervised learning algorithm that separates different manifolds into decision sets, and performs supervised learning within each set. Our algorithm involves a novel application of Hellinger distance and size-constrained spectral clustering. Experiments demonstrate the benefit of our multimanifold semi-supervised learning approach.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Goldberg, Andrew and Zhu, Xiaojin and Singh, Aarti and Xu, Zhiting and Nowak, Robert},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1506622324},
 pages = {169--176},
 pdf = {http://proceedings.mlr.press/v5/goldberg09a/goldberg09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-Manifold Semi-Supervised Learning},
 url = {https://proceedings.mlr.press/v5/goldberg09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-gonzalez09a,
 abstract = {As computer architectures move towards multicore we must build a theoretical understanding of parallelism in machine learning. In this paper we focus on parallel inference in graphical models. We demonstrate that the natural, fully synchronous parallelization of belief propagation is highly inefficient. By bounding the achievable parallel performance in chain graphical models we develop a theoretical understanding of the parallel limitations of belief propagation. We then provide a new parallel belief propagation algorithm which achieves optimal performance. Using two challenging real-world tasks, we empirically evaluate the performance of our algorithm on large cyclic graphical models where we achieve near linear parallel scaling and out perform alternative algorithms.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Gonzalez, Joseph and Low, Yucheng and Guestrin, Carlos},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1809663628},
 pages = {177--184},
 pdf = {http://proceedings.mlr.press/v5/gonzalez09a/gonzalez09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Residual Splash for Optimally Parallelizing Belief Propagation},
 url = {https://proceedings.mlr.press/v5/gonzalez09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-guan09a,
 abstract = {Principal component analysis (PCA) is a popular dimensionality reduction algorithm. However, it is not easy to interpret which of the original features are important based on the principal components. Recent methods improve interpretability by sparsifying PCA through adding an L1 regularizer. In this paper, we introduce a probabilistic formulation for sparse PCA. By presenting sparse PCA as a probabilistic Bayesian formulation, we gain the benet of automatic model selection. We examine three dierent priors for achieving sparsication:},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Guan, Yue and Dy, Jennifer},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2097463421},
 pages = {185--192},
 pdf = {http://proceedings.mlr.press/v5/guan09a/guan09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sparse Probabilistic Principal Component Analysis},
 url = {https://proceedings.mlr.press/v5/guan09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-guha09a,
 abstract = {Comprehensive visualization that preserves the information in a large complex dataset requires a visualization database (VDB): many displays, some with many pages, and with one or more panels per page. A single display using a specific display method results from partitioning the data into subsets, sampling the subsets, and applying the method to each sample, typically one per panel. The time of the analyst to generate a display is not increased by choosing a large sample over a small one. Displays and display viewers can be designed to allow rapid scanning. Often, it is not necessary to view every page of a display. VDBs, already successful just with off-the-shelf tools, can be greatly improved by research that rethinks all of the areas of data visualization in the context of VDBs.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Guha, Saptarshi and Kidwell, Paul and Hafen, Ryan P. and Cleveland, William S.},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2105627672},
 pages = {193--200},
 pdf = {http://proceedings.mlr.press/v5/guha09a/guha09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Visualization Databases for the Analysis of Large Complex Datasets},
 url = {https://proceedings.mlr.press/v5/guha09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-guillory09a,
 abstract = {We propose a new view of active learning algorithms as optimization. We show that many online active learning algorithms can be viewed as stochastic gradient descent on non-convex objective functions. Variations of some of these algorithms and objective functions have been previously proposed without noting this connection. We also point out a connection between the standard min-margin offline active learning algorithm and non-convex losses. Finally, we discuss and show empirically how viewing active learning as non-convex loss minimization helps explain two previously observed phenomena: certain active learning algorithms achieve better generalization error than passive learning algorithms on certain data sets (Schohn and Cohn, 2000; Bordes et al., 2005) and on other data sets many active learning algorithms are prone to local minima (Sch¨ utze et al., 2006). 1 Background We address the active learning problem in this paper. We assume data points X 2 R d and labels Y 2 fi1;1g are drawn from some fixed unknown distribution p(x;y). We wish to choose the classifier f 2 F that minimizes the expected loss EX;Y [l(Y;f;X)] where l is a loss function. In general, the size of F may be uncountable. In standard passive supervised learning we approximately minimize this expected loss by minimizing instead the loss on a set or stream of i.i.d. labeled training examples (xi;yi) » p(x;y). In active learning, we also have access to i.i.d. xi, but we selectively query the labels yi for the examples. In},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Guillory, Andrew and Chastain, Erick and Bilmes, Jeff},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2165798027},
 pages = {201--208},
 pdf = {http://proceedings.mlr.press/v5/guillory09a/guillory09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Active Learning as Non-Convex Optimization},
 url = {https://proceedings.mlr.press/v5/guillory09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-hanneke09a,
 abstract = {We study the problem of learning the topology of an undirected network by observing a random subsample. Specifically, the sample is chosen by randomly selecting a fixed number of vertices, and for each we are allowed to observe all edges it is incident with. We analyze a general formalization of learning from such samples, and derive confidence bounds on the number of differences between the true and learned topologies, as a function of the number of observed mistakes and the algorithm’s bias. In addition to this general analysis, we also analyze a variant of the problem under a stochastic block model assumption.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Hanneke, Steve and Xing, Eric P.},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2142562746},
 pages = {209--215},
 pdf = {http://proceedings.mlr.press/v5/hanneke09a/hanneke09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Network Completion and Survey Sampling},
 url = {https://proceedings.mlr.press/v5/hanneke09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-haupt09a,
 abstract = {A selective sampling procedure called distilled sensing (DS) is proposed, and shown to be an efiective method for recovering sparse signals in noise. Based on the notion that it is often easier to rule out locations that do not contain signal than it is to directly identify non-zero signal components, DS is a sequential method that systematically focuses sensing resources towards the signal subspace. This adaptivity in sensing results in rather surprising gains in sparse signal recovery| dramatically weaker sparse signals can be recovered using DS compared with conventional non-adaptive sensing procedures.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Haupt, Jarvis and Castro, Rui and Nowak, Robert},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2138820382},
 pages = {216--223},
 pdf = {http://proceedings.mlr.press/v5/haupt09a/haupt09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Distilled sensing : selective sampling for sparse signal recovery},
 url = {https://proceedings.mlr.press/v5/haupt09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-heller09a,
 abstract = {In this paper we present the Infinite Hierarchical Hidden Markov Model (IHHMM), a nonparametric generalization of Hierarchical Hidden Markov Models (HHMMs). HHMMs have been used for modeling sequential data in applications such as speech recognition, detecting topic transitions in video and extracting information from text. The IHHMM provides more flexible modeling of sequential data by allowing a potentially unbounded number of levels in the hierarchy, instead of requiring the specification of a fixed hierarchy depth. Inference and learning are performed eciently using Gibbs sampling and a modified forward-backtrack algorithm. We present encouraging results on toy sequences and English text data.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Heller, Katherine and Teh, Yee Whye and Gorur, Dilan},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2150196753},
 pages = {224--231},
 pdf = {http://proceedings.mlr.press/v5/heller09a/heller09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Infinite Hierarchical Hidden Markov Models},
 url = {https://proceedings.mlr.press/v5/heller09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-hoffman09a,
 abstract = {We derive a new expectation maximization algorithm for policy optimization in linear Gaussian Markov decision processes, where the reward function is parameterized in terms of a flexible mixture of Gaussians. This approach exploits both analytical tractability and numerical optimization. Consequently, on the one hand, it is more flexible and general than closed-form solutions, such as the widely used linear quadratic Gaussian (LQG) controllers. On the other hand, it is more accurate and faster than optimization methods that rely on approximation and simulation. Partial analytical solutions (though costly) eliminate the need for simulation and, hence, avoid approximation error. The experiments will show that for the same cost of computation, policy optimization methods that rely on analytical tractability have higher value than the ones that rely on simulation.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Hoffman, Matthew and Freitas, Nando and Doucet, Arnaud and Peters, Jan},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2111467524},
 pages = {232--239},
 pdf = {http://proceedings.mlr.press/v5/hoffman09a/hoffman09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An Expectation Maximization Algorithm for Continuous Markov Decision Processes with Arbitrary Reward},
 url = {https://proceedings.mlr.press/v5/hoffman09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-huang09a,
 abstract = {We demonstrate a generalization of Maximum Entropy Density Estimation that elegantly handles incomplete presence-only data. We provide a formulation that is able to learn from known values of incomplete data without having to learn imputed values, which may be inaccurate. This saves the effort needed to perform accurate imputation while observing the principle of maximum entropy throughout the learning process. We provide analysis and examples of our algorithm under different settings of missing data.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Huang, Bert and Salleb-Aouissi, Ansaf},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2113181297},
 pages = {240--247},
 pdf = {http://proceedings.mlr.press/v5/huang09a/huang09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Maximum Entropy Density Estimation with Incomplete Presence-Only Data},
 url = {https://proceedings.mlr.press/v5/huang09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-huang09b,
 abstract = {Permutations are ubiquitous in many real world problems, such as voting, rankings and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities. Recent Fourier-based approaches can be used to provide a compact representation over low-frequency components of the distribution. Though polynomial, the complexity of these representations grows very rapidly, especially if we want to maintain reasonable estimates for peaked distributions. In this paper, we rst characterize the notion of probabilistic independence for distributions over permutations. We then present a method for factoring distributions into independent components in the Fourier domain, and use our algorithms to decompose large problems into much smaller ones. We demonstrate that our method provides very signi cant improvements in terms of running time, on real tracking data.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Huang, Jonathan and Guestrin, Carlos and Jiang, Xiaoye and Guibas, Leonidas},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1617088490},
 pages = {248--255},
 pdf = {http://proceedings.mlr.press/v5/huang09b/huang09b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Exploiting Probabilistic Independence for Permutations},
 url = {https://proceedings.mlr.press/v5/huang09b.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-ihler09a,
 abstract = {The popularity of particle filtering for inference in Markov chain models defined over random variables with very large or continuous domains makes it natural to consider sampleâbased versions of belief propagation (BP) for more general (treeâstructured or loopy) graphs. Already, several such algorithms have been proposed in the literature.  However, many questions remain open about the behavior of particleâbased BP algorithms, and little theory has been developed to analyze their performance. In this paper, we describe a generic particle belief propagation (PBP) algorithm which is closely related to previously proposed methods.  We prove that this algorithm is consistent, approaching the true BP messages as the number of samples grows large. We then use concentration bounds to analyze the finite-sample behavior and give a convergence rate for the algorithm on treeâstructured graphs. Our convergence rate is $O(1/\sqrt{n})$ where n is the number of samples, independent of the domain size of the variables.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Ihler, Alexander and McAllester, David},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W94449998},
 pages = {256--263},
 pdf = {http://proceedings.mlr.press/v5/ihler09a/ihler09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Particle Belief Propagation},
 url = {https://proceedings.mlr.press/v5/ihler09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-johanson09a,
 abstract = {The problem of exploiting information about the environment while still being robust to inaccurate or incomplete information arises in many domains. Competitive imperfect information games where the goal is to maximally exploit an unknown opponent’s weaknesses are an example of this problem. Agents for these games must balance two objectives. First, they should aim to exploit data from past interactions with the opponent, seeking a best-response counter strategy. Second, they should aim to minimize losses since the limited data may be misleading or the opponent’s strategy may have changed, suggesting an opponent-agnostic Nash equilibrium strategy. In this paper, we show how to partially satisfy both of these objectives at the same time, producing strategies with favourable tradeoffs between the ability to exploit an opponent and the capacity to be exploited. Like a recently published technique, our approach involves solving a modified game; however the result is more generally applicable and even performs well in situations with very limited data. We evaluate our technique in the game of two-player, Limit Texas Hold’em.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Johanson, Michael and Bowling, Michael},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2188529601},
 pages = {264--271},
 pdf = {http://proceedings.mlr.press/v5/johanson09a/johanson09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Data Biased Robust Counter Strategies},
 url = {https://proceedings.mlr.press/v5/johanson09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-kanade09a,
 abstract = {We consider the problem of selecting actions in order to maximize rewards chosen by an adversary, where the set of actions available on any given round is selected stochastically. We present the first polynomial-time no-regret algorithm for this setting. In the full-observation (experts) version of the problem, we present an exponential-weights algorithm that achieves regret O( p T logn), which is the best possible. For the bandit setting (where the algorithm only observes the reward of the action selected), we present a no-regret algorithm based on follow-theperturbed-leader. This algorithm runs in polynomial time, unlike the EXP4 algorithm which can also be applied to this setting. Our algorithm has the interesting interpretation of solving a geometric experts problem where the actual embedding is never explicitly constructed. We argue that this adversarialreward, stochastic-availability formulation is important in practice, as assuming stationary stochastic rewards is unrealistic in many domains.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Kanade, Varun and McMahan, H. Brendan and Bryan, Brent},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2121136483},
 pages = {272--279},
 pdf = {http://proceedings.mlr.press/v5/kanade09a/kanade09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sleeping Experts and Bandits with Stochastic Action Availability and Adversarial Rewards},
 url = {https://proceedings.mlr.press/v5/kanade09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-kim09a,
 abstract = {We consider the task of dimensionality reduction for regression (DRR) informed by realvalued multivariate labels. The problem is often treated as a regression task where the goal is to nd a low dimensional representation of the input data that preserves the statistical correlation with the targets. Recently, Covariance Operator Inverse Regression (COIR) was proposed as an eective solution that exploits the covariance structures of both input and output. COIR addresses known limitations of recent DRR techniques and allows a closed-form solution without resorting to explicit output space slicing often required by existing IR-based methods. In this work we provide a unifying view of COIR and other DRR techniques and relate them to the popular supervised dimensionality reduction methods including the canonical correlation analysis (CCA) and the linear discriminant analysis (LDA). We then show that COIR can be eectively extended to a semi-supervised learning setting where many of the input points lack their corresponding multivariate targets. A study of benets of},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Kim, Minyoung and Pavlovic, Vladimir},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2150352612},
 pages = {280--287},
 pdf = {http://proceedings.mlr.press/v5/kim09a/kim09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Covariance Operator Based Dimensionality Reduction with Extension to Semi-Supervised Settings},
 url = {https://proceedings.mlr.press/v5/kim09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-kramer09a,
 abstract = {The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is quadratic in the number of examples. However, the necessity of obtaining sensitivity measures as degrees of freedom for model selection or confidence intervals for more detailed analysis requires cubic runtime, and thus constitutes a computational bottleneck in real-world data analysis. We propose a novel algorithm for KPLS which not only computes (a) the fit, but also (b) its approximate degrees of freedom and (c) error bars in quadratic runtime. The algorithm exploits a close connection between Kernel PLS and the Lanczos algorithm for approximating the eigenvalues of symmetric matrices, and uses this approximation to compute the trace of powers of the kernel matrix in quadratic runtime.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Kramer, Nicole and Sugiyama, Masashi and Braun, Mikio},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2951721368},
 pages = {288--295},
 pdf = {http://proceedings.mlr.press/v5/kramer09a/kramer09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Lanczos Approximations for the Speedup of Kernel Partial Least Squares Regression},
 url = {https://proceedings.mlr.press/v5/kramer09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-kulis09a,
 abstract = {Many important machine learning problems are modeled and solved via semidefinite programs; examples include metric learning, nonlinear embedding, and certain clustering problems. Often, off-the-shelf software is invoked for the associated optimization, which can be inappropriate due to excessive computational and storage requirements. In this paper, we introduce the use of convex perturbations for solving semidefinite programs (SDPs), and for a specific perturbation we derive an algorithm that has several advantages over existing techniques: a) it is simple, requiring only a few lines of MATLAB, b) it is a first-order method, and thereby scalable, and c) it can easily exploit the structure of a given SDP (e.g., when the constraint matrices are low-rank, a situation common to several machine learning SDPs). A pleasant byproduct of our method is a fast, kernelized version of the large-margin nearest neighbor metric learning algorithm (Weinberger et al., 2005). We demonstrate that our algorithm is effective in finding fast approximations to large-scale SDPs arising in some machine learning applications.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Kulis, Brian and Sra, Suvrit and Dhillon, Inderjit},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2117562221},
 pages = {296--303},
 pdf = {http://proceedings.mlr.press/v5/kulis09a/kulis09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Convex Perturbations for Scalable Semidefinite Programming},
 url = {https://proceedings.mlr.press/v5/kulis09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-kumar09a,
 abstract = {The Nystrom method is an efficient technique to generate low-rank matrix approximations and is used in several large-scale learning applications. A key aspect of this method is the distribution according to which columns are sampled from the original matrix. In this work, we present an analysis of different sampling techniques for the Nystrom method. Our analysis includes both empirical and theoretical components. We first present novel experiments with several real world datasets, comparing the performance of the Nystrom method when used with uniform versus non-uniform sampling distributions. Our results suggest that uniform sampling without replacement, in addition to being more efficient both in time and space, produces more effective approximations. This motivates the theoretical part of our analysis which gives the first performance bounds for the Nystrom method precisely when used with uniform sampling without replacement.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1558808826},
 pages = {304--311},
 pdf = {http://proceedings.mlr.press/v5/kumar09a/kumar09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sampling Techniques for the Nystrom Method},
 url = {https://proceedings.mlr.press/v5/kumar09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-larochelle09a,
 abstract = {We investigate a simple yet effective method to introduce inhibitory and excitatory interactions between units in the layers of a deep neural network classifier. The method is based on the greedy layer-wise procedure of deep learning algorithms and extends the denoising autoencoder (Vincent et al., 2008) by adding asymmetric lateral connections between its hidden coding units, in a manner that is much simpler and computationally more efficient than previously proposed approaches. We present experiments on two character recognition problems which show for the first time that lateral connections can significantly improve the classification performance of deep networks.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Larochelle, Hugo and Erhan, Dumitru and Vincent, Pascal},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W151672344},
 pages = {312--319},
 pdf = {http://proceedings.mlr.press/v5/larochelle09a/larochelle09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Deep Learning using Robust Interdependent Codes},
 url = {https://proceedings.mlr.press/v5/larochelle09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-lee09a,
 abstract = {Given electroencephalogram (EEG) data measured from several subjects under the same conditions, our goal is to estimate common task-related bases in a linear model that capture intra-subject variations as well as inter-subject variations. Such bases capture the common phenomenon in group data, which is a core of group analysis. In this paper we present a method of nonnegative matrix factorization (NMF) that is well suited to analyzing EEG data of multiple subjects. The method is referred to as group nonnegative matrix factorization (GNMF) where we seek task-related common bases reflecting both intra-subject and inter-subject variations, as well as bases involving individual characteristics. We compare GNMF with NMF and some modified NMFs, in the task of learning spectral features from EEG data. Experiments on brain computer interface (BCI) competition data indicate that GNMF improves the EEG classification performance. In addition, we also show that GNMF is useful in the task of subject-tosubject transfer where the prediction for an unseen subject is performed based on a linear model learned from different subjects in the same group.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Lee, Hyekyoung and Choi, Seungjin},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2530593441},
 pages = {320--327},
 pdf = {http://proceedings.mlr.press/v5/lee09a/lee09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Group Nonnegative Matrix Factorization for EEG Classification},
 url = {https://proceedings.mlr.press/v5/lee09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-li09a,
 abstract = {We study the problem of learning a kernel matrix from an apriori kernel and training data. An unconstrained convex optimization formulation is proposed, with an arbitrary convex smooth loss function on kernel entries and a LogDet divergence for regularization. Since the number of variables is of order O(n), standard Newton and quasi-Newton methods are too time-consuming. An operator form Hessian is used to develop an O(n) trust-region inexact Newton method, where the Newton direction is computed using several conjugate gradient steps on the Hessian operator equation. On the uspst dataset, our algorithm can handle 2 million optimization variables within one hour. Experiments are shown for both linear (Mahalanobis) metric learning and for kernel learning. The convergence rate, speed and performance of several loss functions and algorithms are discussed.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Li, Fuxin and Fu, Yunshan and Dai, Yu-Hong and Sminchisescu, Cristian and Wang, Jue},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1938690017},
 pages = {328--335},
 pdf = {http://proceedings.mlr.press/v5/li09a/li09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Kernel Learning by Unconstrained Optimization},
 url = {https://proceedings.mlr.press/v5/li09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-li09b,
 abstract = {One main concern towards kernel classifiers is on their sensitivity to the choice of kernel function or kernel matrix which characterizes the similarity between instances. Many realworld data, such as web pages and proteinprotein interaction data, are relational in nature in the sense that dierent instances are correlated (linked) with each other. The relational information available in such data often provides strong hints on the correlation (or similarity) between instances. In this paper, we propose a novel relational kernel learning model based on latent Wishart processes (LWP) to learn the kernel function for relational data. This is done by seamlessly integrating the relational information and the input attributes into the kernel learning process. Through extensive experiments on realworld applications, we demonstrate that our LWP model can give very promising performance in practice.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Li, Wu-Jun and Zhang, Zhihua and Yeung, Dit-Yan},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2172132049},
 pages = {336--343},
 pdf = {http://proceedings.mlr.press/v5/li09b/li09b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Latent Wishart Processes for Relational Kernel Learning},
 url = {https://proceedings.mlr.press/v5/li09b.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-li09c,
 abstract = {Maximum margin principle has been successfully applied to many supervised and semi-supervised problems in machine learning. Recently, this principle was extended for clustering, referred to as Maximum Margin Clustering (MMC) and achieved promising performance in recent studies. To avoid the problem of local minima, MMC can be solved globally via convex semi-definite programming (SDP) relaxation. Although many efficient approaches have been proposed to alleviate the computational burden of SDP, convex MMCs are still not scalable for medium data sets. In this paper, we propose a novel convex optimization method, LG-MMC, which maximizes the margin of opposite clusters via “Label Generation”. It can be shown that LG-MMC is much more scalable than existing convex approaches. Moreover, we show that our convex relaxation is tighter than state-of-art convex MMCs. Experiments on seventeen UCI datasets and MNIST dataset show significant improvement over existing MMC algorithms.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Li, Yu-Feng and Tsang, Ivor W. and Kwok, Jame and Zhou, Zhi-Hua},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W129176699},
 pages = {344--351},
 pdf = {http://proceedings.mlr.press/v5/li09c/li09c.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Tighter and convex maximum margin clustering},
 url = {https://proceedings.mlr.press/v5/li09c.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-li09d,
 abstract = {Options are important instruments in modern nance. In this paper, we investigate reinforcement learning (RL) methods| in particular, least-squares policy iteration (LSPI)|for the problem of learning exercise policies for American options. We develop nite-time bounds on the performance of the policy obtained with LSPI and compare LSPI and the tted Q-iteration algorithm (FQI) with the Longsta-Sc hwartz method (LSM), the standard least-squares Monte Carlo algorithm from the nance community. Our empirical results show that the exercise policies discovered by LSPI and FQI gain larger payos than those discovered by LSM, on both real and synthetic data. Furthermore, we nd that for all methods the policies learned from real data generally gain similar payos to the policies learned from simulated data. Our work shows that solution methods developed in machine learning can advance the state-of-the-art in an important and challenging application area, while demonstrating that computational nance remains a promising area for future applications of machine learning methods.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Li, Yuxi and Szepesvari, Csaba and Schuurmans, Dale},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2107333057},
 pages = {352--359},
 pdf = {http://proceedings.mlr.press/v5/li09d/li09d.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Exercise Policies for American Options},
 url = {https://proceedings.mlr.press/v5/li09d.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-lin09a,
 abstract = {Learning the sparse structure of a general Markov network is a hard computational problem. One of the main difficulties is the computation of the generally intractable partition function. To circumvent this difficulty, we propose to learn the network structure using an ensemble-oftrees (ET) model. The ET model was first introduced by Meila and Jaakkola (2006), and it represents a multivariate distribution using a mixture of all possible spanning trees. The advantage of the ET model is that, although it needs to sum over super-exponentially many trees, its partition function as well as data likelihood can be computed in a closed form. Furthermore, because the ET model tends to represent a Markov network using as small number of trees as possible, it provides a natural regularization for finding a sparse network structure. Our simulation results show that the proposed ET approach is able to accurately recover the true Markov network connectivity and outperform the state-of-art approaches for both discrete and continuous random variable networks when a small number of data samples is available. Furthermore, we also demonstrate the usage of the ET model for discovering the network of words from blog posts.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Lin, Yuanqing and Zhu, Shenghuo and Lee, Daniel and Taskar, Ben},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1555384764},
 pages = {360--367},
 pdf = {http://proceedings.mlr.press/v5/lin09a/lin09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Sparse Markov Network Structure via Ensemble-of-Trees Models},
 url = {https://proceedings.mlr.press/v5/lin09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-lippert09a,
 abstract = {Network inference is the problem of inferring edges between a set of real-world objects, for instance, interactions between pairs of proteins in bioinformatics. Current kernelbased approaches to this problem share a set of common features: (i) they are supervised and hence require labeled training data; (ii) edges in the network are treated as mutually independent and hence topological properties are largely ignored; (iii) they lack a statistical interpretation. We argue that these common assumptions are often undesirable for network inference, and propose (i) an unsupervised kernel method (ii) that takes the global structure of the network into account and (iii) is statistically motivated. We show that our approach can explain commonly used heuristics in statistical terms. In experiments on social networks, dierent variants of our method demonstrate appealing predictive performance.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Lippert, Christoph and Stegle, Oliver and Ghahramani, Zoubin and Borgwardt, Karsten},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2145404290},
 pages = {368--375},
 pdf = {http://proceedings.mlr.press/v5/lippert09a/lippert09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A kernel method for unsupervised structured network inference},
 url = {https://proceedings.mlr.press/v5/lippert09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-liu09a,
 abstract = {We extend the `2-consistency result of (Meinshausen and Yu 2008) from the Lasso to the group Lasso. Our main theorem shows that the group Lasso achieves estimation consistency under a mild condition and an asymptotic upper bound on the number of selected variables can be obtained. As a result, we can apply the nonnegative garrote procedure to the group Lasso result to obtain an estimator which is simultaneously estimation and variable selection consistent. In particular, our setting allows both the number of groups and the number of variables per group increase and thus is applicable to high-dimensional problems. We also provide estimation consistency analysis for a version of the sparse additive models with increasing dimensions. Some finite-sample results are also reported.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Liu, Han and Zhang, Jian},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W166844906},
 pages = {376--383},
 pdf = {http://proceedings.mlr.press/v5/liu09a/liu09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Estimation Consistency of the Group Lasso and its Applications},
 url = {https://proceedings.mlr.press/v5/liu09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-maaten09a,
 abstract = {The paper presents a new unsupervised dimensionality reduction technique, called parametric t-SNE, that learns a parametric mapping between the high-dimensional data space and the low-dimensional latent space. Parametric t-SNE learns the parametric mapping in such a way that the local structure of the data is preserved as well as possible in the latent space. We evaluate the performance of parametric t-SNE in experiments on three datasets, in which we compare it to the performance of two other unsupervised parametric dimensionality reduction techniques. The results of experiments illustrate the strong performance of parametric t-SNE, in particular, in learning settings in which the dimensionality of the latent space is relatively low.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {van der Maaten, Laurens},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2116516955},
 pages = {384--391},
 pdf = {http://proceedings.mlr.press/v5/maaten09a/maaten09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning a Parametric Embedding by Preserving Local Structure},
 url = {https://proceedings.mlr.press/v5/maaten09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-mandhani09a,
 abstract = {We consider the problem of learning the Generalized Mallows (GM) model of [Fligner and Verducci, 1986], which represents a probability distribution over all possible permutations (or rankings) of a given set of objects. The training data consists of a set of permutations. This problem generalizes the well known rank aggregation problem. Maximum Likelihood estimation of the GM model is NP-hard. An exact but inefficient searchbased method was recently proposed for this problem. Here we introduce the first nontrivial heuristic function for this search. We justify it theoretically, and show why it is admissible in practice. We experimentally demonstrate its effectiveness, and show that it is superior to existing techniques for learning the GM model. We also show good performance of a family of faster approximate methods of search.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Mandhani, Bhushan and Meila, Marina},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2135036692},
 pages = {392--399},
 pdf = {http://proceedings.mlr.press/v5/mandhani09a/mandhani09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Tractable Search for Learning Exponential Models of Rankings},
 url = {https://proceedings.mlr.press/v5/mandhani09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-mansinghka09a,
 abstract = {We introduce adaptive sequential rejection sampling, an algorithm for generating exact samples from high-dimensional, discrete distributions, building on ideas from classical AI search. Just as systematic search algorithms like A* recursively build complete solutions from partial solutions, sequential rejection sampling recursively builds exact samples over high-dimensional spaces from exact samples over lower-dimensional subspaces. Our algorithm recovers widely-used particle filters as an approximate variant without adaptation, and a randomized version of depth first search with backtracking when applied to deterministic problems. In this paper, we present the mathematical and algorithmic underpinnings of our approach and measure its behavior on undirected and directed graphical models, obtaining exact and approximate samples in a range of situations.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Mansinghka, Vikash and Roy, Daniel and Jonas, Eric and Tenenbaum, Joshua},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2134771075},
 pages = {400--407},
 pdf = {http://proceedings.mlr.press/v5/mansinghka09a/mansinghka09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Exact and approximate sampling by systematic stochastic search},
 url = {https://proceedings.mlr.press/v5/mansinghka09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-pletscher09a,
 abstract = {In this work we show that one can train Conditional Random Fields of intractable graphs eectively and eciently by considering a mixture of random spanning trees of the underlying graphical model. Furthermore, we show how a maximum-likelihood estimator of such a training objective can subsequently be used for prediction on the full graph. We present experimental results which improve on the state-of-the-art. Additionally, the training objective is less sensitive to the regularization than pseudo-likelihood based training approaches. We perform the experimental validation on two classes of data sets where structure is important: image denoising and multilabel classication.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Pletscher, Patrick and Ong, Cheng Soon and Buhmann, Joachim},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2154848136},
 pages = {408--415},
 pdf = {http://proceedings.mlr.press/v5/pletscher09a/pletscher09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Spanning Tree Approximations for Conditional Random Fields},
 url = {https://proceedings.mlr.press/v5/pletscher09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-ralaivola09a,
 abstract = {PAC-Bayes bounds are among the most accurate generalization bounds for classifiers learned with IID data, and it is particularly so for margin classifiers. However, there are many practical cases where the training data show some dependencies and where the traditional IID assumption does not apply. Stating generalization bounds for such frameworks is therefore of the utmost interest, both from theoretical and practical standpoints. In this work, we propose the first ‐ to the best of our knowledge ‐ PAC-Bayes generalization bounds for classifiers trained on data exhibiting interdependencies. The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data, in sets of independent data, through the tool of graph fractional covers. Our bounds are very general, since being able to find an upper bound on the (fractional) chromatic number of the dependency graph is sufficient to get new PAC-Bayes bounds for specific settings. We show how our results can be used to derive bounds for bipartite ranking and windowed prediction on sequential data.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Ralaivola, Liva and Szafranski, Marie and Stempfel, Guillaume},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2148611466},
 pages = {416--423},
 pdf = {http://proceedings.mlr.press/v5/ralaivola09a/ralaivola09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Chromatic PAC-Bayes Bounds for Non-IID Data},
 url = {https://proceedings.mlr.press/v5/ralaivola09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-ratliff09a,
 abstract = {One common approach to imitation learning is behavioral cloning (BC), which employs straightforward supervised learning (i.e., classification) to directly map observations to controls. A second approach is inverse optimal control (IOC), which formalizes the problem of learning sequential decision-making behavior over long horizons as a problem of recovering a utility function that explains observed behavior. This paper presents inverse optimal heuristic control (IOHC), a novel approach to imitation learning that capitalizes on the strengths of both paradigms. It employs long-horizon IOC-style modeling in a low-dimensional space where inference remains tractable, while incorporating an additional descriptive set of BC-style features to guide a higher-dimensional overall action selection. We provide experimental results demonstrating the capabilities of our model on a simple illustrative problem as well as on two real world problems: turn-prediction for taxi drivers, and pedestrian prediction within an office environment.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Ratliff, Nathan and Ziebart, Brian and Peterson, Kevin and Bagnell, J. Andrew and Hebert, Martial and Dey, Anind K. and Srinivasa, Siddhartha},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2530285197},
 pages = {424--431},
 pdf = {http://proceedings.mlr.press/v5/ratliff09a/ratliff09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Inverse optimal heuristic control for imitation learning},
 url = {https://proceedings.mlr.press/v5/ratliff09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-rooij09a,
 abstract = {The expert tracking algorithm Fixed-Share depends on a parameter �, called the switching rate. The switching rate can be learned online with regret 1 logT + O(1) bits. The current fastest method to achieve this is based on optimal discretisation of the Bernoulli distributions into O( √ T) bins and runs in O(T √ T) time. However, the exact locations of these bins have to be determined algorithmically, and the final number of outcomes T must be known in advance. This paper introduces a new discretisation scheme with the same regret bound for known T, that specifies the number and positions of the discretisation points explicitly. The scheme is especially useful, however, when T is not known in advance: a new fully online algorithm is presented, which runs in O(T √ T logT) time and achieves a regret of 1 2 log3logT +O(loglogT) bits.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Rooij, Steven and Erven, Tim},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2163017424},
 pages = {432--439},
 pdf = {http://proceedings.mlr.press/v5/rooij09a/rooij09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning the Switching Rate by Discretising Bernoulli Sources Online},
 url = {https://proceedings.mlr.press/v5/rooij09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-roth09a,
 abstract = {Many classification problems with structured outputs can be regarded as a set of interrelated sub-problems where constraints dictate valid variable assignments. The standard approaches to these problems include either independent learning of individual classifiers for each of the sub-problems or joint learning of the entire set of classifiers with the constraints enforced during learning. We propose an intermediate approach where we learn these classifiers in a sequence using previously learned classifiers to guide learning of the next classifier by enforcing constraints between their outputs. We provide a theoretical motivation to explain why this learning protocol is expected to outperform both alternatives when individual problems have different ‘complexity’. This analysis motivates an algorithm for choosing a preferred order of classifier learning. We evaluate our technique on artificial experiments and on the entity and relation identification problem where the proposed method outperforms both joint and independent learning.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Roth, Dan and Small, Kevin and Titov, Ivan},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1902270342},
 pages = {440--447},
 pdf = {http://proceedings.mlr.press/v5/roth09a/roth09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sequential learning of classifiers for structured prediction problems},
 url = {https://proceedings.mlr.press/v5/roth09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-salakhutdinov09a,
 abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W189596042},
 pages = {448--455},
 pdf = {http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Deep Boltzmann machines},
 url = {https://proceedings.mlr.press/v5/salakhutdinov09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-schmidt09a,
 abstract = {An optimization algorithm for minimizing a smooth function over a convex set is described. Each iteration of the method computes a descent direction by minimizing, over the original constraints, a diagonal plus lowrank quadratic approximation to the function. The quadratic approximation is constructed using a limited-memory quasi-Newton update. The method is suitable for large-scale problems where evaluation of the function is substantially more expensive than projection onto the constraint set. Numerical experiments on one-norm regularized test problems indicate that the proposed method is competitive with state-of-the-art methods such as boundconstrained L-BFGS and orthant-wise descent. We further show that the method generalizes to a wide class of problems, and substantially improves on state-of-the-art methods for problems such as learning the structure of Gaussian graphical models and Markov random elds.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Schmidt, Mark and Berg, Ewout and Friedlander, Michael and Murphy, Kevin},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2118163005},
 pages = {456--463},
 pdf = {http://proceedings.mlr.press/v5/schmidt09a/schmidt09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm},
 url = {https://proceedings.mlr.press/v5/schmidt09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-scott09a,
 abstract = {In machine learning, one formulation of the novelty detection problem is to build a detector based on a training sample consisting of only nominal data. The standard (inductive) approach to this problem has been to declare novelties where the nominal density is low, which reduces the problem to density level set estimation. In this talk we consider the setting where an unlabeled and possibly contaminated sample is also available at learning time. We argue that novelty detection in this semi-supervised setting is naturally solved by a general reduction to a binary classification problem. In particular, a detector with a desired false positive rate can be achieved through a reduction to Neyman-Pearson classification. Unlike the inductive approach, our approach yields detectors that are optimal (e.g., statistically consistent) regardless of the distribution on novelties. Therefore, in novelty detection, unlabeled data have a substantial impact on the theoretical properties of the decision rule.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Scott, Clayton and Blanchard, Gilles},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2101571195},
 pages = {464--471},
 pdf = {http://proceedings.mlr.press/v5/scott09a/scott09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Novelty detection: Unlabeled data definitely help},
 url = {https://proceedings.mlr.press/v5/scott09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-seldin09a,
 abstract = {We derive a PAC-Bayesian generalization bound for density estimation. Similar to the PAC-Bayesian generalization bound for classication, the result has the appealingly simple form of a tradeo between empirical performance and the KL-divergence of the posterior from the prior. Moreover, the PACBayesian generalization bound for classication can be derived as a special case of the bound for density estimation. To illustrate a possible application of our bound we derive a generalization bound for co-clustering. The bound provides a criterion to evaluate the ability of co-clustering to predict new co-occurrences, thus introducing the notion of generalization to this traditionally unsupervised task.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Seldin, Yevgeny and Tishby, Naftali},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2098085882},
 pages = {472--479},
 pdf = {http://proceedings.mlr.press/v5/seldin09a/seldin09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {PAC-Bayesian Generalization Bound for Density Estimation with Application to Co-clustering},
 url = {https://proceedings.mlr.press/v5/seldin09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-shawe-taylor09a,
 abstract = {We extend and apply the PAC-Bayes theorem to the analysis of maximum entropy learning by considering maximum entropy classification. The theory introduces a multiple sampling technique that controls an effective margin of the bound. We further develop a dual implementation of the convex optimisation that optimises the bound. This algorithm is tested on some simple datasets and the value of the bound compared with the test error.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Shawe-Taylor, John and Hardoon, David},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1953521701},
 pages = {480--487},
 pdf = {http://proceedings.mlr.press/v5/shawe-taylor09a/shawe-taylor09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {PAC-Bayes Analysis Of Maximum Entropy Classification},
 url = {https://proceedings.mlr.press/v5/shawe-taylor09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-shervashidze09a,
 abstract = {State-of-the-art graph kernels do not scale to large graphs with hundreds of nodes and thousands of edges. In this article we propose to compare graphs by counting graphlets, i.e., subgraphs with k nodes where k ∈ {3, 4, 5}. Exhaustive enumeration of all graphlets being prohibitively expensive, we introduce two theoretically grounded speedup schemes, one based on sampling and the second one specifically designed for bounded degree graphs. In our experimental evaluation, our novel kernels allow us to efficiently compare large graphs that cannot be tackled by existing graph kernels.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Shervashidze, Nino and Vishwanathan, SVN and Petri, Tobias and Mehlhorn, Kurt and Borgwardt, Karsten},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2159156271},
 pages = {488--495},
 pdf = {http://proceedings.mlr.press/v5/shervashidze09a/shervashidze09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Efficient Graphlet Kernels for Large Graph Comparison},
 url = {https://proceedings.mlr.press/v5/shervashidze09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-shi09a,
 abstract = {We propose hashing to facilitate efficient kernels. This generalizes    previous work using sampling and we show a principled way to compute    the kernel matrix for data streams and sparse feature    spaces. Moreover, we give deviation bounds from the exact kernel    matrix. This has applications to estimation on strings and graphs.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Shi, Qinfeng and Petterson, James and Dror, Gideon and Langford, John and Smola, Alex and Strehl, Alex and Vishwanathan, S. V. N.},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 pages = {496--503},
 pdf = {http://proceedings.mlr.press/v5/shi09a/shi09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Hash Kernels},
 url = {https://proceedings.mlr.press/v5/shi09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-silander09a,
 abstract = {We propose an information-theoretic approach for predictive modeling with Bayesian networks. Our approach is based on the minimax optimal Normalized Maximum Likelihood (NML) distribution, motivated by the MDL principle. In particular, we present a parameter learning method which, together with a previously introduced NML-based model selection criterion, provides a way to construct highly predictive Bayesian network models from data. The method is parameterfree and robust, unlike the currently popular Bayesian marginal likelihood approach which has been shown to be sensitive to the choice of prior hyperparameters. Empirical tests show that the proposed method compares favorably with the Bayesian approach in predictive tasks.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Silander, Tomi and Roos, Teemu and MyllymÃ¤ki, Petri},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2113525614},
 pages = {504--511},
 pdf = {http://proceedings.mlr.press/v5/silander09a/silander09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Locally Minimax Optimal Predictive Modeling with Bayesian Networks},
 url = {https://proceedings.mlr.press/v5/silander09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-silva09a,
 abstract = {Applications of copula models have been increasing in number in recent years. This class of models provides a modular parameterization of joint distributions: the specification of the marginal distributions is parameterized separately from the dependence structure of the joint, a convenient way of encoding a model for domains such as finance. Some recent advances on how to specify copulas for arbitrary dimensions have been proposed, by means of mixtures of decomposable graphical models. This paper introduces a Bayesian approach for dealing with mixtures of copulas which, due to the lack of prior conjugacy, raise computational challenges. We motivate and present families of Markov chain Monte Carlo (MCMC) proposals that exploit the particular structure of mixtures of copulas. Different algorithms are evaluated according to their mixing properties, and an application in financial forecasting with missing data illustrates the usefulness of the methodology.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Silva, Ricardo and Gramacy, Robert},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2126415516},
 pages = {512--519},
 pdf = {http://proceedings.mlr.press/v5/silva09a/silva09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {MCMC Methods for Bayesian Mixtures of Copulas},
 url = {https://proceedings.mlr.press/v5/silva09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-silva09b,
 abstract = {Marginal independence constraints play an important role in learning with graphical models. One way of parameterizing a model of marginal independencies is by building a latent variable model where two independent observed variables have no common latent source. In sparse domains, however, it might be advantageous to model the marginal observed distribution directly, without explicitly including latent variables in the model. There have been recent advances in Gaussian and binary models of marginal independence, but no models with non-linear dependencies between continuous variables has been proposed so far. In this paper, we describe how to generalize the Gaussian model of marginal independencies based on mixtures, and how to learn parameters. This requires a nonstandard parameterization and raises difficult non-linear optimization issues.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Silva, Ricardo and Ghahramani, Zoubin},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2144845984},
 pages = {520--527},
 pdf = {http://proceedings.mlr.press/v5/silva09b/silva09b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Factorial mixture of gaussians and the marginal independence model},
 url = {https://proceedings.mlr.press/v5/silva09b.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-siracusa09a,
 abstract = {We consider the problem of Bayesian inference of graphical structure describing the interactions among multiple vector time-series. A directed temporal interaction model is presented which assumes a fixed dependence structure among time-series. Using a conjugate prior over this model’s structure and parameters, we focus our attention on characterizing the exact posterior uncertainty in the structure given data. The model is extended via the introduction of a dynamically evolving latent variable which indexes dependence structures over time. Performing inference using this model yields promising results when analyzing the interaction of multiple tracked moving objects.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Siracusa, Michael and III, John Fisher},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2100241850},
 pages = {528--535},
 pdf = {http://proceedings.mlr.press/v5/siracusa09a/siracusa09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Tractable Bayesian Inference of Time-Series Dependence Structure},
 url = {https://proceedings.mlr.press/v5/siracusa09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-smola09a,
 abstract = {Novelty detection is an important tool for unsupervised data analysis. It relies on nding regions of low density within which events are then agged as novel. By design this is dependent on the underlying measure of the space. In this paper we derive a formulation which is able to address this problem by allowing for a reference measure to be given in the form of a sample from an alternative distribution. We show that this optimization problem can be solved eciently and that it works well in practice.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Smola, Alex and Song, Le and Teo, Choon Hui},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2131584489},
 pages = {536--543},
 pdf = {http://proceedings.mlr.press/v5/smola09a/smola09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Relative Novelty Detection},
 url = {https://proceedings.mlr.press/v5/smola09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-sontag09a,
 abstract = {A number of linear programming relaxations have been proposed for finding most likely settings of the variables (MAP) in large probabilistic models. The relaxations are often succinctly expressed in the dual and reduce to dierent types of reparameterizations of the original model. The dual objectives are typically solved by performing local block coordinate descent steps. In this work, we show how to perform block coordinate descent on spanning trees of the graphical model. We also show how all of the earlier dual algorithms are related to each other, giving transformations from one type of reparameterization to another while maintaining monotonicity relative to a common objective function. Finally, we quantify when the MAP solution can and cannot be decoded directly from the dual LP relaxation.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Sontag, David and Jaakkola, Tommi},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2158863086},
 pages = {544--551},
 pdf = {http://proceedings.mlr.press/v5/sontag09a/sontag09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Tree block coordinate descent for map in graphical models},
 url = {https://proceedings.mlr.press/v5/sontag09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-stepleton09a,
 abstract = {The Infinite Hidden Markov Model (IHMM) extends hidden Markov models to have a countably infinite number of hidden states (Beal et al., 2002; Teh et al., 2006). We present a generalization of this framework that introduces nearly block-diagonal structure in the transitions between the hidden states, where blocks correspond to “subbehaviors” exhibited by data sequences. In identifying such structure, the model classifies, or partitions, sequence data according to these sub-behaviors in an unsupervised way. We present an application of this model to artificial data, a video gesture classification task, and a musical theme labeling task, and show that components of the model can also be applied to graph segmentation.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Stepleton, Thomas and Ghahramani, Zoubin and Gordon, Geoffrey and Lee, Tai-Sing},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1577552086},
 pages = {552--559},
 pdf = {http://proceedings.mlr.press/v5/stepleton09a/stepleton09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The Block Diagonal Infinite Hidden Markov Model},
 url = {https://proceedings.mlr.press/v5/stepleton09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-sunehag09a,
 abstract = {We provide a variable metric stochastic approximation theory. In doing so, we provide a convergence theory for a large class of online variable metric methods including the recently introduced online versions of the BFGS algorithm and its limited-memory LBFGS variant. We also discuss the implications of our results for learning from expert advice.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Sunehag, Peter and Trumpf, Jochen and Vishwanathan, S.V.N. and Schraudolph, Nicol},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2107486802},
 pages = {560--566},
 pdf = {http://proceedings.mlr.press/v5/sunehag09a/sunehag09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variable Metric Stochastic Approximation Theory},
 url = {https://proceedings.mlr.press/v5/sunehag09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-titsias09a,
 abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Titsias, Michalis},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W137285897},
 pages = {567--574},
 pdf = {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
 url = {https://proceedings.mlr.press/v5/titsias09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-wang09a,
 abstract = {The contributions of this paper are three-fold. First, we present a general formulation for reaping the benefits from both non-negative data factorization and semi-supervised learning, and the solution naturally possesses the characteristics of sparsity, robustness to partial occlusions, and greater discriminating power via extra unlabeled data. Then, an efficient multiplicative updating procedure is proposed along with its theoretic justification of the algorithmic convergency. Finally, the tensorization of this general formulation for non-negative semi-supervised learning is also briefed for handling tensor data of arbitrary order. Extensive experiments compared with the state-of-the-art algorithms for non-negative data factorization and semi-supervised learning demonstrate the algorithmic properties in sparsity, classification power, and robustness to image occlusions.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Wang, Changhu and Yan, Shuicheng and Zhang, Lei and Zhang, Hongjiang},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W1519867057},
 pages = {575--582},
 pdf = {http://proceedings.mlr.press/v5/wang09a/wang09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Non-Negative Semi-Supervised Learning},
 url = {https://proceedings.mlr.press/v5/wang09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-wang09b,
 abstract = {We develop Markov topic models (MTMs), a novel family of generative probabilistic models that can learn topics simultaneously from multiple corpora, such as papers from different conferences. We apply Gaussian (Markov) random fields to model the correlations of different corpora. MTMs capture both the internal topic structure within each corpus and the relationships between topics across the corpora. We derive an efficient estimation procedure with variational expectation-maximization. We study the performance of our models on a corpus of abstracts from six different computer science conferences. Our analysis reveals qualitative discoveries that are not possible with traditional topic models, and improved quantitative performance over the state of the art.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Wang, Chong and Thiesson, Bo and Meek, Chris and Blei, David},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W153090119},
 pages = {583--590},
 pdf = {http://proceedings.mlr.press/v5/wang09b/wang09b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Markov Topic Models},
 url = {https://proceedings.mlr.press/v5/wang09b.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-wang09c,
 abstract = {In this paper, we propose a framework for metric learning based on information geometry. The key idea is to construct two kernel matrices for the given training data: one is based on the distance metric and the other is based on the assigned class labels. Inspired by the idea of information geometry, we relate these two kernel matrices to two Gaussian distributions, and the difference between the two kernel matrices is then computed by the Kullback-Leibler (KL) divergence between the two Gaussian distributions. The optimal distance metric is then found by minimizing the divergence between the two distributions. We present two metric learning algorithms, one for linear distance metric and one for nonlinear distance using a kernel function. Unlike many existing algorithms for metric learning that require solving a nontrivial optimization problem and are computationally expensive when the data dimension is high, the proposed algorithms have a closed-form solution and are computationally more efficient. Extensive experiments with data classification and face recognition show that the proposed algorithms are comparable to or better than the state-of-the-art algorithms for metric learning.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Wang, Shijun and Jin, Rong},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W158095270},
 pages = {591--598},
 pdf = {http://proceedings.mlr.press/v5/wang09c/wang09c.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An Information Geometry Approach for Distance Metric Learning},
 url = {https://proceedings.mlr.press/v5/wang09c.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-wang09d,
 abstract = {This paper presents a novel learning algorithm for structured classification, where the task is to predict multiple and interacting labels (multilabel) for an input object. The problem of finding a large-margin separation between correct multilabels and incorrect ones is formulated as a linear program. Instead of explicitly writing out the entire problem with an exponentially large constraint set, the linear program is solved iteratively via column generation. In this case, the process of generating most violated constraints is equivalent to searching for highest-scored misclassified incorrect multilabels, which can be easily achieved by decoding the structure based on current estimations. In addition, we also explore the integration of column generation and an extragradient method for linear programming to gain further efficiency. The proposed method has the advantages that it can handle arbitrary structures and larger-scale problems. Experimental results on part-of-speech tagging and statistical machine translation tasks are reported, demonstrating the competitiveness of our approach.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Wang, Zhuoran and Shawe-Taylor, John},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2142557384},
 pages = {599--606},
 pdf = {http://proceedings.mlr.press/v5/wang09d/wang09d.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Large-margin structured prediction via linear programming},
 url = {https://proceedings.mlr.press/v5/wang09d.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-wood09a,
 abstract = {In this paper we present a doubly hierarchical Pitman-Yor process language model. Its bottom layer of hierarchy consists of multiple hierarchical Pitman-Yor process language models, one each for some number of domains. The novel top layer of hierarchy consists of a mechanism to couple together multiple language models such that they share statistical strength. Intuitively this sharing results in the “adaptation” of a latent shared language model to each domain. We introduce a general formalism capable of describing the overall model which we call the graphical Pitman-Yor process and explain how to perform Bayesian inference in it. We present encouraging language model domain adaptation results that both illustrate the potential benefits of our new model and suggest new avenues of inquiry.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Wood, Frank and Teh, Yee Whye},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2103731025},
 pages = {607--614},
 pdf = {http://proceedings.mlr.press/v5/wood09a/wood09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation.},
 url = {https://proceedings.mlr.press/v5/wood09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-xi09a,
 abstract = {Boosting algorithms withl1-regularization are of interest becausel1 regularization leads to sparser composite classifiers. Moreover, Rosset et al. have shown that for separable data, standard lpregularized loss minimization results in a margin maximizing classifier in the limit regularization is relaxed. For the case p = 1, we extend these results by obtaining explicit convergence bounds on the regularization required to yield a margin within prescribed accuracy of the maximum achievable margin. We derive similar rates of convergence for the-AdaBoost algorithm, in the process providing a new proof that -AdaBoost is margin maximizing as converges to 0. Because both of these known algorithms are computationally expensive, we introduce a new hybrid algorithm, AdaBoost+L1, that combines the virtues of AdaBoost with the sparsity of l1regularization in a computationally efficient fashion. We prove that the algorithm is margin maximizing and empirically examine its performance on five datasets.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Xi, Yongxin and Xiang, Zhen and Ramadge, Peter and Schapire, Robert},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2144994347},
 pages = {615--622},
 pdf = {http://proceedings.mlr.press/v5/xi09a/xi09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Speed and Sparsity of Regularized Boosting},
 url = {https://proceedings.mlr.press/v5/xi09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-xu09a,
 abstract = {The Dirichlet process mixture (DPM) is a widely used model for clustering and for general nonparametric Bayesian density estimation. Unfortunately, like in many statistical models, exact inference in a DPM is intractable, and approximate methods are needed to perform efficient inference. While most attention in the literature has been placed on Markov chain Monte Carlo (MCMC) [1, 2, 3], variational Bayesian (VB) [4] and collapsed variational methods [5], [6] recently introduced a novel class of approximation for DPMs based on Bayesian hierarchical clustering (BHC). These tree-based combinatorial approximations efficiently sum over exponentially many ways of partitioning the data and offer a novel lower bound on the marginal likelihood of the DPM [6]. In this paper we make the following contributions: (1) We show empirically that the BHC lower bounds are substantially tighter than the bounds given by VB [4] and by collapsed variational methods [5] on synthetic and real datasets. (2) We also show that BHC offers a more accurate predictive performance on these datasets. (3) We further improve the tree-based lower bounds with an algorithm that efficiently sums contributions from alternative trees. (4) We present a fast approximate method for BHC. Our results suggest that our combinatorial approximate inference methods and lower bounds may be useful not only in DPMs but in other models as well.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Xu, Yang and Heller, Katherine and Ghahramani, Zoubin},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2119680497},
 pages = {623--630},
 pdf = {http://proceedings.mlr.press/v5/xu09a/xu09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Tree-Based Inference for Dirichlet Process Mixtures},
 url = {https://proceedings.mlr.press/v5/xu09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-yang09a,
 abstract = {Recently, researchers have investigated novel dual representations as a basis for dynamic programming and reinforcement learning algorithms. Although the convergence properties of classical dynamic programming algorithms have been established for dual representations, temporal dierence learning algorithms have not yet been analyzed. In this paper, we study the convergence properties of temporal dierence learning using dual representations. We make signican t progress by proving the convergence of dual temporal dierence learning with eligibility traces. Experimental results suggest that the dual algorithms seem to demonstrate empirical benets over standard primal algorithms.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Yang, Min and Li, Yuxi and Schuurmans, Dale},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2162664081},
 pages = {631--638},
 pdf = {http://proceedings.mlr.press/v5/yang09a/yang09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Dual Temporal Difference Learning},
 url = {https://proceedings.mlr.press/v5/yang09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-yu09a,
 abstract = {Labels are often expensive to get, and this motivates active learning which chooses the most informative samples for label  acquisition. In this paper we study active sensing in a multi-view setting, motivated from many problems where grouped features are also expensive to obtain and need to be acquired (or sensed) actively (e.g., in cancer diagnosis each patient might  go through many tests such as CT, Ultrasound and MRI to get valuable  features). The strength of this model is that one actively sensed (sample, view) pair would improve the joint multi-view  classification on all the samples. For this purpose we extend the  Bayesian co-training framework such that it can handle missing views  in a principled way, and introduce two criteria for view acquisition.  Experiments on one toy data and two real-world medical problems show  the effectiveness of this model.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Yu, Shipeng and Krishnapuram, Balaji and Rosales, Romer and Rao, R. Bharat},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 pages = {639--646},
 pdf = {http://proceedings.mlr.press/v5/yu09a/yu09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Active Sensing},
 url = {https://proceedings.mlr.press/v5/yu09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-zhang09a,
 abstract = {Margin-based classification methods are typically devised based on a majorizationminimization procedure, which approximately solves an otherwise intractable minimization problem defined with the 0-l loss. The extension of such methods from the binary classification setting to the more general multicategory setting turns out to be nontrivial. In this paper, our focus is to devise margin-based classification methods that can be seamlessly applied to both settings, with the binary setting simply as a special case. In particular, we propose a new majorization loss function that we call the coherence function, and then devise a new multicategory margin-based boosting algorithm based on the coherence function. Analogous to deterministic annealing, the coherence function is characterized by a temperature factor. It is closely related to the multinomial log-likelihood function and its limit at zero temperature corresponds to a multicategory hinge loss function.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Zhang, Zhihua and Jordan, Michael and Li, Wu-Jun and Yeung, Dit-Yan},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2128613790},
 pages = {647--654},
 pdf = {http://proceedings.mlr.press/v5/zhang09a/zhang09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Coherence Functions for Multicategory Margin-based Classification Methods},
 url = {https://proceedings.mlr.press/v5/zhang09a.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-zhang09b,
 abstract = {The Gaussian process latent variable model (GP-LVM) has been identified to be an effective probabilistic approach for dimensionality reduction because it can obtain a low-dimensional manifold of a data set in an unsupervised fashion. Consequently, the GP-LVM is insufficient for supervised learning tasks (e.g., classification and regression) because it ignores the class label information for dimensionality reduction. In this paper, a supervised GP-LVM is developed for supervised learning tasks, and the maximum a posteriori algorithm is introduced to estimate positions of all samples in the latent variable space. We present experimental evidences suggesting that the supervised GP-LVM is able to use the class label information effectively, and thus, it outperforms the GP-LVM and the discriminative extension of the GP-LVM consistently. The comparison with some supervised classification methods, such as Gaussian process classification and support vector machines, is also given to illustrate the advantage of the proposed method.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Zhang, Zhihua and Jordan, Michael I.},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2108193616},
 pages = {655--662},
 pdf = {http://proceedings.mlr.press/v5/zhang09b/zhang09b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Supervised Gaussian Process Latent Variable Model for Dimensionality Reduction},
 url = {https://proceedings.mlr.press/v5/zhang09b.html},
 volume = {5},
 year = {2009}
}

@inproceedings{pmlr-v5-zhong09a,
 abstract = {We present a fully Bayesian approach to NonNegative Matrix Factorisation (NMF) by developing a Reversible Jump Markov Chain Monte Carlo (RJMCMC) method which provides full posteriors over the matrix components. In addition the NMF model selection issue is addressed, for the first time, as our RJMCMC procedure provides the posterior distribution over the matrix dimensions and therefore the number of components in the NMF model. A comparative analysis is provided with the Bayesian Information Criterion (BIC) and model selection employing estimates of the marginal likelihood. An illustrative synthetic example is provided using blind mixtures of images. This is then followed by a large scale study of the recovery of component spectra from multiplexed Raman readouts. The power and flexibility of the Bayesian methodology and the proposed RJMCMC procedure to objectively assess differing model structures and infer the corresponding plausible component spectra for this complex data is demonstrated convincingly.},
 address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
 author = {Zhong, Mingjun and Girolami, Mark},
 booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
 editor = {van Dyk, David and Welling, Max},
 month = {16--18 Apr},
 openalex = {W2104688968},
 pages = {663--670},
 pdf = {http://proceedings.mlr.press/v5/zhong09a/zhong09a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Reversible Jump MCMC for Non-Negative Matrix Factorization},
 url = {https://proceedings.mlr.press/v5/zhong09a.html},
 volume = {5},
 year = {2009}
}
