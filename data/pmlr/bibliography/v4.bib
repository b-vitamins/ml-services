


@Proceedings{FSDM2008,
  title =     {Proceedings of the Workshop on New Challenges for Feature Selection in Data Mining and Knowledge Discovery at ECML/PKDD 2008},
  booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data Mining and Knowledge Discovery at ECML/PKDD 2008},
  editor =    {Yvan Saeys and Huan Liu and IÃ±aki Inza and Louis Wehenkel and Yves Van de Pee},
  publisher = {PMLR},
  series =    {Proceedings of Machine Learning Research},
  volume =    4
}




@InProceedings{pmlr-v4-saeys08a,
  title = 	 {Preface},
  author = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Van de Peer, Yves},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {1--4},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/saeys08a/saeys08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/saeys08a.html},
  abstract = 	 {Welcome to FSDM'08}
}




@InProceedings{pmlr-v4-suzuki08a,
  title = 	 {Approximating Mutual Information by Maximum Likelihood Density Ratio Estimation},
  author = 	 {Suzuki, Taiji and Sugiyama, Masashi and Sese, Jun and Kanamori, Takafumi},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {5--20},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/suzuki08a/suzuki08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/suzuki08a.html},
  abstract = 	 {Mutual information is useful in various data processing tasks such as feature selection or independent component analysis. In this paper, we propose a new method of approximating mutual information based on maximum likelihood estimation of a density ratio function. Our method, called Maximum Likelihood Mutual Information (MLMI), has several attractive properties, e.g., density estimation is not involved, it is a single-shot procedure, the global optimal solution can be efficiently computed, and cross-validation is available for model selection. Numerical experiments show that MLMI compares favorably with existing methods.}
}




@InProceedings{pmlr-v4-lee08a,
  title = 	 {Quality assessment of nonlinear dimensionality reduction based on K-ary neighborhoods},
  author = 	 {Lee, John and Verleysen, Michel},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {21--35},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/lee08a/lee08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/lee08a.html},
  abstract = 	 {Nonlinear dimensionality reduction aims at providing low-dimensional representions of high-dimensional data sets. Many new methods have been recently proposed, but the question of their assessment and comparison remains open. This paper reviews some of the existing quality measures that are based on distance ranking and K-ary neighborhoods. In this context, the comparison of the ranks in the high- and low-dimensional spaces leads to the definition of the co-ranking matrix. Rank errors and concepts such as neighborhood intrusions and extrusions can be associated with different blocks of the co-ranking matrix. The considered quality criteria are then cast within this unifying framework and the blocks they involve are identified. The same framework allows us to propose simpler criteria, which quantify two aspects of the embedding, namely its overall quality and its tendency to favor either intrusions or extrusions. Eventually, a simple experiment illustrates the soundness of the approach.}
}




@InProceedings{pmlr-v4-zhao08a,
  title = 	 {Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis},
  author = 	 {Zhao, Zheng and Liu, Huan},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {36--47},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/zhao08a/zhao08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/zhao08a.html},
  abstract = 	 {Feature selection is an effective approach to reducing dimensionality by selecting relevant original features. In this work, we studied a novel problem of multi-source feature selection for unlabeled data: given multiple heterogeneous data sources (or data sets), select features from one source of interest by integrating information from various data sources. In essence, we investigate how we can employ the information contained in multiple data sources to effectively derive intrinsic relationships that can help select more meaningful (or domain relevant) features. We studied how to adjust the covariance matrix of a data set using the geometric structure obtained from multiple data sources, and how to select features of the target source using geometry-dependent covariance. We designed and conducted experiments to systematically compare the proposed approach with representative methods in our attempt to solve the novel problem of multi-source feature selection. The empirical study demonstrated the efficacy and potential of multi-source feature selection.}
}




@InProceedings{pmlr-v4-campedel08a,
  title = 	 {Unsupervised feature selection applied to SPOT5 satellite images indexing},
  author = 	 {Campedel, Marine and Kyrgyzov, Ivan and Maitre, Henri},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {48--59},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/campedel08a/campedel08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/campedel08a.html},
  abstract = 	 {Satellite images are numerous and weakly exploited: it is urgent to develop efficient and fast  indexing algorithms to facilitate their access. In order to determinate the best features to be  extracted, we propose a methodology based on automatic feature selection algorithms, applied  unsupervisingly on a strongly redundant features set. In this article we also demonstrate the  usefulness of consensus clustering as a feature selection algorithm, allowing selected number  of features estimation and exploration facilities. The efficiency of our approach is demonstrated on SPOT5 images.}
}




@InProceedings{pmlr-v4-huynhthu08a,
  title = 	 {Exploiting tree-based variable importances to selectively identify relevant variables},
  author = 	 {Huynh-Thu, VÃ¢n Anh and Wehenkel, Louis and Geurts, Pierre},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {60--73},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/huynhthu08a/huynhthu08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/huynhthu08a.html},
  abstract = 	 {This  paper   proposes  a  novel  statistical   procedure  based  on   permutation  tests  for  extracting   a  subset  of  truly  relevant   variables  from   multivariate  importance  rankings   derived  from   tree-based  supervised learning  methods.   It shows  also that  the   direct  extension of  the  classical approach  based on  permutation   tests for  estimating false  discovery rates of  univariate variable   scoring  procedures  does  not  extend  very well  to  the  case  of   multivariate  tree-based  importance  measures.}
}




@InProceedings{pmlr-v4-antal08a,
  title = 	 {A Bayesian View of Challenges in Feature Selection: Feature Aggregation, Multiple Targets, Redundancy and Interaction},
  author = 	 {Antal, Peter and Millinghoffer, Andras and HullÃ¡m, GÃ¡bor and Szalai, Csaba and Falus, AndrÃ¡s},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {74--89},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/antal08a/antal08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/antal08a.html},
  abstract = 	 {In the paper we discuss applications of the Bayesian approach to new challenges in relevance analysis. Earlier, we formulated a Bayesian approach to Feature Subset Selection using Bayesian networks to jointly estimate the posteriors of Markov Blanket Memberships (MBMs), Markov Blanket Sets (MBSs), and Markov Blanket Graphs (MBGs) for a given target variable. These results of the Bayesian Multilevel Analysis of relevance (BMLA) correspond respectively to a model-based pairwise relevance, relevance of sets, and to the interaction models of relevant variables. Now we formulate refined levels in BMLA by introducing the concepts of k-MBSs and k-MBGs, which are intermediate, scalable model properties expressing relevance. We consider the extension of BMLA to multiple targets. We introduce and investigate a score for feature redundancy and interaction based on the decomposability of the structure posterior. Finally, we overview the problems of conditional and contextual relevance. We demonstrate the use of concepts and methods in the field of genomics of asthma.}
}




@InProceedings{pmlr-v4-janecek08a,
  title = 	 {On the Relationship Between Feature Selection and Classification Accuracy},
  author = 	 {Janecek, Andreas and Gansterer, Wilfried and Demel, Michael and Ecker, Gerhard},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {90--105},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/janecek08a/janecek08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/janecek08a.html},
  abstract = 	 {Dimensionality reduction and feature subset selection are two techniques for reducing the attribute space of a feature set, which is an important component of both supervised and unsupervised classification or regression problems. While in feature subset selection a subset of the original attributes is extracted, dimensionality reduction in general produces linear combinations of the original attribute set. In this paper we investigate the relationship between several attribute space reduction techniques and the resulting classification accuracy for two very different application areas. On the one hand, we consider e-mail filtering, where the feature space contains various properties of e-mail messages, and on the other hand, we consider drug discovery problems, where quantitative representations of molecular structures are encoded in terms of information-preserving descriptor values.   Subsets of the original attributes constructed by filter and wrapper techniques as well as subsets of linear combinations of the original attributes constructed by three different variants of the principle component analysis (PCA) are compared in terms of the classification performance achieved with various machine learning algorithms as well as in terms of runtime performance. We successively reduce the size of the attribute sets and investigate the changes in the classification results. Moreover, we explore the relationship between the variance captured in the linear combinations within PCA and the resulting classification accuracy.  The results show that the classification accuracy based on PCA is highly sensitive to the type of data and that the variance captured the principal components is not necessarily a vital indicator for the classification performance.}
}




@InProceedings{pmlr-v4-koehler08a,
  title = 	 {Unsupervised Feature Selection for Pattern Discovery in Seismic Wavefields},
  author = 	 {KÃ¶hler, Andreas and Ohrnberger, Matthias and Riggelsen, Carsten and Scherbaum, Frank},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {106--121},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/koehler08a/koehler08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/koehler08a.html},
  abstract = 	 {This study presents an unsupervised feature selection approach for the discovery  of significant patterns in seismic wavefields. We iteratively reduce the number  of features generated from seismic time series by first considering significance  of individual features. Significance testing is done by assessing the randomness  of the time series with the Wald-Wolfowitz runs test and by comparing observed and  theoretical variability of features. In a second step the in-between feature  dependencies are assessed based on correlation hunting in feature subsets using  Self-Organizing Maps (SOMs). We show the improved discriminative power of our  procedure compared to manually selected feature subsets by cross-validation applied  to synthetic seismic wavefield data. Furthermore, we apply the method to real-world  data with the aim to define suitable features for earthquake detection and seismic  phase classification in seismic recordings.}
}




@InProceedings{pmlr-v4-kiritchenko08a,
  title = 	 {Keyword Optimization in Sponsored Search via Feature Selection},
  author = 	 {Kiritchenko, Svetlana and Jiline, Mikhail},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {122--134},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/kiritchenko08a/kiritchenko08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/kiritchenko08a.html},
  abstract = 	 {Sponsored search is a new application domain for the feature selection area of research.  When a user searches for products or services using the Internet, most of the major search  engines would return two sets of results: regular web pages and paid advertisements.  An advertising company provides a set of keywords associated with an ad. If one of these  keywords is present in a userâs query, the ad is displayed, but the company is charged  only if the user actually clicks on the ad. Ultimately, a company would like to advertise  on the most effective keywords to attract only prospective customers. A set of keywords  can be optimized based on historic performance. We propose to optimize advertising keywords  with feature selection techniques applied to the set of all possible word combinations  comprising past usersâ queries. Unlike previous work in this area, our approach not only  recognizes the most profitable keywords, but also discovers more specific combinations  of keywords and other relevant words.}
}




@InProceedings{pmlr-v4-eruhimov08a,
  title = 	 {Transferring Knowledge by Prior Feature Sampling},
  author = 	 {Eruhimov, Victor and Martyanov, Vladimir and Polovinkin, Aleksey},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {135--147},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/eruhimov08a/eruhimov08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/eruhimov08a.html},
  abstract = 	 {The paper presents a novel method for transfer learning through prior variable sampling. A set of problems defined in the same feature space with similar dependencies of target on features is considered. We suggest a method for learning a decision tree ensemble on each of the problems by prior estimation of variable importance on other problems in the set and using it for regularizing model learning for a small amount of training samples. The method is tested on several simulated and real datasets. In particular, we apply our method for a set of time series classification (TSC) problems. Our analysis demonstrates an intriguing result: a model trained on several TSC problems can learn a new problem with high accuracy from a low number of samples.}
}




@InProceedings{pmlr-v4-ruiz08a,
  title = 	 {Best Agglomerative Ranked Subset for Feature Selection},
  author = 	 {Ruiz, Roberto and Riquelme, JosÃ© C. and Aguilar-Ruiz, JesÃºs S.},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {148--162},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/ruiz08a/ruiz08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/ruiz08a.html},
  abstract = 	 {The enormous increase of the size in databases makes finding an optimal subset  of features extremely difficult. In this paper, a new feature selection method  is proposed that will allow any subset evaluator -including the wrapper  evaluation method- to be used to find a group of features that will allow a  distinction to be made between the different possible classes. The method,  BARS (Best Agglomerative Ranked Subset), is based on the idea of relevance and  redundancy, in the sense that a ranked feature (or set) is more relevant if it  adds information when it is included in the final subset of selected features.  This heuristic method reduces dimensionality drastically and leads to improvements  in the accuracy, in comparison to a complete set and as opposed to other feature  selection algorithms.}
}




@InProceedings{pmlr-v4-guerif08a,
  title = 	 {Unsupervised Variable Selection: when random rankings sound as irrelevancy},
  author = 	 {GuÃ©rif, SÃ©bastien},
  booktitle = 	 {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
  pages = 	 {163--177},
  year = 	 {2008},
  editor = 	 {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
  volume = 	 {4},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Antwerp, Belgium},
  month = 	 {15 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v4/guerif08a/guerif08a.pdf},
  url = 	 {https://proceedings.mlr.press/v4/guerif08a.html},
  abstract = 	 {Whereas the variable selection has been extensively studied in the context of  supervised learning, the unsupervised variable selection has attracted attention  of researchers more recently as the available amount of unlabeled data has exploded.  Many unsupervised variable ranking criteria were proposed and their relevance is usually  demonstrated using either external cluster validity indexes or the accuracy of a classifier  which are both supervised criteria. Actually, the major issue of the variable subset selection  according to a ranking measure has been adressed only by few authors in the unsupervised  learning context. In this paper, we propose to combine multiple ranking to go ahead toward  a stable consensus variable subset in a totally unsupervised fashion.}
}



