@proceedings{FSDM2008,
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Yvan Saeys and Huan Liu and IÃ±aki Inza and Louis Wehenkel and Yves Van de Pee},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the Workshop on New Challenges for Feature Selection in Data Mining and Knowledge Discovery at ECML/PKDD 2008},
 volume = {4}
}

@inproceedings{pmlr-v4-antal08a,
 abstract = {Earlier, we formulated a Bayesian approach to Feature Subset Selection using Bayesian networks, which jointly estimate the posteriors of Markov Blanket Memberships (MBMs), Markov Blanket Sets (MBSs), and Markov Blanket Subgraphs (MBGs) for a given target variable. These results of the Bayesian Multilevel Analysis of relevance (BMLA) correspond respectively to a model-based pairwise relevance, relevance of sets, and to the interaction models of relevant variables. In this paper we discuss applications of the Bayesian approach to new challenges in relevance analysis. First, we formulate refined levels in BMLA by introducing the concepts of k-MBSs and k-MBGs, which are intermediate, scalable model properties expressing relevance. Second, we consider the extension of BMLA to multiple targets. Third, we introduce and investigate a score for feature redundancy and interaction based on the decomposability of the structure posterior. Finally, we overview the problems of conditional and contextual relevance. We demonstrate the concepts and methods in the field of the genomics of asthma.},
 address = {Antwerp, Belgium},
 author = {Antal, Peter and Millinghoffer, Andras and HullÃ¡m, GÃ¡bor and Szalai, Csaba and Falus, AndrÃ¡s},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W25589258},
 pages = {74--89},
 pdf = {http://proceedings.mlr.press/v4/antal08a/antal08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Bayesian View of Challenges in Feature Selection: Feature Aggregation, Multiple Targets, Redundancy and Interaction},
 url = {https://proceedings.mlr.press/v4/antal08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-campedel08a,
 abstract = {Satellite images are numerous and weakly exploited: it is urgent to develop efficient and fast indexing algorithms to facilitate their access. In order to determinate the best features to be extracted, we propose a methodology based on automatic feature selection algorithms, applied unsupervisingly on a strongly redundant features set. In this article we also demonstrate the usefulness of consensus clustering as a feature selection algorithm, allowing selected number of features estimation and exploration facilities. The efficiency of our approach is demonstrated on SPOT5 images.},
 address = {Antwerp, Belgium},
 author = {Campedel, Marine and Kyrgyzov, Ivan and Maitre, Henri},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W92604791},
 pages = {48--59},
 pdf = {http://proceedings.mlr.press/v4/campedel08a/campedel08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Unsupervised feature selection applied to SPOT5 satellite images indexing.},
 url = {https://proceedings.mlr.press/v4/campedel08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-eruhimov08a,
 abstract = {The paper presents a novel method for transfer learning through prior variable sampling. A set of problems defined in the same feature space with similar dependencies of target on features is considered. We suggest a method for learning a decision tree ensemble on each of the problems by prior estimation of variable importance on other problems in the set and using it for regularizing model learning for a small amount of training samples. The method is tested on several simulated and real datasets. In particular, we apply our method for a set of time series classification (TSC) problems. Our analysis demonstrates an intriguing result: a model trained on several TSC problems can learn a new problem with high accuracy from a low number of samples.},
 address = {Antwerp, Belgium},
 author = {Eruhimov, Victor and Martyanov, Vladimir and Polovinkin, Aleksey},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W2110378353},
 pages = {135--147},
 pdf = {http://proceedings.mlr.press/v4/eruhimov08a/eruhimov08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Transferring knowledge by prior feature sampling},
 url = {https://proceedings.mlr.press/v4/eruhimov08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-guerif08a,
 abstract = {Whereas the variable selection has been extensively studied in the context of supervised learning, the unsupervised variable selection has attracted attention of researchers more recently as the available amount of unlabeled data has exploded. Many unsupervised variable ranking criteria were proposed and their relevance is usually demonstrated using either external cluster validity indexes or the accuracy of a classifier which are both supervised criteria. Actually, the major issue of the variable subset selection according to a ranking measure has been adressed only by few authors in the unsupervised learning context. In this paper, we propose to combine multiple ranking to go ahead toward a stable consensus variable subset in a totally unsupervised fashion.},
 address = {Antwerp, Belgium},
 author = {GuÃ©rif, SÃ©bastien},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W2146930485},
 pages = {163--177},
 pdf = {http://proceedings.mlr.press/v4/guerif08a/guerif08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Unsupervised variable selection: when random rankings sound as irrelevancy},
 url = {https://proceedings.mlr.press/v4/guerif08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-huynhthu08a,
 abstract = {This paper proposes a novel statistical procedure based on permutation tests for extracting a subset of truly relevant variables from multivariate importance rankings derived from tree-based supervised learning methods. It shows also that the direct extension of the classical approach based on permutation tests for estimating false discovery rates of univariate variable scoring procedures does not extend very well to the case of multivariate tree-based importance measures.},
 address = {Antwerp, Belgium},
 author = {Huynh-Thu, VÃ¢n Anh and Wehenkel, Louis and Geurts, Pierre},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W3178158344},
 pages = {60--73},
 pdf = {http://proceedings.mlr.press/v4/huynhthu08a/huynhthu08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Exploiting tree-based variable importances to selectively identify relevant variables},
 url = {https://proceedings.mlr.press/v4/huynhthu08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-janecek08a,
 abstract = {Dimensionality reduction and feature subset selection are two techniques for reducing the attribute space of a feature set, which is an important component of both supervised and unsupervised classification or regression problems. While in feature subset selection a subset of the original attributes is extracted, dimensionality reduction in general produces linear combinations of the original attribute set.

In this paper we investigate the relationship between several attribute space reduction techniques and the resulting classification accuracy for two very different application areas. On the one hand, we consider e-mail filtering, where the feature space contains various properties of e-mail messages, and on the other hand, we consider drug discovery problems, where quantitative representations of molecular structures are encoded in terms of information-preserving descriptor values.

Subsets of the original attributes constructed by filter and wrapper techniques as well as subsets of linear combinations of the original attributes constructed by three different variants of the principle component analysis (PCA) are compared in terms of the classification performance achieved with various machine learning algorithms as well as in terms of runtime performance. We successively reduce the size of the attribute sets and investigate the changes in the classification results. Moreover, we explore the relationship between the variance captured in the linear combinations within PCA and the resulting classification accuracy.

The results show that the classification accuracy based on PCA is highly sensitive to the type of data and that the variance captured the principal components is not necessarily a vital indicator for the classification performance.},
 address = {Antwerp, Belgium},
 author = {Janecek, Andreas and Gansterer, Wilfried and Demel, Michael and Ecker, Gerhard},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W2095642553},
 pages = {90--105},
 pdf = {http://proceedings.mlr.press/v4/janecek08a/janecek08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On the relationship between feature selection and classification accuracy},
 url = {https://proceedings.mlr.press/v4/janecek08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-kiritchenko08a,
 abstract = {Sponsored search is a new application domain for the feature selection area of research. When a user searches for products or services using the Internet, most of the major search engines would return two sets of results: regular web pages and paid advertisements. An advertising company provides a set of keywords associated with an ad. If one of these keywords is present in a user's query, the ad is displayed, but the company is charged only if the user actually clicks on the ad. Ultimately, a company would like to advertise on the most effective keywords to attract only prospective customers. A set of keywords can be optimized based on historic performance. We propose to optimize advertising keywords with feature selection techniques applied to the set of all possible word combinations comprising past users' queries. Unlike previous work in this area, our approach not only recognizes the most profitable keywords, but also discovers more specific combinations of keywords and other relevant words.},
 address = {Antwerp, Belgium},
 author = {Kiritchenko, Svetlana and Jiline, Mikhail},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W2148170704},
 pages = {122--134},
 pdf = {http://proceedings.mlr.press/v4/kiritchenko08a/kiritchenko08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Keyword optimization in sponsored search via feature selection},
 url = {https://proceedings.mlr.press/v4/kiritchenko08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-koehler08a,
 abstract = {This study presents an unsupervised feature selection approach for the discovery of significant patterns in seismic wavefields. We iteratively reduce the number of features generated from seismic time series by first considering significance of individual features. Significance testing is done by assessing the randomness of the time series with the Wald-Wolfowitz runs test and by comparing observed and theoretical variability of features. In a second step the in-between feature dependencies are assessed based on correlation hunting in feature subsets using Self-Organizing Maps (SOMs). We show the improved discriminative power of our procedure compared to manually selected feature subsets by cross-validation applied to synthetic seismic wavefield data. Furthermore, we apply the method to real-world data with the aim to define suitable features for earthquake detection and seismic phase classification in seismic recordings.},
 address = {Antwerp, Belgium},
 author = {KÃ¶hler, Andreas and Ohrnberger, Matthias and Riggelsen, Carsten and Scherbaum, Frank},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W190103911},
 pages = {106--121},
 pdf = {http://proceedings.mlr.press/v4/koehler08a/koehler08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Unsupervised Feature Selection for Pattern Discovery in Seismic Wavefields},
 url = {https://proceedings.mlr.press/v4/koehler08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-lee08a,
 abstract = {Nonlinear dimensionality reduction aims at providing low-dimensional representions of high-dimensional data sets. Many new methods have been recently proposed, but the question of their assessment and comparison remains open. This paper reviews some of the existing quality measures that are based on distance ranking and K-ary neighborhoods. In this context, the comparison of the ranks in the high- and low-dimensional spaces leads to the definition of the co-ranking matrix. Rank errors and concepts such as neighborhood intrusions and extrusions can be associated with different blocks of the co-ranking matrix. The considered quality criteria are then cast within this unifying framework and the blocks they involve are identified. The same framework allows us to propose simpler criteria, which quantify two aspects of the embedding, namely its overall quality and its tendency to favor either intrusions or extrusions. Eventually, a simple experiment illustrates the soundness of the approach.},
 address = {Antwerp, Belgium},
 author = {Lee, John and Verleysen, Michel},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W2660674426},
 pages = {21--35},
 pdf = {http://proceedings.mlr.press/v4/lee08a/lee08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Quality assessment of nonlinear dimensionality reduction based on K-ary neighborhoods},
 url = {https://proceedings.mlr.press/v4/lee08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-ruiz08a,
 abstract = {The enormous increase of the size in databases makes finding an optimal subset of features extremely difficult. In this paper, a new feature selection method is proposed that will allow any subset evaluator -including the wrapper evaluation method- to be used to find a group of features that will allow a distinction to be made between the different possible classes. The method, BARS (Best Agglomerative Ranked Subset), is based on the idea of relevance and redundancy, in the sense that a ranked feature (or set) is more relevant if it adds information when it is included in the final subset of selected features. This heuristic method reduces dimensionality drastically and leads to improvements in the accuracy, in comparison to a complete set and as opposed to other feature selection algorithms.},
 address = {Antwerp, Belgium},
 author = {Ruiz, Roberto and Riquelme, JosÃ© C. and Aguilar-Ruiz, JesÃºs S.},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W2169015330},
 pages = {148--162},
 pdf = {http://proceedings.mlr.press/v4/ruiz08a/ruiz08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Best agglomerative ranked subset for feature selection},
 url = {https://proceedings.mlr.press/v4/ruiz08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-saeys08a,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 address = {Antwerp, Belgium},
 author = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Van de Peer, Yves},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W4240465921},
 pages = {1--4},
 pdf = {http://proceedings.mlr.press/v4/saeys08a/saeys08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v4/saeys08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-suzuki08a,
 abstract = {Mutual information is useful in various data processing tasks such as feature selection or independent component analysis. In this paper, we propose a new method of approximating mutual information based on maximum likelihood estimation of a density ratio function. Our method, called Maximum Likelihood Mutual Information (MLMI), has several attractive properties, e.g., density estimation is not involved, it is a single-shot procedure, the global optimal solution can be efficiently computed, and cross-validation is available for model selection. Numerical experiments show that MLMI compares favorably with existing methods.},
 address = {Antwerp, Belgium},
 author = {Suzuki, Taiji and Sugiyama, Masashi and Sese, Jun and Kanamori, Takafumi},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W2136681493},
 pages = {5--20},
 pdf = {http://proceedings.mlr.press/v4/suzuki08a/suzuki08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Approximating mutual information by maximum likelihood density ratio estimation},
 url = {https://proceedings.mlr.press/v4/suzuki08a.html},
 volume = {4},
 year = {2008}
}

@inproceedings{pmlr-v4-zhao08a,
 abstract = {Feature selection is an effective approach to reducing dimensionality by selecting relevant original features. In this work, we studied a novel problem of multi-source feature selection for unlabeled data: given multiple heterogeneous data sources (or data sets), select features from one source of interest by integrating information from various data sources. In essence, we investigate how we can employ the information contained in multiple data sources to effectively derive intrinsic relationships that can help select more meaningful (or domain relevant) features. We studied how to adjust the covariance matrix of a data set using the geometric structure obtained from multiple data sources, and how to select features of the target source using geometry-dependent covariance. We designed and conducted experiments to systematically compare the proposed approach with representative methods in our attempt to solve the novel problem of multi-source feature selection. The empirical study demonstrated the efficacy and potential of multi-source feature selection.},
 address = {Antwerp, Belgium},
 author = {Zhao, Zheng and Liu, Huan},
 booktitle = {Proceedings of the Workshop on New Challenges for Feature Selection in Data  Mining and Knowledge Discovery at ECML/PKDD 2008},
 editor = {Saeys, Yvan and Liu, Huan and Inza, IÃ±aki and Wehenkel, Louis and Pee, Yves Van de},
 month = {15 Sep},
 openalex = {W2131649237},
 pages = {36--47},
 pdf = {http://proceedings.mlr.press/v4/zhao08a/zhao08a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-source feature selection via geometry-dependent covariance analysis},
 url = {https://proceedings.mlr.press/v4/zhao08a.html},
 volume = {4},
 year = {2008}
}
