
@Proceedings{,
  title =     {Proceedings of the 2020 KDD Workshop on Causal Discovery},
  booktitle = {Proceedings of the 2020 KDD Workshop on Causal Discovery},
  editor =    {Thuc Duy Le and Lin Liu and Kun Zhang and Emre KÄ±cÄ±man and Peng Cui and Aapo HyvÃ¤rinen},
  publisher = {PMLR},
  series =    {Proceedings of Machine Learning Research},
  volume =    127
}
@InProceedings{pmlr-v127-le20a,
  title = 	 {Preface: The 2020 ACM SIGKDD Workshop on Causal Discovery },
  author =       {Le, Thuc Duy and Liu, Lin and Zhang, Kun and K{\i}c{\i}man, Emre and Cui, Peng and Hyv\"{a}rinen, Aapo},
  booktitle = 	 {Proceedings of the 2020 KDD Workshop on Causal Discovery},
  pages = 	 {1--3},
  year = 	 {2020},
  editor = 	 {Le, Thuc Duy and Liu, Lin and Zhang, Kun and KÄ±cÄ±man, Emre and Cui, Peng and HyvÃ¤rinen, Aapo},
  volume = 	 {127},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {24 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v127/le20a/le20a.pdf},
  url = 	 {https://proceedings.mlr.press/v127/le20a.html},
  abstract = 	 {Preface to the 2020 KDD Workshop on Causal Discovery (CD 2020)}
}
@InProceedings{pmlr-v127-li20a,
  title = 	 {Continuous Treatment Effect Estimation via Generative Adversarial De-confounding },
  author =       {Li, Yunzhe and Kuang, Kun and Li, Bo and Cui, Peng and Tao, Jianrong and Yang, Hongxia and Wu, Fei},
  booktitle = 	 {Proceedings of the 2020 KDD Workshop on Causal Discovery},
  pages = 	 {4--22},
  year = 	 {2020},
  editor = 	 {},
  volume = 	 {127},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {24 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v127/li20a/li20a.pdf},
  url = 	 {https://proceedings.mlr.press/v127/li20a.html},
  abstract = 	 {One fundamental problem in causal inference is the treatment effect estimation in obser- vational studies, and its key challenge is to handle the confounding bias induced by the associations between covariates and treatment variable. In this paper, we study the prob- lem of effect estimation on continuous treatment from observational data, going beyond previous work on binary treatments. Previous work for binary treatment focuses on de- confounding by balancing the distribution of covariates between the treated and control groups with either propensity score or confounder balancing techniques. In the continuous setting, those methods would fail as we can hardly evaluate the distribution of covariates under each treatment status. To tackle the case of continuous treatments, we propose a novel Generative Adversarial De-confounding (GAD) algorithm to eliminate the associa- tions between covariates and treatment variable with two main steps: (1) generating an âcalibrationâ distribution without associations between covariates and treatment by ran- dom perturbation; (2) learning sample weight that transfer the distribution of observed data to the âcalibrationâ distribution for de-confounding with a Generative Adversarial Network. Extensive experiments on both synthetic and real-world datasets demonstrate that our algorithm outperforms the state-of-the-art methods for effect estimation of con- tinuous treatment with observational data.}
}
@InProceedings{pmlr-v127-ma20a,
  title = 	 {Predictive and Causal Implications of using Shapley Value for Model Interpretation},
  author =       {Ma, Sisi and Tourani, Roshan},
  booktitle = 	 {Proceedings of the 2020 KDD Workshop on Causal Discovery},
  pages = 	 {23--38},
  year = 	 {2020},
  editor = 	 {},
  volume = 	 {127},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {24 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v127/ma20a/ma20a.pdf},
  url = 	 {https://proceedings.mlr.press/v127/ma20a.html},
  abstract = 	 {Shapley value is a concept from game theory. Recently, it has been used for explaining complex models produced by machine learning techniques. Although the mathematical definition of Shapley value is straight-forward, the implication of using it as a model inter- pretation tool is yet to be described. In the current paper, we analyzed Shapley value in the Bayesian network framework. We established the relationship between Shapley value and conditional independence, a key concept in both predictive and causal modeling. Our results indicate that, eliminating a variable with high Shapley value from a model do not necessarily impair predictive performance, whereas eliminating a variable with low Shapley value from a model could impair performance. Therefore, using Shapley value for feature selection do not result in the most parsimonious and predictively optimal model in the general case. More importantly, Shapley value of a variable do not reflect their causal relationship with the target of interest.}
}
@InProceedings{pmlr-v127-sharma20a,
  title = 	 {Hi-CI: Deep Causal Inference in High Dimensions},
  author =       {Sharma, Ankit and Gupta, Garima and Prasad, Ranjitha and Chatterjee, Arnab and Vig, Lovekesh and Shroff, Gautam},
  booktitle = 	 {Proceedings of the 2020 KDD Workshop on Causal Discovery},
  pages = 	 {39--61},
  year = 	 {2020},
  editor = 	 {},
  volume = 	 {127},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {24 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v127/sharma20a/sharma20a.pdf},
  url = 	 {https://proceedings.mlr.press/v127/sharma20a.html},
  abstract = 	 {We address the problem of counterfactual regression using causal inference (CI) in obser- vational studies consisting of high dimensional covariates and high cardinality treatments. Confounding bias, which leads to inaccurate treatment effect estimation, is attributed to covariates that affect both treatments and outcome. The presence of high-dimensional co- variates exacerbates the impact of bias as it is harder to isolate and measure the impact of these confounders. In the presence of high-cardinality treatment variables, CI is rendered ill-posed due to the increase in the number of counterfactual outcomes to be predicted. We propose Hi-CI, a deep neural network (DNN) based framework for estimating causal effects in the presence of large number of covariates, and high-cardinal and continuous treatment variables. The proposed architecture comprises of a decorrelation network and an outcome prediction network. In the decorrelation network, we learn a data representa- tion in lower dimensions as compared to the original covariates, and addresses confounding bias alongside. Subsequently, in the outcome prediction network, we learn an embedding of high-cardinality and continuous treatments, jointly with the data representation. We demonstrate the efficacy of causal effect prediction of the proposed Hi-CI network using synthetic and real-world NEWS datasets.}
}
@InProceedings{pmlr-v127-young20a,
  title = 	 {Learning Latent Causal Structures with a Redundant Input Neural Network},
  author =       {Young, Jonathan D. and Andrews, Bryan and Cooper, Gregory F. and Lu, Xinghua},
  booktitle = 	 {Proceedings of the 2020 KDD Workshop on Causal Discovery},
  pages = 	 {62--91},
  year = 	 {2020},
  editor = 	 {},
  volume = 	 {127},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {24 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v127/young20a/young20a.pdf},
  url = 	 {https://proceedings.mlr.press/v127/young20a.html},
  abstract = 	 {Most causal discovery algorithms find causal structure among a set of observed variables. Learning the causal structure among latent variables remains an important open problem, particularly when using high-dimensional data. In this paper, we address a problem for which it is known that inputs cause outputs, and these causal relationships are encoded by a causal network among a set of an unknown number of latent variables. We developed a deep learning model, which we call a redundant input neural network (RINN), with a modified architecture and a regularized objective function to find causal relationships between input, hidden, and output variables. More specifically, our model allows input variables to directly interact with all latent variables in a neural network to influence what information the latent variables should encode in order to generate the output variables accurately. In this setting, the direct connections between input and latent variables makes the latent variables partially interpretable; furthermore, the connectivity among the latent variables in the neural network serves to model their potential causal relationships to each other and to the output variables. A series of simulation experiments provide support that the RINN method can successfully recover latent causal structure between input and output variables.}
}



