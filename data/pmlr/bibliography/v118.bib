@proceedings{AABI2019,
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Cheng Zhang and Francisco Ruiz and Thang Bui and Adji Bousso Dieng and Dawen Liang},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 volume = {118}
}

@inproceedings{pmlr-v118-alemi20a,
 abstract = {In classic papers, Zellner demonstrated that Bayesian inference could be derived as the solution to an information theoretic functional. Below we derive a generalized form of this functional as a variational lower bound of a predictive information bottleneck objective. This generalized functional encompasses most modern inference procedures and suggests novel ones.},
 author = {Alemi, Alexander A.},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W2981998395},
 pages = {1--6},
 pdf = {http://proceedings.mlr.press/v118/alemi20a/alemi20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variational Predictive Information Bottleneck},
 url = {https://proceedings.mlr.press/v118/alemi20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-berkovich20a,
 abstract = {A simple and widely adopted approach to extend Gaussian processes (GPs) to multiple outputs is to model each output as a linear combination of a collection of shared, unobserved latent GPs. An issue with this approach is choosing the number of latent processes and their kernels. These choices are typically done manually, which can be time consuming and prone to human biases. We propose Gaussian Process Automatic Latent Process Selection (GP-ALPS), which automatically chooses the latent processes by turning off those that do not meaningfully contribute to explaining the data. We develop a variational inference scheme, assess the quality of the variational posterior by comparing it against the gold standard MCMC, and demonstrate the suitability of GP-ALPS in a set of preliminary experiments.},
 author = {Berkovich, Pavel and Perim, Eric and Bruinsma, Wessel},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3037809680},
 pages = {1--14},
 pdf = {http://proceedings.mlr.press/v118/berkovich20a/berkovich20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {GP-ALPS: Automatic Latent Process Selection for Multi-Output Gaussian Process Models},
 url = {https://proceedings.mlr.press/v118/berkovich20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-cherief-abdellatif20a,
 abstract = {In some misspecified settings, the posterior distribution in Bayesian statistics may lead to inconsistent estimates. To fix this issue, it has been suggested to replace the likelihood by a pseudo-likelihood, that is the exponential of a loss function enjoying suitable robustness properties. In this paper, we build a pseudo-likelihood based on the Maximum Mean Discrepancy, defined via an embedding of probability distributions into a reproducing kernel Hilbert space. We show that this MMD-Bayes posterior is consistent and robust to model misspecification. As the posterior obtained in this way might be intractable, we also prove that reasonable variational approximations of this posterior enjoy the same properties. We provide details on a stochastic gradient algorithm to compute these variational approximations. Numerical simulations indeed suggest that our estimator is more robust to misspecification than the ones based on the likelihood.},
 author = {Cherief-Abdellatif, Badr-Eddine and Alquier, Pierre},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3014325088},
 pages = {1--21},
 pdf = {http://proceedings.mlr.press/v118/cherief-abdellatif20a/cherief-abdellatif20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {MMD-Bayes: Robust Bayesian Estimation via Maximum Mean Discrepancy},
 url = {https://proceedings.mlr.press/v118/cherief-abdellatif20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-fjelde20a,
 abstract = {Transforming one probability distribution to another is a powerful tool in Bayesian inference and machine learning. Some prominent examples are constrained-to-unconstrained transformations of distributions for use in Hamiltonian Monte Carlo and constructing exible and learnable densities such as normalizing ows. We present Bijectors.jl, a software package in Julia for transforming distributions, available at github.com/TuringLang/Bijectors.jl. The package provides a exible and composable way of implementing transformations of distributions without being tied to a computational framework. We demonstrate the use of Bijectors.jl on improving variational inference by encoding known statistical dependencies into the variational posterior using normalizing ows, providing a general approach to relaxing the mean-field assumption usually made in variational inference. },
 author = {Fjelde, Tor Erlend and Xu, Kai and Tarek, Mohamed and Yalburgi, Sharan and Ge, Hong},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3021843190},
 pages = {1--17},
 pdf = {http://proceedings.mlr.press/v118/fjelde20a/fjelde20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Bijectors.jl: Flexible transformations for probability distributions},
 url = {https://proceedings.mlr.press/v118/fjelde20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-gong20a,
 abstract = { Despite promising progress on unimodal data imputation (e.g. image inpainting), models for multimodal data imputation are far from satisfactory. In this work, we propose variational selective autoencoder (VSAE) for this task. Learning only from partially-observed data, VSAE can model the joint distribution of observed/unobserved modalities and the imputation mask, resulting in a unied model for various down-stream tasks including data generation and imputation. Evaluation on synthetic high-dimensional and challenging low-dimensional multimodal datasets shows improvement over the state-of-the-art imputation models. },
 author = {Gong, Yu and Hajimirsadeghi, Hossein and He, Jiawei and Nawhal, Megha and Durand, Thibaut and Mori, Greg},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3031152617},
 pages = {1--17},
 pdf = {http://proceedings.mlr.press/v118/gong20a/gong20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variational Selective Autoencoder},
 url = {https://proceedings.mlr.press/v118/gong20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-jaiswal20a,
 abstract = {We study system design problems stated as parameterized stochastic programs with a chance-constraint set. We adopt a Bayesian approach that requires the computation of a posterior predictive integral which is usually intractable. In addition, for the problem to be a well-defined convex program, we must retain the convexity of the feasible set. Consequently, we propose a variational Bayes-based method to approximately compute the posterior predictive integral that ensures tractability and retains the convexity of the feasible set. Under certain regularity conditions, we also show that the solution set obtained using variational Bayes converges to the true solution set as the number of observations tends to infinity. We also provide bounds on the probability of qualifying a true infeasible point (with respect to the true constraints) as feasible under the VB approximation for a given number of samples.},
 author = {Jaiswal, Prateek and Honnappa, Harsh and Rao, Vinayak A.},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W2997792642},
 pages = {1--12},
 pdf = {http://proceedings.mlr.press/v118/jaiswal20a/jaiswal20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variational Bayesian Methods for Stochastically Constrained System Design Problems},
 url = {https://proceedings.mlr.press/v118/jaiswal20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-jia20a,
 abstract = {Normalizing constant (also called partition function, Bayesian evidence, or marginal likelihood) is one of the central goals of Bayesian inference, yet most of the existing methods are both expensive and inaccurate. Here we develop a new approach, starting from posterior samples obtained with a standard Markov Chain Monte Carlo (MCMC). We apply a novel Normalizing Flow (NF) approach to obtain an analytic density estimator from these samples, followed by Optimal Bridge Sampling (OBS) to obtain the normalizing constant. We compare our method which we call Gaussianized Bridge Sampling (GBS) to existing methods such as Nested Sampling (NS) and Annealed Importance Sampling (AIS) on several examples, showing our method is both significantly faster and substantially more accurate than these methods, and comes with a reliable error estimation.},
 author = {Jia, He and Seljak, Uros},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W2996578107},
 pages = {1--14},
 pdf = {http://proceedings.mlr.press/v118/jia20a/jia20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Normalizing Constant Estimation with Gaussianized Bridge Sampling},
 url = {https://proceedings.mlr.press/v118/jia20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-lalchand20a,
 abstract = {Learning in Gaussian Process models occurs through the adaptation of hyperparameters of the mean and the covariance function. The classical approach entails maximizing the marginal likelihood yielding fixed point estimates (an approach called \textit{Type II maximum likelihood} or ML-II). An alternative learning procedure is to infer the posterior over hyperparameters in a hierarchical specification of GPs we call \textit{Fully Bayesian Gaussian Process Regression} (GPR). This work considers two approximation schemes for the intractable hyperparameter posterior: 1) Hamiltonian Monte Carlo (HMC) yielding a sampling-based approximation and 2) Variational Inference (VI) where the posterior over hyperparameters is approximated by a factorized Gaussian (mean-field) or a full-rank Gaussian accounting for correlations between hyperparameters. We analyze the predictive performance for fully Bayesian GPR on a range of benchmark data sets.},
 author = {Lalchand, Vidhi and Rasmussen, Carl Edward},
 booktitle = {Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W2997921382},
 pages = {1--12},
 pdf = {http://proceedings.mlr.press/v118/lalchand20a/lalchand20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Approximate Inference for Fully Bayesian Gaussian Process Regression},
 url = {https://proceedings.mlr.press/v118/lalchand20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-li20a,
 abstract = { We derive reverse-mode (or adjoint) automatic differentiation for solutions of stochastic differential equations (SDEs), allowing time-efficient and constant-memory computation of pathwise gradients, a continuous-time analogue of the reparameterization trick. Specifically, we construct a backward SDE whose solution is the gradient and provide conditions under which numerical solutions converge. We also combine our stochastic adjoint approach with a stochastic variational inference scheme for continuous-time SDE models, allowing us to learn distributions over functions using stochastic gradient descent. Our latent SDE model achieves competitive performance compared to existing approaches on time series modeling.},
 author = {Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud, David K.},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3037213631},
 pages = {1--28},
 pdf = {http://proceedings.mlr.press/v118/li20a/li20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Scalable Gradients and Variational Inference for Stochastic Differential Equations},
 url = {https://proceedings.mlr.press/v118/li20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-ma20a,
 abstract = {In this paper, we propose a very simple but e
ective VAE model (HM-VAE) that can handle real-valued data with heterogeneous marginals, meaning that they have drastically distinct marginal distributions, statistical properties as well as semantics. Preliminary results show that the HM-VAE can learn distributions with heterogeneous marginal distributions, whereas the vanilla VAEs fails. },
 author = {Ma, Chao and Tschiatschek, Sebastian and Li, Yingzhen and Turner, Richard and Hernandez-Lobato, Jose Miguel and Zhang, Cheng},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3037353992},
 pages = {1--8},
 pdf = {http://proceedings.mlr.press/v118/ma20a/ma20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {HM-VAEs: a Deep Generative Model for Real-valued Data with Heterogeneous Marginals},
 url = {https://proceedings.mlr.press/v118/ma20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-marino20a,
 abstract = {We propose an approach for improving sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving frame of reference, removing temporal correlations and simplifying the modeling of higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone flow-based models and as a component within sequential latent variable models. Results are presented on three benchmark video datasets and three other time series datasets, where autoregressive flow-based dynamics improve log-likelihood performance over baseline models. Finally, we illustrate the decorrelation and improved generalization properties of using flow-based dynamics.},
 author = {Marino, Joseph and Chen, Lei and He, Jiawei and Mandt, Stephan},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3217049406},
 pages = {1--16},
 pdf = {http://proceedings.mlr.press/v118/marino20a/marino20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improving Sequential Latent Variable Models with Autoregressive Flows},
 url = {https://proceedings.mlr.press/v118/marino20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-mena20a,
 abstract = { We address the problem of marginal inference for an exponential family defined over the set of permutation matrices. This problem is known to quickly become intractable as the size of the permutation increases, since its involves the computation of the permanent of a matrix, a #P-hard problem. We introduce Sinkhorn variational marginal inference as a scalable alternative, a method whose validity is ultimately justified by the so-called Sinkhorn approximation of the permanent. We demonstrate the effectiveness of our method in the problem of probabilistic identification of neurons in the worm C.elegans.},
 author = {Mena, Gonzalo and Varol, Erdem and Nejatbakhsh, Amin and Yemini, Eviatar and Paninski, Liam},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3038053951},
 pages = {1--9},
 pdf = {http://proceedings.mlr.press/v118/mena20a/mena20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sinkhorn Permutation Variational Marginal Inference},
 url = {https://proceedings.mlr.press/v118/mena20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-pakman20a,
 abstract = { We introduce a neural architecture to perform amortized approximate Bayesian inference over latent random permutations of two sets of objects. The method involves approximating permanents of matrices of pairwise probabilities using recent ideas on functions dened over sets. Each sampled permutation comes with a probability estimate, a quantity unavailable in MCMC approaches. We illustrate the method in sets of 2D points and MNIST images.},
 author = {Pakman, Ari and Wang, Yueqi and Paninski, Liam},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3038088307},
 pages = {1--7},
 pdf = {http://proceedings.mlr.press/v118/pakman20a/pakman20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Neural Permutation Processes.},
 url = {https://proceedings.mlr.press/v118/pakman20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-pearce20a,
 abstract = { We consider the problem of unsupervised learning of a low dimensional, interpretable, latent state of a video containing a moving object. The problem of distilling interpretable dynamics from pixels has been extensively considered through the lens of graphical/state space models (Fraccaro et al., 2017; Lin et al., 2018; Pearce et al., 2018; Chiappa and Paquet, 2019) that exploit Markov structure for cheap computation and structured priors for enforcing interpretability on latent representations. We take a step towards extending these approaches by discarding the Markov structure; inspired by Gaussian process dynamical models (Wang et al., 2006), we instead repurpose the recently proposed Gaussian Process Prior Variational Autoencoder (Casale et al., 2018) for learning interpretable latent dynamics. We describe the model and perform experiments on a synthetic dataset and see that the model reliably reconstructs smooth dynamics exhibiting U-turns and loops. We also observe that this model may be trained without any  annealing or freeze-thaw of training parameters in contrast to previous works, albeit for slightly dierent use cases, where application specic training tricks are often required.},
 author = {Pearce, Michael},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3037449978},
 pages = {1--12},
 pdf = {http://proceedings.mlr.press/v118/pearce20a/pearce20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The Gaussian Process Prior VAE for Interpretable Latent Dynamics from Pixels},
 url = {https://proceedings.mlr.press/v118/pearce20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-perov20a,
 abstract = {We elaborate on using importance sampling for causal reasoning, in particular for counterfactual inference. We show how this can be implemented natively in probabilistic programming. By considering the structure of the counterfactual query, one can significantly optimise the inference process. We also consider design choices to enable further optimisations. We introduce MultiVerse, a probabilistic programming prototype engine for approximate causal reasoning. We provide experimental results and compare with Pyro, an existing probabilistic programming framework with some of causal reasoning tools.},
 author = {Perov, Yura and Graham, Logan and Gourgoulias, Kostis and Richens, Jonathan and Lee, Ciaran and Baker, Adam and Johri, Saurabh},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W2980901947},
 pages = {1--36},
 pdf = {http://proceedings.mlr.press/v118/perov20a/perov20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {MultiVerse: Causal Reasoning using Importance Sampling in Probabilistic Programming},
 url = {https://proceedings.mlr.press/v118/perov20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-sheth20a,
 abstract = {We propose that approximate Bayesian algorithms should optimize a new criterion, directly derived from the loss, to calculate their approximate posterior which we refer to as pseudo-posterior. Unlike standard variational inference which optimizes a lower bound on the log marginal likelihood, the new algorithms can be analyzed to provide loss guarantees on the predictions with the pseudo-posterior. Our criterion can be used to derive new sparse Gaussian process algorithms that have error guarantees applicable to various likelihoods.},
 author = {Sheth, Rishit and Khardon, Roni},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3027291640},
 pages = {1--18},
 pdf = {http://proceedings.mlr.press/v118/sheth20a/sheth20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Pseudo-Bayesian Learning via Direct Loss Minimization with Applications to Sparse Gaussian Process Models.},
 url = {https://proceedings.mlr.press/v118/sheth20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-shwartz-ziv20a,
 abstract = {In this preliminary work, we study the generalization properties of infinite ensembles of infinitely-wide neural networks. Amazingly, this model family admits tractable calculations for many information-theoretic quantities. We report analytical and empirical investigations in the search for signals that correlate with generalization.},
 author = {Shwartz-Ziv, Ravid and Alemi, Alexander A},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W2991227015},
 pages = {1--17},
 pdf = {http://proceedings.mlr.press/v118/shwartz-ziv20a/shwartz-ziv20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Information in Infinite Ensembles of Infinitely-Wide Neural Networks},
 url = {https://proceedings.mlr.press/v118/shwartz-ziv20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-wilk20a,
 abstract = {In this work, we provide a variational lower bound that can be computed without expensive matrix operations like inversion. Our bound can be used as a drop-in replacement to the existing variational method of Hensman et al. (2013, 2015), and can therefore directly be applied in a wide variety of models, such as deep GPs (Damianou and Lawrence, 2013). We focus on the theoretical properties of this new bound, and show some initial experimental results for optimising this bound. We hope to realise the full promise in scalability that this new bound has in future work.},
 author = {van der Wilk, Mark and John, ST and Artemev, Artem and Hensman, James},
 booktitle = {Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3037080172},
 pages = {1--9},
 pdf = {http://proceedings.mlr.press/v118/wilk20a/wilk20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variational Gaussian Process Models without Matrix Inverses.},
 url = {https://proceedings.mlr.press/v118/wilk20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-xu20a,
 abstract = {StanÃ¢Â€Â™s Hamilton Monte Carlo (HMC) has demonstrated remarkable sampling robustness and efficiency in a wide range of Bayesian inference problems through carefully crafted adaption schemes to the celebrated No-U-Turn sampler (NUTS) algorithm. It is challenging to implement these adaption schemes robustly in practice, hindering wider adoption amongst practitioners who are not directly working with the Stan modelling language. AdvancedHMC.jl (AHMC) contributes a modular, well-tested, standalone implementation of NUTS that recovers and extends StanÃ¢Â€Â™s NUTS algorithm. AHMC is written in Julia, a modern high-level language for scientic computing, benefoting from optional hardware acceleration and interoperability with a wealth of existing software written in both Julia and other languages, such as Python. Efficacy is demonstrated empirically by comparison with Stan through a third-party Markov chain Monte Carlo benchmarking suite.},
 author = {Xu, Kai and Ge, Hong and Tebbutt, Will and Tarek, Mohamed and Trapp, Martin and Ghahramani, Zoubin},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 pages = {1--10},
 pdf = {http://proceedings.mlr.press/v118/xu20a/xu20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {AdvancedHMC.jl: A robust, modular and ecient implementation of advanced HMC algorithms },
 url = {https://proceedings.mlr.press/v118/xu20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-yacoby20a,
 abstract = {Variational Auto-encoders (VAEs) are deep generative latent variable models consisting of two components: a generative model that captures a data distribution p(x) by transforming a distribution p(z) over latent space, and an inference model that infers likely latent codes for each data point (Kingma and Welling, 2013). Recent work shows that traditional training methods tend to yield solutions that violate modeling desiderata: (1) the learned generative model captures the observed data distribution but does so while ignoring the latent codes, resulting in codes that do not represent the data (e.g. van den Oord et al. (2017); Kim et al. (2018)); (2) the aggregate of the learned latent codes does not match the prior p(z). This mismatch means that the learned generative model will be unable to generate realistic data with samples from p(z)(e.g. Makhzani et al. (2015); Tomczak and Welling (2017)). In this paper, we demonstrate that both issues stem from the fact that the global optima of the VAE training objective often correspond to undesirable solutions. Our analysis builds on two observations: (1) the generative model is unidentifiable - there exist many generative models that explain the data equally well, each with different (and potentially unwanted) properties and (2) bias in the VAE objective - the VAE objective may prefer generative models that explain the data poorly but have posteriors that are easy to approximate. We present a novel inference method, LiBI, mitigating the problems identified in our analysis. On synthetic datasets, we show that LiBI can learn generative models that capture the data distribution and inference models that better satisfy modeling assumptions when traditional methods struggle to do so.},
 author = {Yacoby, Yaniv and Pan, Weiwei and Doshi-Velez, Finale},
 booktitle = {Proceedings of The 2nd Symposium on
Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3011817866},
 pages = {1--17},
 pdf = {http://proceedings.mlr.press/v118/yacoby20a/yacoby20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Characterizing and Avoiding Problematic Global Optima of Variational Autoencoders},
 url = {https://proceedings.mlr.press/v118/yacoby20a.html},
 volume = {118},
 year = {2020}
}

@inproceedings{pmlr-v118-zhang20a,
 abstract = { Comparing the inferences of diverse candidate models is an essential part of model checking and escaping local optima. To enable efficient comparison, we introduce an amortized variational inference framework that can perform fast and reliable posterior estimation across models of the same architecture. Our Any Parameter Encoder (APE) extends the encoder neural network common in amortized inference to take both a data feature vector and a model parameter vector as input. APE thus reduces posterior inference across unseen data and models to a single forward pass. In experiments comparing candidate topic models for synthetic data and product reviews, our Any Parameter Encoder yields comparable posteriors to more expensive methods in far less time, especially when the encoder architecture is designed in model-aware fashion.},
 author = {Zhang, Lily H. and Hughes, Michael C.},
 booktitle = {Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference},
 editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
 month = {08 Dec},
 openalex = {W3031048193},
 pages = {1--11},
 pdf = {http://proceedings.mlr.press/v118/zhang20a/zhang20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Rapid Model Comparison by Amortizing Across Models.},
 url = {https://proceedings.mlr.press/v118/zhang20a.html},
 volume = {118},
 year = {2020}
}
