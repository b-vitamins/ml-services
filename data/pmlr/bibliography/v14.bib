
@Proceedings{Learning to Rank Challenge2010,
  title =     {Proceedings of the Learning to Rank Challenge},
  booktitle = {Proceedings of the Learning to Rank Challenge},
  editor =    {Olivier Chapelle and Yi Chang and Tie-Yan Liu},
  publisher = {PMLR},
  series =    {Proceedings of Machine Learning Research},
  volume =    14
}
@InProceedings{pmlr-v14-chapelle11a,
  title = 	 {Yahoo! Learning to Rank Challenge Overview},
  author = 	 {Chapelle, Olivier and Chang, Yi},
  booktitle = 	 {Proceedings of the Learning to Rank Challenge},
  pages = 	 {1--24},
  year = 	 {2011},
  editor = 	 {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
  volume = 	 {14},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Haifa, Israel},
  month = 	 {25 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v14/chapelle11a/chapelle11a.pdf},
  url = 	 {https://proceedings.mlr.press/v14/chapelle11a.html},
  abstract = 	 {Learning to rank for information retrieval has gained a lot of interest in the recent years but there is a lack for large real-world datasets to benchmark algorithms. That led us to publicly release two datasets used internally at Yahoo! for learning the web search ranking function. To promote these datasets and foster the development of state-of-the-art learning to rank algorithms, we organized the Yahoo! Learning to Rank Challenge in spring 2010. This paper provides an overview and an analysis of this challenge, along with a detailed description of the released datasets.}
}
@InProceedings{pmlr-v14-burges11a,
  title = 	 {Learning to Rank Using an Ensemble of Lambda-Gradient Models},
  author = 	 {Burges, Christopher and Svore, Krysta and Bennett, Paul and Pastusiak, Andrzej and Wu, Qiang},
  booktitle = 	 {Proceedings of the Learning to Rank Challenge},
  pages = 	 {25--35},
  year = 	 {2011},
  editor = 	 {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
  volume = 	 {14},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Haifa, Israel},
  month = 	 {25 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v14/burges11a/burges11a.pdf},
  url = 	 {https://proceedings.mlr.press/v14/burges11a.html},
  abstract = 	 {We describe the system that won Track 1 of the Yahoo!ÃÂ Learning to Rank Challenge.}
}
@InProceedings{pmlr-v14-busa-fekete11a,
  title = 	 {Ranking by calibrated AdaBoost},
  author = 	 {Busa-Fekete, RÃ³bert and KÃ©gl, BalÃ¡zs and ÃltetÅ, TamÃ¡s and Szarvas, GyÃ¶rgy},
  booktitle = 	 {Proceedings of the Learning to Rank Challenge},
  pages = 	 {37--48},
  year = 	 {2011},
  editor = 	 {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
  volume = 	 {14},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Haifa, Israel},
  month = 	 {25 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v14/busa-fekete11a/busa-fekete11a.pdf},
  url = 	 {https://proceedings.mlr.press/v14/busa-fekete11a.html},
  abstract = 	 {This paper describes the ideas and methodologies that we used in the Yahoo learning-to-rank challenge^1. Our technique is essentially pointwise with a listwise touch at the last combination step. The main ingredients of our approach are 1) preprocessing (querywise normalization) 2) multi-class AdaBoost.MH 3) regression calibration, and 4) an exponentially weighted forecaster for model combination. In post-challenge analysis we found that preprocessing and training AdaBoost with a wide variety of hyperparameters improved individual models significantly, the final listwise ensemble step was crucial, whereas calibration helped only in creating diversity.}
}
@InProceedings{pmlr-v14-geurts11a,
  title = 	 {Learning to rank with extremely randomized trees},
  author = 	 {Geurts, Pierre and Louppe, Gilles},
  booktitle = 	 {Proceedings of the Learning to Rank Challenge},
  pages = 	 {49--61},
  year = 	 {2011},
  editor = 	 {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
  volume = 	 {14},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Haifa, Israel},
  month = 	 {25 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v14/geurts11a/geurts11a.pdf},
  url = 	 {https://proceedings.mlr.press/v14/geurts11a.html},
  abstract = 	 {In this paper, we report on our experiments on the Yahoo! Labs Learning to Rank challenge organized in the context of the 23rd International Conference of Machine Learning (ICML 2010). We competed in both the learning to rank and the transfer learning tracks of the challenge with several tree-based ensemble methods, including Tree Bagging (?), Random Forests (?), and Extremely Randomized Trees (?). Our methods ranked 10th in the first track and 4th in the second track. Although not at the very top of the ranking, our results show that ensembles of randomized trees are quite competitive for the âlearning to rankâ problem. The paper also analyzes computing times of our algorithms and presents some post-challenge experiments with transfer learning methods.}
}
@InProceedings{pmlr-v14-gulin11a,
  title = 	 {Winning The Transfer Learning Track of Yahoo!âs Learning To Rank Challenge with YetiRank},
  author = 	 {Gulin, Andrey and Kuralenok, Igor and Pavlov, Dimitry},
  booktitle = 	 {Proceedings of the Learning to Rank Challenge},
  pages = 	 {63--76},
  year = 	 {2011},
  editor = 	 {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
  volume = 	 {14},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Haifa, Israel},
  month = 	 {25 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v14/gulin11a/gulin11a.pdf},
  url = 	 {https://proceedings.mlr.press/v14/gulin11a.html},
  abstract = 	 {The problem of ranking the documents according to their relevance to a given query is a hot topic in information retrieval. Most learning-to-rank methods are supervised and use human editor judgements for learning. In this paper, we introduce novel pairwise method called YetiRank that modifies Friedmanâs gradient boosting method in part of gradient computation for optimization and takes uncertainty in human judgements into account. Proposed enhancements allowed YetiRank to outperform many state-of-the-art learning to rank methods in offline experiments as well as take the first place in the second track of the Yahoo! learning-to-rank contest. Even more remarkably, the first result in the learning to rank competition that consisted of a transfer learning task was achieved without ever relying on the bigger data from the âtransfer-fromâ domain.}
}
@InProceedings{pmlr-v14-mohan11a,
  title = 	 {Web-Search Ranking with Initialized Gradient Boosted Regression Trees},
  author = 	 {Mohan, Ananth and Chen, Zheng and Weinberger, Kilian},
  booktitle = 	 {Proceedings of the Learning to Rank Challenge},
  pages = 	 {77--89},
  year = 	 {2011},
  editor = 	 {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
  volume = 	 {14},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Haifa, Israel},
  month = 	 {25 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v14/mohan11a/mohan11a.pdf},
  url = 	 {https://proceedings.mlr.press/v14/mohan11a.html},
  abstract = 	 {In May 2010 Yahoo! Inc. hosted the Learning to Rank Challenge. This paper summarizes the approach by the highly placed team Washington University in St. Louis. We investigate Random Forests (RF) as a low-cost alternative algorithm to Gradient Boosted Regression Trees (GBRT) (the de facto standard of web-search ranking). We demonstrate that it yields surprisingly accurate ranking results â comparable to or better than GBRT. We combine the two algorithms by first learning a ranking function with RF and using it as initialization for GBRT. We refer to this setting as iGBRT. Following a recent discussion byÃÂ ?, we show that the results of iGBRT can be improved upon even further when the web-search ranking task is cast as classification instead of regression. We provide an upper bound of the Expected Reciprocal RankÃÂ (?) in terms of classification error and demonstrate that iGBRT outperforms GBRT and RF on the Microsoft Learning to Rank and Yahoo Ranking Competition data sets with surprising consistency.}
}
@InProceedings{pmlr-v14-chapelle11b,
  title = 	 {Future directions in learning to rank},
  author = 	 {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
  booktitle = 	 {Proceedings of the Learning to Rank Challenge},
  pages = 	 {91--100},
  year = 	 {2011},
  editor = 	 {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
  volume = 	 {14},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Haifa, Israel},
  month = 	 {25 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v14/chapelle11b/chapelle11b.pdf},
  url = 	 {https://proceedings.mlr.press/v14/chapelle11b.html},
  abstract = 	 {The results of the learning to rank challenge showed that the quality of the predictions from the top competitors are very close from each other. This raises a question: is learning to rank a solved problem? On the on hand, it is likely that only small incremental progress can be made in the âcoreâ and traditional problematics of learning to rank. The challenge was set in this standard learning to rank scenario: optimize a ranking measure on a test set. But on the other hand, there are a lot of related questions and settings in learning to rank that have not been yet fully explored. We review some of them in this paper and hope that researchers interested in learning to rank will try to answer these challenging and exciting research questions.}
}



