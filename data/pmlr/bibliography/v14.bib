@comment{@Proceedings{Learning to Rank Challenge2010,
  title =     {Proceedings of the Learning to Rank Challenge},
  booktitle = {Proceedings of the Learning to Rank Challenge},
  editor =    {Olivier Chapelle and Yi Chang and Tie-Yan Liu},
  publisher = {PMLR},
  series =    {Proceedings of Machine Learning Research},
  volume =    14
}}

@inproceedings{pmlr-v14-burges11a,
 abstract = {We describe the system that won Track 1 of the Yahoo! Learning to Rank Challenge. The Yahoo! Learning to Rank Challenge, Track 1, was a public competition on a Machine Learning for Information Retrieval task: given a set of queries, and given a set of retrieved documents for each query, train a system to maximize the Expected Reciprocal Rank (Chapelle et al., 2009) on a blind test set, where the training data takes the form of a feature vector x ∈ Rd with label y ∈ Y, Y ≡ {0,1,2,3,4} (a more positive number denoting higher relevance) for each query/document pair (the original, textual data was not made available). The Challenge setup, background information, and results have been extensively covered elsewhere and we refer to Chapelle and Chang (2011) for details. In this paper we summarize the work that resulted in the winning system.1 We limit the work described in this paper to the work done speciﬁcally for the Challenge; the work was done over a four week period prior to the end of the Challenge.},
 address = {Haifa, Israel},
 author = {Burges, Christopher and Svore, Krysta and Bennett, Paul and Pastusiak, Andrzej and Wu, Qiang},
 booktitle = {Proceedings of the Learning to Rank Challenge},
 editor = {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
 month = {25 Jun},
 openalex = {W2883070021},
 pages = {25--35},
 pdf = {http://proceedings.mlr.press/v14/burges11a/burges11a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning to Rank Using an Ensemble of Lambda-Gradient Models},
 url = {https://proceedings.mlr.press/v14/burges11a.html},
 volume = {14},
 year = {2011}
}

@inproceedings{pmlr-v14-busa-fekete11a,
 abstract = {This paper describes the ideas and methodologies that we used in the Yahoo learning-to- rank challenge1. Our technique is essentially pointwise with a listwise touch at the last combination step. The main ingredients of our approach are 1) preprocessing (querywise normalization) 2) multi-class AdaBoost.MH 3) regression calibration, and 4) an expo- nentially weighted forecaster for model combination. In post-challenge analysis we found that preprocessing and training AdaBoost with a wide variety of hyperparameters im- proved individual models significantly, the final listwise ensemble step was crucial, whereas calibration helped only in creating diversity.},
 address = {Haifa, Israel},
 author = {Busa-Fekete, RÃ³bert and KÃ©gl, BalÃ¡zs and ÃltetÅ, TamÃ¡s and Szarvas, GyÃ¶rgy},
 booktitle = {Proceedings of the Learning to Rank Challenge},
 editor = {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
 month = {25 Jun},
 openalex = {W2883372476},
 pages = {37--48},
 pdf = {http://proceedings.mlr.press/v14/busa-fekete11a/busa-fekete11a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Ranking by calibrated AdaBoost},
 url = {https://proceedings.mlr.press/v14/busa-fekete11a.html},
 volume = {14},
 year = {2011}
}

@inproceedings{pmlr-v14-chapelle11a,
 abstract = {Learning to rank for information retrieval has gained a lot of interest in the recent years but there is a lack for large real-world datasets to benchmark algorithms. That led us to publicly release two datasets used internally at Yahoo! for learning the web search ranking function. To promote these datasets and foster the development of state-of-the-art learning to rank algorithms, we organized the Yahoo! Learning to Rank Challenge in spring 2010. This paper provides an overview and an analysis of this challenge, along with a detailed description of the released datasets.},
 address = {Haifa, Israel},
 author = {Chapelle, Olivier and Chang, Yi},
 booktitle = {Proceedings of the Learning to Rank Challenge},
 editor = {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
 month = {25 Jun},
 openalex = {W2884475480},
 pages = {1--24},
 pdf = {http://proceedings.mlr.press/v14/chapelle11a/chapelle11a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Yahoo! Learning to Rank Challenge Overview},
 url = {https://proceedings.mlr.press/v14/chapelle11a.html},
 volume = {14},
 year = {2011}
}

@inproceedings{pmlr-v14-chapelle11b,
 abstract = {The results of the learning to rank challenge showed that the quality of the predictions from the top competitors are very close from each other. This raises a question: is learning to rank a solved problem? On the on hand, it is likely that only small incremental progress can be made in the core and traditional problematics of learning to rank. The challenge was set in this standard learning to rank scenario: optimize a ranking measure on a test set. But on the other hand, there are a lot of related questions and settings in learning to rank that have not been yet fully explored. We review some of them in this paper and hope that researchers interested in learning to rank will try to answer these challenging and exciting research questions.},
 address = {Haifa, Israel},
 author = {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
 booktitle = {Proceedings of the Learning to Rank Challenge},
 editor = {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
 month = {25 Jun},
 openalex = {W2884580467},
 pages = {91--100},
 pdf = {http://proceedings.mlr.press/v14/chapelle11b/chapelle11b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Future directions in learning to rank},
 url = {https://proceedings.mlr.press/v14/chapelle11b.html},
 volume = {14},
 year = {2011}
}

@inproceedings{pmlr-v14-geurts11a,
 abstract = {In this paper, we report on our experiments on the Yahoo! Labs Learning to Rank challenge organized in the context of the 23rd International Conference of Machine Learning (ICML 2010). We competed in both the to and the transfer tracks of the challenge with several tree-based ensemble methods, including Tree Bagging (Breiman, 1996), Random Forests (Breiman, 2001), and Extremely Randomized Trees (Geurts et al., 2006). Our methods ranked 10th in the first track and 4th in the second track. Although not at the very top of the ranking, our results show that ensembles of randomized trees are quite competitive for the learning to rank problem. The paper also analyzes computing times of our algorithms and presents some post-challenge experiments with transfer methods.},
 address = {Haifa, Israel},
 author = {Geurts, Pierre and Louppe, Gilles},
 booktitle = {Proceedings of the Learning to Rank Challenge},
 editor = {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
 month = {25 Jun},
 openalex = {W3194557597},
 pages = {49--61},
 pdf = {http://proceedings.mlr.press/v14/geurts11a/geurts11a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning to rank with extremely randomized trees},
 url = {https://proceedings.mlr.press/v14/geurts11a.html},
 volume = {14},
 year = {2011}
}

@inproceedings{pmlr-v14-gulin11a,
 abstract = {The problem of ranking the documents according to their relevance to a given query is a hot topic in information retrieval. Most learning-to-rank methods are supervised and use human editor judgements for learning. In this paper, we introduce novel pairwise method called YetiRank that modifies Friedmanâs gradient boosting method in part of gradient computation for optimization and takes uncertainty in human judgements into account. Proposed enhancements allowed YetiRank to outperform many state-of-the-art learning to rank methods in offline experiments as well as take the first place in the second track of the Yahoo! learning-to-rank contest. Even more remarkably, the first result in the learning to rank competition that consisted of a transfer learning task was achieved without ever relying on the bigger data from the âtransfer-fromâ domain.},
 address = {Haifa, Israel},
 author = {Gulin, Andrey and Kuralenok, Igor and Pavlov, Dimitry},
 booktitle = {Proceedings of the Learning to Rank Challenge},
 editor = {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
 month = {25 Jun},
 pages = {63--76},
 pdf = {http://proceedings.mlr.press/v14/gulin11a/gulin11a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Winning The Transfer Learning Track of Yahoo!âs Learning To Rank Challenge with YetiRank},
 url = {https://proceedings.mlr.press/v14/gulin11a.html},
 volume = {14},
 year = {2011}
}

@inproceedings{pmlr-v14-mohan11a,
 abstract = {In May 2010 Yahoo! Inc. hosted the Learning to Rank Challenge. This paper summarizes the approach by the highly placed team Washington University in St. Louis. We investigate Random Forests (RF) as a low-cost alternative algorithm to Gradient Boosted Regression Trees (GBRT) (the de facto standard of web-search ranking). We demonstrate that it yields surprisingly accurate ranking results -- comparable to or better than GBRT. We combine the two algorithms by first learning a ranking function with RF and using it as initialization for GBRT. We refer to this setting as iGBRT. Following a recent discussion by Li et al. (2007), we show that the results of iGBRT can be improved upon even further when the web-search ranking task is cast as classification instead of regression. We provide an upper bound of the Expected Reciprocal Rank (Chapelle et al., 2009) in terms of classification error and demonstrate that iGBRT outperforms GBRT and RF on the Microsoft Learning to Rank and Yahoo Ranking Competition data sets with surprising consistency.},
 address = {Haifa, Israel},
 author = {Mohan, Ananth and Chen, Zheng and Weinberger, Kilian},
 booktitle = {Proceedings of the Learning to Rank Challenge},
 editor = {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
 month = {25 Jun},
 openalex = {W2884843966},
 pages = {77--89},
 pdf = {http://proceedings.mlr.press/v14/mohan11a/mohan11a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Web-Search Ranking with Initialized Gradient Boosted Regression Trees},
 url = {https://proceedings.mlr.press/v14/mohan11a.html},
 volume = {14},
 year = {2011}
}
