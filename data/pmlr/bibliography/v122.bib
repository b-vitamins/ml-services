@proceedings{AFFCOMP2019,
 booktitle = {Proceedings of IJCAI 2019 3rd Workshop on Artificial Intelligence in Affective Computing},
 editor = {William Hsu},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of IJCAI 2019 3rd Workshop on Artificial Intelligence in Affective Computing},
 volume = {122}
}

@inproceedings{pmlr-v122-healey20a,
 abstract = {Individual affective responses frequently vary from the mean and often exhibit non-linear and time and sequence dependent properties.  This paper examines the extent to which commonly made assumptions of linearity and sequential independence are valid using skin conductance responses to an acoustic stimulus as an example.  We present 19 sessions of skin conductance traces where participants respond to five 50 millisecond acoustic bursts designed to elicit a startle.  We show the data from the perspective of an online algorithm: individual responses, non-linear and dependent on prior events.  We show that the coefficient of variation depends on sequence position and that these are large at 65%, 97%, 110%, and 100%.  We discuss the risk of making inferences on single impressions.},
 author = {Healey, Jennifer},
 booktitle = {Proceedings of IJCAI 2019 3rd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William},
 month = {10 Aug},
 openalex = {W3114534886},
 pages = {1--8},
 pdf = {http://proceedings.mlr.press/v122/healey20a/healey20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sequential Dependence and Non-linearity in Affective Responses: a Skin Conductance Example.},
 url = {https://proceedings.mlr.press/v122/healey20a.html},
 volume = {122},
 year = {2020}
}

@inproceedings{pmlr-v122-lee20a,
 abstract = {Detecting facial action unit (AU) activations is one of the key
steps in automatic recognition of facial expressions of human
emotion and cognitive states.  While there are different approaches
proposed for this task, most of these are trained only for a
specific (sub)set of AUs. As such, they cannot easily adapt to the
task of detection of new AUs which are not initially used to train
the target models.  In this paper, we propose a deep learning
approach for facial AU detection that can adapt to a new AU and/or
target subject by leveraging only a few labeled samples from the new
task (either an AU or subject). We use the notion of the
model-agnostic meta-learning, originally proposed for the general
image recognition/detection tasks, to design our deep learning
models for AU detection. Specifically, each subject and/or AU is
treated as a new learning task and the model learns to adapt based
on the knowledge of the previously seen tasks. We show on two
benchmark datasets (BP4D and DISFA) for facial AU detection that the
proposed approach can easily be adapted to new tasks. By using as
few as one or five labeled examples from the target task, our
approach achieves large improvements over the baseline (non-adapted)
deep models.},
 author = {Lee, Mihee and Rudovic, Ognjen and Pavlovic, Vladimir and Pantic, Maja},
 booktitle = {Proceedings of IJCAI 2019 3rd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William},
 month = {10 Aug},
 pages = {9--27},
 pdf = {http://proceedings.mlr.press/v122/lee20a/lee20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Fast Adaptation of Deep Models for Facial Action Unit Detection Using Model-Agnostic Meta-Learning},
 url = {https://proceedings.mlr.press/v122/lee20a.html},
 volume = {122},
 year = {2020}
}

@inproceedings{pmlr-v122-niibori20a,
 abstract = {In this paper we propose a method of measuring the communication between two people by analyzing their headsâ information: head pose, gaze vectors and facial action units. Assuming two people are sitting around a table, an omnidirectional camera is used to observe the two people simultaneously.Next, the visual cues of the heads of the two people, including head pose, gaze vectors and facial action units, are extracted using a popular facial behavior analysis toolkit, OpenFace. Then,  a LSTM (Long Short Term Memory) neural network is used to learn measuring the communication between the two people from the temporal sequence of the extracted head information. The preliminary experimental results show the effectiveness of the proposed method.},
 author = {Niibori, Yui and Li, Shigang},
 booktitle = {Proceedings of IJCAI 2019 3rd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William},
 month = {10 Aug},
 openalex = {W3113639103},
 pages = {28--35},
 pdf = {http://proceedings.mlr.press/v122/niibori20a/niibori20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Measuring Two-People Communication from Omnidirectional Video.},
 url = {https://proceedings.mlr.press/v122/niibori20a.html},
 volume = {122},
 year = {2020}
}

@inproceedings{pmlr-v122-sinha20a,
 abstract = {This paper presents preliminary results for developing an online "persuasion score" that will enable digital marketing content authors to compose and edit materials with better persuasive capability.  Inspired by initial insights with digital marketing professionals and research on the foundations of persuasion: pathos, ethos and logos, we extracted features from a data set of over three million consumer reactions to email marketing campaigns covering a three month period.  We report on the most significant features of the content, including image position and text readability as well as the most salient customer features such as time since registration and time since last opened email from the same marketing brand.},
 author = {Sinha, Moumita and Healey, Jennifer and Ahmad, Faran and Gupta, Varun and Ganguly, Niloy},
 booktitle = {Proceedings of IJCAI 2019 3rd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William},
 month = {10 Aug},
 pages = {36--43},
 pdf = {http://proceedings.mlr.press/v122/sinha20a/sinha20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Persuasion: What {J}ane {A}ustin Would Have Written},
 url = {https://proceedings.mlr.press/v122/sinha20a.html},
 volume = {122},
 year = {2020}
}

@inproceedings{pmlr-v122-tripathi20a,
 abstract = {This paper proposes a Convolutional Neural Network (CNN) inspired by Multitask Learning (MTL) and based on speech features trained under the joint supervision of softmax loss and center loss, a powerful metric learning strategy, for the recognition of emotion in speech. Speech features such as Spectrograms and Mel-frequency Cepstral Coefficient s (MFCCs) help retain emotion-related low-level characteristics in speech. We experimented with several Deep Neural Network (DNN) architectures that take in speech features as input and trained them under both softmax and center loss, which resulted in highly discriminative features ideal for Speech Emotion Recognition (SER). Our networks also employ a regularizing effect by simultaneously performing the auxiliary task of reconstructing the input speech features. This sharing of representations among related tasks enables our network to better generalize the original task of SER. Some of our proposed networks contain far fewer parameters when compared to state-of-the-art architectures.},
 author = {Tripathi, Suraj and Ramesh, Abhiram and Kumar, Abhay and Singh, Chirag and Yenigalla, Promod},
 booktitle = {Proceedings of IJCAI 2019 3rd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William},
 month = {10 Aug},
 openalex = {W4297818784},
 pages = {44--53},
 pdf = {http://proceedings.mlr.press/v122/tripathi20a/tripathi20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Discriminative features using Center Loss and Reconstruction as Regularizer for Speech Emotion Recognition},
 url = {https://proceedings.mlr.press/v122/tripathi20a.html},
 volume = {122},
 year = {2020}
}
