@comment{@Proceedings{EWRL 20122012,
  title =     {Proceedings of the Tenth European Workshop on Reinforcement Learning},
  booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
  editor =    {Marc Peter Deisenroth and Csaba SzepesvÃ¡ri and Jan Peters},
  publisher = {PMLR},
  series =    {Proceedings of Machine Learning Research},
  volume =    24
}}

@inproceedings{pmlr-v24-castronovo12a,
 abstract = {We consider the problem of learning high-performance Exploration/Exploitation (E/E) strategies for finite Markov Decision Processes (MDPs) when the MDP to be controlled is supposed to be drawn from a known probability distribution pM(·). The performance criterion is the sum of discounted rewards collected by the E/E strategy over an infinite length trajectory. We propose an approach for solving this problem that works by considering a rich set of candidate E/E strategies and by looking for the one that gives the best average performances on MDPs drawn according to pM(·). As candidate E/E strategies, we consider index-based strategies parametrized by small formulas combining variables that include the estimated reward function, the number of times each transition has occurred and the optimal value functions ˆ V and ˆ Q of the estimated MDP (obtained through value iteration). The search for the best formula is formalized as a multi-armed bandit problem, each arm being associated with a formula. We experimentally compare the performances of the approach with R-max as well as with ǫ-Greedy strategies and the results are promising.},
 address = {Edinburgh, Scotland},
 author = {Castronovo, Michael and Maes, Francis and Fonteneau, Raphael and Ernst, Damien},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2125526403},
 pages = {1--10},
 pdf = {http://proceedings.mlr.press/v24/castronovo12a/castronovo12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Exploration/Exploitation Strategies for Single Trajectory Reinforcement Learning},
 url = {https://proceedings.mlr.press/v24/castronovo12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-daswani12a,
 abstract = {There has recently been much interest in history-based methods using suffix trees to solve POMDPs. However, these suffix trees cannot efficiently represent environments that have long-term dependencies. We extend the recently introduced CTMDP algorithm to the space of looping suffix trees which have previously only been used in solving determinis- tic POMDPs. The resulting algorithm replicates results from CTMDP for environments with short term dependencies, while it outperforms LSTM-based methods on TMaze, a deep memory environment.},
 address = {Edinburgh, Scotland},
 author = {Daswani, Mayank and Sunehag, Peter and Hutter, Marcus},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2150294308},
 pages = {11--24},
 pdf = {http://proceedings.mlr.press/v24/daswani12a/daswani12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feature Reinforcement Learning using Looping Suffix Trees},
 url = {https://proceedings.mlr.press/v24/daswani12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-deisenroth12a,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 address = {Edinburgh, Scotland},
 author = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W4240465921},
 pages = {i--i},
 pdf = {http://proceedings.mlr.press/v24/deisenroth12a/deisenroth12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v24/deisenroth12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-goschin12a,
 abstract = {In some decision-making environments, successful solutions are common. If the evaluation of candidate solutions is noisy, however, the challenge is knowing when a “good enough” answer has been found. We formalize this problem as an infinite-armed bandit and provide upper and lower bounds on the number of evaluations or “pulls” needed to identify a solution whose evaluation exceeds a given threshold r0. We present several algorithms and use them to identify reliable strategies for solving screens from the video games Infinite Mario and Pitfall! We show order of magnitude improvements in sample complexity over a natural approach that pulls each arm until a good estimate of its success probability is known.},
 address = {Edinburgh, Scotland},
 author = {Goschin, Sergiu and Weinstein, Ari and Littman, Michael L. and Chastain, Erick},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2184118689},
 pages = {25--42},
 pdf = {http://proceedings.mlr.press/v24/goschin12a/goschin12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Planning in Reward-Rich Domains via PAC Bandits},
 url = {https://proceedings.mlr.press/v24/goschin12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-heess12a,
 abstract = {We consider reinforcement learning in Markov decision processes with high dimensional state and action spaces. We parametrize policies using energy-based models (particularly restricted Boltzmann machines), and train them using policy gradient learning. Our approach builds upon Sallans and Hinton (2004), who parameterized value functions using energy-based models, trained using a non-linear variant of temporal-di!erence (TD) learning. Unfortunately, non-linear TD is known to diverge in theory and practice. We introduce the first sound and ecient algorithm for training energy-based policies, based on an actorcritic architecture. Our algorithm is computationally ecient, converges close to a local optimum, and outperforms Sallans and Hinton (2004) in several high dimensional domains.},
 address = {Edinburgh, Scotland},
 author = {Heess, Nicolas and Silver, David and Teh, Yee Whye},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2182304831},
 pages = {45--58},
 pdf = {http://proceedings.mlr.press/v24/heess12a/heess12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Actor-Critic Reinforcement Learning with Energy-Based Policies},
 url = {https://proceedings.mlr.press/v24/heess12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-mann12a,
 abstract = {Experimental results suggest that transferred knowledge can reduce the number of exploratory actions needed by reinforcement learning (RL) algorithms to nd acceptable solutions in Markov decision processes compared to learning from scratch. However, most existing transfer learning algorithms for RL are heuristic and transferred knowledge may unexpectedly result in worse performance than learning from scratch (i.e., negative transfer). We introduce a transfer learning algorithm that employs directed exploration, which allows us to motivate our algorithm by analyzing its sample complexity of exploration in the target task. We dene positive and negative transfer from a sample complexity perspective and provide conditions when our algorithm will avoid negative transfer as well as conditions where our algorithm guarantees positive transfer, with high probability. Finally, we demonstrate the advantages of our algorithm experimentally.},
 address = {Edinburgh, Scotland},
 author = {Mann, Timothy A. and Choe, Yoonsuck},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2403765497},
 pages = {59--76},
 pdf = {http://proceedings.mlr.press/v24/mann12a/mann12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Directed Exploration in Reinforcement Learning with Transferred Knowledge},
 url = {https://proceedings.mlr.press/v24/mann12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-metzen12a,
 abstract = {We introduce a new online skill discovery method for reinforcement learning in discrete domains. The method is based on the bottleneck principle and identifies skills using a bottom-up hierarchical clustering of the estimated transition graph. In contrast to prior clustering approaches, it can be used incrementally and thus several times during the learning process. Our empirical evaluation shows that “assuming dense local connectivity in the face of uncertainty” can prevent premature identification of skills. Furthermore, we show that the choice of the linkage criterion is crucial for dealing with non-random sampling policies and stochastic environments.},
 address = {Edinburgh, Scotland},
 author = {Metzen, Jan Hendrik},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2188537882},
 pages = {77--88},
 pdf = {http://proceedings.mlr.press/v24/metzen12a/metzen12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Online Skill Discovery using Graph-based Clustering},
 url = {https://proceedings.mlr.press/v24/metzen12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-paduraru12a,
 abstract = {Off-policy evaluation is the problem of evaluating a decision-making policy using data collected under a different behaviour policy. While several methods are available for addressing off-policy evaluation, little work has been done on identifying the best methods. In this paper, we conduct an in-depth comparative study of several off-policy evaluation methods in non-bandit, finite-horizon MDPs, using randomly generated MDPs, as well as a Mallard population dynamics model [Anderson, 1975] . We find that un-normalized importance sampling can exhibit prohibitively large variance in problems involving look-ahead longer than a few time steps, and that dynamic programming methods perform better than Monte-Carlo style methods.},
 address = {Edinburgh, Scotland},
 author = {PÄduraru, Cosmin and Precup, Doina and Pineau, Joelle and ComÄnici, Gheorghe},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2136723863},
 pages = {89--102},
 pdf = {http://proceedings.mlr.press/v24/paduraru12a/paduraru12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An Empirical Analysis of Off-policy Learning in Discrete MDPs},
 url = {https://proceedings.mlr.press/v24/paduraru12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-seldin12a,
 abstract = {EXP3 is a popular algorithm for adversarial multiarmed bandits, suggested and analyzed in this setting by Auer et al. [2002b]. Recently there was an increased interest in the performance of this algorithm in the stochastic setting, due to its new applications to stochastic multiarmed bandits with side information [Seldin et al., 2011] and to multiarmed bandits in the mixed stochastic-adversarial setting [Bubeck and Slivkins, 2012]. We present an empirical evaluation and improved analysis of the performance of the EXP3 algorithm in the stochastic setting, as well as a modification of the EXP3 algorithm capable of achieving “logarithmic” regret in stochastic environments.},
 address = {Edinburgh, Scotland},
 author = {Seldin, Yevgeny and SzepesvÃ¡ri, Csaba and Auer, Peter and Abbasi-Yadkori, Yasin},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W659523800},
 pages = {103--116},
 pdf = {http://proceedings.mlr.press/v24/seldin12a/seldin12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Evaluation and Analysis of the Performance of the EXP3 Algorithm in Stochastic Environments},
 url = {https://proceedings.mlr.press/v24/seldin12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-silver12a,
 abstract = {Temporal-difference (TD) networks (Sutton and Tanner, 2004) are a predictive representation of state in which each node is an answer to a question about future observations or questions. Unfortunately, existing algorithms for learning TD networks are known to diverge, even in very simple problems. In this paper we present the first sound learning rule for TD networks. Our approach is to develop a true gradient descent algorithm that takes account of all three roles performed by each node in the network: as state, as an answer, and as a target for other questions. Our algorithm combines gradient temporal-difference learning (Maei et al., 2009) with real-time recurrent learning (Williams and Zipser, 1994). We provide a generalisation of the Bellman equation that corresponds to the semantics of the TD network, and prove that our algorithm converges to a fixed point of this equation.},
 address = {Edinburgh, Scotland},
 author = {Silver, David},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2189622310},
 pages = {117--130},
 pdf = {http://proceedings.mlr.press/v24/silver12a/silver12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Gradient Temporal Difference Networks},
 url = {https://proceedings.mlr.press/v24/silver12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-valko12a,
 abstract = {In apprenticeship learning we aim to learn a good policy by observing the behavior of an expert or a set of experts. In particular, we consider the case where the expert acts so as to maximize an unknown reward function defined as a linear combination of a set of state features. In this paper, we consider the setting where we observe many sample trajectories (i.e., sequences of states) but only one or a few of them are labeled as experts’ trajectories. We investigate the conditions under which the remaining unlabeled trajectories can help in learning a policy with a good performance. In particular, we define an extension to the max-margin inverse reinforcement learning proposed by Abbeel and Ng [2004] where, at each iteration, the max-margin optimization step is replaced by a semi-supervised optimization problem which favors classifiers separating clusters of trajectories. Finally, we report empirical results on two grid-world domains showing that the semi-supervised algorithm is able to output a better policy in fewer iterations than the related algorithm that does not take the unlabeled trajectories into account.},
 address = {Edinburgh, Scotland},
 author = {Valko, Michal and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2101230485},
 pages = {131--142},
 pdf = {http://proceedings.mlr.press/v24/valko12a/valko12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Semi-Supervised Apprenticeship Learning},
 url = {https://proceedings.mlr.press/v24/valko12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-vlachos12a,
 abstract = {In the imitation learning paradigm algorithms learn from expert demonstrations in order to become able to accomplish a particular task. Daume III et al. [2009] framed structured prediction in this paradigm and developed the search-based structured prediction algorithm (Searn) which has been applied successfully to various natural language processing tasks with state-of-the-art performance. Recently, Ross et al. [2011] proposed the dataset aggregation algorithm (DAgger) and compared it with Searn in sequential prediction tasks. In this paper, we compare these two algorithms in the context of a more complex structured prediction task, namely biomedical event extraction. We demonstrate that DAgger has more stable performance and faster learning than Searn, and that these advantages are more pronounced in the parameter-free versions of the algorithms.},
 address = {Edinburgh, Scotland},
 author = {Vlachos, Andreas},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2295840606},
 pages = {143--154},
 pdf = {http://proceedings.mlr.press/v24/vlachos12a/vlachos12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An investigation of imitation learning algorithms for structured prediction.},
 url = {https://proceedings.mlr.press/v24/vlachos12a.html},
 volume = {24},
 year = {2013}
}

@inproceedings{pmlr-v24-weinstein12a,
 abstract = {Recently, rollout-based planning and search methods have emerged as an alternative to traditional tree-search methods. The fundamental operation in rollout-based tree search is the generation of trajectories in the search tree from root to leaf. Game-playing programs based on Monte-Carlo rollouts methods such as UCT have proven remarkably effective at using information from trajectories to make state-of-the-art decisions at the root. In this paper, we show that trajectories can be used to prune more aggressively than classical alpha-beta search. We modify a rollout-based method, FSSS, to allow for use in game-tree search and show it outprunes alpha-beta both empirically and formally.},
 address = {Edinburgh, Scotland},
 author = {Weinstein, Ari and Littman, Michael L. and Goschin, Sergiu},
 booktitle = {Proceedings of the Tenth European Workshop on Reinforcement Learning},
 editor = {Deisenroth, Marc Peter and SzepesvÃ¡ri, Csaba and Peters, Jan},
 month = {30 Jun--01 Jul},
 openalex = {W2188928203},
 pages = {155--167},
 pdf = {http://proceedings.mlr.press/v24/weinstein12a/weinstein12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Rollout-based Game-tree Search Outprunes Traditional Alpha-beta},
 url = {https://proceedings.mlr.press/v24/weinstein12a.html},
 volume = {24},
 year = {2013}
}
