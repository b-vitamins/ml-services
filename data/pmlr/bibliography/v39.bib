@proceedings{ACML2014,
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Dinh Phung and Hang Li},
 openalex = {W2565425721},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Volume 39: proceedings of the sixth Asian conference on machine learning},
 volume = {39}
}

@inproceedings{pmlr-v39-alabdulmohsin14,
 abstract = {Training support vector machines (SVM) with indefinite kernels has recently attracted attention in the machine learning community. This is partly due to the fact that many similarity functions that arise in practice are not symmetric positive semidefinite, i.e. the Mercer condition is not satisfied, or the Mercer condition is difficult to verify. Previous work on training SVM with indefinite kernels has generally fallen into three categories: (1) positive semidefinite kernel approximation, (2) non-convex optimization, and (3) learning in Krein spaces. All approaches are not fully satisfactory. They have either introduced sources of inconsistency in handling training and test examples using kernel approximation, settled for approximate local minimum solutions using non-convex optimization, or produced nonsparse solutions. In this paper, we establish both theoretically and experimentally that the 1-norm SVM, proposed more than 10 years ago for embedded feature selection, is a better solution for extending SVM to indefinite kernels. More specifically, 1-norm SVM can be interpreted as a structural risk minimization method that seeks a decision boundary with large similarity margin in the original space. It uses a linear programming formulation that remains convex even if the kernel matrix is indefinite, and hence can always be solved quite efficiently. Also, it uses the indefinite similarity function (or distance) directly without any transformation, and, hence, it always treats both training and test examples consistently. Finally, it achieves the highest accuracy among all methods that train SVM with indefinite kernels with a statistically significant evidence while also retaining sparsity of the support vector set.},
 address = {Nha Trang City, Vietnam},
 author = {Alabdulmohsin, Ibrahim and Gao, Xin and Zhang, Xiangliang Zhang},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W271499721},
 pages = {32--47},
 pdf = {http://proceedings.mlr.press/v39/alabdulmohsin14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Support vector machines with indefinite kernels},
 url = {https://proceedings.mlr.press/v39/alabdulmohsin14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-antoniuk14,
 abstract = {We address a problem of learning ordinal classifiers from partially annotated examples. We introduce a V-shaped interval-insensitive loss function to measure discrepancy between predictions of an ordinal classifier and a partial annotation provided in the form of intervals of candidate labels. We show that under reasonable assumptions on the annotation process the Bayes risk of the ordinal classifier can be bounded by the expectation of an associated interval-insensitive loss. We propose several convex surrogates of the interval-insensitive loss which are used to formulate convex learning problems. We described a variant of the cutting plane method which can solve large instances of the learning problems. Experiments on a real-life application of human age estimation show that the ordinal classifier learned from cheap partially annotated examples can achieve accuracy matching the results of the so-far used supervised methods which require expensive precisely annotated examples.},
 address = {Nha Trang City, Vietnam},
 author = {Antoniuk, Kostiantyn and Franc, Vojtech and Hlavac, Vaclav},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2225869249},
 pages = {189--204},
 pdf = {http://proceedings.mlr.press/v39/antoniuk14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {V-shaped interval insensitive loss for ordinal classification},
 url = {https://proceedings.mlr.press/v39/antoniuk14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-auger14,
 abstract = {Solving zero-sum matrix games is polynomial, because it boils down to linear programming. The approximate solving is sublinear by randomized algorithms on machines with random access memory. Algorithms working separately and independently on columns and rows have been proposed, with the same performance; these versions are compliant with matrix games with stochastic reward. (Flory and Teytaud, 2011) has proposed a new version, empirically performing better on sparse problems, i.e. cases in which the Nash equilibrium has small support. In this paper, we propose a variant, similar to their work, also dedicated to sparse problems, with provably better bounds than existing methods. We then experiment the method on a card game.},
 address = {Nha Trang City, Vietnam},
 author = {Auger, David and Liu, Jianlin and Ruette, Sylkvie and Saint-Pierre, David and Teytaud, Oliver},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2211145119},
 pages = {173--188},
 pdf = {http://proceedings.mlr.press/v39/auger14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sparse Binary Zero-Sum Games},
 url = {https://proceedings.mlr.press/v39/auger14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-canevet14a,
 abstract = {We propose a novel approach to efficiently select informative samples for large-scale learning. Instead of directly feeding a learning algorithm with a very large amount of samples, as it is usually done to reach state-of-the-art performance, we have developed a “distillation” procedure to recursively reduce the size of an initial training set using a criterion that ensures the maximization of the information content of the selected sub-set. We demonstrate the performance of this procedure for two different computer vision problems. First, we show that distillation can be used to improve the traditional bootstrapping approach to object detection. Second, we apply distillation to a classification problem with artificial distortions. We show that in both cases, using the result of a distillation process instead of a random sub-set taken uniformly in the original sample set improves performance significantly.},
 address = {Nha Trang City, Vietnam},
 author = {Canevet, Olivier and Fleuret, Francois},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W227501432},
 pages = {48--63},
 pdf = {http://proceedings.mlr.press/v39/canevet14a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Efficient Sample Mining for Object Detection},
 url = {https://proceedings.mlr.press/v39/canevet14a.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-canevet14b,
 abstract = {We propose a novel approach to eciently select informative samples for large-scale learning. Instead of directly feeding a learning algorithm with a very large amount of samples, as it is usually done to reach state-of-the-art performance, we have developed a \distillation procedure to recursively reduce the size of an initial training set using a criterion that ensures the maximization of the information content of the selected sub-set. We demonstrate the performance of this procedure for two dierent computer vision problems. First, we show that distillation can be used to improve the traditional bootstrapping approach to object detection. Second, we apply distillation to a classication problem with articial distortions. We show that in both cases, using the result of a distillation process instead of a random sub-set taken uniformly in the original sample set improves performance signicantly.},
 address = {Nha Trang City, Vietnam},
 author = {Canevet, Olivier and Lefakis, Leonidas and Fleuret, Francois},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2169448486},
 pages = {64--79},
 pdf = {http://proceedings.mlr.press/v39/canevet14b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sample Distillation for Object Detection and Image Classification},
 url = {https://proceedings.mlr.press/v39/canevet14b.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-chou14,
 abstract = {We study the contextual bandit problem with linear payoff functions, which is a generalization of the traditional multi-armed bandit problem. In the contextual bandit problem, the learner needs to iteratively select an action based on an observed context, and receives a linear score on only the selected action as the reward feedback. Motivated by the observation that better performance is achievable if the other rewards on the non-selected actions can also be revealed to the learner, we propose a new framework that feeds the learner with pseudo-rewards, which are estimates of the rewards on the non-selected actions. We argue that the pseudo-rewards should better contain over-estimates of the true rewards, and propose a forgetting mechanism to decrease the negative influence of the over-estimation in the long run. Then, we couple the two key ideas above with the linear upper confidence bound (LinUCB) algorithm to design a novel algorithm called linear pseudo-reward upper confidence bound (LinPRUCB). We prove that LinPRUCB shares the same order of regret bound to LinUCB, while enjoying the practical observation of faster reward-gathering in the earlier iterations. Experiments on artificial and real-world data sets justify that LinPRUCB is competitive to and sometimes even better than LinUCB. Furthermore, we couple LinPRUCB with a special parameter to formalize a new algorithm that yields faster computation in updating the internal models while keeping the promising practical performance. The two properties match the real-world needs of the contextual bandit problem and make the new algorithm a favorable choice in practice.},
 address = {Nha Trang City, Vietnam},
 author = {Chou, Ku-Chun and Lin, Hsuan-Tien and Chiang, Chao-Kai and Lu, Chi-Jen},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W343811141},
 pages = {344--359},
 pdf = {http://proceedings.mlr.press/v39/chou14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Pseudo-reward Algorithms for Contextual Bandits with Linear Payoff Functions},
 url = {https://proceedings.mlr.press/v39/chou14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-daswani14,
 abstract = {The problem we consider in this paper is reinforcement learning with value advice. In this setting, the agent is given limited access to an oracle that can tell it the expected return (value) of any state-action pair with respect to the optimal policy. The agent must use this value to learn an explicit policy that performs well in the environment. We provide an algorithm called RLAdvice, based on the imitation learning algorithm DAgger. We illustrate the eectiveness of this method in the Arcade Learning Environment on three dierent games, using value estimates from UCT as advice.},
 address = {Nha Trang City, Vietnam},
 author = {Daswani, Mayank and Sunehag, Peter and Hutter, Marcus},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2099308455},
 pages = {299--314},
 pdf = {http://proceedings.mlr.press/v39/daswani14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Reinforcement learning with value advice},
 url = {https://proceedings.mlr.press/v39/daswani14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-givchi14,
 abstract = {Fast convergent and computationally inexpensive policy evaluation is an essential part of reinforcement learning algorithms based on policy iteration. Algorithms such as LSTD, LSPE, FPKF and NTD, have faster convergence rates but they are computationally slow. On the other hand, there are algorithms that are computationally fast but with slower convergence rate, among them are TD, RG, GTD2 and TDC. This paper presents a regularized Quasi Newton Temporal Dierence learning algorithm which uses second-order information while maintaining a fast convergence rate. In simple language, we combine the idea of TD learning with quasi Newton algorithm SGD-QN. We explore the development of QNTD algorithm and discuss its convergence properties. We support our ideas with empirical results on four standard benchmarks in reinforcement learning literature with two small problems, Random Walk and Boyan chain and two bigger problems, cart-pole and linked-pole balancing. Empirical studies show that QNTD speeds up convergence and provides better accuracy in comparison to the conventional TD.},
 address = {Nha Trang City, Vietnam},
 author = {Givchi, Arash and Palhang, Maziar},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2151568021},
 pages = {159--172},
 pdf = {http://proceedings.mlr.press/v39/givchi14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Quasi Newton Temporal Difference Learning},
 url = {https://proceedings.mlr.press/v39/givchi14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-kimura14,
 abstract = {Nonnegative Matrix Factorization (NMF) is a popular technique in a variety of fields due to its component-based representation with physical interpretablity. NMF finds a nonnegative hidden structures as oblique bases and coefficients. Recently, Orthogonal NMF (ONMF), imposing an orthogonal constraint into NMF, has been gathering a great deal of attention. ONMF is more appropriate for the clustering task because the resultant constrained matrix consisting of the coefficients can be considered as an indicator matrix. All traditional ONMF algorithms are based on multiplicative update rules or project gradient descent method. However, these algorithms are slow in convergence compared with the state-ofthe-art algorithms used for regular NMF. This is because they update a matrix in each iteration step. In this paper, therefore, we propose to update the current matrix columnwisely using Hierarchical Alternating Least Squares (HALS) algorithm that is typically used for NMF. The orthogonality and nonnegativity constraints are both utilized efficiently in the column-wise update procedure. Through experiments on six real-life datasets, it was shown that the proposed algorithm converges faster than the other conventional ONMF algorithms due to a smaller number of iterations, although the theoretical complexity is the same. It was also shown that the orthogonality is also attained in an earlier stage.},
 address = {Nha Trang City, Vietnam},
 author = {Kimura, Keigo and Tanaka, Yuzuru and Kudo, Mineichi},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W3036037941},
 pages = {129--141},
 pdf = {http://proceedings.mlr.press/v39/kimura14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A fast hierarchical alternating least squares algorithm for orthogonal nonnegative matrix factorization},
 url = {https://proceedings.mlr.press/v39/kimura14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-klami14,
 abstract = {Bayesian inference for latent factor models, such as principal component and canonical correlation analysis, is easy for Gaussian likelihoods. In particular, full conjugacy makes both Gibbs samplers and mean-field variational approximations straightforward. For other likelihood potentials one needs to either resort to more complex sampling schemes or to specifying dedicated forms of variational lower bounds. Recently, however, it was shown that for specific likelihoods related to the logistic function it is possible to augment the joint density with auxiliary variables following a Polya-Gamma distribution, leading to closed-form updates for binary and over-dispersed count models. In this paper we describe how Gibbs sampling and mean-field variational approximation for various latent factor models can be implemented for these cases, presenting easy-to-implement and efficient inference schemas.},
 address = {Nha Trang City, Vietnam},
 author = {Klami, Arto},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 pages = {112--128},
 pdf = {http://proceedings.mlr.press/v39/klami14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {{P}olya-gamma augmentations for factor models},
 url = {https://proceedings.mlr.press/v39/klami14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-ko14,
 abstract = {Bilinear models of count data with Poisson distribution are popular in applications such as matrix factorization for recommendation systems, modeling of receptive fields of sensory neurons, and modeling of neural-spike trains. Bayesian inference in such models remains challenging due to the product term of two Gaussian random vectors. In this paper, we propose new algorithms for such models based on variational Gaussian (VG) inference. We make two contributions. First, we show that the VG lower bound for these models, previously known to be intractable, is available in closed form under certain non-trivial constraints on the form of the posterior. Second, we show that the lower bound is biconcave and can be efficiently optimized for mean-field approximations. We also show that bi-concavity generalizes to the larger family of log-concave likelihoods, that subsume the Poisson distribution. We present new inference algorithms based on these results and demonstrate better performance on real-world problems at the cost of a modest increase in computation. Our contributions in this paper, therefore, provide more choices for Bayesian inference in terms of a speed-vs-accuracy tradeoff.},
 address = {Nha Trang City, Vietnam},
 author = {Ko, Young-Jun and Khan, Mohammad},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2243234527},
 pages = {330--343},
 pdf = {http://proceedings.mlr.press/v39/ko14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variational Gaussian Inference for Bilinear Models of Count Data},
 url = {https://proceedings.mlr.press/v39/ko14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-lim14,
 abstract = {Bibliographic analysis considers author’s research areas, the citation network and paper content among other things. In this paper, we combine these three in a topic model that produces a bibliographic model of authors, topics and documents using a non-parametric extension of a combination of the Poisson mixed-topic link model and the author-topic model. We propose a novel and ecient inference algorithm for the model to explore subsets of research publications from CiteSeer X . Our model demonstrates improved performance in both model tting and a clustering task compared to several baselines.},
 address = {Nha Trang City, Vietnam},
 author = {Lim, Kar Wai and Buntine, Wray},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2118111139},
 pages = {142--158},
 pdf = {http://proceedings.mlr.press/v39/lim14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Bibliographic Analysis with the Citation Network Topic Model},
 url = {https://proceedings.mlr.press/v39/lim14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-lin14,
 abstract = {Many real-world applications require varying costs for dierent types of mis-classication errors. Such a cost-sensitive classication setup can be very dierent from the regular classication one, especially in the multiclass case. Thus, traditional meta-algorithms for regular multiclass classication, such as the popular one-versus-one approach, may not always work well under the cost-sensitive classication setup. In this paper, we extend the one-versus-one approach to the eld of cost-sensitive classication. The extension is derived using a rigorous mathematical tool called the cost-transformation technique, and takes the original one-versus-one as a special case. Experimental results demonstrate that the proposed approach can achieve better performance in many cost-sensitive classication scenarios when compared with the original one-versus-one as well as existing cost-sensitive classication algorithms.},
 address = {Nha Trang City, Vietnam},
 author = {Lin, Hsuan-Tien},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2135219455},
 pages = {371--386},
 pdf = {http://proceedings.mlr.press/v39/lin14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Reduction from Cost-Sensitive Multiclass Classification to One-versus-One Binary Classification},
 url = {https://proceedings.mlr.press/v39/lin14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-liu14,
 abstract = {Recommender Systems heavily rely on numerical preferences, whereas the importance of ordinal preferences has only been recognised in recent works of Ordinal Matrix Factorisation (OMF). Although the OMF can eectively exploit ordinal properties, it captures only the higher-order interactions among users and items, without considering the localised interactions properly. This paper employs Markov Random Fields (MRF) to investigate the localised interactions, and proposes a unied model called Ordinal Random Fields (ORF) to take advantages of both the representational power of the MRF and the ease of modelling ordinal preferences by the OMF. Experimental result on public datasets demonstrates that the proposed ORF model can capture both types of interactions, resulting in improved recommendation accuracy.},
 address = {Nha Trang City, Vietnam},
 author = {Liu, Shaowu and Tran, Truyen and Li, Gang},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W1587275696},
 pages = {283--298},
 pdf = {http://proceedings.mlr.press/v39/liu14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Ordinal Random Fields for Recommender Systems},
 url = {https://proceedings.mlr.press/v39/liu14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-lu14,
 abstract = {We investigate online active learning techniques for classification tasks in data stream mining applications. Unlike traditional learning approaches (either batch or online learning) that often require to request the class label of each incoming instance, online active learning queries only a subset of informative incoming instances to update the classification model, which aims to maximize classification performance using minimal human labeling effort during the entire online stream data mining task. In this paper, we present a new family of algorithms for online active learning called Passive-Aggressive Active (PAA) learning algorithms by adapting the popular Passive-Aggressive algorithms in an online active learning setting. Unlike the conventional Perceptron-based approach that employs only the misclassified instances for updating the model, the proposed PAA learning algorithms not only use the misclassified instances to update the classifier, but also exploit correctly classified examples with low prediction confidence. We theoretically analyse the mistake bounds of the proposed algorithms and conduct extensive experiments to examine their empirical performance, in which encouraging results show clear advantages of our algorithms over the baselines.},
 address = {Nha Trang City, Vietnam},
 author = {Lu, Jing and Zhao, Peilin and Hoi, Steven},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W331367574},
 pages = {266--282},
 pdf = {http://proceedings.mlr.press/v39/lu14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Online Passive Aggressive Active Learning and Its Applications},
 url = {https://proceedings.mlr.press/v39/lu14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-moridomi14,
 abstract = {We consider an online matrix prediction problem. FTRL is a standard method to deal with online prediction tasks, which makes predictions by minimizing the cumulative loss function and the regularizer function. There are three popular regularizer functions for matrices, Frobenius norm, negative entropy and log-determinant. We propose an FTRL based algorithm with log-determinant as the regularizer and show a regret bound of the algorithm. Our main contribution is to show that the log-determinant regularization is eective when loss matrices are sparse. We also show that our algorithm is optimal for the online collaborative ltering problem with the log-determinant regularization.},
 address = {Nha Trang City, Vietnam},
 author = {Moridomi, Ken-ichiro and Hatano, Kohei and Takimoto, Eiji and Tsuda, Koji},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W222593245},
 pages = {250--265},
 pdf = {http://proceedings.mlr.press/v39/moridomi14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Online matrix prediction for sparse loss matrices},
 url = {https://proceedings.mlr.press/v39/moridomi14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-nakamura14,
 abstract = {We consider a direct mail problem in which a system repeats the following process everyday during some period: select a set of user-item pairs (u,i), send a recommendation mail of item i to user u for each selected pair (u,i), and receive a response from each user. We assume that each response can be obtained before the next process and through the response, the system can know the user’s evaluation of the recommended item directly or indirectly. Each pair (u,i) can be selected at most once during the period. If the total number of selections is very small compared to the number of entries in the whole useritem matrix, what selection strategy should be used to maximize the total sum of users’ evaluations during the period? We consider a UCB-like strategy for this problem, and show two methods using the strategy. The effectiveness of our methods are demonstrated by experiments using synthetic and real datasets.},
 address = {Nha Trang City, Vietnam},
 author = {Nakamura, Atsuyoshi},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W282081402},
 pages = {315--329},
 pdf = {http://proceedings.mlr.press/v39/nakamura14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A UCB-Like Strategy of Collaborative Filtering},
 url = {https://proceedings.mlr.press/v39/nakamura14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-oliveira14,
 abstract = {This paper proposes an ensemble method for time series forecasting tasks. Combining different forecasting models is a common approach to tackle these problems. State-of-the-art methods track the loss of the available models and adapt their weights accordingly. Metalearning strategies such as stacking are also used in these tasks. We propose a metalearning approach for adaptively combining forecasting models that specializes them across the time series. Our assumption is that different forecasting models have different areas of expertise and a varying relative performance. Moreover, many time series show recurring structures due to factors such as seasonality. Therefore, the ability of a method to deal with changes in relative performance of models as well as recurrent changes in the data distribution can be very useful in dynamic environments. Our approach is based on an ensemble of heterogeneous forecasters, arbitrated by a metalearning model. This strategy is designed to cope with the different dynamics of time series and quickly adapt the ensemble to regime changes. We validate our proposal using time series from several real world domains. Empirical results show the competitiveness of the method in comparison to state-of-the-art approaches for combining forecasters.},
 address = {Nha Trang City, Vietnam},
 author = {Oliveira, Mariana and Torgo, Luis},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2777265487},
 pages = {360--370},
 pdf = {http://proceedings.mlr.press/v39/oliveira14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Arbitrated Ensemble for Time Series Forecasting},
 url = {https://proceedings.mlr.press/v39/oliveira14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-preface,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 address = {Nha Trang City, Vietnam},
 author = {Phung, Dinh and Li, Hang},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W4240465921},
 pages = {i--xiv},
 pdf = {http://proceedings.mlr.press/v39/preface.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v39/preface.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-sun14,
 abstract = {Sparsity in R^m has been widely explored in machine learning. We study sparsity on a statistical simplex consisting of all categorical distributions. This is different from the case in R^m because such a simplex is a Riemannian manifold, a curved space. A learner with sparse constraints should be likely to fall to its low-dimensional boundaries. We present a novel analysis on the statistical simplex as a manifold with boundary. The main contribution is an explicit view of the learning dynamics in between high-dimensional models in the interior of the simplex and low-dimensional models on its boundaries. We prove the differentiability of the cost function, the natural gradient with respect to the Riemannian structure, and convexity around the singular regions. We uncover an interesting relationship with L1 regularization. We apply the proposed technique to social network analysis. Given a directed graph, the task is to rank a subset of influencer nodes. Here, sparsity means that the top-ranked nodes should present diversity in the sense of minimizing influence overlap. We present a ranking algorithm based on the natural gradient. It can scale up to graph datasets with millions of nodes. On real large networks, the top-ranked nodes are the most informative among several commonly-used techniques.},
 address = {Nha Trang City, Vietnam},
 author = {Sun, Ke and Mohamed, Hisham and Marchand-Maillet, Stephane},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W270428378},
 pages = {16--31},
 pdf = {http://proceedings.mlr.press/v39/sun14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sparsity on Statistical Simplexes and Diversity in Social Ranking},
 url = {https://proceedings.mlr.press/v39/sun14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-tagawa14,
 abstract = {This paper proposes a new fault detection and analysis approach which can leverage incomplete prior information. Conventional data-driven approaches suer from the problem of overtting and result in high rates of false positives, and model-driven approaches suer from a lack of specic information about complex systems. We overcome these problems by modifying the denoising autoencoder (DA), a data-driven method, to form a new approach, called the structured denoising autoencoder (StrDA), which can utilize incomplete prior information. The StrDA does not require specic information and can perform well without overtting. In particular, an empirical analysis with synthetic data revealed that the StrDA performs better than the DA even when there is partially incorrect or abstract information. An evaluation using real data from moving cars also showed that the StrDA with incomplete knowledge outperformed conventional methods. Surprisingly, the StrDA results were better even though the parameters of the conventional methods were tuned using faulty data, which are normally unknown. In addition, the StrDA fault analysis was able to extract the true causes of the faulty data; the other methods were unable to do this. Thus, only our proposed method can explain why the faults occurred.},
 address = {Nha Trang City, Vietnam},
 author = {Tagawa, Takaaki and Tadokoro, Yukihiro and Yairi, Takehisa},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2107657144},
 pages = {96--111},
 pdf = {http://proceedings.mlr.press/v39/tagawa14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Structured Denoising Autoencoder for Fault Detection and Analysis},
 url = {https://proceedings.mlr.press/v39/tagawa14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-tanaka14,
 abstract = {For the last few decades, learning based on multiple kernels, such as the ensemble kernel regressor and the multiple kernel regressor, has attracted much attention in the eld of machine learning. Although its ecacy was revealed numerically in many works, its theoretical ground is not investigated suciently. In this paper, we discuss regression problems with a class of kernels whose corresponding reproducing kernel Hilbert spaces have a common subspace with an invariant metric and show that the ensemble kernel regressor (the mean of kernel regressors with those kernels) gives a better learning result than the multiple kernel regressor (the kernel regressor with the sum of those kernels) in terms of the generalization ability of a model space.},
 address = {Nha Trang City, Vietnam},
 author = {Tanaka, Akira and Takigawa, Ichigaku and Imai, Hideyuki and Kudo, Mineichi},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W278877251},
 pages = {1--15},
 pdf = {http://proceedings.mlr.press/v39/tanaka14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Theoretical Analyses on Ensemble and Multiple Kernel Regressors},
 url = {https://proceedings.mlr.press/v39/tanaka14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-than14,
 abstract = {Latent Dirichlet allocation (LDA) provides an efficient tool to analyze very large text collections. In this paper, we discuss three novel contributions: (1) a proof for the tractability of the MAP estimation of topic mixtures under certain conditions that might fit well with practices, even though the problem is known to be intractable in the worst case; (2) a provably fast algorithm (OFW) for inferring topic mixtures; (3) a dual online algorithm (DOLDA) for learning LDA at a large scale. We show that OFW converges to some local optima, but under certain conditions it can converge to global optima. The discussion of OFW is very general and hence can be readily employed to accelerate the MAP estimation in a wide class of probabilistic models. From extensive experiments we find that DOLDA can achieve significantly better predictive performance and more interpretable topics, with lower runtime, than stochastic variational inference. Further, DOLDA enables us to easily analyze text streams or millions of documents.},
 address = {Nha Trang City, Vietnam},
 author = {Than, Khoat and Doan, Tung},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 pages = {80--95},
 pdf = {http://proceedings.mlr.press/v39/than14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Dual online inference for latent {D}irichlet allocation},
 url = {https://proceedings.mlr.press/v39/than14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-xiong14,
 abstract = {Along with the emergence of algorithms such as persistent contrastive divergence (PCD), tempered transition and parallel tempering, the past decade has witnessed a revival of learning undirected graphical models (UGMs) with sampling-based approximations. In this paper, based upon the analogy between Robbins-Monro’s stochastic approximation procedure and sequential Monte Carlo (SMC), we analyze the strengths and limitations of state-of-the-art learning algorithms from an SMC point of view. Moreover, we apply the rationale further in sampling at each iteration, and propose to learn UGMs using persistent sequential Monte Carlo (PSMC). The whole learning procedure is based on the samples from a long, persistent sequence of distributions which are actively constructed. Compared to the above-mentioned algorithms, one critical strength of PSMCbased learning is that it can explore the sampling space more effectively. In particular, it is robust when learning rates are large or model distributions are high-dimensional and thus multi-modal, which often causes other algorithms to deteriorate. We tested PSMC learning, also with other related methods, on carefully-designed experiments with both synthetic and real-world data, and our empirical results demonstrate that PSMC compares favorably with the state of the art.},
 address = {Nha Trang City, Vietnam},
 author = {Xiong, Hanchen and Szedmak, Sandor and Piater, Justus},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2158980522},
 pages = {205--220},
 pdf = {http://proceedings.mlr.press/v39/xiong14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Towards Maximum Likelihood: Learning Undirected Graphical Models using Persistent Sequential Monte Carlo},
 url = {https://proceedings.mlr.press/v39/xiong14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-zhang14,
 abstract = {Dimensionality reduction is a fundamental problem of machine learning, and has been intensively studied, where classication and clustering are two special cases of dimensionality reduction that reduce high-dimensional data to discrete points. Here we describe a simple multilayer network for dimensionality reduction that each layer of the network is a group of mutually independent k-centers clusterings. We nd that the network can be trained successfully layer-by-layer by simply assigning the centers of each clustering by randomly sampled data points from the input. Our results show that the described simple method outperformed 7 well-known dimensionality reduction methods on both very small-scale biomedical data and large-scale image and document data, with less training time than multilayer neural networks on large-scale data.},
 address = {Nha Trang City, Vietnam},
 author = {Zhang, Xiao-Lei},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W1529160966},
 pages = {221--233},
 pdf = {http://proceedings.mlr.press/v39/zhang14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Nonlinear Dimensionality Reduction of Data by Deep Distributed Random Samplings},
 url = {https://proceedings.mlr.press/v39/zhang14.html},
 volume = {39},
 year = {2015}
}

@inproceedings{pmlr-v39-zhu14,
 abstract = {In this paper, we propose the Augmented Multi-Instance View (AMIV) framework to construct a better model by exploiting augmented information. For example, abstract screening tasks may be difficult because only abstract information is available, whereas the performance can be improved when the abstracts of references listed in the document can be exploited as augmented information. If each abstract is represented as an instance (i.e., a feature vector) x, then with the augmented information, it can be represented as an instance-bag pair (x;B), where B is a bag of instances (i.e., the abstracts of references). Note that ifx has a labely, then we assume that there must exist at least one instance in the bag B having the label y. We regard x and B as two views, i.e., a single-instance view augmented with a multi-instance view, and propose the AMIV-lss approach by establishing a latent semantic subspace between the two views. The AMIV framework can be applied when the augmented information is presented as multi-instance bags and to the best of our knowledge, such a learning with augmented multi-instance view problem has not been touched before. Experimental results on twelve TechPaper datasets, five PubMed data sets and a WebPage data set validate the effectiveness of our AMIV-lss approach.},
 address = {Nha Trang City, Vietnam},
 author = {Zhu, Yue and Wu, Jianxin and Jiang, Yuan and Zhou, Zhi-Hua},
 booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning},
 editor = {Phung, Dinh and Li, Hang},
 month = {26--28 Nov},
 openalex = {W2162047884},
 pages = {234--249},
 pdf = {http://proceedings.mlr.press/v39/zhu14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning with Augmented Multi-Instance View},
 url = {https://proceedings.mlr.press/v39/zhu14.html},
 volume = {39},
 year = {2015}
}
