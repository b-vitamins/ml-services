@proceedings{L4DC2020,
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Alexandre M. Bayen and Ali Jadbabaie and George Pappas and Pablo A. Parrilo and Benjamin Recht and Claire Tomlin and Melanie Zeilinger},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 volume = {120}
}

@inproceedings{pmlr-v120-abdulsamad20a,
 abstract = {The control of nonlinear dynamical systems remains a major challenge for autonomous agents. Current trends in reinforcement learning (RL) focus on complex representations of dynamics and policies, which have yielded impressive results in solving a variety of hard control tasks. However, this new sophistication and extremely over-parameterized models have come with the cost of an overall reduction in our ability to interpret the resulting policies. In this paper, we take inspiration from the control community and apply the principles of hybrid switching systems in order to break down complex dynamics into simpler components. We exploit the rich representational power of probabilistic graphical models and derive an expectation-maximization (EM) algorithm for learning a sequence model to capture the temporal structure of the data and automatically decompose nonlinear dynamics into stochastic switching linear dynamical systems. Moreover, we show how this framework of switching models enables extracting hierarchies of Markovian and auto-regressive locally linear controllers from nonlinear experts in an imitation learning scenario.},
 author = {Abdulsamad, Hany and Peters, Jan},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3021457260},
 pages = {904--914},
 pdf = {http://proceedings.mlr.press/v120/abdulsamad20a/abdulsamad20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Hierarchical Decomposition of Nonlinear Dynamics and Control for System Identification and Policy Distillation},
 url = {https://proceedings.mlr.press/v120/abdulsamad20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-abuduweili20a,
 abstract = {High fidelity behavior prediction of intelligent agents is critical in many applications. However, the prediction model trained on the training set may not generalize to the testing set due to domain shift and time variance. The challenge motivates the adoption of online adaptation algorithms to update prediction models in real-time to improve the prediction performance. Inspired by Extended Kalman Filter (EKF), this paper introduces a series of online adaptation methods, which are applicable to neural network-based models. A base adaptation algorithm Modified EKF with forgetting factor (MEKF$_\lambda$) is introduced first, followed by exponential moving average filtering techniques. Then this paper introduces a dynamic multi-epoch update strategy to effectively utilize samples received in real time. With all these extensions, we propose a robust online adaptation algorithm: MEKF with Exponential Moving Average and Dynamic Multi-Epoch strategy (MEKF$_{\text{EMA-DME}}$). The proposed algorithm outperforms existing methods as demonstrated in experiments. The source code is open-sourced in the following link https://github.com/intelligent-control-lab/MEKF_MAME.},
 author = {Abuduweili, Abulikemu and Liu, Changliu},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107501279},
 pages = {65--74},
 pdf = {http://proceedings.mlr.press/v120/abuduweili20a/abuduweili20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Robust Online Model Adaptation by Extended Kalman Filter with Exponential Moving Average and Dynamic Multi-Epoch Strategy},
 url = {https://proceedings.mlr.press/v120/abuduweili20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-agrawal20a,
 abstract = {Many control policies used in various applications determine the input or action by solving a convex optimization problem that depends on the current state and some parameters. Common examples of such convex optimization control policies (COCPs) include the linear quadratic regulator (LQR), convex model predictive control (MPC), and convex control-Lyapunov or approximate dynamic programming (ADP) policies. These types of control policies are tuned by varying the parameters in the optimization problem, such as the LQR weights, to obtain good performance, judged by application-specific metrics. Tuning is often done by hand, or by simple methods such as a crude grid search. In this paper we propose a method to automate this process, by adjusting the parameters using an approximate gradient of the performance metric with respect to the parameters. Our method relies on recently developed methods that can efficiently evaluate the derivative of the solution of a convex optimization problem with respect to its parameters. We illustrate our method on several examples.},
 author = {Agrawal, Akshay and Barratt, Shane and Boyd, Stephen and Stellato, Bartolomeo},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W4287991730},
 pages = {361--373},
 pdf = {http://proceedings.mlr.press/v120/agrawal20a/agrawal20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Convex Optimization Control Policies},
 url = {https://proceedings.mlr.press/v120/agrawal20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-ahmadi20a,
 abstract = {We present a mathematical and computational framework for learning a dynamical system from noisy observations of a few trajectories and subject to side information. Side information is any knowledge we might have about the dynamical system we would like to learn, besides trajectory data, and is typically inferred from domain-specific knowledge or basic principles of a scientific discipline. We are interested in explicitly integrating side information into the learning process in order to compensate for scarcity of trajectory observations. We identify six types of side information that arise naturally in many applications and lead to convex constraints in the learning problem. First, we show that when our model for the unknown dynamical system is parameterized as a polynomial, we can impose our side information constraints computationally via semidefinite programming. We then demonstrate the added value of side information for learning the dynamics of basic models in physics and cell biology, as well as for learning and controlling the dynamics of a model in epidemiology. Finally, we study how well polynomial dynamical systems can approximate continuously differentiable ones while satisfying side information (either exactly or approximately). Our overall learning methodology combines ideas from convex optimization, real algebra, dynamical systems, and functional approximation theory, and can potentially lead to new synergies among these areas.},
 author = {Ahmadi, Amir Ali and Khadir, Bachir El},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3095947501},
 pages = {718--727},
 pdf = {http://proceedings.mlr.press/v120/ahmadi20a/ahmadi20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Dynamical Systems with Side Information},
 url = {https://proceedings.mlr.press/v120/ahmadi20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-akbari20a,
 abstract = {We study an online setting of the linear quadratic Gaussian optimal control problem on a sequence of cost functions, where similar to classical online optimization, the future decisions are made by only knowing the cost in hindsight. We introduce a modified online Riccati update that under some boundedness assumptions, leads to logarithmic regret bounds, improving the best known square-root bound. In particular, for the scalar case we achieve the logarithmic regret without any boundedness assumption. As opposed to earlier work, proposed method does not rely on solving semi-definite programs at each stage.},
 author = {Akbari, Mohammad and Gharesifard, Bahman and Linder, Tamas},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107101281},
 pages = {476--485},
 pdf = {http://proceedings.mlr.press/v120/akbari20a/akbari20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Riccati updates for online linear quadratic control},
 url = {https://proceedings.mlr.press/v120/akbari20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-allibhoy20a,
 abstract = {We consider a networked linear system where system matrices are unknown to the individual agents but sampled data is available to them.  We propose a data-driven method for designing a distributed linear-quadratic controller where agents learn a non-parametric system model from a single sample trajectory in which nodes can predict future trajectories using only data available to themselves and their neighbors. Based on this system representation, we propose a control scheme where a network optimization problem is solved in a receding horizon manner. We show that the proposed control scheme is stabilizing and validate our results through numerical experiments.},
 author = {Allibhoy, Ahmed and Cortes, Jorge},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3042465336},
 pages = {838--839},
 pdf = {http://proceedings.mlr.press/v120/allibhoy20a/allibhoy20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Data-Driven Distributed Predictive Control via Network Optimization},
 url = {https://proceedings.mlr.press/v120/allibhoy20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-amani20a,
 abstract = {Many applications require a learner to make sequential decisions given uncertainty regarding both the systemâs payoff function and safety constraints. When learning algorithms are used in safety-critical systems, it is paramount that the learnerâs actions do not violate the safety constraints at any stage of the learning process. In this paper, we study a stochastic bandit optimization problem where the systemâs unknown payoff and constraint functions are sampled from Gaussian Processes (GPs). We develop a safe variant of the proposed algorithm by Srinivas et al. (2010), GP-UCB, called SGP-UCB, with necessary modifications to respect safety constraints at every round. Our most important contribution is to derive the first sub-linear regret bounds for this problem.},
 author = {Amani, Sanae and Alizadeh, Mahnoosh and Thrampoulidis, Christos},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3199516780},
 pages = {158--159},
 pdf = {http://proceedings.mlr.press/v120/amani20a/amani20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Regret Bound for Safe Gaussian Process Bandit Optimization},
 url = {https://proceedings.mlr.press/v120/amani20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-anguluri20a,
 abstract = {This paper proposes a new framework and several results to quantify the performance of data-driven state-feedback controllers for linear systems against targeted perturbations of the training data. We focus on the case where subsets of the training data are randomly corrupted by an adversary, and derive lower and upper bounds for the stability of the closed-loop system with compromised controller as a function of the perturbation statistics, size of the training data, sensitivity of the data-driven algorithm to perturbation of the training data, and properties of the nominal closed-loop system. Our stability and convergence bounds are probabilistic in nature, and rely on a first-order approximation of the data-driven procedure that designs the state-feedback controller, which can be computed directly using the training data. We illustrate our findings via multiple numerical studies.},
 author = {Anguluri, Rajasekhar and Makdah, Abed Alrahman Al and Katewa, Vaibhav and Pasqualetti, Fabio},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2994885839},
 pages = {404--412},
 pdf = {http://proceedings.mlr.press/v120/anguluri20a/anguluri20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On the Robustness of Data-Driven Controllers for Linear Systems},
 url = {https://proceedings.mlr.press/v120/anguluri20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-arcari20a,
 abstract = {Designing controllers for systems affected by model uncertainty can prove to be a challenge, especially when seeking the optimal compromise between the conflicting goals of identification and control. This trade-off is explicitly taken into account in the dual control problem, for which the exact solution is provided by stochastic dynamic programming. Due to its computational intractability, we propose a sampling-based approximation for systems affected by both parametric and structural model uncertainty. The approach proposed in this paper separates the prediction horizon in a dual and an exploitation part. The dual part is formulated as a scenario tree that actively discriminates among a set of potential models while learning unknown parameters. In the exploitation part, achieved information is fixed for each scenario, and open-loop control sequences are computed for the remainder of the horizon. As a result, we solve one optimization problem over a collection of control sequences for the entire horizon, explicitly considering the knowledge gained in each scenario, leading to a dual model predictive control formulation.},
 author = {Arcari, Elena and Hewing, Lukas and Schlichting, Max and Zeilinger, Melanie},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2995333107},
 pages = {894--903},
 pdf = {http://proceedings.mlr.press/v120/arcari20a/arcari20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Dual Stochastic MPC for Systems with Parametric and Structural Uncertainty},
 url = {https://proceedings.mlr.press/v120/arcari20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-bayen20a,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 author = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W4240465921},
 pages = {1--4},
 pdf = {http://proceedings.mlr.press/v120/bayen20a/bayen20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v120/bayen20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-bedi20a,
 abstract = {Bayesian optimization is a framework for global search via maximum a posteriori updates rather than simulated annealing, and has gained prominence in tuning the hyper-parameters of machine learning algorithms and more broadly, in decision-making under uncertainty. In this work, we cast Bayesian optimization as a multi-armed bandit problem, where the payoff function is sampled from a Gaussian process (GP). Further, we focus on action selections via the GP upper confidence bound (UCB). While numerous prior works use GPs in bandit settings, they do not apply to settings where the total number of iterations $T$ may be large-scale, as the complexity of computing the posterior parameters scales cubically with the number of past observations. To circumvent this computational burden, we propose a simple statistical test: only incorporate an action into the GP posterior when its conditional entropy exceeds an $\epsilon$ threshold. Doing so permits us to derive sublinear regret bounds of GP bandit algorithms up to factors depending on the compression parameter $\epsilon$ for both discrete and continuous action sets. Moreover, the complexity of the GP posterior remains provably finite. Experimentally, we observe state of the art accuracy and complexity tradeoffs for GP bandit algorithms on various hyper-parameter tuning tasks, suggesting the merits of managing the complexity of  GPs in bandit settings.},
 author = {Bedi, Amrit Singh and Peddireddy, Dheeraj and Aggarwal, Vaneet and Koppel, Alec},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107797364},
 pages = {924--934},
 pdf = {http://proceedings.mlr.press/v120/bedi20a/bedi20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Efficient Large-Scale Gaussian Process Bandits by Believing only Informative Actions.},
 url = {https://proceedings.mlr.press/v120/bedi20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-bharadhwaj20a,
 abstract = {Recent works in high-dimensional model-predictive control and model-based reinforcement learning with learned dynamics and reward models have resorted to population-based optimization methods, such as the Cross-Entropy Method (CEM), for planning a sequence of actions. To decide on an action to take, CEM conducts a search for the action sequence with the highest return according to the dynamics model and reward. Action sequences are typically randomly sampled from an unconditional Gaussian distribution and evaluated on the environment. This distribution is iteratively updated towards action sequences with higher returns. However, this planning method can be very inefficient, especially for high-dimensional action spaces. An alternative line of approaches optimize action sequences directly via gradient descent, but are prone to local optima. We propose a method to solve this planning problem by interleaving CEM and gradient descent steps in optimizing the action sequence. Our experiments show faster convergence of the proposed hybrid approach, even for high-dimensional action spaces, avoidance of local minima, and better or equal performance to CEM. Code accompanying the paper is available here https://github.com/homangab/gradcem.},
 author = {Bharadhwaj, Homanga and Xie, Kevin and Shkurti, Florian},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3017090863},
 pages = {277--286},
 pdf = {http://proceedings.mlr.press/v120/bharadhwaj20a/bharadhwaj20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Model-Predictive Control via Cross-Entropy and Gradient-Based Optimization},
 url = {https://proceedings.mlr.press/v120/bharadhwaj20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-bhardwaj20a,
 abstract = {Model-free Reinforcement Learning (RL) works well when experience can be collected cheaply and model-based RL is effective when system dynamics can be modeled accurately. However, both assumptions can be violated in real world problems such as robotics, where querying the system can be expensive and real-world dynamics can be difficult to model. In contrast to RL, Model Predictive Control (MPC) algorithms use a simulator to optimize a simple policy class online, constructing a closed-loop controller that can effectively contend with real-world dynamics. MPC performance is usually limited by factors such as model bias and the limited horizon of optimization. In this work, we present a novel theoretical connection between information theoretic MPC and entropy regularized RL and develop a Q-learning algorithm that can leverage biased models. We validate the proposed algorithm on sim-to-sim control tasks to demonstrate the improvements over optimal control and reinforcement learning from scratch. Our approach paves the way for deploying reinforcement learning algorithms on real systems in a systematic manner.},
 author = {Bhardwaj, Mohak and Handa, Ankur and Fox, Dieter and Boots, Byron},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108175278},
 pages = {840--850},
 pdf = {http://proceedings.mlr.press/v120/bhardwaj20a/bhardwaj20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Information Theoretic Model Predictive Q-Learning},
 url = {https://proceedings.mlr.press/v120/bhardwaj20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-bonassi20a,
 abstract = {The goal of this paper is to analyze Long Short Term Memory (LSTM) neural networks from a dynamical system perspective. The classical recursive equations describing the evolution of LSTM can be recast in state space form, resulting in a time-invariant nonlinear dynamical system. A sufficient condition guaranteeing the Input-to-State (ISS) stability property of this class of systems is provided. The ISS property entails the boundedness of the output reachable set of the LSTM. In light of this result, a novel approach for the safety verification of the network, based on the Scenario Approach, is devised. The proposed method is eventually tested on a pH neutralization process.},
 author = {Bonassi, Fabio and Terzi, Enrico and Farina, Marcello and Scattolini, Riccardo},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2995386400},
 pages = {85--94},
 pdf = {http://proceedings.mlr.press/v120/bonassi20a/bonassi20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {LSTM Neural Networks: Input to State Stability and Probabilistic Safety Verification},
 url = {https://proceedings.mlr.press/v120/bonassi20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-bonzanini20a,
 abstract = {The increasing complexity of modern systems can introduce significant uncertainties to the models that describe them, which poses a great challenge to safe model-based control. This paper presents a learning-based stochastic model predictive control (LB-SMPC) strategy with chance constraints for offset-free trajectory tracking. The LB-SMPC strategy systematically handles plant-model mismatch between the actual system dynamics and a system model via a state-dependent uncertainty term that is intended to correct model predictions at each sampling time. A chance constraint handling method is presented to ensure state constraint satisfaction to a desired level for the case of state-dependent model uncertainty. Closed-loop simulations demonstrate the usefulness of LB- SMPC for predictive control of safety-critical systems with hard-to-model and/or time-varying dynamics.},
 author = {Bonzanini, Angelo Domenico and Mesbah, Ali},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3109537205},
 pages = {571--580},
 pdf = {http://proceedings.mlr.press/v120/bonzanini20a/bonzanini20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning-based Stochastic Model Predictive Control with State-Dependent Uncertainty.},
 url = {https://proceedings.mlr.press/v120/bonzanini20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-bosch20a,
 abstract = {Planning is a powerful approach to control problems with known environment dynamics. In unknown environments the agent needs to learn a model of the system dynamics to make planning applicable. This is particularly challenging when the underlying states are only indirectly observable through images. We propose to learn a deep latent Gaussian process dynamics (DLGPD) model that learns low-dimensional system dynamics from environment interactions with visual observations. The method infers latent state representations from observations using neural networks and models the system dynamics in the learned latent space with Gaussian processes. All parts of the model can be trained jointly by optimizing a lower bound on the likelihood of transitions in image space. We evaluate the proposed approach on the pendulum swing-up task while using the learned dynamics model for planning in latent space in order to solve the control problem. We also demonstrate that our method can quickly adapt a trained agent to changes in the system dynamics from just a few rollouts. We compare our approach to a state-of-the-art purely deep learning based method and demonstrate the advantages of combining Gaussian processes with deep learning for data efficiency and transfer learning.},
 author = {Bosch, Nathanael and Achterhold, Jan and Leal-Taix\'e, Laura and St\"uckler, J\"org},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3102488324},
 pages = {640--650},
 pdf = {http://proceedings.mlr.press/v120/bosch20a/bosch20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Planning from Images with Deep Latent Gaussian Process Dynamics},
 url = {https://proceedings.mlr.press/v120/bosch20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-boyer20a,
 abstract = {One of the most promising devices for realizing power production through nuclear fusion is the tokamak. To maximize performance, it is preferable that tokamak reactors achieve advanced operating scenarios characterized by good plasma confinement, improved magnetohydrodynamic stability, and a largely non-inductively driven plasma current. Such scenarios could enable steady-state reactor operation with high fusion gain â the ratio of produced fusion power to the external power provided through the plasma boundary. Precise and robust control of the evolution of the plasma boundary shape as well as the spatial distribution of the plasma current, density, temperature, and rotation will be essential to achieving and maintaining such scenarios. The complexity of the evolution of tokamak plasmas, arising due to nonlinearities and coupling between various parameters, motivates the use of model-based control algorithms that can account for the system dynamics. In this work, a learning-based accelerated model trained on data from the National Spherical Torus Experiment Upgrade (NSTX-U) is employed to develop planning and control strategies for regulating the density and temperature profile evolution around desired trajectories. The proposed model combines empirical scaling laws developed across multiple devices with neural networks trained on empirical data from NSTX-U and a database of first-principles-based computationally intensive simulations. The reduced execution time of the accelerated model will enable practical application of optimization algorithms and reinforcement learning approaches for scenario planning and control development. An initial demonstration of applying optimization approaches to the learning-based model is presented, including a strategy for mitigating the effect of leaving the finite validity range of the accelerated model. The approach shows promise for actuator planning between experiments and in real-time.},
 author = {Boyer, Mark},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W4363681882},
 pages = {698--707},
 pdf = {http://proceedings.mlr.press/v120/boyer20a/boyer20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Toward fusion plasma scenario planning for NSTX-U using machine-learning-accelerated models},
 url = {https://proceedings.mlr.press/v120/boyer20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-breschi20a,
 abstract = {In control applications where finding a model of the plant is the most costly and time consuming task, Virtual Reference Feedback Tuning (VRFT) represents a valid - purely data-driven - alternative for the design of model reference controllers. However, the selection of a proper reference model within a model-free setting is known to be a critical task, with this model typically playing the role of a hyper-parameter. In this work, we extend the VRFT methodology to compute both a proper reference model and the corresponding optimal controller parameters from data by means of Particle Swarm optimization. The effectiveness of the proposed approach is illustrated on a benchmark simulation example.},
 author = {Breschi, Valentina and Formentin, Simone},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3106576580},
 pages = {37--45},
 pdf = {http://proceedings.mlr.press/v120/breschi20a/breschi20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Virtual Reference Feedback Tuning with data-driven reference model selection},
 url = {https://proceedings.mlr.press/v120/breschi20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-breschi20b,
 abstract = {Input saturation is an ubiquitous nonlinearity in control systems and arises from the fact that all actuators are subject to a maximum power, thereby resulting in a hard limitation on the allowable magnitude of the input effort. In the scientific literature, anti-windup augmentation has been proposed to recover the desired linear closed-loop dynamics during transients, but the effectiveness of such a compensation is strongly linked to the accuracy of the mathematical model of the plant. In this work, it is shown that a feedback controller with embedded anti-windup compensator can be directly identified from data, by suitably extending the existing data-driven design theory. The effectiveness of the resulting method is illustrated on a benchmark simulation example.},
 author = {Breschi, Valentina and Formentin, Simone},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107781972},
 pages = {46--54},
 pdf = {http://proceedings.mlr.press/v120/breschi20b/breschi20b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Direct Data-Driven Control with Embedded Anti-Windup Compensation.},
 url = {https://proceedings.mlr.press/v120/breschi20b.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-buisson-fenet20a,
 abstract = {Despite the availability of ever more data enabled through modern sensor and computer technology, it still remains an open problem to learn dynamical systems in a sample-efficient way. We propose active learning strategies that leverage information-theoretical properties arising naturally during Gaussian process regression, while respecting constraints on the sampling process imposed by the system dynamics. Sample points are selected in regions with high uncertainty, leading to exploratory behavior and data-efficient training of the model. All results are finally verified in an extensive numerical benchmark.},
 author = {Buisson-Fenet, Mona and Solowjow, Friedrich and Trimpe, Sebastian},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2990320948},
 pages = {5--15},
 pdf = {http://proceedings.mlr.press/v120/buisson-fenet20a/buisson-fenet20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Actively Learning Gaussian Process Dynamics},
 url = {https://proceedings.mlr.press/v120/buisson-fenet20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-bujarbaruah20a,
 abstract = {This paper proposes an Adaptive Stochastic Model Predictive Control (MPC) strategy for stable linear time-invariant systems in the presence of bounded disturbances. We consider multi-input, multi-output systems that can be expressed by a Finite Impulse Response (FIR) model. The parameters of the FIR model corresponding to each output are unknown but assumed sparse. We estimate these parameters using the Recursive Least Squares algorithm. The estimates are then improved using set-based bounds obtained by solving the Basis Pursuit Denoising [1] problem. Our approach is able to handle hard input constraints and probabilistic output constraints. Using tools from distributionally robust optimization, we reformulate the probabilistic output constraints as tractable convex second-order cone constraints, which enables us to pose our MPC design task as a convex optimization problem. The efficacy of the developed algorithm is highlighted with a thorough numerical example, where we demonstrate performance gain over the counterpart algorithm of [2], which does not utilize the sparsity information of the system impulse response parameters during control design.},
 author = {Bujarbaruah, Monimoy and Vallon, Charlott},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108013348},
 pages = {137--146},
 pdf = {http://proceedings.mlr.press/v120/bujarbaruah20a/bujarbaruah20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Exploiting Model Sparsity in Adaptive MPC: A Compressed Sensing Viewpoint},
 url = {https://proceedings.mlr.press/v120/bujarbaruah20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-capone20a,
 abstract = {While most dynamic system exploration techniques aim to achieve a globally accurate model, this
is generally unsuited for systems with unbounded state spaces. Furthermore, many applications
do not require a globally accurate model, e.g., local stabilization tasks. In this paper, we propose
an active learning strategy for Gaussian process state space models that aims to obtain an accurate
model on a bounded subset of the state-action space. Our approach aims to maximize the mutual information
of the exploration trajectories with respect to a discretization of the region of interest. By
employing model predictive control, the proposed technique integrates information collected during
exploration and adaptively improves its exploration strategy. To enable computational tractability,
we decouple the choice of most informative data points from the model predictive control optimization
step. This yields two optimization problems that can be solved in parallel. We apply
the proposed method to explore the state space of various dynamical systems and compare our
approach to a commonly used entropy-based exploration strategy. In all experiments, our method
yields a better model within the region of interest than the entropy-based method.},
 author = {Capone, Alexandre and Noske, Gerrit and Umlauft, Jonas and Beckers, Thomas and Lederer, Armin and Hirche, Sandra},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108843476},
 pages = {490--499},
 pdf = {http://proceedings.mlr.press/v120/capone20a/capone20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Localized active learning of Gaussian process state space models},
 url = {https://proceedings.mlr.press/v120/capone20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-castaneda20a,
 abstract = {The main drawbacks of input-output linearizing controllers are the need for precise dynamics models and not being able to account for input constraints. Model uncertainty is common in almost every robotic application and input saturation is present in every real world system. In this paper, we address both challenges for the specific case of bipedal robot control by the use of reinforcement learning techniques. Taking the structure of a standard input-output linearizing controller, we use an additive learned term that compensates for model uncertainty. Moreover, by adding constraints to the learning problem we manage to boost the performance of the final controller when input limits are present. We demonstrate the effectiveness of the designed framework for different levels of uncertainty on the five-link planar walking robot RABBIT.},
 author = {Casta{\~n}eda, Fernando and Wulfman, Mathias and Agrawal, Ayush and Westenbroek, Tyler and Sastry, Shankar and Tomlin, Claire and Sreenath, Koushil},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3017011674},
 pages = {990--999},
 pdf = {http://proceedings.mlr.press/v120/castaneda20a/castaneda20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improving Input-Output Linearizing Controllers for Bipedal Robots via Reinforcement Learning},
 url = {https://proceedings.mlr.press/v120/castaneda20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-chamon20a,
 abstract = {In recent years, considerable work has been done to tackle the issue of designing control laws based on observations to allow unknown dynamical systems to perform pre-specified tasks. At least as important for autonomy, however, is the issue of learning which tasks can be performed in the first place. This is particularly critical in situations where multiple (possibly conflicting) tasks and requirements are demanded from the agent, resulting in infeasible specifications. Such situations arise due to over-specification or dynamic operating conditions and are only aggravated when the dynamical system model is learned through simulations. Often, these issues are tackled using regularization and penalties tuned based on application-specific expert knowledge. Nevertheless, this solution becomes impractical for large-scale systems, unknown operating conditions, and/or in online settings where expert input would be needed during the system operation. Instead, this work enables agents to autonomously pose, tune, and solve optimal control problems by compromising between performance and specification costs. Leveraging duality theory, it puts forward a counterfactual optimization algorithm that directly determines the specification trade-off while solving the optimal control problem.},
 author = {Chamon, Luiz F. O. and Paternain, Santiago and Ribeiro, Alejandro},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3004016940},
 pages = {235--244},
 pdf = {http://proceedings.mlr.press/v120/chamon20a/chamon20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Counterfactual Programming for Optimal Control},
 url = {https://proceedings.mlr.press/v120/chamon20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-coppens20a,
 abstract = {We present a data-driven method for solving the linear quadratic regulator problem for systems with multiplicative disturbances, the distribution of which is only known through sample estimates. We adopt a distributionally robust approach to cast the controller synthesis problem as semidefinite programs. Using results from high dimensional statistics, the proposed methodology ensures that their solution provides mean-square stabilizing controllers with high probability even for low sample sizes. As sample size increases the closed-loop cost approaches that of the optimal controller produced when the distribution is known. We demonstrate the practical applicability and performance of the method through a numerical experiment.},
 author = {Coppens, Peter and Schuurmans, Mathijs and Patrinos, Panagiotis},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W4287991264},
 pages = {521--530},
 pdf = {http://proceedings.mlr.press/v120/coppens20a/coppens20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Data-driven distributionally robust LQR with multiplicative noise},
 url = {https://proceedings.mlr.press/v120/coppens20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-curi20a,
 abstract = {We propose a new variational inference algorithm for learning in Gaussian Process State-Space Models (GPSSMs). Our algorithm enables learning of unstable and partially observable systems, where previous algorithms fail. Our main algorithmic contribution is a novel approximate posterior that can be calculated efficiently using a single forward and backward pass along the training trajectories. The forward-backward pass is inspired on Kalman smoothing for linear dynamical systems but generalizes to GPSSMs. Our second contribution is a modification of the conditioning step that effectively lowers the Kalman gain. This modification is crucial to attaining good test performance where no measurements are available. Finally, we show experimentally that our learning algorithm performs well in stable and unstable real systems with hidden states.},
 author = {Curi, Sebastian and Melchior, Silvan and Berkenkamp, Felix and Krause, Andreas},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 pages = {147--157},
 pdf = {http://proceedings.mlr.press/v120/curi20a/curi20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Structured Variational Inference in Partially Observable UnstableGaussian Process State Space Models},
 url = {https://proceedings.mlr.press/v120/curi20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-dean20a,
 abstract = {Motivated by vision-based control of autonomous vehicles, we consider the problem of controlling a known linear dynamical system for which partial state information, such as vehicle position, is extracted from complex and nonlinear data, such as a camera image. Our approach is to use a learned perception map that predicts some linear function of the state and to design a corresponding safe set and robust controller for the closed loop system with this sensing scheme. We show that under suitable smoothness assumptions on both the perception map and the generative model relating state to complex and nonlinear data, parameters of the safe set can be learned via appropriately dense sampling of the state space. We then prove that the resulting perception-control loop has favorable generalization properties. We illustrate the usefulness of our approach on a synthetic example and on the self-driving car simulation platform CARLA.},
 author = {Dean, Sarah and Matni, Nikolai and Recht, Benjamin and Ye, Vickie},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2953916822},
 pages = {350--360},
 pdf = {http://proceedings.mlr.press/v120/dean20a/dean20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Robust Guarantees for Perception-Based Control},
 url = {https://proceedings.mlr.press/v120/dean20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-devonport20a,
 abstract = {Many practical systems are not amenable to the reachability methods that give guarantees of correctness, since they have dynamics that are strongly nonlinear, uncertain, and possibly unknown. While reachable sets for these kinds of systems can still be estimated in a data-driven way, data-driven methods typically do not guarantee the validity of their results. However, certain data-driven approaches may be given a probabilistic guarantee of correctness, by reframing the problem as a chance-constrained optimization problem that is solved with scenario optimization. We apply this approach to the problem of approximating a reachable set by a norm ball from data. The method requires only O(n^2) sample trajectories and the solution of a convex problem. A variant of the method restricted to axis-aligned norm balls requires only O(n) samples.},
 author = {Devonport, Alex and Arcak, Murat},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107806997},
 pages = {75--84},
 pdf = {http://proceedings.mlr.press/v120/devonport20a/devonport20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Estimating Reachable Sets with Scenario Optimization},
 url = {https://proceedings.mlr.press/v120/devonport20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-doan20a,
 abstract = {Two-time-scale stochastic approximation is a popular iterative method for finding the solution of a system of two equations. Such methods have found broad applications in many areas, especially in machine learning and reinforcement learning. In this paper, we propose a distributed variant of this method over a network of agents, where the agents use two graphs representing their communication at different speeds due to the nature of their two-time-scale updates. Our main contribution is to provide a finite-time analysis for the performance of the proposed method. In particular, we establish an upper bound for the convergence rates of the mean square errors at the agents to zero as a function of the step sizes and the network topology.},
 author = {Doan, Thinh and Romberg, Justin},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2994636779},
 pages = {26--36},
 pdf = {http://proceedings.mlr.press/v120/doan20a/doan20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Finite-Time Performance of Distributed Two-Time-Scale Stochastic Approximation},
 url = {https://proceedings.mlr.press/v120/doan20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-falsone20a,
 abstract = {We consider uncertain multi-agent optimization problems that are formulated as Mixed Integer Linear Programs (MILPs) with an almost separable structure. Specifically, agents have their own cost function and constraints, and need to set their local decision vector subject to coupling constraints due to shared resources. The problem is affected by uncertainty that is only known from data. A scalable decentralized approach to tackle the combinatorial complexity of constraint-coupled multi-agent MILPs has been recently introduced in the literature. However, the presence of uncertainty has been addressed only in a distributed convex optimization framework, i.e., without integer decision variables. This work fills in this gap by proposing a data-driven decentralized scheme to determine a solution with probabilistic feasibility guarantees that depend on the size of the data-set.},
 author = {Falsone, Alessandro and Molinari, Federico and Prandini, Maria},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3109028261},
 pages = {1000--1009},
 pdf = {http://proceedings.mlr.press/v120/falsone20a/falsone20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Uncertain multi-agent MILPs: A data-driven decentralized solution with probabilistic feasibility guarantees},
 url = {https://proceedings.mlr.press/v120/falsone20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-foster20a,
 abstract = {We introduce algorithms for learning nonlinear dynamical systems of the form $x_{t+1}=σ(Θ^{\star}x_t)+\varepsilon_t$, where $Θ^{\star}$ is a weight matrix, $σ$ is a nonlinear link function, and $\varepsilon_t$ is a mean-zero noise process. We give an algorithm that recovers the weight matrix $Θ^{\star}$ from a single trajectory with optimal sample complexity and linear running time. The algorithm succeeds under weaker statistical assumptions than in previous work, and in particular i) does not require a bound on the spectral norm of the weight matrix $Θ^{\star}$ (rather, it depends on a generalization of the spectral radius) and ii) enjoys guarantees for non-strictly-increasing link functions such as the ReLU. Our analysis has two key components: i) we give a general recipe whereby global stability for nonlinear dynamical systems can be used to certify that the state-vector covariance is well-conditioned, and ii) using these tools, we extend well-known algorithms for efficiently learning generalized linear models to the dependent setting.},
 author = {Foster, Dylan and Sarkar, Tuhin and Rakhlin, Alexander},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3021369661},
 pages = {851--861},
 pdf = {http://proceedings.mlr.press/v120/foster20a/foster20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning nonlinear dynamical systems from a single trajectory},
 url = {https://proceedings.mlr.press/v120/foster20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-furieri20a,
 abstract = {We study model-free learning methods for the output-feedback Linear Quadratic (LQ) control problem in finite-horizon subject to subspace constraints on the control policy. Subspace constraints naturally arise in the field of distributed control and present a significant challenge in the sense that standard model-based optimization and learning leads to intractable numerical programs in general. Building upon recent results in zeroth-order optimization, we establish model-free sample-complexity bounds for the class of distributed LQ problems where a local gradient dominance constant exists on any sublevel set of the cost function. %which admit a local gradient dominance constant valid on the sublevel set of the cost function. We prove that a fundamental class of distributed control problems - commonly referred to as Quadratically Invariant (QI) problems - as well as others possess this property. To the best of our knowledge, our result is the first sample-complexity bound guarantee on learning globally optimal distributed output-feedback control policies.},
 author = {Furieri, Luca and Zheng, Yang and Kamgarpour, Maryam},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2995134936},
 pages = {287--297},
 pdf = {http://proceedings.mlr.press/v120/furieri20a/furieri20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning the Globally Optimal Distributed LQ Regulator},
 url = {https://proceedings.mlr.press/v120/furieri20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-gahlawat20a,
 abstract = {We present L1-GP, an architecture based on L1 adaptive control and Gaussian Process Regression (GPR) for safe simultaneous control and learning. On one hand, the L1 adaptive control provides stability and transient performance guarantees, which allows for GPR to efficiently and safely learn the uncertain dynamics. On the other hand, the learned dynamics can be conveniently incorporated into the L1 control architecture without sacrificing robustness and tracking performance. Subsequently, the learned dynamics can lead to less conservative designs for performance/robustness tradeoff. We illustrate the efficacy of the proposed architecture via numerical simulations.},
 author = {Gahlawat, Aditya and Zhao, Pan and Patterson, Andrew and Hovakimyan, Naira and Theodorou, Evangelos},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3096360491},
 pages = {826--837},
 pdf = {http://proceedings.mlr.press/v120/gahlawat20a/gahlawat20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {L1-GP: L1 Adaptive Control with Bayesian Learning},
 url = {https://proceedings.mlr.press/v120/gahlawat20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-geist20a,
 abstract = {The identification of the constrained dynamics of mechanical systems is often challenging. Learning methods promise to ease an analytical analysis, but require considerable amounts of data for training. We propose to combine insights from analytical mechanics with Gaussian process regression to improve the modelâs data efficiency and constraint integrity. The result is a Gaussian process model that incorporates a priori constraint knowledge such that its predictions adhere Gaussâ principle of least constraint. In return, predictions of the systemâs acceleration naturally respect potentially non-ideal (non-)holonomic equality constraints. As corollary results, our model enables to infer the acceleration of the unconstrained system from data of the constrained system and enables knowledge transfer between differing constraint configurations. },
 author = {Geist, Andreas and Trimpe, Sebastian},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 pages = {225--234},
 pdf = {http://proceedings.mlr.press/v120/geist20a/geist20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Constrained Dynamics  with Gaussâ Principle adhering Gaussian Processes},
 url = {https://proceedings.mlr.press/v120/geist20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-gong20a,
 abstract = {In light of the Bellman duality, we propose a novel value-policy gradient algorithm to explore and act in infinite-horizon Average-reward Markov Decision Process (AMDP) and show that it has sublinear regret. The algorithm is motivated by the Bellman saddle point formulation. It learns the optimal state-action distribution, which encodes a randomized policy, by interacting with the environment along a single trajectory and making primal-dual updates. The key to the analysis is to establish a connection between the min-max duality gap of Bellman saddle point and the cumulative regret of the learning agent. We show that, for ergodic AMDPs with finite state space $\mathcal{S}$ and action space $\mathcal{A}$ and uniformly bounded mixing times, the algorithmâs $T$-time step regret is $$ R(T)=\tilde{\mathcal{O}}\left( \left(t_{mix}^*\right)^2 \tau^{\frac{3}{2}} \sqrt{(\tau^3 + |\mathcal{A}|) |\mathcal{S}| T} \right), $$ where $t_{mix}^*$ is the worst-case mixing time, $\tau$ is an ergodicity parameter, $T$ is the number of time steps and $\tilde{\mathcal{O}}$ hides polylog factors.},
 author = {Gong, Hao and Wang, Mengdi},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3109999264},
 pages = {862--883},
 pdf = {http://proceedings.mlr.press/v120/gong20a/gong20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Duality Approach for Regret Minimization in Average-Award Ergodic Markov Decision Processes},
 url = {https://proceedings.mlr.press/v120/gong20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-gonzalez20a,
 abstract = {In this paper, we study non-asymptotic deviation bounds of the least squares estimator in Gaussian AR($n$) processes. By relying on martingale concentration inequalities and a tail-bound for $χ^2$ distributed variables, we provide a concentration bound for the sample covariance matrix of the process output. With this, we present a problem-dependent finite-time bound on the deviation probability of any fixed linear combination of the estimated parameters of the AR$(n)$ process. We discuss extensions and limitations of our approach.},
 author = {Gonz\'alez, Rodrigo A. and Rojas, Cristian R.},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2996175837},
 pages = {191--200},
 pdf = {http://proceedings.mlr.press/v120/gonzalez20a/gonzalez20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Finite-Sample Deviation Bound for Stable Autoregressive Processes},
 url = {https://proceedings.mlr.press/v120/gonzalez20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-goyal20a,
 abstract = {In this paper, we propose a framework for performing state space exploration of closed loop control systems. Our approach involves approximating sensitivity and a newly introduced notion of inverse sensitivity by a neural network. We show how the approximation of sensitivity and inverse sensitivity can be used for computing estimates of the reachable set. We then outline algorithms for performing state space exploration by generating trajectories that reach a neighborhood. We demonstrate the effectiveness of our approach by applying it not only to standard linear and nonlinear dynamical systems, but also to nonlinear hybrid systems and also neural network based feedback control systems.},
 author = {Goyal, Manish and Duggirala, Parasara Sridhar},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3095422560},
 pages = {697--697},
 pdf = {http://proceedings.mlr.press/v120/goyal20a/goyal20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {NeuralExplorer: State Space Exploration of Closed Loop Control Systems Using Neural Networks},
 url = {https://proceedings.mlr.press/v120/goyal20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-gravell20a,
 abstract = {Despite decades of research and recent progress in adaptive control and reinforcement learning, there remains a fundamental lack of understanding in designing controllers that provide robustness to inherent non-asymptotic uncertainties arising from models estimated with finite, noisy data. We propose a robust adaptive control algorithm that explicitly incorporates such non-asymptotic uncertainties into the control design. The algorithm has three components: (1) a least-squares nominal model estimator; (2) a bootstrap resampling method that quantifies non-asymptotic variance of the nominal model estimate; and (3) a non-conventional robust control design method using an optimal linear quadratic regulator (LQR) with multiplicative noise. A key advantage of the proposed approach is that the system identification and robust control design procedures both use stochastic uncertainty representations, so that the actual inherent statistical estimation uncertainty directly aligns with the uncertainty the robust controller is being designed against. We show through numerical experiments that the proposed robust adaptive controller can significantly outperform the certainty equivalent controller on both expected regret and measures of regret risk.},
 author = {Gravell, Benjamin and Summers, Tyler},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3008923367},
 pages = {599--607},
 pdf = {http://proceedings.mlr.press/v120/gravell20a/gravell20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Robust Learning-Based Control via Bootstrapped Multiplicative Noise},
 url = {https://proceedings.mlr.press/v120/gravell20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-gupta20a,
 abstract = {Model-based methods are the dominant paradigm for controlling robotic systems, though their efficacy depends heavily on the accuracy of the model used. Deep neural networks have been used to learn models of robot dynamics from data, but they suffer from data-inefficiency and the difficulty to incorporate prior knowledge. We introduce Structured Mechanical Models, a flexible model class for mechanical systems that are data-efficient, easily amenable to prior knowledge, and easily usable with model-based control techniques. The goal of this work is to demonstrate the benefits of using Structured Mechanical Models in lieu of black-box neural networks when modeling robot dynamics. We demonstrate that they generalize better from limited data and yield more reliable model-based controllers on a variety of simulated robotic domains.},
 author = {Gupta, Jayesh K. and Menda, Kunal and Manchester, Zachary and Kochenderfer, Mykel},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3106989601},
 pages = {328--337},
 pdf = {http://proceedings.mlr.press/v120/gupta20a/gupta20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Structured Mechanical Models for Robot Learning and Control},
 url = {https://proceedings.mlr.press/v120/gupta20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-han20a,
 abstract = {Our previous work proposed an approach to localized adaptive and robust control over a large-scale network of systems subject to a single topological modification. In this paper, we develop this approach into an iterative scheme to handle multiple topological modifications over time, which switch between configurations in a finite-state Markov chain. Each system in the network uses its local information to robustly control its own state while also learning the current state of the network topology (i.e. which state of the Markov chain it is currently in). Additionally, each system maintains an estimate of certain parameters for the overall network, for instance, the transition probabilities of the Markov chain, and each system uses standard average consensus methods to update its estimate. We simulate a simple centered hexagon network with 7 systems and 4 different topological states, and show that each system in the network manages to stabilize under a control law that uses only local information, and adapts to the current topology within a reasonable amount of time after a switch is made.},
 author = {Han, Soojean},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3110517554},
 pages = {687--696},
 pdf = {http://proceedings.mlr.press/v120/han20a/han20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Localized Learning of Robust Controllers for Networked Systems with Dynamic Topology},
 url = {https://proceedings.mlr.press/v120/han20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-hanson20a,
 abstract = {It is well-known that continuous-time recurrent neural nets are universal approximators for continuous-time dynamical systems. However, existing results provide approximation guarantees only for finite-time trajectories. In this work, we show that infinite-time trajectories generated by dynamical systems that are stable in a certain sense can be reproduced arbitrarily accurately by recurrent neural nets. For a subclass of these stable systems, we provide quantitative estimates on the sufficient number of neurons needed to achieve a specified error tolerance. },
 author = {Hanson, Joshua and Raginsky, Maxim},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3095255062},
 pages = {384--392},
 pdf = {http://proceedings.mlr.press/v120/hanson20a/hanson20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Universal Simulation of Stable Dynamical Systems by Recurrent Neural Nets},
 url = {https://proceedings.mlr.press/v120/hanson20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-hewing20a,
 abstract = {Established techniques for simulation and prediction with Gaussian process (GP) dynamics often implicitly make use of an independence assumption on successive function evaluations of the dynamics model. This can result in significant error and underestimation of the prediction uncertainty, potentially leading to failures in safety-critical applications. This paper discusses methods that explicitly take the correlation of successive function evaluations into account. We first describe two sampling-based techniques; one approach provides samples of the true trajectory distribution, suitable for `ground truth' simulations, while the other draws function samples from basis function approximations of the GP. Second, we propose a linearization-based technique that directly provides approximations of the trajectory distribution, taking correlations explicitly into account. We demonstrate the procedures in simple numerical examples, contrasting the results with established methods.},
 author = {Hewing, Lukas and Arcari, Elena and Fr\"ohlich, Lukas P. and Zeilinger, Melanie N.},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2996180273},
 pages = {424--434},
 pdf = {http://proceedings.mlr.press/v120/hewing20a/hewing20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On Simulation and Trajectory Prediction with Gaussian Process Dynamics},
 url = {https://proceedings.mlr.press/v120/hewing20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-jain20a,
 abstract = {Model predictive control (MPC) can provide significant energy cost savings in building operations in the form of energy-efficient control with better occupant comfort, lower peak demand charges, and risk-free participation in demand response. However, the engineering effort required to obtain physics-based models of buildings is considered to be the biggest bottleneck in making MPC scalable to real buildings. In this paper, we propose a data-driven control algorithm based on neural networks to reduce this cost of model identification. Our approach does not require building domain expertise or retrofitting of existing heating and cooling systems. We validate our learning and control algorithms on a two-story building with ten independently controlled zones, located in Italy. We learn dynamical models of energy consumption and zone temperatures with high accuracy and demonstrate energy savings and better occupant comfort compared to the default system controller.},
 author = {Jain, Achin and Smarra, Francesco and Reticcioli, Enrico and D'Innocenzo, Alessandro and Morari, Manfred},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W4287902146},
 pages = {445--454},
 pdf = {http://proceedings.mlr.press/v120/jain20a/jain20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {NeurOpt: Neural network based optimization for building energy management and climate control},
 url = {https://proceedings.mlr.press/v120/jain20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-jansch-porto20a,
 abstract = {Markovian jump linear systems (MJLS) are an important class of dynamical systems that arise in many control applications. In this paper, we introduce the problem of controlling unknown (discrete-time) MJLS as a new benchmark for policy-based reinforcement learning of Markov decision processes (MDPs) with mixed continuous/discrete state variables. Compared with the traditional linear quadratic regulator (LQR), our proposed problem leads to a special hybrid MDP (with mixed continuous and discrete variables) and poses significant new challenges due to the appearance of an underlying Markov jump parameter governing the mode of the system dynamics. Specifically, the state of a MJLS does not form a Markov chain and hence one cannot study the MJLS control problem as a MDP with solely continuous state variable. However, one can augment the state and the jump parameter to obtain a MDP with a mixed continuous/discrete state space. We discuss how control theory sheds light on the policy parameterization of such hybrid MDPs. Then we modify the widely used natural policy gradient method to directly learn the optimal state feedback control policy for MJLS without identifying either the system dynamics or the transition probability of the switching parameter. We implement the (data-driven) natural policy gradient method on different MJLS examples. Our simulation results suggest that the natural gradient method can efficiently learn the optimal controller for MJLS with unknown dynamics.},
 author = {Jansch-Porto, Joao Paulo and Hu, Bin and Dullerud, Geir},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108902193},
 pages = {947--957},
 pdf = {http://proceedings.mlr.press/v120/jansch-porto20a/jansch-porto20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Policy Learning of MDPs with Mixed Continuous/Discrete Variables: A Case Study on Model-Free Control of Markovian Jump Systems},
 url = {https://proceedings.mlr.press/v120/jansch-porto20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-ji20a,
 abstract = {In this paper we build a bridge between feed-forward neural networks and delayed dynamical systems. As an initial demonstration, we capture the car-following behavior of a connected automated vehicle that includes time delay by using both simulation data and experimental data. We construct a delayed feed-forward neural network (DFNN) and introduce a training algorithm in order to learn the delay. We demonstrate that this algorithm works well on the proposed structures.},
 author = {Ji, Xunbi A. and Moln\'ar, Tam\'as G. and Avedisov, Sergei S. and Orosz, G\'abor},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3093263913},
 pages = {127--136},
 pdf = {http://proceedings.mlr.press/v120/ji20a/ji20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feed-forward Neural Networks with Trainable Delay},
 url = {https://proceedings.mlr.press/v120/ji20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-jia20a,
 abstract = {This paper studies model-based reinforcement learning (RL) for regret minimization. We focus on finite-horizon episodic RL where the transition model $P$ belongs to a known family of models $\mathcal{P}$, a special case of which is when models in $\mathcal{P}$ take the form of linear mixtures: $P_{\theta} = \sum_{i=1}^{d} \theta_{i}P_{i}$. We propose a model based RL algorithm that is based on optimism principle: In each episode, the set of models that are `consistent' with the data collected is constructed. The criterion of consistency is based on the total squared error of that the model incurs on the task of predicting \emph{values} as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, which, in the special case of linear mixtures, the regret bound takes the form $\tilde{\mathcal{O}}(d\sqrt{H^{3}T})$, where $H$, $T$ and $d$ are the horizon, total number of steps and dimension of $\theta$, respectively. In particular, this regret bound is independent of the total number of states or actions, and is close to a lower bound $\Omega(\sqrt{HdT})$. For a general model family $\mathcal{P}$, the regret bound is derived using the notion of the so-called Eluder dimension proposed by Russo & Van Roy (2014).},
 author = {Jia, Zeyu and Yang, Lin and Szepesvari, Csaba and Wang, Mengdi},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3029753614},
 pages = {666--686},
 pdf = {http://proceedings.mlr.press/v120/jia20a/jia20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Model-Based Reinforcement Learning with Value-Targeted Regression},
 url = {https://proceedings.mlr.press/v120/jia20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-karnchanachari20a,
 abstract = {Model Predictive Control (MPC) is a powerful control technique that handles constraints, takes the system's dynamics into account, and optimizes for a given cost function. In practice, however, it often requires an expert to craft and tune this cost function and find trade-offs between different state penalties to satisfy simple high level objectives. In this paper, we use Reinforcement Learning and in particular value learning to approximate the value function given only high level objectives, which can be sparse and binary. Building upon previous works, we present improvements that allowed us to successfully deploy the method on a real world unmanned ground vehicle. Our experiments show that our method can learn the cost function from scratch and without human intervention, while reaching a performance level similar to that of an expert-tuned MPC. We perform a quantitative comparison of these methods with standard MPC approaches both in simulation and on the real robot.},
 author = {Karnchanachari, Napat and de la Iglesia Valls, Miguel and Hoeller, David and Hutter, Marco},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3092793148},
 pages = {211--224},
 pdf = {http://proceedings.mlr.press/v120/karnchanachari20a/karnchanachari20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Practical Reinforcement Learning For MPC: Learning from sparse objectives in under an hour on a real robot},
 url = {https://proceedings.mlr.press/v120/karnchanachari20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-khodayi-mehr20a,
 abstract = {In this paper we propose a new model-based unsupervised learning method, called VarNet, for the solution of partial differential equations (PDEs) using deep neural networks (NNs). Particularly, we propose a novel loss function that relies on the variational (integral) form of PDEs as apposed to their differential form which is commonly used in the literature. Our loss function is discretization-free, highly parallelizable, and more effective in capturing the solution of PDEs since it employs lower-order derivatives and trains over measure non-zero regions of space-time. Given this loss function, we also propose an approach to optimally select the space-time samples, used to train the NN, that is based on the feedback provided from the PDE residual. The models obtained using VarNet are smooth and do not require interpolation. They are also easily differentiable and can directly be used for control and optimization of PDEs. Finally, VarNet can straight-forwardly incorporate parametric PDE models making it a natural tool for model order reduction (MOR) of PDEs. We demonstrate the performance of our method through extensive numerical experiments for the advection-diffusion PDE as an important case-study.},
 author = {Khodayi-Mehr, Reza and Zavlanos, Michael},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2994946702},
 pages = {298--307},
 pdf = {http://proceedings.mlr.press/v120/khodayi-mehr20a/khodayi-mehr20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {VarNet: Variational Neural Networks for the Solution of Partial Differential Equations},
 url = {https://proceedings.mlr.press/v120/khodayi-mehr20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-khojasteh20a,
 abstract = {This paper focuses on learning a model of system dynamics online while satisfying safety constraints.Our motivation is to avoid offline system identification or hand-specified dynamics models and allowa system to safely and autonomously estimate and adapt its own model during online operation.Given streaming observations of the system state, we use Bayesian learning to obtain a distributionover the system dynamics. In turn, the distribution is used to optimize the system behavior andensure safety with high probability, by specifying a chance constraint over a control barrier function.},
 author = {Khojasteh, Mohammad Javad and Dhiman, Vikas and Franceschetti, Massimo and Atanasov, Nikolay},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2996109218},
 pages = {781--792},
 pdf = {http://proceedings.mlr.press/v120/khojasteh20a/khojasteh20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Probabilistic Safety Constraints for Learned High Relative Degree System Dynamics},
 url = {https://proceedings.mlr.press/v120/khojasteh20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-kim20a,
 abstract = {Many dynamical systems exhibit similar structure, as often captured by hand-designed simplified models that can be used for analysis and control. We develop a method for learning to correspond pairs of dynamical systems via a learned latent dynamical system. Given trajectory data from two dynamical systems, we learn a shared latent state space and a shared latent dynamics model, along with an encoder-decoder pair for each of the original systems. With the learned correspondences in place, we can use a simulation of one system to produce an imagined motion of its counterpart. We can also simulate in the learned latent dynamics and synthesize the motions of both corresponding systems, as a form of bisimulation. We demonstrate the approach using pairs of controlled bipedal walkers, as well as by pairing a walker with a controlled pendulum.},
 author = {Kim, Nam Hee and Xie, Zhaoming and van de Panne, Michiel},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2993139026},
 pages = {105--117},
 pdf = {http://proceedings.mlr.press/v120/kim20a/kim20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning to Correspond Dynamical Systems},
 url = {https://proceedings.mlr.press/v120/kim20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-kim20b,
 abstract = {In this paper, we introduce Hamilton-Jacobi-Bellman (HJB) equations for Q-functions in continuous time optimal control problems with Lipschitz continuous controls. The standard Q-function used in reinforcement learning is shown to be the unique viscosity solution of the HJB equation. A necessary and sufficient condition for optimality is provided using the viscosity solution framework. By using the HJB equation, we develop a Q-learning method for continuous-time dynamical systems. A DQN-like algorithm is also proposed for high-dimensional state and control spaces. The performance of the proposed Q-learning algorithm is demonstrated using 1-, 10- and 20-dimensional dynamical systems.},
 author = {Kim, Jeongho and Yang, Insoon},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2995968686},
 pages = {739--748},
 pdf = {http://proceedings.mlr.press/v120/kim20b/kim20b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Hamilton-Jacobi-Bellman Equations for Q-Learning in Continuous Time},
 url = {https://proceedings.mlr.press/v120/kim20b.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-lambert20a,
 abstract = {Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, with little development of the general framework. In this paper, we identify a fundamental issue of the standard MBRL framework -- what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t.~the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of one-step ahead predictions is not always correlated with control performance. This observation highlights a critical limitation in the MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of research for addressing this issue.},
 author = {Lambert, Nathan and Amos, Brandon and Yadan, Omry and Calandra, Roberto},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3006331850},
 pages = {761--770},
 pdf = {http://proceedings.mlr.press/v120/lambert20a/lambert20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Objective Mismatch in Model-based Reinforcement Learning},
 url = {https://proceedings.mlr.press/v120/lambert20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-laracy20a,
 abstract = { This paper provides a novel combination of Reference Governors (RG) and Iterative Learning Control (ILC) to address the issue of simultaneous learning and constraint management in systems that perform a task repeatedly. The proposed control strategy leverages the measured output from the previous iterations to improve tracking, while guaranteeing constraint satisfaction during the learning process. To achieve this, the system is modeled by a linear system with polytopic uncertainties. An RG solution based on a robust Maximal Admissable Set (MAS) is proposed that endows the ILC algorithm with constraint management capabilities. An update law on the MAS is proposed to further improve performance.},
 author = {Laracy, Aidan and Ossareh, Hamid},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107231407},
 pages = {340--349},
 pdf = {http://proceedings.mlr.press/v120/laracy20a/laracy20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Constraint Management for Batch Processes Using Iterative Learning Control and Reference Governors},
 url = {https://proceedings.mlr.press/v120/laracy20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-lederer20a,
 abstract = {Supervised machine learning is often applied to identify system dynamics where first principle methods fail. When combining learning with control methods, probabilistic regression is typically applied to increase robustness against learning errors and analyze the stability of the closed-loop system. Although this approach allows to formulate performance guarantees for many control techniques, the obtained bounds are usually conservative, and cannot be employed for efficient control parameter tuning. Therefore, we reformulate the parameter tuning problem using robust optimization with performance constraints based on Lyapunov theory. By relaxing the problem through scenario optimization we derive a provably optimal method for control parameter tuning. We demonstrate its flexibility and efficiency on parameter tuning problems for a feedback linearizing and a computed torque controller.},
 author = {Lederer, Armin and Capone, Alexandre and Hirche, Sandra},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3087397555},
 pages = {465--475},
 pdf = {http://proceedings.mlr.press/v120/lederer20a/lederer20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Parameter Optimization for Learning-based Control of Control-Affine Systems},
 url = {https://proceedings.mlr.press/v120/lederer20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-lee20a,
 abstract = {The use of target networks is a common practice in deep reinforcement learning for stabilizing the training; however, theoretical understanding of this technique is still limited. In this paper, we study the so-called periodic Q-learning algorithm (PQ-learning for short), which resembles the technique used in deep Q-learning for solving infinite-horizon discounted Markov decision processes (DMDP) in the tabular setting. PQ-learning maintains two separate Q-value estimates - the online estimate and target estimate. The online estimate follows the standard Q-learning update, while the target estimate is updated periodically. In contrast to the standard Q-learning, PQ-learning enjoys a simple finite time analysis and achieves better sample complexity for finding an epsilon-optimal policy. Our result provides a preliminary justification of the effectiveness of utilizing target estimates or networks in Q-learning algorithms.},
 author = {Lee, Donghwan and He, Niao},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3007716277},
 pages = {582--598},
 pdf = {http://proceedings.mlr.press/v120/lee20a/lee20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Periodic Q-Learning},
 url = {https://proceedings.mlr.press/v120/lee20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-li20a,
 abstract = {In Bansal et al. (2019), a novel visual navigation framework that combines learning-based and model-based approaches has been proposed. Specifically, a Convolutional Neural Network (CNN) predicts a waypoint that is used by the dynamics model for planning and tracking a trajectory to the waypoint. However, the CNN inevitably makes prediction errors which often lead to collisions in cluttered and tight spaces. In this paper, we present a novel Hamilton-Jacobi (HJ) reachability-based method to generate supervision for the CNN for waypoint prediction in an unseen environment. By modeling CNN prediction error as "disturbances" in robot's dynamics, our generated waypoints are robust to these disturbances, and consequently to the prediction errors. Moreover, using globally optimal HJ reachability analysis leads to predicting waypoints that are time-efficient and avoid greedy behavior. Through simulations and hardware experiments, we demonstrate the advantages of the proposed approach on navigating through cluttered, narrow indoor environments.},
 author = {Li, Anjian and Bansal, Somil and Giovanis, Georgios and Tolani, Varun and Tomlin, Claire and Chen, Mo},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2996666233},
 pages = {500--510},
 pdf = {http://proceedings.mlr.press/v120/li20a/li20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Generating Robust Supervision for Learning-Based Visual Navigation Using Hamilton-Jacobi Reachability},
 url = {https://proceedings.mlr.press/v120/li20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-li20b,
 abstract = {Abstract dynamic programming models are used to analyze $λ$-policy iteration with randomization algorithms. Particularly, contractive models with infinite policies are considered and it is shown that well-posedness of the $λ$-operator plays a central role in the algorithm. The operator is known to be well-posed for problems with finite states, but our analysis shows that it is also well-defined for the contractive models with infinite states studied. Similarly, the algorithm we analyze is known to converge for problems with finite policies, but we identify the conditions required to guarantee convergence with probability one when the policy space is infinite regardless of the number of states. Guided by the analysis, we exemplify a data-driven approximated implementation of the algorithm for estimation of optimal costs of constrained linear and nonlinear control problems. Numerical results indicate potentials of this method in practice.},
 author = {Li, Yuchao and Johansson, Karl Henrik and M{\aa}rtensson, Jonas},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2995761102},
 pages = {540--549},
 pdf = {http://proceedings.mlr.press/v120/li20b/li20b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Lambda-Policy Iteration with Randomization for Contractive Models with Infinite Policies: Well-Posedness and Convergence (Extended Version)},
 url = {https://proceedings.mlr.press/v120/li20b.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-li20c,
 abstract = {This article considers a distributed reinforcement learning problem for decentralized linear quadratic (LQ) control with partial state observations and local costs. We propose a zero-order distributed policy optimization algorithm (ZODPO) that learns linear local controllers in a distributed fashion, leveraging the ideas of policy gradient, zero-order optimization, and consensus algorithms. In ZODPO, each agent estimates the global cost by consensus, and then conducts local policy gradient in parallel based on zero-order gradient estimation. ZODPO only requires limited communication and storage even in large-scale systems. Further, we investigate the nonasymptotic performance of ZODPO and show that the sample complexity to approach a stationary point is polynomial with the error tolerance’s inverse and the problem dimensions, demonstrating the scalability of ZODPO. We also show that the controllers generated throughout ZODPO are stabilizing controllers with high probability. Last, we numerically test ZODPO on multizone HVAC systems.},
 author = {Li, Yingying and Tang, Yujie and Zhang, Runyu and Li, Na},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3213818741},
 pages = {814--814},
 pdf = {http://proceedings.mlr.press/v120/li20c/li20c.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Distributed Reinforcement Learning for Decentralized Linear Quadratic Control: A Derivative-Free Policy Optimization Approach},
 url = {https://proceedings.mlr.press/v120/li20c.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-liu20a,
 abstract = {We study the problem of safe learning and exploration in sequential control problems. The goal is to safely collect data samples from operating in an environment, in order to learn to achieve a challenging control goal (e.g., an agile maneuver close to a boundary). A central challenge in this setting is how to quantify uncertainty in order to choose provably-safe actions that allow us to collect informative data and reduce uncertainty, thereby achieving both improved controller safety and optimality. To address this challenge, we present a deep robust regression model that is trained to directly predict the uncertainty bounds for safe exploration. We derive generalization bounds for learning, and connect them with safety and stability bounds in control. We demonstrate empirically that our robust regression approach can outperform the conventional Gaussian process (GP) based safe exploration in settings where it is difficult to specify a good GP prior.},
 author = {Liu, Anqi and Shi, Guanya and Chung, Soon-Jo and Anandkumar, Anima and Yue, Yisong},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107744858},
 pages = {608--619},
 pdf = {http://proceedings.mlr.press/v120/liu20a/liu20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Robust Regression for Safe Exploration in Control},
 url = {https://proceedings.mlr.press/v120/liu20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-matschek20a,
 abstract = {Increased autonomy of controllers in tasks with uncertainties stemming from the interaction with the environment can be achieved by incorporation of learning. Examples are control tasks where the system should follow a reference which depends on measurement data from surrounding systems as e.g. humans or other control systems. We propose a learning strategy for Gaussian processes to model, filter and predict references for control systems under model predictive control. Hereby constraints in the learning are included to achieve safety guarantees as trackability and recursive feasibility. An illustrative simulation example for motion compensation is given which shows performance improvements of combined constrained learning and predictive control besides the provided guarantees.},
 author = {Matschek, Janine and Findeisen, Rolf},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3100331536},
 pages = {511--520},
 pdf = {http://proceedings.mlr.press/v120/matschek20a/matschek20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning supported Model Predictive Control for Tracking of Periodic References},
 url = {https://proceedings.mlr.press/v120/matschek20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-mazzoleni20a,
 abstract = {Continuous-time parametric models of dynamical systems are usually preferred given their physical interpretation. When there is a lack of prior physical knowledge, the user is faced with the model selection issue. In this paper, we propose a non-parametric approach to estimate a continuous-time stable linear model from data, while automatically selecting a proper structure of the transfer function and guaranteeing to preserve the system stability properties. Results show how the proposed approach outperforms the state of the art.},
 author = {Mazzoleni, Mirko and Scandella, Matteo and Formentin, Simone and Previdi, Fabio},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107167251},
 pages = {267--276},
 pdf = {http://proceedings.mlr.press/v120/mazzoleni20a/mazzoleni20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Black-box continuous-time transfer function estimation with stability guarantees: a kernel-based approach},
 url = {https://proceedings.mlr.press/v120/mazzoleni20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-menta20a,
 abstract = {Hybrid control problems are complicated by the need to make a suitable sequence of discrete decisions related to future modes of operation of the system. Model predictive control (MPC) encodes a finite-horizon truncation of such problems as a mixed-integer program, and then imposes a cost and/or constraints on the terminal state intended to reflect all post-horizon behaviour. However, these are often ad hoc choices tuned by hand after empirically observing performance. We present a learning method that sidesteps this problem, in which the so-called N-step Q-function of the problem is approximated from below, using Bendersâ decomposition. The function takes a state and a sequence of N control decisions as arguments, and therefore extends the traditional notion of a Q-function from reinforcement learning. After learning it from a training process exploring the state-input space, we use it in place of the usual MPC objective. We take an example hybrid control task and show that it can be completed successfully with a shorter planning horizon than conventional hybrid MPC thanks to our proposed method. Furthermore, we report that Q-functions trained with long horizons can be truncated to a shorter horizon for online use, yielding simpler control laws with apparently little loss of performance.},
 author = {Menta, Sandeep and Warrington, Joseph and Lygeros, John and Morari, Manfred},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108484707},
 pages = {118--126},
 pdf = {http://proceedings.mlr.press/v120/menta20a/menta20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning solutions to hybrid control problems using Benders cuts},
 url = {https://proceedings.mlr.press/v120/menta20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-moe20a,
 abstract = {Recurrent Neural Networks (RNNs) have a form of memory where the output from a node at one timestep is fed back as input the next timestep in addition to data from the previous layer. This makes them highly suitable for timeseries analysis. However, standard RNNs have known weaknesses such as exploding/vanishing gradient and thereby struggle with a long-term memory. In this paper, we suggest a new recurrent network structure called Linear Antisymmetric RNN (LARNN). This structure is based on the numerical solution to an Ordinary Differential Equation (ODE) with stability properties resulting in a stable solution, which corresponds to long-term memory and trainability. Three different numerical methods are suggested to solve the ODE: Forward and Backward Euler and the midpoint method. The suggested structure has been implemented in Keras and several simulated datasets have been used to evaluate the performance. In the investigated cases, the LARNN performs better or similar to the Long Short Term Memory (LSTM) network which is the current state of the art for RNNs.},
 author = {Moe, Signe and Remonato, Filippo and Gr{\o}tli, Esten Ingar and Gravdahl, Jan Tommy},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3109220768},
 pages = {170--178},
 pdf = {http://proceedings.mlr.press/v120/moe20a/moe20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Linear Antisymmetric Recurrent Neural Networks},
 url = {https://proceedings.mlr.press/v120/moe20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-mohammadi20a,
 abstract = {Model-free reinforcement learning attempts to find an optimal control action for an unknown dynamical system by directly searching over the parameter space of controllers. The convergence behavior and statistical properties of these approaches are often poorly understood because of the nonconvex nature of the underlying optimization problems as well as the lack of exact gradient computation. In this paper, we examine the standard infinite-horizon linear quadratic regulator problem for continuous-time systems with unknown state-space parameters. We provide theoretical bounds on the convergence rate and sample complexity of a random search method. Our results demonstrate that the required simulation time for achieving $\epsilon$-accuracy in a model-free setup and the total number of function evaluations are both of $O (\log \, (1/\epsilon) )$.},
 author = {Mohammadi, Hesameddin and Jovanovic', Mihailo R. and Soltanolkotabi, Mahdi},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3098615412},
 pages = {531--539},
 pdf = {http://proceedings.mlr.press/v120/mohammadi20a/mohammadi20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning the model-free linear quadratic regulator via random search.},
 url = {https://proceedings.mlr.press/v120/mohammadi20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-palan20a,
 abstract = {We consider the problem of learning a linear control policy for a linear dynamical system, from demonstrations of an expert regulating the system. The standard approach to this problem is policy fitting, which fits a linear policy by minimizing a loss function between the demonstrations and the policy's outputs plus a regularization function that encodes prior knowledge. Despite its simplicity, this method fails to learn policies with low or even finite cost when there are few demonstrations. We propose to add an additional constraint to policy fitting, that the policy is the solution to some LQR problem, i.e., optimal in the stochastic control sense for some choice of quadratic cost. We refer to this constraint as a Kalman constraint. Policy fitting with a Kalman constraint requires solving an optimization problem with convex cost and bilinear constraints. We propose a heuristic method, based on the alternating direction method of multipliers (ADMM), to approximately solve this problem. Numerical experiments demonstrate that adding the Kalman constraint allows us to learn good, i.e., low cost, policies even when very few data are available.},
 author = {Palan, Malayandi and Barratt, Shane and McCauley, Alex and Sadigh, Dorsa and Sindhwani, Vikas and Boyd, Stephen},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108150703},
 pages = {374--383},
 pdf = {http://proceedings.mlr.press/v120/palan20a/palan20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Fitting a Linear Control Policy to Demonstrations with a Kalman Constraint},
 url = {https://proceedings.mlr.press/v120/palan20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-pereira20a,
 abstract = {We present a deep recurrent neural network architecture to solve a class of stochastic optimal control problems described by fully nonlinear Hamilton Jacobi Bellman partial differential equations. Such PDEs arise when considering stochastic dynamics characterized by uncertainties that are additive, state dependent, and control multiplicative. Stochastic models with these characteristics are important in computational neuroscience, biology, finance, and aerospace systems and provide a more accurate representation of actuation than models with only additive uncertainty. Previous literature has established the inadequacy of the linear HJB theory for such problems, so instead, methods relying on the generalized version of the Feynman-Kac lemma have been proposed resulting in a system of second-order Forward-Backward SDEs. However, so far, these methods suffer from compounding errors resulting in lack of scalability. In this paper, we propose a deep learning based algorithm that leverages the second-order FBSDE representation and LSTM-based recurrent neural networks to not only solve such stochastic optimal control problems but also overcome the problems faced by traditional approaches, including scalability. The resulting control algorithm is tested on a high-dimensional linear system and three nonlinear systems from robotics and biomechanics in simulation to demonstrate feasibility and out-performance against previous methods.},
 author = {Pereira, Marcus and Wang, Ziyi and Chen, Tianrong and Reed, Emily and Theodorou, Evangelos},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108861688},
 pages = {728--738},
 pdf = {http://proceedings.mlr.press/v120/pereira20a/pereira20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feynman-Kac Neural Network Architectures for Stochastic Control Using Second-Order FBSDE Theory.},
 url = {https://proceedings.mlr.press/v120/pereira20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-pertsch20a,
 abstract = {Temporal observations such as videos contain essential information about the dynamics of the underlying scene, but they are often interleaved with inessential, predictable details. One way of dealing with this problem is by focusing on the most informative moments in a sequence. We propose a model that learns to discover these important events and the times when they occur and uses them to represent the full sequence. We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates a video's keyframes and then inpaints the rest by generating the frames at the intervening times. We propose a fully differentiable formulation to efficiently learn this procedure. We show that KeyIn finds informative keyframes in several datasets with different dynamics and visual properties. KeyIn outperforms other recent hierarchical predictive models for planning. For more details, please see the project website at \url{this https URL}.},
 author = {Pertsch, Karl and Rybkin, Oleh and Yang, Jingyun and Zhou, Shenghao and Derpanis, Konstantinos and Daniilidis, Kostas and Lim, Joseph and Jaegle, Andrew},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107295910},
 pages = {969--979},
 pdf = {http://proceedings.mlr.press/v120/pertsch20a/pertsch20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning},
 url = {https://proceedings.mlr.press/v120/pertsch20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-qu20a,
 abstract = {We study reinforcement learning (RL) in a setting with a network of agents whose states and actions interact in a local manner where the objective is to find localized policies such that the (discounted) global reward is maximized. A fundamental challenge in this setting is that the state-action space size scales exponentially in the number of agents, rendering the problem intractable for large networks. In this paper, we propose a Scalable Actor-Critic (SAC) framework that exploits the network structure and finds a localized policy that is a $O(\rho^\kappa)$-approximation of a stationary point of the objective for some $\rho\in(0,1)$, with complexity that scales with the local state-action space size of the largest $\kappa$-hop neighborhood of the network.},
 author = {Qu, Guannan and Wierman, Adam and Li, Na},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2993990522},
 pages = {256--266},
 pdf = {http://proceedings.mlr.press/v120/qu20a/qu20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Scalable Reinforcement Learning of Localized Policies for Multi-Agent Networked Systems},
 url = {https://proceedings.mlr.press/v120/qu20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-rana20a,
 abstract = {Robotic tasks often require motions with complex geometric structures. We present an approach to learn such motions from a limited number of human demonstrations by exploiting the regularity properties of human motions e.g. stability, smoothness, and boundedness. The complex motions are encoded as rollouts of a stable dynamical system, which, under a change of coordinates defined by a diffeomorphism, is equivalent to a simple, hand-specified dynamical system. As an immediate result of using diffeomorphisms, the stability property of the hand-specified dynamical system directly carry over to the learned dynamical system. Inspired by recent works in density estimation, we propose to represent the diffeomorphism as a composition of simple parameterized diffeomorphisms. Additional structure is imposed to provide guarantees on the smoothness of the generated motions. The efficacy of this approach is demonstrated through validation on an established benchmark as well demonstrations collected on a real-world robotic system.},
 author = {Rana, Muhammad Asif and Li, Anqi and Fox, Dieter and Boots, Byron and Ramos, Fabio and Ratliff, Nathan},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3029926839},
 pages = {630--639},
 pdf = {http://proceedings.mlr.press/v120/rana20a/rana20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Euclideanizing Flows: Diffeomorphic Reduction for Learning Stable Dynamical Systems},
 url = {https://proceedings.mlr.press/v120/rana20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-revay20a,
 abstract = {Stability of recurrent models is closely linked with trainability, generalizability and in some applications, safety. Methods that train stable recurrent neural networks, however, do so at a significant cost to expressibility. We propose an implicit model structure that allows for a convex parametrization of stable models using contraction analysis of non-linear systems. Using these stability conditions we propose a new approach to model initialization and then provide a number of empirical results comparing the performance of our proposed model set to previous stable RNNs and vanilla RNNs. By carefully controlling stability in the model, we observe a significant increase in the speed of training and model performance.},
 author = {Revay, Max and Manchester, Ian},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3106729350},
 pages = {393--403},
 pdf = {http://proceedings.mlr.press/v120/revay20a/revay20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Contracting Implicit Recurrent Neural Networks: Stable Models with Improved Trainability.},
 url = {https://proceedings.mlr.press/v120/revay20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-ruiz20a,
 abstract = {Vector autoregressive (VAR) models are widely used for causal discovery and forecasting in multivariate time series analyses in fields as diverse as neuroscience, environmental science, and econometrics. In the high-dimensional setting, model parameters are typically estimated by L1-regularized maximum likelihood; yet, when applied to VAR models, this technique produces a sizable trade-off between sparsity and bias with the choice of the regularization hyperparameter, and thus between causal discovery and prediction. That is, low-bias estimation entails dense parameter selection, and sparse selection entails increased bias; the former is useful in forecasting but less likely to yield scientific insight leading to discovery of causal influences, and conversely for the latter. This paper presents a scalable algorithm for simultaneous low-bias and low-variance estimation (hence good prediction) with sparse selection for high-dimensional VAR models. The method leverages the recently developed Union of Intersections (UoI) algorithmic framework for flexible, modular, and scalable feature selection and estimation that allows control of false discovery and false omission in feature selection while maintaining low bias and low variance. This paper demonstrates the superior performance of the UoI-VAR algorithm compared with other methods in simulation studies, exhibits its application in data analysis, and illustrates its good algorithmic scalability in multi-node distributed memory implementations.},
 author = {Ruiz, Trevor and Bhattacharyya, Sharmodeep and Balasubramanian, Mahesh and Bouchard, Kristofer},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2970272092},
 pages = {55--64},
 pdf = {http://proceedings.mlr.press/v120/ruiz20a/ruiz20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sparse, Low-bias, and Scalable Estimation of High Dimensional Vector Autoregressive Models via Union of Intersections},
 url = {https://proceedings.mlr.press/v120/ruiz20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-seidman20a,
 abstract = {The fragility of deep neural networks to adversarially-chosen inputs has motivated the need to revisit deep learning algorithms. Including adversarial examples during training is a popular defense mechanism against adversarial attacks. This mechanism can be formulated as a min-max optimization problem, where the adversary seeks to maximize the loss function using an iterative first-order algorithm while the learner attempts to minimize it. However, finding adversarial examples in this way causes excessive computational overhead during training. By interpreting the min-max problem as an optimal control problem, it has recently been shown that one can exploit the compositional structure of neural networks in the optimization problem to improve the training time significantly. In this paper, we provide the first convergence analysis of this adversarial training algorithm by combining techniques from robust optimal control and inexact oracle methods in optimization. Our analysis sheds light on how the hyperparameters of the algorithm affect the its stability and convergence. We support our insights with experiments on a robust classification problem.},
 author = {Seidman, Jacob H. and Fazlyab, Mahyar and Preciado, Victor M. and Pappas, George J.},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108730336},
 pages = {884--893},
 pdf = {http://proceedings.mlr.press/v120/seidman20a/seidman20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Robust Deep Learning as Optimal Control: Insights and Convergence Guarantees},
 url = {https://proceedings.mlr.press/v120/seidman20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-sekhon20a,
 abstract = {Ships, or vessels, often sail in and out of cluttered environments over the course of their trajectories. Safe navigation in such cluttered scenarios requires an accurate estimation of the intent of neighboring vessels and their effect on the self and vice-versa well into the future. In manned vessels, this is achieved by constant communication between people on board, nautical experience, and audio and visual signals. In this paper we propose a deep neural network based architecture to predict intent of neighboring vessels into the future for an unmanned vessel solely based on positional data.},
 author = {Sekhon, Jasmine and Fleming, Cody},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2996268324},
 pages = {318--327},
 pdf = {http://proceedings.mlr.press/v120/sekhon20a/sekhon20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Spatially and Temporally Attentive Joint Trajectory Prediction Framework for Modeling Vessel Intent},
 url = {https://proceedings.mlr.press/v120/sekhon20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-senoz20a,
 abstract = {We address the problem of online Bayesian state and parameter tracking in autoregressive (AR) models with time-varying process noise variance. The involved marginalization and expectation integrals cannot be analytically solved. Moreover, the online tracking constraint makes sampling and batch learning methods unsuitable for this problem. We propose a hybrid variational message passing algorithm that robustly tracks the time-varying dynamics of the latent states, AR coefficients and process noise variance. Since message passing in a factor graph is a highly modular inference approach, the proposed methods easily extend to other non-stationary dynamic modeling problems.},
 author = {Senoz, Ismail and Podusenko, Albert and Kouw, Wouter M. and de Vries, Bert},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3034921006},
 pages = {95--104},
 pdf = {http://proceedings.mlr.press/v120/senoz20a/senoz20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Bayesian joint state and parameter tracking in autoregressive models},
 url = {https://proceedings.mlr.press/v120/senoz20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-serrano20a,
 abstract = {We consider the problem of computing optimal policies in average-reward Markov decision processes. This classical problem can be formulated as a linear program directly amenable to saddle-point optimization methods, albeit with a number of variables that is linear in the number of states. To address this issue, recent work has considered a linearly relaxed version of the resulting saddle-point problem. Our work aims at achieving a better understanding of this relaxed optimization problem by characterizing the conditions necessary for convergence to the optimal policy, and designing an optimization algorithm enjoying fast convergence rates that are independent of the size of the state space. Notably, our characterization points out some potential issues with previous work.},
 author = {Serrano, Joan Bas and Neu, Gergely},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2977139627},
 pages = {413--423},
 pdf = {http://proceedings.mlr.press/v120/serrano20a/serrano20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Faster saddle-point optimization for solving large-scale Markov decision processes},
 url = {https://proceedings.mlr.press/v120/serrano20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-seyde20a,
 abstract = {Deep exploration requires coordinated long-term planning. We present a model-based reinforcement learning algorithm that guides policy learning through a value function that exhibits optimism in the face of uncertainty. We capture uncertainty over values by combining predictions from an ensemble of models and formulate an upper confidence bound (UCB) objective to recover optimistic estimates. Training the policy on ensemble rollouts with the learned value function as the terminal cost allows for projecting long-term interactions into a limited planning horizon, thus enabling deep optimistic exploration. We do not assume a priori knowledge of either the dynamics or reward function. We demonstrate that our approach can accommodate both dense and sparse reward signals, while improving sample complexity on a variety of benchmarking tasks.},
 author = {Seyde, Tim and Schwarting, Wilko and Karaman, Sertac and Rus, Daniela},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3092610802},
 pages = {815--825},
 pdf = {http://proceedings.mlr.press/v120/seyde20a/seyde20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning to Plan via Deep Optimistic Value Exploration},
 url = {https://proceedings.mlr.press/v120/seyde20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-shah20a,
 abstract = {We consider the problem of reinforcement learning (RL) with unbounded state space motivated by the classical problem of scheduling in a queueing network. Traditional policies as well as error metric that are designed for finite, bounded or compact state space, require infinite samples for providing any meaningful performance guarantee (e.g. $\ell_\infty$ error) for unbounded state space. That is, we need a new notion of performance metric. As the main contribution of this work, inspired by the literature in queuing systems and control theory, we propose stability as the notion of "goodness": the state dynamics under the policy should remain in a bounded region with high probability. As a proof of concept, we propose an RL policy using Sparse-Sampling-based Monte Carlo Oracle and argue that it satisfies the stability property as long as the system dynamics under the optimal policy respects a Lyapunov function. The assumption of existence of a Lyapunov function is not restrictive as it is equivalent to the positive recurrence or stability property of any Markov chain, i.e., if there is any policy that can stabilize the system then it must possess a Lyapunov function. And, our policy does not utilize the knowledge of the specific Lyapunov function. To make our method sample efficient, we provide an improved, sample efficient Sparse-Sampling-based Monte Carlo Oracle with Lipschitz value function that may be of interest in its own right. Furthermore, we design an adaptive version of the algorithm, based on carefully constructed statistical tests, which finds the correct tuning parameter automatically.},
 author = {Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3033687861},
 pages = {581--581},
 pdf = {http://proceedings.mlr.press/v120/shah20a/shah20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Stable Reinforcement Learning with Unbounded State Space},
 url = {https://proceedings.mlr.press/v120/shah20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-singh20a,
 abstract = {One major obstacle that precludes the success of reinforcement learning in real-world applications is the lack of robustness, either to model uncertainties or external disturbances, of the trained policies. Robustness is critical when the policies are trained in simulations instead of real world environment. In this work, we propose a risk-aware algorithm to learn robust policies in order to bridge the gap between simulation training and real-world implementation. Our algorithm is based on recently discovered distributional RL framework. We incorporate CVaR risk measure in sample based distributional policy gradients (SDPG) for learning risk-averse policies to achieve robustness against a range of system disturbances. We validate the robustness of risk-aware SDPG on multiple environments.},
 author = {Singh, Rahul and Zhang, Qinsheng and Chen, Yongxin},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3021267234},
 pages = {958--968},
 pdf = {http://proceedings.mlr.press/v120/singh20a/singh20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improving Robustness via Risk Averse Distributional Reinforcement Learning},
 url = {https://proceedings.mlr.press/v120/singh20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-sintov20a,
 abstract = {Precise in-hand manipulation is an important skill for a robot to perform tasks in human environments. Practical robotic hands must be low-cost, easy to control and capable. 3D-printed underactuated adaptive hands provide such properties as they are cheap to fabricate and adapt to objects of uncertain geometry with stable grasps. Challenges still remain, however, before such hands can attain human-like performance due to complex dynamics and contacts. In particular, useful models for planning, control or model-based reinforcement learning are still lacking. Recently, data-driven approaches for such models have shown promise. This work provides the first large public dataset of real within-hand manipulation that facilitates building such models, along with baseline data-driven modeling results. Furthermore, it contributes ROS-based physics-engine model of such hands for independent data collection, experimentation and sim-to-reality transfer work. },
 author = {Sintov, Avishai and Kimmel, Andrew and Wen, Bowen and Boularias, Abdeslam and Bekris, Kostas},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108777372},
 pages = {771--780},
 pdf = {http://proceedings.mlr.press/v120/sintov20a/sintov20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Tools for Data-driven Modeling of Within-Hand Manipulation with Underactuated Adaptive Hands.},
 url = {https://proceedings.mlr.press/v120/sintov20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-sivaranjani20a,
 abstract = {In model-based learning, it is desirable for the learned model to preserve structural properties of the system that may facilitate easier control design or provide performance, stability or safety guarantees. Here, we consider an unknown nonlinear system possessing such a structural property - passivity, that can be used to ensure robust stability with a learned controller. We present an algorithm to learn a passive linear model of this nonlinear system from time domain input-output data. We first learn an approximate linear model of this system using any standard system identification technique. We then enforce passivity by perturbing the system matrices of the linear model, while ensuring that the perturbed model closely approximates the input-output behavior of the nonlinear system. Finally, we derive a trade-off between the perturbation size and the radius of the region in which the passivity of the linear model guarantees local passivity of the unknown nonlinear system. },
 author = {Sivaranjani, S. and Agarwal, Etika and Gupta, Vijay},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3110224397},
 pages = {338--339},
 pdf = {http://proceedings.mlr.press/v120/sivaranjani20a/sivaranjani20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Data-driven Identification of Approximate Passive Linear Models for Nonlinear Systems},
 url = {https://proceedings.mlr.press/v120/sivaranjani20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-song20a,
 abstract = {This paper proposes a new method for manipulating unknown objects through a sequence of non-prehensile actions that displace an object from its initial configuration to a given goal configuration on a flat surface. The proposed method leverages recent progress in differentiable physics models to identify unknown mechanical properties of manipulated objects, such as inertia matrix, friction coefficients and external forces acting on the object. To this end, a recently proposed differentiable physics engine for two-dimensional objects is adopted in this work and extended to deal forces in the three-dimensional space. The proposed model identification technique analytically computes the gradient of the distance between forecasted poses of objects and their actual observed poses, and utilizes that gradient to search for values of the mechanical properties that reduce the reality gap.},
 author = {Song, Changkyu and Boularias, Abdeslam},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3110593952},
 pages = {749--760},
 pdf = {http://proceedings.mlr.press/v120/song20a/song20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Identifying Mechanical Models of Unknown Objects with Differentiable Physics Simulations},
 url = {https://proceedings.mlr.press/v120/song20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-summers20a,
 abstract = {We introduce Lyceum, a high-performance computational ecosystem for robot learning. Lyceum is built on top of the Julia programming language and the MuJoCo physics simulator, combining the ease-of-use of a high-level programming language with the performance of native C. In addition, Lyceum has a straightforward API to support parallel computation across multiple cores and machines. Overall, depending on the complexity of the environment, Lyceum is 5-30x faster compared to other popular abstractions like OpenAI's Gym and DeepMind's dm-control. This substantially reduces training time for various reinforcement learning algorithms; and is also fast enough to support real-time model predictive control through MuJoCo. The code, tutorials, and demonstration videos can be found at: this http URL.},
 author = {Summers, Colin and Lowrey, Kendall and Rajeswaran, Aravind and Srinivasa, Siddhartha and Todorov, Emanuel},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107706751},
 pages = {793--803},
 pdf = {http://proceedings.mlr.press/v120/summers20a/summers20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Lyceum: An efficient and scalable ecosystem for robot learning},
 url = {https://proceedings.mlr.press/v120/summers20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-sun20a,
 abstract = {This paper studies the optimality of regularized regression for low order linear system identification. The nuclear norm of the systemâs Hankel matrix is added as a regularizer to the least squares cost function due to the following advantages: (1) its easy to tune regularzation weight, (2) lower sample complexity, (3) returning a Hankel matrix with a clear singular value gap, which robustly recovers a low-order linear system from noisy output observations. Recently, the performance of unregularized least squares formulations have been studied statistically in terms of finite sample complexity and recovery error; however, no results are known for the regularized approach. In this work, we show that with the advantage of sample complexity kept, the regularized algorithm beats unregularized least squares in Hankel spectral norm bound.},
 author = {Sun, Yue and Oymak, Samet and Fazel, Maryam},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3107164321},
 pages = {16--25},
 pdf = {http://proceedings.mlr.press/v120/sun20a/sun20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Finite Sample System Identification: Optimal Rates and the Role of Regularization.},
 url = {https://proceedings.mlr.press/v120/sun20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-sutanto20a,
 abstract = {The recursive Newton-Euler Algorithm (RNEA) is a popular technique for computing the dynamics of robots. RNEA can be framed as a differentiable computational graph, enabling the dynamics parameters of the robot to be learned from data via modern auto-differentiation toolboxes. However, the dynamics parameters learned in this manner can be physically implausible. In this work, we incorporate physical constraints in the learning by adding structure to the learned parameters. This results in a framework that can learn physically plausible dynamics via gradient descent, improving the training speed as well as generalization of the learned dynamics models. We evaluate our method on real-time inverse dynamics control tasks on a 7 degree of freedom robot arm, both in simulation and on the real robot. Our experiments study a spectrum of structure added to the parameters of the differentiable RNEA algorithm, and compare their performance and generalization.},
 author = {Sutanto, Giovanni and Wang, Austin and Lin, Yixin and Mukadam, Mustafa and Sukhatme, Gaurav and Rai, Akshara and Meier, Franziska},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3002324011},
 pages = {804--813},
 pdf = {http://proceedings.mlr.press/v120/sutanto20a/sutanto20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Encoding Physical Constraints in Differentiable Newton-Euler Algorithm},
 url = {https://proceedings.mlr.press/v120/sutanto20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-taylor20a,
 abstract = {Modern nonlinear control theory seeks to endow systems with properties of stability and safety, and have been deployed successfully in multiple domains. Despite this success, model uncertainty remains a significant challenge in synthesizing safe controllers, leading to degradation in the properties provided by the controllers. This paper develops a machine learning framework utilizing Control Barrier Functions (CBFs) to reduce model uncertainty as it impact the safe behavior of a system. This approach iteratively collects data and updates a controller, ultimately achieving safe behavior. We validate this method in simulation and experimentally on a Segway platform.},
 author = {Taylor, Andrew and Singletary, Andrew and Yue, Yisong and Ames, Aaron},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2996196387},
 pages = {708--717},
 pdf = {http://proceedings.mlr.press/v120/taylor20a/taylor20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning for Safety-Critical Control with Control Barrier Functions},
 url = {https://proceedings.mlr.press/v120/taylor20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-tsiamis20a,
 abstract = {In this paper, we consider the task of designing a Kalman Filter (KF) for an unknown and partially observed autonomous linear time invariant system driven by process and sensor noise. To do so, we propose studying the following two step process: first, using system identification tools rooted in subspace methods, we obtain coarse finite-data estimates of the state-space parameters and Kalman gain describing the autonomous system; and second, we use these approximate parameters to design a filter which produces estimates of the system state. We show that when the system identification step produces sufficiently accurate estimates, or when the underlying true KF is sufficiently robust, that a Certainty Equivalent (CE) KF, i.e., one designed using the estimated parameters directly, enjoys provable sub-optimality guarantees. We further show that when these conditions fail, and in particular, when the CE KF is marginally stable (i.e., has eigenvalues very close to the unit circle), that imposing additional robustness constraints on the filter leads to similar sub-optimality guarantees. We further show that with high probability, both the CE and robust filters have mean prediction error bounded by $\tilde O(1/\sqrt{N})$, where $N$ is the number of data points collected in the system identification step. To the best of our knowledge, these are the first end-to-end sample complexity bounds for the Kalman Filtering of an unknown system.},
 author = {Tsiamis, Anastasios and Matni, Nikolai and Pappas, George},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3110352256},
 pages = {435--444},
 pdf = {http://proceedings.mlr.press/v120/tsiamis20a/tsiamis20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sample Complexity of Kalman Filtering for Unknown Systems},
 url = {https://proceedings.mlr.press/v120/tsiamis20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-umenberger20a,
 abstract = {Recent work by Mania et al. has proved that certainty equivalent control achieves nearly optimal regret for linear systems with quadratic costs. However, when parameter uncertainty is large, certainty equivalence cannot be relied upon to stabilize the true, unknown system. In this paper, we present a dual control strategy that attempts to combine the performance of certainty equivalence, with the practical utility of robustness. The formulation preserves structure in the representation of parametric uncertainty, which allows the controller to target reduction of uncertainty in the parameters that `matter most' for the control task, while robustly stabilizing the uncertain system. Control synthesis proceeds via convex optimization, and the method is illustrated on a numerical example.},
 author = {Umenberger, Jack and Sch\"on, Thomas B.},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108579696},
 pages = {550--560},
 pdf = {http://proceedings.mlr.press/v120/umenberger20a/umenberger20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Optimistic robust linear quadratic dual control},
 url = {https://proceedings.mlr.press/v120/umenberger20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-umlauft20a,
 abstract = {The identification of unknown dynamical systems using supervised learning 
enables model-based control of systems that cannot be 
modeled 
based on first principles. While most control literature focuses 
on the analysis of a static dataset, online learning control, where 
data points are added while the controller is running, has rarely been studied 
in depth. In this paper, we present a novel approach for online learning 
control 
based on Gaussian process models. To 
avoid computational difficulties with growing datasets, we propose a 
safe forgetting mechanism. 
Using an entropy criterion, data points are evaluated with respect to the 
future trajectory of the closed loop system and are ``forgotten'' if the 
stability of the system can further be 
guaranteed. The approach is evaluated in a simulation and in a robotic 
experiment to show its real-time capability.},
 author = {Umlauft, Jonas and Beckers, Thomas and Capone, Alexandre and Lederer, Armin and Hirche, Sandra},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3010324891},
 pages = {160--169},
 pdf = {http://proceedings.mlr.press/v120/umlauft20a/umlauft20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Smart Forgetting for Safe Online Learning with Gaussian Processes},
 url = {https://proceedings.mlr.press/v120/umlauft20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-usmanova20a,
 abstract = {For safety-critical black-box optimization tasks, observations of the constraints and the objective are often noisy and available only for the feasible points. We propose an approach based on log barriers to find a local solution of a non-convex non-smooth black-box optimization problem $\min f^0(x)$ subject to $f^i(x)\leq 0,~ i = 1,\ldots, m$, at the same time, guaranteeing constraint satisfaction while learning an optimal solution with high probability. Our proposed algorithm exploits noisy observations to iteratively improve on an initial safe point until convergence. We derive the convergence rate and prove safety of our algorithm. We demonstrate its performance in an application to an iterative control design problem.},
 author = {Usmanova, Ilnura and Krause, Andreas and Kamgarpour, Maryam},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2995173309},
 pages = {980--989},
 pdf = {http://proceedings.mlr.press/v120/usmanova20a/usmanova20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Safe non-smooth black-box optimization with application to policy search},
 url = {https://proceedings.mlr.press/v120/usmanova20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-venkataraman20a,
 abstract = {Signal temporal logic (STL) is an expressive language to specify time-bound real-world robotic tasks and safety specifications. Recently, there has been an interest in learning optimal policies to satisfy STL specifications via reinforcement learning (RL). Learning to satisfy STL specifications often needs a sufficient length of state history to compute reward and the next action. The need for history results in exponential state-space growth for the learning problem. Thus the learning problem becomes computationally intractable for most real-world applications. In this paper, we propose a compact means to capture state history in a new augmented state-space representation. An approximation to the objective (maximizing probability of satisfaction) is proposed and solved for in the new augmented state-space. We show the performance bound of the approximate solution and compare it with the solution of an existing technique via simulations.},
 author = {Venkataraman, Harish and Aksaray, Derya and Seiler, Peter},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3001024844},
 pages = {308--317},
 pdf = {http://proceedings.mlr.press/v120/venkataraman20a/venkataraman20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Tractable Reinforcement Learning of Signal Temporal Logic Objectives},
 url = {https://proceedings.mlr.press/v120/venkataraman20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-wabersich20a,
 abstract = {Tight performance specifications in combination with operational constraints make model predictive control (MPC) the method of choice in various industries. As the performance of an MPC controller depends on a sufficiently accurate objective and prediction model of the process, a significant effort in the MPC design procedure is dedicated to modeling and identification. Driven by the increasing amount of available system data and advances in the field of machine learning, data-driven MPC techniques have been developed to facilitate the MPC controller design. While these methods are able to leverage available data, they typically do not provide principled mechanisms to automatically trade off exploitation of available data and exploration to improve and update the objective and prediction model. To this end, we present a learning-based MPC formulation using posterior sampling techniques, which provides finite-time regret bounds on the learning performance while being simple to implement using off-the-shelf MPC software and algorithms. The performance analysis of the method is based on posterior sampling theory and its practical efficiency is illustrated using a numerical example of a highly nonlinear dynamical car-trailer system.},
 author = {Wabersich, Kim Peter and Zeilinger, Melanie},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3028014124},
 pages = {455--464},
 pdf = {http://proceedings.mlr.press/v120/wabersich20a/wabersich20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Bayesian model predictive control: Efficient model exploration and regret bounds using posterior sampling},
 url = {https://proceedings.mlr.press/v120/wabersich20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-wang20a,
 abstract = {This paper focuses on inverse reinforcement learning (IRL) for autonomous
robot navigation using semantic observations. The objective is to infer a cost
function that explains demonstrated behavior while relying only on the expert's
observations and state-control trajectory. We develop a map encoder, which
infers semantic class probabilities from the observation sequence, and a cost
encoder, defined as deep neural network over the semantic features. Since the
expert cost is not directly observable, the representation parameters can only
be optimized by differentiating the error between demonstrated controls and a
control policy computed from the cost estimate. The error is optimized using a
closed-form subgradient computed only over a subset of promising states via a
motion planning algorithm. We show that our approach learns to follow traffic
rules in the autonomous driving CARLA simulator by relying on semantic
observations of cars, sidewalks and road lanes.},
 author = {Wang, Tianyu and Dhiman, Vikas and Atanasov, Nikolay},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3106971295},
 pages = {245--255},
 pdf = {http://proceedings.mlr.press/v120/wang20a/wang20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Navigation Costs from Demonstration with Semantic Observations},
 url = {https://proceedings.mlr.press/v120/wang20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-wang20b,
 abstract = {We propose a novel differentiable physics engine for system identification of complex spring-rod assemblies. Unlike black-box data-driven methods for learning the evolution of a dynamical system and its parameters, we modularize the design of our engine using a discrete form of the governing equations of motion, similar to a traditional physics engine. We further reduce the dimension from 3D to 1D for each module, which allows efficient learning of system parameters using linear regression. As a side benefit, the regression parameters correspond to physical quantities, such as spring stiffness or the mass of the rod, making the pipeline explainable. The approach significantly reduces the amount of training data required, and also avoids iterative identification of data sampling and model training. We compare the performance of the proposed engine with previous solutions, and demonstrate its efficacy on tensegrity systems, such as NASA's icosahedron.},
 author = {Wang, Kun and Aanjaneya, Mridul and Bekris, Kostas},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3108888711},
 pages = {651--665},
 pdf = {http://proceedings.mlr.press/v120/wang20b/wang20b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A First Principles Approach for Data-Efficient System Identification of Spring-Rod Systems via Differentiable Physics Engines},
 url = {https://proceedings.mlr.press/v120/wang20b.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-wu20a,
 abstract = {We study a Bayesian learning dynamics induced by agents who repeatedly allocate loads on a set of resources based on their belief of an unknown parameter that affects the cost distributions of resources. In each step, belief update is performed according to Bayesâ rule using the agentsâ current load and a realization of costs on resources that they utilized. Then, agents choose a new load using an adaptive strategy update rule that accounts for their preferred allocation based on the updated belief. We prove that beliefs and loads generated by this learning dynamics converge almost surely. The convergent belief accurately estimates cost distributions of resources that are utilized by the convergent load. We establish conditions on the initial load and strategy updates under which the cost estimation is accurate on all resources. These results apply to Bayesian learning in congestion games with unknown latency functions. Particularly, we provide conditions under which the load converges to an equilibrium or socially optimal load with complete information of cost parameter. We also design an adaptive tolling mechanism that eventually induces the socially optimal outcome. },
 author = {Wu, Manxi and Amin, Saurabh and Ozdaglar, Asuman},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3096110041},
 pages = {561--570},
 pdf = {http://proceedings.mlr.press/v120/wu20a/wu20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Bayesian Learning with Adaptive Load Allocation Strategies.},
 url = {https://proceedings.mlr.press/v120/wu20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-yang20a,
 abstract = {Despite the great empirical success of deep reinforcement learning, its theoretical foundation is less well understood. In this work, we make the first attempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. In specific, we focus on a slight simplification of DQN that fully captures its key features. Under mild assumptions, we establish the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by DQN. In particular, the statistical error characterizes the bias and variance that arise from approximating the action-value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, our analysis provides justifications for the techniques of experience replay and target network, which are crucial to the empirical success of DQN. Furthermore, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum Markov game with two players. Borrowing the analysis of DQN, we also quantify the difference between the policies obtained by Minimax-DQN and the Nash equilibrium of the Markov game in terms of both the algorithmic and statistical rates of convergence.},
 author = {Fan, Jianqing and Wang, Zhaoran and Xie, Yuchen and Yang, Zhuoran},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2907626093},
 pages = {486--489},
 pdf = {http://proceedings.mlr.press/v120/yang20a/yang20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Theoretical Analysis of Deep Q-Learning},
 url = {https://proceedings.mlr.press/v120/yang20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-yang20b,
 abstract = {In this paper we introduce plan2vec, an unsupervised representation learning approach that is inspired by reinforcement learning. Plan2vec constructs a weighted graph on an image dataset using near-neighbor distances, and then extrapolates this local metric to a global embedding by distilling path-integral over planned path. When applied to control, plan2vec offers a way to learn goal-conditioned value estimates that are accurate over long horizons that is both compute and sample efficient. We demonstrate the effectiveness of plan2vec on one simulated and two challenging real-world image datasets. Experimental results show that plan2vec successfully amortizes the planning cost, enabling reactive planning that is linear in memory and computation complexity rather than exhaustive over the entire state space.},
 author = {Yang, Ge and Zhang, Amy and Morcos, Ari and Pineau, Joelle and Abbeel, Pieter and Calandra, Roberto},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W2995110369},
 pages = {935--946},
 pdf = {http://proceedings.mlr.press/v120/yang20b/yang20b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Plan2Vec: Unsupervised Representation Learning by Latent Plans},
 url = {https://proceedings.mlr.press/v120/yang20b.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-zhang20a,
 abstract = {Policy optimization (PO) is a key ingredient for modern reinforcement learning (RL). For control design, certain constraints are usually enforced on the policies to optimize, accounting for either the stability, robustness, or safety concerns on the system. Hence, PO is by nature a constrained (nonconvex) optimization in most cases, whose global convergence is challenging to analyze in general. More importantly, some constraints that are safety-critical, e.g., the closed-loop stability, or the $\mathcal{H}_{\infty}$-norm constraint that guarantees the system robustness, can be difficult to enforce on the controller being learned as the PO methods proceed. In this paper, we study the convergence theory of PO for $\mathcal{H}_{2}$ linear control with $\mathcal{H}_{\infty}$ robustness guarantee. This general framework includes risk-sensitive linear control as a special case. One significant new feature of this problem, in contrast to the standard $\mathcal{H}_{2}$ linear control, namely, linear quadratic regulator (LQR) problems, is the lack of coercivity of the cost function. This makes it challenging to guarantee the feasibility, namely, the $\mathcal{H}_{\infty}$ robustness, of the iterates. Interestingly, we propose two PO algorithms that enjoy the implicit regularization property, i.e., the iterates preserve the $\mathcal{H}_{\infty}$ robustness, as if they are regularized by the algorithms. Furthermore, convergence to the globally optimal policies with globally sublinear and locally (super-)linear rates are provided under certain conditions, despite the nonconvexity of the problem. To the best of our knowledge, our work offers the first results on the implicit regularization property and global convergence of PO methods for robust/risk-sensitive control.},
 author = {Zhang, Kaiqing and Hu, Bin and Basar, Tamer},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 pages = {179--190},
 pdf = {http://proceedings.mlr.press/v120/zhang20a/zhang20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Policy Optimization for $\mathcal{H}_{2}$ Linear Control with $\mathcal{H}_{\infty}$ Robustness Guarantee: Implicit Regularization and Global Convergence},
 url = {https://proceedings.mlr.press/v120/zhang20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-zhang20b,
 abstract = {We study data poisoning attacks in the online learning setting, where training data arrive sequentially, and the attacker is eavesdropping the data stream and has the ability to contaminate the current data point to affect the online learning process. We formulate the optimal online attack problem as a stochastic optimal control problem, and provide a systematic solution using tools from model predictive control and deep reinforcement learning. We further provide theoretical analysis on the regret suffered by the attacker for not knowing the true data sequence. Experiments validate our control approach in generating near-optimal attacks on both supervised and unsupervised learning tasks.},
 author = {Zhang, Xuezhou and Zhu, Xiaojin and Lessard, Laurent},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3095509234},
 pages = {201--210},
 pdf = {http://proceedings.mlr.press/v120/zhang20b/zhang20b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Online Data Poisoning Attacks},
 url = {https://proceedings.mlr.press/v120/zhang20b.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-zheng20a,
 abstract = {Constrained Markov Decision Processes are a class of stochastic decision problems in which the decision maker must select a policy that satisfies auxiliary cost constraints. This paper extends upper confidence reinforcement learning for settings in which the reward function and the constraints, described by cost functions, are unknown a priori but the transition kernel is known. Such a setting is well-motivated by a number of applications including exploration of unknown, potentially unsafe, environments. We present an algorithm C-UCRL and show that it achieves sub-linear regret ($ O(T^{\frac{3}{4}}\sqrt{\log(T/δ)})$) with respect to the reward while satisfying the constraints even while learning with probability $1-δ$. Illustrative examples are provided.},
 author = {Zheng, Liyuan and Ratliff, Lillian},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3001756029},
 pages = {620--629},
 pdf = {http://proceedings.mlr.press/v120/zheng20a/zheng20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Constrained Upper Confidence Reinforcement Learning},
 url = {https://proceedings.mlr.press/v120/zheng20a.html},
 volume = {120},
 year = {2020}
}

@inproceedings{pmlr-v120-zhu20a,
 abstract = {We apply kernel mean embedding methods to sample-based stochastic optimization and control. Specifically, we use the reduced-set expansion method as a way to discard sampled scenarios. The effect of such constraint removal is improved optimality and decreased conservativeness. This is achieved by solving a distributional-distance-regularized optimization problem. We demonstrated this optimization formulation is well-motivated in theory, computationally tractable and effective in numerical algorithms.},
 author = {Zhu, Jia-Jie and Schoelkopf, Bernhard and Diehl, Moritz},
 booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
 editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
 month = {10--11 Jun},
 openalex = {W3004391707},
 pages = {915--923},
 pdf = {http://proceedings.mlr.press/v120/zhu20a/zhu20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Kernel Mean Embedding Approach to Reducing Conservativeness in Stochastic Programming and Control},
 url = {https://proceedings.mlr.press/v120/zhu20a.html},
 volume = {120},
 year = {2020}
}
