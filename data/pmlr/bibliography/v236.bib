@proceedings{CLeaR2024,
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Francesco Locatello and Vanessa Didelez},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 volume = {236}
}

@inproceedings{pmlr-v236-bacco24a,
 abstract = {We present a causality-inspired adjusted plus-minus model for evaluating individual players from their performance on a team. We take an explicitly causal approach to this problem, defining the value of a player to be the expected change in the score had we substituted the player for one who has zero value. (This quantity is âcausalâ in the sense that it is an inference about a hypothetical intervention.) We adapt recent ideas of factor modeling to handle the indirectly measured confounding in estimating player values, considering each player to be a âtreatmentâ who contributes to the outcome of the game. We demonstrate the behavior of the model on data about soccer and basketball.},
 author = {Bacco, Caterina De and Wang, Yixin and Blei, David},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 pages = {769--792},
 pdf = {https://proceedings.mlr.press/v236/bacco24a/bacco24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A causality-inspired  plus-minus model for player evaluation in team sports},
 url = {https://proceedings.mlr.press/v236/bacco24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-bach24a,
 abstract = {Proper hyperparameter tuning is essential for achieving optimal performance of modern machine learning (ML) methods in predictive tasks. While there is an extensive literature on tuning ML learners for prediction, there is only little guidance available on tuning ML learners for causal machine learning and how to select among different ML learners. In this paper, we empirically assess the relationship between the predictive performance of ML methods and the resulting causal estimation based on the Double Machine Learning (DML) approach by Chernozhukov et al. (2018). DML relies on estimating so-called nuisance parameters by treating them as supervised learning problems and using them as plug-in estimates to solve for the (causal) parameter. We conduct an extensive simulation study using data from the 2019 Atlantic Causal Inference Conference Data Challenge. We provide empirical insights on the role of hyperparameter tuning and other practical decisions for causal estimation with DML. First, we assess the importance of data splitting schemes for tuning ML learners within Double Machine Learning. Second, we investigate how the choice of ML methods and hyperparameters, including recent AutoML frameworks, impacts the estimation performance for a causal parameter of interest. Third, we assess to what extent the choice of a particular causal model, as characterized by incorporated parametric assumptions, can be based on predictive performance metrics.},
 author = {Bach, Philipp and Schacht, Oliver and Chernozhukov, Victor and Klaassen, Sven and Spindler, Martin},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4391673065},
 pages = {1065--1117},
 pdf = {https://proceedings.mlr.press/v236/bach24a/bach24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Hyperparameter Tuning for Causal Inference with Double Machine Learning:
  A Simulation Study},
 url = {https://proceedings.mlr.press/v236/bach24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-barin-pacela24a,
 abstract = {Disentanglement aims to recover meaningful latent ground-truth factors from the observed distribution solely, and is formalized through the theory of identifiability. The identifiability of independent latent factors is proven to be impossible in the unsupervised i.i.d. setting under a general nonlinear map from factors to observations. In this work, however, we demonstrate that it is possible to recover quantized latent factors under a generic nonlinear diffeomorphism. We only assume that the latent factors have independent discontinuities in their density, without requiring the factors to be statistically independent. We introduce this novel form of identifiability, termed quantized factor identifiability, and provide a comprehensive proof of the recovery of the quantized factors.},
 author = {Barin-Pacela, Vit\'oria and Ahuja, Kartik and Lacoste-Julien, Simon and Vincent, Pascal},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4382619204},
 pages = {384--422},
 pdf = {https://proceedings.mlr.press/v236/barin-pacela24a/barin-pacela24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On the Identifiability of Quantized Factors},
 url = {https://proceedings.mlr.press/v236/barin-pacela24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-bing24a,
 abstract = {The task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also includes the shared assumption of single-node interventions of previous works. The main idea behind our approach is to exploit the trace that interventions leave on the variance of the ground truth causal variables and regularizing for a specific notion of sparsity with respect to this trace. In addition to and inspired by our theoretical contributions, we present a practical algorithm to learn causal representations from multi-node interventional data and provide empirical evidence that validates our identifiability results.},
 author = {Bing, Simon and Ninad, Urmi and Wahl, Jonas and Runge, Jakob},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4388482522},
 pages = {843--867},
 pdf = {https://proceedings.mlr.press/v236/bing24a/bing24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions},
 url = {https://proceedings.mlr.press/v236/bing24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-binkyte24a,
 abstract = {Differential privacy is a widely adopted framework designed to safeguard the sensitive information of data providers within a data set. It is based on the application of controlled noise at the interface between the server that stores and processes the data, and the data consumers. Local differential privacy is a variant that allows data providers to apply the privatization mechanism themselves on their data individually. Therefore it provides protection also in contexts in which the server, or even the data collector, cannot be trusted. The introduction of noise, however, inevitably affects the utility of the data, particularly by distorting the correlations between individual data components. This distortion can prove detrimental to tasks such as causal discovery. In this paper, we consider various well-known locally differentially private mechanisms and compare the trade-off between the privacy they provide, and the accuracy of the causal structure produced by algorithms for causal learning when applied to data obfuscated by these mechanisms. Our analysis yields valuable insights for selecting appropriate local differentially private protocols for causal discovery tasks. We foresee that our findings will aid researchers and practitioners in conducting locally private causal discovery.},
 author = {Binkyte, Ruta and Pinz\'on, Carlos Antonio and Lesty\'an, Szilvia and Jung, Kangsoo and Arcolezi, H\'eber Hwang and Palamidessi, Catuscia},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4388514832},
 pages = {325--383},
 pdf = {https://proceedings.mlr.press/v236/binkyte24a/binkyte24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Discovery Under Local Privacy},
 url = {https://proceedings.mlr.press/v236/binkyte24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-boeken24a,
 abstract = {When predicting a target variable $Y$ from features $X$, the prediction $\hat{Y}$ can be performative: an agent might act on this prediction, affecting the value of $Y$ that we eventually observe. Performative predictions are deliberately prevalent in algorithmic decision support, where a Decision Support System (DSS) provides a prediction for an agent to affect the value of the target variable. When deploying a DSS in high-stakes settings (e.g. healthcare, law, predictive policing, or child welfare screening) it is imperative to carefully assess the performative effects of the DSS. In the case that the DSS serves as an alarm for a predicted negative outcome, naive retraining of the prediction model is bound to result in a model that underestimates the risk, due to effective workings of the previous model. In this work, we propose to model the deployment of a DSS as causal domain shift and provide novel cross-domain identification results for the conditional expectation $E[Y | X]$, allowing for pre- and post-hoc assessment of the deployment of the DSS, and for retraining of a model that assesses the risk under a baseline policy where the DSS is not deployed. Using a running example, we empirically show that a repeated regression procedure provides a practical framework for estimating these quantities, even when the data is affected by sample selection bias and selective labelling, offering for a practical, unified solution for multiple forms of target variable bias.},
 author = {Boeken, Philip and Zoeter, Onno and Mooij, Joris},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4392490337},
 pages = {551--569},
 pdf = {https://proceedings.mlr.press/v236/boeken24a/boeken24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Evaluating and Correcting Performative Effects of Decision Support
  Systems via Causal Domain Shift},
 url = {https://proceedings.mlr.press/v236/boeken24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-cook24a,
 abstract = {We consider the problem of efficient inference of the Average Treatment Effect in a sequential experiment where the policy governing the assignment of subjects to treatment or control can change over time. We first provide a central limit theorem for the Adaptive Augmented Inverse-Probability Weighted estimator, which is semiparametric efficient, under weaker assumptions than those previously made in the literature. This central limit theorem enables efficient inference at fixed sample sizes. We then consider a sequential inference setting, deriving both asymptotic and nonasymptotic confidence sequences that are considerably tighter than previous methods. These anytime-valid methods enable inference under data-dependent stopping times (sample sizes). Additionally, we use propensity score truncation techniques from the recent off-policy estimation literature to reduce the finite sample variance of our estimator without affecting the asymptotic variance. Empirical results demonstrate that our methods yield narrower confidence sequences than those previously developed in the literature while maintaining time-uniform error control.},
 author = {Cook, Thomas and Mishler, Alan and Ramdas, Aaditya},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4389260955},
 pages = {1033--1064},
 pdf = {https://proceedings.mlr.press/v236/cook24a/cook24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Semiparametric Efficient Inference in Adaptive Experiments},
 url = {https://proceedings.mlr.press/v236/cook24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-corazza24a,
 abstract = {Reinforcement learning (RL) algorithms struggle with learning optimal policies for tasks where reward feedback is sparse and depends on a complex sequence of events in the environment. Probabilistic reward machines (PRMs) are finite-state formalisms that can capture temporal dependencies in the reward signal, along with nondeterministic task outcomes. While special RL algorithms can exploit this finite-state structure to expedite learning, PRMs remain difficult to modify and design by hand. This hinders the already difficult tasks of utilizing high-level causal knowledge about the environment, and transferring the reward formalism into a new domain with a different causal structure. This paper proposes a novel method to incorporate causal information in the form of Temporal Logic-based Causal Diagrams into the reward formalism, thereby expediting policy learning and aiding the transfer of task specifications to new environments. Furthermore, we provide a theoretical result about convergence to optimal policy for our method, and demonstrate its strengths empirically.},
 author = {Corazza, Jan and Aria, Hadi Partovi and Neider, Daniel and Xu, Zhe},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 pages = {643--664},
 pdf = {https://proceedings.mlr.press/v236/corazza24a/corazza24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment},
 url = {https://proceedings.mlr.press/v236/corazza24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-curth24a,
 abstract = {Synthetic control (SC) methods have gained rapid popularity in economics recently, where they have been applied in the context of inferring the effects of treatments on standard continuous outcomes assuming linear input-output relations. In medical applications, conversely, survival outcomes are often of primary interest, a setup in which both commonly assumed data-generating processes (DGPs) and target parameters are different. In this paper, we therefore investigate whether and when SCs could serve as an alternative to matching methods in survival analyses. We find that, because SCs rely on a linearity assumption, they will generally be biased for the true expected survival time in commonly assumed survival DGPs -- even when taking into account the possibility of linearity on another scale as in accelerated failure time models. Additionally, we find that, because SC units follow distributions with lower variance than real control units, summaries of their distributions, such as survival curves, will be biased for the parameters of interest in many survival analyses. Nonetheless, we also highlight that using SCs can still improve upon matching whenever the biases described above are outweighed by extrapolation biases exhibited by imperfect matches, and investigate the use of regularization to trade off the shortcomings of both approaches.},
 author = {Curth, Alicia and Poon, Hoifung and Nori, Aditya V. and Gonz\'alez, Javier},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4389326035},
 pages = {143--159},
 pdf = {https://proceedings.mlr.press/v236/curth24a/curth24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Cautionary Tales on Synthetic Controls in Survival Analyses},
 url = {https://proceedings.mlr.press/v236/curth24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-d-textsc-char13acunto24a,
 abstract = {The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics, shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it. Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fit and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nodes related to sensory processing play a crucial role. Finally, our analysis of individual multiscale causal structures confirms the existence of a causal fingerprint of brain connectivity, thus supporting the existing extensive research in brain connectivity fingerprinting from a causal perspective.},
 author = {D\textsc{\char13}Acunto, Gabriele and Bonchi, Francesco and Morales, Gianmarco De Francisci and Petri, Giovanni},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4388274880},
 pages = {265--295},
 pdf = {https://proceedings.mlr.press/v236/d-textsc-char13acunto24a/d-textsc-char13acunto24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Extracting the Multiscale Causal Backbone of Brain Dynamics},
 url = {https://proceedings.mlr.press/v236/d-textsc-char13acunto24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-dai24a,
 abstract = {Large-scale genome-wide association studies (GWAS) have offered an exciting opportunity to discover putative causal genes or risk factors associated with diseases by using SNPs as instrumental variables (IVs). However, conventional approaches assume linear causal relations partly for simplicity and partly for the availability of GWAS summary data. In this work, we propose a novel model for transcriptome-wide association studies (TWAS) to incorporate nonlinear relationships across IVs, an exposure/gene, and an outcome, which is robust against violations of the valid IV assumptions, permits the use of GWAS summary data, and covers two-stage least squares (2SLS) as a special case.  We decouple the estimation of a marginal causal effect and a nonlinear transformation, where the former is estimated via sliced inverse regression and a sparse instrumental variable regression, and the latter is estimated by a ratio-adjusted inverse regression. On this ground, we propose an inferential procedure. An application of the proposed method to the ADNI gene expression data and the IGAP GWAS summary data identifies 18 causal genes associated with Alzheimerâs disease, including APOE and TOMM40, in addition to 7 other genes missed by 2SLS considering only linear relationships. Our findings suggest that nonlinear modeling is required to unleash the power of IV regression for identifying potentially nonlinear gene-trait associations. The source code and accompanying software *nl-causal* can be accessed through the link: [https://github.com/statmlben/nonlinear-causal](https://github.com/statmlben/nonlinear-causal).},
 author = {Dai, Ben and Li, Chunlin and Xue, Haoran and Pan, Wei and Shen, Xiaotong},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 pages = {793--826},
 pdf = {https://proceedings.mlr.press/v236/dai24a/dai24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Inference of nonlinear causal effects with application to TWAS with GWAS summary data},
 url = {https://proceedings.mlr.press/v236/dai24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-dalvi24a,
 abstract = {Matching is one of the simplest approaches for estimating causal effects from observational data. Matching techniques compare the observed outcomes across pairs of individuals with similar covariate values but different treatment statuses in order to estimate causal effects. However, traditional matching techniques are unreliable given high-dimensional covariates due to the infamous curse of dimensionality. To overcome this challenge, we propose a simple, fast, yet highly effective approach to matching using Random Hyperplane Tessellations (RHPT). First, we prove that the RHPT representation is an approximate balancing score -- thus maintaining the strong ignorability assumption -- and provide empirical evidence for this claim. Second, we report results of extensive experiments showing that matching using RHPT outperforms traditional matching techniques and is competitive with state-of-the-art deep learning methods for causal effect estimation. In addition, RHPT avoids the need for computationally expensive training of deep neural networks.},
 author = {Dalvi, Abhishek and Ashtekar, Neil and Honavar, Vasant G},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4394947054},
 pages = {688--702},
 pdf = {https://proceedings.mlr.press/v236/dalvi24a/dalvi24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Effect Estimation Using Random Hyperplane Tessellations},
 url = {https://proceedings.mlr.press/v236/dalvi24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-debeire24a,
 abstract = {Learning causal graphs from multivariate time series is a ubiquitous challenge in all application domains dealing with time-dependent systems, such as in Earth sciences, biology, or engineering, to name a few. Recent developments for this causal discovery learning task have shown considerable skill, notably the specific time-series adaptations of the popular conditional independence-based learning framework. However, uncertainty estimation is challenging for conditional independence-based methods. Here, we introduce a novel bootstrap approach designed for time series causal discovery that preserves the temporal dependencies and lag structure. It can be combined with a range of time series causal discovery methods and provides a measure of confidence for the links of the time series graphs. Furthermore, next to confidence estimation, an aggregation, also called bagging, of the bootstrapped graphs by majority voting results in bagged causal discovery methods. In this work, we combine this approach with the state-of-the-art conditional-independence-based algorithm PCMCI+. With extensive numerical experiments we empirically demonstrate that, in addition to providing confidence measures for links, Bagged-PCMCI+ improves in precision and recall as compared to its base algorithm PCMCI+, at the cost of higher computational demands. These statistical performance improvements are especially pronounced in the more challenging settings (short time sample size, large number of variables, high autocorrelation). Our bootstrap approach can also be combined with other time series causal discovery algorithms and can be of considerable use in many real-world applications.},
 author = {Debeire, Kevin and Gerhardus, Andreas and Runge, Jakob and Eyring, Veronika},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4380994081},
 pages = {979--1007},
 pdf = {https://proceedings.mlr.press/v236/debeire24a/debeire24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Bootstrap aggregation and confidence measures to improve time series causal discovery},
 url = {https://proceedings.mlr.press/v236/debeire24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-dettling24a,
 abstract = {Graphical continuous Lyapunov models offer a new perspective on modeling causally interpretable dependence structure in multivariate data by treating each independent observation as a one-time cross-sectional snapshot of a temporal process. Specifically, the models assume that the observations are cross-sections of independent multivariate Ornstein-Uhlenbeck processes in equilibrium. The Gaussian equilibrium exists under a stability assumption on the drift matrix, and the equilibrium covariance matrix is determined by the continuous Lyapunov equation. Each graphical continuous Lyapunov model assumes the drift matrix to be sparse, with a support determined by a directed graph. A natural approach to model selection in this setting is to use an $\ell_1$-regularization technique that, based on a given sample covariance matrix, seeks to find a sparse approximate solution to the Lyapunov equation. We study the model selection properties of the resulting lasso technique to arrive at a consistency result. Our detailed analysis reveals that the involved irrepresentability condition is surprisingly difficult to satisfy. While this may prevent asymptotic consistency in model selection, our numerical experiments indicate that even if the theoretical requirements for consistency are not met, the lasso approach is able to recover relevant structure of the drift matrix and is robust to aspects of model misspecification.},
 author = {Dettling, Philipp and Drton, Mathias and Kolar, Mladen},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4293820486},
 pages = {514--550},
 pdf = {https://proceedings.mlr.press/v236/dettling24a/dettling24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On the Lasso for Graphical Continuous Lyapunov Models},
 url = {https://proceedings.mlr.press/v236/dettling24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-du24a,
 abstract = {We study the data-generating mechanism for reconstructive SSL to shed light on its effectiveness. With an infinite amount of labeled samples, we provide a sufficient and necessary condition for perfect linear approximation. The condition reveals a full-rank component that preserves the label classes of Y, along with a redundant component. Motivated by the condition, we propose to approximate the redundant component by a low-rank factorization and measure the approximation quality by introducing a new quantity $\epsilon_s$, parameterized by the rank of factorization s. We incorporate $\epsilon_s$ into the excess risk analysis under both linear regression and ridge regression settings, where the latter regularization approach is to handle scenarios when the dimension of the learned features is much larger than the number of labeled samples n for downstream tasks. We design three stylized experiments to compare SSL with supervised learning under different settings to support our theoretical findings.},
 author = {Du, Kang and Xiang, Yu},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4391800553},
 pages = {1008--1032},
 pdf = {https://proceedings.mlr.press/v236/du24a/du24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Low-Rank Approximation of Structural Redundancy for Self-Supervised
  Learning},
 url = {https://proceedings.mlr.press/v236/du24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-elazar24a,
 abstract = {What is the effect of releasing a preprint of a paper before it is submitted for peer review? No randomized controlled trial has been conducted, so we turn to observational data to answer this question. We use data from the ICLR conference (2018--2022) and apply methods from causal inference to estimate the effect of arXiving a paper before the reviewing period (early arXiving) on its acceptance to the conference. Adjusting for confounders such as topic, authors, and quality, we may estimate the causal effect. However, since quality is a challenging construct to estimate, we use the negative outcome control method, using paper citation count as a control variable to debias the quality confounding effect. Our results suggest that early arXiving may have a small effect on a paper's chances of acceptance. However, this effect (when existing) does not differ significantly across different groups of authors, as grouped by author citation count and institute rank. This suggests that early arXiving does not provide an advantage to any particular group.},
 author = {Elazar, Yanai and Zhang, Jiayao and Wadden, David and Zhang, Bo and Smith, Noah A.},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4382319685},
 pages = {913--933},
 pdf = {https://proceedings.mlr.press/v236/elazar24a/elazar24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Estimating the Causal Effect of Early ArXiving on Paper Acceptance},
 url = {https://proceedings.mlr.press/v236/elazar24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-feigenbaum24a,
 abstract = {Causal discovery aims to recover information about an unobserved causal graph from the observable data it generates. Layerings are orderings of the variables which place causes before effects. In this paper, we provide ways to recover layerings of a graph by accessing the data via a conditional entropy oracle, when distributions are discrete. Our algorithms work by repeatedly removing sources or sinks from the graph. Under appropriate assumptions and conditioning, we can separate the sources or sinks from the remainder of the nodes by comparing their conditional entropy to the unconditional entropy of their noise. Our algorithms are provably correct and run in worst-case quadratic time. The main assumptions are faithfulness and injective noise, and either known noise entropies or weakly monotonically increasing noise entropies along directed paths. In addition, we require one of either a very mild extension of faithfulness, or strictly monotonically increasing noise entropies, or expanding noise injectivity to include an additional single argument in the structural functions.},
 author = {Feigenbaum, Itai and Arpit, Devansh and Heinecke, Shelby and Niebles, Juan Carlos and Yao, Weiran and Xiong, Caiming and Savarese, Silvio and Wang, Huan},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4391124047},
 pages = {1176--1191},
 pdf = {https://proceedings.mlr.press/v236/feigenbaum24a/feigenbaum24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Layering via Conditional Entropy},
 url = {https://proceedings.mlr.press/v236/feigenbaum24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-felekis24a,
 abstract = {Causal abstraction (CA) theory establishes formal criteria for relating multiple structural causal models (SCMs) at different levels of granularity by defining maps between them. These maps have significant relevance for real-world challenges such as synthesizing causal evidence from multiple experimental environments, learning causally consistent representations at different resolutions, and linking interventions across multiple SCMs. In this work, we propose COTA, the first method to learn abstraction maps from observational and interventional data without assuming complete knowledge of the underlying SCMs. In particular, we introduce a multi-marginal Optimal Transport (OT) formulation that enforces do-calculus causal constraints, together with a cost function that relies on interventional information. We extensively evaluate COTA on synthetic and real world problems, and showcase its advantages over non-causal, independent and aggregated COTA formulations. Finally, we demonstrate the efficiency of our method as a data augmentation tool by comparing it against the state-of-the-art CA learning framework, which assumes fully specified SCMs, on a real-world downstream task.},
 author = {Felekis, Yorgos and Zennaro, Fabio Massimo and Branchini, Nicola and Damoulas, Theodoros},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4389768606},
 pages = {462--498},
 pdf = {https://proceedings.mlr.press/v236/felekis24a/felekis24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Optimal Transport of Abstractions},
 url = {https://proceedings.mlr.press/v236/felekis24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-feng24a,
 abstract = {After a machine learning (ML)-based system is deployed, monitoring its performance is important to ensure the safety and effectiveness of the algorithm over time. When an ML algorithm interacts with its environment, the algorithm can affect the data-generating mechanism and be a major source of bias when evaluating its standalone performance, an issue known as performativity. Although prior work has shown how to <em>validate</em> models in the presence of performativity using causal inference techniques, there has been little work on how to <em>monitor</em> models in the presence of performativity. Unlike the setting of model validation, there is much less agreement on which performance metrics to monitor. Different monitoring criteria impact how interpretable the resulting test statistic is, what assumptions are needed for identifiability, and the speed of detection. When this choice is further coupled with the decision to use observational versus interventional data, ML deployment teams are faced with a multitude of monitoring options. The aim of this work is to highlight the relatively under-appreciated complexity of designing a monitoring strategy and how causal reasoning can provide a systematic framework for choosing between these options. As a motivating example, we consider an ML-based risk prediction algorithm for predicting unplanned readmissions. Bringing together tools from causal inference and statistical process control, we consider six monitoring procedures (three candidate monitoring criteria and two data sources) and investigate their operating characteristics in simulation studies. Results from this case study emphasize the seemingly simple (and obvious) fact that <em>not all monitoring systems are created equal</em>, which has real-world impacts on the design and documentation of ML monitoring systems.},
 author = {Feng, Jean and Subbaswamy, Adarsh and Gossmann, Alexej and Singh, Harvineet and Sahiner, Berkman and Kim, Mi-Ok and Pennello, Gene Anthony and Petrick, Nicholas and Pirracchio, Romain and Xia, Fan},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 pages = {587--608},
 pdf = {https://proceedings.mlr.press/v236/feng24a/feng24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Designing monitoring strategies for deployed machine learning algorithms: navigating performativity through a causal lens},
 url = {https://proceedings.mlr.press/v236/feng24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-geiger24a,
 abstract = {Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS removes previous obstacles to conducting causal abstraction analyses and allows us to find conceptual structure in trained neural nets.},
 author = {Geiger, Atticus and Wu, Zhengxuan and Potts, Christopher and Icard, Thomas and Goodman, Noah},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4323557327},
 pages = {160--187},
 pdf = {https://proceedings.mlr.press/v236/geiger24a/geiger24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations},
 url = {https://proceedings.mlr.press/v236/geiger24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-gobler24a,
 abstract = {Algorithms for causal discovery have recently undergone rapid advances and increasingly draw on flexible nonparametric methods to process complex data. With these advances comes a need for adequate empirical validation of the causal relationships learned by different algorithms. However, for most real and complex data sources true causal relations remain unknown. This issue is further compounded by privacy concerns surrounding the release of suitable high-quality data. To tackle these challenges, we introduce $\texttt{causalAssembly}$, a semisynthetic data generator designed to facilitate the benchmarking of causal discovery methods. The tool is built using a complex real-world dataset comprised of measurements collected along an assembly line in a manufacturing setting. For these measurements, we establish a partial set of ground truth causal relationships through a detailed study of the physics underlying the processes carried out in the assembly line. The partial ground truth is sufficiently informative to allow for estimation of a full causal graph by mere nonparametric regression. To overcome potential confounding and privacy concerns, we use distributional random forests to estimate and represent conditional distributions implied by the ground truth causal graph. These conditionals are combined into a joint distribution that strictly adheres to a causal model over the observed variables. Sampling from this distribution, $\texttt{causalAssembly}$ generates data that are guaranteed to be Markovian with respect to the ground truth. Using our tool, we showcase how to benchmark several well-known causal discovery algorithms.},
 author = {G\"obler, Konstantin and Windisch, Tobias and Drton, Mathias and Pychynski, Tim and Roth, Martin and Sonntag, Steffen},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 pages = {609--642},
 pdf = {https://proceedings.mlr.press/v236/gobler24a/gobler24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {$     exttt{causalAssembly}$: Generating Realistic Production Data for Benchmarking Causal Discovery},
 url = {https://proceedings.mlr.press/v236/gobler24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-gultchin24a,
 abstract = {We introduce a causal framework for designing optimal policies that satisfy fairness constraints. We take a pragmatic approach asking what we can do with an action space available to us and only with access to historical data. We propose two different fairness constraints: a moderation breaking constraint which aims at blocking moderation paths from the action and sensitive attribute to the outcome, and by that at reducing disparity in outcome levels as much as the provided action space permits; and an equal benefit constraint which aims at distributing gain from the new and maximized policy equally across sensitive attribute levels, and thus at keeping pre-existing preferential treatment in place or avoiding the introduction of new disparity. We introduce practical methods for implementing the constraints and illustrate their uses on experiments with semi-synthetic models.},
 author = {Gultchin, Limor and Guo, Siyuan and Malek, Alan and Chiappa, Silvia and Silva, Ricardo},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4318751694},
 pages = {243--264},
 pdf = {https://proceedings.mlr.press/v236/gultchin24a/gultchin24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Pragmatic Fairness: Developing Policies with Outcome Disparity Control},
 url = {https://proceedings.mlr.press/v236/gultchin24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-hardt24a,
 abstract = {Identifying root causes for unexpected or undesirable behavior in complex systems is a prevalent challenge. This issue becomes especially crucial in modern cloud applications that employ numerous microservices. Although the machine learning and systems research communities have proposed various techniques to tackle this problem, there is currently a lack of standardized datasets for quantitative benchmarking. Consequently, research groups are compelled to create their own datasets for experimentation. This paper introduces a dataset specifically designed for evaluating root cause analyses in microservice-based applications. The dataset encompasses latency, requests, and availability metrics emitted in 5-minute intervals from a distributed application.  In addition to normal operation metrics, the dataset includes 68 injected performance issues, which increase latency and reduce availability throughout the system. We showcase how this dataset can be used to evaluate the accuracy of a variety of methods spanning different causal and non-causal characterisations of the root cause analysis problem.  We hope the new dataset, available at https://github.com/amazon-science/petshop-root-cause-analysis, enables further development of techniques in this important area.},
 author = {Hardt, Michaela and Orchard, William Roy and Bl\"obaum, Patrick and Kirschbaum, Elke and Kasiviswanathan, Shiva},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 pages = {957--978},
 pdf = {https://proceedings.mlr.press/v236/hardt24a/hardt24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The PetShop Dataset â Finding Causes of Performance Issues across Microservices},
 url = {https://proceedings.mlr.press/v236/hardt24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-hatt24a,
 abstract = {Using observational data to estimate the effect of a treatment is a powerful tool for decision-making when randomized experiments are infeasible or costly. However, observational data often yields biased estimates of treatment effects, since treatment assignment can be confounded by unobserved variables. A remedy is offered by deconfounding methods that adjust for such unobserved confounders. In this paper, we develop the Sequential Deconfounder, a method that enables estimating individualized treatment effects over time in presence of unobserved confounders. This is the first deconfounding method that can be used in a general sequential setting (i.e., with one or more treatments assigned at each timestep). The Sequential Deconfounder uses a novel Gaussian process latent variable model to infer substitutes for the unobserved confounders, which are then used in conjunction with an outcome model to estimate treatment effects over time. We prove that using our method yields unbiased estimates of individualized treatment responses over time. Using simulated and real medical data, we demonstrate the efficacy of our method in deconfounding the estimation of treatment responses over time.},
 author = {Hatt, Tobias and Feuerriegel, Stefan},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W3155884514},
 pages = {934--956},
 pdf = {https://proceedings.mlr.press/v236/hatt24a/hatt24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sequential Deconfounding for Causal Inference with Unobserved Confounders.},
 url = {https://proceedings.mlr.press/v236/hatt24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-jamshidi24a,
 abstract = {We study the problem of learning 'good' interventions in a stochastic environment modeled by its underlying causal graph. Good interventions refer to interventions that maximize rewards. Specifically, we consider the setting of a pre-specified budget constraint, where interventions can have non-uniform costs. We show that this problem can be formulated as maximizing the expected reward for a stochastic multi-armed bandit with side information. We propose an algorithm to minimize the cumulative regret in general causal graphs. This algorithm trades off observations and interventions based on their costs to achieve the optimal reward. This algorithm generalizes the state-of-the-art methods by allowing non-uniform costs and hidden confounders in the causal graph. Furthermore, we develop an algorithm to minimize the simple regret in the budgeted setting with non-uniform costs and also general causal graphs. We provide theoretical guarantees, including both upper and lower bounds, as well as empirical evaluations of our algorithms. Our empirical results showcase that our algorithms outperform the state of the art.},
 author = {Jamshidi, Fateme and Etesami, Jalal and Kiyavash, Negar},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4391009432},
 pages = {423--461},
 pdf = {https://proceedings.mlr.press/v236/jamshidi24a/jamshidi24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Confounded Budgeted Causal Bandits},
 url = {https://proceedings.mlr.press/v236/jamshidi24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-liu24a,
 abstract = {Offline reinforcement learning (RL) aims to learn policy from the passively collected offline dataset. Applying existing RL methods on the static dataset straightforwardly will raise distribution shift, causing these unconstrained RL methods to fail. To cope with the distribution shift problem, a common practice in offline RL is to constrain the policy explicitly or implicitly close to behavioral policy. However, the available dataset usually contains sub-optimal or inferior actions, constraining the policy near all these actions will make the policy inevitably learn inferior behaviors, limiting the performance of the algorithm. Based on this observation, we propose a weighted policy constraints (wPC) method that only constrains the learned policy to desirable behaviors, making room for policy improvement on other parts. Our algorithm outperforms existing state-of-the-art offline RL algorithms on the D4RL offline gym datasets. Moreover, the proposed algorithm is simple to implement with few hyper-parameters, making the proposed wPC algorithm a robust offline RL method with low computational complexity.},
 author = {Liu, Yang and Hofert, Marius},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4382318909},
 pages = {499--513},
 pdf = {https://proceedings.mlr.press/v236/liu24a/liu24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Weighted Policy Constraints for Offline Reinforcement Learning},
 url = {https://proceedings.mlr.press/v236/liu24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-liu24b,
 abstract = {Estimating the structure of directed acyclic graphs (DAGs) from observational data is challenging due to the super-exponential growth of the search space with the number of nodes. Previous research primarily focuses on identifying a unique DAG under specific model constraints in linear or nonlinear scenarios. However, real-world scenarios often involve causal mechanisms with a mixture of linear and nonlinear characteristics, which has received limited attention in existing literature. Due to unidentifiability, existing algorithms relying on fully identifiable conditions may produce erroneous results. Although traditional methods like the PC algorithm can be employed to uncover such graphs, they typically yield only a Markov equivalence class. This paper introduces a novel causal discovery approach that extends beyond the Markov equivalence class, aiming to uncover as many edge directions as possible when the causal graph is not fully identifiable. Our approach exploits the second derivative of the log-likelihood in observational data, harnessing scalable machine learning approaches to approximate the score function. Overall, our approach demonstrates competitive accuracy comparable to current state-of-the-art techniques while offering a significant improvement in computational speed.},
 author = {Liu, Wenqin and Huang, Biwei and Gao, Erdun and Ke, Qiuhong and Bondell, Howard and Gong, Mingming},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 pages = {1237--1263},
 pdf = {https://proceedings.mlr.press/v236/liu24b/liu24b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Discovery with Mixed Linear and Nonlinear Additive Noise Models: A Scalable Approach},
 url = {https://proceedings.mlr.press/v236/liu24b.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-lopez24a,
 abstract = {Deep Generative Models (DGMs) are versatile tools for learning data representations while adequately incorporating domain knowledge such as the specification of conditional probability distributions. Recently proposed DGMs tackle the important task of comparing data sets from different sources. One such example is the setting of contrastive analysis that focuses on describing patterns that are enriched in a target data set compared to a background data set. The practical deployment of those models often assumes that DGMs naturally infer interpretable and modular latent representations, which is known to be an issue in practice. Consequently, existing methods often rely on ad-hoc regularization schemes, although without any theoretical grounding. Here, we propose a theory of identifiability for comparative DGMs by extending recent advances in the field of non-linear independent component analysis. We show that, while these models lack identifiability across a general class of mixing functions, they surprisingly become identifiable when the mixing function is piece-wise affine (e.g., parameterized by a ReLU neural network). We also investigate the impact of model misspecification, and empirically show that previously proposed regularization techniques for fitting comparative DGMs help with identifiability when the number of latent variables is not known in advance. Finally, we introduce a novel methodology for fitting comparative DGMs that improves the treatment of multiple data sources via multi-objective optimization and that helps adjust the hyperparameter for the regularization in an interpretable manner, using constrained optimization. We empirically validate our theory and new methodology using simulated data as well as a recent data set of genetic perturbations in cells profiled via single-cell RNA sequencing.},
 author = {Lopez, Romain and Huetter, Jan-Christian and Hajiramezanali, Ehsan and Pritchard, Jonathan K and Regev, Aviv},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4391376589},
 pages = {868--912},
 pdf = {https://proceedings.mlr.press/v236/lopez24a/lopez24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Toward the Identifiability of Comparative Deep Generative Models.},
 url = {https://proceedings.mlr.press/v236/lopez24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-lu24a,
 abstract = {Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an extension of RD that goes beyond sub-rewards to provide more informative explanations. Our approach is centred on a causal learning framework that leverages information-theoretic measures for explanation objectives that encourage three crucial properties of causal factors: causal sufficiency, sparseness, and orthogonality. These properties help us distill the cause-and-effect relationships between the agent's states and actions or rewards, allowing for a deeper understanding of its decision-making processes. Our framework is designed to generate local explanations and can be applied to a wide range of RL tasks with multiple reward channels. Through a series of experiments, we demonstrate that our approach offers more meaningful and insightful explanations for the agent's action selections.},
 author = {Lu, Wenhao and Zhao, Xufeng and Fryen, Thilo and Lee, Jae Hee and Li, Mengdi and Magg, Sven and Wermter, Stefan},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4390529028},
 pages = {106--142},
 pdf = {https://proceedings.mlr.press/v236/lu24a/lu24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal State Distillation for Explainable Reinforcement Learning},
 url = {https://proceedings.mlr.press/v236/lu24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-luttermann24a,
 abstract = {Lifted inference exploits symmetries in probabilistic graphical models by using a representative for indistinguishable objects, thereby speeding up query answering while maintaining exact answers. Even though lifting is a well-established technique for the task of probabilistic inference in relational domains, it has not yet been applied to the task of causal inference. In this paper, we show how lifting can be applied to efficiently compute causal effects in relational domains. More specifically, we introduce parametric causal factor graphs as an extension of parametric factor graphs incorporating causal knowledge and give a formal semantics of interventions therein. We further present the lifted causal inference algorithm to compute causal effects on a lifted level, thereby drastically speeding up causal inference compared to propositional inference, e.g., in causal Bayesian networks. In our empirical evaluation, we demonstrate the effectiveness of our approach.},
 author = {Luttermann, Malte and Hartwig, Mattis and Braun, Tanya and M\"oller, Ralf and Gehrke, Marcel},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4392930084},
 pages = {827--842},
 pdf = {https://proceedings.mlr.press/v236/luttermann24a/luttermann24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Lifted Causal Inference in Relational Domains},
 url = {https://proceedings.mlr.press/v236/luttermann24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-machlanski24a,
 abstract = {Hyperparameters play a critical role in machine learning. Hyperparameter tuning can make the difference between state-of-the-art and poor prediction performance for any algorithm, but it is particularly challenging for structure learning due to its unsupervised nature. As a result, hyperparameter tuning is often neglected in favour of using the default values provided by a particular implementation of an algorithm. While there have been numerous studies on performance evaluation of causal discovery algorithms, how hyperparameters affect individual algorithms, as well as the choice of the best algorithm for a specific problem, has not been studied in depth before. This work addresses this gap by investigating the influence of hyperparameters on causal structure learning tasks. Specifically, we perform an empirical evaluation of hyperparameter selection for some seminal learning algorithms on datasets of varying levels of complexity. We find that, while the choice of algorithm remains crucial to obtaining state-of-the-art performance, hyperparameter selection in ensemble settings strongly influences the choice of algorithm, in that a poor choice of hyperparameters can lead to analysts using algorithms which do not give state-of-the-art performance for their data.},
 author = {Machlanski, Damian and Samothrakis, Spyridon and Clarke, Paul S},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4388032077},
 pages = {703--739},
 pdf = {https://proceedings.mlr.press/v236/machlanski24a/machlanski24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice},
 url = {https://proceedings.mlr.press/v236/machlanski24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-marmarelis24a,
 abstract = {Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Recent work has extended prediction intervals with finite-sample guarantees to partially identifiable causal outcomes, by means of a sensitivity model for hidden confounding. In deep learning, predictors can exploit their inductive biases for better generalization out of sample. We argue that the structure inherent to a deep ensemble should inform a tighter partial identification of the causal outcomes that they predict. We therefore introduce an approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. We present a simple approach to partial identification using existing causal sensitivity models and show empirically that Caus-Modens gives tighter outcome intervals, as measured by the necessary interval size to achieve sufficient coverage. The last of our three diverse benchmarks is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.},
 author = {Marmarelis, Myrl G. and Steeg, Greg Ver and Galstyan, Aram and Morstatter, Fred},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4381247842},
 pages = {18--40},
 pdf = {https://proceedings.mlr.press/v236/marmarelis24a/marmarelis24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Ensembled Prediction Intervals for Causal Outcomes Under Hidden Confounding},
 url = {https://proceedings.mlr.press/v236/marmarelis24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-mogensen24a,
 abstract = {Causal discovery outputs a causal structure, represented by a graph, from observed data. For time series data, there is a variety of methods, however, it is difficult to evaluate these on real data as realistic use cases very rarely come with a known causal graph to which output can be compared. In this paper, we present a dataset from an industrial subsystem at the European Spallation Source along with its causal graph which has been constructed from expert knowledge. This provides a testbed for causal discovery from time series observations of complex systems, and we believe this can help inform the development of causal discovery methodology.},
 author = {Mogensen, S{\o}ren Wengel and Rathsman, Karin and Nilsson, Per},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4388092734},
 pages = {1218--1236},
 pdf = {https://proceedings.mlr.press/v236/mogensen24a/mogensen24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal discovery in a complex industrial system: A time series benchmark},
 url = {https://proceedings.mlr.press/v236/mogensen24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-moreira24a,
 abstract = {Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine learning model by approximating its predictions while producing the respective explanations. Because of this, DiConStruct generates explanations efficiently while not impacting the black-box prediction task. We validate our method on an image dataset and a tabular dataset, showing that DiConStruct approximates the black-box models with higher fidelity than other concept explainability baselines, while providing explanations that include the causal relations between the concepts.},
 author = {Moreira, Ricardo Miguel de Oliveira and Bono, Jacopo and Cardoso, M\'ario and Saleiro, Pedro and Figueiredo, M\'ario A. T. and Bizarro, Pedro},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4390963180},
 pages = {740--768},
 pdf = {https://proceedings.mlr.press/v236/moreira24a/moreira24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {DiConStruct: Causal Concept-based Explanations through Black-Box Distillation},
 url = {https://proceedings.mlr.press/v236/moreira24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-ng24a,
 abstract = {This paper investigates in which cases continuous optimization for directed acyclic graph (DAG) structure learning can and cannot perform well and why this happens, and suggests possible directions to make the search procedure more reliable. Reisach et al. (2021) suggested that the remarkable performance of several continuous structure learning approaches is primarily driven by a high agreement between the order of increasing marginal variances and the topological order, and demonstrated that these approaches do not perform well after data standardization. We analyze this phenomenon for continuous approaches assuming equal and non-equal noise variances, and show that the statement may not hold in either case by providing counterexamples, justifications, and possible alternative explanations. We further demonstrate that nonconvexity may be a main concern especially for the non-equal noise variances formulation, while recent advances in continuous structure learning fail to achieve improvement in this case. Our findings suggest that future works should take into account the non-equal noise variances formulation to handle more general settings and for a more comprehensive empirical evaluation. Lastly, we provide insights into other aspects of the search procedure, including thresholding and sparsity, and show that they play an important role in the final solutions.},
 author = {Ng, Ignavier and Huang, Biwei and Zhang, Kun},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4362679195},
 pages = {71--105},
 pdf = {https://proceedings.mlr.press/v236/ng24a/ng24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Structure Learning with Continuous Optimization: A Sober Look and Beyond},
 url = {https://proceedings.mlr.press/v236/ng24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-pawar24a,
 abstract = {In the context of Machine Learning(ML) and Artificial Intelligence (AI), the concepts of sufficiency and necessity of features offer nuanced perspectives on the cause-and-effect relationships underlying a modelâs outputs. These concepts are, therefore, essential in Explainable AI (XAI) as they can provide a more holistic understanding of a âblack-box" AI model. Addressing this need, our study explored the relationships between the XAIâs explanations and the sufficiency and necessity of features in data. This is achieved by emphasising the impact of neighbourhoods, which are central in generating explanations. By analysing a diverse set of neighbourhoods, we highlighted how they influence the alignment between the feature rankings by XAI and the measures of sufficiency and necessity. This work offers two contributions. First, it provides a comprehensive discussion on how XAI frameworks relate to sufficiency and necessity with respect to their operating neighbourhoods; and second, it empirically demonstrates the effectiveness of these neighbourhoods in conveying the sufficiency and necessity of features by the XAI frameworks.},
 author = {Pawar, Urja and Beder, Christian and O\textsc{\char13}Reilly, Ruairi and O\textsc{\char13}Shea, Donna},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 pages = {570--586},
 pdf = {https://proceedings.mlr.press/v236/pawar24a/pawar24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On the Impact of Neighbourhood Sampling to Satisfy Sufficiency and Necessity Criteria in Explainable AI},
 url = {https://proceedings.mlr.press/v236/pawar24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-pham24a,
 abstract = {We consider the problem of estimating the counterfactual joint distribution of multiple quantities of interests (e.g., outcomes) in a multivariate causal model extended from the classical difference-in-difference design. Existing methods for this task either ignore the correlation structures among dimensions of the multivariate outcome by considering univariate causal models on each dimension separately and hence produce incorrect counterfactual distributions, or poorly scale even for moderate-size datasets when directly dealing with such multivariate causal model. We propose a method that alleviates both issues simultaneously by leveraging a robust latent one-dimensional subspace of the original high-dimension space and exploiting the efficient estimation from the univariate causal model on such space. Since the construction of the one-dimensional subspace uses information from all the dimensions, our method can capture the correlation structures and produce good estimates of the counterfactual distribution. We demonstrate the advantages of our approach over existing methods on both synthetic and real-world data.},
 author = {Pham, Thong and Shimizu, Shohei and Hino, Hideitsu and Le, Tam},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4388328932},
 pages = {1118--1140},
 pdf = {https://proceedings.mlr.press/v236/pham24a/pham24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Scalable Counterfactual Distribution Estimation in Multivariate Causal Models},
 url = {https://proceedings.mlr.press/v236/pham24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-rajendran24a,
 abstract = {We investigate the relationship between system identification and intervention design in dynamical systems. While previous research demonstrated how identifiable representation learning methods, such as Independent Component Analysis (ICA), can reveal cause-effect relationships, it relied on a passive perspective without considering how to collect data. Our work shows that in Gaussian Linear Time-Invariant (LTI) systems, the system parameters can be identified by introducing diverse intervention signals in a multi-environment setting. By harnessing appropriate diversity assumptions motivated by the ICA literature, our findings connect experiment design and representational identifiability in dynamical systems. We corroborate our findings on synthetic and (simulated) physical data. Additionally, we show that Hidden Markov Models, in general, and (Gaussian) LTI systems, in particular, fulfil a generalization of the Causal de Finetti theorem with continuous parameters.},
 author = {Rajendran, Goutham and Reizinger, Patrik and Brendel, Wieland and Ravikumar, Pradeep Kumar},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4389260677},
 pages = {41--70},
 pdf = {https://proceedings.mlr.press/v236/rajendran24a/rajendran24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An Interventional Perspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis},
 url = {https://proceedings.mlr.press/v236/rajendran24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-ribot24a,
 abstract = {We consider the task of causal imputation, where we aim to predict the outcomes of some set of actions across a wide range of possible contexts. As a running example, we consider predicting how different drugs affect cells from different cell types. We study the index-only setting, where the actions and contexts are categorical variables with a finite number of possible values. Even in this simple setting, a practical challenge arises, since often only a small subset of possible action-context pairs have been studied. Thus, models must extrapolate to novel action-context pairs, which can be framed as a form of matrix completion with rows indexed by actions, columns indexed by contexts, and matrix entries corresponding to outcomes. We introduce a novel SCM-based model class, where the outcome is expressed as a counterfactual, actions are expressed as interventions on an instrumental variable, and contexts are defined based on the initial state of the system. We show that, under a linearity assumption, this setup induces a latent factor model over the matrix of outcomes, with an additional fixed effect term. To perform causal prediction based on this model class, we introduce simple extension to the Synthetic Interventions estimator (Agarwal et al., 2020). We evaluate several matrix completion approaches on the PRISM drug repurposing dataset, showing that our method outperforms all other considered matrix completion approaches.},
 author = {Ribot, \'Alvaro and Squires, Chandler and Uhler, Caroline},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4392122266},
 pages = {1141--1175},
 pdf = {https://proceedings.mlr.press/v236/ribot24a/ribot24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent
  Factor Models},
 url = {https://proceedings.mlr.press/v236/ribot24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-rohbeck24a,
 abstract = {While a growing number of algorithms for causal discovery of directed acyclic graphs from observational and interventional data have been proposed, the robust identification of cyclic causal graphs in particular remains an open problem. Solutions to this challenge would have a considerable impact in various application domains, including single-cell genomics, where gene regulatory networks are known to contain feedback loops. Recent work has shown promise to address this challenge by describing the expression states in a population of cells as the steady-state solution of a stochastic dynamical system. However, this formulation cannot account for information on interventions in the population, and consequently, it ignores the associated causal inductive biases, which are key assets to obtain meaningful results and improve identifiability. In this work, we propose a novel method, Bicycle, which (i) infers cyclic causal relationships from i.i.d. data, (ii) explicitly accounts for information on the perturbation state of cells by a realization of the independent causal mechanism principle and (iii) models causal effects in a latent space rather than on observed data. We benchmark Bicycle in the context of existing approaches, demonstrating improved recovery of simulated causal graphs and improved out-of-distribution prediction performance on unseen perturbations in real single-cell datasets.},
 author = {Rohbeck, Martin and Clarke, Brian and Mikulik, Katharina and Pettet, Alexandra and Stegle, Oliver and Ueltzh\"offer, Kai},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 pages = {209--242},
 pdf = {https://proceedings.mlr.press/v236/rohbeck24a/rohbeck24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Bicycle: Intervention-Based Causal Discovery with Cycles},
 url = {https://proceedings.mlr.press/v236/rohbeck24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-simoes24a,
 abstract = {Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks through the study of recently-proposed information theoretic quantities grounded in considerations about causality.},
 author = {Simoes, Francisco N. F. Q. and Dastani, Mehdi and Ommen, Thijs van},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4391556039},
 pages = {188--208},
 pdf = {https://proceedings.mlr.press/v236/simoes24a/simoes24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Fundamental Properties of Causal Entropy and Information Gain},
 url = {https://proceedings.mlr.press/v236/simoes24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-strieder24a,
 abstract = {Knowledge of the underlying causal relations is essential for inferring the effect of interventions in complex systems. In a widely studied approach, structural causal models postulate noisy functional relations among interacting variables, where the underlying causal structure is then naturally represented by a directed graph whose edges indicate direct causal dependencies. In the typical application, this underlying causal structure must be learned from data, and thus, the remaining structure uncertainty needs to be incorporated into causal inference in order to draw reliable conclusions. In recent work, test inversions provide an ansatz to account for this data-driven model choice and, therefore, combine structure learning with causal inference. In this article, we propose the use of dual likelihood to greatly simplify the treatment of the involved testing problem. Indeed, dual likelihood leads to a closed-form solution for constructing confidence regions for total causal effects that rigorously capture both sources of uncertainty: causal structure and numerical size of nonzero effects. The proposed confidence regions can be computed with a bottom-up procedure starting from sink nodes. To render the causal structure identifiable, we develop our ideas in the context of linear causal relations with equal error variances.},
 author = {Strieder, David and Drton, Mathias},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4391833656},
 pages = {1--17},
 pdf = {https://proceedings.mlr.press/v236/strieder24a/strieder24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Dual Likelihood for Causal Inference under Structure Uncertainty},
 url = {https://proceedings.mlr.press/v236/strieder24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-talon24a,
 abstract = {Causal Representation Learning (CRL) aims at identifying high-level causal factors and their relationships from high-dimensional observations, e.g., images. While most CRL works focus on learning causal representations in a single environment, in this work we instead propose a first step towards learning causal representations from temporal sequences of images that can be adapted in a new environment, or composed across multiple related environments. In particular, we introduce DECAF, a framework that detects which causal factors can be reused and which need to be adapted from previously learned causal representations. Our approach is based on the availability of intervention targets, that indicate which variables are perturbed at each time step. Experiments on three benchmark datasets show that integrating our framework with four state-of-the-art CRL approaches leads to accurate representations in a new environment with only a few samples.},
 author = {Talon, Davide and Lippe, Phillip and James, Stuart and Bue, Alessio Del and Magliacane, Sara},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4392929894},
 pages = {296--324},
 pdf = {https://proceedings.mlr.press/v236/talon24a/talon24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Towards the Reusability and Compositionality of Causal Representations},
 url = {https://proceedings.mlr.press/v236/talon24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-ying24a,
 abstract = {The paramount obstacle in longitudinal studies for causal inference is the complex "treatment-confounder feedback." Traditional methodologies for elucidating causal effects in longitudinal analyses are primarily based on the assumption that time moves in specific intervals or that changes in treatment occur discretely. This conventional view confines treatment-confounder feedback to a limited, countable scope. The advent of real-time monitoring in modern medical research introduces functional longitudinal data with dynamically time-varying outcomes, treatments, and confounders, necessitating dealing with a potentially uncountably infinite treatment-confounder feedback. Thus, there is an urgent need for a more elaborate and refined theoretical framework to navigate these intricacies. Recently, Ying (2024) proposed a preliminary framework focusing on end-of-study outcomes and addressing the causality in functional longitudinal data. Our paper expands significantly upon his foundation in fourfold: First, we conduct a comprehensive review of existing literature, which not only fosters a deeper understanding of the underlying concepts but also illuminates the genesis of both Ying (2024)'s and ours. Second, we extend Ying (2024) to fully embrace a functional time-varying outcome process, incorporating right censoring and truncation by death, which are both significant and practical concerns. Third, we formalize previously informal propositions in Ying (2024), demonstrating how this framework broadens the existing frameworks in a nonparametric manner. Lastly, we delve into a detailed discussion on the interpretability and feasibility of our assumptions, and outlining a strategy for future numerical studies.},
 author = {Ying, Andrew},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4283702236},
 pages = {665--687},
 pdf = {https://proceedings.mlr.press/v236/ying24a/ying24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Causality of Functional Longitudinal Data},
 url = {https://proceedings.mlr.press/v236/ying24a.html},
 volume = {236},
 year = {2024}
}

@inproceedings{pmlr-v236-zhu24a,
 abstract = {In aggregated variables the impact of interventions is typically ill-defined because different micro-realizations of the same macro-intervention can result in different changes of downstream macro-variables. We show that this ill-definedness of causality on aggregated variables can turn unconfounded causal relations into confounded ones and vice versa, depending on the respective micro-realization. We argue that it is practically infeasible to only use aggregated causal systems when we are free from this ill-definedness. Instead, we need to accept that macro causal relations are typically defined only with reference to the micro states. On the positive side, we show that cause-effect relations can be aggregated when the macro interventions are such that the distribution of micro states is the same as in the observational distribution; we term this natural macro interventions. We also discuss generalizations of this observation.},
 author = {Zhu, Yuchen and Budhathoki, Kailash and K\"ubler, Jonas M. and Janzing, Dominik},
 booktitle = {Proceedings of the Third Conference on Causal Learning and Reasoning},
 editor = {Locatello, Francesco and Didelez, Vanessa},
 month = {01--03 Apr},
 openalex = {W4366999751},
 pages = {1192--1217},
 pdf = {https://proceedings.mlr.press/v236/zhu24a/zhu24a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Meaningful Causal Aggregation and Paradoxical Confounding},
 url = {https://proceedings.mlr.press/v236/zhu24a.html},
 volume = {236},
 year = {2024}
}
