@comment{@Proceedings{ACML 20122012,
  title =     {Proceedings of the Fourth Asian Conference on Machine Learning},
  booktitle = {Proceedings of the Fourth Asian Conference on Machine Learning},
  editor =    {Steven C. H. Hoi and Wray Buntine},
  publisher = {PMLR},
  series =    {Proceedings of Machine Learning Research},
  volume =    25
}}

@inproceedings{pmlr-v25-aboumoustafa12,
 abstract = {Multivariate Gaussian densities are pervasive in pattern recognition and machine learning. A central operation that appears in most of these areas is to measure the dierence between two multivariate Gaussians. Unfortunately, traditional measures based on the Kullback{ Leibler (KL) divergence and the Bhattacharyya distance do not satisfy all metric axioms necessary for many algorithms. In this paper we propose a modication for the KL divergence and the Bhattacharyya distance, for multivariate Gaussian densities, that transforms the two measures into distance metrics. Next, we show how these metric axioms impact the unfolding process of manifold learning algorithms. Finally, we illustrate the ecacy of the proposed metrics on two dierent manifold learning algorithms when used for motion clustering in video data. Our results show that, in this particular application, the new proposed metrics lead to boosts in performance (at least 7%) when compared to other divergence measures.},
 address = {Singapore Management University, Singapore},
 author = {Abou-Moustafa, Karim T. and Ferrie, Frank P.},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2406566318},
 pages = {1--15},
 pdf = {http://proceedings.mlr.press/v25/aboumoustafa12/aboumoustafa12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Note on Metric Properties for Some Divergence Measures: The Gaussian Case},
 url = {https://proceedings.mlr.press/v25/aboumoustafa12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-adhikari12,
 abstract = {Observing natural phenomena at several levels of detail results in multiresolution data. Extending models and algorithms to cope with multiresolution data is a prerequisite for wide-spread exploitation of the data represented in the multiple resolutions. Mixture models are widely used probabilistic models, however, the mixture models in their standard form can be used to analyze the data represented in a single resolution. In this paper, we propose a multiresolution mixture model based on merging of the mixture components across models represented in dierent resolutions. Result of such an analysis scenario is to have multiple mixture models, one mixture model for each resolution of data. Our proposed solution is based on the idea on the interaction between mixture models. More specically, we repeatedly merge component distributions of mixture models across dierent resolutions. We experiment our proposed algorithm on the two real-world chromosomal aberration datasets represented in two dierent resolutions. Results show an improvement},
 address = {Singapore Management University, Singapore},
 author = {Adhikari, Prem Raj and HollmÃ©n, Jaakko},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2293175044},
 pages = {17--32},
 pdf = {http://proceedings.mlr.press/v25/adhikari12/adhikari12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multiresolution Mixture Modeling using Merging of Mixture Components},
 url = {https://proceedings.mlr.press/v25/adhikari12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-asbeh12,
 abstract = {Identication of latent variables that govern a problem and the relationships among them given measurements in the observed world are important for causal discovery. This identication can be made by analyzing constraints imposed by the latents in the measurements. We introduce the concept of pairwise cluster comparison PCC to identify causal relationships from clusters and a two-stage algorithm, called LPCC, that learns a latent variable model (LVM) using PCC. First, LPCC learns the exogenous and the collider latents, as well as their observed descendants, by utilizing pairwise comparisons between clusters in the measurement space that may explain latent causes. Second, LPCC learns the non-collider endogenous latents and their children by splitting these latents from their previously learned latent ancestors. LPCC is not limited to linear or latent-tree models and does not make assumptions about the distribution. Using simulated and real-world datasets, we show that LPCC improves accuracy with the sample size, can learn large LVMs, and is accurate in learning compared to state-of-the-art algorithms.},
 address = {Singapore Management University, Singapore},
 author = {Asbeh, Nuaman and Lerner, Boaz},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2189221962},
 pages = {33--48},
 pdf = {http://proceedings.mlr.press/v25/asbeh12/asbeh12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Latent Variable Models by Pairwise Cluster Comparison},
 url = {https://proceedings.mlr.press/v25/asbeh12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-azmandian12,
 abstract = {Selecting features is an important step of any machine learning task, though most of the focus has been to choose features relevant for classication and regression. In this work, we present a novel non-parametric evaluation criterion for lter-based feature selection which enhances outlier detection. Our proposed method seeks the subset of features that represents the inherent characteristics of the normal dataset while forcing outliers to stand out, making them more easily distinguished by outlier detection algorithms. Experimental results on real datasets show the advantage of this feature selection algorithm compared to popular and state-of-the-art methods. We also show that the proposed algorithm is able to overcome the small sample space problem and perform well on highly imbalanced datasets.},
 address = {Singapore Management University, Singapore},
 author = {Azmandian, Fatemeh and Dy, Jennifer G. and Aslam, Javed A. and Kaeli, David R.},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2188756823},
 pages = {49--64},
 pdf = {http://proceedings.mlr.press/v25/azmandian12/azmandian12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Local Kernel Density Ratio-Based Feature Selection for Outlier Detection},
 url = {https://proceedings.mlr.press/v25/azmandian12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-chatzis12,
 abstract = {The dramatic rates new digital content becomes available has brought collaborative filtering systems in the epicenter of computer science research in the last decade. In this paper, we propose a novel methodology for rating prediction utilizing concepts from the field of Bayesian nonparametrics. The basic concept that underlies our approach is that each user rates a presented item based on the latent genres of the item and the latent interests of the user. Each item may belong to more than one genre, and each user may belong to more than one latent interest class. The number of existing latent genres and interests are not known beforehand, but should be inferred in a data-driven fashion. We devise a novel hierarchical factor analysis model to formulate our approach under these assumptions. We impose suitable priors over the allocation of items into genres, and users into interests; specifically, we utilize a novel scheme which comprises two coupled Indian buffet process priors that allow the number of latent classes (genres/interests) to be automatically inferred. We experiment on a large set of real ratings data, and show that our approach outperforms four common baselines, including two very competitive state-of-the-art approaches.},
 address = {Singapore Management University, Singapore},
 author = {Chatzis, Sotirios P.},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2240198344},
 pages = {65--79},
 pdf = {http://proceedings.mlr.press/v25/chatzis12/chatzis12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A coupled indian buffet process model for collaborative filtering},
 url = {https://proceedings.mlr.press/v25/chatzis12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-chiang12,
 abstract = {Multi-label classication has attracted a great deal of attention in recent years. This paper presents an interesting nding, namely, being able to identify neighbors with trustable labels can signicantly improve the classication accuracy. Based on this nding, we propose a k-nearest-neighbor-based ranking approach to solve the multi-label classication problem. The approach exploits a ranking model to learn which neighbor’s labels are more trustable candidates for a weighted KNN-based strategy, and then assigns higher weights to those candidates when making weighted-voting decisions. The weights can then be determined by using a generalized pattern search technique. We collect several real-word data sets from various domains for the experiment. Our experiment results demonstrate that the proposed method outperforms state-of-the-art instance-based learning approaches. We believe that appropriately exploiting k-nearest neighbors is useful to solve the multi-label problem.},
 address = {Singapore Management University, Singapore},
 author = {Chiang, Tsung-Hsien and Lo, Hung-Yi and Lin, Shou-De},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2182472154},
 pages = {81--96},
 pdf = {http://proceedings.mlr.press/v25/chiang12/chiang12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Ranking-based KNN Approach for Multi-Label Classification},
 url = {https://proceedings.mlr.press/v25/chiang12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-demyanov12,
 abstract = {We study the problem of selecting the best parameter values to use for a support vector machine (SVM) with RBF kernel. Our methods extend the well-known formulas for AIC and BIC, and we present two alternative approaches for calculating the necessary likelihood functions for these formulas. Our rst approach is based on using the distances of support vectors from the separating hyperplane. Our second approach estimates the probability that the SVM hyperplane coincides with the Bayes classier, by analysing the disposition of points in the kernel feature space. We experimentally compare our two approaches with several existing methods and show they are able to achieve good accuracy, whilst also having low running time.},
 address = {Singapore Management University, Singapore},
 author = {Demyanov, Sergey and Bailey, James and Ramamohanarao, Kotagiri and Leckie, Christopher},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W3041639100},
 pages = {97--112},
 pdf = {http://proceedings.mlr.press/v25/demyanov12/demyanov12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {AIC and BIC based approaches for SVM parameter value estimation with RBF kernels.},
 url = {https://proceedings.mlr.press/v25/demyanov12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-fan12,
 abstract = {Online algorithms allow data instances to be processed in a sequential way, which is important for large-scale and real-time applications. In this paper, we propose a novel online clustering approach based on a Dirichlet process mixture of generalized Dirichlet (GD) distributions, which can be considered as an extension of the finite GD mixture model to the infinite case. Our approach is built on nonparametric Bayesian analysis where the determination of the number of clusters is sidestepped by assuming an infinite number of mixture components. Moreover, an unsupervised localized feature selection scheme is integrated with the proposed nonparametric framework to improve the clustering performance. By learning the proposed model in an online manner using a variational approach, all the involved parameters and features saliencies are estimated simultaneously and effectively in closed forms. The proposed online infinite mixture model is validated through both synthetic data sets and two challenging real-world applications namely text document clustering and online human face detection.},
 address = {Singapore Management University, Singapore},
 author = {Fan, Wentao and Bouguila, Nizar},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 pages = {113--128},
 pdf = {http://proceedings.mlr.press/v25/fan12/fan12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Online Learning of a {D}irichlet Process Mixture of Generalized {D}irichlet Distributions for Simultaneous Clustering and Localized Feature Selection},
 url = {https://proceedings.mlr.press/v25/fan12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-feraud12,
 abstract = {Stochastic multi-armed bandit algorithms are used to solve the exploration and exploitation dilemma in sequential optimization problems. The algorithms based on upper condence bounds oer strong theoretical guarantees, they are easy to implement and ecient in practice. We considers a new bandit setting, called \scratch-games, where arm budgets are limited and reward are drawn without replacement. Using Sering inequality, we propose an upper condence bound algorithm adapted to this setting. We show that the bound of expectation to play a suboptimal arm is lower than the one of UCB1 policy. We illustrate this result on both synthetic problems and realistic problems (ad-serving and emailing campaigns optimization).},
 address = {Singapore Management University, Singapore},
 author = {FÃ©raud, RaphaÃ«l and Urvoy, Tanguy},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2182485257},
 pages = {129--143},
 pdf = {http://proceedings.mlr.press/v25/feraud12/feraud12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A stochastic bandit algorithm for scratch games},
 url = {https://proceedings.mlr.press/v25/feraud12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-gu12,
 abstract = {In this paper, we investigate the problem of exploiting global information to improve the performance of SVMs on large scale classication problems. We rst present a unied general framework for the existing min-max machine methods in terms of within-class dispersions and between-class dispersions. By dening a new within-class dispersion measure, we then propose a novel max-margin ratio machine (MMRM) method that can be formulated as a linear programming problem with scalability for large data sets. Kernels can be easily incorporated into our method to address non-linear classication problems. Our empirical results show that the proposed MMRM approach achieves promising results on large data sets.},
 address = {Singapore Management University, Singapore},
 author = {Gu, Suicheng and Guo, Yuhong},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2181385233},
 pages = {145--157},
 pdf = {http://proceedings.mlr.press/v25/gu12/gu12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Max-Margin Ratio Machine},
 url = {https://proceedings.mlr.press/v25/gu12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-guillame-bert12,
 abstract = {We introduce a temporal pattern model called Temporal Interval Tree Association Rules (Tita rules or Titar). This pattern model can express both uncertainty and temporal inaccuracy of temporal events. Among other things, Tita rules can express the usual time point operators, synchronicity, order, and chaining, as well as temporal negation and disjunctive temporal constraints. Using this representation, we present the Titar learner algorithm that can be used to extract Tita rules from large datasets expressed as Symbolic Time Sequences. The selection of temporal constraints (or time-frames) is at the core of the temporal learning. Our learning algorithm is based on two novel approaches for this problem. This rst one is designed to select temporal constraints for the head of temporal association rules. The second selects temporal constraints for the body of such rules. We discuss the evaluation of probabilistic temporal association rules, evaluate our technique with two experiments, introduce a metric to evaluate sets of temporal rules, compare the results with two other approaches and discuss the results.},
 address = {Singapore Management University, Singapore},
 author = {Guillame-Bert, Mathieu and Crowley, James L.},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2188655722},
 pages = {159--174},
 pdf = {http://proceedings.mlr.press/v25/guillame-bert12/guillame-bert12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning temporal association rules on Symbolic time sequences},
 url = {https://proceedings.mlr.press/v25/guillame-bert12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-haimovitch12,
 abstract = {We describe a bootstrapping algorithm to learn from partially labeled data, and the results of an empirical study for using it to improve performance of sentiment classification using up to 15 million unlabeled Amazon product reviews. Our experiments cover semi-supervised learning, domain adaptation and weakly supervised learning. In some cases our methods were able to reduce test error by more than half using such large amount of data. NOTICE: This is only the supplementary material.},
 address = {Singapore Management University, Singapore},
 author = {Haimovitch, Yoav and Crammer, Koby and Mannor, Shie},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W1812250178},
 pages = {175--190},
 pdf = {http://proceedings.mlr.press/v25/haimovitch12/haimovitch12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {More Is Better: Large Scale Partially-supervised Sentiment Classification - Appendix},
 url = {https://proceedings.mlr.press/v25/haimovitch12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-hoi12,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 address = {Singapore Management University, Singapore},
 author = {Hoi, Steven C. H. and Buntine, Wray},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W4240465921},
 pages = {i--xvii},
 pdf = {http://proceedings.mlr.press/v25/hoi12/hoi12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v25/hoi12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-kawajiri12,
 abstract = {Recently wireless LAN (WLAN) localization systems are gaining popularity in pervasive computing, machine learning and sensor networks communities, especially indoor scenarios where GPS coverage is limited. To accurately predict location, a large amount of ngerprints composed of received signal strength values is necessary. Moreover, standard supervised or semi-supervised approaches also require location information to each ngerprint, where annotation work is rather tedious and time consuming. To reduce the eorts and time required to build calibration data, we present a novel calibration methodology \routeannotation and a self-training algorithm for learning from route information eectively. On the proposed calibration methodology, an annotator walks around while measuring ngerprints, then occasionally stops to annotate ngerprints with route from previous location to current location. This calibration reduces work time even compared to partially annotation, while routes have richer information for learning. The proposed learning algorithm comprises following two iterative steps: 1) inferring locations of each ngerprint under route constraints and 2) updating parameters. Experimental results on real-world datasets demonstrate learning from route-annotated data is comparable to state-of-the-art supervised and semi-supervised approaches trained with large amount of calibration data.},
 address = {Singapore Management University, Singapore},
 author = {Kawajiri, Ryoma and Shimosaka, Masamichi and Fukui, Rui and Sato, Tonomasa},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2408045045},
 pages = {191--204},
 pdf = {http://proceedings.mlr.press/v25/kawajiri12/kawajiri12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Frustratingly Simplified Deployment in WLAN Localization by Learning from Route Annotation.},
 url = {https://proceedings.mlr.press/v25/kawajiri12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-klami12,
 abstract = {Matching of samples refers to the problem of inferring unknown co-occurrence or alignment between observations in two data sets. Given two sets of equally many samples, the task is to nd for each sample a representative sample in the other set, without prior knowledge on a distance measure between the sets. Recently a few alternative solutions have been suggested, based on maximization of joint likelihood or various measures of between-data statistical dependency. In this work we present an variational Bayesian solution for the problem, learning a Bayesian canonical correlation analysis model with a permutation parameter for re-ordering the samples in one of the sets. We approximate the posterior over the permutations, and demonstrate that the resulting matching algorithm clearly outperforms all of the earlier solutions.},
 address = {Singapore Management University, Singapore},
 author = {Klami, Arto},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2138047313},
 pages = {205--220},
 pdf = {http://proceedings.mlr.press/v25/klami12/klami12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Variational Bayesian Matching},
 url = {https://proceedings.mlr.press/v25/klami12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-li12,
 abstract = {The abundance of real-world data and limited labeling budget calls for active learning, which is an important learning paradigm for reducing human labeling eorts. Many recently developed active learning algorithms consider both uncertainty and representativeness when making querying decisions. However, exploiting representativeness with uncertainty concurrently usually requires tackling sophisticated and challenging learning tasks, such as clustering. In this paper, we propose a new active learning framework, called hinted sampling, which takes both uncertainty and representativeness into account in a simpler way. We design a novel active learning algorithm within the hinted sampling framework with an extended support vector machine. Experimental results validate that the novel active learning algorithm can result in a better and more stable performance than that achieved by state-of-the-art algorithms.},
 address = {Singapore Management University, Singapore},
 author = {Li, Chun-Liang and Ferng, Chun-Sung and Lin, Hsuan-Tien},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2182891756},
 pages = {221--235},
 pdf = {http://proceedings.mlr.press/v25/li12/li12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Active Learning with Hinted Support Vector Machine},
 url = {https://proceedings.mlr.press/v25/li12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-liu12a,
 abstract = {Based on the convex-concave relaxation procedure (CCRP), the (extended) path following algorithms were recently proposed to approximately solve the equal-sized graph matching problem, and exhibited a state-of-the-art performance (Zaslavskiy et al., 2009; Liu et al., 2012). However, they cannot be used for subgraph matching since either their convex or concave relaxation becomes no longer applicable. In this paper we extend the CCRP to tackle subgraph matching, by proposing a convex as well as a concave relaxation of the problem. Since in the context of CCRP, the convex relaxation can be viewed as an initialization of a concave programming, we introduce two other initializations for comparison. Meanwhile, the graduated assignment algorithm is also introduced in the experimental comparisons, which witness the validity of the proposed algorithm.},
 address = {Singapore Management University, Singapore},
 author = {Liu, Zhi-Yong and Qiao, Hong},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2395138445},
 pages = {237--252},
 pdf = {http://proceedings.mlr.press/v25/liu12a/liu12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Convex-Concave Relaxation Procedure Based Subgraph Matching Algorithm},
 url = {https://proceedings.mlr.press/v25/liu12a.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-liu12b,
 abstract = {The goal of traditional multi-instance learning (MIL) is to predict the labels of the bags, whereas in many real applications, it is desirable to get the instance labels, especially the labels of key instances that trigger the bag labels, in addition to getting bag labels. Such a problem has been largely unexplored before. In this paper, we formulate the Key Instance Detection (KID) problem, and propose a voting framework (VF) solution to KID. The key of VF is to exploit the relationship among instances, represented by a citer kNN graph. This graph is dierent from commonly used nearest neighbor graphs, but is suitable for KID. Experiments validate the eectiveness of VF for KID. Additionally, VF also outperforms state-of-the-art MIL approaches on the performance of bag label prediction.},
 address = {Singapore Management University, Singapore},
 author = {Liu, Guoqing and Wu, Jianxin and Zhou, Zhi-Hua},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2115672776},
 pages = {253--268},
 pdf = {http://proceedings.mlr.press/v25/liu12b/liu12b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Key Instance Detection in Multi-Instance Learning},
 url = {https://proceedings.mlr.press/v25/liu12b.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-madani12,
 abstract = {Consider learning tasks where the precision requirement is very high, for example a 99 % precision requirement for a video classification application. We report that when very different sources of evidence such as text, audio, and video features are available, combining the outputs of base classifiers trained on each feature type separately, aka late fusion, can substantially increase the recall of the combination at high precisions, compared to the performance of a single classifier trained on all the feature types, i.e., early fusion, or compared to the individual base classifiers. We show how the probability of a joint false-positive mistake can be less—in some cases significantly less—than the product of individual probabilities of conditional false-positive mistakes (a NoisyOR combination). Our analysis highlights a simple key criterion for this boosted precision phenomenon and justifies referring to such feature families as (nearly) independent. We assess the relevant factors for achieving high precision empirically, and explore combination techniques informed by the analysis.},
 address = {Singapore Management University, Singapore},
 author = {Madani, Omid and Georg, Manfred and Ross, David A.},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2127581298},
 pages = {269--284},
 pdf = {http://proceedings.mlr.press/v25/madani12/madani12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On using nearly-independent feature families for high precision and confidence},
 url = {https://proceedings.mlr.press/v25/madani12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-mao12,
 abstract = {We introduce a new statistical relational learning (SRL) approach in which models for structured data, especially network data, are constructed as networks of communicating nite probabilistic automata. Leveraging existing automata learning methods from the area of grammatical inference, we can learn generic models for network entities in the form of automata templates. As is characteristic for SRL techniques, the abstraction level aorded by learning generic templates enables one to apply the learned model to new domains. A main benet of learning models based on nite automata lies in the fact that one can analyse the resulting models using formal model-checking techniques, which adds a dimension of model analysis not usually available for traditional SRL modeling frameworks.},
 address = {Singapore Management University, Singapore},
 author = {Mao, Hua and Jaeger, Manfred},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2119047371},
 pages = {285--300},
 pdf = {http://proceedings.mlr.press/v25/mao12/mao12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning and Model-Checking Networks of I/O Automata},
 url = {https://proceedings.mlr.press/v25/mao12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-nakajima12,
 abstract = {Principal component analysis (PCA) can be regarded as approximating a data matrix with a low-rank one by imposing sparsity on its singular values, and its robust variant further captures sparse noise. In this paper, we extend such sparse matrix learning methods, and propose a novel unied framework called sparse additive matrix factorization (SAMF). SAMF systematically induces various types of sparsity by the so-called model-induced regularization in the Bayesian framework. We propose an iterative algorithm called the mean update (MU) for the variational Bayesian approximation to SAMF, which gives the global optimal solution for a large subset of parameters in each step. We demonstrate the usefulness of our method on articial data and the foreground/background video separation.},
 address = {Singapore Management University, Singapore},
 author = {Nakajima, Shinichi and Sugiyama, Masashi and Babacan, S. Derin},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2294786191},
 pages = {301--316},
 pdf = {http://proceedings.mlr.press/v25/nakajima12/nakajima12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sparse Additive Matrix Factorization for Robust PCA and Its Generalization},
 url = {https://proceedings.mlr.press/v25/nakajima12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-navaroli12,
 abstract = {As digital communication devices play an increasingly prominent role in our daily lives, the ability to analyze and understand our communication patterns becomes more important. In this paper, we investigate a latent variable modeling approach for extracting information from individual email histories, focusing in particular on understanding how an individual communicates over time with recipients in their social network. The proposed model consists of latent groups of recipients, each of which is associated with a piecewise-constant Poisson rate over time. Inference of group memberships, temporal changepoints, and rate parameters is carried out via Markov Chain Monte Carlo (MCMC) methods. We illustrate the utility of the model by applying it to both simulated and real-world email data sets.},
 address = {Singapore Management University, Singapore},
 author = {Navaroli, Nicholas and DuBois, Christopher and Smyth, Padhraic},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2186417619},
 pages = {317--332},
 pdf = {http://proceedings.mlr.press/v25/navaroli12/navaroli12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Statistical Models for Exploring Individual Email Communication Behavior},
 url = {https://proceedings.mlr.press/v25/navaroli12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-neven12,
 abstract = {We introduce a novel discrete optimization method for training in the context of a boosting framework for large scale binary classifiers. The motivation is to cast the training problem into the format required by existing adiabatic quantum hardware. First we provide theoretical arguments concerning the transformation of an originally continuous optimization problem into one with discrete variables of low bit depth. Next we propose QBoost as an iterative training algorithm in which a subset of weak classifiers is selected by solving a hard optimization problem in each iteration. A strong classifier is incrementally constructed by concatenating the subsets of weak classifiers. We supplement the findings with experiments on one synthetic and two natural data sets and compare against the performance of existing boosting algorithms. Finally, by conducting a quantum Monte Carlo simulation we gather evidence that adiabatic quantum optimization is able to handle the discrete optimization problems generated by QBoost.},
 address = {Singapore Management University, Singapore},
 author = {Neven, Hartmut and Denchev, Vasil S. and Rose, Geordie and Macready, William G.},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 pages = {333--348},
 pdf = {http://proceedings.mlr.press/v25/neven12/neven12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {QBoost: Large Scale Classifier Training withAdiabatic Quantum Optimization},
 url = {https://proceedings.mlr.press/v25/neven12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-prabhakaran12,
 abstract = {A fully probabilistic approach to reconstructing Gaussian graphical models from distance data is presented. The main idea is to extend the usual central Wishart model in traditional methods to using a likelihood depending only on pairwise distances, thus being independent of geometric assumptions about the underlying Euclidean space. This extension has two advantages: the model becomes invariant against potential bias terms in the measurements, and can be used in situations which on input use a kernel- or distance matrix, without requiring direct access to the underlying vectors. The latter aspect opens up a huge new application field for Gaussian graphical models, as network reconstruction is now possible from any Mercer kernel, be it on graphs, strings, probabilities or more complex objects. We combine this likelihood with a suitable prior to enable Bayesian network inference. We present an efficient MCMC sampler for this model and discuss the estimation of module networks. Experiments depict the high quality and usefulness of the inferred networks.},
 address = {Singapore Management University, Singapore},
 author = {Prabhakaran, Sandhya and Metzner, Karin J. and BÃ¶hm, Alexander and Roth, Volker},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2152098322},
 pages = {349--364},
 pdf = {http://proceedings.mlr.press/v25/prabhakaran12/prabhakaran12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Recovering networks from distance data},
 url = {https://proceedings.mlr.press/v25/prabhakaran12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-sasaki12,
 abstract = {Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants are assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the source where the components can have linear and higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations. In numerical experiments on articial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data.},
 address = {Singapore Management University, Singapore},
 author = {Sasaki, Hiroaki and Gutmann, Michael U. and Shouno, Hayaru and HyvÃ¤rinen, Aapo},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2189550056},
 pages = {365--378},
 pdf = {http://proceedings.mlr.press/v25/sasaki12/sasaki12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Topographic Analysis of Correlated Components},
 url = {https://proceedings.mlr.press/v25/sasaki12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-shariat12,
 abstract = {Traditional pairwise sequence alignment is based on matching individual samples from two sequences, under time monotonicity constraints. However, in some instances matching two segments of points may be preferred and can result in increased noise robustness. This paper presents an approach to segmental sequence alignment based on adaptive pairwise segmentation. We introduce a distance metric between segments based on average pairwise distances, which addresses deficiencies of prior approaches. We then present a modified pair-HMM that incorporates the proposed distance metric and use it to devise an eÂ¡cient algorithm to jointly segment and align the two sequences. Our results demonstrate that this new measure of sequence similarity can lead to improved classification performance, while being resilient to noise, on a variety of problems, from EEG to motion sequence classification.},
 address = {Singapore Management University, Singapore},
 author = {Shariat, Shahriar and Pavlovic, Vladimir},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2407459982},
 pages = {379--394},
 pdf = {http://proceedings.mlr.press/v25/shariat12/shariat12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improved Sequence Classification Using Adaptive Segmental Sequence Alignment},
 url = {https://proceedings.mlr.press/v25/shariat12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-than12,
 abstract = {We consider supervised dimension reduction (SDR) for problems with discrete variables. Existing methods are computationally expensive, and often do not take the local structure of data into consideration when searching for a low-dimensional space. In this paper, we propose a novel framework for SDR which is (1) general and ∞exible so that it can be easily adapted to various unsupervised topic models, (2) able to inherit scalability of unsupervised topic models, and (3) can exploit well label information and local structure of data when searching for a new space. Extensive experiments with adaptations to three models demonstrate that our framework can yield scalable and qualitative methods for SDR. One of those adaptations can perform better than the state-of-the-art method for SDR while enjoying signiflcantly faster speed.},
 address = {Singapore Management University, Singapore},
 author = {Than, Khoat and Ho, Tu Bao and Nguyen, Duy Khuong and Pham, Ngoc Khanh},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2187237088},
 pages = {395--410},
 pdf = {http://proceedings.mlr.press/v25/than12/than12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Supervised dimension reduction with topic models},
 url = {https://proceedings.mlr.press/v25/than12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-tran12a,
 abstract = {Ordinal data is omnipresent in almost all multiuser-generated feedback - questionnaires, preferences etc. This paper investigates modelling of ordinal data with Gaussian restricted Boltzmann machines (RBMs). In particular, we present the model architecture, learning and inference procedures for both vector-variate and matrix-variate ordinal data. We show that our model is able to capture latent opinion profile of citizens around the world, and is competitive against state-of-art collaborative filtering techniques on large-scale public datasets. The model thus has the potential to extend application of RBMs to diverse domains such as recommendation systems, product reviews and expert assessments.},
 address = {Singapore Management University, Singapore},
 author = {Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 pages = {411--426},
 pdf = {http://proceedings.mlr.press/v25/tran12a/tran12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Cumulative Restricted {B}oltzmann Machines for Ordinal Matrix Data Analysis},
 url = {https://proceedings.mlr.press/v25/tran12a.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-tran12b,
 abstract = {Ranking over sets arise when users choose between groups of items. For example, a group may be of those movies deemed 5 stars to them, or a customized tour package. It turns out, to model this data type properly, we need to investigate the general combinatorics problem of partitioning a set and ordering the subsets. Here we construct a probabilistic log-linear model over a set of ordered subsets. Inference in this combinatorial space is highly challenging: The space size approaches (N!=2)6:93145 N+1 as N approaches innity. We propose a split-and-merge Metropolis-Hastings procedure that can explore the statespace eciently. For discovering hidden aspects in the data, we enrich the model with latent binary variables so that the posteriors can be eciently evaluated. Finally, we evaluate the proposed model on large-scale collaborative ltering tasks and demonstrate that it is},
 address = {Singapore Management University, Singapore},
 author = {Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2963348843},
 pages = {427--442},
 pdf = {http://proceedings.mlr.press/v25/tran12b/tran12b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning From Ordered Sets and Applications in Collaborative Ranking},
 url = {https://proceedings.mlr.press/v25/tran12b.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-tran12c,
 abstract = {In machine learning, we often encounter datasets that can be described using simple rules and regular exception patterns describing situations where those rules do not apply. In this paper, we propose a two-way parallel class expression learning algorithm that is suitable for this kind of problem. This is a top-down renement-based class expression learning algorithm for Description Logic (DL). It is distinguished from similar DL learning algorithms in the way it uses the concepts generated by the renement operator. In our approach, we unify the computation of concepts describing positive and negative examples, but we maintain them separately, and combine them at the end. By doing so, we can avoid the use of negation in the renement without any loss of generality. Evaluation shows that our approach can reduce the search space signicantly, and therefore the learning time is reduced. Our implementation is based on the DL-Learner framework and we inherit the Parallel Class Expression Learning (ParCEL) algorithm design for parallelisation.},
 address = {Singapore Management University, Singapore},
 author = {Tran, An C. and Dietrich, Jens and Guesgen, Hans W. and Marsland, Stephen},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2185146562},
 pages = {443--458},
 pdf = {http://proceedings.mlr.press/v25/tran12c/tran12c.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Two-way Parallel Class Expression Learning},
 url = {https://proceedings.mlr.press/v25/tran12c.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-trapeznikov12,
 abstract = {In many classification systems, sensing modalities have different acquisition costs. It is often unnecessary to use every modality to classify a majority of examples. We study a multi-stage system in a prediction time cost reduction setting, where the full data is available for training, but for a test example, measurements in a new modality can be acquired at each stage for an additional cost. We seek decision rules to reduce the average measurement acquisition cost. We formulate an empirical risk minimization problem (ERM) for a multi-stage reject classifier, wherein the stage k classifier either classifies a sample using only the measurements acquired so far or rejects it to the next stage where more attributes can be acquired for a cost. If we restrict ourselves to binary classification setting then, to solve the ERM problem, we show that the optimal reject classifier at each stage is a combination of two binary classifiers, one biased towards positive examples and the other biased towards negative examples. We use this parameterization to construct stage-by-stage global surrogate risk, develop an iterative algorithm in the boosting framework and present convergence and generalization results. We test our work on synthetic, medical and explosives detection datasets. Our results demonstrate that substantial cost reduction without a significant sacrifice in accuracy is achievable.},
 address = {Singapore Management University, Singapore},
 author = {Trapeznikov, Kirill and Saligrama, Venkatesh and CastaÃ±Ã³n, David},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W1979871898},
 pages = {459--474},
 pdf = {http://proceedings.mlr.press/v25/trapeznikov12/trapeznikov12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-stage classifier design},
 url = {https://proceedings.mlr.press/v25/trapeznikov12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-vovk12,
 abstract = {Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given confidence level. Inductive conformal predictors are a computationally efficient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have only been known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modifications. In particular, it discusses a convenient expression of one of the modifications in terms of ROC curves.},
 address = {Singapore Management University, Singapore},
 author = {Vovk, Vladimir},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2166693382},
 pages = {475--490},
 pdf = {http://proceedings.mlr.press/v25/vovk12/vovk12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Conditional validity of inductive conformal predictors},
 url = {https://proceedings.mlr.press/v25/vovk12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-wang12a,
 abstract = {Nonlinear encoding of SIFT features has recently shown good promise in image classification. This scheme is able to reduce the training complexity of the traditional bag-of-feature approaches while achieving better performance. As a result, it is suitable for large-scale image classification applications. However, existing nonlinear encoding methods do not explicitly consider the spatial relationship when encoding the local features, but merely leaving the spatial information used at a later stage, e.g. through the spatial pyramid matching, is largely inadequate. In this paper, we propose a joint sparse coding and dictionary learning scheme that take the spatial information into consideration in encoding. Our experiments on synthetic data and benchmark data demonstrate that the proposed scheme can learn a better dictionary and achieve higher classification accuracy.},
 address = {Singapore Management University, Singapore},
 author = {Wang, Jiang and Yuan, Junsong and Chen, Zhouyuan and Wu, Ying},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2108658819},
 pages = {491--505},
 pdf = {http://proceedings.mlr.press/v25/wang12a/wang12a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Spatial Locality-Aware Sparse Coding and Dictionary Learning},
 url = {https://proceedings.mlr.press/v25/wang12a.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-wang12b,
 abstract = {Concerned with multi-objective reinforcement learning (MORL), this paper presents MO-MCTS, an extension of Monte-Carlo Tree Search to multi-objective sequential decision making. The known multi-objective indicator referred to as hyper-volume indicatorÂ is used to define an action selection criterion, replacing the UCB criterion in order to deal with multi-dimensional rewards. MO-MCTSÂ is firstly compared with an existing MORL algorithm on the artificial Deep Sea Treasure problem. Then a scalability study of MO-MCTSÂ is made on the NP-hard problem of grid scheduling, showing that the performance of MO-MCTSÂ matches the non RL-based state of the art albeit with a higher computational cost.},
 address = {Singapore Management University, Singapore},
 author = {Wang, Weijia and Sebag, MichÃ¨le},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 pages = {507--522},
 pdf = {http://proceedings.mlr.press/v25/wang12b/wang12b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-objective {M}onte-{C}arlo Tree Search},
 url = {https://proceedings.mlr.press/v25/wang12b.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-yang12,
 abstract = {For classication problems with millions of training examples or dimensions, accuracy, training and testing speed and memory usage are the main concerns. Recent advances have allowed linear SVM to tackle problems with moderate time and space cost, but for many tasks in computer vision, additive kernels would have higher accuracies. In this paper, we propose the PmSVM-LUT algorithm that employs Look-Up Tables to boost the training and testing speed and save memory usage of additive kernel SVM classication, in order to meet the needs of large scale problems. The PmSVM-LUT algorithm is based on PmSVM (Wu, 2012), which employed polynomial approximation for the gradient function to speedup the dual coordinate descent method. We also analyze the polynomial approximation numerically to demonstrate its validity. Empirically, our algorithm is faster than PmSVM and feature mapping in many datasets with higher classication accuracies and can save up to 60% memory usage as well.},
 address = {Singapore Management University, Singapore},
 author = {Yang, Hao and Wu, Jianxin},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2139251730},
 pages = {523--538},
 pdf = {http://proceedings.mlr.press/v25/yang12/yang12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Practical large scale classification with additive kernels},
 url = {https://proceedings.mlr.press/v25/yang12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-yasutake12,
 abstract = {We consider an online learning framework where the task is to predict a permutation which represents a ranking of n xed objects. At each trial, the learner incurs a loss dened as Kendall tau distance between the predicted permutation and the true permutation given by the adversary. This setting is quite natural in many situations such as information retrieval and recommendation tasks. We prove a lower bound of the cumulative loss and hardness results. Then, we propose an algorithm for this problem and prove its relative loss bound which shows our algorithm is close to optimal.},
 address = {Singapore Management University, Singapore},
 author = {Yasutake, Shota and Hatano, Kohei and Takimoto, Eiji and Takeda, Masayuki},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2887146843},
 pages = {539--553},
 pdf = {http://proceedings.mlr.press/v25/yasutake12/yasutake12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Online Rank Aggregation},
 url = {https://proceedings.mlr.press/v25/yasutake12.html},
 volume = {25},
 year = {2012}
}

@inproceedings{pmlr-v25-zhou12,
 abstract = {Learning with Positive and Unlabeled instances (PU learning) arises widely in information retrieval applications. To address the unavailability issue of negative instances, most existing PU learning approaches require to either identify a reliable set of negative instances from the unlabeled data or estimate probability densities as an intermediate step. However, inaccurate negative-instance identication or poor density estimation may severely degrade overall performance of the nal predictive model. To this end, we propose a novel PU learning method based on density ratio estimation without constructing any sets of negative instances or estimating any intermediate densities. To further boost PU learning performance, we extend our proposed learning method in a multi-view manner by utilizing multiple heterogeneous sources. Extensive experimental studies demonstrate the eectiveness of our proposed methods, especially when positive labeled data are limited.},
 address = {Singapore Management University, Singapore},
 author = {Zhou, Joey Tianyi and Pan, Sinno Jialin and Mao, Qi and Tsang, Ivor W.},
 booktitle = {Proceedings of the Asian Conference on Machine Learning},
 editor = {Hoi, Steven C. H. and Buntine, Wray},
 month = {04--06 Nov},
 openalex = {W2166452903},
 pages = {555--570},
 pdf = {http://proceedings.mlr.press/v25/zhou12/zhou12.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-view Positive and Unlabeled Learning},
 url = {https://proceedings.mlr.press/v25/zhou12.html},
 volume = {25},
 year = {2012}
}
