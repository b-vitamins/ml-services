@proceedings{MIDL2021,
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Mattias Heinrich and Qi Dou and Marleen Bruijne and Jan Lellmann and Alexander SchlÃ¤fer and Floris Ernst},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 volume = {143}
}

@inproceedings{pmlr-v143-abbet21a,
 abstract = {Supervised learning is conditioned by the availability of labeled data, which are especially expensive to acquire in the field of medical image analysis. Making use of open-source data for pre-training or using domain adaptation can be a way to overcome this issue. However, pre-trained networks often fail to generalize to new test domains that are not distributed identically due to variations in tissue stainings, types, and textures. Additionally, current domain adaptation methods mainly rely on fully-labeled source datasets. In this work, we propose Self-Rule to Adapt (SRA) which takes advantage of self-supervised learning to perform domain adaptation and removes the burden of fully-labeled source datasets. SRA can effectively transfer the discriminative knowledge obtained from a few labeled source domain to a new target domain without requiring additional tissue annotations. Our method harnesses both domainsâ structures by capturing visual similarity with intra-domain and cross-domain self-supervision. We show that our proposed method outperforms baselines across diverse domain adaptation settings and further validate our approach to our in-house clinical cohort.},
 author = {Abbet, Christian and Studer, Linda and Fischer, Andreas and Dawson, Heather and Zlobec, Inti and Bozorgtabar, Behzad and Thiran, Jean-Philippe},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3155242851},
 pages = {5--21},
 pdf = {https://proceedings.mlr.press/v143/abbet21a/abbet21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Self-Rule to Adapt: Learning Generalized Features from Sparsely-Labeled Data Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Phenotyping},
 url = {https://proceedings.mlr.press/v143/abbet21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-amador21a,
 abstract = {Acute ischemic stroke is caused by a blockage in the cerebral arteries, resulting in long-term disability and sometimes death. To determine the optimal treatment strategy, a patient-specific assessment is often based on advanced neuroimaging data, such as spatio-temporal (4D) CT Perfusion (CTP) imaging. To date, perfusion maps are typically calculated from 4D CTP data and then thresholded to localize and quantify the stroke lesion core and tissue-at-risk. A few studies have recently developed deep learning methods to predict stroke lesion outcomes from perfusion maps. The basic idea of these is to train a model, using perfusion maps acquired at baseline and their corresponding follow-up images acquired several days after treatment, to automatically estimate the final lesion location and volume in new patients. Nevertheless, model training based on the original 4D CTP scans might be desirable, as they could contain more valuable information not directly represented in perfusion maps. Therefore, we aimed to develop and evaluate a temporal convolutional neural network (TCN) to predict stroke lesion outcomes directly from 4D CTP datasets acquired at admission, without computing any perfusion maps. Using a total of 176 CTP scans, we investigated the impact of the time window size by training the proposed TCN on various numbers of CTP frames: 8, 16, and 32 time points. For comparison purposes, we also trained a convolutional neural network based on perfusion maps. The results show that the model trained on 32 time points yielded significantly higher Dice values (0.33$\pm$0.21) than the models trained on 8 time points (0.25$\pm$0.20; P$<$0.05), 16 time points (0.28$\pm$0.21; P$<$0.001), and perfusion maps (0.23$\pm$0.18; P$<$0.05). These experiments demonstrate that the proposed model effectively extracts spatio-temporal data from CTP scans to predict stroke lesion outcomes, which leads to better results than using perfusion maps.},
 author = {Amador, Kimberly and Wilms, Matthias and Winder, Anthony and Fiehler, Jens and Forkert, Nils},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3159942920},
 pages = {22--33},
 pdf = {https://proceedings.mlr.press/v143/amador21a/amador21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Stroke Lesion Outcome Prediction Based on 4D CT Perfusion Data Using Temporal Convolutional Networks},
 url = {https://proceedings.mlr.press/v143/amador21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-bargsten21a,
 abstract = {Using intracoronary imaging modalities like intravascular ultrasound (IVUS) has a positive impact on the results of percutaneous coronary interventions. Efficient extraction of important vessel metrics like lumen diameter, vessel wall thickness or plaque burden via automatic segmentation of IVUS images can improve the clinical workflow. State-of-the-art segmentation results are usually achieved by data-driven methods like convolutional neural networks (CNNs). However, clinical data sets are often rather small leading to extraction of image features which are not very meaningful and thus decreasing performance. This is also the case for some applications which inherently allow for only small amounts of available data, e.g., detection of diseases with extremely small prevalence or online-adaptation of an existing algorithm to individual patients. In this work we investigate how integrating scattering transformations - as special forms of wavelet transformations - into CNNs could improve the extraction of meaningful features. To this end, we developed a novel network module which uses features of a scattering transform for an attention mechanism. We observed that this approach improves the results of calcium segmentation up to 8.2% (relatively) in terms of the Dice coefficient and 24.8% in terms of the modified Hausdorff distance. In the case of lumen and vessel wall segmentation, the improvements are up to 2.3% (relatively) in terms of the Dice coefficient and 30.8% in terms of the modified Hausdorff distance.Incorporating scattering transformations as a component of an attention block into CNNs improves the segmentation results on small IVUS segmentation data sets. In general, scattering transformations can help in situations where efficient feature extractors can not be learned via the training data. This makes our attention module an interesting candidate for applications like few-shot learning for patient adaptation or detection of rare diseases.},
 author = {Bargsten, Lennart and Riedl, Katharina A. and Wissel, Tobias and Brunner, Fabian J. and Schaefers, Klaus and Grass, Michael and Blankenberg, Stefan and Seiffert, Moritz and Schlaefer, Alexander},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3202560867},
 pages = {34--47},
 pdf = {https://proceedings.mlr.press/v143/bargsten21a/bargsten21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Attention via Scattering Transforms for Segmentation of Small Intravascular Ultrasound Data Sets},
 url = {https://proceedings.mlr.press/v143/bargsten21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-brudfors21a,
 abstract = {While convolutional neural networks (CNNs) trained by back-propagation have seen unprecedented success at semantic segmentation tasks, they are known to struggle on out-of-distribution data. Markov random fields (MRFs) on the other hand, encode simpler distributions over labels that, although less flexible than UNets, are less prone to over-fitting. In this paper, we propose to fuse both strategies by computing the product of distributions of a UNet and an MRF. As this product is intractable, we solve for an approximate distribution using an iterative mean-field approach. The resulting MRF-UNet is trained jointly by back-propagation. Compared to other works using conditional random fields (CRFs), the MRF has no dependency on the imaging data, which should allow for less over-fitting. We show on 3D neuroimaging data that this novel network improves generalisation to out-of-distribution samples. Furthermore, it allows the overall number of parameters to be reduced while preserving high accuracy. These results suggest that a classic MRF smoothness prior can allow for less over-fitting when principally integrated into a CNN model. Our implementation is available at https://github.com/balbasty/nitorch.},
 author = {Brudfors, Mikael and Balbastre, Ya{\"e}l and Ashburner, John and Rees, Geraint and Nachev, Parashkev and Ourselin, Sebastien and Cardoso, M. Jorge},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 pages = {48--59},
 pdf = {https://proceedings.mlr.press/v143/brudfors21a/brudfors21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An {MRF}-{UN}et Product of Experts for Image Segmentation},
 url = {https://proceedings.mlr.press/v143/brudfors21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-chu21a,
 abstract = {We introduce the concept of multi-task learning to weakly-supervised lesion segmentation, one of the most critical and challenging tasks in medical imaging. Due to the lesionsâ heterogeneous nature, it is difficult for machine learning models to capture the corresponding variability. We propose to jointly train a lesion segmentation model and a lesion classifier in a multi-task learning fashion, where the supervision of the latter is obtained by clustering the RECIST measurements of the lesions. We evaluate our approach specifically on liver lesion segmentation and more generally on lesion segmentation in computed tomography (CT), as well as segmentation of skin lesions from dermatoscopic images. We show that the proposed joint training improves the quality of the lesion segmentation by 4% percent according to the Dice coefficient and 6% according to averaged Hausdorff distance (AVD), while reducing the training time required by up to 75%.},
 author = {Chu, Tianshu and Li, Xinmeng and Vo, Huy V. and Summers, Ronald M. and Sizikova, Elena},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3144012894},
 pages = {60--73},
 pdf = {https://proceedings.mlr.press/v143/chu21a/chu21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improving Weakly Supervised Lesion Segmentation using Multi-Task Learning},
 url = {https://proceedings.mlr.press/v143/chu21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-cohen21a,
 abstract = {Motivation: Traditional image attribution methods struggle to satisfactorily explain predictions of neural networks. Prediction explanation is important, especially in medical imaging, for avoiding the unintended consequences of deploying AI systems when false positive predictions can impact patient care. Thus, there is a pressing need to develop improved models for model explainability and introspection. Specific problem: A new approach is to transform input images to increase or decrease features which cause the prediction. However, current approaches are difficult to implement as they are monolithic or rely on GANs. These hurdles prevent wide adoption. Our approach: Given an arbitrary classifier, we propose a simple autoencoder and gradient update (Latent Shift) that can transform the latent representation of a specific input image to exaggerate or curtail the features used for prediction. We use this method to study chest X-ray classifiers and evaluate their performance. We conduct a reader study with two radiologists assessing 240 chest X-ray predictions to identify which ones are false positives (half are) using traditional attribution maps or our proposed method. Results: We found low overlap with ground truth pathology masks for models with reasonably high accuracy. However, the results from our reader study indicate that these models are generally looking at the correct features. We also found that the Latent Shift explanation allows a user to have more confidence in true positive predictions compared to traditional approaches (0.15$\pm$0.95 in a 5 point scale with p=0.01) with only a small increase in false positive predictions (0.04$\pm$1.06 with p=0.57). Accompanying webpage: https://mlmed.org/gifsplanation Source code: https://github.com/mlmed/gifsplanation},
 author = {Cohen, Joseph Paul and Brooks, Rupert and En, Sovann and Zucker, Evan and Pareek, Anuj and Lungren, Matthew P. and Chaudhari, Akshay},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3159250167},
 pages = {74--104},
 pdf = {https://proceedings.mlr.press/v143/cohen21a/cohen21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Gifsplanation via Latent Shift: A Simple Autoencoder Approach to Counterfactual Generation for Chest X-rays},
 url = {https://proceedings.mlr.press/v143/cohen21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-czolbe21a,
 abstract = {We propose a semantic similarity metric for image registration. Existing metrics like Euclidean Distance or Normalized Cross-Correlation focus on aligning intensity values, giving difficulties with low intensity contrast or noise. Our approach learns dataset-specific features that drive the optimization of a learning-based registration model. We train both an unsupervised approach using an auto-encoder, and a semi-supervised approach using supplemental segmentation data to extract semantic features for image registration. Comparing to existing methods across multiple image modalities and applications, we achieve consistently high registration accuracy. A learned invariance to noise gives smoother transformations on low-quality images.},
 author = {Czolbe, Steffen and Krause, Oswin and Feragen, Aasa},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W4287204777},
 pages = {105--118},
 pdf = {https://proceedings.mlr.press/v143/czolbe21a/czolbe21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Semantic similarity metrics for learned image registration},
 url = {https://proceedings.mlr.press/v143/czolbe21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-dsouza21a,
 abstract = {We propose a multimodal graph convolutional network (M-GCN) that integrates resting-state fMRI connectivity and diffusion tensor imaging tractography to predict phenotypic measures. Our specialized M-GCN filters act topologically on the functional connectivity matrices, as guided by the subject-wise structural connectomes. The inclusion of structural information also acts as a regularizer and helps extract rich data embeddings that are predictive of clinical outcomes. We validate our framework on 275 healthy individuals from the Human Connectome Project and 57 individuals diagnosed with Autism Spectrum Disorder from an in-house data to predict cognitive measures and behavioral deficits respectively. We demonstrate that the M-GCN outperforms several state-of-the-art baselines in a five-fold cross validated setting and extracts predictive biomarkers from both healthy and autistic populations. Our framework thus provides the representational flexibility to exploit the complementary nature of structure and function and map this information to phenotypic measures in the presence of limited training data.},
 author = {Dsouza, Niharika Shimona and Nebel, Mary Beth and Crocetti, Deana and Robinson, Joshua and Mostofsky, Stewart and Venkataraman, Archana},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3178791461},
 pages = {119--130},
 pdf = {https://proceedings.mlr.press/v143/dsouza21a/dsouza21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {M-GCN: A Multimodal Graph Convolutional Network to Integrate Functional and Structural Connectomics Data to Predict Multidimensional Phenotypic Characterizations},
 url = {https://proceedings.mlr.press/v143/dsouza21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-dunnhofer21a,
 abstract = {This paper presents MRPyrNet, a new convolutional neural network (CNN) architecture that improves the capabilities of CNN-based pipelines for knee injury detection via magnetic resonance imaging (MRI). Existing works showed that anomalies are localized in small-sized knee regions that appear in particular areas of MRI scans. Based on such facts, MRPyrNet exploits a Feature Pyramid Network to enhance small appearing features and Pyramidal Detail Pooling to capture such relevant information in a robust way. Experimental results on two publicly available datasets demonstrate that MRPyrNet improves the ACL tear and meniscal tear diagnosis capabilities of two state-of-the-art methodologies. Code is available at https://git.io/JtMPH.},
 author = {Dunnhofer, Matteo and Martinel, Niki and Micheloni, Christian},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3140571173},
 pages = {131--147},
 pdf = {https://proceedings.mlr.press/v143/dunnhofer21a/dunnhofer21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improving MRI-based Knee Disorder Diagnosis with Pyramidal Feature Details},
 url = {https://proceedings.mlr.press/v143/dunnhofer21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-dzyubachyk21a,
 abstract = {Intensity of acquired electron microscopy data is subjected to large variability due to the interplay of many different factors, such as microscope and camera settings used for data acquisition, sample thickness, specimen staining protocol and more. In this work, we developed an efficient method for performing intensity inhomogeneity correction on a single set of combined transmission electron microscopy (TEM) images and demonstrated its positive impact on training a neural network on these data. In addition, we investigated what impact different intensity standardization methods have on the training performance, both for data originating from a single source as well as from several different sources. As a concrete example, we considered the problem of segmenting mitochondria from EM data and demonstrated that we were able to obtain promising results when training our network on a large array of highly-variable in-house TEM data.},
 author = {Dzyubachyk, Oleh and Koning, Roman I and Mulder, Aat A and Avramut, M. Christina and Faas, Frank GA and Koster, Abraham J},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3202215207},
 pages = {148--157},
 pdf = {https://proceedings.mlr.press/v143/dzyubachyk21a/dzyubachyk21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Intensity Correction and Standardization for Electron Microscopy Data},
 url = {https://proceedings.mlr.press/v143/dzyubachyk21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-el-jurdi21a,
 abstract = {Deep convolutional networks recently made many breakthroughs in medical image segmentation. Still, some anatomical artefacts may be observed in the segmentation results, with holes or inaccuracies near the object boundaries. To address these issues, loss functions that incorporate constraints, such as spatial information or prior knowledge, have been introduced. An example of such prior losses are the contour-based losses, which exploit distance maps to conduct point-by-point optimization between ground-truth and predicted contours. However, such losses may be computationally expensive or susceptible to trivial local solutions and vanishing gradient problems. Moreover, they depend on distance maps which tend to underestimate the contour-to-contour distances. We propose a novel loss constraint that optimizes the perimeter length of the segmented object relative to the ground-truth segmentation. The novelty lies in computing the perimeter with a soft approximation of the contour of the probability map via specialized non-trainable layers in the network. Moreover, we optimize the mean squared error between the predicted perimeter length and ground-truth perimeter length. This soft optimization of contour boundaries allows the network to take into consideration border irregularities within organs while still being efficient. Our experiments on three public datasets (spleen, hippocampus and cardiac structures) show that the proposed method outperforms state-of-the-art boundary losses for both single and multi-organ segmentation.},
 author = {{EL Jurdi}, Rosana and Petitjean, Caroline and Honeine, Paul and Cheplygina, Veronika and Abdallah, Fahed},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3139154670},
 pages = {158--167},
 pdf = {https://proceedings.mlr.press/v143/el-jurdi21a/el-jurdi21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Surprisingly Effective Perimeter-based Loss for Medical Image Segmentation},
 url = {https://proceedings.mlr.press/v143/el-jurdi21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-faryna21a,
 abstract = {Convolutional neural networks (CNN) are sensitive to domain shifts, which can result in poor generalization. In medical imaging, data acquisition conditions differ among institutions, which leads to variations in image properties and thus domain shift. Stain variation in histopathological slides is a prominent example. Data augmentation is one way to make CNNs robust to varying forms of domain shift, but requires extensive hyperparameter tuning. Due to the large search space, this is cumbersome and often leads to sub-optimal generalization performance. In this work, we focus on automated and computationally efficient data augmentation policy selection for histopathological slides. Building upon the RandAugment framework, we introduce several domain-specific modifications relevant to histopathological images, increasing generalizability. We test these modifications on H&E-stained histopathology slides from Camelyon17 dataset. Our proposed framework outperforms the state-of-the-art manually engineered data augmentation strategy, achieving an area under the ROC curve of 0.964 compared to 0.958, respectively.},
 author = {Faryna, Khrystyna and van der Laak, Jeroen and Litjens, Geert},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3158261327},
 pages = {168--178},
 pdf = {https://proceedings.mlr.press/v143/faryna21a/faryna21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Tailoring automated data augmentation to H&E-stained histopathology},
 url = {https://proceedings.mlr.press/v143/faryna21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-feng21a,
 abstract = {The tumor microenvironment is an area of intense interest in cancer research and may be a clinically actionable aspect of cancer care. One way to study the tumor microenvironment is to characterize the spatial interactions between various types of nuclei in cancer tissue from H&E whole slide images, which require nucleus segmentation and classification. Current methods of nucleus classification rely on extensive labeling from pathologists and are limited by the number of categories a nucleus can be classified into. In this work, leveraging existing nucleus segmentation and contrastive representation learning methods, we developed a method that learns vector embeddings of nuclei based on their morphology in histopathology images. We show that the embeddings learned by this model capture distinctive morphological features of nuclei and can be used to group them into meaningful subtypes. These embeddings can provide a much richer characterization of the statistics of the spatial distribution of nuclei in cancer and open new possibilities in the quantitative study of the tumor microenvironment.},
 author = {Feng, Chao and Vanderbilt, Chad and Fuchs, Thomas},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3172539865},
 pages = {179--189},
 pdf = {https://proceedings.mlr.press/v143/feng21a/feng21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Nuc2Vec: Learning Representations of Nuclei in Histopathology Images with Contrastive Loss},
 url = {https://proceedings.mlr.press/v143/feng21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-gadgil21a,
 abstract = {Medical image segmentation models are typically supervised by expert annotations at the pixel-level, which can be expensive to acquire. In this work, we propose a method that combines the high quality of pixel-level expert annotations with the scale of coarse DNN-generated saliency maps for training multi-label semantic segmentation models. We demonstrate the application of our semi-supervised method, which we call CheXseg, on multi-label chest X-ray interpretation. We find that CheXseg improves upon the performance (mIoU) of fully-supervised methods that use only pixel-level expert annotations by 9.7% and weakly-supervised methods that use only DNN-generated saliency maps by 73.1%. Our best method is able to match radiologist agreement on three out of ten pathologies and reduces the overall performance gap by 57.2% as compared to weakly-supervised methods.},
 author = {Gadgil, Soham Uday and Endo, Mark and Wen, Emily and Ng, Andrew Y. and Rajpurkar, Pranav},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3132126007},
 pages = {190--204},
 pdf = {https://proceedings.mlr.press/v143/gadgil21a/gadgil21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {CheXseg: Combining Expert Annotations with DNN-generated Saliency Maps for X-ray Segmentation},
 url = {https://proceedings.mlr.press/v143/gadgil21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-gonzalez21a,
 abstract = {The segmentation of cardiac structures in Cine Magnetic Resonance imaging (CMR) plays an important role in monitoring ventricular function, and many deep learning solutions have been introduced that successfully automate this task. Yet due to variabilities in the CMR acquisition process, images from different centers or acquisition protocols differ considerably. This causes deep learning models to fail silently. It is therefore crucial to identify out-of-distribution (OOD) samples for which the trained model is unsuitable. For models with a self-supervised proxy task, we propose a simple method to identify OOD samples that does not require adapting the model architecture or access to a separate OOD dataset during training. As the performance of self-supervised tasks can be assessed without ground truth information, it indicates during test time when a sample differs from the training distribution. The proposed method combines a voxel-wise uncertainty estimate with the self-supervision information. Our approach is validated across three CMR datasets and two different proxy tasks. We find that it is more effective at detecting OOD samples than state-of-the-art post-hoc OOD detection and uncertainty estimation approaches.},
 author = {Gonzalez, Camila and Mukhopadhyay, Anirban},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3154079005},
 pages = {205--218},
 pdf = {https://proceedings.mlr.press/v143/gonzalez21a/gonzalez21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Self-supervised Out-of-distribution Detection for Cardiac CMR Segmentation},
 url = {https://proceedings.mlr.press/v143/gonzalez21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-gruening21a,
 abstract = {With in-line holography, it is possible to record biological cells over time in a three-dimensional hydrogel without the need for staining, providing the capability of observing cell behavior in a minimally invasive manner. However, this setup currently requires computationally intensive image-reconstruction algorithms to determine the required cell statistics. In this work, we directly extract cell positions from the holographic data by using deep neural networks and thus avoid several reconstruction steps. We show that our method is capable of substantially decreasing the time needed to extract information from the raw data without loss in quality.},
 author = {Gruening, Philipp and Nette, Falk and Heldt, Noah and de Souza, Ana Cristina Guerra and Barth, Erhardt},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3198751139},
 pages = {219--227},
 pdf = {https://proceedings.mlr.press/v143/gruening21a/gruening21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Direct Inference of Cell Positions using Lens-Free Microscopy and Deep Learning.},
 url = {https://proceedings.mlr.press/v143/gruening21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-gupta21a,
 abstract = {Ensuring the privacy of research participants is vital, even more so in healthcare environments. Deep learning approaches to neuroimaging require large datasets, and this often necessitates sharing data between multiple sites, which is antithetical to the privacy objectives. Federated learning is a commonly proposed solution to this problem. It circumvents the need for data sharing by sharing parameters during the training process. However, we demonstrate that allowing access to parameters may leak private information even if data is never directly shared. In particular, we show that it is possible to infer if a sample was used to train the model given only access to the model prediction (black-box) or access to the model itself (white-box) and some leaked samples from the training data distribution. Such attacks are commonly referred to as Membership Inference attacks. We show realistic Membership Inference attacks on deep learning models trained for 3D neuroimaging tasks in a centralized as well as decentralized setup. We demonstrate feasible attacks on brain age prediction models (deep learning models that predict a person's age from their brain MRI scan). We correctly identified whether an MRI scan was used in model training with a 60% to over 80% success rate depending on model complexity and security assumptions.},
 author = {Gupta, Umang and Stripelis, Dimitris and Lam, Pradeep K. and Thompson, Paul and Ambite, Jose Luis and Steeg, Greg Ver},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W4287184471},
 pages = {228--251},
 pdf = {https://proceedings.mlr.press/v143/gupta21a/gupta21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Membership Inference Attacks on Deep Regression Models for Neuroimaging},
 url = {https://proceedings.mlr.press/v143/gupta21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-hagenah21a,
 author = {Hagenah, Jannis and Ernst, Floris},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3158382153},
 pages = {252--267},
 pdf = {https://proceedings.mlr.press/v143/hagenah21a/hagenah21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Discrete Pseudohealthy Synthesis: Aortic Root Shape Typification and Type Classification with Pathological Prior},
 url = {https://proceedings.mlr.press/v143/hagenah21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-harten21a,
 abstract = {Motility of the small intestine is a valuable metric in the evaluation of gastrointestinal disorders. Cine-MRI of the abdomen is a non-invasive imaging technique allowing evaluation of this motility. While 2D cine-MR imaging is increasingly used for this purpose in both clinical practice and in research settings, the potential of 3D cine-MR imaging has been largely underexplored. In the absence of image analysis tools enabling investigation of the intestines as 3D structures, the assessment of motility in 3D cine-images is generally limited to the evaluation of movement in separate 2D slices. Hence, to obtain an untangled representation of the small intestine in 3D cine-MRI, we propose a method to extract a centerline of the intestine, thereby allowing easier (visual) assessment by human observers, as well as providing a possible starting point for automatic analysis methods quantifying peristaltic bowel movement along intestinal segments. The proposed method automatically tracks individual sections of the small intestine in 3D space, using a stochastic tracker built on top of a CNN-based orientation classifier. We show that the proposed method outperforms a non-stochastic iterative tracking approach.},
 author = {van Harten, Louis and de Jonge, Catharina and Stoker, Jaap and Isgum, Ivana},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3198291803},
 pages = {802--812},
 pdf = {https://proceedings.mlr.press/v143/harten21a/harten21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Untangling the Small Intestine in 3D cine-MRI using Deep Stochastic Tracking.},
 url = {https://proceedings.mlr.press/v143/harten21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-he21a,
 abstract = {Weakly supervised segmentation is an important problem in medical image analysis due to the high cost of pixelwise annotation. Prior methods, while often focusing on weak labels of 2D images, exploit few structural cues of volumetric medical images. To address this, we propose a novel weakly-supervised segmentation strategy capable of better capturing 3D shape prior in both model prediction and learning. Our main idea is to extract a self-taught shape representation by leveraging weak labels, and then integrate this representation into segmentation prediction for shape refinement. To this end, we design a deep network consisting of a segmentation module and a shape denoising module, which are trained by an iterative learning strategy. Moreover, we introduce a weak annotation scheme with a hybrid label design for volumetric images, which improves model learning without increasing the overall annotation cost. The empirical experiments show that our approach outperforms existing SOTA strategies on three organ segmentation benchmarks with distinctive shape properties. Notably, we can achieve strong performance with even 10\% labeled slices, which is significantly superior to other methods.},
 author = {He, Qian and Li, Shuailin and He, Xuming},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3150022293},
 pages = {268--285},
 pdf = {https://proceedings.mlr.press/v143/he21a/he21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Weakly Supervised Volumetric Segmentation via Self-taught Shape Denoising Model},
 url = {https://proceedings.mlr.press/v143/he21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-heer21a,
 abstract = {Deep unsupervised generative models are regarded as a promising alternative to supervised counterparts in the field of MRI-based lesion detection. They denote a principled approach for detecting unseen types of anomalies without relying on large amounts of expensive ground truth annotations. To this end, deep generative models are trained exclusively on data from healthy patients and detect lesions as Out-of-Distribution (OOD) data at test time (i.e. low likelihood). While this is a promising way of bypassing the need for costly annotations, this work demonstrates that it also renders this widely used unsupervised anomaly detection approach particularly vulnerable to non-lesion-based OOD data (e.g. data from different sensors). Since models are likely to be exposed to such OOD data in production, it is crucial to employ safety mechanisms to filter for such samples and run inference only on input for which the model is able to provide reliable results. We first show extensively that conventional, unsupervised anomaly detection mechanisms fail when being presented with true OOD data. Secondly, we apply prior knowledge to disentangle lesion-based OOD from their non-lesion-based counterparts.},
 author = {Heer, Matth{\"a}us and Postels, Janis and Chen, Xiaoran and Konukoglu, Ender and Albarqouni, Shadi},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3152679304},
 pages = {286--300},
 pdf = {https://proceedings.mlr.press/v143/heer21a/heer21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The OOD Blind Spot of Unsupervised Anomaly Detection.},
 url = {https://proceedings.mlr.press/v143/heer21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-heinrich21a,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 author = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and Schl{\"a}fer, Alexander and Ernst, Floris},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W4240465921},
 pages = {1--4},
 pdf = {https://proceedings.mlr.press/v143/heinrich21a/heinrich21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v143/heinrich21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-hemati21a,
 abstract = {Digital pathology has enabled us to capture, store and analyze scanned biopsy samples as digital images. Recent advances in deep learning are contributing to computational pathology to improve diagnosis and treatment. However, considering challenges inherent to whole slide images (WSIs), it is not easy to employ deep learning in digital pathology. More importantly, computational bottlenecks induced by the gigapixel WSIs make it difficult to use deep learning for end-to-end image representation. To mitigate this challenge, many patch-based approaches have been proposed. Although patching WSIs enables us to use deep learning, we end up with a bag of patches or set representation which makes downstream tasks non-trivial. More importantly, considering set representation per WSI, it is not clear how one can obtain similarity between two WSIs (sets) for tasks like image search matching. To address this challenge, we propose a neural network based on Convolutions Neural Network (CNN) and Deep Sets to learn one permutation invariant vector representation per WSI in an end-to-end manner. Considering available labels at the WSI level - namely, primary site and cancer subtypes - we train the proposed network in a multi-label setting to encode both primary site and diagnosis. Having in mind that every primary site has its own specific cancer subtypes, we propose to use the predicted label for the primary site to recognize the cancer subtype. The proposed architecture is used for transfer learning of WSIs and validated two different tasks, i.e., search and classification. The results show that the proposed architecture can be used to obtain WSI representations that achieve better performance both in terms of retrieval performance and search time against \emph{Yottixel}, a recently developed search engine for pathology images. Further, the model achieved competitive performance against the state-of-art in lung cancer classification.},
 author = {Hemati, Sobhan and Kalra, Shivam and Meaney, Cameron and Babaie, Morteza and Ghodsi, Ali and Tizhoosh, Hamid},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3179005715},
 pages = {301--311},
 pdf = {https://proceedings.mlr.press/v143/hemati21a/hemati21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {CNN and Deep Sets for End-to-End Whole Slide Image Representation Learning},
 url = {https://proceedings.mlr.press/v143/hemati21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-hering21a,
 abstract = {In follow-up CT examinations of cancer patients, therapy success is evaluated by estimating the change in tumor size. This process is time-consuming and error-prone. We present a pipeline that automates the segmentation and measurement of matching lesions, given a point annotation in the baseline lesion. First, a region around the point annotation is extracted, in which a deep-learning-based segmentation of the lesion is performed. Afterward, a registration algorithm finds the corresponding image region in the follow-up scan and the convolutional neural network segments lesions inside this region. In the final step, the corresponding lesion is selected. We evaluate our pipeline on clinical follow-up data comprising 125 soft-tissue lesions from 43 patients with metastatic melanoma. Our pipeline succeeded for $96%$ of the baseline and $80%$ of the follow-up lesions, showing that we have laid the foundation for an efficient quantitative follow-up assessment in clinical routine.},
 author = {Hering, Alessa and Peisen, Felix and Amaral, Teresa and Gatidis, Sergios and Eigentler, Thomas and Othman, Ahmed and Moltz, Jan Hendrik},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3158810081},
 pages = {312--326},
 pdf = {https://proceedings.mlr.press/v143/hering21a/hering21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Whole-Body Soft-Tissue Lesion Tracking and Segmentation in Longitudinal CT Imaging Studies},
 url = {https://proceedings.mlr.press/v143/hering21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-hu21a,
 abstract = {Artifacts, blur, and noise are the common distortions degrading MRI images during the acquisition process, and deep neural networks have been demonstrated to help in improving image quality. To well exploit global structural information and self-similarity details, we propose a novel MR image enhancement network, named Feedback Graph Attention Convolutional Network (FB-GACN). As a key innovation, we consider the global structure of an image by building a graph network from image sub-regions that we consider to be node features, linking them non-locally according to their similarity. The proposed model consists of three main parts: 1) The parallel graph similarity branch and content branch, where the graph similarity branch aims at exploiting the similarity and symmetry across different image sub-regions in low-resolution feature space and provides additional priors for the content branch to enhance texture details. 2) A feedback mechanism with a recurrent structure to refine low-level representations with high-level information and generate powerful high-level texture details by handling the feedback connections.  3) A reconstruction to remove the artifacts and recover super-resolution images by using the estimated sub-region self-similarity priors obtained from the graph similarity branch. We evaluate our method on two image enhancement tasks: i) cross-protocol super resolution of diffusion MRI; ii) artifact removal of FLAIR MR images. Experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art methods.},
 author = {Hu, Xiaobin and Yan, Yanyang and Ren, Wenqi and Li, Hongwei and Bayat, Amirhossein and Zhao, Yu and Menze, Bjoern},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3165281680},
 pages = {327--337},
 pdf = {https://proceedings.mlr.press/v143/hu21a/hu21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feedback Graph Attention Convolutional Network for MR Images Enhancement by Exploring Self-Similarity Features},
 url = {https://proceedings.mlr.press/v143/hu21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-kanavati21a,
 abstract = {Transfer learning from ImageNet is the go-to approach when applying deep learning to medical images. The approach is either to fine-tune a pre-trained model or use it as a feature extractor. Most modern architecture contain batch normalisation layers, and fine-tuning a model with such layers requires taking a few precautions as they consist of trainable and non-trainable weights and have two operating modes: training and inference. Attention is primarily given to the non-trainable weights used during inference, as they are the primary source of unexpected behaviour or degradation in performance during transfer learning. It is typically recommended to fine-tune the model with the batch normalisation layers kept in inference mode during both training and inference. In this paper, we pay closer attention instead to the trainable weights of the batch normalisation layers, and we explore their expressive influence in the context of transfer learning. We find that only fine-tuning the trainable weights (scale and centre) of the batch normalisation layers leads to similar performance as to fine-tuning all of the weights, with the added benefit of faster convergence. We demonstrate this on a variety of seven publicly available medical imaging datasets, using four different model architectures.},
 author = {Kanavati, Fahdi and Tsuneki, Masayuki},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3126714798},
 pages = {338--353},
 pdf = {https://proceedings.mlr.press/v143/kanavati21a/kanavati21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning},
 url = {https://proceedings.mlr.press/v143/kanavati21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-kervadec21a,
 abstract = {Standard losses for training deep segmentation networks could be seen as individual classifications of pixels, instead of supervising the global shape of the predicted segmentations. While effective, they require exact knowledge of the label of each pixel in an image. 
This study investigates how effective global geometric shape descriptors could be, when used on their own as segmentation losses for training deep networks. Not only interesting theoretically, there exist deeper motivations to posing segmentation problems as a reconstruction of shape descriptors: Annotations to obtain approximations of low-order shape moments could be much less cumbersome than their full-mask counterparts, and anatomical priors could be readily encoded into invariant shape descriptions, which might alleviate the annotation burden. Also, and most importantly, we hypothesize that, given a task, certain shape descriptions might be invariant across image acquisition protocols/modalities and subject populations, which might open interesting research avenues for generalization in medical image segmentation. 
We introduce and formulate a few shape descriptors in the context of deep segmentation, and evaluate their potential as standalone losses on two different challenging tasks. Inspired by recent works in constrained optimization for deep networks, we propose a way to use those descriptors to supervise segmentation, without any pixel-level label. Very surprisingly, as little as 4 descriptors values per class can approach the performance of a segmentation mask with 65k individual discrete labels. We also found that shape descriptors can be a valid way to encode anatomical priors about the task, enabling to leverage expert knowledge without additional annotations. Our implementation is publicly available and can be easily extended to other tasks and descriptors: this https URL},
 author = {Kervadec, Hoel and Bahig, Houda and Letourneau-Guillon, Laurent and Dolz, Jose and Ayed, Ismail Ben},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3203663820},
 pages = {354--368},
 pdf = {https://proceedings.mlr.press/v143/kervadec21a/kervadec21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Beyond pixel-wise supervision for segmentation: A few global shape descriptors might be surprisingly good!},
 url = {https://proceedings.mlr.press/v143/kervadec21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-kist21a,
 abstract = {Images offer a two-dimensional (2D) representation of a three-dimensional (3D) environment. However, in many biomedical tasks, a 3D view is crucial for diagnosis. Projecting structured light, such as a regular laser grid, onto the surface of interest allows to reconstruct its 3D structure. For reconstruction, it is crucial to correctly identify and assign each laser ray to its respective position in the laser grid. Current methods for this task use semi-automatic, yet highly manual annotations. Hence, a fully automatic, reliable method is desired. Here, we show that this assignment can be approached as an image registration. We first separate the laser rays from the background using semantic segmentation. We found that registration of the extracted laser rays directly to the fixed laser grid image fails, when we use state-of-the-art intensity-based image registration techniques, such as ANTs. Using our feature-based custom loss and a deep neural network, we are able to use a U-Net-like architecture to compute deformation fields to successfully register the laser rays onto the fixed image accompanied with a custom post-processing sorting step. Using synthetic data, we show that the network is in general able to learn affine and non-linear transformations. Our method is also robust to missing or occluded rays. Using an ex vivo dataset, we achieved an registration accuracy of 91%. In summary, we provide a new platform to perform feature-based registration and showcase this on a biomedical dataset. In future, we will evaluate different architectural designs and more complex datasets.},
 author = {Kist, Andreas M and Zilker, Julian and D{\"o}llinger, Michael and Semmler, Marion},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3135632318},
 pages = {369--383},
 pdf = {https://proceedings.mlr.press/v143/kist21a/kist21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feature-based image registration in structured light endoscopy},
 url = {https://proceedings.mlr.press/v143/kist21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-konwer21a,
 abstract = {Automated analyses of chest imaging in Coronavirus Disease 2019 (COVID-19) have largely focused on a single timepoint, usually at disease presentation, and have not explicitly taken into account temporal disease manifestations. We present a deep learning-based approach for prediction of imaging progression from serial chest radiographs (CXRs) of COVID-19 patients. Our method first utilizes convolutional neural networks (CNNs) for feature extraction from patches within the concerned lung zone, and also from neighboring areas to enhance the contextual phenotypic information. The framework further incorporates two distinct spatio-temporal Long Short Term Memory (LSTM) modules for effective predictions. The first LSTM module captures spatial dependencies between patches and the second exploits the temporal context of sequential CXR scans. The resulting network focuses on critical image regions that provide relevant information for learning the progression of lung infiltrates without the explicit need for infiltrate segmentation. The second LSTM provides an encoded context vector used as an input to a decoder module to predict future severity grades. Our novel multi-institutional dataset comprises sequential CXR scans from N=100 patients. Specifically, our framework predicts zone-wise disease severity for a patient on the last day by learning representations from the previous temporal CXRs. We design two baseline approaches - one using fine-tuned VGG-16 features and the other using radiomic descriptors. Experimental results demonstrate that our proposed approach outperforms both baselines in average accuracy by 10.33% and 12.16%, respectively, in predicting COVID-19 progression severity.},
 author = {Konwer, Aishik and Bae, Joseph and Singh, Gagandeep and Gattu, Rishabh and Ali, Syed and Green, Jeremy and Phatak, Tej and Gupta, Amit and Chen, Chao and Saltz, Joel and Prasanna, Prateek},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3164012719},
 pages = {384--398},
 pdf = {https://proceedings.mlr.press/v143/konwer21a/konwer21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Predicting COVID-19 Lung Infiltrate Progression on Chest Radiographs Using Spatio-temporal LSTM based Encoder-Decoder Network},
 url = {https://proceedings.mlr.press/v143/konwer21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-lalit21a,
 abstract = {Automatic detection and segmentation of biological objects in 2D and 3D image data is central for countless biomedical research questions to be answered. While many existing computational methods are used to reduce manual labeling time, there is still a huge demand for further quality improvements of automated solutions. In the natural image domain, spatial embedding-based instance segmentation methods are known to yield high-quality results, but their utility to biomedical data is largely unexplored. Here we introduce EmbedSeg, an embedding-based instance segmentation method designed to segment instances of desired objects visible in 2D or 3D biomedical image data. We apply our method to four 2D and seven 3D benchmark datasets, showing that we either match or outperform existing state-of-the-art methods. While the 2D datasets and three of the 3D datasets are well known, we have created the required training data for four new 3D datasets, which we make publicly available online. Next to performance, also usability is important for a method to be useful. Hence, EmbedSeg is fully open source (https://github.com/juglab/EmbedSeg), offering (i) tutorial notebooks to train EmbedSeg models and use them to segment object instances in new data, and (ii) a napari plugin that can also be used for training and segmentation without requiring any programming experience. We believe that this renders EmbedSeg accessible to virtually everyone who requires high-quality instance segmentations in 2D or 3D biomedical image data.},
 author = {Lalit, Manan and Tomancak, Pavel and Jug, Florian},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W4283793798},
 pages = {399--415},
 pdf = {https://proceedings.mlr.press/v143/lalit21a/lalit21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {EmbedSeg: Embedding-based Instance Segmentation for Biomedical Microscopy Data},
 url = {https://proceedings.mlr.press/v143/lalit21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-lemay21a,
 abstract = {Medical images are often accompanied by metadata describing the image (vendor, acquisition parameters) and the patient (disease type or severity, demographics, genomics). This metadata is usually disregarded by image segmentation methods. In this work, we adapt a linear conditioning method called FiLM (Feature-wise Linear Modulation) for image segmentation tasks. This FiLM adaptation enables integrating metadata into segmentation models for better performance. We observed an average Dice score increase of 5.1% on spinal cord tumor segmentation when incorporating the tumor type with FiLM. The metadata modulates the segmentation process through low-cost affine transformations applied on feature maps which can be included in any neural network's architecture. Additionally, we assess the relevance of segmentation FiLM layers for tackling common challenges in medical imaging: training with limited or unbalanced number of annotated data, multi-class training with missing segmentations, and model adaptation to multiple tasks. Our results demonstrated the following benefits of FiLM for segmentation: FiLMed U-Net was robust to missing labels and reached higher Dice scores with few labels (up to 16.7%) compared to single-task U-Net. The code is open-source and available at this http URL.},
 author = {Lemay, Andreanne and Gros, Charley and Vincent, Olivier and Liu, Yaou and Cohen, Joseph Paul and Cohen-Adad, Julien},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3157173699},
 pages = {416--430},
 pdf = {https://proceedings.mlr.press/v143/lemay21a/lemay21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Benefits of Linear Conditioning for Segmentation using Metadata},
 url = {https://proceedings.mlr.press/v143/lemay21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-liu21a,
 author = {Liu, Juan},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3200657416},
 pages = {431--450},
 pdf = {https://proceedings.mlr.press/v143/liu21a/liu21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improved model-based deep learning for quantitative susceptibility mapping},
 url = {https://proceedings.mlr.press/v143/liu21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-liu21b,
 abstract = {In the last few years, deep learning classifiers have shown promising results in image-based medical diagnosis. However, interpreting the outputs of these models remains a challenge. In cancer diagnosis, interpretability can be achieved by localizing the region of the input image responsible for the output, i.e. the location of a lesion. Alternatively, segmentation or detection models can be trained with pixel-wise annotations indicating the locations of malignant lesions. Unfortunately, acquiring such labels is labor-intensive and requires medical expertise. To overcome this difficulty, weakly-supervised localization can be utilized. These methods allow neural network classifiers to output saliency maps highlighting the regions of the input most relevant to the classification task (e.g. malignant lesions in mammograms) using only image-level labels (e.g. whether the patient has cancer or not) during training. When applied to high-resolution images, existing methods produce low-resolution saliency maps. This is problematic in applications in which suspicious lesions are small in relation to the image size. In this work, we introduce a novel neural network architecture to perform weakly-supervised segmentation of high-resolution images. The proposed model selects regions of interest via coarse-level localization, and then performs fine-grained segmentation of those regions. We apply this model to breast cancer diagnosis with screening mammography, and validate it on a large clinically-realistic dataset. Measured by Dice similarity score, our approach outperforms existing methods by a large margin in terms of localization performance of benign and malignant lesions, relatively improving the performance by 39.6% and 20.0%, respectively. Code and the weights of some of the models are available at https://github.com/nyukat/GLAM.},
 author = {Liu, Kangning and Shen, Yiqiu and Wu, Nan and Ch{\l}{\k{e}}dowski, Jakub Piotr and Fernandez-Granda, Carlos and Geras, Krzysztof J.},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W4287102660},
 pages = {451--472},
 pdf = {https://proceedings.mlr.press/v143/liu21b/liu21b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis},
 url = {https://proceedings.mlr.press/v143/liu21b.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-maheshwari21a,
 author = {Maheshwari, Harsh and Goel, Vidit and Sethuraman, Ramanathan and Sheet, Debdoot},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3194168374},
 pages = {473--483},
 pdf = {https://proceedings.mlr.press/v143/maheshwari21a/maheshwari21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Distill DSM: Computationally efficient method for segmentation of medical imaging volumes},
 url = {https://proceedings.mlr.press/v143/maheshwari21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-mairhofer21a,
 abstract = {The quality of radiographs is of major importance for diagnosis and treatment planning. While most research regarding automated radiograph quality assessment uses technical features such as noise or contrast, we propose to use anatomical structures as more appropriate features. We show that based on such anatomical features, a modular deep-learning framework can serve as a quality control mechanism for the diagnostic quality of ankle radiographs. For evaluation, a dataset consisting of 950 ankle radiographs was collected and their quality was labeled by radiologists. We obtain an average accuracy of 94.1%, which is better than the expert radiologists are on average.},
 author = {Mairh{\"o}fer, Dominik and Laufer, Manuel and Simon, Paul Martin and Sieren, Malte and Bischof, Arpad and K{\"a}ster, Thomas and Barth, Erhardt and Barkhausen, J{\"o}rg and Martinetz, Thomas},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3201813287},
 pages = {484--496},
 pdf = {https://proceedings.mlr.press/v143/mairhofer21a/mairhofer21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {An AI-based Framework for Diagnostic Quality Assessment of Ankle Radiographs},
 url = {https://proceedings.mlr.press/v143/mairhofer21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-mouches21a,
 abstract = {Age-related morphological brain changes are known to be different in healthy and disease-affected aging. Biological brain age estimation from MRI scans is a common way to quantify this effect whereas differences between biological and chronological age indicate degenerative processes. The ability to visualize and analyze the morphological age-related changes in the image space directly is essential to improve the understanding of brain aging. In this work, we propose a novel deep learning based approach to unify biological brain age estimation and age-conditioned template creation in a single, consistent model. We achieve this by developing a deterministic autoencoder that successfully disentangles age-related morphological changes and subject-specific variations. This allows its use as a brain age regressor as well as a generative brain aging model. The proposed approach demonstrates accurate biological brain age prediction, and realistic generation of age-conditioned brain templates and simulated age-specific brain images when applied to a database of more than 2000 subjects.},
 author = {Mouches, Pauline and Wilms, Matthias and Rajashekar, Deepthi and Langner, Sonke and Forkert, Nils},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3191370428},
 pages = {497--506},
 pdf = {https://proceedings.mlr.press/v143/mouches21a/mouches21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Unifying Brain Age Prediction and Age-Conditioned Template Generation with a Deterministic Autoencoder},
 url = {https://proceedings.mlr.press/v143/mouches21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-muhamedrahimov21a,
 abstract = {In classification, categories are typically treated as independent of one-another. In many problems, however, this neglects the natural relations that exist between categories, which are often dictated by an underlying biological or physical process. In this work, we propose novel formulations of the classification problem, aimed at reintroducing class relations into the training process. We demonstrate the benefit of these approaches for the classification of intravenous contrast enhancement phase in CT images, an important task in the medical imaging domain. First, we propose manual ways reintroduce knowledge about problem-specific interclass relations into the training process. Second, we propose a general approach to jointly learn categorical label representations that can implicitly encode natural interclass relations, alleviating the need for strong prior assumptions or knowledge. We show that these improvements are most significant for smaller training sets, typical in the medical imaging domain where access to large amounts of labelled data is often not trivial.},
 author = {Muhamedrahimov, Raouf and Bar, Amir and Akselrod-Ballin, Ayelet},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3172965805},
 pages = {507--519},
 pdf = {https://proceedings.mlr.press/v143/muhamedrahimov21a/muhamedrahimov21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Interclass Relations for Intravenous Contrast Phase Classification in CT},
 url = {https://proceedings.mlr.press/v143/muhamedrahimov21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-muhammad21a,
 abstract = {Histopathology-based survival modelling has two major hurdles. Firstly, a well-performing survival model has minimal clinical application if it does not contribute to the stratification of a cancer patient cohort into different risk groups, preferably driven by histologic morphologies. In the clinical setting, individuals are not given specific prognostic predictions, but are rather predicted to lie within a risk group which has a general survival trend. Thus, It is imperative that a survival model produces well-stratified risk groups. Secondly, until now, survival modelling was done in a two-stage approach (encoding and aggregation). EPIC-Survival bridges encoding and aggregation into an end-to-end survival modelling approach, while introducing stratification boosting to encourage the model to not only optimize ranking, but also to discriminate between risk groups. In this study we show that EPIC-Survival performs better than other approaches in modelling intrahepatic cholangiocarcinoma (ICC), a historically difficult cancer to model. We found that stratification boosting further improves model performance and helps identify specific histologic differences, not commonly sought out in ICC.},
 author = {Muhammad, Hassan and Xie, Chensu and Sigel, Carlie S and Doukas, Michael and Alpert, Lindsay and Simpson, Amber Lea and Fuchs, Thomas J},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3202672547},
 pages = {520--531},
 pdf = {https://proceedings.mlr.press/v143/muhammad21a/muhammad21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {EPIC-Survival: End-to-end Part Inferred Clustering for Survival Analysis, with Prognostic Stratification Boosting},
 url = {https://proceedings.mlr.press/v143/muhammad21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-neimark21a,
 abstract = {Prior work demonstrated the ability of machine learning to automatically recognize surgical workflow steps from videos. However, these studies focused on only a single type of procedure. In this work, we analyze, for the first time, surgical step recognition on four different laparoscopic surgeries: Cholecystectomy, Right Hemicolectomy, Sleeve Gastrectomy, and Appendectomy. Inspired by the traditional apprenticeship model, in which surgical training is based on the Halstedian method, we paraphrase the âsee one, do one, teach oneâ approach for the surgical intelligence domain as âtrain one, classify one, teach oneâ. In machine learning, this approach is often referred to as transfer learning. To analyze the impact of transfer learning across different laparoscopic procedures, we explore various time-series architectures and examine their performance on each target domain. We introduce a new architecture, the Time-Series Adaptation Network (TSAN), an architecture optimized for transfer learning of surgical step recognition, and we show how TSAN can be pre-trained using self-supervised learning on a Sequence Sorting task. Such pre-training enables TSAN to learn workflow steps of a new laparoscopic procedure type from only a small number of labeled samples from the target procedure. Our proposed architecture leads to better performance compared to other possible architectures, reaching over 90% accuracy when transferring from laparoscopic Cholecystectomy to the other three procedure types.},
 author = {Neimark, Daniel and Bar, Omri and Zohar, Maya and Hager, Gregory D. and Asselmann, Dotan},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 pages = {532--544},
 pdf = {https://proceedings.mlr.press/v143/neimark21a/neimark21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {{â}Train one, Classify one, Teach one{â} - Cross-surgery transfer learning for surgical step recognition},
 url = {https://proceedings.mlr.press/v143/neimark21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-nguyen21a,
 abstract = {Deep learning in medical image analysis often requires an extensive amount of high-quality labeled data for training to achieve Human-level accuracy. We propose Gist-set Online Active Learning (GOAL), a novel solution for limited high-quality labeled data in medical imaging analysis. Our approach advances the existing active learning methods in three aspects. Firstly, we improve the classification performance with fewer manual annotations by presenting a sample selection strategy called gist set selection. Secondly, unlike traditional methods focusing only on random uncertain samples of low prediction confidence, we propose a new method in which only informative uncertain samples are selected for human annotation. Thirdly, we propose an application of online learning where high-confidence samples are automatically selected, iteratively assigned, and pseudo-labels are updated. We validated our approach on two private and one public dataset. The experimental results show that, by applying GOAL, we can reduce required labeled data up to 88% while maintaining the same F1 scores compared to the models trained on full datasets},
 author = {Nguyen, Chanh and Huynh, Minh Thanh and Tran, Minh Quan and Nguyen, Ngoc Hoang and Jain, Mudit and Ngo, Van Doan and Vo, Tan Duc and Bui, Trung and Truong, Steven Quoc Hung},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3162947543},
 pages = {545--553},
 pdf = {https://proceedings.mlr.press/v143/nguyen21a/nguyen21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {GOAL: Gist-set Online Active Learning for Efficient Chest X-ray Image Annotation},
 url = {https://proceedings.mlr.press/v143/nguyen21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-olivier21a,
 abstract = {In this paper, we propose a novel approach to overcome the problem of imbalanced datasets for object detection tasks, when the distribution is not uniform over all classes. The general idea is to compute a probability vector, encoding the probability for each image to be fed to the network during the training phase. This probability vector is computed by solving some quadratic optimization problem and ensures that all classes are seen with similar frequency. We apply this method to a fetal anatomies detection problem, and conduct a thorough statistical analysis of the resulting performance to show that it performs significantly better than two baseline models: one with images sampled uniformly and one implementing classical oversampling.},
 author = {Olivier, Antoine and Raynaud, Caroline},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3158046740},
 pages = {554--566},
 pdf = {https://proceedings.mlr.press/v143/olivier21a/olivier21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Balanced sampling for an object detection problem - application to fetal anatomies detection},
 url = {https://proceedings.mlr.press/v143/olivier21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-ozer21a,
 abstract = {Medical image quality assessment is an important aspect of image acquisition where poor-quality images may lead to misdiagnosis. In addition, manual labelling of image quality after the acquisition is often tedious and can lead to some misleading results. Despite much research on the automated analysis of image quality for tackling this problem, relatively little work has been done for the explanation of the methodologies. In this work, we propose an explainable image quality assessment system and validate our idea on foreign objects in a Chest X-Ray (Object-CXR) dataset. Our explainable pipeline relies on NormGrad, an algorithm, which can efficiently localize the image quality issues with saliency maps of the classifier. We compare our method with a range of saliency detection methods and illustrate the superior performance of NormGrad by obtaining a Pointing Game accuracy of 0.862 on the test dataset of the Object-CXR dataset. We also verify our findings through a qualitative analysis by visualizing attention maps for foreign objects on X-Ray images.},
 author = {Ozer, Caner and Oksuz, Ilkay},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3140653919},
 pages = {567--580},
 pdf = {https://proceedings.mlr.press/v143/ozer21a/ozer21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Explainable Image Quality Analysis of Chest X-Rays},
 url = {https://proceedings.mlr.press/v143/ozer21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-philipp21a,
 abstract = {Towards computer-assisted neurosurgery, robust methods for instrument localization on neurosurgical microscope video data are needed. Specifically for neurosurgical data, challenges arise from visual conditions such as strong blur and from an unknowingly large variety of instrument types. For neurosurgical domain, instrument localization methods must generalize across different sub-disciplines such as cranial tumor and aneurysm surgeries which exhibit different visual properties. We present and evaluate a methodology towards robust instrument tip localization for neurosurgical microscope data, formulated as coarse saliency prediction. For our analysis, we build a comprehensive dataset comprising in-the-wild data from several neurosurgical sub-disciplines as well as phantom surgeries. Comparing single stream networks using either image or optical flow information, we find complementary performance of both networks. Plain optical flow enables better cross-domain generalization, while the image-based network performs better on surgeries from the training domain. Based on these findings, we present a two-stream architecture that fuses image and optical flow information to utilize the complementary performance of both. Being trained on tumor surgeries, our architecture outperforms both single stream networks and shows improved robustness on data from different neurosurgical sub-disciplines. From our findings, future work must focus more on how to incorporate optical flow information into fusion architectures to further improve cross-domain generalization.},
 author = {Philipp, Markus and Alperovich, Anna and Gutt-Will, Marielena and Mathis, Andrea and Saur, Stefan and Raabe, Andreas and Mathis-Ullrich, Franziska},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3184481732},
 pages = {581--595},
 pdf = {https://proceedings.mlr.press/v143/philipp21a/philipp21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Localizing Neurosurgical Instruments Across Domains and in the Wild},
 url = {https://proceedings.mlr.press/v143/philipp21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-pinaya21a,
 abstract = {Pathological brain appearances may be so heterogeneous as to be intelligible only as anomalies, defined by their deviation from normality rather than any specific pathological characteristic. Amongst the hardest tasks in medical imaging, detecting such anomalies requires models of the normal brain that combine compactness with the expressivity of the complex, long-range interactions that characterise its structural organisation. These are requirements transformers have arguably greater potential to satisfy than other current candidate architectures, but their application has been inhibited by their demands on data and computational resource. Here we combine the latent representation of vector quantised variational autoencoders with an ensemble of autoregressive transformers to enable unsupervised anomaly detection and segmentation defined by deviation from healthy brain imaging data, achievable at low computational cost, within relative modest data regimes. We compare our method to current state-of-the-art approaches across a series of experiments involving synthetic and real pathological lesions. On real lesions, we train our models on 15,000 radiologically normal participants from UK Biobank, and evaluate performance on four different brain MR datasets with small vessel disease, demyelinating lesions, and tumours. We demonstrate superior anomaly detection performance both image-wise and pixel-wise, achievable without post-processing. These results draw attention to the potential of transformers in this most challenging of imaging tasks.},
 author = {Pinaya, Walter Hugo Lopez and Tudosiu, Petru-Daniel and Gray, Robert and Rees, Geraint and Nachev, Parashkev and Ourselin, S{\'e}bastien and Cardoso, M. Jorge},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3133297714},
 pages = {596--617},
 pdf = {https://proceedings.mlr.press/v143/pinaya21a/pinaya21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Unsupervised Brain Anomaly Detection and Segmentation with Transformers},
 url = {https://proceedings.mlr.press/v143/pinaya21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-pirkl21a,
 abstract = {Subject motion is one of the major challenges in clinical routine MR imaging. Despite ongoing research, motion correction has remained a complex problem without a universal solution. In advanced quantitative MR techniques, such as MR Fingerprinting, motion does not only affect a single image, like in single-contrast MRI, but disrupts the entire temporal evolution of the magnetization and causes parameter quantification errors due to a mismatch between the acquired and simulated signals. In this work, we present a deep learning-empowered retrospective motion correction for rapid 3D whole-brain multiparametric MRI based on Quantitative Transient-state Imaging (QTI). We propose a patch-based 3D multiscale convolutional neural network (CNN) that learns the residual error, i.e. after initial navigator-based correction, between motion-affected quantitative T1, T2 and proton density maps and their motion-free counterparts. For efficient model training despite limited data availability, we propose a physics-informed simulation to apply continuous motion-patterns to motion-free data. We evaluate the performance of the residual CNN on 1.5T and 3T MRI data of ten healthy volunteers. We analyze the generalizability of the model when applied to real clinical cases, including pediatric and adult patients with large brain lesions. Our study demonstrates that image quality can be significantly improved after correcting for subject motion. This has important implications in clinical setups where large amounts of motion affected data must be discarded.},
 author = {Pirkl, Carolin and Cencini, Matteo and Kurzawski, Jan W. and Waldmannstetter, Diana and Li, Hongwei and Sekuboyina, Anjany and Endt, Sebastian and Peretti, Luca and Donatelli, Graziella and Pasquariello, Rosa and Tosetti, Michela and Costagli, Mauro and Buonincontri, Guido and Menzel, Marion I. and Menze, Bjoern H.},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3164282812},
 pages = {618--632},
 pdf = {https://proceedings.mlr.press/v143/pirkl21a/pirkl21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Residual learning for 3D motion corrected quantitative MRI: Robust clinical T1, T2 and proton density mapping},
 url = {https://proceedings.mlr.press/v143/pirkl21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-prieto21a,
 abstract = {Chlamydia trachomatous is an infectious ocular condition that can cause the eyelid to turn inward so that one or more eyelashes touch the eyeball, a condition call trachomatous trichiasis (TT), which can lead to blindness. Community-based screeners are used in rural areas to identify patients with TT, who can then be referred for proper medical care. Having automatic methods to detect TT will reduce the amount of time required to train screeners and improve accuracy of detection. This paper proposes a method to automatically identify regions of an eye and identify TT, using photographs taken with smartphones in the field. The attention-based gated deep learning networks in combination with a regionidentification network can identify TT with an accuracy of 91%, sensitivity of 92% and specificity of 87%, showing that these methods have the potential to be deployed in the field.},
 author = {Prieto, Juan Carlos and Shah, Hina and Jones, Kasey and Chew, Robert F and Kana, Hashiya M. and Weaver, Jerusha and Flueckiger, Rebecca M. and McPherson, Scott and Gower, Emily W.},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3171734902},
 pages = {633--644},
 pdf = {https://proceedings.mlr.press/v143/prieto21a/prieto21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Image Sequence Generation and Analysis via GRU and Attention for Trachomatous Trichiasis Classification.},
 url = {https://proceedings.mlr.press/v143/prieto21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-qiu21a,
 abstract = {We present a deep learning (DL) registration framework for fast mono-modal and multi-modal image registration using differentiable mutual information and diffeomorphic B-spline free-form deformation (FFD). Deep learning registration has been shown to achieve competitive accuracy and significant speedups from traditional iterative registration methods. In this paper, we propose to use a B-spline FFD parameterisation of Stationary Velocity Field (SVF) to in DL registration in order to achieve smooth diffeomorphic deformation while being computationally-efficient. In contrast to most DL registration methods which use intensity similarity metrics that assume linear intensity relationship, we apply a differentiable variant of a classic similarity metric, mutual information, to achieve robust mono-modal and multi-modal registration. We carefully evaluated our proposed framework on mono- and multi-modal registration using 3D brain MR images and 2D cardiac MR images.},
 author = {Qiu, Huaqi and Qin, Chen and Schuh, Andreas and Hammernik, Kerstin and Rueckert, Daniel},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3152828339},
 pages = {645--664},
 pdf = {https://proceedings.mlr.press/v143/qiu21a/qiu21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Diffeomorphic and Modality-invariant Registration using B-splines.},
 url = {https://proceedings.mlr.press/v143/qiu21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-rajagopal21a,
 abstract = {Fully-convolutional neural networks, such as the 2D or 3D UNet, are now pervasive in medical imaging for semantic segmentation, classification, image denoising, domain translation, and reconstruction. However, evaluation of UNet performance, as with most CNNs, has mostly been relegated to evaluation of a few performance metrics (e.g. accuracy, IoU, SSIM, etc.) using the networkâs final predictions, which provides little insight into important issues such as dataset shift that occur in clinical application. In this paper, we propose techniques for understanding and visualizing the generalization performance of UNets in image classification and regression tasks, giving rise to metrics that are indicative of performance on a withheld test-set without the need for groundtruth annotations.},
 author = {Rajagopal, Abhejit and Madala, Vamshi Chowdary and Hope, Thomas A and Larson, Peder},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 pages = {665--681},
 pdf = {https://proceedings.mlr.press/v143/rajagopal21a/rajagopal21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Understanding and Visualizing Generalization in {UN}ets},
 url = {https://proceedings.mlr.press/v143/rajagopal21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-sharma21a,
 abstract = {In recent years, the availability of digitized Whole Slide Images (WSIs) has enabled the use of deep learning-based computer vision techniques for automated disease diagnosis. However, WSIs present unique computational and algorithmic challenges. WSIs are gigapixel-sized ($\sim$100K pixels), making them infeasible to be used directly for training deep neural networks. Also, often only slide-level labels are available for training as detailed annotations are tedious and can be time-consuming for experts. Approaches using multiple-instance learning (MIL) frameworks have been shown to overcome these challenges. Current state-of-the-art approaches divide the learning framework into two decoupled parts: a convolutional neural network (CNN) for encoding the patches followed by an independent aggregation approach for slide-level prediction. In this approach, the aggregation step has no bearing on the representations learned by the CNN encoder. We have proposed an end-to-end framework that clusters the patches from a WSI into ${k}$-groups, samples ${k}'$ patches from each group for training, and uses an adaptive attention mechanism for slide level prediction; Cluster-to-Conquer (C2C). We have demonstrated that dividing a WSI into clusters can improve the model training by exposing it to diverse discriminative features extracted from the patches. We regularized the clustering mechanism by introducing a KL-divergence loss between the attention weights of patches in a cluster and the uniform distribution. The framework is optimized end-to-end on slide-level cross-entropy, patch-level cross-entropy, and KL-divergence loss (Implementation: https://github.com/YashSharma/C2C).},
 author = {Sharma, Yash and Shrivastava, Aman and Ehsan, Lubaina and Moskaluk, Christopher A. and Syed, Sana and Brown, Donald},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3137165224},
 pages = {682--698},
 pdf = {https://proceedings.mlr.press/v143/sharma21a/sharma21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning for Whole Slide Image Classification},
 url = {https://proceedings.mlr.press/v143/sharma21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-shi21a,
 abstract = {We systematically evaluate the performance of deep learning models in the presence of diseases not labeled for or present during training. First, we evaluate whether deep learning models trained on a subset of diseases (seen diseases) can detect the presence of any one of a larger set of diseases. We find that models tend to falsely classify diseases outside of the subset (unseen diseases) as "no disease". Second, we evaluate whether models trained on seen diseases can detect seen diseases when co-occurring with diseases outside the subset (unseen diseases). We find that models are still able to detect seen diseases even when co-occurring with unseen diseases. Third, we evaluate whether feature representations learned by models may be used to detect the presence of unseen diseases given a small labeled set of unseen diseases. We find that the penultimate layer of the deep neural network provides useful features for unseen disease detection. Our results can inform the safe clinical deployment of deep learning models trained on a non-exhaustive set of disease classes.},
 author = {Shi, Siyu and Malhi, Ishaan and Tran, Kevin and Ng, Andrew Y. and Rajpurkar, Pranav},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3138492598},
 pages = {699--712},
 pdf = {https://proceedings.mlr.press/v143/shi21a/shi21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {CheXseen: Unseen Disease Detection for Deep Learning Interpretation of Chest X-rays},
 url = {https://proceedings.mlr.press/v143/shi21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-simko21a,
 abstract = {The contrast settings to select before acquiring magnetic resonance imaging (MRI) signal depend heavily on the subsequent tasks. As each contrast highlights different tissues, automated segmentation tools for example might be optimized for a certain contrast. While for radiotherapy, multiple scans of the same region with different contrasts can achieve a better accuracy for delineating tumours and organs at risk. Unfortunately, the optimal contrast for the subsequent automated methods might not be known during the time of signal acquisition, and performing multiple scans with different contrasts increases the total examination time and registering the sequences introduces extra work and potential errors. Building on the recent achievements of deep learning in medical applications, the presented work describes a novel approach for transferring any contrast to any other. The novel model architecture incorporates the signal equation for spin echo sequences, and hence the model inherently learns the unknown quantitative maps for proton density, $T1$ and $T2$ relaxation times ($PD$, $T1$ and $T2$, respectively). This grants the model the ability to retrospectively reconstruct spin echo sequences by changing the contrast settings Echo and Repetition Time ($TE$ and $TR$, respectively). The model learns to identify the contrast of pelvic MR images, therefore no paired data of the same anatomy from different contrasts is required for training. This means that the experiments are easily reproducible with other contrasts or other patient anatomies. Despite the contrast of the input image, the model achieves accurate results for reconstructing signal with contrasts available for evaluation. For the same anatomy, the quantitative maps are consistent for a range of contrasts of input images. Realized in practice, the proposed method would greatly simplify the modern radiotherapy pipeline. The trained model is made public together with a tool for testing the model on example images.},
 author = {Simko, Attila Tibor and L{\"o}fstedt, Tommy and Garpebring, Anders and Bylund, Mikael and Nyholm, Tufve and Jonsson, Joakim},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3158830848},
 pages = {713--727},
 pdf = {https://proceedings.mlr.press/v143/simko21a/simko21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Changing the Contrast of Magnetic Resonance Imaging Signals using Deep Learning},
 url = {https://proceedings.mlr.press/v143/simko21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-sowrirajan21a,
 abstract = {Contrastive learning is a form of self-supervision that can leverage unlabeled data to produce pretrained models. While contrastive learning has demonstrated promising results on natural image classification tasks, its application to medical imaging tasks like chest X-ray interpretation has been limited. In this work, we propose MoCo-CXR, which is an adaptation of the contrastive learning method Momentum Contrast (MoCo), to produce models with better representations and initializations for the detection of pathologies in chest X-rays. In detecting pleural effusion, we find that linear models trained on MoCo-CXR-pretrained representations outperform those without MoCo-CXR-pretrained representations, indicating that MoCo-CXR-pretrained representations are of higher-quality. End-to-end fine-tuning experiments reveal that a model initialized via MoCo-CXR-pretraining outperforms its non-MoCo-CXR-pretrained counterpart. We find that MoCo-CXR-pretraining provides the most benefit with limited labeled training data. Finally, we demonstrate similar results on a target Tuberculosis dataset unseen during pretraining, indicating that MoCo-CXR-pretraining endows models with representations and transferability that can be applied across chest X-ray datasets and tasks.},
 author = {Sowrirajan, Hari and Yang, Jingbo and Ng, Andrew Y. and Rajpurkar, Pranav},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3092787476},
 pages = {728--744},
 pdf = {https://proceedings.mlr.press/v143/sowrirajan21a/sowrirajan21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {MoCo-CXR: MoCo Pretraining Improves Representation and Transferability of Chest X-ray Models},
 url = {https://proceedings.mlr.press/v143/sowrirajan21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-tolle21a,
 abstract = {Exploiting the deep image prior property of convolutional auto-encoder networks is especially interesting for medical image processing as it avoids hallucinations by omitting supervised learning. Its spectral bias towards lower frequencies makes it suitable for inverse image problems such as denoising and super-resolution, but manual early stopping has to be applied to act as a low-pass filter. In this paper, we present a novel Bayesian approach to deep image prior using mean-field variational inference. This allows for uncertainty quantification on a per-pixel level and, given the right prior distribution on the network weights, omits the need for early stopping. We optimize the parameters of the weight prior towards reconstruction accuracy using Bayesian optimization with Gaussian Process regression. We evaluate our approach on different inverse tasks on a variety of modalities and demonstrate that an optimized weight prior outperforms former state-of-the-art Bayesian deep image prior approaches. We show that a badly selected prior leads to worse accuracy and calibration and that it is sufficient to optimize the weight prior parameter per task domain.},
 author = {T{\"o}lle, Malte and Laves, Max-Heinrich and Schlaefer, Alexander},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3159735194},
 pages = {745--760},
 pdf = {https://proceedings.mlr.press/v143/tolle21a/tolle21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Mean-Field Variational Inference Approach to Deep Image Prior for Inverse Problems in Medical Imaging},
 url = {https://proceedings.mlr.press/v143/tolle21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-turja21a,
 abstract = {The excessive deposition of misfolded proteins such as amyloid-$\beta$Â (A$\beta$) protein is an aging event underlying several neurodegenerative diseases. Mounting evidence shows that the spreading of neuropathological burden has a strong association to the white matter tracts in the brain which can be measured using diffusion-weighted imaging and tractography technologies. Most of the previous studies analyze the dynamic progression of amyloid using cross-sectional data which is not robust to the heterogeneous A$\beta$ dynamics across the population. In this regard, we propose a graph neural network-based learning framework to capture the disease-related dynamics by tracking the spreading of amyloid across brain networks from the subject-specific longitudinal PET images. To learn from limited (2 â 3 timestamps) and noisy longitudinal data, we restrict the space of amyloid propagation patterns to a latent heat diffusion model which is constrained by the anatomical connectivity of the brain. Our experiments show that restricting the dynamics to be a heat diffusion mechanism helps to train a robust deep neural network for predicting future time points and classifying Alzheimerâs disease brain.},
 author = {Turja, Md Asadullah and Wu, Guorong and Yang, Defu and Styner, Martin Andreas},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3157839596},
 pages = {761--773},
 pdf = {https://proceedings.mlr.press/v143/turja21a/turja21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning the Latent Heat Diffusion Process through Structural Brain Network from Longitudinal $\beta$-Amyloid Data},
 url = {https://proceedings.mlr.press/v143/turja21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-uzunova21a,
 abstract = {The disentanglement of shape and appearance is a prominent computer vision task, that has become relevant in the medical imaging domain in recent years. Medical images are often acquired in different hospitals, by different devices and using different parameters, resulting in varying intensity profiles. However, when performing population-based analysis over various datasets, e.g. from different hospitals, it is important to be able to distinguish between changes in the anatomical shapes and device-dependent intensity changes.},
 author = {Uzunova, Hristina and Handels, Heinz and Ehrhardt, Jan},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W4226372903},
 pages = {774--786},
 pdf = {https://proceedings.mlr.press/v143/uzunova21a/uzunova21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Abstract: Guided Filter Regularization for Improved Disentanglement of Shape and Appearance in Diffeomorphic Autoencoders},
 url = {https://proceedings.mlr.press/v143/uzunova21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-vadacchino21a,
 abstract = {Segmentation of enhancing tumours or lesions from MRI is important for detecting new disease activity in many clinical contexts. However, accurate segmentation requires the inclusion of medical images (e.g., T1 post contrast MRI) acquired after injecting patients with a contrast agent (e.g., Gadolinium), a process no longer thought to be safe. Although a number of modality-agnostic segmentation networks have been developed over the past few years, they have been met with limited success in the context of enhancing pathology segmentation. In this work, we present HAD-Net, a novel offline adversarial knowledge distillation (KD) technique, whereby a pre-trained teacher segmentation network, with access to all MRI sequences, teaches a student network, via hierarchical adversarial training, to better overcome the large domain shift presented when crucial images are absent during inference. In particular, we apply HAD-Net to the challenging task of enhancing tumour segmentation when access to post-contrast imaging is not available. The proposed network is trained and tested on the BraTS 2019 brain tumour segmentation challenge dataset, where it achieves performance improvements in the ranges of 16% - 26% over (a) recent modality-agnostic segmentation methods (U-HeMIS, U-HVED), (b) KD-Net adapted to this problem, (c) the pre-trained student network and (d) a non-hierarchical version of the network (AD-Net), in terms of Dice scores for enhancing tumour (ET). The network also shows improvements in tumour core (TC) Dice scores. Finally, the network outperforms both the baseline student network and AD-Net in terms of uncertainty quantification for enhancing tumour segmentation based on the BraTs 2019 uncertainty challenge metrics. Our code is publicly available at: https://github.com/SaverioVad/HAD_Net},
 author = {Vadacchino, Saverio and Mehta, Raghav and Sepahvand, Nazanin Mohammadi and Nichyporuk, Brennan and Clark, James J. and Arbel, Tal},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3150202690},
 pages = {787--801},
 pdf = {https://proceedings.mlr.press/v143/vadacchino21a/vadacchino21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {HAD-Net: A Hierarchical Adversarial Knowledge Distillation Network for Improved Enhanced Tumour Segmentation Without Post-Contrast Images},
 url = {https://proceedings.mlr.press/v143/vadacchino21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-wood21a,
 abstract = {The growing demand for head magnetic resonance imaging (MRI) examinations, along with a global shortage of radiologists, has led to an increase in the time taken to report head MRI scans around the world.For many neurological conditions, this delay can result in increased morbidity and mortality.An automated triaging tool could reduce reporting times for abnormal examinations by identifying abnormalities at the time of imaging and prioritizing the reporting of these scans.In this work, we present a convolutional neural network for detecting clinically-relevant abnormalities in T 2 -weighted head MRI scans.Using a validated neuroradiology report classifier, we generated a labelled dataset of 43,754 scans from two large UK hospitals for model training, and demonstrate accurate classification (area under the receiver operating curve (AUC) = 0.943) on a test set of 800 scans labelled by a team of neuroradiologists.Importantly, when trained on scans from only a single hospital the model generalized to scans from the other hospital (∆AUC ≤ 0.02).A simulation study demonstrated that our model would reduce the mean reporting time for abnormal examinations from 28 days to 14 days and from 9 days to 5 days at the two hospitals, demonstrating feasibility for use in a clinical triage environment.},
 author = {Wood, David A. and Kafiabadi, Sina and Al Busaidi, Aisha and Guilhem, Emily and Montvila, Antanas and Agarwal, Siddarth and Lynch, Jeremy and Townend, Matthew and Barker, Gareth and Ourselin, Sebastian and Cole, James H. and Booth, Thomas C.},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3203524210},
 pages = {813--841},
 pdf = {https://proceedings.mlr.press/v143/wood21a/wood21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Automated triaging of head MRI examinations using convolutional neural networks},
 url = {https://proceedings.mlr.press/v143/wood21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-zhang21a,
 abstract = {To develop deep learning-based models for automatic analysis of histopathology whole slide images (WSIs), the atomic entities to be directly processed are often the smaller patches cropped from WSIs as it is not always possible to feed a whole WSI to a model given its enormous size. However, a trained model tends to relate the slide-specific characteristics to diagnosis results because a large number of patches cropped from the same WSI will share common slide features and thus have strong correlations between them, resulting in deteriorated generalization capability of the trained model. Current approaches to alleviate this issue include data pre-processing (stain normalization or color augmentation) and adversarial learning, both of which introduce extra complications in computations. Alternatively, we propose to reduce the impact of this issue by introducing a new regularization term to the standard loss function to reduce the correlation of the patches from the same WSI. It is intuitive and easy-to-implement and introduces comparably smaller computation overhead compared to existing approaches. Experimental results prove that the proposed regularization term is able to enhance the generalization capability of learning models and consequently to achieve better performance. The code is available in: \url{https://github.com/hrzhang1123/SlideCorrelationReduction}.},
 author = {Zhang, Hongrun and Meng, Yanda and Qian, Xuesheng and Yang, Xiaoyun and Coupland, Sarah E. and Zheng, Yalin},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3155068666},
 pages = {842--854},
 pdf = {https://proceedings.mlr.press/v143/zhang21a/zhang21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A regularization term for slide correlation reduction in whole slide image analysis with deep learning},
 url = {https://proceedings.mlr.press/v143/zhang21a.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-zhang21b,
 abstract = {Surgical workflow recognition has been playing an essential role in computer-assisted interventional systems for modern operating rooms. In this paper, we present a computer vision-based method named SWNet that focuses on utilizing spatial information and temporal information from the surgical video to achieve surgical workflow recognition. As the first step, we utilize Interaction-Preserved Channel-Separated Convolutional Network (IP-CSN) to extract features that contain spatial information and local temporal information from the surgical video through segments. Secondly, we train a Multi-Stage Temporal Convolutional Network (MS-TCN) with those extracted features to capture global temporal information from the full surgical video. Finally, by utilizing Prior Knowledge Noise Filtering (PKNF), prediction noise from the output of MS-TCN is filtered. We evaluate SWNet for Sleeve Gastrectomy surgical workflow recognition. SWNet achieves 90% frame-level accuracy and reaches a weighted Jaccard Score of 0.8256. This demonstrates that SWNet has considerable potential to solve the surgical workflow recognition problem.},
 author = {Zhang, Bokai and Ghanem, Amer and Simes, Alexander and Choi, Henry and Yoo, Andrew and Min, Andrew},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 pages = {855--869},
 pdf = {https://proceedings.mlr.press/v143/zhang21b/zhang21b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {{SWN}et: Surgical Workflow Recognition with Deep Convolutional Network},
 url = {https://proceedings.mlr.press/v143/zhang21b.html},
 volume = {143},
 year = {2021}
}

@inproceedings{pmlr-v143-zhang21c,
 abstract = {A Hybrid Optimization Between Iterative and network fine-Tuning (HOBIT) reconstruction method is proposed to solve quantitative susceptibility mapping (QSM) inverse problem in MRI. In HOBIT, a convolutional neural network (CNN) is first trained on healthy subjectsâ data with gold standard labels. Domain adaptation to patientsâ data with hemorrhagic lesions is then deployed by minimizing fidelity loss on the patient training dataset. During test time, a fidelity loss is imposed on each patient test case, where alternating direction method of multiplier (ADMM) is used to split the time consuming fidelity imposed network update into iterative reconstruction and network update subproblems alternatively in ADMM, and only a subnet of the pre-trained CNN is updated during the process. Compared to the method FINE where such fidelity imposing strategy was initially proposed to solve QSM, HOBIT achieved both performance gain of reconstruction accuracy and vast reduction of computational time.},
 author = {Zhang, Jinwei and Zhang, Hang and Spincemaille, Pascal and Nguyen, Thanh and Sabuncu, Mert R. and Wang, Yi},
 booktitle = {Proceedings of the Fourth Conference on Medical Imaging with Deep Learning},
 editor = {Heinrich, Mattias and Dou, Qi and de Bruijne, Marleen and Lellmann, Jan and SchlÃ¤fer, Alexander and Ernst, Floris},
 month = {07--09 Jul},
 openalex = {W3136438933},
 pages = {870--880},
 pdf = {https://proceedings.mlr.press/v143/zhang21c/zhang21c.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Hybrid optimization between iterative and network fine-tuning reconstructions for fast quantitative susceptibility mapping},
 url = {https://proceedings.mlr.press/v143/zhang21c.html},
 volume = {143},
 year = {2021}
}
