@proceedings{AFFCOMP2018,
 booktitle = {Proceedings of IJCAI 2018 2nd Workshop on Artificial Intelligence in Affective Computing},
 editor = {William Hsu and Heath Yates},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of IJCAI 2018 2nd Workshop on Artificial Intelligence in Affective Computing},
 volume = {86}
}

@inproceedings{pmlr-v86-gjoreski20a,
 abstract = {The computer science field Affective Computing, which studies and develops emotional intelligent systems, has been active for almost two decades now with limited results. Arousal as the dimension that represents the intensity of the emotions, represents similar recognition problems. This is the ï¬rst study that analyzes six publicly available datasets for arousal recognition from physiological signals and proposes a method capable of combining them. The novel method, an inter-domain Deep Neural Network (DNN) ensemble, is compared to classical machine learning (ML). For both methods, the raw data from Galvanic Skin Response (GSR), Electrocardiography ECG, and Blood Volume Pulse (BVP) sensors is processed and transformed into a common spectro-temporal space of R-R intervals and GSR data. For the classical ML algorithms, features are extracted, and for the DNN algorithms, two diï¬erent approaches were taken: a fully connected DNN trained with the same features as the classical ML algorithms (DNN-Features) and a Convolutional Neural Network (CNN) trained with the temporal representation of the GSR signal (CNN-GSR). Finally, a fully connected DNN meta learner is trained to utilize the knowledge from the two different DNNs and to tune the DNN models for the target dataset. The experimental results showed that the novel DNN ensemble method outperforms the classical ML methods and the non-ensemble DNN methods. Additionally, the CNN-GSR model learned that the peaks of the GSR signal contain the most information regarding the arousal, thus the network developed filters to emphasize those parts.},
 author = {Gjoreski, Martin and Gjoreski, Hristijian and Lustrek, Mitja and Gams, Matjaz},
 booktitle = {Proceedings of IJCAI 2018 2nd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William and Yates, Heath},
 month = {15 Jul},
 openalex = {W3046800815},
 pages = {52--64},
 pdf = {http://proceedings.mlr.press/v86/gjoreski20a/gjoreski20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Deep Ensembles for Inter-Domain Arousal Recognition.},
 url = {http://proceedings.mlr.press/v86/gjoreski20a.html},
 volume = {86},
 year = {2020}
}

@inproceedings{pmlr-v86-jaques20a,
 abstract = {In the quest towards general artificial intelligence (AI), researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations, and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN, an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers, by optimizing the model to produce sketches that it predicts will lead to more positive facial expressions. We show in multiple independent evaluations that the model trained with facial feedback produced sketches that are more highly rated, and induce significantly more positive facial expressions. Thus, we establish that implicit social feedback can improve the output of a deep learning model.},
 author = {Jaques, Natasha and McCleary, Jennifer and Engel, Jesse and Ha, David and Bertsch, Fred and Eck, Douglas and Picard, Rosalind},
 booktitle = {Proceedings of IJCAI 2018 2nd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William and Yates, Heath},
 month = {15 Jul},
 openalex = {W2889498027},
 pages = {1--9},
 pdf = {http://proceedings.mlr.press/v86/jaques20a/jaques20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning via social awareness: Improving a deep generative sketching model with facial feedback},
 url = {http://proceedings.mlr.press/v86/jaques20a.html},
 volume = {86},
 year = {2020}
}

@inproceedings{pmlr-v86-melhart20a,
 abstract = {The question of representing emotion computationally remains largely unanswered: popular approaches require annotators to assign a magnitude (or a class) of some emotional dimension, while an alternative is to focus on the relationship between two or more options. Recent evidence in aï¬ective computing suggests that following a methodology of ordinal annotations and processing leads to better reliability and validity of the model. This paper compares the generality of classiï¬cation methods versus preference learning methods in predicting the levels of arousal in two widely used aï¬ective datasets. Findings of this initial study further validate the hypothesis that approaching aï¬ect labels as ordinal data and building models via preference learning yields models of better validity.},
 author = {Melhart, David and Sfikas, Konstantinos and Giannakakis, Giorgos and Liapis, Georgios Yannakakis Antonios},
 booktitle = {Proceedings of IJCAI 2018 2nd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William and Yates, Heath},
 month = {15 Jul},
 openalex = {W3046518251},
 pages = {27--34},
 pdf = {http://proceedings.mlr.press/v86/melhart20a/melhart20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Study on Affect Model Validity: Nominal vs Ordinal Labels.},
 url = {http://proceedings.mlr.press/v86/melhart20a.html},
 volume = {86},
 year = {2020}
}

@inproceedings{pmlr-v86-yates20a,
 abstract = {The goal of this paper is to develop a methodology and model to classify and characterize the arousal state of participants in a built environment. Demonstrating this showcases the potential of developing an intelligent system capable of both classifying and predicting biometric arousal state. This classiï¬cation process is traditionally performed by human experts. Our approach can be leveraged to take advantage of the diversity of real-time sensor data to inform the development of smart(er) environments to improve human health.},
 author = {Yates, Heath and Chamberlain, Brent and Hsu, William},
 booktitle = {Proceedings of IJCAI 2018 2nd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William and Yates, Heath},
 month = {15 Jul},
 openalex = {W3046826541},
 pages = {35--51},
 pdf = {http://proceedings.mlr.press/v86/yates20a/yates20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Binary Classification of Arousal in Built Environments using Machine Learning.},
 url = {http://proceedings.mlr.press/v86/yates20a.html},
 volume = {86},
 year = {2020}
}

@inproceedings{pmlr-v86-yin20a,
 abstract = {In this paper, we present a multimodal approach to simultaneously analyze facial movements and several peripheral physiological signals to decode individualized affective experiences under positive and negative emotional contexts, while considering their personalized resting dynamics. We propose a person-specific recurrence network to quantify the dynamics present in the person's facial movements and physiological data. Facial movement is represented using a robust head vs. 3D face landmark localization and tracking approach, and physiological data are processed by extracting known attributes related to the underlying affective experience. The dynamical coupling between different input modalities is then assessed through the extraction of several complex recurrent network metrics. Inference models are then trained using these metrics as features to predict individual's affective experience in a given context, after their resting dynamics are excluded from their response. We validated our approach using a multimodal dataset consists of (i) facial videos and (ii) several peripheral physiological signals, synchronously recorded from 12 participants while watching 4 emotion-eliciting video-based stimuli. The affective experience prediction results signified that our multimodal fusion method improves the prediction accuracy up to 19% when compared to the prediction using only one or a subset of the input modalities. Furthermore, we gained prediction improvement for affective experience by considering the effect of individualized resting dynamics.},
 author = {Yin, Yu and Nabian, Mohsen and Ostadabbas, Sarah},
 booktitle = {Proceedings of IJCAI 2018 2nd Workshop on Artificial Intelligence in Affective Computing},
 editor = {Hsu, William and Yates, Heath},
 month = {15 Jul},
 openalex = {W2900515000},
 pages = {10--26},
 pdf = {http://proceedings.mlr.press/v86/yin20a/yin20a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Facial Expression and Peripheral Physiology Fusion to Decode Individualized Affective Experience},
 url = {http://proceedings.mlr.press/v86/yin20a.html},
 volume = {86},
 year = {2020}
}
