@proceedings{NeurIPS-CellSeg2023,
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Jun Ma and Ronald Xie and Anubha Gupta and JosÃ© Almeida and Gary D. Bader and Bo Wang},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 volume = {212}
}

@inproceedings{pmlr-v212-bai23a,
 abstract = {Cell instance segmentation, which identifies each specific cell area within a mi- croscope image, is helpful for cell analysis. Because of the high computational cost brought on by the large number of objects in the scene, mainstream instance segmentation techniques require much time and computational resources. In this paper, we proposed a two-stage method in which the first stage detects the bounding boxes of cells, and the second stage is segmentation in the detected bounding boxes. This method reduces inference time by more than 30% on images that image size is larger than 1024 pixels by 1024 pixels compared to the mainstream instance segmentation method while maintaining reasonable accuracy without using any external data.},
 author = {Bai, Bizhe and Tian, Jie and Luo, Sicong and Wang, Tao and Lyu, Sisuo},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--15},
 pdf = {https://proceedings.mlr.press/v212/bai23a/bai23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {YUSEG: Yolo and Unet is all you need for cell instance segmentation},
 url = {https://proceedings.mlr.press/v212/bai23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-cai23a,
 abstract = {Cell instance segmentation is a fundamental task in analyzing microscopy images, with applications in computer-aided biomedical research. In recent years, deep learning techniques have been widely used in this field. However, existing methods exhibit inadequate generalization ability towards multi-modal cellular images and require a considerable amount of manually labeled data for training. To overcome these limitations, we present VSM, a versatile semi-supervised model for multi-modal cell instance segmentation. Our method delivers high accuracy and efficiency, as verified through comprehensive experiments. Additionally, VSM achieved a top-five ranking in the Weakly Supervised Cell Segmentation category of the multi-modal High-Resolution Microscopy competition.},
 author = {Cai, Xiaochen and Cai, Hengxing and Xu, Kele and Tu, Wei-Wei and Li, Wu-Jun},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--13},
 pdf = {https://proceedings.mlr.press/v212/cai23a/cai23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {VSM: A Versatile Semi-supervised Model for Multi-modal Cell Instance Segmentation},
 url = {https://proceedings.mlr.press/v212/cai23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-hu23a,
 abstract = {Cell Segmentation is an initial and fundamental step in biomedical image analysis, which strongly affects the experimental results of this analysis. Recently, deep learning based segmentation methods have shown great power in segmentation accuracy and efficiency. However, these data-driven methods still face many challenges, such as lack of annotations, multi-modality, and complex morphology, where morphological complexity significantly limits model performance. In this paper, we propose a new all-purpose framework with high morphological adaptability for multi-modality cell segmentation, termed Cell Segmenter (CS). For high convex cells with an arbitrary size, the Anchor-based Watershed Framework (AWF) precisely locates well-defined cell centers and generates segmentation based on these markers. For those elongated or non-convex cells, the center-independent segmentation method Omnipose is adopted to obtain satisfying masks. In the inference time, confidence-based quality estimation is conducted on the branch predictions if needed, and then the better result is chosen as the final segmentation. The F1-score of the proposed method reaches 0.8537 on TuningSet and 0.6216 on the final test set of the NeurIPS 2022 Cell Segmentation Challenge.},
 author = {Hu, Kaiwen and Zhang, Shengxuming and Jia, Zhijie and Cheng, Lechao and Zunlei, Feng},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--12},
 pdf = {https://proceedings.mlr.press/v212/hu23a/hu23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Cell Segmenter: A General Framework for Multi-modality Cell Segmentation },
 url = {https://proceedings.mlr.press/v212/hu23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-joubbi23a,
 abstract = {Segmenting microscopy images is a crucial step for quantitatively analyzing biological imaging data. Classical tools for biological image segmentation need to be adjusted to the cell type and image conditions to get decent results. Another limitation is the lack of high-quality labeled data to train alternative methods like Deep Learning since manual labeling is costly and time-consuming. <em> Weakly Supervised Cell Segmentation in Multi-modality High-Resolution Microscopy Images </em> was organized by NeurIPS to solve this problem. The aim of the challenge was to develop a versatile method that can work with high variability, with few labeled images, a lot of unlabeled images, and with no human interaction. We developed CrossCT, a framework based on the crossâteaching between a CNN and a Transformer. The main idea behind this work was to improve the organizersâ baseline methods and use both labeled and unlabeled data. Experiments show that our method outperforms the baseline methods based on a supervised learning approach. We achieved an F1 score of 0.5988 for the Transformer and 0.5626 for the CNN respecting the time limits imposed for inference.},
 author = {Joubbi, Sara and Ciano, Giorgio and Cardamone, Dario and Maccari, Giuseppe and Medini, Duccio},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--14},
 pdf = {https://proceedings.mlr.press/v212/joubbi23a/joubbi23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {CrossCT: CNN and Transformer crossâteaching for multimodal image cell segmentation},
 url = {https://proceedings.mlr.press/v212/joubbi23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-lee23a,
 abstract = {Cell segmentation is a fundamental task for computational biology analysis. Identifying the cell instances is often the first step in various downstream biomedical studies. However, many cell segmentation algorithms, including the recently emerging deep learning-based methods, still show limited generality under the multi-modality environment. Weakly Supervised Cell Segmentation in Multi-modality High-Resolution Microscopy Images was hosted at NeurIPS 2022 to tackle this problem. We propose MEDIAR, a holistic pipeline for cell instance segmentation under multi-modality in this challenge. MEDIAR harmonizes data-centric and model-centric approaches as the learning and inference strategies, achieving a 0.9067 F1-score at the validation phase while satisfying the time budget. To facilitate subsequent research, we provide the source code and trained model as open-source: https://github.com/Lee-Gihun/MEDIAR},
 author = {Lee, Gihun and Kim, SangMook and Kim, Joonkee and Yun, Se-Young.},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 openalex = {W4310922815},
 pages = {1--16},
 pdf = {https://proceedings.mlr.press/v212/lee23a/lee23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {MEDIAR: Harmony of Data-Centric and Model-Centric for Multi-Modality Microscopy},
 url = {https://proceedings.mlr.press/v212/lee23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-lee23b,
 abstract = {Deep learning has achieved significant improvement in cell segmentation of microscopy images in the field of Biology. However, a lack of generalization has been a major bottleneck of segmentation models since the performance is largely degraded with out-of-distribution data or unseen class data. Developing a generalized segmentation model is challenging due to the diversity of modalities, different staining methods, complicated cell shapes, and extremely high image resolution in microscopy images. The dataset for the ââWeakly Supervised Cell Segmentation in Multi-modality High-Resolution Microscopy Imagesâ challenge consists of images with these diverse characteristics. To address these challenges, we trained the Cellpose model to competently perform instance segmentation on datasets with various characteristics. For that, we 1) specified the model to only use green and blue channels for all types of cell images, and 2) investigated the effect and performance of the existing diameter estimation model to determine the areas where it performs best, using images of various resolutions. As a result, we achieved an F1 score of 0.7607 for the validation (Tuning) set.},
 author = {Lee, Kwanyoung and Byun, Hyungjo and Shim, Hyunjung},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {},
 month = {28 Nov--09 Dec},
 pages = {1--11},
 pdf = {https://proceedings.mlr.press/v212/lee23b/lee23b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Cell Segmentation in Multi-modality High-Resolution Microscopy Images with Cellpose},
 url = {https://proceedings.mlr.press/v212/lee23b.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-lou23a,
 abstract = {Cell segmentation for multi-modal microscopy images remains a challenge due to the complex textures, patterns, and cell shapes in these images. To tackle the problem, we first develop an automatic cell classification pipeline to label the microscopy images based on their low-level image characteristics, and then train a classification model based on the category labels. Afterward, we train a separate segmentation model for each category using the images in the corresponding category. Besides, we further deploy two types of segmentation models to segment cells with roundish and irregular shapes respectively. Moreover, an efficient and powerful backbone model is utilized to enhance the efficiency of our segmentation model. Evaluated on the Tuning Set of NeurIPS 2022 Cell Segmentation Challenge, our method achieves an F1-score of 0.8795 and the running time for all cases is within the time tolerance.},
 author = {Lou, Wei and Yu, Xinyi and Liu, Chenyu and Wan, Xiang and Li, Guanbin and Liu, Siqi and Li, Haofeng},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {},
 month = {28 Nov--09 Dec},
 openalex = {W4387928934},
 pages = {1--10},
 pdf = {https://proceedings.mlr.press/v212/lou23a/lou23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-stream Cell Segmentation with Low-level Cues for Multi-modality Images},
 url = {https://proceedings.mlr.press/v212/lou23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-lu23a,
 abstract = {Cell segmentation is an important initial task in medical image analysis, and in recent years, data-driven deep learning methods have made groundbreaking achievements in this field. In this challenge, a multi-modal and partially labeled dataset is provided. In this paper, we propose a multi-modality cell segmentation framework called Re-Unet, which is based on the nnU-Net pipeline and an iterative self-training method. Re-Unet enriches the original data and fully considers the information of cell intervals while making full use of the semi-supervised data. Our proposed method achieves a mean F1 score of 0.6101 on the tuning set and a F1 score of 0.4492 on the testing set.},
 author = {Lu, Haotian and Feng, Jinghao and Peng, Zelin and Shen, Wei},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--9},
 pdf = {https://proceedings.mlr.press/v212/lu23a/lu23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Re-Unet:Multi-Modality Cell Segmentation based on nnU-Net Pipeline},
 url = {https://proceedings.mlr.press/v212/lu23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-nguyen-hai23a,
 abstract = {Cell segmentation is a fundamental task in biomedical image analysis, which involves the identification and separation of individual cells from microscopy images. Large-size images and unannotated data are two canailing problems degrading the performance in cell segmentation. Regarding these issues, we propose sliding window and pseudo-labeling techniques by conducting several experiments on different neural architectures. Following this approach, our method achieves a significant performance improvement and a final result of 0.8097 F1 score on the tuning set and 0.6379 F1 score on the test set of Weakly Supervised Cell Segmentation in Multi-modality Microscopy challenge hosted at NeurIPS 2022.},
 author = {Nguyen Hai, Minh and Le Huy, Duong and Nguyen The, Nam and Bui Nhat, Truong and Dam Trong, Tuyen and Le Thi, Hanh and Nguyen Kha Ngoc, Anh and Le Trung, Kien and Nguyen Cong Hoang, Anh and Nguyen Ngoc, Anh and Nguyen Hai, Duong},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--10},
 pdf = {https://proceedings.mlr.press/v212/nguyen-hai23a/nguyen-hai23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sliding Window and Pseudo-labeling techniques for Cellular Segmentation},
 url = {https://proceedings.mlr.press/v212/nguyen-hai23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-upschulte23a,
 abstract = {We present a simple framework for cell segmentation, based on uncertainty-aware Contour Proposal Networks (CPNs). It is designed to provide high segmentation accuracy while remaining computationally efficient, which makes it an ideal solution for high throughput microscopy applications. Each predicted cell is provided with four uncertainty estimations that give information about the localization accuracy of the detected cell boundaries. Such additional insights are valuable for downstream single-cell analysis in microscopy image-based biology and biomedical research. In the context of the NeurIPS 22 Cell Segmentation Challenge, the proposed solution is shown to generalize well in a multi-modality setting, while respecting domain-specific requirements such as focusing on specific cell types. Without an ensemble or test-time augmentation the method achieves an F1 score of 0.8986 on the challengeâs validation set.},
 author = {Upschulte, Eric and Harmeling, Stefan and Amunts, Katrin and Dickscheid, Timo},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--12},
 pdf = {https://proceedings.mlr.press/v212/upschulte23a/upschulte23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Uncertainty-Aware Contour Proposal Networks for Cell Segmentation in Multi-Modality High-Resolution Microscopy Images},
 url = {https://proceedings.mlr.press/v212/upschulte23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-wang23a,
 abstract = {Many clinical and biological tasks depend on accurate cell instance segmentation. Currently, the rapid development of deep learning realizes the automation of cell segmentation, which significantly decreases the workload of clinicians and researchers. However, most existing cell segmentation frameworks are fully supervised and modality-specific. Towards this end, this paper proposes a semi-supervised cell instance segmentation framework for multi-modality microscope images. Firstly, $K$-Means clustering is utilized to discriminate the image modality. Then, for phase contrast and differential interference contrast images, Cellpose is adopted. For brightfield images, we subdivide them into two sub-categories according to the cell diameter by $K$-Means and optimize a U-Net for the large diameter group. For fluorescence images, we propose a semi-supervised learning strategy using CDNet. The leaderboard shows that our proposed framework reaches an F1 score of 0.8428 on the tuning set, which ranks 6th among all teams.},
 author = {Wang, Ziyue and Fang, Zijie and Chen, Yang and Yang, Zexi and Liu, Xinhao and Zhang, Yongbing},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--11},
 pdf = {https://proceedings.mlr.press/v212/wang23a/wang23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Semi-Supervised Cell Instance Segmentation for Multi-Modality Microscope Images},
 url = {https://proceedings.mlr.press/v212/wang23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-wangkai23a,
 abstract = {Automatic cell segmentation enjoys great popularity with the development of deep learning. However, existing methods tend to focus on the binary segmentation between foreground and background in a single domain, but fail to generalize to multi-modality cell images and to exploit numerous valuable unlabeled data. To mitigate these limitations, we propose a Modality-aware Anti-ambiguity UNet (MAUNet) in a unified deep model via an encoder-decoder structure for robust cell segmentation. The proposed MAUNet model enjoys several merits. First, the proposed instance-aware decode endows pixel features with better cell boundary discrimination capabilities benefiting from cell-wise distance field. And the ambiguity-aware decode aims at alleviating the domain gap caused by multimodality cell images credited to a customized anti-ambiguity proxy for domaininvariant learning. Second, we prepend the consistency regularization to enable exploration of unlabeled images, and a novel post-processing strategy to incorporate morphology prior to cell instance segmentation. Experimental results on the official validation set demonstrate the effectiveness of our method. },
 author = {Wangkai, Li and Zhaoyang, Li and Rui, Sun and Huayu, Mai and Naisong, Luo and Wang, Yuan and Yuwen, Pan and Guoxin, Xiong and Huakai, Lai and Zhiwei, Xiong and Tianzhu, Zhang},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--12},
 pdf = {https://proceedings.mlr.press/v212/wangkai23a/wangkai23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {MAUNet: Modality-Aware Anti-Ambiguity U-Net for Multi-Modality Cell Segmentation },
 url = {https://proceedings.mlr.press/v212/wangkai23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-xue23a,
 abstract = {Instance segmentation of multi-modality high-resolution microscopy images is an important task in computational pathology. We extended HoVer-Net[1], originally developed for segmentation and classification of nuclei in multi-Tissue histology images, to apply it under weakly supervised situation. According to the final tests, this modification also works for multi-modality microscopy.},
 author = {Xue, Ming},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--8},
 pdf = {https://proceedings.mlr.press/v212/xue23a/xue23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Weakly Supervised Cell Instance Segmentation for Multi-Modality Microscopy},
 url = {https://proceedings.mlr.press/v212/xue23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-yang23a,
 abstract = {Cell segmentation is one of the most fundamental tasks in the areas of medical image analysis, which assists in cell recognition and number counting. The seg- mentation results obtained will be poor due to the diverse cell morphology and the frequent presence of impurities in the cell pictures. In order to solve the cell segmentation which are from a competition held by Neural Information Processing Systems(NIPS), we present a network that combines attention gates with U-Net++ to segment varied sizes of cells. Using the feature filtering of the attention gate can adjust the convolution blockâs output, so as to improve the segmentation effect. The F1 score of our method reached 0.5874, Rank Running Time get 2.5431 seconds.},
 author = {Yang, Xinye and Chen, Hao},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--10},
 pdf = {https://proceedings.mlr.press/v212/yang23a/yang23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multi-modal Cell Segmentation based on U-Net++ and Attention Gate},
 url = {https://proceedings.mlr.press/v212/yang23a.html},
 volume = {212},
 year = {2023}
}

@inproceedings{pmlr-v212-zhang23a,
 abstract = {Cell segmentation is significant for downstream single-cell analysis in biological and biomedical research. Recently, image segmentation methods based on supervised learning have achieved promising results. However, most of them rely on intensive manual annotations, which are extremely time-consuming and expensive for cell segmentation. In addition, existing methods are often trained for a specific modality with poor generalization ability. In this paper, a novel semi-supervised cell segmentation method is proposed to segment microscopy images from multiple modalities. Specifically, Mean Teacher model is introduced to a multi-task learning framework, named Multi-task Mean Teacher (MT$^{2}$), in which both the classification and the regression heads are utilized to improve the prediction performance. Moreover, new data augmentation and multi-scale inference strategies are presented to enhance the robustness and generalization ability. For the quantitative evaluation on the Tuning Set of NeurIPS 2022 Cell Segmentation Competition, our method achieves the F1 Score of 0.8690, which demonstrates the effectiveness of the proposed semi-supervised learning method. Code is available at \url{https://github.com/djh-dzxw/MT2}},
 author = {Zhang, Binyu and Dong, Junhao and Zhao, Zhicheng and Meng, Zhu and Su, Fei},
 booktitle = {Proceedings of The Cell Segmentation Challenge in Multi-modality High-Resolution Microscopy Images},
 editor = {Ma, Jun and Xie, Ronald and Gupta, Anubha and Guilherme de Almeida, JosÃ© and Bader, Gary D. and Wang, Bo},
 month = {28 Nov--09 Dec},
 pages = {1--13},
 pdf = {https://proceedings.mlr.press/v212/zhang23a/zhang23a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {MT2: Multi-task Mean Teacher for Semi-Supervised Cell Segmentation},
 url = {https://proceedings.mlr.press/v212/zhang23a.html},
 volume = {212},
 year = {2023}
}
