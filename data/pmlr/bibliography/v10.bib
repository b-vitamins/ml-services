
@Proceedings{FSDM2010,
  title =     {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  editor =    {Huan Liu and Hiroshi Motoda and Rudy Setiono and Zheng Zhao},
  publisher = {PMLR},
  series =    {Proceedings of Machine Learning Research},
  volume =    10
}
@InProceedings{pmlr-v10-liu10a,
  title = 	 {Preface},
  author = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {1--3},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/liu10a/liu10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/liu10a.html},
  abstract = 	 {Preface to the Proceedings of the Fourth International Workshop on Feature Selection in Data Mining  June 21st, 2010, Hyderabad, India}
}
@InProceedings{pmlr-v10-liu10b,
  title = 	 {Feature Selection: An Ever Evolving Frontier in Data Mining},
  author = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {4--13},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/liu10b/liu10b.pdf},
  url = 	 {https://proceedings.mlr.press/v10/liu10b.html},
  abstract = 	 {The rapid advance of computer technologies in data processing, collection, and storage has provided unparalleled opportunities to expand capabilities in production, services, communications, and research. However, immense quantities of high-dimensional data renew the challenges to the state-of-the-art data mining techniques. Feature selection is an effective technique for dimension reduction and an essential step in successful data mining applications. It is a research area of great practical significance and has been developed and evolved to answer the challenges due to data of increasingly high dimensionality. Its direct benefits include: building simpler and more comprehensible models, improving data mining performance, and helping prepare, clean, and understand data. We first briefly introduce the key components of feature selection, and review its developments with the growth of data mining. We then overview FSDM and the papers of FSDM10, which showcases of a vibrant research field of some contemporary interests, new applications, and ongoing research efforts. We then examine nascent demands in data-intensive applications and identify some potential lines of research that require multidisciplinary efforts.}
}
@InProceedings{pmlr-v10-chawla10a,
  title = 	 {Feature Selection, Association Rules Network and Theory Building},
  author = 	 {Chawla, Sanjay},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {14--21},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/chawla10a/chawla10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/chawla10a.html},
  abstract = 	 {As the size and dimensionality of data sets increase, the task of feature selection has become increasingly important. In this paper we demonstrate how association rules can be used to build a network of features, which we refer to as an association rules network, to extract features from large data sets. Association rules network can play a fundamental role in *theory building* - which is a task common to all data sciences- statistics, machine learning and data mining.}
}
@InProceedings{pmlr-v10-salehi10a,
  title = 	 {A Statistical Implicative Analysis Based Algorithm and MMPC Algorithm for Detecting Multiple Dependencies},
  author = 	 {Salehi, Elham and Nyayachavadi, Jayashree and Gras, Robin},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {22--34},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/salehi10a/salehi10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/salehi10a.html},
  abstract = 	 {Discovering the dependencies among the variables of a domain from examples is an important problem in optimization. Many methods have been proposed for this purpose, but few large-scale evaluations were conducted. Most of these methods are based on measurements of conditional probability. The statistical implicative analysis offers another perspective of dependencies. It is important to compare the results obtained using this approach with one of the best methods currently available for this task: the MMPC heuristic. As the SIA is not used directly to address this problem, we designed an extension of it for our purpose. We conducted a large number of experiments by varying parameters such as the number of dependencies, the number of variables involved or the type of their distribution to compare the two approaches. The results show strong complementarities of the two methods.}
}
@InProceedings{pmlr-v10-zagoruiko10a,
  title = 	 {Attribute Selection Based on FRiS-Compactness},
  author = 	 {Zagoruiko, Nikolay and Borisova, Irina and Dyubanov, Vladimir and Kutnenko, Olga},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {35--44},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/zagoruiko10a/zagoruiko10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/zagoruiko10a.html},
  abstract = 	 {Commonly to classify new object in Data Mining one should estimate its similarity with given classes. Function of Rival Similarity (FRiS) is assigned to calculate quantitative measure of similarity considering a competitive situation. FRiS-function allows constructing new effective algorithms for various Data Mining tasks solving. In particular, it enables to obtain quantitative estimation of compactness of patterns which can be used as indirect criterion for informative attributes selection. FRiS-compactness predicts reliability of recognition of control sample more precisely, than such widespread methods as One-Leave-Out and Cross-Validation. Presented in the paper results of real genetic task solving confirm efficiency of FRiS-function using in attributes selection and decision rules construction.}
}
@InProceedings{pmlr-v10-esseghir10a,
  title = 	 {Effective Wrapper-Filter hybridization through GRASP Schemata},
  author = 	 {Esseghir, Mohamed Amir},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {45--54},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/esseghir10a/esseghir10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/esseghir10a.html},
  abstract = 	 {Of all of the challenges which face the selection of relevant features for predictive data mining or pattern recognition modeling, the adaptation of computational intelligence techniques to feature selection problem requirements is one of the primary impediments. A new improved metaheuristic based on \textitGreedy Randomized Adaptive Search Procedure (GRASP) is proposed for the problem of Feature Selection. Our devised optimization approach provides an effective scheme for wrapper-filter hybridization through the adaptation of GRASP components. The paper investigates, the GRASP component design as well as its adaptation to the feature selection problem. Carried out experiments showed Empirical effectiveness of the devised approach.}
}
@InProceedings{pmlr-v10-gorodetsky10a,
  title = 	 {Feature Extraction for Machine Learning: Logic-Probabilistic Approach},
  author = 	 {Gorodetsky, Vladimir and Samoylov, Vladimir},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {55--65},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/gorodetsky10a/gorodetsky10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/gorodetsky10a.html},
  abstract = 	 {The paper analyzes peculiarities of preprocessing of learning data represented in object data bases constituted by multiple relational tables with ontology on top of it. Exactly such learning data structures are peculiar to many novel challenging applications. The paper proposes a new technology supported by a number of novel algorithms intended for ontology-centered transformation of heterogeneous possibly poor structured learning data into homogeneous informative binary feature space based on (1) aggregation of the ontology notion instances and their attribute domains and subsequent probabilistic cause-consequence analysis aimed at extraction more informative features. The proposed technology is fully implemented and validated on several case studies.}
}
@InProceedings{pmlr-v10-nguyen10a,
  title = 	 {Feature Extraction for Outlier Detection in High-Dimensional Spaces},
  author = 	 {Nguyen, Hoang Vu and Gopalkrishnan, Vivekanand},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {66--75},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/nguyen10a/nguyen10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/nguyen10a.html},
  abstract = 	 {This work addresses the problem of feature extraction for boosting the performance of outlier detectors in high-dimensional spaces. Recent years have observed the prominence of multidimensional data on which traditional detection techniques usually fail to work as expected due to the curse of dimensionality. This paper introduces an efficient feature extraction method which brings nontrivial improvements in detection accuracy when applied on two popular detection techniques. Experiments carried out on real datasets demonstrate the feasibility of feature extraction in outlier detection.}
}
@InProceedings{pmlr-v10-sanasam10a,
  title = 	 {Feature Selection for Text Classification Based on Gini Coefficient of Inequality},
  author = 	 {Sanasam, Ranbir and Murthy, Hema and Gonsalves, Timothy},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {76--85},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/sanasam10a/sanasam10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/sanasam10a.html},
  abstract = 	 {A number of feature selection mechanisms have been explored in text categorization, among which mutual information, information gain and chi-square are considered most effective. In this paper, we study another method known as \it within class popularity to deal with feature selection based on the concept \it Gini coefficient of inequality (a commonly used measure of inequality of \textitincome). The proposed measure explores the relative distribution of a feature among different classes. From extensive experiments with four text classifiers over three datasets of different levels of heterogeneity, we observe that the proposed measure outperforms the mutual information, information gain and chi-square static with an average improvement of approximately 28.5%, 19% and 9.2% respectively.}
}
@InProceedings{pmlr-v10-jaiantilal10a,
  title = 	 {Increasing Feature Selection Accuracy for L1 Regularized Linear Models},
  author = 	 {Jaiantilal, Abhishek and Grudic, Gregory},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {86--96},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/jaiantilal10a/jaiantilal10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/jaiantilal10a.html},
  abstract = 	 {L1 (also referred to as the 1-norm or Lasso) penalty based formulations have been shown to be effective in problem domains when noisy features are present. However, the L1 penalty does not give favorable asymptotic properties with respect to feature selection, and has been shown to be inconsistent as a feature selection estimator; e.g. when noisy features are correlated with the relevant features. This can affect the estimation of the correct feature set, in certain domains like robotics, when both the number of examples and the number of features are large. The weighted lasso penalty by (Zou, 2006) has been proposed to rectify this problem of correct estimation of the feature set. This paper proposes a novel method for identifying problem specific L1 feature weights by utilizing the results from (Zou, 2006) and (Rocha et al., 2009) and is applicable to regression and classification algorithms. Our method increases the accuracy of L1 penalized algorithms through randomized experiments on subsets of the training data as a fast pre-processing step. We show experimental and theoretical results supporting the efficacy of the proposed method on two L1 penalized classification algorithms.}
}
@InProceedings{pmlr-v10-xie10a,
  title = 	 {Learning Dissimilarities for Categorical Symbols},
  author = 	 {Xie, Jierui and Szymanski, Boleslaw and Zaki, Mohammed},
  booktitle = 	 {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
  pages = 	 {97--106},
  year = 	 {2010},
  editor = 	 {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
  volume = 	 {10},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hyderabad, India},
  month = 	 {21 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v10/xie10a/xie10a.pdf},
  url = 	 {https://proceedings.mlr.press/v10/xie10a.html},
  abstract = 	 {In this paper we learn a dissimilarity measure for categorical data, for effective classification of the data points. Each categorical feature (with values taken from a finite set of symbols) is mapped onto a continuous feature whose values are real numbers. Guided by the classification error based on a nearest neighbor based technique, we repeatedly update the assignment of categorical symbols to real numbers to minimize this error. Intuitively, the algorithm pushes together points with the same class label, while enlarging the distances to points labeled differently. Our experiments show that 1) the learned dissimilarities improve classification accuracy by using the affinities of categorical symbols; 2) they outperform dissimilarities produced by previous data-driven methods; 3) our enhanced nearest neighbor classifier (called LD) based on the new space is competitive compared with classifiers such as decision trees, RBF neural networks, Naive Bayes and support vector machines, on a range of categorical datasets.}
}



