@proceedings{FSDM2010,
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Huan Liu and Hiroshi Motoda and Rudy Setiono and Zheng Zhao},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 volume = {10}
}

@inproceedings{pmlr-v10-chawla10a,
 abstract = {As the size and dimensionality of data sets increase, the task of feature selection has become increasingly important. In this paper we demonstrate how association rules can be used to build a network of features, which we refer to as an association rules network, to extract features from large data sets. Association rules network can play a fundamental role in theory building - which is a task common to all data sciences- statistics, machine learning and data mining. The process of carrying out research is undergoing a dramatic shift in the twenty rst century. The cause of the shift is due to the preponderance of data available in all almost all research disciplines. From anthropology to zoology, manufacturing to surveillance, all domains are witnessing an explosion of data. The availability of massive and cheap data has opened up the possibility of carrying out data-driven research and data mining is the discipline which provides tools and techniques for carrying out this endeavour. However much of these vast repositories of data generated are observational as opposed to experimental. Observational data is undirected and is often collected without any specic task in mind. For example, web servers generate a log of client activity. The web log can then be used for a myriad of tasks ranging from tracking search engine spiders to personalization of web sites. Experimental data, on the other hand, is directed and is generated to test a specic},
 address = {Hyderabad, India},
 author = {Chawla, Sanjay},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W131387386},
 pages = {14--21},
 pdf = {http://proceedings.mlr.press/v10/chawla10a/chawla10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feature Selection, Association Rules Network and Theory Building},
 url = {https://proceedings.mlr.press/v10/chawla10a.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-esseghir10a,
 abstract = {Of all of the challenges which face the selection of relevant features for predictive data mining or pattern recognition modeling, the adaptation of computational intelligence techniques to feature selection problem requirements is one of the primary impediments. A new improved metaheuristic based on Greedy Randomized Adaptive Search Procedure (GRASP) is proposed for the problem of Feature Selection. Our devised optimization approach provides an effective scheme for wrapper-filter hybridization through the adaptation of GRASP components. The paper investigates, the GRASP component design as well as its adaptation to the feature selection problem. Carried out experiments showed Empirical effectiveness of the devised approach.},
 address = {Hyderabad, India},
 author = {Esseghir, Mohamed Amir},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W205989289},
 pages = {45--54},
 pdf = {http://proceedings.mlr.press/v10/esseghir10a/esseghir10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Effective Wrapper-Filter hybridization through GRASP Schemata.},
 url = {https://proceedings.mlr.press/v10/esseghir10a.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-gorodetsky10a,
 abstract = {The paper analyzes peculiarities of preprocessing of learning data represented in object data bases constituted by multiple relational tables with ontology on top of it. Exactly such learning data structures are peculiar to many novel challenging applications. The paper proposes a new technology supported by a number of novel algorithms intended for ontology-centered transformation of heterogeneous possibly poor structured learning data into homogeneous informative binary feature space based on (1) aggregation of the ontology notion instances and their attribute domains and subsequent probabilistic causeconsequence analysis aimed at extraction more informative features. The proposed technology is fully implemented and validated on several case studies.},
 address = {Hyderabad, India},
 author = {Gorodetsky, Vladimir and Samoylov, Vladimir},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W2104519923},
 pages = {55--65},
 pdf = {http://proceedings.mlr.press/v10/gorodetsky10a/gorodetsky10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feature Extraction for Machine Learning: Logic{Probabilistic Approach},
 url = {https://proceedings.mlr.press/v10/gorodetsky10a.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-jaiantilal10a,
 abstract = {L1 (also referred to as the 1-norm or Lasso) penalty based formulations have been shown to be effective in problem domains when noisy features are present. However, the L1 penalty does not give favorable asymptotic properties with respect to feature selection, and has been shown to be inconsistent as a feature selection estimator; e.g. when noisy features are correlated with the relevant features. This can affect the estimation of the correct feature set, in certain domains like robotics, when both the number of examples and the number of features are large. The weighted lasso penalty by (Zou, 2006) has been proposed to rectify this problem of correct estimation of the feature set. This paper proposes a novel method for identifying problem specific L1 feature weights by utilizing the results from (Zou, 2006) and (Rocha et al., 2009) and is applicable to regression and classification algorithms. Our method increases the accuracy of L1 penalized algorithms through randomized experiments on subsets of the training data as a fast pre-processing step. We show experimental and theoretical results supporting the efficacy of the proposed method on two L1 penalized classification algorithms.},
 address = {Hyderabad, India},
 author = {Jaiantilal, Abhishek and Grudic, Gregory},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W1259247377},
 pages = {86--96},
 pdf = {http://proceedings.mlr.press/v10/jaiantilal10a/jaiantilal10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Increasing Feature Selection Accuracy for L1 Regularized Linear Models.},
 url = {https://proceedings.mlr.press/v10/jaiantilal10a.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-liu10a,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 address = {Hyderabad, India},
 author = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W4240465921},
 pages = {1--3},
 pdf = {http://proceedings.mlr.press/v10/liu10a/liu10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v10/liu10a.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-liu10b,
 abstract = {The rapid advance of computer technologies in data processing, collection, and storage has provided unparalleled opportunities to expand capabilities in production, services, communications, and research. However, immense quantities of high-dimensional data renew the challenges to the state-of-the-art data mining techniques. Feature selection is an eective technique for dimension reduction and an essential step in successful data mining applications. It is a research area of great practical signicance and has been developed and evolved to answer the challenges due to data of increasingly high dimensionality. Its direct benets include: building simpler and more comprehensible models, improving data mining performance, and helping prepare, clean, and understand data. We rst briey introduce the key components of feature selection, and review its developments with the growth of data mining. We then overview FSDM and the papers of FSDM10, which showcases of a vibrant research eld of some contemporary interests, new applications, and ongoing research eorts. We then examine nascent demands in data-intensive applications and identify some potential lines of research that require multidisciplinary eorts.},
 address = {Hyderabad, India},
 author = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W2157690157},
 pages = {4--13},
 pdf = {http://proceedings.mlr.press/v10/liu10b/liu10b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feature Selection: An Ever Evolving Frontier in Data Mining},
 url = {https://proceedings.mlr.press/v10/liu10b.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-nguyen10a,
 abstract = {This work addresses the problem of feature extraction for boosting the performance of outlier detectors in high-dimensional spaces. Recent years have observed the prominence of multidimensional data on which traditional detection techniques usually fail to work as expected due to the curse of dimensionality. This paper introduces an efficient feature extraction method which brings nontrivial improvements in detection accuracy when applied on two popular detection techniques. Experiments carried out on real datasets demonstrate the feasibility of feature extraction in outlier detection.},
 address = {Hyderabad, India},
 author = {Nguyen, Hoang Vu and Gopalkrishnan, Vivekanand},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W2127453665},
 pages = {66--75},
 pdf = {http://proceedings.mlr.press/v10/nguyen10a/nguyen10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feature Extraction for Outlier Detection in High-Dimensional Spaces},
 url = {https://proceedings.mlr.press/v10/nguyen10a.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-salehi10a,
 abstract = {Discovering the dependencies among the variables of a domain from examples is an important problem in optimization. Many methods have been proposed for this purpose, but few large-scale evaluations were conducted. Most of these methods are based on measurements of conditional probability. The statistical implicative analysis oers another perspective of dependencies. It is important to compare the results obtained using this approach with one of the best methods currently available for this task: the MMPC heuristic. As the SIA is not used directly to address this problem, we designed an extension of it for our purpose. We conducted a large number of experiments by varying parameters such as the number of dependencies, the number of variables involved or the type of their distribution to compare the two approaches. The results show strong complementarities of the two methods.},
 address = {Hyderabad, India},
 author = {Salehi, Elham and Nyayachavadi, Jayashree and Gras, Robin},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W2107359261},
 pages = {22--34},
 pdf = {http://proceedings.mlr.press/v10/salehi10a/salehi10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Statistical Implicative Analysis Based Algorithm and MMPC Algorithm for Detecting Multiple Dependencies},
 url = {https://proceedings.mlr.press/v10/salehi10a.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-sanasam10a,
 abstract = {A number of feature selection mechanisms have been explored in text categorization, among which mutual information, information gain and chi-square are considered most effective. In this paper, we study another method known as within class popularity to deal with feature selection based on the concept Gini coefficient of inequality (a commonly used measure of inequality of income). The proposed measure explores the relative distribution of a feature among different classes. From extensive experiments with four text classifiers over three datasets of different levels of heterogeneity, we observe that the proposed measure outperforms the mutual information, information gain and chi-square static with an average improvement of approximately 28.5%, 19% and 9.2% respectively.},
 address = {Hyderabad, India},
 author = {Sanasam, Ranbir and Murthy, Hema and Gonsalves, Timothy},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W2168939622},
 pages = {76--85},
 pdf = {http://proceedings.mlr.press/v10/sanasam10a/sanasam10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Feature Selection for Text Classification Based on Gini Coefficient of Inequality},
 url = {https://proceedings.mlr.press/v10/sanasam10a.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-xie10a,
 abstract = {In this paper we learn a dissimilarity measure for categorical data, for eective classication of the data points. Each categorical feature (with values taken from a nite set of symbols) is mapped onto a continuous feature whose values are real numbers. Guided by the classication error based on a nearest neighbor based technique, we repeatedly update the assignment of categorical symbols to real numbers to minimize this error. Intuitively, the algorithm pushes together points with the same class label, while enlarging the distances to points labeled dierently. Our experiments show that 1) the learned dissimilarities improve classication accuracy by using the anities of categorical symbols; 2) they outperform dissimilarities produced by previous data-driven methods; 3) our enhanced nearest neighbor classier (called LD) based on the new space is competitive compared with classiers such},
 address = {Hyderabad, India},
 author = {Xie, Jierui and Szymanski, Boleslaw and Zaki, Mohammed},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W2135368688},
 pages = {97--106},
 pdf = {http://proceedings.mlr.press/v10/xie10a/xie10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Dissimilarities for Categorical Symbols},
 url = {https://proceedings.mlr.press/v10/xie10a.html},
 volume = {10},
 year = {2010}
}

@inproceedings{pmlr-v10-zagoruiko10a,
 abstract = {Commonly to classify new object in Data Mining one should estimate its similarity with given classes. Function of Rival Similarity (FRiS) is assigned to calculate quantitative measure of similarity considering a competitive situation. FRiS-function allows constructing new effective algorithms for various Data Mining tasks solving. In particular, it enables to obtain quantitative estimation of compactness of patterns which can be used as indirect criterion for informative attributes selection. FRiS-compactness predicts reliability of recognition of control sample more precisely, than such widespread methods as One-LeaveOut and Cross-Validation. Presented in the paper results of real genetic task solving confirm efficiency of FRiS-function using in attributes selection and decision rules construction.},
 address = {Hyderabad, India},
 author = {Zagoruiko, Nikolay and Borisova, Irina and Dyubanov, Vladimir and Kutnenko, Olga},
 booktitle = {Proceedings of the Fourth International Workshop on Feature Selection in Data Mining},
 editor = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
 month = {21 Jun},
 openalex = {W2155756967},
 pages = {35--44},
 pdf = {http://proceedings.mlr.press/v10/zagoruiko10a/zagoruiko10a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Attribute Selection Based on FRiS-Compactness},
 url = {https://proceedings.mlr.press/v10/zagoruiko10a.html},
 volume = {10},
 year = {2010}
}
