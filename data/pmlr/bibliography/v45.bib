@proceedings{ACML2015,
 booktitle = {Proceedings of The 7th Asian Conference on Machine Learning},
 editor = {Geoffrey Holmes and Tie-Yan Liu},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of The 7th Asian Conference on Machine Learning},
 volume = {45}
}

@inproceedings{pmlr-v45-Antoniuk15,
 abstract = {In this paper we study statistical consistency of partial losses suitable for learning structured output predictors from examples containing missing labels. We provide sufficient conditions on data generating distribution which admit to prove that the expected risk of the structured predictor learned by minimizing the partial loss converges to the optimal Bayes risk defined by an associated complete loss. We define a concept of surrogate classification calibrated partial losses which are easier to optimize yet their minimization preserves the statistical consistency. We give some concrete examples of surrogate partial losses which are classification calibrated. In particular, we show that the ramp-loss which is in the core of many existing algorithms is classification calibrated.},
 address = {Hong Kong},
 author = {Antoniuk, Kostiantyn and Franc, Vojtech and Hlavac, Vaclav},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2308398441},
 pages = {81--95},
 pdf = {http://proceedings.mlr.press/v45/Antoniuk15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Consistency of structured output learning with missing labels},
 url = {https://proceedings.mlr.press/v45/Antoniuk15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Bao15,
 abstract = {Machine learning algorithms have been traditionally used to understand user behavior or system performance. In computer networks, with a subset of input features as controllable network parameters, we envision developing a data-driven network resource allocation framework that can optimize user experience. In particular, we explore how to leverage a classier learned from training instances to optimally guide network resource allocation to improve the overall performance on test instances. Based on logistic regression, we propose an optimal resource allocation algorithm, as well as heuristics with low-complexity. We evaluate the performance of the proposed algorithms using a synthetic Gaussian dataset, a real world dataset on video streaming over throttled networks, and a tier-one cellular operator’s customer complaint traces. The evaluation demonstrates the eectiveness of the},
 address = {Hong Kong},
 author = {Bao, Yanan and Liu, Xin and Pande, Amit},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2305625703},
 pages = {127--142},
 pdf = {http://proceedings.mlr.press/v45/Bao15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Data-Guided Approach for Learning and Improving User Experience in Computer Networks},
 url = {https://proceedings.mlr.press/v45/Bao15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Cheung15,
 abstract = {Proximal average (PA) is an approximation technique proposed recently to handle nonsmooth composite regularizer in empirical risk minimization problem. For nonsmooth composite regularizer, it is often difficult to directly derive the corresponding proximal update when solving with popular proximal update. While traditional approaches resort to complex splitting methods like ADMM, proximal average provides an alternative, featuring the tractability of implementation and theoretical analysis. Nevertheless, compared to SDCA-ADMM and SAG-ADMM which are examples of ADMM-based methods achieving faster convergence rate and low per-iteration complexity, existing PA-based approaches either converge slowly (e.g. PA-ASGD) or suffer from high per-iteration cost (e.g. PA-APG). In this paper, we therefore propose a new PA-based algorithm called PA-SAGA, which is optimal in both convergence rate and per-iteration cost, by incorporating into incremental gradient-based framework. },
 address = {Hong Kong},
 author = {Cheung, Yiu-ming and Lou, Jian},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2306626342},
 pages = {205--220},
 pdf = {http://proceedings.mlr.press/v45/Cheung15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proximal Average Approximated Incremental Gradient Method for Composite Penalty Regularized Empirical Risk Minimization},
 url = {https://proceedings.mlr.press/v45/Cheung15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Christoffel15,
 abstract = {We consider the problem of estimating the class prior in an unlabeled dataset. Under the assumption that an additional labeled dataset is available, the class prior can be estimated by fitting a mixture of class-wise data distributions to the unlabeled data distribution. However, in practice, such an additional labeled dataset is often not available. In this paper, we show that, with additional samples coming only from the positive class, the class prior of the unlabeled dataset can be estimated correctly. Our key idea is to use properly penalized divergences for model fitting to cancel the error caused by the absence of negative samples. We further show that the use of the penalized $L_1$-distance gives a computationally efficient algorithm with an analytic solution. The consistency, stability, and estimation error are theoretically analyzed. Finally, we experimentally demonstrate the usefulness of the proposed method.},
 address = {Hong Kong},
 author = {Christoffel, Marthinus and Niu, Gang and Sugiyama, Masashi},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W3104154887},
 pages = {221--236},
 pdf = {http://proceedings.mlr.press/v45/Christoffel15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Class-prior estimation for learning from positive and unlabeled data},
 url = {https://proceedings.mlr.press/v45/Christoffel15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Dai15,
 abstract = {During the past decade, Statistical Relational Learning (SRL) and Probabilistic Inductive Logic Programming (PILP), owing to their strength in capturing structure information, have attracted much attention for learning relational models such as weighted logic rules. Typically, a generative model is assumed for the structured joint distribution, and the learning process is accomplished in an enormous relational space. In this paper, we propose a new framework, i.e., Statistical Unfolded Logic (SUL) learning. In contrast to learning rules in the relational space directly, SUL propositionalizes the structure information into an attribute-value data set, and thus, statistical discriminative learning which is much more ecient than generative relational learning can be executed. In addition to achieving better generalization performance, SUL is able to conduct predicate invention that is hard to be realized by traditional SRL and PILP approaches. Experiments on real tasks show that our proposed approach is superior to state-of-the-art weighted rules learning approaches.},
 address = {Hong Kong},
 author = {Dai, Wang-Zhou and Zhou, Zhi-Hua},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2320781667},
 pages = {349--361},
 pdf = {http://proceedings.mlr.press/v45/Dai15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Statistical Unfolded Logic Learning},
 url = {https://proceedings.mlr.press/v45/Dai15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Horev15,
 abstract = {Symmetric positive definite (SPD) matrices in the form of covariance matrices, for example, are ubiquitous in machine learning applications. However, because their size grows quadratically with respect to the number of variables, high-dimensionality can pose a difficulty when working with them. So, it may be advantageous to apply to them dimensionality reduction techniques. Principal component analysis (PCA) is a canonical tool for dimensionality reduction, which for vector data maximizes the preserved variance. Yet, the commonly used, naive extensions of PCA to matrices result in sub-optimal variance retention. Moreover, when applied to SPD matrices, they ignore the geometric structure of the space of SPD matrices, further degrading the performance. In this paper we develop a new Riemannian geometry based formulation of PCA for SPD matrices that (1) preserves more data variance by appropriately extending PCA to matrix data, and (2) extends the standard definition from the Euclidean to the Riemannian geometries. We experimentally demonstrate the usefulness of our approach as pre-processing for EEG signals and for texture image classification.},
 address = {Hong Kong},
 author = {Horev, Inbal and Yger, Florian and Sugiyama, Masashi},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2554291319},
 pages = {1--16},
 pdf = {http://proceedings.mlr.press/v45/Horev15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Geometry-aware principal component analysis for symmetric positive definite matrices},
 url = {https://proceedings.mlr.press/v45/Horev15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Huynh15,
 abstract = {Bayesian nonparametric models are theoretically suitable to learn streaming data due to their complexity relaxation to the volume of observed data. However, most of the existing variational inference algorithms are not applicable to streaming applications since they re-quire truncation on variational distributions. In this paper, we present two truncation-free variational algorithms, one for mix-membership inference called TFVB (truncation-free variational Bayes), and the other for hard clustering inference called TFME (truncation-free maximization expectation). With these algorithms, we further developed a streaming learning framework for the popular Dirichlet process mixture (DPM) models. Our ex-periments demonstrate the usefulness of our framework in both synthetic and real-world data.},
 address = {Hong Kong},
 author = {Huynh, Viet and Phung, Dinh and Venkatesh, Svetha},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2301970748},
 pages = {237--252},
 pdf = {http://proceedings.mlr.press/v45/Huynh15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Streaming variational inference for dirichlet process mixtures},
 url = {https://proceedings.mlr.press/v45/Huynh15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Irsoy15,
 abstract = {We discuss an autoencoder model in which the encoding and decoding functions are implemented by decision trees. We use the soft decision tree where internal nodes realize soft multivariate splits given by a gating function and the overall output is the average of all leaves weighted by the gating values on their path. The encoder tree takes the input and generates a lower dimensional representation in the leaves and the decoder tree takes this and reconstructs the original input. Exploiting the continuity of the trees, autoencoder trees are trained with stochastic gradient descent. On handwritten digit and news data, we see that the autoencoder trees yield good reconstruction error compared to traditional autoencoder perceptrons. We also see that the autoencoder tree captures hierarchical representations at different granularities of the data on its different levels and the leaves capture the localities in the input space.},
 address = {Hong Kong},
 author = {Ä°rsoy, Ozan and Alpaydin, Ethem},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W3037321046},
 pages = {378--390},
 pdf = {http://proceedings.mlr.press/v45/Irsoy15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Autoencoder Trees},
 url = {https://proceedings.mlr.press/v45/Irsoy15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Kaban15a,
 abstract = {We provide a non-asymptotic analysis of the generalisation error of compressive Fisher linear discriminant (FLD) classication that is dimension free under mild assumptions. Our analysis includes the eects that random projection has on classication performance under covariance model misspecication, as well as various good and bad eects of random projections that contribute to the overall performance of compressive FLD. We also give an asymptotic bound as a corollary of our nite sample result. An important ingredient of our analysis is to develop new dimension-free bounds on the largest and smallest eigenvalue of the compressive covariance, which may be of independent interest.},
 address = {Hong Kong},
 author = {Kaban, Ata},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2330931980},
 pages = {17--32},
 pdf = {http://proceedings.mlr.press/v45/Kaban15a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Non-asymptotic Analysis of Compressive Fisher Discriminants in terms of the Effective Dimension},
 url = {https://proceedings.mlr.press/v45/Kaban15a.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Kaban15b,
 abstract = {It is well known that in general, the nearest neighbour rule (NN) has sample complexity that is exponential in the input space dimension d when only smoothness is assumed on the label posterior function. Here we consider NN on randomly projected data, and we show that, if the input domain has a small size, then the sample complexity becomes exponential in the metric entropy integral of the set of normalised chords of the input domain. This metric entropy integral measures the complexity of the input domain, and can be much smaller thand { for instance in cases when the data lies in a linear or a smooth nonlinear subspace of the ambient space, or when it has a sparse representation. We then show that the guarantees we obtain for the compressive NN also hold for the dataspace NN in bounded domains; thus the random projection takes the role of an analytic tool to identify benign structures under which NN learning is possible from a small sample size. Numerical simulations on data designed to have intrinsically low complexity conrm our theoretical ndings, and display a striking agreement in the empirical performances of compressive NN and dataspace NN. This suggests that high dimensional data sets that have a low complexity underlying structure are well suited for computationally cheap compressive NN learning.},
 address = {Hong Kong},
 author = {Kaban, Ata},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2307023955},
 pages = {65--80},
 pdf = {http://proceedings.mlr.press/v45/Kaban15b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A New Look at Nearest Neighbours: Identifying Benign Input Geometries via Random Projections},
 url = {https://proceedings.mlr.press/v45/Kaban15b.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Ko15,
 abstract = {The Poisson likelihood with rectied linear function as non-linearity is a physically plausible model to discribe the stochastic arrival process of photons or other particles at a detector. At low emission rates the discrete nature of this process leads to measurement noise that behaves very dierently from additive white Gaussian noise. To address the intractable inference problem for such models, we present a novel ecient and robust Expectation Propagation algorithm entirely based on analytically tractable computations operating reliably in regimes where quadrature based implementations can fail. Full posterior inference therefore becomes an attractive alternative in areas generally dominated by methods of point estimation. Moreover, we discuss the rectied linear function in the context of other},
 address = {Hong Kong},
 author = {Ko, Young-Jun and Seeger, Matthias W.},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2247147043},
 pages = {253--268},
 pdf = {http://proceedings.mlr.press/v45/Ko15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Expectation Propagation for Rectified Linear Poisson Regression},
 url = {https://proceedings.mlr.press/v45/Ko15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Kondo15,
 abstract = {A common strategy for sparse linear regression is to introduce regularization, which eliminates irrelevant features by letting the corresponding weights be zeros. However, regularization often shrinks the estimator for relevant features, which leads to incorrect feature selection. Motivated by the above-mentioned issue, we propose Bayesian masking (BM), a sparse estimation method which imposes no regularization on the weights. The key concept of BM is to introduce binary latent variables that randomly mask features. Estimating the masking rates determines the relevance of the features automatically. We derive a variational Bayesian inference algorithm that maximizes the lower bound of the factorized information criterion (FIC), which is a recently developed asymptotic criterion for evaluating the marginal log-likelihood. In addition, we propose reparametrization to accelerate the convergence of the derived algorithm. Finally, we show that BM outperforms Lasso and automatic relevance determination (ARD) in terms of the sparsity-shrinkage trade-off.},
 address = {Hong Kong},
 author = {Kondo, Yohei and Maeda, Shin-ichi and Hayashi, Kohei},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2223242284},
 pages = {49--64},
 pdf = {http://proceedings.mlr.press/v45/Kondo15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias},
 url = {https://proceedings.mlr.press/v45/Kondo15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Kotlowski15,
 abstract = {We consider optimization of generalized performance metrics for binary classification by means of surrogate losses. We focus on a class of metrics, which are linear-fractional functions of the false positive and false negative rates (examples of which include $$F_{\beta }$$ -measure, Jaccard similarity coefficient, AM measure, and many others). Our analysis concerns the following two-step procedure. First, a real-valued function f is learned by minimizing a surrogate loss for binary classification on the training sample. It is assumed that the surrogate loss is a strongly proper composite loss function (examples of which include logistic loss, squared-error loss, exponential loss, etc.). Then, given f, a threshold $$\widehat{\theta }$$ is tuned on a separate validation sample, by direct optimization of the target performance metric. We show that the regret of the resulting classifier (obtained from thresholding f on $$\widehat{\theta }$$ ) measured with respect to the target metric is upperbounded by the regret of f measured with respect to the surrogate loss. We also extend our results to cover multilabel classification and provide regret bounds for micro- and macro-averaging measures. Our findings are further analyzed in a computational study on both synthetic and real data sets.},
 address = {Hong Kong},
 author = {Kotlowski, Wojciech and DembczyÅski, Krzysztof},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2285335739},
 pages = {301--316},
 pdf = {http://proceedings.mlr.press/v45/Kotlowski15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Surrogate regret bounds for generalized classification performance metrics},
 url = {https://proceedings.mlr.press/v45/Kotlowski15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Li15,
 abstract = {ABSTRACT We develop a state‐of‐the‐art fraud prediction model using a machine learning approach. We demonstrate the value of combining domain knowledge and machine learning methods in model building. We select our model input based on existing accounting theories, but we differ from prior accounting research by using raw accounting numbers rather than financial ratios. We employ one of the most powerful machine learning methods, ensemble learning, rather than the commonly used method of logistic regression. To assess the performance of fraud prediction models, we introduce a new performance evaluation metric commonly used in ranking problems that is more appropriate for the fraud prediction task. Starting with an identical set of theory‐motivated raw accounting numbers, we show that our new fraud prediction model outperforms two benchmark models by a large margin: the Dechow et al. logistic regression model based on financial ratios, and the Cecchini et al. support‐vector‐machine model with a financial kernel that maps raw accounting numbers into a broader set of ratios.},
 address = {Hong Kong},
 author = {Li, Bin and Yu, Julia and Zhang, Jie and Ke, Bin},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W3125433491},
 pages = {173--188},
 pdf = {http://proceedings.mlr.press/v45/Li15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Detecting Accounting Fraud in Publicly Traded U.S. Firms Using a Machine Learning Approach},
 url = {https://proceedings.mlr.press/v45/Li15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Liu15,
 abstract = {A preference relation-based Top-N recommendation approach is proposed to capture both second-order and higher-order interactions among users and items. Traditionally Top-N recommendation was achieved by predicting the item ratings first, and then inferring the item rankings, based on the assumption of availability of explicit feedback such as ratings, and the assumption that optimizing the ratings is equivalent to optimizing the item rankings. Nevertheless, both assumptions are not always true in real world applications. The proposed approach drops these assumptions by exploiting preference relations, a more practical user feedback. Furthermore, the proposed approach enjoys the representational power of Markov Random Fields thus side information such as item and user attributes can be easily incorporated. Comparing to related work, the proposed approach has the unique property of modeling both second-order and higher-order interactions among users and items. To the best of our knowledge, this is the first time both types of interactions have been captured in preference-relation based methods. Experimental results on public datasets demonstrate that both types of interactions have been properly captured, and significantly improved Top-N recommendation performance has been achieved.},
 address = {Hong Kong},
 author = {Liu, Shaowu and Li, Gang and Tran, Truyen and Jiang, Yuan},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2551968180},
 pages = {157--172},
 pdf = {http://proceedings.mlr.press/v45/Liu15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preference Relation-based Markov Random Fields for Recommender Systems},
 url = {https://proceedings.mlr.press/v45/Liu15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Nguyen15,
 abstract = {Supervised learning in machine learning concerns inferring an underlying relation between covariate x and target y based on training covariate-target data. It is traditionally assumed that training data and test data, on which the generalization performance of a learning algorithm is measured, follow the same probability distribution. However, this standard assumption is often violated in many real-world applications such as computer vision, natural language processing, robot control, or survey design, due to intrinsic non-stationarity of the environment or inevitable sample selection bias. This situation is called dataset shift and has attracted a great deal of attention recently. In the paper, we consider supervised learning problems under the target shift scenario, where the target marginal distribution p(y) changes between the training and testing phases, while the target-conditioned covariate distribution p(x|y) remains unchanged. Although various methods for mitigating target shift in classification (a.k.a. class prior change) have been developed so far, few methods can be applied to continuous targets. In this paper, we propose methods for continuous target shift adaptation in regression and conditional density estimation. More specifically, our contribution is a novel importance weight estimator for continuous targets. Through experiments, the usefulness of the proposed method is demonstrated.},
 address = {Hong Kong},
 author = {Nguyen, Tuan Duong and Christoffel, Marthinus and Sugiyama, Masashi},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2333272926},
 pages = {285--300},
 pdf = {http://proceedings.mlr.press/v45/Nguyen15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Continuous Target Shift Adaptation in Supervised Learning},
 url = {https://proceedings.mlr.press/v45/Nguyen15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Niu15,
 abstract = {Distributed word representations have achieved great success in natural language processing (NLP) area. However, most distributed models focus on local context properties and learn task-specific representations individually, therefore lack the ability to fuse multi-attributes and learn jointly. In this paper, we propose a unified framework which jointly learns distributed representations of word and attributes: characteristics of word. In our models, we consider three types of attributes: topic, lemma and document. Besides learning distributed attribute representations, we find that using additional attributes is beneficial to improve word representations. Several experiments are conducted to evaluate the performance of the learned topic representations, document representations, and improved word representations, respectively. The experimental results show that our models achieve significant and competitive results. },
 address = {Hong Kong},
 author = {Niu, Liqiang and Dai, Xin-Yu and Huang, Shujian and Chen, Jiajun},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2311049292},
 pages = {143--156},
 pdf = {http://proceedings.mlr.press/v45/Niu15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Unified Framework for Jointly Learning Distributed Representations of Word and Attributes},
 url = {https://proceedings.mlr.press/v45/Niu15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-preface,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 address = {Hong Kong},
 author = {Holmes, Geoffrey and Liu, Tie-Yan},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W4240465921},
 pages = {i--xx},
 pdf = {http://proceedings.mlr.press/v45/preface.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v45/preface.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Sankar15,
 abstract = {Energy-based deep learning models like Restricted Boltzmann Machines are increasingly used for real-world applications. However, all these models inherently depend on the Contrastive Divergence (CD) method for training and maximization of log likelihood of generating the given data distribution. CD, which internally uses Gibbs sampling, often does not perform well due to issues such as biased samples, poor mixing of Markov chains and highmass probability modes. Variants of CD such as PCD, Fast PCD and Tempered MCMC have been proposed to address this issue. In this work, we propose a new approach to CDbased methods, called Diss-CD, which uses dissimilar data to allow the Markov chain to explore new modes in the probability space. This method can be used with all variants of CD (or PCD), and across all energy-based deep learning models. Our experiments on using this approach on standard datasets including MNIST, Caltech-101 Silhouette and Synthetic Transformations, demonstrate the promise of this approach, showing fast convergence of error in learning and also a better approximation of log likelihood of the data.},
 address = {Hong Kong},
 author = {Sankar, Adepu Ravi and Balasubramanian, Vineeth N},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2319974058},
 pages = {391--406},
 pdf = {http://proceedings.mlr.press/v45/Sankar15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Similarity-based Contrastive Divergence Methods for Energy-based Deep Learning Models},
 url = {https://proceedings.mlr.press/v45/Sankar15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Sasaki15,
 abstract = {Sufficient dimension reduction (SDR) is a framework of supervised linear dimension reduction, and is aimed at finding a low-dimensional orthogonal projection matrix for input data such that the projected input data retains maximal information on output data. A computationally efficient approach employs gradient estimates of the conditional density of the output given input data to find an appropriate projection matrix. However, since the gradients of the conditional densities are typically estimated by a local linear smoother, it does not perform well when the input dimensionality is high. In this paper, we propose a novel estimator of the gradients of logarithmic conditional densities called the \emphleast-squares logarithmic conditional density gradients (LSLCG), which fits a gradient model \emphdirectly to the true gradient without conditional density estimation under the squared loss. Thanks to the simple least-squares formulation, LSLCG gives a closed-form solution that can be computed efficiently. In addition, all the parameters can be automatically determined by cross-validation. Through experiments on a large variety of artificial and benchmark datasets, we demonstrate that the SDR method based on LSLCG outperforms existing SDR methods both in estimation accuracy and computational efficiency.},
 address = {Hong Kong},
 author = {Sasaki, Hiroaki and Tangkaratt, Voot and Sugiyama, Masashi},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2969813877},
 pages = {33--48},
 pdf = {http://proceedings.mlr.press/v45/Sasaki15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Sufficient Dimension Reduction via Direct Estimation of the Gradients of Logarithmic Conditional Densities},
 url = {https://proceedings.mlr.press/v45/Sasaki15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Shu15,
 abstract = {Multi-view clustering takes diversity of multiple views (representations) into consideration. Multiple views may be obtained from various sources or dierent feature subsets and often provide complementary information to each other. In this paper, we propose a novel graph-based approach to integrate multiple representations to improve clustering performance. While original graphs have been widely used in many existing multi-view clustering approaches, the key idea of our approach is to integrate multiple views by exploring higher order information. In particular, given graphs constructed separately from single view data, we build cross-view tensor product graphs (TPGs), each of which is a Kronecker product of a pair of single-view graphs. Since each cross-view TPG captures higher order relationships of data under two dierent views, it is no surprise that we obtain more reliable similarities. We linearly combine multiple cross-view TPGs to integrate higher order information. Ecient graph diusion process on the fusion TPG helps to reveal the underlying cluster structure and boosts the clustering performance. Empirical study shows that the proposed approach outperforms state-of-the-art methods on benchmark datasets.},
 address = {Hong Kong},
 author = {Shu, Le and Latecki, Longin Jan},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2316282451},
 pages = {362--377},
 pdf = {http://proceedings.mlr.press/v45/Shu15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Integration of Single-view Graphs with Diffusion of Tensor Product Graphs for Multi-view Spectral Clustering},
 url = {https://proceedings.mlr.press/v45/Shu15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Xia15,
 abstract = {We study the budgeted bandit problem, where each arm is associated with both a reward and a cost. In a budgeted bandit problem, the objective is to design an arm pulling algorithm in order to maximize the total reward before the budget runs out. In this work, we study both multi-armed bandits and linear bandits, and focus on the setting with continuous random costs. We propose an upper condence bound based algorithm for multi-armed bandits and a condence ball based algorithm for linear bandits, and prove logarithmic regret bounds for both algorithms. We conduct simulations on the proposed algorithms, which verify the eectiveness of our proposed algorithms.},
 address = {Hong Kong},
 author = {Xia, Yingce and Ding, Wenkui and Zhang, Xu-Dong and Yu, Nenghai and Qin, Tao},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2298590021},
 pages = {317--332},
 pdf = {http://proceedings.mlr.press/v45/Xia15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Budgeted Bandit Problems with Continuous Random Costs},
 url = {https://proceedings.mlr.press/v45/Xia15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Yu15,
 abstract = {Partial label learning aims to learn from training examples each associated with a set of candidate labels, among which only one label is valid for the training example. The basic strategy to learn from partial label examples is disambiguation, i.e. by trying to recover the ground-truth labeling information from the candidate label set. As one of the popular machine learning paradigms, maximum margin techniques have been employed to solve the partial label learning problem. Existing attempts perform disambiguation by optimizing the margin between the maximum modeling output from candidate labels and that from non-candidate ones. Nonetheless, this formulation ignores considering the margin between the ground-truth label and other candidate labels. In this paper, a new maximum margin formulation for partial label learning is proposed which directly optimizes the margin between the ground-truth label and all other labels. Specifically, the predictive model is learned via an alternating optimization procedure which coordinates the task of ground-truth label identification and margin maximization iteratively. Extensive experiments on artificial as well as real-world datasets show that the proposed approach is highly competitive to other well-established partial label learning approaches.},
 address = {Hong Kong},
 author = {Yu, Fei and Zhang, Min-Ling},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2338068721},
 pages = {96--111},
 pdf = {http://proceedings.mlr.press/v45/Yu15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Maximum margin partial label learning},
 url = {https://proceedings.mlr.press/v45/Yu15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Zhang15a,
 abstract = {We consider the problem of multivariate linear regression with a small fraction of the responses being missing and grossly corrupted, where the magnitudes and locations of such occurrences are not known in priori. This is addressed in our approach by explicitly taking into account the error source and its sparseness nature. Moreover, our approach allows each regression task to possess its distinct noise level. We also propose a new algorithm that is theoretically shown to always converge to the optimal solution of its induced non-smooth optimization problem. Experiments on controlled simulations suggest the competitiveness of our algorithm comparing to existing multivariate regression models. In particular, we apply our model to predict the Big-Five personality from user behaviors at Social Network Sites (SNSs) and microblogs, an important yet dicult problem in psychology, where empirical results demonstrate its superior performance with respect to related learning methods.},
 address = {Hong Kong},
 author = {Zhang, Xiaowei and Cheng, Li and Zhu, Tingshao},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2308743088},
 pages = {112--126},
 pdf = {http://proceedings.mlr.press/v45/Zhang15a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Robust Multivariate Regression with Grossly Corrupted Observations and Its Application to Personality Prediction},
 url = {https://proceedings.mlr.press/v45/Zhang15a.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Zhang15b,
 abstract = {Due to their open and anonymous nature, online social networks are particularly vulnerable to Sybil attacks. In recent years, there has been a rising interest in leveraging social network topological structures to combat Sybil attacks. Unfortunately, due to their strong dependency on unrealistic assumptions, existing graph-based Sybil defense mechanisms suffer from high false detection rates. In this paper, we focus on enhancing those mechanisms by considering additional graph structural information underlying social networks. Our solutions are based on our novel understanding and interpretation of Sybil detection as the problem of partially labeled classication. Specically, we rst propose an eective graph pruning technique to enhance the robustness of existing Sybil defense mechanisms against target attacks, by utilizing the local structural similarity between neighboring nodes in a social network. Second, we design a domain-specic graph regularization method to further improve the performance of those mechanisms by exploiting the relational property of the social network. Experimental results on four popular online social network datasets demonstrate that our proposed techniques can signicantly improve the detection accuracy},
 address = {Hong Kong},
 author = {Zhang, Huanhuan and Zhang, Jie and Fung, Carol and Xu, Chang},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2299294434},
 pages = {189--204},
 pdf = {http://proceedings.mlr.press/v45/Zhang15b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Improving Sybil Detection via Graph Pruning and Regularization Techniques},
 url = {https://proceedings.mlr.press/v45/Zhang15b.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Zhao15a,
 abstract = {I Bayesian Network (BN) . A directed acyclic graph (DAG) where nodes are random variables and directed edges represent probability dependencies among variables I BN Structure Learning . Firstly construct the topology (structure) of the network . Then estimate the parameters (CPDs) given the fixed structure I Curriculum Learning (CL) [Yoshua Bengio et al. ICML 2009 ] . Ideas: learn with the simpler samples or easier tasks as the start . Definition: a curriculum is a sequence of weighting schemes of the training data 〈W1,W2, . . . ,Wn〉, where W1 assigns more weight to easier samples, then each next scheme assigns more weight to harder samples, at last Wn assigns uniform weight to all samples},
 address = {Hong Kong},
 author = {Zhao, Yanpeng and Chen, Yetian and Tu, Kewei and Tian, Jin},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2306514254},
 pages = {269--284},
 pdf = {http://proceedings.mlr.press/v45/Zhao15a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Curriculum Learning of Bayesian Network Structures},
 url = {https://proceedings.mlr.press/v45/Zhao15a.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Zhao15b,
 abstract = {Policy gradient algorithms are widely used in reinforcement learning problems with continuous action spaces, which update the policy parameters along the steepest direction of the expected return. However, large variance of policy gradient estimation often causes instability of policy update. In this paper, we propose to suppress the variance of gradient estimation by directly employing the variance of policy gradients as a regularizer. Through experiments, we demonstrate that the proposed variance-regularization technique combined with parameter-based exploration and baseline subtraction provides more reliable policy updates than non-regularized counterparts.},
 address = {Hong Kong},
 author = {Zhao, Tingting and Niu, Gang and Xie, Ning and Yang, Jucheng and Sugiyama, Masashi},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2338705551},
 pages = {333--348},
 pdf = {http://proceedings.mlr.press/v45/Zhao15b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Regularized Policy Gradients: Direct Variance Reduction in Policy Gradient Estimation},
 url = {https://proceedings.mlr.press/v45/Zhao15b.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Zhou15,
 abstract = {Instance-transfer learning has emerged as a promising learning framework to boost performance of prediction models on newly-arrived tasks. The success of the framework depends on the relevance of the source data to the target data. This paper proposes a new approach to source data selection for instance-transfer learning. The approach is capable of selecting the largest subset S of the source data which relevance to the target data is statistically guaranteed to be the highest among any superset ofS . The approach is formally described and theoretically justied. Experimental results on real-world data sets demonstrate that},
 address = {Hong Kong},
 author = {Zhou, Shuang and Schoenmakers, Gijs and Smirnov, Evgueni and Peeters, Ralf and Driessens, Kurt and Chen, Siqi},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2300378260},
 pages = {423--438},
 pdf = {http://proceedings.mlr.press/v45/Zhou15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Largest Source Subset Selection for Instance Transfer},
 url = {https://proceedings.mlr.press/v45/Zhou15.html},
 volume = {45},
 year = {2016}
}

@inproceedings{pmlr-v45-Zhu15,
 abstract = {Multi-view learning has been an important learning paradigm where data come from multiple channels or appear in multiple modalities. Many approaches have been developed in this eld, and have achieved better performance than single-view ones. Those approaches, however, always work on small-size datasets with low dimensionality, owing to their high computational cost. In recent years, it has been witnessed that many applications involve large-scale multi-view data, e.g., hundreds of hours of video (including visual, audio and text views) is uploaded to YouTube every minute, bringing a big challenge to previous multi-view algorithms. This work concentrates on the large-scale multi-view learning for classication and proposes the One-Pass Multi-View (OPMV) framework which goes through the training data only once without storing the entire training examples. This approach jointly optimizes the composite objective functions with consistency linear constraints for dierent views. We verify, both theoretically and empirically, the eectiveness of the proposed algorithm.},
 address = {Hong Kong},
 author = {Zhu, Yue and Gao, Wei and Zhou, Zhi-Hua},
 booktitle = {Asian Conference on Machine Learning},
 editor = {Holmes, Geoffrey and Liu, Tie-Yan},
 month = {20--22 Nov},
 openalex = {W2307479074},
 pages = {407--422},
 pdf = {http://proceedings.mlr.press/v45/Zhu15.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {One-Pass Multi-View Learning},
 url = {https://proceedings.mlr.press/v45/Zhu15.html},
 volume = {45},
 year = {2016}
}
