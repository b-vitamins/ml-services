@proceedings{FAT*2018,
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Sorelle A. Friedler and Christo Wilson},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 volume = {81}
}

@inproceedings{pmlr-v81-barabas18a,
 abstract = {Actuarial risk assessments might be unduly perceived as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, resting on the contested use of variables that act as proxies for characteristics legally protected against discrimination, such as race and gender. We argue that a core ethical debate surrounding the use of regression in risk assessments is not simply one of bias or accuracy. Rather, it's one of purpose. If machine learning is operationalized merely in the service of predicting individual future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, but rather to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation.},
 author = {Barabas, Chelsea and Virza, Madars and Dinakar, Karthik and Ito, Joichi and Zittrain, Jonathan},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2964276035},
 pages = {62--76},
 pdf = {http://proceedings.mlr.press/v81/barabas18a/barabas18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment},
 url = {https://proceedings.mlr.press/v81/barabas18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-binns18a,
 abstract = {What does it mean for a machine learning model to be `fair', in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise `fairness' in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.},
 author = {Binns, Reuben},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2963808661},
 pages = {149--159},
 pdf = {http://proceedings.mlr.press/v81/binns18a/binns18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Fairness in Machine Learning: Lessons from Political Philosophy},
 url = {https://proceedings.mlr.press/v81/binns18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-buolamwini18a,
 abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
 author = {Buolamwini, Joy and Gebru, Timnit},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2788481061},
 pages = {77--91},
 pdf = {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
 url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-burke18a,
 abstract = {Fairness has emerged as an important category of analysis for machine learning systems in some application areas. In extending the concept of fairness to recommender systems, there is an essential tension between the goals of fairness and those of personalization. However, there are contexts in which  equity across recommendation outcomes is a desirable goal. It is also the case that in some applications fairness may be a multisided concept, in which the impacts on multiple groups of individuals must be considered. In this paper, we examine two different cases of fairness-aware recommender systems: consumer-centered and provider-centered. We  explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes. We show that a modified version of the Sparse Linear Method (SLIM) can be used to improve the balance of user and item neighborhoods, with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance.},
 author = {Burke, Robin and Sonboli, Nasim and Ordonez-Gauger, Aldo},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2793995607},
 pages = {202--214},
 pdf = {http://proceedings.mlr.press/v81/burke18a/burke18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Balanced Neighborhoods for Multi-sided Fairness in Recommendation},
 url = {https://proceedings.mlr.press/v81/burke18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-chouldechova18a,
 abstract = {Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communitiesâsuch as those in poverty or from particular racial and ethnic groupsâwill be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.},
 author = {Chouldechova, Alexandra and Benavides-Prado, Diana and Fialko, Oleksandr and Vaithianathan, Rhema},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2790628304},
 pages = {134--148},
 pdf = {http://proceedings.mlr.press/v81/chouldechova18a/chouldechova18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions},
 url = {https://proceedings.mlr.press/v81/chouldechova18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-datta18a,
 abstract = {We explore ways in which discrimination may arise in the targeting of job-related advertising, noting the potential for multiple parties to contribute to its occurrence.  We then examine the statutes and case law interpreting the prohibition on advertisements that indicate a preference based on protected class, and consider its application to online advertising.  We focus on its interaction with Section 230 of the Communications Decency Act, which provides interactive computer services with immunity for providing access to  information created by a third party.  We argue that such services can lose that immunity if they target ads toward or away from protected classes without explicit instructions from advertisers to do so.},
 author = {Datta, Amit and Datta, Anupam and Makagon, Jael and Mulligan, Deirdre K. and Tschantz, Michael Carl},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2802143909},
 pages = {20--34},
 pdf = {http://proceedings.mlr.press/v81/datta18a/datta18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Discrimination in Online Advertising: A Multidisciplinary Inquiry},
 url = {https://proceedings.mlr.press/v81/datta18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-duarte18a,
 abstract = {Governments and companies are turning to automated tools to make sense of what people post on social media. Policymakers routinely call for social media companies to identify and take down hate speech, terrorist propaganda, harassment, âfake newsâ or disinformation. Other policy proposals have focused on mining social media to inform law enforcement and immigration decisions. But these proposals wrongly assume that automated technology can accomplish on a large scale the kind of nuanced analysis that humans can do on a small scale. Todayâs tools for analyzing social media text have limited ability to parse the meaning of human communication or detect the intent of the speaker.  A knowledge gap exists between data scientists studying natural language processing (NLP) and policymakers advocating for wide adoption of automated social media analysis and moderation. Policymakers must understand the capabilities and limits of NLP before endorsing or adopting automated content analysis tools, particularly for making decisions that affect fundamental rights or access to government benefits. Without proper safeguards, these tools can facilitate overbroad censorship and biased enforcement of laws or terms of service.  This paper draws on existing research to explain the capabilities and limitations of text classifiers for social media posts and other online content. It is aimed at helping researchers and technical experts address the gaps in policymakersâ knowledge about what is possible with automated text analysis.},
 author = {Duarte, Natasha and Llanso, Emma and Loup, Anna},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2792334037},
 pages = {106--106},
 pdf = {http://proceedings.mlr.press/v81/duarte18a/duarte18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Mixed Messages? The Limits of Automated Social Media Content Analysis},
 url = {https://proceedings.mlr.press/v81/duarte18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-dwork18a,
 abstract = {When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.},
 author = {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 pages = {119--133},
 pdf = {http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Decoupled Classifiers for Group-Fair and Efficient Machine Learning},
 url = {https://proceedings.mlr.press/v81/dwork18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-ekstrand18a,
 abstract = {In this position paper, we argue for applying recent research on ensuring sociotechnical systems are fair and non-discriminatory to the privacy protections those systems may provide. Privacy literature seldom considers whether a proposed privacy scheme protects all persons uniformly, irrespective of membership in protected classes or particular risk in the face of privacy failure. Just as algorithmic decision-making systems may have discriminatory outcomes even without explicit or deliberate discrimination, so also privacy regimes may disproportionately fail to protect vulnerable members of their target population, resulting in disparate impact with respect to the effectiveness of privacy protections.We propose a research agenda that will illuminate this issue, along with related issues in the intersection of fairness and privacy, and present case studies that show how the outcomes of this research may change existing privacy and fairness research. We believe it is important to ensure that technologies and policies intended to protect the users and subjects of information systems provide such protection in an equitable fashion.},
 author = {Ekstrand, Michael D. and Joshaghani, Rezvan and Mehrpouyan, Hoda},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2785487418},
 pages = {35--47},
 pdf = {http://proceedings.mlr.press/v81/ekstrand18a/ekstrand18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Privacy for All: Ensuring Fair and Equitable Privacy Protections},
 url = {https://proceedings.mlr.press/v81/ekstrand18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-ekstrand18b,
 abstract = {In the research literature, evaluations of recommender system effectiveness typically report results over a given data set, providing an aggregate measure of effectiveness over each instance (e.g. user) in the data set. Recent advances in information retrieval evaluation, however, demonstrate the importance of considering the distribution of effectiveness across diverse groups of varying sizes. For example, do users of different ages or genders obtain similar utility from the system, particularly if their group is a relatively small subset of the user base? We apply this consideration to recommender systems, using offline evaluation and a utility-based metric of recommendation effectiveness to explore whether different user demographic groups experience similar recommendation accuracy. We find demographic differences in measured recommender effectiveness across two data sets containing different types of feedback in different domains; these differences sometimes, but not always, correlate with the size of the user group in question. Demographic effects also have a complexâand likely detrimentalâinteraction with popularity bias, a known deficiency of recommender evaluation. These results demonstrate the need for recommender system evaluation protocols that explicitly quantify the degree to which the system is meeting the information needs of all its users, as well as the need for researchers and operators to move beyond naÃ¯ve evaluations that favor the needs of larger subsets of the user population while ignoring smaller subsets.},
 author = {Ekstrand, Michael D. and Tian, Mucun and Azpiazu, Ion Madrazo and Ekstrand, Jennifer D. and Anuyah, Oghenemaro and McNeill, David and Pera, Maria Soledad},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2786599105},
 pages = {172--186},
 pdf = {http://proceedings.mlr.press/v81/ekstrand18b/ekstrand18b.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {All The Cool Kids, How Do They Fit In?: Popularity and Demographic Biases in Recommender Evaluation and Effectiveness},
 url = {https://proceedings.mlr.press/v81/ekstrand18b.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-ensign18a,
 abstract = {Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate. 
In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned. Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which \emph{reported} incidents of crime (those reported by residents) and \emph{discovered} incidents of crime (i.e. those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.},
 author = {Ensign, Danielle and Friedler, Sorelle A. and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W3126362025},
 pages = {160--171},
 pdf = {http://proceedings.mlr.press/v81/ensign18a/ensign18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Runaway Feedback Loops in Predictive Policing},
 url = {https://proceedings.mlr.press/v81/ensign18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-friedler18a,
 abstract = {Fractional calculus has gained considerable popularity and importance during the past three decades mainly because of its demonstrated applications in numerous seemingly diverse and widespread fields of science and engineering. The chapter presents results, including the existence and uniqueness of solutions for the Cauchy Type and Cauchy problems involving nonlinear ordinary fractional differential equations, explicit solutions of linear differential equations and of the corresponding initial-value problems by their reduction to Volterra integral equations and by using operational and compositional methods; applications of the one-and multidimensional Laplace, Mellin, and Fourier integral transforms in deriving the closed-form solutions of ordinary and partial differential equations; and a theory of the so-called “sequential linear fractional differential equations,” including a generalization of the classical Frobenius method.},
 author = {Friedler, Sorelle A. and Wilson, Christo},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W4240465921},
 pages = {1--2},
 pdf = {http://proceedings.mlr.press/v81/friedler18a/friedler18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Preface},
 url = {https://proceedings.mlr.press/v81/friedler18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-hellman18a,
 abstract = {At CRYPTO 1991, Koblitz proposed the anomalous binary curves for speeding up scalar multiplication in elliptic curve cryptosystem. At CRYPTO 1997, Solinas proposed the tau-NAF method on Koblitz curves and reduced the Hamming weight of the scalar to n/3 over the field F2n. At PKC 2004, Avanzi et al combined the tau-NAF with one point halving and reduced the Hamming weight of the scalar to 2n/7. Recently, Avanzi et al improved this method by introducing the wide-double-NAF whose Hamming weight is n/4. In this paper, we propose the wide-w-NAF, which is an extension of Avanzi's wide-double-NAF, and reduce the Hamming weight to n/(w + 1). When n > 144, our method is at least 43%-56% faster than Solinas's tau-NAF method and 21%-39% faster than Avanzi's wide-double-NAF method without additional memory requirements.},
 author = {Hellman, Deborah},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2171634424},
 pages = {4--4},
 pdf = {http://proceedings.mlr.press/v81/hellman18a/hellman18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Keynote 2},
 url = {https://proceedings.mlr.press/v81/hellman18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-kamishima18a,
 abstract = {This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.},
 author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 pages = {187--201},
 pdf = {http://proceedings.mlr.press/v81/kamishima18a/kamishima18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Recommendation Independence},
 url = {https://proceedings.mlr.press/v81/kamishima18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-madaan18a,
 abstract = {The presence of gender stereotypes in many aspects of society is a well-known phenomenon. In this paper, we focus on studying such stereotypes and bias in Hindi movie industry (\it Bollywood) and propose an algorithm to remove these stereotypes from text. We analyze movie plots and posters for all movies released since 1970. The gender bias is detected by semantic modeling of plots at sentence and intra-sentence level. Different features like occupation, introductions, associated actions and descriptions are captured to show the pervasiveness of gender bias and stereotype in movies. Using the derived semantic graph, we compute centrality of each character and observe similar bias there. We also show that such bias is not applicable for movie posters where females get equal importance even though their character has little or no impact on the movie plot. The silver lining is that our system was able to identify 30 movies over last 3 years where such stereotypes were broken. The next step, is to generate debiased stories. The proposed debiasing algorithm extracts gender biased graphs from unstructured piece of text in stories from movies and de-bias these graphs to generate plausible unbiased stories.},
 author = {Madaan, Nishtha and Mehta, Sameep and Agrawaal, Taneea and Malhotra, Vrinda and Aggarwal, Aditi and Gupta, Yatin and Saxena, Mayank},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2789246134},
 pages = {92--105},
 pdf = {http://proceedings.mlr.press/v81/madaan18a/madaan18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Analyze, Detect and Remove Gender Stereotyping from Bollywood Movies},
 url = {https://proceedings.mlr.press/v81/madaan18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-menon18a,
 abstract = {Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive featuresâ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.},
 author = {Menon, Aditya Krishna and Williamson, Robert C.},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2790025105},
 pages = {107--118},
 pdf = {http://proceedings.mlr.press/v81/menon18a/menon18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {The cost of fairness in binary classification},
 url = {https://proceedings.mlr.press/v81/menon18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-phillips18a,
 abstract = {Active learning has long been a topic of study in machine learning. However, as increasingly complex and opaque models have become standard practice, the process of active learning, too, has become more opaque. There has been little investigation into interpreting what specific trends and patterns an active learning strategy may be exploring. This work expands on the Local Interpretable Model-agnostic Explanations framework (LIME) to provide explanations for active learning recommendations. We demonstrate how LIME can be used to generate locally faithful explanations for an active learning strategy, and how these explanations can be used to understand how different models and datasets explore a problem space over time. In order to quantify the per-subgroup differences in how an active learning strategy queries spatial regions, we introduce a notion of uncertainty bias (based on disparate impact) to measure the discrepancy in the confidence for a model's predictions between one subgroup and another. Using the uncertainty bias measure, we show that our query explanations accurately reflect the subgroup focus of the active learning queries, allowing for an interpretable explanation of what is being learned as points with similar sources of uncertainty have their uncertainty bias resolved. We demonstrate that this technique can be applied to track uncertainty bias over user-defined clusters or automatically generated clusters based on the source of uncertainty.},
 author = {Phillips, Richard and Chang, Kyu Hyun and Friedler, Sorelle A.},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2740409505},
 pages = {49--61},
 pdf = {http://proceedings.mlr.press/v81/phillips18a/phillips18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Interpretable Active Learning},
 url = {https://proceedings.mlr.press/v81/phillips18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-selbst18a,
 abstract = {There is no single, neat statutory provision labeled the âright to explanationâ in Europeâs new General Data Protection Regulation (GDPR). But nor is such a right illusory. Responding to two prominent papers that, in turn, conjure and critique the right to explanation in the context of automated decision-making, we advocate a return to the text of the GDPR. Articles 13â15 provide rights to âmeaningful information about the logic involvedâ in automated decisions. This is a right to explanation, whether one uses the phrase or not. The right to explanation should be interpreted functionally, flexibly, and should, at a minimum, enable a data subject to exercise his or her rights under the GDPR and human rights law.},
 author = {Selbst, Andrew and Powles, Julia},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 pages = {48--48},
 pdf = {http://proceedings.mlr.press/v81/selbst18a/selbst18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {âMeaningful Informationâ and the Right to Explanation},
 url = {https://proceedings.mlr.press/v81/selbst18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-speicher18a,
 abstract = {Recently, online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups, i.e., to exclude users belonging to a certain race or gender from receiving their ads. Such criticisms have led, for instance, Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services. In this paper, we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious. We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting. We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising. We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes. Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising.},
 author = {Speicher, Till and Ali, Muhammad and Venkatadri, Giridhari and Ribeiro, Filipe Nunes and Arvanitakis, George and Benevenuto, FabrÃ­cio and Gummadi, Krishna P. and Loiseau, Patrick and Mislove, Alan},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2794342582},
 pages = {5--19},
 pdf = {http://proceedings.mlr.press/v81/speicher18a/speicher18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Potential for Discrimination in Online Targeted Advertising},
 url = {https://proceedings.mlr.press/v81/speicher18a.html},
 volume = {81},
 year = {2018}
}

@inproceedings{pmlr-v81-sweeney18a,
 abstract = {To promote the model-based software engineering development of user interfaces, this paper proposes an EIP model that can design user interfaces in abstract and can support automatic generation of user interfaces. In the model, functionality and composition are taken as the direct descriptive objects of engineering modeling, making user interface construction more intuitive and easily acceptable; and a layered approach is adopted for both abstraction and presentation of user interface description, making the modeling powerful enough in dealing with complex user interfaces. In the research, internal data models and their related user interface design patterns are isolated as the user interface modeling components, making presentation and layout more easily constructed. Based on EIP and tools, the automatic generation of user interfaces is achieved.},
 author = {Sweeney, Latanya},
 booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
 editor = {Friedler, Sorelle A. and Wilson, Christo},
 month = {23--24 Feb},
 openalex = {W2136574651},
 pages = {3--3},
 pdf = {http://proceedings.mlr.press/v81/sweeney18a/sweeney18a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Keynote 1},
 url = {https://proceedings.mlr.press/v81/sweeney18a.html},
 volume = {81},
 year = {2018}
}
