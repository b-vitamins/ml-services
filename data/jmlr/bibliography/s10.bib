@article{JMLR:v16:addarioberry15a,
 abstract = {In this paper we explore maximal deviations of large random structures from their typical behavior. We introduce a model for a high-dimensional random graph process and ask analogous questions to those of Vapnik and Chervonenkis for deviations of averages: how "rich" does the process have to be so that one sees atypical behavior. In particular, we study a natural process of Erdős-Rényi random graphs indexed by unit vectors in $\mathbb{R}^d$. We investigate the deviations of the process with respect to three fundamental properties: clique number, chromatic number, and connectivity. In all cases we establish upper and lower bounds for the minimal dimension $d$ that guarantees the existence of "exceptional directions" in which the random graph behaves atypically with respect to the property. For each of the three properties, four theorems are established, to describe upper and lower bounds for the threshold dimension in the subcritical and supercritical regimes.},
 author = {Louigi Addario-Berry and Shankar Bhamidi and S{{\'e}}bastien Bubeck and Luc Devroye and G{{\'a}}bor Lugosi and Roberto Imbuzeiro Oliveira},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W1880169967},
 pages = {1893--1922},
 title = {Exceptional rotations of random graphs: a VC theory},
 url = {http://jmlr.org/papers/v16/addarioberry15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:bellec15a,
 abstract = {We derive oracle inequalities for the problems of isotonic and convex regression using the combination of Q-aggregation procedure and sparsity pattern aggregation. This improves upon the previous results including the oracle inequalities for the constrained least squares estimator. One of the improvements is that our oracle inequalities are sharp, i.e., with leading constant 1. It allows us to obtain bounds for the minimax regret thus accounting for model misspecification, which was not possible based on the previous results. Another improvement is that we obtain oracle inequalities both with high probability and in expectation.},
 author = {Pierre C. Bellec and Alexandre B. Tsybakov},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2962703606},
 pages = {1879--1892},
 title = {Sharp oracle bounds for monotone and convex regression through aggregation},
 url = {http://jmlr.org/papers/v16/bellec15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:gammerman15a,
 abstract = {Preview this article: Preface to the special issue: Queering borders: Language, sexuality and migration, Page 1 of 1 < Previous page | Next page > /docserver/preview/fulltext/jls.3.1.01mur-1.gif},
 author = {Alex Gammerman and Vladimir Vovk},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W1967498427},
 pages = {1677--1681},
 title = {Preface to the special issue},
 url = {http://jmlr.org/papers/v16/gammerman15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:gammerman15b,
 abstract = {This introduction to Alexey Chervonenkis’s bibliography, which is published next in this issue, mainly consists of historical notes. The bibliography is doubtless incomplete, and it is just a first step in compiling more comprehensive ones. En route we also give some basic information about Alexey as a researcher and person; for further details, see, e.g., the short biography (Editors, 2015) in the Chervonenkis Festschrift. In this introduction, the numbers in square brackets refer to Chervonenkis’s bibliography, and the author/year citations refer to the list of references at the end of this introduction. Alexey Chervonenkis was born in Moscow in 1938. In 1955 he became a student at the MIPT, Moscow Institute of Physics and Technology (Faculty 1, Radio Engineering, nowadays Radio Engineering and Cybernetics). As part of his course of studies at the MIPT, he was attached to a laboratory at the ICS (the Institute of Control Sciences, called the Institute of Automation and Remote Control at the time), an institution in a huge system known as the Soviet Academy of Sciences.},
 author = {Alex Gammerman and Vladimir Vovk},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W1824309071},
 pages = {2051--2066},
 title = {Alexey Chervonenkis's bibliography: introductory comments},
 url = {http://jmlr.org/papers/v16/gammerman15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:gammerman15c,
 abstract = {This bibliography does not contain Alexey’s patents (he has at least two), technical reports, unpublished manuscripts, and collections edited by him. NA indicates that a journal paper was not assigned to a volume; e.g., it is common for Russian journals (such as Проблемы управления and, in some years, Автоматика и телемеханика) not to have volumes, and also to have pages numbered separately inside each issue. All papers published by Alexey before 2001 (and afterwards in the case of papers whose original language was Russian) have author lists ordered according to the Cyrillic alphabetic order; for other papers the order may reflect the authors’ contributions (people who contributed most tend to be listed first) and administrative positions (bosses tend to be listed last). The bibliography is given by the year of the original publication (which may be different from the year of the English translation, always given first when available).},
 author = {Alex Gammerman and Vladimir Vovk},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W2132116238},
 pages = {2067--2080},
 title = {Alexey Chervonenkis's bibliography},
 url = {http://jmlr.org/papers/v16/gammerman15c.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:gyorfi15a,
 abstract = {An estimate of the second moment of the regression function is introduced. Its asymptotic normality is proved such that the asymptotic variance depends neither on the dimension of the observation vector, nor on the smoothness properties of the regression function. The asymptotic variance is given explicitly.},
 author = {L{{\'a}}szl{{\'o}} Gy{{\"o}}rfi and Harro Walk},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W1649659002},
 pages = {1863--1877},
 title = {On the asymptotic normality of an estimate of a regression functional},
 url = {http://jmlr.org/papers/v16/gyorfi15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:herbster15a,
 abstract = {We study the problem of predicting online the labeling of a graph. We consider a novel setting for this problem in which, in addition to observing vertices and labels on the graph, we also observe a sequence of just vertices on a second graph. A latent labeling of the second graph selects one of K labelings to be active on the first graph. We propose a polynomial time algorithm for online prediction in this setting and derive a mistake bound for the algorithm. The bound is controlled by the geometric cut of the observed and latent labelings, as well as the resistance diameters of the graphs. When specialized to multitask prediction and online switching problems the bound gives new and sharper results under certain conditions.},
 author = {Mark Herbster and Stephen Pasteris and Massimiliano Pontil},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2190548589},
 pages = {2003--2022},
 title = {Predicting a switching sequence of graph labelings},
 url = {http://jmlr.org/papers/v16/herbster15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:janzing15a,
 abstract = {According to a recently stated 'independence postulate', the distribution Pcause contains no information about the conditional Peffect|cause while Peffect may contain information about Pcause|effect. Since semi-supervised learning (SSL) attempts to exploit information from PX to assist in predicting Y from X, it should only work in anticausal direction, i.e., when Y is the cause and X is the effect. In causal direction, when X is the cause and Y the effect, unlabelled x-values should be useless. To shed light on this asymmetry, we study a deterministic causal relation Y = f(X) as recently assayed in Information-Geometric Causal Inference (IGCI). Within this model, we discuss two options to formalize the independence of PX and f as an orthogonality of vectors in appropriate inner product spaces. We prove that unlabelled data help for the problem of interpolating a monotonically increasing function if and only if the orthogonality conditions are violated - which we only expect for the anticausal direction. Here, performance of SSL and its supervised baseline analogue is measured in terms of two different loss functions: first, the mean squared error and second the surprise in a Bayesian prediction scenario.},
 author = {Dominik Janzing and Bernhard Sch{{\"o}}lkopf},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W1845812186},
 pages = {1923--1948},
 title = {Semi-supervised interpolation in an anticausal learning scenario},
 url = {http://jmlr.org/papers/v16/janzing15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:koltchinskii15a,
 abstract = {The density matrices are positively semi-definite Hermitian matrices of unit trace that describe the state of a quantum system. The goal of the paper is to develop minimax lower bounds on error rates of estimation of low rank density matrices in trace regression models used in quantum state tomography (in particular, in the case of Pauli measurements) with explicit dependence of the bounds on the rank and other complexity parameters. Such bounds are established for several statistically relevant distances, including quantum versions of Kullback-Leibler divergence (relative entropy distance) and of Hellinger distance (so called Bures distance), and Schatten p-norm distances. Sharp upper bounds and oracle inequalities for least squares estimator with von Neumann entropy penalization are obtained showing that minimax lower bounds are attained (up to logarithmic factors) for these distances.},
 author = {Vladimir Koltchinskii and Dong Xia},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W2963441353},
 pages = {1757--1792},
 title = {Optimal estimation of low rank density matrices},
 url = {http://jmlr.org/papers/v16/koltchinskii15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:swaminathan15a,
 abstract = {We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem (Bottou et al., 2013) through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. In analogy to the Structural Risk Minimization principle of Wapnik and Tscherwonenkis (1979), these constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method--called Policy Optimizer for Exponential Models (POEM)--for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. The effectiveness and efficiency of POEM is evaluated on several simulated multi-label classification problems, as well as on a real-world information retrieval problem. The empirical results show that the CRM objective implemented in POEM provides improved robustness and generalization performance compared to the state-of-the-art.},
 author = {Adith Swaminathan and Thorsten Joachims},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W1835900096},
 pages = {1731--1755},
 title = {Batch learning from logged bandit feedback through counterfactual risk minimization},
 url = {http://jmlr.org/papers/v16/swaminathan15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:thomann15a,
 abstract = {We propose some axioms for hierarchical clustering of probability measures and investigate their ramifications. The basic idea is to let the user stipulate the clusters for some elementary measures. This is done without the need of any notion of metric, similarity or dissimilarity. Our main results then show that for each suitable choice of user-defined clustering on elementary measures we obtain a unique notion of clustering on a large set of distributions satisfying a set of additivity and continuity axioms. We illustrate the developed theory by numerous examples including some with and some without a density.},
 author = {Philipp Thomann and Ingo Steinwart and Nico Schmid},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W4293771442},
 pages = {1949--2002},
 title = {Towards an Axiomatic Approach to Hierarchical Clustering of Measures},
 url = {http://jmlr.org/papers/v16/thomann15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:vanerven15a,
 abstract = {The speed with which a learning algorithm converges as it is presented with more data is a central problem in machine learning -- a fast rate of convergence means less data is needed for the same level of performance. The pursuit of fast rates in online and statistical learning has led to the discovery of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for 'proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that under surprisingly weak assumptions both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov margin condition, both of which have played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying conditions thus provide a substantial step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.},
 author = {Tim van Erven and Peter D. Gr{{\"u}}nwald and Nishant A. Mehta and Mark D. Reid and Robert C. Williamson},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W1831843218},
 pages = {1793--1861},
 title = {Fast rates in statistical and online learning},
 url = {http://jmlr.org/papers/v16/vanerven15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:vapnik15a,
 abstract = {This paper presents direct settings and rigorous solutions of the main Statistical Inference problems. It shows that rigorous solutions require solving multidimensional Fredholm integral equations of the first kind in the situation where not only the right-hand side of the equation is an approximation, but the operator in the equation is also defined approximately. Using Stefanuyk-Vapnik theory for solving such ill-posed operator equations, constructive methods of empirical inference are introduced. These methods are based on a new concept called V-matrix. This matrix captures geometric properties of the observation data that are ignored by classical statistical methods.},
 author = {Vladimir Vapnik and Rauf Izmailov},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W1854718785},
 pages = {1683--1730},
 title = {V-matrix method of solving statistical inference problems},
 url = {http://jmlr.org/papers/v16/vapnik15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:vapnik15b,
 abstract = {This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classification of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for significantly accelerating the speed of Student's learning using privileged information: (1) correction of Student's concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer.},
 author = {Vladimir Vapnik and Rauf Izmailov},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W2173379916},
 pages = {2023--2049},
 title = {Learning using privileged information: similarity control and knowledge transfer},
 url = {http://jmlr.org/papers/v16/vapnik15b.html},
 volume = {16},
 year = {2015}
}
