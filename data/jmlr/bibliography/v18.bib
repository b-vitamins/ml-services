@article{JMLR:v18:10-231,
 author = {Fran{\c{c}}ois Caron and Willie Neiswanger and Frank Wood and Arnaud Doucet and Manuel Davy},
 journal = {Journal of Machine Learning Research},
 number = {27},
 pages = {1--32},
 title = {Generalized P{\'o}lya Urn for Time-Varying Pitman-Yor Processes},
 url = {http://jmlr.org/papers/v18/10-231.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:13-336,
 abstract = {We derive generalization error bounds for traditional time-series forecasting models. Our results hold for many standard forecasting tools including autoregressive models, moving average models, and, more generally, linear state-space models. These nonasymptotic bounds need only weak assumptions on the data-generating process, yet allow forecasters to select among competing models and to guarantee, with high probability, that their chosen model will perform well. We motivate our techniques with and apply them to standard economic and financial forecasting tools--a GARCH model for predicting equity volatility and a dynamic stochastic general equilibrium model (DSGE), the standard tool in macroeconomic forecasting. We demonstrate in particular how our techniques can aid forecasters and policy makers in choosing models which behave well under uncertainty and mis-specification.},
 author = {Daniel J. McDonald and Cosma Rohilla Shalizi and Mark Schervish},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W2964286225},
 pages = {1--40},
 title = {Nonparametric risk bounds for time-series forecasting},
 url = {http://jmlr.org/papers/v18/13-336.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-188,
 abstract = {Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This pa- per presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts.},
 author = {Tianlin Shi and Jun Zhu},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W2100897665},
 pages = {1--39},
 title = {Online Bayesian Passive-Aggressive Learning},
 url = {http://jmlr.org/papers/v18/14-188.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-223,
 abstract = {We propose a computationally efficient random walk on a convex body which rapidly mixes and closely tracks a time-varying log-concave distribution. We develop general theoretical guarantees on the required number of steps; this number can be calculated on the fly according to the distance from and the shape of the next distribution. We then illustrate the technique on several examples. Within the context of exponential families, the proposed method produces samples from a posterior distribution which is updated as data arrive in a streaming fashion. The sampling technique can be used to track time-varying truncated distributions, as well as to obtain samples from a changing mixture model, fitted in a streaming fashion to data. In the setting of linear optimization, the proposed method has oracle complexity with best known dependence on the dimension for certain geometries. In the context of online learning and repeated games, the algorithm is an efficient method for implementing no-regret mixture forecasting strategies. Remarkably, in some of these examples, only one step of the random walk is needed to track the next distribution.},
 author = {Hariharan Narayanan and Alexer Rakhlin},
 journal = {Journal of Machine Learning Research},
 number = {112},
 openalex = {W4301185658},
 pages = {1--29},
 title = {Efficient Sampling from Time-Varying Log-Concave Distributions},
 url = {http://jmlr.org/papers/v18/14-223.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-249,
 abstract = {This paper presents the Averaged CVB (ACVB) inference and oers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence-guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non-expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence.},
 author = {Katsuhiko Ishiguro and Issei Sato and Naonori Ueda},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2605000734},
 pages = {1--29},
 title = {Averaged collapsed variational bayes inference},
 url = {http://jmlr.org/papers/v18/14-249.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-317,
 abstract = {In machine learning, one often encounters data sets where a general pattern is violated by a relatively small number of exceptions (for example, a rule that says that all birds can fly is violated by examples such as penguins). This complicates the concept learning process and may lead to the rejection of some simple and expressive rules that cover many cases. In this paper we present an approach to this problem in description logic learning by computing partial descriptions (which are not necessarily entirely complete) of both positive and negative examples and combining them. Our Symmetric Parallel Class Expression Learning approach enables the generation of general rules with exception patterns included. We demonstrate that this algorithm provides significantly better results (in terms of metrics such as accuracy, search space covered, and learning time) than standard approaches on some typical data sets. Further, the approach has the added benefit that it can be parallelised relatively simply, leading to much faster exploration of the search tree on modern computers.},
 author = {An C. Tran and Jens Dietrich and Hans W. Guesgen and Stephen Marsl and },
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W2763425701},
 pages = {1--34},
 title = {Parallel symmetric class expression learning},
 url = {http://jmlr.org/papers/v18/14-317.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-318,
 abstract = {We propose a spectral clustering method based on local principal components analysis (PCA). After performing local PCA in selected neighborhoods, the algorithm builds a nearest neighbor graph weigh...},
 author = {Ery Arias-Castro and Gilad Lerman and Teng Zhang},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W3126225255},
 pages = {1--57},
 title = {Spectral clustering based on local PCA},
 url = {http://jmlr.org/papers/v18/14-318.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-348,
 abstract = {Sparsity is an important modeling tool that expands the applicability of convex formulations for data analysis, however it also creates significant challenges for efficient algorithm design. In this paper we investigate the generalized conditional gradient (GCG) algorithm for solving sparse optimization problems--demonstrating that, with some enhancements, it can provide a more efficient alternative to current state of the art approaches. After studying the convergence properties of GCG for general convex composite problems, we develop efficient methods for evaluating polar operators, a subroutine that is required in each GCG iteration. In particular, we show how the polar operator can be efficiently evaluated in learning low-rank matrices, instantiated with detailed examples on matrix completion and dictionary learning. A further improvement is achieved by interleaving GCG with fixed-rank local subspace optimization. A series of experiments on matrix completion, multi-class classification, and multi-view dictionary learning shows that the proposed method can significantly reduce the training cost of current alternatives.},
 author = {Yaoliang Yu and Xinhua Zhang and Dale Schuurmans},
 journal = {Journal of Machine Learning Research},
 number = {144},
 openalex = {W2963856360},
 pages = {1--46},
 title = {Generalized Conditional Gradient for Sparse Estimation},
 url = {http://jmlr.org/papers/v18/14-348.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-400,
 abstract = {A typical viral marketing model identifies influential users in a social network to maximize a single product adoption assuming unlimited user attention, campaign budgets, and time. In reality, multiple products need campaigns, users have limited attention, convincing users incurs costs, and advertisers have limited budgets and expect the adoptions to be maximized soon. Facing these user, monetary, and timing constraints, we formulate the problem as a submodular maximization task in a continuous-time diffusion model under the intersection of a matroid and multiple knapsack constraints. We propose a randomized algorithm estimating the user influence in a network ($|\mathcal{V}|$ nodes, $|\mathcal{E}|$ edges) to an accuracy of $\epsilon$ with $n=\mathcal{O}(1/\epsilon^2)$ randomizations and $\tilde{\mathcal{O}}(n|\mathcal{E}|+n|\mathcal{V}|)$ computations. By exploiting the influence estimation algorithm as a subroutine, we develop an adaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2 k)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active. Extensive experiments on networks of millions of nodes demonstrate that the proposed algorithms achieve the state-of-the-art in terms of effectiveness and scalability.},
 author = {Nan Du and Yingyu Liang and Maria-Florina Balcan and Manuel Gomez-Rodriguez and Hongyuan Zha and Le Song},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W2949796662},
 pages = {1--45},
 title = {Scalable Influence Maximization for Multiple Products in Continuous-Time Diffusion Networks},
 url = {http://jmlr.org/papers/v18/14-400.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-415,
 author = {Xiao-Tong Yuan and Ping Li and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {166},
 openalex = {W2804280820},
 pages = {1--43},
 title = {Gradient Hard Thresholding Pursuit},
 url = {http://jmlr.org/papers/v18/14-415.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:14-428,
 abstract = {We present tools for the analysis of Follow-The-Regularized-Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, proxfunction or learning rate schedule) ...},
 author = {H. Brendan McMahan},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W2964323557},
 pages = {1--50},
 title = {A survey of Algorithms and Analysis for Adaptive Online Learning},
 url = {http://jmlr.org/papers/v18/14-428.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-453,
 abstract = {In this paper we introduce a new optimization formulation for sparse regression and compressed sensing, called CLOT (Combined L-One and Two), wherein the regularizer is a convex combination of the ℓ <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">1</sub> - and ℓ <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">2</sub> -norms. This formulation differs from the Elastic Net (EN) formulation, in which the regularizer is a convex combination of the ℓ <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">1</sub> - and ℓ <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">2</sub> -norm squared. This seemingly simple modification has fairly significant consequences. In particular, it is shown in this paper that the EN formulation does not achieve robust recovery of sparse vectors in the context of compressed sensing, whereas the new CLOT formulation does so. Also, like EN but unlike LASSO, the CLOT formulation achieves the grouping effect, wherein coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values. It is noteworthy that LASSO does not have the grouping effect and EN (as shown here) does not achieve robust sparse recovery. Therefore the CLOT formulation combines the best features of both LASSO (robust sparse recovery) and EN (grouping effect). The CLOT formulation is a special case of another one called SGL (Sparse Group LASSO) which was introduced into the literature previously, but without any analysis of either the grouping effect or robust sparse recovery. It is shown here that SGL achieves robust sparse recovery, and also achieves a version of the grouping effect in that coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values, if the columns belong to the same group.},
 author = {Mehmet Eren Ahsen and Niharika Challapalli and Mathukumalli Vidyasagar},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W2964129273},
 pages = {1--24},
 title = {Two new approaches to compressed sensing exhibiting both robust sparse recovery and the grouping effect},
 url = {http://jmlr.org/papers/v18/14-453.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-467,
 abstract = {We present a canonical way to turn any smooth parametric family of probability distributions on an arbitrary search space $X$ into a continuous-time black-box optimization method on $X$, the \emph{information-geometric optimization} (IGO) method. Invariance as a design principle minimizes the number of arbitrary choices. The resulting \emph{IGO flow} conducts the natural gradient ascent of an adaptive, time-dependent, quantile-based transformation of the objective function. It makes no assumptions on the objective function to be optimized. The IGO method produces explicit IGO algorithms through time discretization. It naturally recovers versions of known algorithms and offers a systematic way to derive new ones. The cross-entropy method is recovered in a particular case, and can be extended into a smoothed, parametrization-independent maximum likelihood update (IGO-ML). For Gaussian distributions on $\mathbb{R}^d$, IGO is related to natural evolution strategies (NES) and recovers a version of the CMA-ES algorithm. For Bernoulli distributions on $\{0,1\}^d$, we recover the PBIL algorithm. From restricted Boltzmann machines, we obtain a novel algorithm for optimization on $\{0,1\}^d$. All these algorithms are unified under a single information-geometric optimization framework. Thanks to its intrinsic formulation, the IGO method achieves invariance under reparametrization of the search space $X$, under a change of parameters of the probability distributions, and under increasing transformations of the objective function. Theory strongly suggests that IGO algorithms have minimal loss in diversity during optimization, provided the initial diversity is high. First experiments using restricted Boltzmann machines confirm this insight. Thus IGO seems to provide, from information theory, an elegant way to spontaneously explore several valleys of a fitness landscape in a single run.},
 author = {Yann Ollivier and Ludovic Arnold and Anne Auger and Nikolaus Hansen},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W1480347379},
 pages = {1--65},
 title = {Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles},
 url = {http://jmlr.org/papers/v18/14-467.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-484,
 author = {Dhruv Mahajan and S. Sathiya Keerthi and S. Sundararajan},
 journal = {Journal of Machine Learning Research},
 number = {91},
 pages = {1--35},
 title = {A distributed block coordinate descent method for training l1 regularized linear classifiers},
 url = {http://jmlr.org/papers/v18/14-484.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:14-546,
 abstract = {We consider neural networks with a single hidden layer and non-decreasing homogeneous activa-tion functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of ob-servations. In addition, we provide a simple geometric interpretation to the non-convex problem of addition of a new unit, which is the core potentially hard computational element in the framework of learning from continuously many basis functions. We provide simple conditions for convex relaxations to achieve the same generalization error bounds, even when constant-factor approxi-mations cannot be found (e.g., because it is NP-hard such as for the zero-homogeneous activation function). We were not able to find strong enough convex relaxations and leave open the existence or non-existence of polynomial-time algorithms.},
 author = {Francis Bach},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2107822587},
 pages = {1--53},
 title = {Breaking the Curse of Dimensionality with Convex Neural Networks},
 url = {http://jmlr.org/papers/v18/14-546.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-025,
 abstract = {Scaling kernel machines to massive data sets is a major challenge due to storage and computation issues in handling large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low-rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation framework - Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the covtype dataset with half a million samples, MEKA takes around 70 seconds and uses less than 80 MB memory on a single machine to achieve 10% relative approximation error, while standard Nystrom approximation is about 6 times slower and uses more than 400MB memory to achieve similar approximation. We also present extensive experiments on applying MEKA to speed up kernel ridge regression.},
 author = {Si Si and Cho-Jui Hsieh and Inderjit S. Dhillon},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W2606823780},
 pages = {1--32},
 title = {Memory efficient kernel approximation},
 url = {http://jmlr.org/papers/v18/15-025.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-038,
 abstract = {We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non-asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models.},
 author = {Yves F. Atchad{{\'e}} and Gersende Fort and Eric Moulines},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2464084515},
 pages = {1--33},
 title = {On perturbed proximal gradient algorithms},
 url = {http://jmlr.org/papers/v18/15-038.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-076,
 abstract = {This paper considers the problem of matrix completion when the observed entries are noisy and contain outliers. It begins with introducing a new optimization criterion for which the recovered matrix is defined as its solution. This criterion uses the celebrated Huber function from the robust statistics literature to downweigh the effects of outliers. A practical algorithm is developed to solve the optimization involved. This algorithm is fast, straightforward to implement, and monotonic convergent. Furthermore, the proposed methodology is theoretically shown to be stable in a well defined sense. Its promising empirical performance is demonstrated via a sequence of simulation experiments, including image inpainting.},
 author = {Raymond K. W. Wong and Thomas C. M. Lee},
 journal = {Journal of Machine Learning Research},
 number = {147},
 openalex = {W2963862965},
 pages = {1--25},
 title = {Matrix completion with noisy entries and outliers},
 url = {http://jmlr.org/papers/v18/15-076.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-085,
 abstract = {We study the design of interactive clustering algorithms for data sets satisfying natural stability assumptions. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable features in many applications. We show that in this constrained setting one can still design provably efficient algorithms that produce accurate clusterings. We also show that our algorithms perform well on real-world data.},
 author = {Pranjal Awasthi and Maria Florina Balcan and Konstantin Voevodski},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W2158249114},
 pages = {1--35},
 title = {Local algorithms for interactive clustering},
 url = {http://jmlr.org/papers/v18/15-085.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-104,
 abstract = {Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries in active learning. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective.},
 author = {Jamshid Sourati and Murat Akcakaya and Todd K. Leen and Deniz Erdogmus and Jennifer G. Dy},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W2412764668},
 pages = {1--41},
 title = {Asymptotic Analysis of Objectives based on Fisher Information in Active Learning},
 url = {http://jmlr.org/papers/v18/15-104.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-143,
 abstract = {The scalability of statistical estimators is of increasing importance in modern applications. One approach to implementing scalable algorithms is to compress data into a low dimensional latent space using dimension reduction methods. In this paper we develop an approach for dimension reduction that exploits the assumption of low rank structure in high dimensional data to gain both computational and statistical advantages. We adapt recent randomized low-rank approximation algorithms to provide an efficient solution to principal component analysis (PCA), and we use this efficient solver to improve parameter estimation in large-scale linear mixed models (LMM) for association mapping in statistical and quantitative genomics. A key observation in this paper is that randomization serves a dual role, improving both computational and statistical performance by implicitly regularizing the covariance matrix estimate of the random effect in a LMM. These statistical and computational advantages are highlighted in our experiments on simulated data and large-scale genomic studies.},
 author = {Gregory Darnell and Stoyan Georgiev and Sayan Mukherjee and Barbara E Engelhardt},
 journal = {Journal of Machine Learning Research},
 number = {140},
 openalex = {W2962786706},
 pages = {1--30},
 title = {Adaptive Randomized Dimension Reduction on Massive Data},
 url = {http://jmlr.org/papers/v18/15-143.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-154,
 abstract = {Consider the problem of sampling sequentially from a finite number of N ≥ 2 populations, specified by random variables Xki, i = 1,...,N; and k = 1,2,..., where Xki denotes the outcome from population i the kth time it is sampled. It is assumed that for each fixed i, {Xki}k≥1 is a sequence of i.i.d. normal random variables, with unknown mean µi and unknown variance σi2. The objective is to have a policy π for deciding from which of the N populations to sample from at any time t = 1,2, ... so as to maximize the expected sum of outcomes of n total samples or equivalently to minimize the regret due to lack on information of the parameters µi and σi2. In this paper, we present a simple inflated sample mean (ISM) index policy that is asymptotically optimal in the sense of Theorem 4 below. This resolves a standing open problem from Burnetas and Katehakis (1996b). Additionally, finite horizon regret bounds are given.},
 author = {Wesley Cowan and Junya Honda and Michael N. Katehakis},
 journal = {Journal of Machine Learning Research},
 number = {154},
 openalex = {W2803364674},
 pages = {1--28},
 title = {Normal bandits of unknown means and variances},
 url = {http://jmlr.org/papers/v18/15-154.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-178,
 abstract = {We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in L2- and L$\infty$-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipschitz-continuous losses.},
 author = {Francis Bach},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2963709899},
 pages = {1--38},
 title = {On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions},
 url = {http://jmlr.org/papers/v18/15-178.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-205,
 abstract = {Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number $n$ of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis-Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach which samples from a distribution provably close to the posterior distribution of interest, yet can require less than $O(n)$ data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor.},
 author = {R{{\'e}}mi Bardenet and Arnaud Doucet and Chris Holmes},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W1599118604},
 pages = {1--43},
 title = {On Markov chain Monte Carlo methods for tall data},
 url = {http://jmlr.org/papers/v18/15-205.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-226,
 abstract = {We study binary classification in the presence of class-conditional random noise, where the learner gets to see labels that are flipped independently with some probability, and where the flip probability depends on the class. Our goal is to devise learning algorithms that are efficient and statistically consistent with respect to commonly used utility measures. In particular, we look at a family of measures motivated by their application in domains where cost-sensitive learning is necessary (for example, when there is class imbalance). In contrast to most of the existing literature on consistent classification that are limited to the classical 0-1 loss, our analysis includes more general utility measures such as the AM measure (arithmetic mean of True Positive Rate and True Negative Rate). For this problem of cost-sensitive learning under class-conditional random noise, we develop two approaches that are based on suitably modifying surrogate losses. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical utility maximization in the presence of i.i.d. data with noisy labels. If the loss function satis_es a simple symmetry condition, we show that using unbiased estimator leads to an efficient algorithm for empirical maximization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong utility bounds. This approach implies that methods already used in practice, such as biased SVM and weighted logistic regression, are provably noise-tolerant. For two practically important measures in our family, we show that the proposed methods are competitive with respect to recently proposed methods for dealing with label noise in several benchmark data sets.},
 author = {Nagarajan Natarajan and Inderjit S. Dhillon and Pradeep Ravikumar and Ambuj Tewari},
 journal = {Journal of Machine Learning Research},
 number = {155},
 openalex = {W2803642127},
 pages = {1--33},
 title = {Cost-sensitive learning with noisy labels},
 url = {http://jmlr.org/papers/v18/15-226.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-233,
 abstract = {We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks (Drineas et al., 2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and Singh, 2014). Our analysis shows that, under the assumption that the input data matrix has incoherent rows but possibly coherent columns, all algorithms provably converge to the best low-rank approximation of the original data as number of selected columns increases. Furthermore, two of the proposed algorithms enjoy a relative error bound, which is preferred for column subset selection and matrix approximation purposes. We also demonstrate through both theoretical and empirical analysis the power of feedback driven sampling compared to uniform random sampling on input matrices with highly correlated columns.},
 author = {Yining Wang and Aarti Singh},
 journal = {Journal of Machine Learning Research},
 number = {156},
 openalex = {W2963094949},
 pages = {1--42},
 title = {Provably correct algorithms for matrix column subset selection with selectively sampled data},
 url = {http://jmlr.org/papers/v18/15-233.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-240,
 abstract = {There is a large literature explaining why AdaBoost is a successful classifier. The literature on AdaBoost focuses on classifier margins and boosting's interpretation as the optimization of an exponential likelihood function. These existing explanations, however, have been pointed out to be incomplete. A random forest is another popular ensemble method for which there is substantially less explanation in the literature. We introduce a novel perspective on AdaBoost and random forests that proposes that the two algorithms work for similar reasons. While both classifiers achieve similar predictive accuracy, random forests cannot be conceived as a direct optimization procedure. Rather, random forests is a self-averaging, interpolating algorithm which creates what we denote as a spiked-smooth classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and random forests succeed because of this mechanism. We provide a number of examples to support this explanation. In the process, we question the conventional wisdom that suggests that boosting algorithms for classification require regularization or early stopping and should be limited to low complexity classes of learners, such as decision stumps. We conclude that boosting should be used like random forests: with large decision trees, without regularization or early stopping.},
 author = {Abraham J. Wyner and Matthew Olson and Justin Bleich and David Mease},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W2964161291},
 pages = {1--33},
 title = {Explaining the success of adaboost and random forests as interpolating classifiers},
 url = {http://jmlr.org/papers/v18/15-240.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-251,
 abstract = {A popular approach for online decision-making in large MDPs is time-bounded tree search. The effectiveness of tree search, however, is largely influenced by the action branching factor, which limit...},
 author = {Jervis Pinto and Alan Fern},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W3005675092},
 pages = {1--35},
 title = {Learning partial policies to speedup MDP tree search via reduction to I.I.D. learning},
 url = {http://jmlr.org/papers/v18/15-251.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-257,
 abstract = {Differential privacy formalises privacy-preserving mechanisms that provide access to a database. Can Bayesian inference be used directly to provide private access to data? The answer is yes: under certain conditions on the prior, sampling from the posterior distribution can lead to a desired level of privacy and utility. For a uniform treatment, we define differential privacy over arbitrary data set metrics, outcome spaces and distribution families. This allows us to also deal with non-i.i.d or non-tabular data sets. We then prove bounds on the sensitivity of the posterior to the data, which delivers a measure of robustness. We also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. Finally, we provide bounds on the utility of answers to queries and on the ability of an adversary to distinguish between data sets. The latter are complemented by a novel use of Le Cam's method to obtain lower bounds on distinguishability. Our results hold for arbitrary metrics, including those for the common definition of differential privacy. For specific choices of the metric, we give a number of examples satisfying our assumptions.},
 author = {Christos Dimitrakakis and Blaine Nelson and Zuhe Zhang and Aikaterini Mitrokotsa and Benjamin I. P. Rubinstein},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W2608609295},
 pages = {1--39},
 title = {Differential Privacy for Bayesian Inference through Posterior Sampling},
 url = {http://jmlr.org/papers/v18/15-257.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-373,
 abstract = {Supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes. In this work, we propose a theoretical study of supervised manifold learning for classification. We consider nonlinear dimensionality reduction algorithms that yield linearly separable embeddings of training data and present generalization bounds for this type of algorithms. A necessary condition for satisfactory generalization performance is that the embedding allow the construction of a sufficiently regular interpolation function in relation with the separation margin of the embedding. We show that for supervised embeddings satisfying this condition, the classification error decays at an exponential rate with the number of training samples. Finally, we examine the separability of supervised nonlinear embeddings that aim to preserve the low-dimensional geometric structure of data based on graph representations. The proposed analysis is supported by experiments on several real data sets.},
 author = {Elif Vural and Christine Guillemot},
 journal = {Journal of Machine Learning Research},
 number = {157},
 openalex = {W2963051525},
 pages = {1--55},
 title = {A Study of the Classification of Low-Dimensional Data with Supervised Manifold Learning},
 url = {http://jmlr.org/papers/v18/15-373.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-376,
 abstract = {We propose a novel class of kernels to alleviate the high computational cost of large-scale nonparametric learning with kernel methods. The proposed kernel is defined based on a hierarchical partitioning of the underlying data domain, where the Nyström method (a globally low-rank approximation) is married with a locally lossless approximation in a hierarchical fashion. The kernel maintains (strict) positive-definiteness. The corresponding kernel matrix admits a recursively off-diagonal low-rank structure, which allows for fast linear algebra computations. Suppressing the factor of data dimension, the memory and arithmetic complexities for training a regression or a classifier are reduced from $O(n^2)$ and $O(n^3)$ to $O(nr)$ and $O(nr^2)$, respectively, where $n$ is the number of training examples and $r$ is the rank on each level of the hierarchy. Although other randomized approximate kernels entail a similar complexity, empirical results show that the proposed kernel achieves a matching performance with a smaller $r$. We demonstrate comprehensive experiments to show the effective use of the proposed kernel on data sizes up to the order of millions.},
 author = {Jie Chen and Haim Avron and Vikas Sindhwani},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2963518317},
 pages = {1--42},
 title = {Hierarchically Compositional Kernels for Scalable Nonparametric Learning},
 url = {http://jmlr.org/papers/v18/15-376.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-397,
 abstract = {This paper presents an original Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler introduced in Andrieu et al. (2009, 2010). The resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio and can be implemented using existing sequential Monte Carlo libraries. We investigate its performance experimentally on synthetic problems as well as on geolocation data. Our results show that for a given computational budget, the Particle Gibbs Split-Merge sampler empirically outperforms existing split merge methods. The code and instructions allowing to reproduce the experiments is available at https://github.com/aroth85/pgsm.},
 author = {Alexandre Bouchard-C{{\^o}}t{{\'e}} and Arnaud Doucet and Andrew Roth},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2963647411},
 pages = {1--39},
 title = {Particle gibbs split-merge sampling for Bayesian inference in mixture models},
 url = {http://jmlr.org/papers/v18/15-397.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-403,
 author = {Mathieu Guillame-Bert and Artur Dubrawski},
 journal = {Journal of Machine Learning Research},
 number = {121},
 openalex = {W2770490030},
 pages = {1--34},
 title = {Classification of Time Sequences using Graphs of Temporal Constraints},
 url = {http://jmlr.org/papers/v18/15-403.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-441,
 abstract = {We introduce Refinery, an open source platform for exploring large text document collections with topic models. Refinery is a standalone web application driven by a graphical interface, so it is usable by those without machine learning or programming expertise. Users can interactively organize articles by topic and also refine this organization with phrase-level analysis. Under the hood, we train Bayesian nonparametric topic models that can adapt model complexity to the provided data with scalable learning algorithms. The project website http://daeilkim.github.io/refinery/ contains Python code and further documentation.},
 author = {Daeil Kim and Benjamin F. Swanson and Michael C. Hughes and Erik B. Sudderth},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W2608020720},
 pages = {1--5},
 title = {Refinery: an open source topic modeling web platform},
 url = {http://jmlr.org/papers/v18/15-441.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-449,
 abstract = {Biological brains can learn, recognize, organize, and re-generate large repertoires of temporal patterns. Here I propose a mechanism of neurodynamical pattern learning and representation, called co...},
 author = {Herbert Jaeger},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W3014685280},
 pages = {1--43},
 title = {Using conceptors to manage neural long-term memories for temporal patterns},
 url = {http://jmlr.org/papers/v18/15-449.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-464,
 abstract = {In this work, we address the problem of solving a series of underdetermined linear inverse problems subject to a sparsity constraint. We generalize the spike-and-slab prior distribution to encode a priori correlation of the support of the solution in both space and time by imposing a transformed Gaussian process on the spike-and-slab probabilities. An expectation propagation (EP) algorithm for posterior inference under the proposed model is derived. For large scale problems, the standard EP algorithm can be prohibitively slow. We therefore introduce three different approximation schemes to reduce the computational complexity. Finally, we demonstrate the proposed model using numerical experiments based on both synthetic and real data sets.},
 author = {Michael Riis Andersen and Aki Vehtari and Ole Winther and Lars Kai Hansen},
 journal = {Journal of Machine Learning Research},
 number = {139},
 openalex = {W2963993380},
 pages = {1--58},
 title = {Bayesian Inference for Spatio-temporal Spike-and-Slab Priors},
 url = {http://jmlr.org/papers/v18/15-464.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-468,
 abstract = {Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large data sets.},
 author = {Igor Melnyk and Arindam Banerjee},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W2164467169},
 pages = {1--39},
 title = {A spectral algorithm for inference in hidden semi-Markov models},
 url = {http://jmlr.org/papers/v18/15-468.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-481,
 abstract = {Ranking and comparing items is crucial for collecting information about preferences in many areas, from marketing to politics. The Mallows rank model is among the most successful approaches to analyse rank data, but its computational complexity has limited its use to a particular form based on Kendall distance. We develop new computationally tractable methods for Bayesian inference in Mallows models that work with any right-invariant distance. Our method performs inference on the consensus ranking of the items, also when based on partial rankings, such as top-k items or pairwise comparisons. We prove that items that none of the assessors has ranked do not influence the maximum a posteriori consensus ranking, and can therefore be ignored. When assessors are many or heterogeneous, we propose a mixture model for clustering them in homogeneous subgroups, with cluster-specific consensus rankings. We develop approximate stochastic algorithms that allow a fully probabilistic analysis, leading to coherent quantifications of uncertainties. We make probabilistic predictions on the class membership of assessors based on their ranking of just some items, and predict missing individual preferences, as needed in recommendation systems. We test our approach using several experimental and benchmark datasets.},
 author = {Valeria Vitelli and {{\O}}ystein S{{\o}}rensen and Marta Crispino and Arnoldo Frigessi and Elja Arjas},
 journal = {Journal of Machine Learning Research},
 number = {158},
 openalex = {W2964331234},
 pages = {1--49},
 title = {Probabilistic preference learning with the Mallows rank model},
 url = {http://jmlr.org/papers/v18/15-481.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-482,
 abstract = {We study a set of regularization methods for high-dimensional linear regression models. These penalized estimators have the square root of the residual sum of squared errors as loss function, and any weakly decomposable norm as penalty function. This fit measure is chosen because of its property that the estimator does not depend on the unknown standard deviation of the noise. On the other hand, a generalized weakly decomposable norm penalty is very useful in being able to deal with different underlying sparsity structures. We can choose a different sparsity inducing norm depending on how we want to interpret the unknown parameter vector $\beta$. Structured sparsity norms, as defined in Micchelli et al. [18], are special cases of weakly decomposable norms, therefore we also include the square root LASSO (Belloni et al. [3]), the group square root LASSO (Bunea et al. [10]) and a new method called the square root SLOPE (in a similar fashion to the SLOPE from Bogdan et al. [6]). For this collection of estimators our results provide sharp oracle inequalities with the Karush-Kuhn-Tucker conditions. We discuss some examples of estimators. Based on a simulation we illustrate some advantages of the square root SLOPE.},
 author = {Benjamin Stucky and Sara van de Geer},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W2963915879},
 pages = {1--29},
 title = {Sharp Oracle Inequalities for Square Root Regularization},
 url = {http://jmlr.org/papers/v18/15-482.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-484,
 abstract = {Let P be a distribution with support S. The salient features of S can be quantified with persistent homology, which summarizes topological features of the sublevel sets of the distance function (the distance of any point x to S). Given a sample from P we can infer the persistent homology using an empirical version of the distance function. However, the empirical distance function is highly non-robust to noise and outliers. Even one outlier is deadly. The distance-to-a-measure (DTM), introduced by Chazal et al. (2011), and the kernel distance, introduced by Phillips et al. (2014), are smooth functions that provide useful topological information but are robust to noise and outliers. Chazal et al. (2014) derived concentration bounds for DTM. Building on these results, we derive limiting distributions and confidence sets, and we propose a method for choosing tuning parameters.},
 author = {Fr{\'e}d{\'e}ric Chazal and Brittany Fasy and Fabrizio Lecci and Bertr and Michel and Aless and ro Rinaldo and Larry Wasserman},
 journal = {Journal of Machine Learning Research},
 number = {159},
 openalex = {W1492288577},
 pages = {1--40},
 title = {Robust Topological Inference: Distance To a Measure and Kernel Distance},
 url = {http://jmlr.org/papers/v18/15-484.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-486,
 abstract = {We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multiview mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components k is much larger than the data dimension d, up to k = o(d1.5). We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models.},
 author = {Animashree An and kumar and Rong Ge and Majid Janzamin},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W122168659},
 pages = {1--40},
 title = {Analyzing tensor power method dynamics in overcomplete regime},
 url = {http://jmlr.org/papers/v18/15-486.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-492,
 abstract = {SnapVX is a high-performance solver for convex optimization problems defined on networks. For problems of this form, SnapVX provides a fast and scalable solution with guaranteed global convergence. It combines the capabilities of two open source software packages: Snap.py and CVXPY. Snap.py is a large scale graph processing library, and CVXPY provides a general modeling framework for small-scale subproblems. SnapVX offers a customizable yet easy-to-use Python interface with "out-of-the-box" functionality. Based on the Alternating Direction Method of Multipliers (ADMM), it is able to efficiently store, analyze, parallelize, and solve large optimization problems from a variety of different applications. Documentation, examples, and more can be found on the SnapVX website at http://snap.stanford.edu/snapvx.},
 author = {David Hallac and Christopher Wong and Steven Diamond and Abhijit Sharang and Rok Sosi{\v{c}} and Stephen Boyd and Jure Leskovec},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2964051704},
 pages = {1--5},
 title = {SnapVX: A Network-Based Convex Optimization Solver.},
 url = {http://jmlr.org/papers/v18/15-492.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-495,
 abstract = {Many of the ordinal regression models that have been proposed in the literature can be seen as methods that minimize a convex surrogate of the zero-one, absolute, or squared loss functions. A key property that allows to study the statistical implications of such approximations is that of Fisher consistency. Fisher consistency is a desirable property for surrogate loss functions and implies that in the population setting, i.e., if the probability distribution that generates the data were available, then optimization of the surrogate would yield the best possible model. In this paper we will characterize the Fisher consistency of a rich family of surrogate loss functions used in the context of ordinal regression, including support vector ordinal regression, ORBoosting and least absolute deviation. We will see that, for a family of surrogate loss functions that subsumes support vector ordinal regression and ORBoosting, consistency can be fully characterized by the derivative of a real-valued function at zero, as happens for convex margin-based surrogates in binary classification. We also derive excess risk bounds for a surrogate of the absolute error that generalize existing risk bounds for binary classification. Finally, our analysis suggests a novel surrogate of the squared error loss. We compare this novel surrogate with competing approaches on 9 different datasets. Our method shows to be highly competitive in practice, outperforming the least squares loss on 7 out of 9 datasets.},
 author = {Fabian Pedregosa and Francis Bach and Alexandre Gramfort},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W2107505626},
 pages = {1--35},
 title = {On the Consistency of Ordinal Regression Methods},
 url = {http://jmlr.org/papers/v18/15-495.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-506,
 abstract = {How can we train a statistical mixture model on a massive data set? In this work we show how to construct coresets for mixtures of Gaussians. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can harness computationally intensive algorithms to compute a good approximation on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings and do not impose restrictions on the data generating process. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new combinatorial complexity results for mixtures of Gaussians. Empirical evaluation on several real-world data sets suggests that our coreset-based approach enables significant reduction in training-time with negligible approximation error.},
 author = {Mario Lucic and Matthew Faulkner and Andreas Krause and Dan Feldman},
 journal = {Journal of Machine Learning Research},
 number = {160},
 openalex = {W2601251344},
 pages = {1--25},
 title = {Training Gaussian mixture models at scale via coresets},
 url = {http://jmlr.org/papers/v18/15-506.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-566,
 abstract = {In this paper, we show that the popular C-SVM, soft-margin support vector classifier is equivalent to minimization of Buffered Probability of Exceedance (bPOE) by introducing of a new SVM formulation, called the EC-SVM, which is derived as a bPOE minimization problem. Being derived from a simple bPOE minimization problem, we show that the EC-SVM is simple to interpret with a meaningful free parameter, optimal objective value, and probabilistic derivation. We then connect the EC-SVM to existing SVM formulations. We first show that the C-SVM, formulated with any regularization norm, produces the same set of solutions as the EC-SVM over the same parameter range. Additionally, we show that the Eν-SVM, formulated with any regularization norm, produces the same set of solutions as the EC-SVM over their entire parameter range. These equivalences, coupled with the interpretability of the EC-SVM, allow us to gain surprising new insights into the C-SVM and fully connect soft margin support vector classification with superquantile and bPOE concepts. Additionally, we provide general dual formulations for the EC-SVM and C-SVM allowing for a brief discussion of application of the kernel trick to the EC-SVM.},
 author = {Matthew Norton and Alexander Mafusalov and Stan Uryasev},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2756994905},
 pages = {1--43},
 title = {Soft Margin Support Vector Classification as Buffered Probability Minimization},
 url = {http://jmlr.org/papers/v18/15-566.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-586,
 abstract = {We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds in expectation in both the $L^2$-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our error bounds are sharp and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in the literature.},
 author = {Shao-Bo Lin and Xin Guo and Ding-Xuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W2963020641},
 pages = {1--31},
 title = {Distributed Learning with Regularized Least Squares},
 url = {http://jmlr.org/papers/v18/15-586.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-592,
 abstract = {This paper aims at achieving a "good" estimator for the gradient of a function on a high-dimensional space. Often such functions are not sensitive in all coordinates and the gradient of the function is almost sparse. We propose a method for gradient estimation that combines ideas from Spall's Simultaneous Perturbation Stochastic Approximation with compressive sensing. The aim is to obtain "good" estimator without too many function evaluations. Application to estimating gradient outer product matrix as well as standard optimization problems are illustrated via simulations.},
 author = {Vivek S. Borkar and Vikranth R. Dwaracherla and Neeraja Sahasrabudhe},
 journal = {Journal of Machine Learning Research},
 number = {161},
 openalex = {W2173041192},
 pages = {1--27},
 title = {Gradient Estimation with Simultaneous Perturbation and Compressive Sensing},
 url = {http://jmlr.org/papers/v18/15-592.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-595,
 abstract = {Latent Dirichlet Allocation (LDA) is a well known topic model that is often used to make inference regarding the properties of collections of text documents. LDA is a hierarchical Bayesian model, a...},
 author = {Clint P. George and Hani Doss},
 journal = {Journal of Machine Learning Research},
 number = {162},
 openalex = {W2803512450},
 pages = {1--38},
 title = {Principled Selection of Hyperparameters in the Latent Dirichlet Allocation Model},
 url = {http://jmlr.org/papers/v18/15-595.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-596,
 author = {Takashi Takenouchi and Takafumi Kanamori},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2739319285},
 pages = {1--26},
 title = {Statistical Inference with Unnormalized Discrete Models and Localized Homogeneous Divergences},
 url = {http://jmlr.org/papers/v18/15-596.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-613,
 abstract = {Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics and econometrics. In this paper, we revisit the classical rank-constrained FA problem, which seeks to approximate an observed covariance matrix ($\boldsymbol\Sigma$), by the sum of a Positive Semidefinite (PSD) low-rank component ($\boldsymbol\Theta$) and a diagonal matrix ($\boldsymbol\Phi$) (with nonnegative entries) subject to $\boldsymbol\Sigma - \boldsymbol\Phi$ being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are certifiably optimal in many cases, even for problems with thousands of variables. Our techniques are general and make no assumption on the underlying problem data. The estimator proposed herein, aids statistical interpretability, provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in discrete optimization.},
 author = {Dimitris Bertsimas and Martin S. Copenhaver and Rahul Mazumder},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W4295827631},
 pages = {1--53},
 title = {Certifiably Optimal Low Rank Factor Analysis},
 url = {http://jmlr.org/papers/v18/15-613.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-615,
 abstract = {Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search-based techniques. DPVI is based on a novel family of particle-based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well-defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper defines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can offer appealing time/accuracy trade-offs as compared to multiple alternatives. ∗First two authors contributed equally.},
 author = {Ardavan Saeedi and Tejas D. Kulkarni and Vikash K. Mansinghka and Samuel J. Gershman},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W2963704192},
 pages = {1--29},
 title = {Variational Particle Approximations},
 url = {http://jmlr.org/papers/v18/15-615.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-619,
 abstract = {Binary embeddings provide efficient and powerful ways to perform operations on large scale data. However binary embedding typically requires long codes in order to preserve the discriminative power of the input space. Thus binary coding methods traditionally suffer from high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure allows us to use Fast Fourier Transform algorithms to speed up the computation. For obtaining k-bit binary codes from d-dimensional data, our method improves the time complexity from O(dk) to O(d log d), and the space complexity from O(dk) to O(d).

We study two settings, which differ in the way we choose the parameters of the circulant matrix. In the first, the parameters are chosen randomly and in the second, the parameters are learned using the data. For randomized CBE, we give a theoretical analysis comparing it with binary embedding using an unstructured random projection matrix. The challenge here is to show that the dependencies in the entries of the circulant matrix do not lead to a loss in performance. In the second setting, we design a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. In both the settings, we show by extensive experiments that the CBE approach gives much better performance than the state-of-the-art approaches if we fix a running time, and provides much faster computation with negligible performance degradation if we fix the number of bits in the embedding.},
 author = {Felix X. Yu and Aditya Bhaskara and Sanjiv Kumar and Yunchao Gong and Shih-Fu Chang},
 journal = {Journal of Machine Learning Research},
 number = {150},
 openalex = {W2962872625},
 pages = {1--30},
 title = {On binary embedding using circulant matrices},
 url = {http://jmlr.org/papers/v18/15-619.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-631,
 abstract = {A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hingeloss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.},
 author = {Stephen H. Bach and Matthias Broecheler and Bert Huang and Lise Getoor},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W2963572185},
 pages = {1--67},
 title = {Hinge-loss Markov random fields and probabilistic soft logic},
 url = {http://jmlr.org/papers/v18/15-631.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-636,
 abstract = {In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account \emph{risk}, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. 
Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application.},
 author = {Yinlam Chow and Mohammad Ghavamzadeh and Lucas Janson and Marco Pavone},
 journal = {Journal of Machine Learning Research},
 number = {167},
 openalex = {W2963082979},
 pages = {1--51},
 title = {Risk-Constrained Reinforcement Learning with Percentile Risk Criteria},
 url = {http://jmlr.org/papers/v18/15-636.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:15-650,
 abstract = {We study the problem of identifying unreliable and adversarial workers in crowdsourcing systems where workers (or users) provide labels for tasks (or items). Most existing studies assume that worker responses follow specific probabilistic models; however, recent evidence shows the presence of workers adopting non-random or even malicious strategies. To account for such workers, we suppose that workers comprise a mixture of honest and adversarial workers. Honest workers may be reliable or unreliable, and they provide labels according to an unknown but explicit probabilistic model. Adversaries adopt labeling strategies different from those of honest workers, whether probabilistic or not. We propose two reputation algorithms to identify unreliable honest workers and adversarial workers from only their responses. Our algorithms assume that honest workers are in the majority, and they classify workers with outlier label patterns as adversaries. Theoretically, we show that our algorithms successfully identify unreliable honest workers, workers adopting deterministic strategies, and worst-case sophisticated adversaries who can adopt arbitrary labeling strategies to degrade the accuracy of the inferred task labels. Empirically, we show that filtering out outliers using our algorithms can significantly improve the accuracy of several state-of-the-art label aggregation algorithms in real-world crowdsourcing datasets.},
 author = {Srikanth Jagabathula and Lakshminarayanan Subramanian and Ashwin Venkataraman},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W2761817502},
 pages = {1--67},
 title = {Identifying unreliable and adversarial workers in crowdsourced labeling tasks},
 url = {http://jmlr.org/papers/v18/15-650.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-651,
 abstract = {In this paper, we investigate a group sparse optimization problem via lp,q regularization in three aspects: theory, algorithm and application. In the theoretical aspect, by introducing a notion of ...},
 author = {Yaohua Hu and Chong Li and Kaiwen Meng and Jing Qin and Xiaoqi Yang},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W3006343684},
 pages = {1--52},
 title = {Group sparse optimization via lp,q regularization},
 url = {http://jmlr.org/papers/v18/15-651.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:15-659,
 abstract = {We present a general framework for graph clustering and bi-clustering where we are given a general observation (called a label) between each pair of nodes. This framework allows a rich encoding of various types of pairwise interactions between nodes. We propose a new tractable and robust approach to this problem based on convex optimization and maximum likelihood estimators. We analyze our algorithms under a general statistical model extending the planted partition and stochastic block models. Both sufficient and necessary conditions are provided for successful recovery of the underlying clusters. Our theoretical results subsume many existing graph clustering results for a wide range of settings, including planted partition, weighted clustering, submatrix localization and partially observed graphs. Furthermore, our results are applicable to novel settings including time-varying graphs, providing new insights to solutions of these problems. We provide empirical results on both synthetic and real data that corroborate with our theoretical findings.},
 author = {Shiau Hong Lim and Yudong Chen and Huan Xu},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W2652776740},
 pages = {1--47},
 title = {Clustering from general pairwise observations with applications to time-varying graphs},
 url = {http://jmlr.org/papers/v18/15-659.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-002,
 abstract = {We devise a communication-efficient approach to distributed sparse regression in the high-dimensional setting. The key idea is to average debiased or desparsified lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines, and consistently estimates the support under weaker conditions than the lasso. On the computational side, we propose a new parallel and computationally-efficient algorithm to compute the approximate inverse covariance required in the debiasing approach, when the dataset is split across samples. We further extend the approach to generalized linear models.},
 author = {Jason D. Lee and Qiang Liu and Yuekai Sun and Jonathan E. Taylor},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2595198670},
 pages = {1--30},
 title = {Communication-efficient sparse regression},
 url = {http://jmlr.org/papers/v18/16-002.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-003,
 abstract = {We present a machine learning algorithm for building classifiers that are comprised of a small number of short rules. These are restricted disjunctive normal form models. An example of a classifier of this form is as follows: If X satisfies (condition A AND condition B) OR (condition C) OR ..., then Y = 1. Models of this form have the advantage of being interpretable to human experts since they produce a set of rules that concisely describe a specific class. We present two probabilistic models with prior parameters that the user can set to encourage the model to have a desired size and shape, to conform with a domain-specific definition of interpretability. We provide a scalable MAP inference approach and develop theoretical bounds to reduce computation by iteratively pruning the search space. We apply our method (Bayesian Rule Sets - BRS) to characterize and predict user behavior with respect to in-vehicle context-aware personalized recommender systems. Our method has a major advantage over classical associative classification methods and decision trees in that it does not greedily grow the model.},
 author = {Tong Wang and Cynthia Rudin and Finale Doshi-Velez and Yimin Liu and Erica Klampfl and Perry MacNeille},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W2762409054},
 pages = {1--37},
 title = {A Bayesian framework for learning rule sets for interpretable classification},
 url = {http://jmlr.org/papers/v18/16-003.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-011,
 abstract = {In this paper, we consider an infinite dimensional exponential family P of probability densities,
which are parametrized by functions in a reproducing kernel Hilbert space H, and
show it to be quite rich in the sense that a broad class of densities on R
d
can be approximated
arbitrarily well in Kullback-Leibler (KL) divergence by elements in P. Motivated by
this approximation property, the paper addresses the question of estimating an unknown
density p0 through an element in P. Standard techniques like maximum likelihood estimation
(MLE) or pseudo MLE (based on the method of sieves), which are based on minimizing
the KL divergence between p0 and P, do not yield practically useful estimators because of
their inability to efficiently handle the log-partition function. We propose an estimator ˆpn
based on minimizing the Fisher divergence, J(p0kp) between p0 and p ∈ P, which involves
solving a simple finite-dimensional linear system. When p0 ∈ P, we show that the proposed
estimator is consistent, and provide a convergence rate of n
− min{
2
3
,
2β+1
2β+2 } in Fisher
divergence under the smoothness assumption that log p0 ∈ R(C
β
) for some β ≥ 0, where
C is a certain Hilbert-Schmidt operator on H and R(C
β
) denotes the image of C
β
. We
also investigate the misspecified case of p0 ∈ P/ and show that J(p0kpˆn) → infp∈P J(p0kp)
as n → ∞, and provide a rate for this convergence under a similar smoothness condition
as above. Through numerical simulations we demonstrate that the proposed estimator
outperforms the non-parametric kernel density estimator, and that the advantage of the
proposed estimator grows as d increases.},
 author = {Bharath Sriperumbudur and Kenji Fukumizu and Arthur Gretton and Aapo Hyv\"{a}rinen and Revant Kumar},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W2963993553},
 pages = {1--59},
 title = {Density Estimation in Infinite Dimensional Exponential Families},
 url = {http://jmlr.org/papers/v18/16-011.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-017,
 abstract = {We present an approach based on feed-forward neural networks for learning the distribution of textual documents. This approach is inspired by the Neural Autoregressive Distribution Estimator(NADE) model, which has been shown to be a good estimator of the distribution of discrete-valued igh-dimensional vectors. In this paper, we present how NADE can successfully be adapted to the case of textual data, retaining from NADE the property that sampling or computing the probability of observations can be done exactly and efficiently. The approach can also be used to learn deep representations of documents that are competitive to those learned by the alternative topic modeling approaches. Finally, we describe how the approach can be combined with a regular neural network N-gram model and substantially improve its performance, by making its learned representation sensitive to the larger, document-specific context.},
 author = {Stanislas Lauly and Yin Zheng and Alex and re Allauzen and Hugo Larochelle},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W2962836999},
 pages = {1--24},
 title = {Document Neural Autoregressive Distribution Estimation},
 url = {http://jmlr.org/papers/v18/16-017.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-046,
 abstract = {Dictionaries are collections of vectors used for representations of random vectors in Euclidean spaces. Recent research on optimal dictionaries is focused on constructing dictionaries that offer sparse representations, i.e., $\ell_0$-optimal representations. Here we consider the problem of finding optimal dictionaries with which representations of samples of a random vector are optimal in an $\ell_2$-sense: optimality of representation is defined as attaining the minimal average $\ell_2$-norm of the coefficients used to represent the random vector. With the help of recent results on rank-$1$ decompositions of symmetric positive semidefinite matrices, we provide an explicit description of $\ell_2$-optimal dictionaries as well as their algorithmic constructions in polynomial time.},
 author = {Mohammed Rayyan Sheriff and Debasish Chatterjee},
 journal = {Journal of Machine Learning Research},
 number = {107},
 openalex = {W2294542821},
 pages = {1--28},
 title = {Optimal dictionary for least squares representation},
 url = {http://jmlr.org/papers/v18/16-046.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-061,
 abstract = {In recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as $d(A,B)<d(C,D)$. For many problems in machine learning and statistics it is unclear how to solve them in such a scenario. Up to now, the main approach is to explicitly construct an ordinal embedding of the data points in the Euclidean space, an approach that has a number of drawbacks. In this paper, we propose algorithms for the problems of medoid estimation, outlier identification, classification, and clustering when given only ordinal data. They are based on estimating the lens depth function and the $k$-relative neighborhood graph on a data set. Our algorithms are simple, are much faster than an ordinal embedding approach and avoid some of its drawbacks, and can easily be parallelized.},
 author = {Matth{{\"a}}us Kleindessner and Ulrike von Luxburg},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W2963374513},
 pages = {1--52},
 title = {Lens Depth Function and k-Relative Neighborhood Graph: Versatile Tools for Ordinal Data Analysis},
 url = {http://jmlr.org/papers/v18/16-061.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-070,
 abstract = {Inference methods are often formulated as variational approximations: these approximations allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima.},
 author = {Jack Raymond and Federico Ricci-Tersenghi},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W4297667953},
 pages = {1--36},
 title = {Improving variational methods via pairwise linear response identities},
 url = {http://jmlr.org/papers/v18/16-070.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-079,
 author = {A. Adam Ding and Jennifer G. Dy and Yi Li and Yale Chang},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2758404062},
 pages = {1--46},
 title = {A Robust-Equitable Measure for Feature Ranking and Selection},
 url = {http://jmlr.org/papers/v18/16-079.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-087,
 author = {Simone Filice and Giuseppe Castellucci and Giovanni Da San Martino and Aless and ro Moschitti and Danilo Croce and Roberto Basili},
 journal = {Journal of Machine Learning Research},
 number = {191},
 openalex = {W2808660397},
 pages = {1--5},
 title = {KELP: a Kernel-based Learning Platform},
 url = {http://jmlr.org/papers/v18/16-087.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-093,
 abstract = {The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling of discrete time series, with applications including speech recognition, computational biology, computer vision and econometrics. Estimating an HMM from its observation process is often addressed via the Baum-Welch algorithm, which is known to be susceptible to local optima. In this paper, we first give a general characterization of the basin of attraction associated with any global optimum of the population likelihood. By exploiting this characterization, we provide non-asymptotic finite sample guarantees on the Baum-Welch updates, guaranteeing geometric convergence to a small ball of radius on the order of the minimax rate around a global optimum. As a concrete example, we prove a linear rate of convergence for a hidden Markov mixture of two isotropic Gaussians given a suitable mean separation and an initialization within a ball of large radius around (one of) the true parameters. To our knowledge, these are the first rigorous local convergence guarantees to global optima for the Baum-Welch algorithm in a setting where the likelihood function is nonconvex. We complement our theoretical results with thorough numerical simulations studying the convergence of the Baum-Welch algorithm and illustrating the accuracy of our predictions.},
 author = {Fanny Yang and Sivaraman Balakrishnan and Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {125},
 openalex = {W2782579196},
 pages = {1--53},
 title = {Statistical and Computational Guarantees for the Baum-Welch Algorithm},
 url = {http://jmlr.org/papers/v18/16-093.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-100,
 abstract = {In a series of recent works, we have generalised the consistency results in the stochastic block model literature to the case of uniform and non-uniform hypergraphs. The present paper continues the same line of study, where we focus on partitioning weighted uniform hypergraphs---a problem often encountered in computer vision. This work is motivated by two issues that arise when a hypergraph partitioning approach is used to tackle computer vision problems: (i) The uniform hypergraphs constructed for higher-order learning contain all edges, but most have negligible weights. Thus, the adjacency tensor is nearly sparse, and yet, not binary. (ii) A more serious concern is that standard partitioning algorithms need to compute all edge weights, which is computationally expensive for hypergraphs. This is usually resolved in practice by merging the clustering algorithm with a tensor sampling strategy---an approach that is yet to be analysed rigorously. We build on our earlier work on partitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati, ICML, 2015), and address the aforementioned issues by proposing provable and efficient partitioning algorithms. Our analysis justifies the empirical success of practical sampling techniques. We also complement our theoretical findings by elaborate empirical comparison of various hypergraph partitioning schemes.},
 author = {Debarghya Ghoshdastidar and Ambedkar Dukkipati},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W2287783892},
 pages = {1--41},
 title = {Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques},
 url = {http://jmlr.org/papers/v18/16-100.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-107,
 abstract = {Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models--no conjugacy assumptions are required. We study ADVI across ten modern probabilistic models and apply it to a dataset with millions of observations. We deploy ADVI as part of Stan, a probabilistic programming system.},
 author = {Alp Kucukelbir and Dustin Tran and Rajesh Ranganath and Andrew Gelman and David M. Blei},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W2962994101},
 pages = {1--45},
 title = {Automatic differentiation variational inference},
 url = {http://jmlr.org/papers/v18/16-107.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-108,
 abstract = {This paper presents a multiscale approach to efficiently compute approximate optimal transport plans between point sets. It is particularly well-suited for point sets that are in high-dimensions, but are close to being intrinsically low-dimensional. The approach is based on an adaptive multiscale decomposition of the point sets. The multiscale decomposition yields a sequence of optimal transport problems, that are solved in a top-to-bottom fashion from the coarsest to the finest scale. We provide numerical evidence that this multiscale approach scales approximately linearly, in time and memory, in the number of nodes, instead of quadratically or worse for a direct solution. Empirically, the multiscale approach results in less than one percent relative error in the objective function. Furthermore, the multiscale plans constructed are of interest by themselves as they may be used to introduce novel features and notions of distances between point sets. An analysis of sets of brain MRI based on optimal transport distances illustrates the effectiveness of the proposed method on a real world data set. The application demonstrates that multiscale optimal transport distances have the potential to improve on state-of-the-art metrics currently used in computational anatomy.},
 author = {Samuel Gerber and Mauro Maggioni},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W2963805841},
 pages = {1--32},
 title = {Multiscale Strategies for Computing Optimal Transport},
 url = {http://jmlr.org/papers/v18/16-108.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-119,
 author = {Siqi Wu and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {168},
 openalex = {W2962922286},
 pages = {1--56},
 title = {Local Identifiability of $\ell_1$-minimization Dictionary Learning: a Sufficient and Almost Necessary Condition},
 url = {http://jmlr.org/papers/v18/16-119.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-131,
 abstract = {Java Statistical Analysis Tool (JSAT) is a Machine Learning library written in pure Java. It works to fill a void in the Java ecosystem for a general purpose library that is relatively high performance and flexible, which is not adequately fulfilled by Weka (Hall et al., 2009) and Java-ML (Abeel et al., 2009). Almost all of the algorithms are independently implemented using an Object-Oriented framework. JSAT is made available under the GNU GPL license here: https://github.com/EdwardRaff/JSAT.},
 author = {Edward Raff},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2608296545},
 pages = {1--5},
 title = {JSAT: Java statistical analysis tool, a library for machine learning},
 url = {http://jmlr.org/papers/v18/16-131.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-132,
 abstract = {Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics

We propose a temporal point process model, Coevolve, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks such as Twitter. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives.},
 author = {Mehrdad Farajtabar and Yichen Wang and Manuel Gomez-Rodriguez and Shuang Li and Hongyuan Zha and Le Song},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W2962702810},
 pages = {1--49},
 title = {COEVOLVE: a joint point process model for information diffusion and network evolution},
 url = {http://jmlr.org/papers/v18/16-132.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-140,
 abstract = {In statistics and machine learning, people are often interested in the eigenvectors (or singular vectors) of certain matrices (e.g. covariance matrices, data matrices, etc). However, those matrices are usually perturbed by noises or statistical errors, either from random sampling or structural patterns. One usually employs Davis-Kahan $\sin \theta$ theorem to bound the difference between the eigenvectors of a matrix $A$ and those of a perturbed matrix $\widetilde{A} = A + E$, in terms of $\ell_2$ norm. In this paper, we prove that when $A$ is a low-rank and incoherent matrix, the $\ell_{\infty}$ norm perturbation bound of singular vectors (or eigenvectors in the symmetric case) is smaller by a factor of $\sqrt{d_1}$ or $\sqrt{d_2}$ for left and right vectors, where $d_1$ and $d_2$ are the matrix dimensions. The power of this new perturbation result is shown in robust covariance estimation, particularly when random variables have heavy tails. There, we propose new robust covariance estimators and establish their asymptotic properties using the newly developed perturbation bound. Our theoretical results are verified through extensive numerical experiments.},
 author = {Jianqing Fan and Weichen Wang and Yiqiao Zhong},
 journal = {Journal of Machine Learning Research},
 number = {207},
 openalex = {W2295296939},
 pages = {1--42},
 title = {An $\ell_{\infty}$ Eigenvector Perturbation Bound and Its Application to Robust Covariance Estimation},
 url = {http://jmlr.org/papers/v18/16-140.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-142,
 abstract = {Learning complex control policies from non-linear and redundant sensory input is an important challenge for reinforcement learning algorithms. Non-parametric methods that approximate values functions or transition models can address this problem, by adapting to the complexity of the data set. Yet, many current non-parametric approaches rely on unstable greedy maximization of approximate value functions, which might lead to poor convergence or oscillations in the policy update. A more robust policy update can be obtained by limiting the information loss between successive state-action distributions. In this paper, we develop a policy search algorithm with policy updates that are both robust and non-parametric. Our method can learn non- parametric control policies for infinite horizon continuous Markov decision processes with non-linear and redundant sensory representations. We investigate how we can use approximations of the kernel function to reduce the time requirements of the demanding non-parametric computations. In our experiments, we show the strong performance of the proposed method, and how it can be approximated efficiently. Finally, we show that our algorithm can learn a real-robot under-powered swing-up task directly from image data.},
 author = {Herke van Hoof and Gerhard Neumann and Jan Peters},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2751302235},
 pages = {1--46},
 title = {Non-parametric policy search with limited information loss},
 url = {http://jmlr.org/papers/v18/16-142.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-146,
 abstract = {In this paper, we aim at recovering an undirected weighted graph of $N$ vertices from the knowledge of a perturbed version of the eigenspaces of its adjacency matrix $W$. For instance, this situation arises for stationary signals on graphs or for Markov chains observed at random times. Our approach is based on minimizing a cost function given by the Frobenius norm of the commutator $\mathsf{A} \mathsf{B}-\mathsf{B} \mathsf{A}$ between symmetric matrices $\mathsf{A}$ and $\mathsf{B}$. In the Erd\H{o}s-R\'enyi model with no self-loops, we show that identifiability (i.e., the ability to reconstruct $W$ from the knowledge of its eigenspaces) follows a sharp phase transition on the expected number of edges with threshold function $N\log N/2$. Given an estimation of the eigenspaces based on a $n$-sample, we provide support selection procedures from theoretical and practical point of views. In particular, when deleting an edge from the active support, our study unveils that our test statistic is the order of $\mathcal O(1/n)$ when we overestimate the true support and lower bounded by a positive constant when the estimated support is smaller than the true support. This feature leads to a powerful practical support estimation procedure. Simulated and real life numerical experiments assert our new methodology.},
 author = {Yohann De Castro and Thibault Espinasse and Paul Rochet},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W2312416291},
 pages = {1--24},
 title = {Reconstructing undirected graphs from eigenspaces},
 url = {http://jmlr.org/papers/v18/16-146.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-147,
 abstract = {We study the usefulness of conditional gradient like methods for determining projections onto convex sets, in particular, projections onto naturally arising convex sets in reproducing kernel Hilbert spaces. Our work is motivated by the recently introduced kernel herding algorithm which is closely related to the Conditional Gradient Method (CGM). It is known that the herding algorithm converges with a rate of 1/t , where t counts the number of iterations, when a point in the interior of a convex set is approximated. We generalize this result and we provide a necessary and sufficient condition for the algorithm to approximate projections with a rate of 1/t . The CGM, which is in general vastly superior to the herding algorithm, achieves only an inferior rate of 1/√t in this setting. We study the usefulness of such projection algorithms further by exploring ways to use these for solving concrete machine learning problems. In particular, we derive non-parametric regression algorithms which use at their core a slightly modified kernel herding algorithm to determine projections. We derive bounds to control approximation errors of these methods and we demonstrate via experiments that the developed regressors are en-par with state-of-the-art regression algorithms for large scale problems.},
 author = {Steffen Gr{{\"u}}new{{\"a}}lder},
 journal = {Journal of Machine Learning Research},
 number = {219},
 openalex = {W2884339463},
 pages = {1--43},
 title = {Compact convex projections},
 url = {http://jmlr.org/papers/v18/16-147.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-166,
 abstract = {Obtaining a non-parametric expression for an interventional distribution is one of the most fundamental tasks in causal inference. Such an expression can be obtained for an identifiable causal effect by an algorithm or by manual application of do-calculus. Often we are left with a complicated expression which can lead to biased or inefficient estimates when missing data or measurement errors are involved.

We present an automatic simplification algorithm that seeks to eliminate symbolically unnecessary variables from these expressions by taking advantage of the structure of the underlying graphical model. Our method is applicable to all causal effect formulas and is readily available in the R package causaleffect.},
 author = {Santtu Tikka and Juha Karvanen},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W2610740996},
 pages = {1--30},
 title = {Simplifying probabilistic expressions in causal inference},
 url = {http://jmlr.org/papers/v18/16-166.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-172,
 abstract = {Model selection consistency in the high-dimensional regression setting can be achieved only if strong assumptions are fulfilled. We therefore suggest to pursue a different goal, which we call a minimal class of models. The minimal class of models includes models that are similar in their prediction accuracy but not necessarily in their elements. We suggest a random search algorithm to reveal candidate models. The algorithm implements simulated annealing while using a score for each predictor that we suggest to derive using a combination of the Lasso and the Elastic Net. The utility of using a minimal class of models is demonstrated in the analysis of two datasets.},
 author = {Daniel Nevo and Ya'acov Ritov},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W2963480832},
 pages = {1--29},
 title = {Identifying a minimal class of models for high-dimensional data},
 url = {http://jmlr.org/papers/v18/16-172.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-174,
 abstract = {Tuning the regularisation and kernel hyperparameters is a vital step in optimising the generalisation performance of kernel methods, such as the support vector machine (SVM). This is most often per...},
 author = {Jacques Wainer and Gavin Cawley},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W3006551545},
 pages = {1--35},
 title = {Empirical evaluation of resampling procedures for optimising SVM hyperparameters},
 url = {http://jmlr.org/papers/v18/16-174.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-184,
 author = {Martin Bilodeau and Aur{{\'e}}lien Guetsop Nangue},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W2756560744},
 pages = {1--40},
 title = {Tests of Mutual or Serial Independence of Random Vectors with Applications},
 url = {http://jmlr.org/papers/v18/16-184.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-190,
 abstract = {Stochastic gradient descent (SGD) is commonly used for optimization in large-scale machine learning problems. Langford et al. (2009) introduce a sparse online learning method to induce sparsity via truncated gradient. With high-dimensional sparse data, however, the method suffers from slow convergence and high variance due to the heterogeneity in feature sparsity. To mitigate this issue, we introduce a stabilized truncated stochastic gradient descent algorithm. We employ a soft-thresholding scheme on the weight vector where the imposed shrinkage is adaptive to the amount of information available in each feature. The variability in the resulted sparse weight vector is further controlled by stability selection integrated with the informative truncation. To facilitate better convergence, we adopt an annealing strategy on the truncation rate, which leads to a balanced trade-off between exploration and exploitation in learning a sparse weight vector. Numerical experiments show that our algorithm compares favorably with the original algorithm in terms of prediction accuracy, achieved sparsity and stability.},
 author = {Yuting Ma and Tian Zheng},
 journal = {Journal of Machine Learning Research},
 number = {131},
 openalex = {W2963625598},
 pages = {1--36},
 title = {Stabilized Sparse Online Learning for Sparse Data},
 url = {http://jmlr.org/papers/v18/16-190.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-191,
 abstract = {One of the most challenging problems in kernel online learning is to bound the model size and to promote the model sparsity. Sparse models not only improve computation and memory usage, but also enhance the generalization capacity, a principle that concurs with the law of parsimony. However, inappropriate sparsity modeling may also significantly degrade the performance. In this paper, we propose Approximation Vector Machine (AVM), a model that can simultaneously encourage the sparsity and safeguard its risk in compromising the performance. When an incoming instance arrives, we approximate this instance by one of its neighbors whose distance to it is less than a predefined threshold. Our key intuition is that since the newly seen instance is expressed by its nearby neighbor the optimal performance can be analytically formulated and maintained. We develop theoretical foundations to support this intuition and further establish an analysis to characterize the gap between the approximation and optimal solutions. This gap crucially depends on the frequency of approximation and the predefined threshold. We perform the convergence analysis for a wide spectrum of loss functions including Hinge, smooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and $ε$-insensitive for regression task. We conducted extensive experiments for classification task in batch and online modes, and regression task in online mode over several benchmark datasets. The results show that our proposed AVM achieved a comparable predictive performance with current state-of-the-art methods while simultaneously achieving significant computational speed-up due to the ability of the proposed AVM in maintaining the model size.},
 author = {Trung Le and Tu Dinh Nguyen and Vu Nguyen and Dinh Phung},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W2963785424},
 pages = {1--55},
 title = {Approximation Vector Machines for Large-scale Online Learning},
 url = {http://jmlr.org/papers/v18/16-191.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-198,
 abstract = {In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. Our method is also applied to improve the performance of discriminant analysis in sound recording classification.},
 author = {Guo Yu and Jacob Bien},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W2342854590},
 pages = {1--60},
 title = {Learning Local Dependence In Ordered Data},
 url = {http://jmlr.org/papers/v18/16-198.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-206,
 abstract = {We consider data in the form of pairwise comparisons of n items, with the goal of precisely identifying the top k items for some value of k &lt; n, or alternatively, recovering a ranking of all the items. We analyze the Copeland counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) its computational efficiency leads to speed-ups of several orders of magnitude in computation time as compared to prior work; (b) it is robust in that theoretical guarantees impose no conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) it is an optimal method up to constant factors, meaning that it achieves the information-theoretic limits for recovering the top k-subset. We extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisfies a simple and natural monotonicity condition.},
 author = {Nihar B. Shah and Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {199},
 openalex = {W2221311665},
 pages = {1--38},
 title = {Simple, Robust and Optimal Ranking from Pairwise Comparisons},
 url = {http://jmlr.org/papers/v18/16-206.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-212,
 abstract = {Many applications, such as human action recognition and object detection, can be formulated as a multiclass classification problem. One-vs-rest (OVR) is one of the most widely used approaches for multiclass classification due to its simplicity and excellent performance. However, many confusing classes in such applications will degrade its results. For example, hand clap and boxing are two confusing actions. Hand clap is easily misclassified as boxing, and vice versa. Therefore, precisely classifying confusing classes remains a challenging task. To obtain better performance for multiclass classifications that have confusing classes, we first develop a classifier chain model for multiclass classification (CCMC) to transfer class information between classifiers. Then, based on an analysis of our proposed model, we propose an easy-to-hard learning paradigm for multiclass classification to automatically identify easy and hard classes and then use the predictions from simpler classes to help solve harder classes. Similar to CCMC, the classifier chain (CC) model is also proposed by Read et al. (2009) to capture the label dependency for multi-label classification. However, CC does not consider the order of di_culty of the labels and achieves degenerated performance when there are many confusing labels. Therefore, it is non-trivial to learn the appropriate label order for CC. Motivated by our analysis for CCMC, we also propose the easy-to-hard learning paradigm for multi-label classification to automatically identify easy and hard labels, and then use the predictions from simpler labels to help solve harder labels. We also demonstrate that our proposed strategy can be successfully applied to a wide range of applications, such as ordinal classification and relationship prediction. Extensive empirical studies validate our analysis and the efiectiveness of our proposed easy-to-hard learning strategies.},
 author = {Weiwei Liu and Ivor W. Tsang and Klaus-Robert M\"{u}ller},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W2761674588},
 pages = {1--38},
 title = {An easy-to-hard learning paradigm for multiple classes and multiple labels},
 url = {http://jmlr.org/papers/v18/16-212.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-214,
 abstract = {We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers for people connected by a social network; by predicting these user profile fields, the network can provide a better experience to its users. Existing approaches such as Label Propagation (Zhu et al., 2003) fail to consider interactions between the label types. Our proposed method, called Edge-Explain, explicitly models these interactions, while still allowing scalable inference under a distributed message-passing architecture. On a large subset of the Facebook social network, collected in a previous study (Chakrabarti et al., 2014), EdgeExplain outperforms label propagation for several label types, with lifts of up to 120% for [email protected] and 60% for [email protected]},
 author = {Deepayan Chakrabarti and Stanislav Funiak and Jonathan Chang and Sofus A. Macskassy},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W2736324466},
 pages = {1--39},
 title = {Joint label inference in networks},
 url = {http://jmlr.org/papers/v18/16-214.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-217,
 abstract = {We initiate the rigorous study of classification in semimetric spaces, which are point sets with a distance function that is non-negative and symmetric, but need not satisfy the triangle inequality. We define the density dimension dens and discover that it plays a central role in the statistical and algorithmic feasibility of learning in semimetric spaces. We compute this quantity for several widely used semimetrics and present nearly optimal sample compression algorithms, which are then used to obtain generalization guarantees, including fast rates.

Our claim of near-optimality holds in both computational and statistical senses. When the sample has radius R and margin γ, we show that it can be compressed down to roughly d = (R/γ )dens points, and further that finding a significantly better compression is algorithmically intractable unless P=NP. This compression implies generalization via standard Occam-type arguments, to which we provide a nearly matching lower bound.},
 author = {Lee-Ad Gottlieb and Aryeh Kontorovich and Pinhas Nisnevitch},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2963565028},
 pages = {1--22},
 title = {Nearly optimal classification for semimetrics},
 url = {http://jmlr.org/papers/v18/16-217.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-223,
 abstract = {This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of cooptimization. It explains...},
 author = {Elena Popovici},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W2999633534},
 pages = {1--39},
 title = {Bridging supervised learning and test-based co-optimization},
 url = {http://jmlr.org/papers/v18/16-223.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-232,
 abstract = {Information about intrinsic dimension is crucial to perform dimensionality reduction, compress information, design efficient algorithms, and do statistical adaptation. In this paper we propose an estimator for the intrinsic dimension of a data set. The estimator is based on binary neighbourhood information about the observations in the form of two adjacency matrices, and does not require any explicit distance information. The underlying graph is modelled according to a subset of a specific random connection model, sometimes referred to as the Poisson blob model. Computationally the estimator scales like n log n, and we specify its asymptotic distribution and rate of convergence. A simulation study on both real and simulated data shows that our approach compares favourably with some competing methods from the literature, including approaches that rely on distance information.},
 author = {Paulo Serra and Michel M and jes},
 journal = {Journal of Machine Learning Research},
 number = {138},
 openalex = {W2963660864},
 pages = {1--35},
 title = {Dimension Estimation Using Random Connection Models},
 url = {http://jmlr.org/papers/v18/16-232.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-245,
 abstract = {Community detection is a fundamental statistical problem in network data analysis. In this paper, we present a polynomial time two-stage method that provably achieves optimal statistical performanc...},
 author = {Chao Gao and Zongming Ma and Anderson Y. Zhang and Harrison H. Zhou},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2963974511},
 pages = {1--45},
 title = {Achieving Optimal Misclassification Proportion in Stochastic Block Models},
 url = {http://jmlr.org/papers/v18/16-245.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-256,
 abstract = {We study learning of description logic TBoxes in Angluin et al.'s framework of exact learning via queries. We admit entailment queries (is a given subsumption entailed by the target TBox?) and equivalence queries (is a given TBox equivalent to the target TBox?), assuming that the signature and logic of the target TBox are known. We present three main results: (1) TBoxes formulated in DL-Lite with role inclusions and composite concepts on the right-hand side of concept inclusions can be learned in polynomial time; (2) EL TBoxes with only concept names on the right-hand side of concept inclusions can be learned in polynomial time; and (3) EL TBoxes cannot be learned in polynomial time. It follows that non-polynomial time learnability of EL TBoxes is caused by the interaction between existential restrictions on the right- and left-hand sides of concept inclusions. We also show that neither entailment nor equivalence queries alone are sufficient in cases (1) and (2) above.},
 author = {Boris Konev and Carsten Lutz and Ana Ozaki and Frank Wolter},
 journal = {Journal of Machine Learning Research},
 number = {201},
 openalex = {W2808285130},
 pages = {1--63},
 title = {Exact Learning of Lightweight Description Logic Ontologies},
 url = {http://jmlr.org/papers/v18/16-256.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-258,
 abstract = {This paper addresses how well we can recover a data matrix when only given a few of its elements. We present a randomized algorithm that element-wise sparsifies the data, retaining only a few of it...},
 author = {Abhisek Kundu and Petros Drineas and Malik Magdon-Ismail},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W3172851589},
 pages = {1--34},
 title = {Recovering PCA and sparse PCA via hybrid- (l1, l2)sparse sampling of data elements},
 url = {http://jmlr.org/papers/v18/16-258.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-261,
 abstract = {Many different machine learning algorithms exist; taking into account each algorithm’s hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider feature selection techniques and all machine learning approaches implemented in WEKA’s standard distribution, spanning 2 ensemble methods, 10 meta-methods, 28 base learners, and hyperparameter settings for each learner. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
 author = {Lars Kotthoff and Chris Thornton and Holger H. Hoos and Frank Hutter and Kevin Leyton-Brown},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W2608595939},
 pages = {1--5},
 title = {Auto-WEKA: Automatic Model Selection and Hyperparameter Optimization in WEKA},
 url = {http://jmlr.org/papers/v18/16-261.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-266,
 abstract = {We consider the problem of estimating a low-rank signal matrix from noisy measurements under the assumption that the distribution of the data matrix belongs to an exponential family. In this setting, we derive generalized Stein's unbiased risk estimation (SURE) formulas that hold for any spectral estimators which shrink or threshold the singular values of the data matrix. This leads to new data-driven spectral estimators, whose optimality is discussed using tools from random matrix theory and through numerical experiments. Under the spiked population model and in the asymptotic setting where the dimensions of the data matrix are let going to infinity, some theoretical properties of our approach are compared to recent results on asymptotically optimal shrinking rules for Gaussian noise. It also leads to new procedures for singular values shrinkage in finite-dimensional matrix denoising for Gamma-distributed and Poisson-distributed measurements.},
 author = {J{{\'e}}r{{\'e}}mie Bigot and Charles Deledalle and Delphine F{{\'e}}ral},
 journal = {Journal of Machine Learning Research},
 number = {137},
 openalex = {W2404992943},
 pages = {1--50},
 title = {Generalized SURE for optimal shrinkage of singular values in low-rank matrix denoising},
 url = {http://jmlr.org/papers/v18/16-266.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-270,
 abstract = {Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low-dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs.},
 author = {Adam S. Charles and Dong Yin and Christopher J. Rozell},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W2964202535},
 pages = {1--37},
 title = {Distributed sequence memory of multidimensional inputs in recurrent networks},
 url = {http://jmlr.org/papers/v18/16-270.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-274,
 abstract = {Binary classification is the problem of predicting the class a given sample belongs to. To achieve a good prediction performance, it is important to find a suitable model for a given dataset. However, it is often time consuming and impractical for practitioners to try various classification models because each model employs a different formulation and algorithm. The difficulty can be mitigated if we have a unified formulation and an efficient universal algorithmic framework for various classification models to expedite the comparison of performance of different models for a given dataset. In this paper, we present a unified formulation of various classification models (including C-SVM, l2-SVM, ν-SVM, MM-FDA, MM-MPM, logistic regression, distance weighted discrimination) and develop a general optimization algorithm based on an accelerated proximal gradient (APG) method for the formulation. We design various techniques such as backtracking line search and adaptive restarting strategy in order to speed up the practical convergence of our method. We also give a theoretical convergence guarantee for the proposed fast APG algorithm. Numerical experiments show that our algorithm is stable and highly competitive to specialized algorithms designed for specific models (e.g., sequential minimal optimization (SMO) for SVM).},
 author = {Naoki Ito and Akiko Takeda and Kim-Chuan Toh},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2607522257},
 pages = {1--49},
 title = {A unified formulation and fast accelerated proximal gradient method for classification},
 url = {http://jmlr.org/papers/v18/16-274.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-285,
 abstract = {We consider two settings of online learning to rank where feedback is restricted to top ranked items. The problem is cast as an online game between a learner and sequence of users, over T rounds. In both settings, the learners objective is to present ranked list of items to the users. The learner's performance is judged on the entire ranked list and true relevances of the items. However, the learner receives highly restricted feedback at end of each round, in form of relevances of only the top k ranked items, where k ≪ m. The first setting is non-contextual, where the list of items to be ranked is fixed. The second setting is contextual, where lists of items vary, in form of traditional query-document lists. No stochastic assumption is made on the generation process of relevances of items and contexts. We provide efficient ranking strategies for both the settings. The strategies achieve O(T2/3) regret, where regret is based on popular ranking measures in first setting and ranking surrogates in second setting. We also provide impossibility results for certain ranking measures and a certain class of surrogates, when feedback is restricted to the top ranked item, i.e. k = 1. We empirically demonstrate the performance of our algorithms on simulated and real world data sets.},
 author = {Sougata Chaudhuri and Ambuj Tewari},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W2962908014},
 pages = {1--50},
 title = {Online learning to rank with top-k feedback},
 url = {http://jmlr.org/papers/v18/16-285.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-289,
 abstract = {In this paper, we explore statistical versus computational trade-off to address a basic question in the application of a distributed algorithm: what is the minimal computational cost in obtaining statistical optimality? In smoothing spline setup, we observe a phase transition phenomenon for the number of deployed machines that ends up being a simple proxy for computing cost. Specifically, a sharp upper bound for the number of machines is established: when the number is below this bound, statistical optimality (in terms of nonparametric estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. These sharp bounds partly capture intrinsic computational limits of the distributed algorithm considered in this paper, and turn out to be fully determined by the smoothness of the regression function. As a side remark, we argue that sample splitting may be viewed as an alternative form of regularization, playing a similar role as smoothing parameter.},
 author = {Zuofeng Shang and Guang Cheng},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W2963675684},
 pages = {1--37},
 title = {Computational Limits of A Distributed Algorithm For Smoothing Spline},
 url = {http://jmlr.org/papers/v18/16-289.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-296,
 abstract = {In this paper, we describe an unsupervised measure for quantifying the 'informativeness' of correlation matrices formed from the pairwise similarities or relationships among data instances. The measure quantifies the heterogeneity of the correlations and is defined as the distance between a correlation matrix and the nearest correlation matrix with constant off-diagonal entries. This non-parametric notion generalizes existing test statistics for equality of correlation coefficients by allowing for alternative distance metrics, such as the Bures and other distances from quantum information theory. For several distance and dissimilarity metrics, we derive closed-form expressions of informativeness, which can be applied as objective functions for machine learning applications. Empirically, we demonstrate that informativeness is a useful criterion for selecting kernel parameters, choosing the dimension for kernel-based nonlinear dimensionality reduction, and identifying structured graphs. We also consider the problem of finding a maximally informative correlation matrix around a target matrix, and explore parameterizing the optimization in terms of the coordinates of the sample or through a lower-dimensional embedding. In the latter case, we find that maximizing the Bures-based informativeness measure, which is maximal for centered rank-1 correlation matrices, is equivalent to minimizing a specific matrix norm, and present an algorithm to solve the minimization problem using the norm's proximal operator. The proposed correlation denoising algorithm consistently improves spectral clustering. Overall, we find informativeness to be a novel and useful criterion for identifying non-trivial correlation structure..},
 author = {Austin J. Brockmeier and Tingting Mu and Sophia Ananiadou and John Y. Goulermas},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W2741401375},
 pages = {1--61},
 title = {Quantifying the Informativeness of Similarity Measurements},
 url = {http://jmlr.org/papers/v18/16-296.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-299,
 abstract = {This paper is concerned with the hard thresholding operator which sets all but the $k$ largest absolute elements of a vector to zero. We establish a {\em tight} bound to quantitatively characterize the deviation of the thresholded solution from a given signal. Our theoretical result is universal in the sense that it holds for all choices of parameters, and the underlying analysis depends only on fundamental arguments in mathematical optimization. We discuss the implications for two domains: 
Compressed Sensing. On account of the crucial estimate, we bridge the connection between the restricted isometry property (RIP) and the sparsity parameter for a vast volume of hard thresholding based algorithms, which renders an improvement on the RIP condition especially when the true sparsity is unknown. This suggests that in essence, many more kinds of sensing matrices or fewer measurements are admissible for the data acquisition procedure. 
Machine Learning. In terms of large-scale machine learning, a significant yet challenging problem is learning accurate sparse models in an efficient manner. In stark contrast to prior work that attempted the $\ell_1$-relaxation for promoting sparsity, we present a novel stochastic algorithm which performs hard thresholding in each iteration, hence ensuring such parsimonious solutions. Equipped with the developed bound, we prove the {\em global linear convergence} for a number of prevalent statistical models under mild assumptions, even though the problem turns out to be non-convex.},
 author = {Jie Shen and Ping Li},
 journal = {Journal of Machine Learning Research},
 number = {208},
 openalex = {W2962975464},
 pages = {1--42},
 title = {A Tight Bound of Hard Thresholding},
 url = {http://jmlr.org/papers/v18/16-299.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-300,
 abstract = {POMDPs.jl is an open-source framework for solving Markov decision processes (MDPs) and partially observable MDPs (POMDPs). POMDPs.jl allows users to specify sequential decision making problems with minimal effort without sacrificing the expressive nature of POMDPs, making this framework viable for both educational and research purposes. It is written in the Julia language to allow flexible prototyping and large-scale computation that leverages the high-performance nature of the language. The associated JuliaPOMDP community also provides a number of state-of-the-art MDP and POMDP solvers and a rich library of support tools to help with implementing new solvers and evaluating the solution results. The most recent version of POMDPs.jl, the related packages, and documentation can be found at https://github.com/JuliaPOMDP/POMDPs.jl.},
 author = {Maxim Egorov and Zachary N. Sunberg and Edward Balaban and Tim A. Wheeler and Jayesh K. Gupta and Mykel J. Kochenderfer},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2607868186},
 pages = {1--5},
 title = {POMDPs.jl: a framework for sequential decision making under uncertainty},
 url = {http://jmlr.org/papers/v18/16-300.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-305,
 abstract = {The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it.},
 author = {Alessio Benavoli and Giorgio Corani and Janez Dem{\v{s}}ar and Marco Zaffalon},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W4301186107},
 pages = {1--36},
 title = {Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis},
 url = {http://jmlr.org/papers/v18/16-305.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-315,
 abstract = {It is usual in machine learning theory to assume that the training and testing sets comprise of draws from the same distribution. This is rarely, if ever, true and one must admit the presence of co...},
 author = {Brendan van Rooyen and Robert C. Williamson},
 journal = {Journal of Machine Learning Research},
 number = {228},
 openalex = {W2886811283},
 pages = {1--50},
 title = {A Theory of Learning with Corrupted Labels},
 url = {http://jmlr.org/papers/v18/16-315.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-319,
 abstract = {We present a graphical criterion for covariate adjustment that is sound and complete for four different classes of causal graphical models: directed acyclic graphs (DAGs), maximum ancestral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs), and partial ancestral graphs (PAGs). Our criterion unifies covariate adjustment for a large set of graph classes. Moreover, we define an explicit set that satisfies our criterion, if there is any set that satisfies our criterion. We also give efficient algorithms for constructing all sets that fulfill our criterion, implemented in the R package dagitty. Finally, we discuss the relationship between our criterion and other criteria for adjustment, and we provide new soundness and completeness proofs for the adjustment criterion for DAGs.},
 author = {Emilija Perkovi\'c and Johannes Textor and Markus Kalisch and Marloes H. Maathuis},
 journal = {Journal of Machine Learning Research},
 number = {220},
 openalex = {W2963304627},
 pages = {1--62},
 title = {Complete Graphical Characterization and Construction of Adjustment Sets in Markov Equivalence Classes of Ancestral Graphs},
 url = {http://jmlr.org/papers/v18/16-319.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-326,
 abstract = {Probabilistic planners have improved recently to the point that they can solve difficult tasks with complex and expressive models. In contrast, learners cannot tackle yet the expressive models that planners do, which forces complex models to be mostly handcrafted. We propose a new learning approach that can learn relational probabilistic models with both action effects and exogenous effects. The proposed learning approach combines a multi-valued variant of inductive logic programming for the generation of candidate models, with an optimization method to select the best set of planning operators to model a problem. We also show how to combine this learner with reinforcement learning algorithms to solve complete problems. Finally, experimental validation is provided that shows improvements over previous work in both simulation and a robotic task. The robotic task involves a dynamic scenario with several agents where a manipulator robot has to clear the tableware on a table. We show that the exogenous effects learned by our approach allowed the robot to clear the table in a more efficient way.},
 author = {David Mart\'{i}nez and Guillem Aleny\`{a} and Tony Ribeiro and Katsumi Inoue and Carme Torras},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W2760799532},
 pages = {1--44},
 title = {Relational Reinforcement Learning for Planning with Exogenous Effects},
 url = {http://jmlr.org/papers/v18/16-326.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-335,
 abstract = {We consider the optimization of a quadratic objective function whose gradients are only accessible through a stochastic oracle that returns the gradient at any given point plus a zero-mean finite variance random error. We present the first algorithm that achieves jointly the optimal prediction error rates for least-squares regression, both in terms of forgetting of initial conditions in O(1/n 2), and in terms of dependence on the noise and dimension d of the problem, as O(d/n). Our new algorithm is based on averaged accelerated regularized gradient descent, and may also be analyzed through finer assumptions on initial conditions and the Hessian matrix, leading to dimension-free quantities that may still be small while the " optimal " terms above are large. In order to characterize the tightness of these new bounds, we consider an application to non-parametric regression and use the known lower bounds on the statistical performance (without computational limits), which happen to match our bounds obtained from a single pass on the data and thus show optimality of our algorithm in a wide variety of particular trade-offs between bias and variance.},
 author = {Aymeric Dieuleveut and Nicolas Flammarion and Francis Bach},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W2963189955},
 pages = {1--51},
 title = {Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression},
 url = {http://jmlr.org/papers/v18/16-335.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-337,
 abstract = {Many datasets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a dataset. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite-dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs.},
 author = {Henry Adams and Tegan Emerson and Michael Kirby and Rachel Neville and Chris Peterson and Patrick Shipman and Sofya Chepushtanova and Eric Hanson and Francis Motta and Lori Ziegelmeier},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W2964237352},
 pages = {1--35},
 title = {Persistence Images: A Stable Vector Representation of Persistent Homology},
 url = {http://jmlr.org/papers/v18/16-337.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-340,
 abstract = {We study the stochastic multi-armed bandit (MAB) problem in the presence of side-observations across actions that occur as a result of an underlying network structure. In our model, a bipartite graph captures the relationship between actions and a common set of unknowns such that choosing an action reveals observations for the unknowns that it is connected to. This models a common scenario in online social networks where users respond to their friends' activity, thus providing side information about each other's preferences. Our contributions are as follows: 1) We derive an asymptotic lower bound (with respect to time) as a function of the bi-partite network structure on the regret of any uniformly good policy that achieves the maximum long-term average reward. 2) We propose two policies - a randomized policy; and a policy based on the well-known upper confidence bound (UCB) policies - both of which explore each action at a rate that is a function of its network position. We show, under mild assumptions, that these policies achieve the asymptotic lower bound on the regret up to a multiplicative factor, independent of the network structure. Finally, we use numerical examples on a real-world social network and a routing example network to demonstrate the benefits obtained by our policies over other existing policies.},
 author = {Swapna Buccapatnam and Fang Liu and Atilla Eryilmaz and Ness B. Shroff},
 journal = {Journal of Machine Learning Research},
 number = {216},
 openalex = {W2609921936},
 pages = {1--34},
 title = {Reward Maximization Under Uncertainty: Leveraging Side-Observations on Networks},
 url = {http://jmlr.org/papers/v18/16-340.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-342,
 author = {Lin Lin and Jia Li},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W2769342206},
 pages = {1--49},
 title = {Clustering with Hidden Markov Model on Variable Blocks},
 url = {http://jmlr.org/papers/v18/16-342.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-362,
 abstract = {We propose a Bayesian approach to regression with a scalar response on vector and tensor covariates. Vectorization of the tensor prior to analysis fails to exploit the structure, often leading to p...},
 author = {Rajarshi Guhaniyogi and Shaan Qamar and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W2963404198},
 pages = {1--31},
 title = {Bayesian Tensor Regression},
 url = {http://jmlr.org/papers/v18/16-362.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-365,
 abstract = {Imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub: https://github.com/scikit-learn-contrib/imbalanced-learn.},
 author = {Guillaume  Lema{{\^i}}tre and Fernando Nogueira and Christos K. Aridas},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W4293713156},
 pages = {1--5},
 title = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},
 url = {http://jmlr.org/papers/v18/16-365.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-374,
 abstract = {This paper addresses the task of zero-shot image classification. The key contribution of the proposed approach is to control the semantic embedding of images -- one of the main ingredients of zero-shot learning -- by formulating it as a metric learning problem. The optimized empirical criterion associates two types of sub-task constraints: metric discriminating capacity and accurate attribute prediction. This results in a novel expression of zero-shot learning not requiring the notion of class in the training phase: only pairs of image/attributes, augmented with a consistency indicator, are given as ground truth. At test time, the learned model can predict the consistency of a test image with a given set of attributes , allowing flexible ways to produce recognition inferences. Despite its simplicity, the proposed approach gives state-of-the-art results on four challenging datasets used for zero-shot recognition evaluation.},
 author = {Christophe Dupuy and Francis Bach},
 journal = {Journal of Machine Learning Research},
 number = {126},
 openalex = {W2488270314},
 pages = {1--45},
 title = {Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling},
 url = {http://jmlr.org/papers/v18/16-374.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-382,
 abstract = {The robust improper maximum likelihood estimator (RIMLE) is a new method for robust multivariate clustering finding approximately Gaussian clusters. It maximizes a pseudo- likelihood defined by adding a component with improper constant density for accommodating outliers to a Gaussian mixture. A special case of the RIMLE is MLE for multivariate finite Gaussian mixture models. In this paper we treat existence, consistency, and breakdown theory for the RIMLE comprehensively. RIMLE's existence is proved under non-smooth covariance matrix constraints. It is shown that these can be implemented via a computationally feasible Expectation-Conditional Maximization algorithm.},
 author = {Pietro Coretto and Christian Hennig},
 journal = {Journal of Machine Learning Research},
 number = {142},
 openalex = {W2963962140},
 pages = {1--39},
 title = {Consistency, Breakdown Robustness, and Algorithms for Robust Improper Maximum Likelihood Clustering},
 url = {http://jmlr.org/papers/v18/16-382.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-391,
 abstract = {A plethora of networks is being collected in a growing number of fields, including disease transmission, international relations, social interactions, and others. As data streams continue to grow, the complexity associated with these highly multidimensional connectivity data presents novel challenges. In this paper, we focus on the time-varying interconnections among a set of actors in multiple contexts, called layers. Current literature lacks flexible statistical models for dynamic multilayer networks, which can enhance quality in inference and prediction by efficiently borrowing information within each network, across time, and between layers. Motivated by this gap, we develop a Bayesian nonparametric model leveraging latent space representations. Our formulation characterizes the edge probabilities as a function of shared and layer-specific actors positions in a latent space, with these positions changing in time via Gaussian processes. This representation facilitates dimensionality reduction and incorporates different sources of information in the observed data. In addition, we obtain tractable procedures for posterior computation, inference, and prediction. We provide theoretical results on the flexibility of our model. Our methods are tested on simulations and infection studies monitoring dynamic face-to-face contacts among individuals in multiple days, where we perform better than current methods in inference and prediction.},
 author = {Daniele Durante and Nabanita Mukherjee and Rebecca C. Steorts},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W2963104654},
 pages = {1--29},
 title = {Bayesian learning of dynamic multilayer networks},
 url = {http://jmlr.org/papers/v18/16-391.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-410,
 abstract = {Nesterov's momentum trick is famously known for accelerating gradient descent, and has been proven useful in building fast iterative algorithms. However, in the stochastic setting, counterexamples exist and prevent Nesterov's momentum from providing similar acceleration, even if the underlying problem is convex.},
 author = {Zeyuan Allen-Zhu},
 journal = {Journal of Machine Learning Research},
 number = {221},
 openalex = {W2963607709},
 pages = {1--51},
 title = {Katyusha: the first direct acceleration of stochastic gradient methods},
 url = {http://jmlr.org/papers/v18/16-410.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-421,
 abstract = {In a recent paper, Caron and Fox suggest a probabilistic model for sparse graphs which are exchangeable when associating each vertex with a time parameter in $\mathbb{R}_+$. Here we show that by generalizing the classical definition of graphons as functions over probability spaces to functions over $σ$-finite measure spaces, we can model a large family of exchangeable graphs, including the Caron-Fox graphs and the traditional exchangeable dense graphs as special cases. Explicitly, modelling the underlying space of features by a $σ$-finite measure space $(S,\mathcal{S},μ)$ and the connection probabilities by an integrable function $W\colon S\times S\to [0,1]$, we construct a random family $(G_t)_{t\geq 0}$ of growing graphs such that the vertices of $G_t$ are given by a Poisson point process on $S$ with intensity $tμ$, with two points $x,y$ of the point process connected with probability $W(x,y)$. We call such a random family a graphon process. We prove that a graphon process has convergent subgraph frequencies (with possibly infinite limits) and that, in the natural extension of the cut metric to our setting, the sequence converges to the generating graphon. We also show that the underlying graphon is identifiable only as an equivalence class over graphons with cut distance zero. More generally, we study metric convergence for arbitrary (not necessarily random) sequences of graphs, and show that a sequence of graphs has a convergent subsequence if and only if it has a subsequence satisfying a property we call uniform regularity of tails. Finally, we prove that every graphon is equivalent to a graphon on $\mathbb{R}_+$ equipped with Lebesgue measure.},
 author = {Christian Borgs and Jennifer T. Chayes and Henry Cohn and Nina Holden},
 journal = {Journal of Machine Learning Research},
 number = {210},
 openalex = {W2256423373},
 pages = {1--71},
 title = {Sparse exchangeable graphs and their limits via graphon processes},
 url = {http://jmlr.org/papers/v18/16-421.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-422,
 abstract = {For a convex class of functions $F$, a regularization functions $\Psi(\cdot)$ and given the random data $(X_i, Y_i)_{i=1}^N$, we study estimation properties of regularization procedures of the form \begin{equation*} 
\hat f \in {\rm argmin}_{f\in 
F}\Big(\frac{1}{N}\sum_{i=1}^N\big(Y_i-f(X_i)\big)^2+\lambda \Psi(f)\Big) \end{equation*} for some well chosen regularization parameter $\lambda$. 
We obtain bounds on the $L_2$ estimation error rate that depend on the complexity of the true model $F^*:=\{f\in F: \Psi(f)\leq\Psi(f^*)\}$, where $f^*\in {\rm argmin}_{f\in F}\mathbb{E}(Y-f(X))^2$ and the $(X_i,Y_i)$'s are independent and distributed as $(X,Y)$. Our estimate holds under weak stochastic assumptions -- one of which being a small-ball condition satisfied by $F$ -- and for rather flexible choices of regularization functions $\Psi(\cdot)$. Moreover, the result holds in the learning theory framework: we do not assume any a-priori connection between the output $Y$ and the input $X$. 
As a proof of concept, we apply our general estimation bound to various choices of $\Psi$, for example, the $\ell_p$ and $S_p$-norms (for $p\geq1$), weak-$\ell_p$, atomic norms, max-norm and SLOPE. In many cases, the estimation rate almost coincides with the minimax rate in the class $F^*$.},
 author = {Guillaume Lecu{{\'e}} and Shahar Mendelson},
 journal = {Journal of Machine Learning Research},
 number = {146},
 openalex = {W2963517436},
 pages = {1--48},
 title = {Regularization and the small-ball method II: complexity dependent error rates},
 url = {http://jmlr.org/papers/v18/16-422.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-424,
 author = {Sivan Sabato and Tom Hess},
 journal = {Journal of Machine Learning Research},
 number = {229},
 openalex = {W2885965167},
 pages = {1--39},
 title = {Interactive Algorithms: Pool, Stream and Precognitive Stream},
 url = {http://jmlr.org/papers/v18/16-424.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-429,
 abstract = {Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from "noise-looking" variables. We cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem, (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions, (c) we propose a natural extension for the multi-label scenario, (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form $d=O(\sqrt{n})$ for the affine invariant case and $d=O(n)$ for the sparse case, where $n$ is the number of examples and $d$ the ambient dimension, and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to $O(nd^2)$, improving on earlier algorithms which had quadratic complexity in the number of examples.},
 author = {Nicolas Flammarion and Balamurugan Palaniappan and Francis Bach},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W2513765142},
 pages = {1--50},
 title = {Robust Discriminative Clustering with Sparse Regularizers},
 url = {http://jmlr.org/papers/v18/16-429.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-456,
 abstract = {We introduce a method to train Quantized Neural Networks (QNNs) --- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At train-time the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves $51\%$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online.},
 author = {Itay Hubara and Matthieu Courbariaux and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
 journal = {Journal of Machine Learning Research},
 number = {187},
 openalex = {W2524428287},
 pages = {1--30},
 title = {Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations},
 url = {http://jmlr.org/papers/v18/16-456.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-460,
 abstract = {We introduce a new model of named teaching and a corresponding complexity parameter--the preference-based dimension (PBTD)--representing the worstcase number of examples needed to teach any concept in a given concept class. Although the PBTD coincides with the well-known recursive dimension (RTD) on finite classes, it is radically different on infinite ones: the RTD becomes infinite already for trivial infinite classes (such as half-intervals) whereas the PBTD evaluates to reasonably small values for a wide collection of infinite classes including classes consisting of so-called closed sets w.r.t. a given closure operator, including various classes related to linear sets over N0 (whose RTD had been studied quite recently) and including the class of Euclidean half-spaces. On top of presenting these concrete results, we provide the reader with a theoretical framework (of a combinatorial flavor) which helps to derive bounds on the PBTD.},
 author = {Ziyuan Gao and Christoph Ries and Hans U. Simon and S and ra Zilles},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W2465040775},
 pages = {1--32},
 title = {Preference-based teaching},
 url = {http://jmlr.org/papers/v18/16-460.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-463,
 abstract = {In modern large-scale machine learning applications, the training are often partitioned and stored on multiple machines. It is customary to employ the data approach, where the aggregated training loss is minimized without moving across machines. In this paper, we introduce a novel distributed dual formulation for regularized loss minimization problems that can directly handle parallelism in the distributed setting. This formulation allows us to systematically derive dual coordinate optimization procedures, which we refer to as Distributed Alternating Dual Maximization (DADM). The framework extends earlier studies described in (Boyd et al., 2011; Ma et al., 2015a; Jaggi et al., 2014; Yang, 2013) and has rigorous theoretical analyses. Moreover with the help of the new formulation, we develop the accelerated version of DADM (Acc-DADM) by generalizing the acceleration technique from (Shalev-Shwartz and Zhang, 2014) to the distributed setting. We also provide theoretical results for the proposed accelerated version and the new result improves previous ones (Yang, 2013; Ma et al., 2015a) whose runtimes grow linearly on the condition number. Our empirical studies validate our theory and show that our accelerated approach significantly improves the previous state-of-the-art distributed dual coordinate optimization algorithms.},
 author = {Shun Zheng and Jialei Wang and Fen Xia and Wei Xu and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {115},
 openalex = {W2963839817},
 pages = {1--52},
 title = {A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization},
 url = {http://jmlr.org/papers/v18/16-463.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-466,
 abstract = {Due to the non-linear but highly interpretable representations, decision tree (DT) models have significantly attracted a lot of attention of researchers. However, it is difficult to understand and interpret DT models in ultrahigh dimensions and DT models usually suffer from the curse of dimensionality and achieve degenerated performance when there are many noisy features. To address these issues, this paper first presents a novel data-dependent generalization error bound for the perceptron decision tree (PDT), which provides the theoretical justification to learn a sparse linear hyperplane in each decision node and to prune the tree. Following our analysis, we introduce the notion of budget-aware classifier (BAC) with a budget constraint on the weight coefficients, and propose a supervised budgeted tree (SBT) algorithm to achieve non-linear prediction performance. To avoid generating an unstable and complicated decision tree and improve the generalization of the SBT, we present a pruning strategy by learning classifiers to minimize cross-validation errors on each BAC. To deal with ultrahigh label dimensions, based on three important phenomena of real-world data sets from a variety of application domains, we develop a sparse coding tree framework for multi-label annotation problems and provide the theoretical analysis. Extensive empirical studies verify that 1) SBT is easy to understand and interpret in ultrahigh dimensions and is more resilient to noisy features. 2) Compared with state-of-the-art algorithms, our proposed sparse coding tree framework is more efficient, yet accurate in ultrahigh label and feature dimensions.},
 author = {Weiwei Liu and Ivor W. Tsang},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W2757686304},
 pages = {1--36},
 title = {Making decision trees feasible in ultrahigh feature and label dimensions},
 url = {http://jmlr.org/papers/v18/16-466.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-478,
 abstract = {This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a data set is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks. 
Keywords: Distributed Learning, Large Scale Learning, Deep Learning, Bayesian Learn- ing, Variational Inference, Expectation Propagation, Stochastic Approximation, Natural Gradient, Markov chain Monte Carlo, Parameter Server, Posterior Server.},
 author = {Leonard Hasenclever and Stefan Webb and Thibaut Lienart and Sebastian Vollmer and Balaji Lakshminarayanan and Charles Blundell and Yee Whye Teh},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W2963275203},
 pages = {1--37},
 title = {Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server},
 url = {http://jmlr.org/papers/v18/16-478.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-480,
 abstract = {The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences. 
This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten-Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds. 
The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed.},
 author = {Emmanuel Abbe},
 journal = {Journal of Machine Learning Research},
 number = {177},
 openalex = {W2602671714},
 pages = {1--86},
 title = {Community detection and stochastic block models: recent developments},
 url = {http://jmlr.org/papers/v18/16-480.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-483,
 abstract = {We consider the task of learning the parameters of a {\em single} component of a mixture model, for the case when we are given {\em side information} about that component, we call this the "search problem" in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components. Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain parameter estimates with greater accuracy, and also improved computation complexity than existing moment based mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real data sets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy.},
 author = {Avik Ray and Joe Neeman and Sujay Sanghavi and Sanjay Shakkottai},
 journal = {Journal of Machine Learning Research},
 number = {206},
 openalex = {W2528487152},
 pages = {1--61},
 title = {The Search Problem in Mixture Models},
 url = {http://jmlr.org/papers/v18/16-483.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-486,
 abstract = {Estimation of Markov Random Field and covariance models from high-dimensional data represents a canonical problem that has received a lot of attention in the literature. A key assumption, widely em...},
 author = {Davoud Ataee Tarzanagh and George Michailidis},
 journal = {Journal of Machine Learning Research},
 number = {209},
 openalex = {W2964158835},
 pages = {1--48},
 title = {Estimation of Graphical Models through Structured Norm Minimization},
 url = {http://jmlr.org/papers/v18/16-486.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-491,
 abstract = {First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improve upon the overall running time over popular first-order methods. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data.},
 author = {Naman Agarwal and Brian Bullins and Elad Hazan},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W2962961534},
 pages = {1--40},
 title = {Second-Order Stochastic Optimization for Machine Learning in Linear Time},
 url = {http://jmlr.org/papers/v18/16-491.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-497,
 abstract = {Our work in this paper is motivated by an elementary but also fundamental and highly practical observation - that uncertainty in constructing a network graph ^ G, as an approximation (or estimate) of some true graph G, manifests as errors in the status of (non)edges that must necessarily propagate to any summaries (G) we seek. Mimicking the common practice of using plug-in estimates ( ^ G) as proxies for (G), our goal is to characterize the distribution of the discrepencyD = ( ^ G) (G), in the specific case where ( ) is a subgraph count. In the empirically relevant setting of large, sparse graphs with low-rate measurement errors, we demonstrate under an independent and unbiased error model and for the specific case of counting edges that a Poisson-like regime maintains. Specifically, we show that the appropriate limiting distribution is a Skellam distribution, rather than a normal distribution. Next, because dependent errors typically can be expected when counting subgraphs in practice, either at the level of the edges themselves or due to overlap among subgraphs, we develop a parallel formalism for using the Skellam distribution in such cases. In particular, using Stein's method, we present a series of results leading to the quantification of the accuracy with which the difference of two sums of dependent Bernoulli random variables may be approximated by a Skellam. This formulation is general and likely of some independent interest. We then illustrate the use of these results in our original context of subgraph counts, where we examine (i) the case of counting edges, under a simple dependent error model, and (ii) the case of counting chains of length 2 under an independent error model. We finish with a discussion of various open problems raised by our work.},
 author = {Prakash Balach and ran and Eric D. Kolaczyk and Weston D. Viles},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W2963290493},
 pages = {1--33},
 title = {On the Propagation of Low-Rate Measurement Error to Subgraph Counts in Large Networks},
 url = {http://jmlr.org/papers/v18/16-497.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-498,
 abstract = {Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.},
 author = {Maruan Al-Shedivat and Andrew Gordon Wilson and Yunus Saatchi and Zhiting Hu and Eric P. Xing},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W4301360474},
 pages = {1--37},
 title = {Learning Scalable Deep Kernels with Recurrent Structure},
 url = {http://jmlr.org/papers/v18/16-498.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-499,
 abstract = {We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure.},
 author = {Aryeh Kontorovich and Sivan Sabato and Ruth Urner},
 journal = {Journal of Machine Learning Research},
 number = {195},
 openalex = {W2395688401},
 pages = {1--38},
 title = {Active Nearest-Neighbor Learning in Metric Spaces},
 url = {http://jmlr.org/papers/v18/16-499.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-501,
 abstract = {Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differentially privacy, which uses a more rigorous definition of privacy loss, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform minimax optimization. In addition, the paper proposes a noisy minimax filter which combines minimax filter and differentially-private mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or higher target task accuracy and lower inference accuracy, often significantly lower than previous methods.},
 author = {Jihun Hamm},
 journal = {Journal of Machine Learning Research},
 number = {129},
 openalex = {W2963508973},
 pages = {1--31},
 title = {Minimax filter: learning to preserve privacy from inference attacks},
 url = {http://jmlr.org/papers/v18/16-501.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-503,
 abstract = {We show that the average stability notion introduced by \cite{kearns1999algorithmic, bousquet2002stability} is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We strengthen the recent bounds of \cite{hardt2015train} on the stability rate of the Stochastic Gradient Descent algorithm.},
 author = {Alon Gonen and Shai Shalev-Shwartz},
 journal = {Journal of Machine Learning Research},
 number = {222},
 openalex = {W2963192864},
 pages = {1--13},
 title = {Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization},
 url = {http://jmlr.org/papers/v18/16-503.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-504,
 abstract = {Consider the stochastic composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method, which updates based on queries to the sampling oracle using two different timescales. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We further demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments.},
 author = {Mengdi Wang and Ji Liu and Ethan X. Fang},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W2964158744},
 pages = {1--23},
 title = {Accelerating Stochastic Composition Optimization},
 url = {http://jmlr.org/papers/v18/16-504.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-505,
 abstract = {Convolutional neural networks (CNN) have led to many state-of-the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional, recurrent and residual networks, and has better theoretical guarantees.},
 author = {Vardan Papyan and Yaniv Romano and Michael Elad},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W4322615131},
 pages = {1--52},
 title = {Convolutional Neural Networks Analyzed via Convolutional Sparse Coding},
 url = {http://jmlr.org/papers/v18/16-505.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-509,
 abstract = {The R package GFA provides a full pipeline for factor analysis of multiple data sources that are represented as matrices with co-occurring samples. It allows learning dependencies between subsets of the data sources, decomposed into latent factors. The package also implements sparse priors for the factorization, providing interpretable biclusters of the multi-source data},
 author = {Eemeli Lepp{{\"a}}aho and Muhammad Ammad-ud-din and Samuel Kaski},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W2952874200},
 pages = {1--5},
 title = {GFA: Exploratory Analysis of Multiple Data Sources with Group Factor Analysis},
 url = {http://jmlr.org/papers/v18/16-509.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-512,
 abstract = {Reference EPFL-REPORT-229237 URL: https://arxiv.org/abs/1611.02189 Record created on 2017-06-21, modified on 2017-07-12},
 author = {Virginia Smith and Simone Forte and Chenxin Ma and Martin Tak{{\'a}}{\v{c}} and Michael I. Jordan and Martin Jaggi},
 journal = {Journal of Machine Learning Research},
 number = {230},
 openalex = {W2962741697},
 pages = {1--49},
 title = {CoCoA: A General Framework for Communication-Efficient Distributed Optimization},
 url = {http://jmlr.org/papers/v18/16-512.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-526,
 abstract = {We introduce a novel approach for estimating Latent Dirichlet Allocation (LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single additional collapsed Gibbs sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. Our estimators can straightforwardly be applied to the output of any existing implementation of CGS, including modern accelerated variants. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multi-label classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so.},
 author = {Yannis Papanikolaou and James R. Foulds and Timothy N. Rubin and Grigorios Tsoumakas},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W2963926240},
 pages = {1--58},
 title = {Dense Distributions from Sparse Samples: Improved Gibbs Sampling Parameter Estimators for LDA},
 url = {http://jmlr.org/papers/v18/16-526.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-532,
 abstract = {The fused lasso, also known as (anisotropic) total variation denoising, is widely used for piecewise constant signal estimation with respect to a given undirected graph. The fused lasso estimate is highly nontrivial to compute when the underlying graph is large and has an arbitrary structure. But for a special graph structure, namely, the chain graph, the fused lasso--or simply, 1d fused lasso--can be computed in linear time. In this paper, we revisit a result recently established in the online classification literature (Herbster et al., 2009; Cesa-Bianchi et al., 2013) and show that it has important implications for signal denoising on graphs. The result can be translated to our setting as follows. Given a general graph, if we run the standard depth-first search (DFS) traversal algorithm, then the total variation of any signal over the chain graph induced by DFS is no more than twice its total variation over the original graph.

This result leads to several interesting theoretical and computational conclusions. Letting m and n denote the number of edges and nodes, respectively, of the graph in consideration, it implies that for an underlying signal with total variation t over the graph, the fused lasso (properly tuned) achieves a mean squared error rate of t2/3n-2/3. Moreover, precisely the same mean squared error rate is achieved by running the 1d fused lasso on the DFS-induced chain graph. Importantly, the latter estimator is simple and computationally cheap, requiring O(m) operations to construct the DFS-induced chain and O(n) operations to compute the 1d fused lasso solution over this chain. Further, for trees that have bounded maximum degree, the error rate of t2/3n-2/3 cannot be improved, in the sense that it is the minimax rate for signals that have total variation t over the tree. Finally, several related results also hold--for example, the analogous result holds for a roughness measure defined by the l0 norm of differences across edges in place of the total variation metric.},
 author = {Oscar Hernan Madrid Padilla and James Sharpnack and James G. Scott and Ryan J. Tibshirani},
 journal = {Journal of Machine Learning Research},
 number = {176},
 openalex = {W2963094681},
 pages = {1--36},
 title = {The DFS fused lasso: linear-time denoising over general graphs},
 url = {http://jmlr.org/papers/v18/16-532.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-537,
 abstract = {GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware.},
 author = {Alexander G. de G. Matthews and Mark van der Wilk and Tom Nickson and Keisuke Fujii and Alexis Boukouvalas and Pablo Le{\'o}n-Villagr{\'a} and Zoubin Ghahramani and James Hensman},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W4301091646},
 pages = {1--6},
 title = {GPflow: A Gaussian process library using TensorFlow},
 url = {http://jmlr.org/papers/v18/16-537.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-538,
 abstract = {Kernel regression or classification (also referred to as weighted e-NN methods in Machine Learning) are appealing for their simplicity and therefore ubiquitous in data analysis. However, practical implementations of kernel regression or classification consist of quantizing or sub-sampling data for improving time efficiency, often at the cost of prediction quality. While such tradeoffs are necessary in practice, their statistical implications are generally not well understood, hence practical implementations come with few performance guarantees. In particular, it is unclear whether it is possible to maintain the statistical accuracy of kernel prediction--crucial in some applications--while improving prediction time.

The present work provides guiding principles for combining kernel prediction with data-quantization so as to guarantee good tradeoffs between prediction time and accuracy, and in particular so as to approximately maintain the good accuracy of vanilla kernel prediction. Furthermore, our tradeoff guarantees are worked out explicitly in terms of a tuning parameter which acts as a knob that favors either time or accuracy depending on practical needs. On one end of the knob, prediction time is of the same order as that of single-nearestneighbor prediction (which is statistically inconsistent) while maintaining consistency; on the other end of the knob, the prediction risk is nearly minimax-optimal (in terms of the original data size) while still reducing time complexity. The analysis thus reveals the interaction between the data-quantization approach and the kernel prediction method, and most importantly gives explicit control of the tradeoff to the practitioner rather than fixing the tradeoff in advance or leaving it opaque.

The theoretical results are validated on data from a range of real-world application domains; in particular we demonstrate that the theoretical knob performs as expected.},
 author = {Samory Kpotufe and Nakul Verma},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W2614865032},
 pages = {1--29},
 title = {Time-accuracy tradeoffs in kernel prediction: controlling prediction quality},
 url = {http://jmlr.org/papers/v18/16-538.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-541,
 abstract = {We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015.},
 author = {Valerio Perrone and Paul A. Jenkins and Dario Span{{\`o}} and Yee Whye Teh},
 journal = {Journal of Machine Learning Research},
 number = {127},
 openalex = {W2953074199},
 pages = {1--45},
 title = {Poisson Random Fields for Dynamic Feature Models},
 url = {http://jmlr.org/papers/v18/16-541.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-549,
 abstract = {We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress.},
 author = {David P. Helmbold and Philip M. Long},
 journal = {Journal of Machine Learning Research},
 number = {200},
 openalex = {W2963649970},
 pages = {1--28},
 title = {Surprising properties of dropout in deep networks},
 url = {http://jmlr.org/papers/v18/16-549.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-556,
 abstract = {This paper considers inference over distributed linear Gaussian models using factor graphs and Gaussian belief propagation (BP). The distributed inference algorithm involves only local computation of the information matrix and of the mean vector, and message passing between neighbors. Under broad conditions, it is shown that the message information matrix converges to a unique positive definite limit matrix for arbitrary positive semidefinite initialization, and it approaches an arbitrarily small neighborhood of this limit matrix at a doubly exponential rate. A necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator is provided under the assumption that the message information matrix is initialized as a positive semidefinite matrix. Further, it is shown that Gaussian BP always converges when the underlying factor graph is given by the union of a forest and a single loop. The proposed convergence condition in the setup of distributed linear Gaussian models is shown to be strictly weaker than other existing convergence conditions and requirements, including the Gaussian Markov random field based walk-summability condition, and applicable to a large class of scenarios.},
 author = {Jian Du and Shaodan Ma and Yik-Chung Wu and Soummya Kar and Jos{{\'e}} M. F. Moura},
 journal = {Journal of Machine Learning Research},
 number = {172},
 openalex = {W2557150367},
 pages = {1--38},
 title = {Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation},
 url = {http://jmlr.org/papers/v18/16-556.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-558,
 abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
 author = {Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
 journal = {Journal of Machine Learning Research},
 number = {185},
 openalex = {W2963815651},
 pages = {1--52},
 title = {Hyperband: a novel bandit-based approach to hyperparameter optimization},
 url = {http://jmlr.org/papers/v18/16-558.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-563,
 abstract = {In statistical relational learning, knowledge graph completion deals with automatically understanding the structure of large knowledge graphs--labeled directed graphs-- and predicting missing relationships--labeled edges. State-of-the-art embedding models propose different trade-offs between modeling expressiveness, and time and space complexity. We reconcile both expressiveness and complexity through the use of complex-valued embeddings and explore the link between such complex-valued embeddings and unitary diagonalization. We corroborate our approach theoretically and show that all real square matrices--thus all possible relation/adjacency matrices--are the real part of some unitarily diagonalizable matrix. This results opens the door to a lot of other applications of square matrices factorization. Our approach based on complex embeddings is arguably simple, as it only involves a Hermitian dot product, the complex counterpart of the standard dot product between real vectors, whereas other methods resort to more and more complicated composition functions to increase their expressiveness. The proposed complex embeddings are scalable to large data sets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
 author = {Th{{\'e}}o Trouillon and Christopher R. Dance and {{\'E}}ric Gaussier and Johannes Welbl and Sebastian Riedel and Guillaume Bouchard},
 journal = {Journal of Machine Learning Research},
 number = {130},
 openalex = {W2962936385},
 pages = {1--38},
 title = {Knowledge graph completion via complex tensor factorization},
 url = {http://jmlr.org/papers/v18/16-563.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-564,
 abstract = {For spiked population model, we investigate the large dimension N and large sample size M asymptotic behavior of the Support Vector Machine (SVM) classification method in the limit of N, M → ∞ at fixed α = M/N. We focus on the generalization performance by analytically evaluating the angle between the normal direction vectors of SVM separating hyperplane and corresponding Bayes optimal separating hyperplane. This is an analogous result to the one shown in Paul (2007) and Nadler (2008) for the angle between the sample eigenvector and the population eigenvector in random matrix theorem. We provide not just bound, but sharp prediction of the asymptotic behavior of SVM that can be determined by a set of nonlinear equations. Based on the analytical results, we propose a new method of selecting tuning parameter which significantly reduces the computational cost. A surprising finding is that SVM achieves its best performance at small value of the tuning parameter under spiked population model. These results are confirmed to be correct by comparing with those of numerical simulations on finite-size systems. We also apply our formulas to an actual dataset of breast cancer and find agreement between analytical derivations and numerical computations based on cross validation.},
 author = {Hanwen Huang},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W2614751510},
 pages = {1--21},
 title = {Asymptotic behavior of support vector machine for spiked population model},
 url = {http://jmlr.org/papers/v18/16-564.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-568,
 abstract = {We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate method, which alternates between maximizing over one (or more) randomly chosen dual variable and minimizing over the primal variable. We also develop an extension to non-smooth and nonstrongly convex loss functions, and an extension with better convergence rate on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods.},
 author = {Yuchen Zhang and Lin Xiao},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W1948582444},
 pages = {1--42},
 title = {Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization},
 url = {http://jmlr.org/papers/v18/16-568.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-572,
 author = {Huishuai Zhang and Yingbin Liang and Yuejie Chi},
 journal = {Journal of Machine Learning Research},
 number = {141},
 openalex = {W2783859821},
 pages = {1--35},
 title = {A Nonconvex Approach for Phase Retrieval: Reshaped Wirtinger Flow and Incremental Algorithms},
 url = {http://jmlr.org/papers/v18/16-572.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-577,
 author = {Eugene Ndiaye and Olivier Fercoq and Alex and re Gramfort and Joseph Salmon},
 journal = {Journal of Machine Learning Research},
 number = {128},
 openalex = {W2962939576},
 pages = {1--33},
 title = {Gap Safe screening rules for sparsity enforcing penalties},
 url = {http://jmlr.org/papers/v18/16-577.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-579,
 abstract = {This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matern kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the data set, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the number of data and M is the number of features.},
 author = {James Hensman and Nicolas Durrande and Arno Solin},
 journal = {Journal of Machine Learning Research},
 number = {151},
 openalex = {W2550412943},
 pages = {1--52},
 title = {Variational Fourier features for Gaussian processes},
 url = {http://jmlr.org/papers/v18/16-579.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-587,
 abstract = {Large-scale regression problems where both the number of variables, $p$, and the number of observations, $n$, may be large and in the order of millions or more, are becoming increasingly more common. Typically the data are sparse: only a fraction of a percent of the entries in the design matrix are non-zero. Nevertheless, often the only computationally feasible approach is to perform dimension reduction to obtain a new design matrix with far fewer columns and then work with this compressed data. 
$b$-bit min-wise hashing (Li and Konig, 2011) is a promising dimension reduction scheme for sparse matrices which produces a set of random features such that regression on the resulting design matrix approximates a kernel regression with the resemblance kernel. In this work, we derive bounds on the prediction error of such regressions. For both linear and logistic models we show that the average prediction error vanishes asymptotically as long as $q \|\beta^*\|_2^2 /n \rightarrow 0$, where $q$ is the average number of non-zero entries in each row of the design matrix and $\beta^*$ is the coefficient of the linear predictor. 
We also show that ordinary least squares or ridge regression applied to the reduced data can in fact allow us fit more flexible models. We obtain non-asymptotic prediction error bounds for interaction models and for models where an unknown row normalisation must be applied in order for the signal to be linear in the predictors.},
 author = {Rajen D. Shah and Nicolai Meinshausen},
 journal = {Journal of Machine Learning Research},
 number = {178},
 openalex = {W2963499975},
 pages = {1--42},
 title = {On $b$-bit Min-wise Hashing for Large-scale Regression and Classification with Sparse Data},
 url = {http://jmlr.org/papers/v18/16-587.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-590,
 author = {Julia Vinogradska and Bastian Bischoff and Duy Nguyen-Tuong and Jan Peters},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2767089347},
 pages = {1--37},
 title = {Stability of Controllers for Gaussian Process Dynamics},
 url = {http://jmlr.org/papers/v18/16-590.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-595,
 abstract = {This work characterizes the benefits of averaging schemes widely used in conjunction with stochastic gradient descent (SGD). In particular, this work provides a sharp analysis of: (1) mini-batching, a method of averaging many samples of a stochastic gradient to both reduce the variance of the stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD to decrease the variance in SGD's final iterate. This work presents non-asymptotic excess risk bounds for these schemes for the stochastic approximation problem of least squares regression. Furthermore, this work establishes a precise problem-dependent extent to which mini-batch SGD yields provable near-linear parallelization speedups over SGD with batch size one. This allows for understanding learning rate versus batch size tradeoffs for the final iterate of an SGD method. These results are then utilized in providing a highly parallelizable SGD method that obtains the minimax risk with nearly the same number of serial updates as batch gradient descent, improving significantly over existing SGD methods. A non-asymptotic analysis of communication efficient parallelization schemes such as model-averaging/parameter mixing methods is then provided. Finally, this work sheds light on some fundamental differences in SGD's behavior when dealing with agnostic noise in the (non-realizable) least squares regression problem. In particular, the work shows that the stepsizes that ensure minimax risk for the agnostic case must be a function of the noise properties. This paper builds on the operator view of analyzing SGD methods, introduced by Defossez and Bach (2015), followed by developing a novel analysis in bounding these operators to characterize the excess risk. These techniques are of broader interest in analyzing computational aspects of stochastic approximation.},
 author = {Prateek Jain and Sham M. Kakade and Rahul Kidambi and Praneeth Netrapalli and Aaron Sidford},
 journal = {Journal of Machine Learning Research},
 number = {223},
 openalex = {W2796196168},
 pages = {1--42},
 title = {Parallelizing Stochastic Gradient Descent for Least Squares Regression: mini-batching, averaging, and model misspecification},
 url = {http://jmlr.org/papers/v18/16-595.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:16-596,
 abstract = {Multiclass classification problems such as image annotation can involve a large number of classes. In this context, confusion between classes can occur, and single label classification may be misleading. We provide in the present paper a general device that, given an unlabeled dataset and a score function defined as the minimizer of some empirical and convex risk, outputs a set of class labels, instead of a single one. Interestingly, this procedure does not require that the unlabeled dataset explores the whole classes. Even more, the method is calibrated to control the expected size of the output set while minimizing the classification risk. We show the statistical optimality of the procedure and establish rates of convergence under the Tsybakov margin condition. It turns out that these rates are linear on the number of labels. We apply our methodology to convex aggregation of confidence sets based on the V-fold cross validation principle also known as the superlearning principle. We illustrate the numerical performance of the procedure on real data and demonstrate in particular that with moderate expected size, w.r.t. the number of labels, the procedure provides significant improvement of the classification risk.},
 author = {Christophe Denis and Mohamed Hebiri},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W2517118512},
 pages = {1--28},
 title = {Confidence sets with expected sizes for Multiclass Classification},
 url = {http://jmlr.org/papers/v18/16-596.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-601,
 abstract = {This paper provides error analysis for distributed semi-supervised learning with kernel ridge regression (DSKRR) based on a divide-and-conquer strategy. DSKRR applies kernel ridge regression (KRR) ...},
 author = {Xiangyu Chang and Shao-Bo Lin and Ding-Xuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W3017435008},
 pages = {1--22},
 title = {Distributed semi-supervised learning with kernel ridge regression},
 url = {http://jmlr.org/papers/v18/16-601.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-603,
 abstract = {Gaussian processes (GPs) are flexible distributions over functions that enable high-level assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper, we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, EP and Power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way, all of approximation is performed at `inference time' rather than at `modelling time' resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks.},
 author = {Thang D. Bui and Josiah Yan and Richard E. Turner},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W2763914732},
 pages = {1--72},
 title = {A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation},
 url = {http://jmlr.org/papers/v18/16-603.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-632,
 abstract = {We consider the closely related problems of bandit convex optimization with two-point feedback, and zero-order stochastic convex optimization with two function evaluations per round. We provide a simple algorithm and analysis which is optimal for convex Lipschitz functions. This improves on \cite{dujww13}, which only provides an optimal result for smooth functions; Moreover, the algorithm and analysis are simpler, and readily extend to non-Euclidean problems. The algorithm is based on a small but surprisingly powerful modification of the gradient estimator.},
 author = {Ohad Shamir},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2963308146},
 pages = {1--11},
 title = {An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback},
 url = {http://jmlr.org/papers/v18/16-632.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-634,
 abstract = {Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL.},
 author = {Christian Wirth and Riad Akrour and Gerhard Neumann and Johannes F{{\"u}}rnkranz},
 journal = {Journal of Machine Learning Research},
 number = {136},
 openalex = {W2785324569},
 pages = {1--46},
 title = {A survey of preference-based reinforcement learning methods},
 url = {http://jmlr.org/papers/v18/16-634.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-640,
 author = {Jason D. Lee and Qihang Lin and Tengyu Ma and Tianbao Yang},
 journal = {Journal of Machine Learning Research},
 number = {122},
 openalex = {W2769669795},
 pages = {1--43},
 title = {Distributed stochastic variance reduced gradient methods by sampling extra data with replacement},
 url = {http://jmlr.org/papers/v18/16-640.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-645,
 abstract = {Multilayer networks are a useful way to capture and model multiple, binary or weighted relationships among a fixed group of objects. While community detection has proven to be a useful exploratory technique for the analysis of single-layer networks, the development of community detection methods for multilayer networks is still in its infancy. We propose and investigate a procedure, called Multilayer Extraction, that identifies densely connected vertex-layer sets in multilayer networks. Multilayer Extraction makes use of a significance based score that quantifies the connectivity of an observed vertex-layer set through comparison with a fixed degree random graph model. Multilayer Extraction directly handles networks with heterogeneous layers where community structure may be different from layer to layer. The procedure can capture overlapping communities, as well as background vertex-layer pairs that do not belong to any community. We establish consistency of the vertex-layer set optimizer of our proposed multilayer score under the multilayer stochastic block model. We investigate the performance of Multilayer Extraction on three applications and a test bed of simulations. Our theoretical and numerical evaluations suggest that Multilayer Extraction is an effective exploratory tool for analyzing complex multilayer networks. Publicly available code is available at https://github.com/jdwilson4/MultilayerExtraction.},
 author = {James D. Wilson and John Palowitch and Shankar Bhamidi and Andrew B. Nobel},
 journal = {Journal of Machine Learning Research},
 number = {149},
 openalex = {W2963562062},
 pages = {1--49},
 title = {Community extraction in multilayer networks with heterogeneous community structure},
 url = {http://jmlr.org/papers/v18/16-645.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-655,
 abstract = {We propose a novel approach to Bayesian analysis that is provably robust to outliers in the data and often has computational advantages over standard methods. Our technique is based on splitting the data into non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the resulting measures. The main novelty of our approach is the proposed aggregation step, which is based on the evaluation of a median in the space of probability measures equipped with a suitable collection of distances that can be quickly and efficiently evaluated in practice. We present both theoretical and numerical evidence illustrating the improvements achieved by our method.},
 author = {Stanislav Minsker and Sanvesh Srivastava and Lizhen Lin and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {124},
 openalex = {W2963888630},
 pages = {1--40},
 title = {Robust and Scalable Bayes via a Median of Subset Posterior Measures},
 url = {http://jmlr.org/papers/v18/16-655.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:16-657,
 author = {Christopher Tosh and Sanjoy Dasgupta},
 journal = {Journal of Machine Learning Research},
 number = {175},
 openalex = {W2887549123},
 pages = {1--11},
 title = {Maximum Likelihood Estimation for Mixtures of Spherical Gaussians is NP-hard},
 url = {http://jmlr.org/papers/v18/16-657.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-003,
 abstract = {Classification is an important supervised learning technique with numerous applications. We develop an angle-based multicategory distance-weighted support vector machine (MDWSVM) classification met...},
 author = {Hui Sun and Bruce A. Craig and Lingsong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W2760503384},
 pages = {1--21},
 title = {Angle-based Multicategory Distance-weighted SVM},
 url = {http://jmlr.org/papers/v18/17-003.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-007,
 abstract = {We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models. We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to $10\%$ in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems. We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.},
 author = {Shannon Fenn and Pablo Moscato},
 journal = {Journal of Machine Learning Research},
 number = {114},
 openalex = {W4299293130},
 pages = {1--26},
 title = {Target Curricula via Selection of Minimum Feature Sets: a Case Study in Boolean Networks},
 url = {http://jmlr.org/papers/v18/17-007.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-014,
 abstract = {Learning a causal effect from observational data requires strong assumptions. One possible method is to use instrumental variables, which are typically justified by background knowledge. It is possible, under further assumptions, to discover whether a variable is structurally instrumental to a target causal effect X → Y. However, the few existing approaches are lacking on how general these assumptions can be, and how to express possible equivalence classes of solutions. We present instrumental variable discovery methods that systematically characterize which set of causal effects can and cannot be discovered under local graphical criteria that define instrumental variables, without reconstructing full causal graphs. We also introduce the first methods to exploit non-Gaussianity assumptions, highlighting identifiability problems and solutions. Due to the difficulty of estimating such models from finite data, we investigate how to strengthen assumptions in order to make the statistical problem more manageable.},
 author = {Ricardo Silva and Shohei Shimizu},
 journal = {Journal of Machine Learning Research},
 number = {120},
 openalex = {W2768642709},
 pages = {1--49},
 title = {Learning instrumental variables with structural and non-gaussianity assumptions},
 url = {http://jmlr.org/papers/v18/17-014.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-019,
 abstract = {We consider joint estimation of multiple graphical models arising from heterogeneous and high-dimensional observations. Unlike most previous approaches which assume that the cluster structure is given in advance, an appealing feature of our method is to learn cluster structure while estimating heterogeneous graphical models. This is achieved via a high dimensional version of Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993). A joint graphical lasso penalty is imposed on the conditional maximization step to extract both homogeneity and heterogeneity components across all clusters. Our algorithm is computationally efficient due to fast sparse learning routines and can be implemented without unsupervised learning knowledge. The superior performance of our method is demonstrated by extensive experiments and its application to a Glioblastoma cancer dataset reveals some new insights in understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound is established for the output directly from our high dimensional ECM algorithm, and it consists of two quantities: statistical error (statistical accuracy) and optimization error (computational complexity). Such a result gives a theoretical guideline in terminating our ECM iterations.},
 author = {Botao Hao and Will Wei Sun and Yufeng Liu and Guang Cheng},
 journal = {Journal of Machine Learning Research},
 number = {217},
 openalex = {W2558869262},
 pages = {1--58},
 title = {Simultaneous Clustering and Estimation of Heterogeneous Graphical Models},
 url = {http://jmlr.org/papers/v18/17-019.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-032,
 abstract = {In this paper, we study the minimax estimation of the Bochner integral $$\mu_k(P):=\int_{\mathcal{X}} k(\cdot,x)\,dP(x),$$ also called as the kernel mean embedding, based on random samples drawn i.i.d.~from $P$, where $k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}$ is a positive definite kernel. Various estimators (including the empirical estimator), $\hat{\theta}_n$ of $\mu_k(P)$ are studied in the literature wherein all of them satisfy $\bigl\| \hat{\theta}_n-\mu_k(P)\bigr\|_{\mathcal{H}_k}=O_P(n^{-1/2})$ with $\mathcal{H}_k$ being the reproducing kernel Hilbert space induced by $k$. The main contribution of the paper is in showing that the above mentioned rate of $n^{-1/2}$ is minimax in $\|\cdot\|_{\mathcal{H}_k}$ and $\|\cdot\|_{L^2(\mathbb{R}^d)}$-norms over the class of discrete measures and the class of measures that has an infinitely differentiable density, with $k$ being a continuous translation-invariant kernel on $\mathbb{R}^d$. The interesting aspect of this result is that the minimax rate is independent of the smoothness of the kernel and the density of $P$ (if it exists). This result has practical consequences in statistical applications as the mean embedding has been widely employed in non-parametric hypothesis testing, density estimation, causal inference and feature selection, through its relation to energy distance (and distance covariance).},
 author = {Ilya Tolstikhin and Bharath K. Sriperumbudur and Krikamol Mu and et},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W2962835476},
 pages = {1--47},
 title = {Minimax Estimation of Kernel Mean Embeddings},
 url = {http://jmlr.org/papers/v18/17-032.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-033,
 abstract = {We propose a mixed integer programming (MIP) model and iterative algorithms based on topological orders to solve optimization problems with acyclic constraints on a directed graph. The proposed MIP model has a significantly lower number of constraints compared to popular MIP models based on cycle elimination constraints and triangular inequalities. The proposed iterative algorithms use gradient descent and iterative reordering approaches, respectively, for searching topological orders. A computational experiment is presented for the Gaussian Bayesian network learning problem, an optimization problem minimizing the sum of squared errors of regression models with L1 penalty over a feature network with application of gene network inference in bioinformatics.},
 author = {Young Woong Park and Diego Klabjan},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W2582888981},
 pages = {1--32},
 title = {Bayesian Network Learning via Topological Order},
 url = {http://jmlr.org/papers/v18/17-033.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-039,
 abstract = {Abstract Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, multiple runs of K-means clustering returns clusterings with a fixed number of clusters, while the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on the ranking of similar clustering pairs, and the evaluation of a clustering method with respect to a random baseline; thus, the choice of random clustering model should be carefully justified.},
 author = {Alexander J. Gates and Yong-Yeol Ahn},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W2951429788},
 pages = {1--28},
 title = {The Impact of Random Models on Clustering Similarity},
 url = {http://jmlr.org/papers/v18/17-039.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-044,
 abstract = {In recent years, stochastic gradient descent (SGD) methods and randomized linear algebra (RLA) algorithms have been applied to many large-scale problems in machine learning and data analysis. We aim to bridge the gap between these two methods in solving constrained overdetermined linear regression problems---e.g., $\ell_2$ and $\ell_1$ regression problems. We propose a hybrid algorithm named pwSGD that uses RLA techniques for preconditioning and constructing an importance sampling distribution, and then performs an SGD-like iterative process with weighted sampling on the preconditioned system. We prove that pwSGD inherits faster convergence rates that only depend on the lower dimension of the linear system, while maintaining low computation complexity. Particularly, when solving $\ell_1$ regression with size $n$ by $d$, pwSGD returns an approximate solution with $\epsilon$ relative error in the objective value in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)/\epsilon^2)$ time. This complexity is uniformly better than that of RLA methods in terms of both $\epsilon$ and $d$ when the problem is unconstrained. For $\ell_2$ regression, pwSGD returns an approximate solution with $\epsilon$ relative error in the objective value and the solution vector measured in prediction norm in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d) \log(1/\epsilon) /\epsilon)$ time. We also provide lower bounds on the coreset complexity for more general regression problems, indicating that still new ideas will be needed to extend similar RLA preconditioning ideas to weighted SGD algorithms for more general regression problems. Finally, the effectiveness of such algorithms is illustrated numerically on both synthetic and real datasets.},
 author = {Jiyan Yang and Yin-Lam Chow and Christopher R{{\'e}} and Michael W. Mahoney},
 journal = {Journal of Machine Learning Research},
 number = {211},
 openalex = {W1909276365},
 pages = {1--43},
 title = {Weighted SGD for $\ell_p$ Regression with Randomized Preconditioning},
 url = {http://jmlr.org/papers/v18/17-044.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-048,
 abstract = {We introduce Fisher consistency in the sense of unbiasedness as a desirable property for estimators of class prior probabilities. Lack of Fisher consistency could be used as a criterion to dismiss estimators that are unlikely to deliver precise estimates in test datasets under prior probability and more general dataset shift. The usefulness of this unbiasedness concept is demonstrated with three examples of classifiers used for quantification: Adjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted Classify & Count and EM-algorithm are Fisher consistent. A counter-example shows that CDE-Iterate is not Fisher consistent and, therefore, cannot be trusted to deliver reliable estimates of class probabilities.},
 author = {Dirk Tasche},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W2963577430},
 pages = {1--32},
 title = {Fisher Consistency for Prior Probability Shift},
 url = {http://jmlr.org/papers/v18/17-048.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-049,
 abstract = {In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.},
 author = {Maren Mahsereci and Philipp Hennig},
 journal = {Journal of Machine Learning Research},
 number = {119},
 openalex = {W2771005692},
 pages = {1--59},
 title = {Probabilistic Line Searches for Stochastic Optimization},
 url = {http://jmlr.org/papers/v18/17-049.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-055,
 abstract = {Dynamical systems comprising of multiple components that can be partitioned into distinct blocks originate in many scientific areas. A pertinent example is the interactions between financial assets and selected macroeconomic indicators, which has been studied at aggregate level---e.g. a stock index and an employment index---extensively in the macroeconomics literature. A key shortcoming of this approach is that it ignores potential influences from other related components (e.g. Gross Domestic Product) that may exert influence on the system's dynamics and structure and thus produces incorrect results. To mitigate this issue, we consider a multi-block linear dynamical system with Granger-causal ordering between blocks, wherein the blocks' temporal dynamics are described by vector autoregressive processes and are influenced by blocks higher in the system hierarchy. We derive the maximum likelihood estimator for the posited model for Gaussian data in the high-dimensional setting based on appropriate regularization schemes for the parameters of the block components. To optimize the underlying non-convex likelihood function, we develop an iterative algorithm with convergence guarantees. We establish theoretical properties of the maximum likelihood estimates, leveraging the decomposability of the regularizers and a careful analysis of the iterates. Finally, we develop testing procedures for the null hypothesis of whether a block "Granger-causes" another block of variables. The performance of the model and the testing procedures are evaluated on synthetic data, and illustrated on a data set involving log-returns of the US S&amp;P100 component stocks and key macroeconomic variables for the 2001--16 period.},
 author = {Jiahe Lin and George Michailidis},
 journal = {Journal of Machine Learning Research},
 number = {117},
 openalex = {W2963670397},
 pages = {1--49},
 title = {Regularized Estimation and Testing for High-Dimensional Multi-Block Vector-Autoregressive Models},
 url = {http://jmlr.org/papers/v18/17-055.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-061,
 abstract = {We consider a firm that sells a large number of products to its customers in an online fashion. Each product is described by a high dimensional feature vector, and the market value of a product is assumed to be linear in the values of its features. Parameters of the valuation model are unknown and can change over time. The firm sequentially observes a product's features and can use the historical sales data (binary sale/no sale feedbacks) to set the price of current product, with the objective of maximizing the collected revenue. We measure the performance of a dynamic pricing policy via regret, which is the expected revenue loss compared to a clairvoyant that knows the sequence of model parameters in advance. We propose a pricing policy based on projected stochastic gradient descent (PSGD) and characterize its regret in terms of time $T$, features dimension $d$, and the temporal variability in the model parameters, $δ_t$. We consider two settings. In the first one, feature vectors are chosen antagonistically by nature and we prove that the regret of PSGD pricing policy is of order $O(\sqrt{T} + \sum_{t=1}^T \sqrt{t}δ_t)$. In the second setting (referred to as stochastic features model), the feature vectors are drawn independently from an unknown distribution. We show that in this case, the regret of PSGD pricing policy is of order $O(d^2 \log T + \sum_{t=1}^T tδ_t/d)$.},
 author = {Adel Javanmard},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W2963764445},
 pages = {1--31},
 title = {Perishability of Data: Dynamic Pricing under Varying-Coefficient Models},
 url = {http://jmlr.org/papers/v18/17-061.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-069,
 author = {Fred Morstatter and Huan Liu},
 journal = {Journal of Machine Learning Research},
 number = {169},
 openalex = {W2808151906},
 pages = {1--32},
 title = {In Search of Coherence and Consensus: Measuring the Interpretability of Statistical Topics},
 url = {http://jmlr.org/papers/v18/17-069.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-073,
 abstract = {Missing data is a common problem in real-world settings and for this reason has attracted significant attention in the statistical literature. We propose a flexible framework based on formal optimization to impute missing data with mixed continuous and categorical variables. This framework can readily incorporate various predictive models including K- nearest neighbors, support vector machines, and decision tree based methods, and can be adapted for multiple imputation. We derive fast first-order methods that obtain high quality solutions in seconds following a general imputation algorithm opt.impute presented in this paper. We demonstrate that our proposed method improves out-of-sample accuracy in large-scale computational experiments across a sample of 84 data sets taken from the UCI Machine Learning Repository. In all scenarios of missing at random mechanisms and various missing percentages, opt.impute produces the best overall imputation in most data sets benchmarked against five other methods: mean impute, K-nearest neighbors, iterative knn, Bayesian PCA, and predictive-mean matching, with an average reduction in mean absolute error of 8.3% against the best cross-validated benchmark method. Moreover, opt.impute leads to improved out-of-sample performance of learning algorithms trained using the imputed data, demonstrated by computational experiments on 10 downstream tasks. For models trained using opt.impute single imputations with 50% data missing, the average out-of-sample R2 is 0.339 in the regression tasks and the average out-of-sample accuracy is 86.1% in the classification tasks, compared to 0.315 and 84.4% for the best cross-validated benchmark method. In the multiple imputation setting, downstream models trained using opt.impute obtain a statistically significant improvement over models trained using multivariate imputation by chained equations (mice) in 8/10 missing data scenarios considered.},
 author = {Dimitris Bertsimas and Colin Pawlowski and Ying Daisy Zhuo},
 journal = {Journal of Machine Learning Research},
 number = {196},
 openalex = {W2808622828},
 pages = {1--39},
 title = {From predictive methods to missing data imputation: an optimization approach},
 url = {http://jmlr.org/papers/v18/17-073.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-078,
 abstract = {The use of convex regularizers allows for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, for a large class of nonconvex regularizers, we propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex regularizer, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the proximal algorithm, Frank-Wolfe algorithm, alternating direction method of multipliers and stochastic gradient descent). Extensions are made when the convexified regularizer does not have closed-form proximal step, and when the loss function is nonconvex, nonsmooth. Extensive experiments on a variety of machine learning application scenarios show that optimizing the transformed problem is much faster than running the state-of-the-art on the original problem.},
 author = {Quanming Yao and James T. Kwok},
 journal = {Journal of Machine Learning Research},
 number = {179},
 openalex = {W2809317144},
 pages = {1--52},
 title = {Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity},
 url = {http://jmlr.org/papers/v18/17-078.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-079,
 author = {Ruitong Huang and Tor Lattimore and Andr{{\'a}}s Gy{{\"o}}rgy and Csaba Szepesv{{\'a}}ri},
 journal = {Journal of Machine Learning Research},
 number = {145},
 openalex = {W2783423588},
 pages = {1--31},
 title = {Following the Leader and Fast Rates in Online Linear Prediction: Curved Constraint Sets and Other Regularities},
 url = {http://jmlr.org/papers/v18/17-079.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-081,
 abstract = {We study the cost function for hierarchical clusterings introduced by [Dasgupta, 2015] where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [Dasgupta, 2015] that a top-down algorithm returns a hierarchical clustering of cost at most \(O\left(\alpha_n \log n\right)\) times the cost of the optimal hierarchical clustering, where \(\alpha_n\) is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top down algorithm returns a hierarchical clustering of cost at most \(O\left(\log^{3/2} n\right)\) times the cost of the optimal solution. We improve this by giving an \(O(\log{n})\)-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of \emph{sphere growing} which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an \(O(\log{n})\)-approximate hierarchical clustering for a generalization of this cost function also studied in [Dasgupta, 2015]. Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with an inapproximability result for this problem, namely that no polynomial sized LP or SDP can be used to obtain a constant factor approximation for this problem.},
 author = {Aurko Roy and Sebastian Pokutta},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W2542513214},
 pages = {1--35},
 title = {Hierarchical Clustering via Spreading Metrics},
 url = {http://jmlr.org/papers/v18/17-081.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-101,
 abstract = {We consider the submodular function minimization (SFM) and the quadratic minimization problemsregularized by the Lov'asz extension of the submodular function. These optimization problemsare intimately related; for example,min-cut problems and total variation denoising problems, wherethe cut function is submodular and its Lov'asz extension is given by the associated total variation.When a quadratic loss is regularized by the total variation of a cut function, it thus becomes atotal variation denoising problem and we use the same terminology in this paper for "general" submodularfunctions. We propose a new active-set algorithm for total variation denoising with theassumption of an oracle that solves the corresponding SFM problem. This can be seen as localdescent algorithm over ordered partitions with explicit convergence guarantees. It is more flexiblethan the existing algorithms with the ability for warm-restarts using the solution of a closely relatedproblem. Further, we also consider the case when a submodular function can be decomposed intothe sum of two submodular functions F1 and F2 and assume SFM oracles for these two functions.We propose a new active-set algorithm for total variation denoising (and hence SFM by thresholdingthe solution at zero). This algorithm also performs local descent over ordered partitions and itsability to warm start considerably improves the performance of the algorithm. In the experiments,we compare the performance of the proposed algorithms with state-of-the-art algorithms, showingthat it reduces the calls to SFM oracles.},
 author = {K. S. Sesh Kumar and Francis Bach},
 journal = {Journal of Machine Learning Research},
 number = {132},
 openalex = {W2549354558},
 pages = {1--31},
 title = {Active-set Methods for Submodular Minimization Problems},
 url = {http://jmlr.org/papers/v18/17-101.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-113,
 abstract = {We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e.g., acoustic or visual low-level descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed subbags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool are exemplified in two sample scenarios: time-continuous speech-based emotion recognition and sentiment analysis in tweets where improved results over other feature representation forms were observed.},
 author = {Maximilian Schmitt and Bj{{\"o}}rn Schuller},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W4293571337},
 pages = {1--5},
 title = {openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit},
 url = {http://jmlr.org/papers/v18/17-113.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-145,
 abstract = {We propose a novel class of time-varying nonparanormal graphical models, which allows us to model high dimensional heavy-tailed systems and the evolution of their latent network structures. Under this model we develop statistical tests for presence of edges both locally at a fixed index value and globally over a range of values. The tests are developed for a high-dimensional regime, are robust to model selection mistakes and do not require commonly assumed minimum signal strength. The testing procedures are based on a high dimensional, debiasing-free moment estimator, which uses a novel kernel smoothed Kendall's tau correlation matrix as an input statistic. The estimator consistently estimates the latent inverse Pearson correlation matrix uniformly in both the index variable and kernel bandwidth. Its rate of convergence is shown to be minimax optimal. Our method is supported by thorough numerical simulations and an application to a neural imaging data set.},
 author = {Junwei Lu and Mladen Kolar and Han Liu},
 journal = {Journal of Machine Learning Research},
 number = {203},
 openalex = {W2963221427},
 pages = {1--78},
 title = {Post-regularization inference for time-varying nonparanormal graphical models},
 url = {http://jmlr.org/papers/v18/17-145.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-151,
 author = {Fabrizio Angiulli},
 journal = {Journal of Machine Learning Research},
 number = {170},
 openalex = {W2808130767},
 pages = {1--60},
 title = {On the Behavior of Intrinsically High-Dimensional Spaces: Distances, Direct and Reverse Nearest Neighbors, and Hubness},
 url = {http://jmlr.org/papers/v18/17-151.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-156,
 author = {Frans A. Oliehoek and Matthijs T. J. Spaan and Bas Terwijn and Philipp Robbel and Jo\~{a}o V. Messias},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W2903219079},
 pages = {1--5},
 title = {The MADP Toolbox: An Open-Source Library for Planning and Learning in (Multi-)Agent Systems.},
 url = {http://jmlr.org/papers/v18/17-156.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-157,
 abstract = {The cyclic block coordinate descent-type (CBCD-type) methods, which performs iterative updates for a few coordinates (a block) simultaneously throughout the procedure, have shown remarkable computational performance for solving strongly convex minimization problems. Typical applications include many popular statistical machine learning methods such as elastic-net regression, ridge penalized logistic regression, and sparse additive regression. Existing optimization literature has shown that for strongly convex minimization, the CBCD-type methods attain iteration complexity of $\mathcal{O}(p\log(1/\epsilon))$, where $\epsilon$ is a pre-specified accuracy of the objective value, and $p$ is the number of blocks. However, such iteration complexity explicitly depends on $p$, and therefore is at least $p$ times worse than the complexity $\mathcal{O}(\log(1/\epsilon))$ of gradient descent (GD) methods. To bridge this theoretical gap, we propose an improved convergence analysis for the CBCD-type methods. In particular, we first show that for a family of quadratic minimization problems, the iteration complexity $\mathcal{O}(\log^2(p)\cdot\log(1/\epsilon))$ of the CBCD-type methods matches that of the GD methods in term of dependency on $p$, up to a $\log^2 p$ factor. Thus our complexity bounds are sharper than the existing bounds by at least a factor of $p/\log^2(p)$. We also provide a lower bound to confirm that our improved complexity bounds are tight (up to a $\log^2 (p)$ factor), under the assumption that the largest and smallest eigenvalues of the Hessian matrix do not scale with $p$. Finally, we generalize our analysis to other strongly convex minimization problems beyond quadratic ones.},
 author = {Xingguo Li and Tuo Zhao and Raman Arora and Han Liu and Mingyi Hong},
 journal = {Journal of Machine Learning Research},
 number = {184},
 openalex = {W2963821730},
 pages = {1--24},
 title = {On faster convergence of cyclic block coordinate descent-type methods for strongly convex minimization},
 url = {http://jmlr.org/papers/v18/17-157.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-159,
 abstract = {To find optimal decision rule, Fan et al. (2016) proposed an innovative concordance-assisted learning algorithm which is based on maximum rank correlation estimator. It makes better use of the available information through pairwise comparison. However the objective function is discontinuous and computationally hard to optimize. In this paper, we consider a convex surrogate loss function to solve this problem. In addition, our algorithm ensures sparsity of decision rule and renders easy interpretation. We derive the L2 error bound of the estimated coefficients under ultra-high dimension. Simulation results of various settings and application to STAR*D both illustrate that the proposed method can still estimate optimal treatment regime successfully when the number of covariates is large.},
 author = {Shuhan Liang and Wenbin Lu and Rui Song and Lan Wang},
 journal = {Journal of Machine Learning Research},
 number = {202},
 openalex = {W2808139858},
 pages = {1--26},
 title = {Sparse concordance-assisted learning for optimal treatment decision.},
 url = {http://jmlr.org/papers/v18/17-159.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-175,
 abstract = {We derive computationally tractable methods to select a small subset of experiment settings from a large pool of given design points. The primary focus is on linear regression models, while the technique extends to generalized linear models and Delta's method (estimating functions of linear regression models) as well. The algorithms are based on a continuous relaxation of an otherwise intractable combinatorial optimization problem, with sampling or greedy procedures as post-processing steps. Formal approximation guarantees are established for both algorithms, and numerical results on both synthetic and real-world data confirm the effectiveness of the proposed methods.},
 author = {Yining Wang and Adams Wei Yu and Aarti Singh},
 journal = {Journal of Machine Learning Research},
 number = {143},
 openalex = {W2964314596},
 pages = {1--41},
 title = {On Computationally Tractable Selection of Experiments in Measurement-Constrained Regression Models},
 url = {http://jmlr.org/papers/v18/17-175.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-176,
 abstract = {We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. We study how regularization properties are controlled by the step-size, the number of passes and the mini-batch size. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases. As a byproduct, we derive optimal convergence results for batch gradient methods (even in the non-attainable cases).},
 author = {Junhong Lin and Lorenzo Rosasco},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W2963290222},
 pages = {1--47},
 title = {Optimal Rates for Multi-pass Stochastic Gradient Methods},
 url = {http://jmlr.org/papers/v18/17-176.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-178,
 abstract = {We extend the adaptive regression spline model by incorporating saturation, the natural requirement that a function extend as a constant outside a certain range. We fit saturating splines to data via a convex optimization problem over a space of measures, which we solve using an efficient algorithm based on the conditional gradient method. Unlike many existing approaches, our algorithm solves the original infinite-dimensional (for splines of degree at least two) optimization problem without pre-specified knot locations. We then adapt our algorithm to fit generalized additive models with saturating splines as coordinate functions and show that the saturation requirement allows our model to simultaneously perform feature selection and nonlinear function fitting. Finally, we briefly sketch how the method can be extended to higher order splines and to different requirements on the extension outside the data range.},
 author = {Nicholas Boyd and Trevor Hastie and Stephen Boyd and Benjamin Recht and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {197},
 openalex = {W2962941624},
 pages = {1--32},
 title = {Saturating Splines and Feature Selection.},
 url = {http://jmlr.org/papers/v18/17-178.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-189,
 abstract = {We consider the problem of low canonical polyadic (CP) rank tensor completion. A completion is a tensor whose entries agree with the observed entries and its rank matches the given CP rank. We analyze the manifold structure corresponding to the tensors with the given rank and define a set of polynomials based on the sampling pattern and CP decomposition. Then, we show that finite completability of the sampled tensor is equivalent to having a certain number of algebraically independent polynomials among the defined polynomials. Our proposed approach results in characterizing the maximum number of algebraically independent polynomials in terms of a simple geometric structure of the sampling pattern, and therefore we obtain the deterministic necessary and sufficient condition on the sampling pattern for finite completability of the sampled tensor. Moreover, assuming that the entries of the tensor are sampled independently with probability $p$ and using the mentioned deterministic analysis, we propose a combinatorial method to derive a lower bound on the sampling probability $p$, or equivalently, the number of sampled entries that guarantees finite completability with high probability. We also show that the existing result for the matrix completion problem can be used to obtain a loose lower bound on the sampling probability $p$. In addition, we obtain deterministic and probabilistic conditions for unique completability. It is seen that the number of samples required for finite or unique completability obtained by the proposed analysis on the CP manifold is orders-of-magnitude lower than that is obtained by the existing analysis on the Grassmannian manifold.},
 author = {Morteza Ashraphijuo and Xiaodong Wang},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W2963476323},
 pages = {1--29},
 title = {Fundamental Conditions for Low-CP-Rank Tensor Completion},
 url = {http://jmlr.org/papers/v18/17-189.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-197,
 abstract = {We propose a generic Bayesian mixed-effects model to estimate the temporal progression of a biological phenomenon from observations obtained at multiple time points for a group of individuals. The progression is modeled by continuous trajectories in the space of measurements. Individual trajectories of progression result from spatiotemporal transformations of an average trajectory. These transformations allow to quantify the changes in direction and pace at which the trajectories are followed. The framework of Rieman-nian geometry allows the model to be used with any kind of measurements with smooth constraints. A stochastic version of the Expectation-Maximization algorithm is used to produce produce maximum a posteriori estimates of the parameters. We evaluate our method using series of neuropsychological test scores from patients with mild cognitive impairments later diagnosed with Alzheimer's disease, and simulated evolutions of symmetric positive definite matrices. The data-driven model of the impairment of cognitive functions shows the variability in the ordering and timing of the decline of these functions in the population. We show also that the estimated spatiotemporal transformations effectively put into correspondence significant events in the progression of individuals.},
 author = {Jean-Baptiste Schiratti and St{{\'e}}phanie Allassonni{{\`e}}re and Olivier Colliot and Stanley Durrleman},
 journal = {Journal of Machine Learning Research},
 number = {133},
 openalex = {W2743901732},
 pages = {1--33},
 title = {A Bayesian mixed-effects model to learn trajectories of changes from repeated manifold-valued observations},
 url = {http://jmlr.org/papers/v18/17-197.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-203,
 abstract = {Motivated by applications in neuroimaging analysis, we propose a new regression model, Sparse TensOr REsponse regression (STORE), with a tensor response and a vector predictor. STORE embeds two key sparse structures: element-wise sparsity and low-rankness. It can handle both a non-symmetric and a symmetric tensor response, and thus is applicable to both structural and functional neuroimaging data. We formulate the parameter estimation as a non-convex optimization problem, and develop an efficient alternating updating algorithm. We establish a non-asymptotic estimation error bound for the actual estimator obtained from the proposed algorithm. This error bound reveals an interesting interaction between the computational efficiency and the statistical rate of convergence. When the distribution of the error tensor is Gaussian, we further obtain a fast estimation error rate which allows the tensor dimension to grow exponentially with the sample size. We illustrate the efficacy of our model through intensive simulations and an analysis of the Autism spectrum disorder neuroimaging data.},
 author = {Will Wei Sun and Lexin Li},
 journal = {Journal of Machine Learning Research},
 number = {135},
 openalex = {W2591263026},
 pages = {1--37},
 title = {STORE: Sparse Tensor Response Regression and Neuroimaging Analysis},
 url = {http://jmlr.org/papers/v18/17-203.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-214,
 abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show t...},
 author = {Stephan M and t and Matthew D. Hoffman and David M. Blei},
 journal = {Journal of Machine Learning Research},
 number = {134},
 openalex = {W2962915600},
 pages = {1--35},
 title = {Stochastic Gradient Descent as Approximate Bayesian Inference},
 url = {http://jmlr.org/papers/v18/17-214.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-228,
 abstract = {We introduce \texttt{pycobra}, a Python library devoted to ensemble learning (regression and classification) and visualisation. Its main assets are the implementation of several ensemble learning algorithms, a flexible and generic interface to compare and blend any existing machine learning algorithm available in Python libraries (as long as a \texttt{predict} method is given), and visualisation tools such as Voronoi tessellations. \texttt{pycobra} is fully \texttt{scikit-learn} compatible and is released under the MIT open-source license. \texttt{pycobra} can be downloaded from the Python Package Index (PyPi) and Machine Learning Open Source Software (MLOSS). The current version (along with Jupyter notebooks, extensive documentation, and continuous integration tests) is available at \href{https://github.com/bhargavvader/pycobra}{https://github.com/bhargavvader/pycobra} and official documentation website is \href{https://modal.lille.inria.fr/pycobra}{https://modal.lille.inria.fr/pycobra}.},
 author = {Benjamin Guedj and Bhargav Srinivasa Desikan},
 journal = {Journal of Machine Learning Research},
 number = {190},
 openalex = {W2732215177},
 pages = {1--5},
 title = {Pycobra: A Python Toolbox for Ensemble Learning and Visualisation},
 url = {http://jmlr.org/papers/v18/17-228.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-234,
 abstract = {This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.},
 author = {Jennifer Wortman Vaughan},
 journal = {Journal of Machine Learning Research},
 number = {193},
 openalex = {W2807760453},
 pages = {1--46},
 title = {Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research},
 url = {http://jmlr.org/papers/v18/17-234.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-243,
 abstract = {Learning DAG or Bayesian network models is an important problem in multi-variate causal inference. However, a number of challenges arises in learning large-scale DAG models including model identifiability and computational complexity since the space of directed graphs is huge. In this paper, we address these issues in a number of steps for a broad class of DAG models where the noise or variance is signal-dependent. Firstly we introduce a new class of identifiable DAG models, where each node has a distribution where the variance is a quadratic function of the mean (QVF DAG models). Our QVF DAG models include many interesting classes of distributions such as Poisson, Binomial, Geometric, Exponential, Gamma and many other distributions in which the noise variance depends on the mean. We prove that this class of QVF DAG models is identifiable, and introduce a new algorithm, the OverDispersion Scoring (ODS) algorithm, for learning large-scale QVF DAG models. Our algorithm is based on firstly learning the moralized or undirected graphical model representation of the DAG to reduce the DAG search-space, and then exploiting the quadratic variance property to learn the causal ordering. We show through theoretical results and simulations that our algorithm is statistically consistent in the high-dimensional p>n setting provided that the degree of the moralized graph is bounded and performs well compared to state-of-the-art DAG-learning algorithms.},
 author = {Gunwoong Park and Garvesh Raskutti},
 journal = {Journal of Machine Learning Research},
 number = {224},
 openalex = {W2963078077},
 pages = {1--44},
 title = {Learning Quadratic Variance Function (QVF) DAG Models via OverDispersion Scoring (ODS)},
 url = {http://jmlr.org/papers/v18/17-243.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-247,
 abstract = {In this article, we propose and study the performance of spectral community detection for a family of α-normalized adjacency matrices A, of the type D −α AD −α with D the degree matrix, in heterogeneous dense graph models. We show that the previously used normaliza-tion methods based on A or D −1 AD −1 are in general suboptimal in terms of correct recovery rates and, relying on advanced random matrix methods, we prove instead the existence of an optimal value α opt of the parameter α in our generic model; we further provide an online estimation of α opt only based on the node degrees in the graph. Numerical simulations show that the proposed method outperforms state-of-the-art spectral approaches on moderately dense to dense heterogeneous graphs.},
 author = {Hafiz TIOMOKO ALI and Romain COUILLET},
 journal = {Journal of Machine Learning Research},
 number = {225},
 openalex = {W2883292836},
 pages = {1--49},
 title = {Improved spectral community detection in large heterogeneous networks},
 url = {http://jmlr.org/papers/v18/17-247.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-269,
 abstract = {The number of trees T in the random forest (RF) algorithm for supervised learning has to be set by the user. It is controversial whether T should simply be set to the largest computationally manageable value or whether a smaller T may in some cases be better. While the principle underlying bagging is that "more trees are better", in practice the classification error rate sometimes reaches a minimum before increasing again for increasing number of trees. The goal of this paper is four-fold: (i) providing theoretical results showing that the expected error rate may be a non-monotonous function of the number of trees and explaining under which circumstances this happens; (ii) providing theoretical results showing that such non-monotonous patterns cannot be observed for other performance measures such as the Brier score and the logarithmic loss (for classification) and the mean squared error (for regression); (iii) illustrating the extent of the problem through an application to a large number (n = 306) of datasets from the public database OpenML; (iv) finally arguing in favor of setting it to a computationally feasible large number, depending on convergence properties of the desired performance measure.},
 author = {Philipp Probst and Anne-Laure Boulesteix},
 journal = {Journal of Machine Learning Research},
 number = {181},
 openalex = {W4297729205},
 pages = {1--18},
 title = {To tune or not to tune the number of trees in random forest?},
 url = {http://jmlr.org/papers/v18/17-269.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-275,
 abstract = {A main question in graphical models and causal inference is whether, given a probability
distribution P (which is usually an underlying distribution of data), there is a graph (or
graphs) to which P is faithful. The main goal of this paper is to provide a theoretical answer
to this problem. We work with general independence models, which contain probabilistic
independence models as a special case. We exploit a generalization of ordering, called
preordering, of the nodes of (mixed) graphs. This allows us to provide sufficient conditions
for a given independence model to be Markov to a graph with the minimum possible number
of edges, and more importantly, necessary and sufficient conditions for a given probability
distribution to be faithful to a graph. We present our results for the general case of
mixed graphs, but specialize the definitions and results to the better-known subclasses of
undirected (concentration) and bidirected (covariance) graphs as well as directed acyclic
graphs.},
 author = {Kayvan Sadeghi},
 journal = {Journal of Machine Learning Research},
 number = {148},
 openalex = {W2963927326},
 pages = {1--29},
 title = {Faithfulness of Probability Distributions and Graphs},
 url = {http://jmlr.org/papers/v18/17-275.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-284,
 abstract = {We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each nodes of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. A consequence is that it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the third-order integrated cumulants of the process. We show on numerical experiments that our approach is indeed very robust to the shape of the kernels, and gives appealing results on the MemeTracker database.},
 author = {Massil Achab and Emmanuel Bacry and St{{\'e}}phane Ga{{\"i}}ffas and Iacopo Mastromatteo and Jean-Fran{\c{c}}ois Muzy},
 journal = {Journal of Machine Learning Research},
 number = {192},
 openalex = {W2488712513},
 pages = {1--28},
 title = {Uncovering Causality from Multivariate Hawkes Integrated Cumulants},
 url = {http://jmlr.org/papers/v18/17-284.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-297,
 abstract = {The principal submatrix localization problem deals with recovering a $K\times K$ principal submatrix of elevated mean $μ$ in a large $n\times n$ symmetric matrix subject to additive standard Gaussian noise. This problem serves as a prototypical example for community detection, in which the community corresponds to the support of the submatrix. The main result of this paper is that in the regime $Ω(\sqrt{n}) \leq K \leq o(n)$, the support of the submatrix can be weakly recovered (with $o(K)$ misclassification errors on average) by an optimized message passing algorithm if $λ= μ^2K^2/n$, the signal-to-noise ratio, exceeds $1/e$. This extends a result by Deshpande and Montanari previously obtained for $K=Θ(\sqrt{n}).$ In addition, the algorithm can be extended to provide exact recovery whenever information-theoretically possible and achieve the information limit of exact recovery as long as $K \geq \frac{n}{\log n} (\frac{1}{8e} + o(1))$. The total running time of the algorithm is $O(n^2\log n)$. Another version of the submatrix localization problem, known as noisy biclustering, aims to recover a $K_1\times K_2$ submatrix of elevated mean $μ$ in a large $n_1\times n_2$ Gaussian matrix. The optimized message passing algorithm and its analysis are adapted to the bicluster problem assuming $Ω(\sqrt{n_i}) \leq K_i \leq o(n_i)$ and $K_1\asymp K_2.$ A sharp information-theoretic condition for the weak recovery of both clusters is also identified.},
 author = {Bruce Hajek and Yihong Wu and Jiaming Xu},
 journal = {Journal of Machine Learning Research},
 number = {186},
 openalex = {W2963618569},
 pages = {1--52},
 title = {Submatrix localization via message passing},
 url = {http://jmlr.org/papers/v18/17-297.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-306,
 abstract = {We consider the kernel partial least squares algorithm for non-parametric regression with stationary dependent data. Probabilistic convergence rates of the kernel partial least squares estimator to the true regression function are established under a source and an effective dimensionality condition. It is shown both theoretically and in simulations that long range dependence results in slower convergence rates. A protein dynamics example shows high predictive power of kernel partial least squares.},
 author = {Marco Singer and Tatyana Krivobokova and Axel Munk},
 journal = {Journal of Machine Learning Research},
 number = {123},
 openalex = {W2964086354},
 pages = {1--41},
 title = {Kernel Partial Least Squares for Stationary Data},
 url = {http://jmlr.org/papers/v18/17-306.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-313,
 abstract = {We address the statistical and optimization impacts of the classical sketch and Hessian sketch used to approximately solve the Matrix Ridge Regression (MRR) problem. Prior research has quantified the effects of classical sketch on the strictly simpler least squares regression (LSR) problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR: namely, it recovers nearly optimal solutions. By contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the mass in the responses and the optimal objective value.

For both types of approximation, the regularization in the sketched MRR problem results in significantly different statistical properties from those of the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the bias and variance of sketched MRR; these bounds show that classical sketch significantly increases the variance, while Hessian sketch significantly increases the bias. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions.

We establish theoretically and empirically that model averaging greatly decreases the gap between the risks of the true and sketched solutions to the MRR problem. Thus, in parallel or distributed settings, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the increased statistical risk incurred by sketching.},
 author = {Shusen Wang and Alex Gittens and Michael W. Mahoney},
 journal = {Journal of Machine Learning Research},
 number = {218},
 openalex = {W2590556189},
 pages = {1--50},
 title = {Sketched ridge regression: optimization perspective, statistical perspective, and model averaging},
 url = {http://jmlr.org/papers/v18/17-313.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-317,
 abstract = {Topological data analysis is an emerging mathematical concept for characterizing shapes in multi-scale data. In this field, persistence diagrams are widely used as a descriptor of the input data, and can distinguish robust and noisy topological properties. Nowadays, it is highly desired to develop a statistical framework on persistence diagrams to deal with practical data. This paper proposes a kernel method on persistence diagrams. A theoretical contribution of our method is that the proposed kernel allows one to control the effect of persistence, and, if necessary, noisy topological properties can be discounted in data analysis. Furthermore, the method provides a fast approximation technique. The method is applied into several problems including practical data in physics, and the results show the advantage compared to the existing kernel method on persistence diagrams.},
 author = {Genki Kusano and Kenji Fukumizu and Yasuaki Hiraoka},
 journal = {Journal of Machine Learning Research},
 number = {189},
 openalex = {W2963806772},
 pages = {1--41},
 title = {Kernel Method for Persistence Diagrams via Kernel Embedding and Weight Factor},
 url = {http://jmlr.org/papers/v18/17-317.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-343,
 author = {Heng Lian and Zengyan Fan},
 journal = {Journal of Machine Learning Research},
 number = {182},
 pages = {1--26},
 title = {Divide-and-Conquer for Debiased $l_1$-norm Support Vector Machine in Ultra-high Dimensions},
 url = {http://jmlr.org/papers/v18/17-343.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-347,
 author = {Andrei Patrascu and Ion Necoara},
 journal = {Journal of Machine Learning Research},
 number = {198},
 openalex = {W2963747836},
 pages = {1--42},
 title = {Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization},
 url = {http://jmlr.org/papers/v18/17-347.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-364,
 abstract = {Recent work on follow the perturbed leader (FTPL) algorithms for the adversarial multi-armed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations. Assuming that the hazard rate is bounded, it is possible to provide regret analyses for a variety of FTPL algorithms for the multi-armed bandit problem. This paper pushes the inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate condition. There are good reasons to do so: natural distributions such as the uniform and Gaussian violate the condition. We give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition. We also disprove a conjecture that the Gaussian distribution cannot lead to a low-regret algorithm. In fact, it turns out that it leads to near optimal regret, up to logarithmic factors. A key ingredient in our approach is the introduction of a new notion called the generalized hazard rate.},
 author = {Zifan Li and Ambuj Tewari},
 journal = {Journal of Machine Learning Research},
 number = {183},
 openalex = {W2594887820},
 pages = {1--24},
 title = {Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial Multi-armed Bandits},
 url = {http://jmlr.org/papers/v18/17-364.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-375,
 abstract = {Recently, fundamental conditions on the sampling patterns have been obtained for finite completability of low-rank matrices or tensors given the corresponding ranks. In this paper, we consider the scenario where the rank is not given and we aim to approximate the unknown rank based on the location of sampled entries and some given completion. We consider a number of data models, including single-view matrix, multi-view matrix, CP tensor, tensor-train tensor and Tucker tensor. For each of these data models, we provide an upper bound on the rank when an arbitrary low-rank completion is given. We characterize these bounds both deterministically, i.e., with probability one given that the sampling pattern satisfies certain combinatorial properties, and probabilistically, i.e., with high probability given that the sampling probability is above some threshold. Moreover, for both single-view matrix and CP tensor, we are able to show that the obtained upper bound is exactly equal to the unknown rank if the lowest-rank completion is given. Furthermore, we provide numerical experiments for the case of single-view matrix, where we use nuclear norm minimization to find a low-rank completion of the sampled data and we observe that in most of the cases the proposed upper bound on the rank is equal to the true rank.},
 author = {Morteza Ashraphijuo and Xiaodong Wang and Vaneet Aggarwal},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W4294373705},
 pages = {1--29},
 title = {Rank Determination for Low-Rank Data Completion},
 url = {http://jmlr.org/papers/v18/17-375.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-377,
 abstract = {Community detection is the process of grouping strongly connected nodes in a network. Many community detection methods for un-weighted networks have a theoretical basis in a null model. Communities discovered by these methods therefore have interpretations in terms of statistical significance. In this paper, we introduce a null for weighted networks called the continuous configuration model. First, we propose a community extraction algorithm for weighted networks which incorporates iterative hypothesis testing under the null. We prove a central limit theorem for edge-weight sums and asymptotic consistency of the algorithm under a weighted stochastic block model. We then incorporate the algorithm in a community detection method called CCME. To benchmark the method, we provide a simulation framework involving the null to plant "background" nodes in weighted networks with communities. We show that the empirical performance of CCME on these simulations is competitive with existing methods, particularly when overlapping communities and background nodes are present. To further validate the method, we present two real-world networks with potential background nodes and analyze them with CCME, yielding results that reveal macro-features of the corresponding systems.},
 author = {John Palowitch and Shankar Bhamidi and Andrew B. Nobel},
 journal = {Journal of Machine Learning Research},
 number = {188},
 openalex = {W2963925793},
 pages = {1--48},
 title = {Significance-based community detection in weighted networks.},
 url = {http://jmlr.org/papers/v18/17-377.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-380,
 abstract = {Modes and ridges of the probability density function behind observed data are useful geometric features. Mode-seeking clustering assigns cluster labels by associating data samples with the nearest modes, and estimation of density ridges enables us to find lower-dimensional structures hidden in data. A key technical challenge both in mode-seeking clustering and density ridge estimation is accurate estimation of the ratios of the first- and second-order density derivatives to the density. A naive approach takes a three-step approach of first estimating the data density, then computing its derivatives, and finally taking their ratios. However, this three-step approach can be unreliable because a good density estimator does not necessarily mean a good density derivative estimator, and division by the estimated density could significantly magnify the estimation error. To cope with these problems, we propose a novel estimator for the \emph{density-derivative-ratios}. The proposed estimator does not involve density estimation, but rather \emph{directly} approximates the ratios of density derivatives of any order. Moreover, we establish a convergence rate of the proposed estimator. Based on the proposed estimator, novel methods both for mode-seeking clustering and density ridge estimation are developed, and the respective convergence rates to the mode and ridge of the underlying density are also established. Finally, we experimentally demonstrate that the developed methods significantly outperform existing methods, particularly for relatively high-dimensional data.},
 author = {Hiroaki Sasaki and Takafumi Kanamori and Aapo Hyv{{\"a}}rinen and Gang Niu and Masashi Sugiyama},
 journal = {Journal of Machine Learning Research},
 number = {180},
 openalex = {W2964023070},
 pages = {1--47},
 title = {Mode-Seeking Clustering and Density Ridge Estimation via Direct Estimation of Density-Derivative-Ratios},
 url = {http://jmlr.org/papers/v18/17-380.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-381,
 author = {Emmanuel Bacry and Martin Bompaire and Philip Deegan and St{{\'e}}phane Ga{{\"i}}ffas and S{{\o}}ren V. Poulsen},
 journal = {Journal of Machine Learning Research},
 number = {214},
 openalex = {W2808127013},
 pages = {1--5},
 title = {tick: a Python Library for Statistical Learning, with an emphasis on Hawkes Processes and Time-Dependent Models},
 url = {http://jmlr.org/papers/v18/17-381.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-398,
 abstract = {The Information Bottleneck (IB) is a conceptual method for extracting the most compact, yet informative, representation of a set of variables, with respect to the target. It generalizes the notion of minimal sufficient statistics from classical parametric statistics to a broader information-theoretic sense. The IB curve defines the optimal trade-off between representation complexity and its predictive power. Specifically, it is achieved by minimizing the level of mutual information (MI) between the representation and the original variables, subject to a minimal level of MI between the representation and the target. This problem is shown to be in general NP hard. One important exception is the multivariate Gaussian case, for which the Gaussian IB (GIB) is known to obtain an analytical closed form solution, similar to Canonical Correlation Analysis (CCA). In this work we introduce a Gaussian lower bound to the IB curve; we find an embedding of the data which maximizes its "Gaussian part", on which we apply the GIB. This embedding provides an efficient (and practical) representation of any arbitrary data-set (in the IB sense), which in addition holds the favorable properties of a Gaussian distribution. Importantly, we show that the optimal Gaussian embedding is bounded from above by non-linear CCA. This allows a fundamental limit for our ability to Gaussianize arbitrary data-sets and solve complex problems by linear methods.},
 author = {Amichai Painsky and Naftali Tishby},
 journal = {Journal of Machine Learning Research},
 number = {213},
 openalex = {W2767510709},
 pages = {1--29},
 title = {Gaussian Lower Bound for the Information Bottleneck Limit},
 url = {http://jmlr.org/papers/v18/17-398.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-406,
 abstract = {auDeep is a Python toolkit for deep unsupervised representation learning from acoustic data. It is based on a recurrent sequence to sequence autoencoder approach which can learn representations of time series data by taking into account their temporal dynamics. We provide an extensive command line interface in addition to a Python API for users and developers, both of which are comprehensively documented and publicly available at https://github.com/auDeep/auDeep. Experimental results indicate that auDeep features are competitive with state-of-the art audio classification.},
 author = {Michael Freitag and Shahin Amiriparian and Sergey Pugachevskiy and Nicholas Cummins and Bj\"{o}rn Schuller},
 journal = {Journal of Machine Learning Research},
 number = {173},
 openalex = {W2963194800},
 pages = {1--5},
 title = {auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks},
 url = {http://jmlr.org/papers/v18/17-406.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-409,
 abstract = {To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, increasing robustness, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and performance.},
 author = {Quan Zhang and Mingyuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {204},
 openalex = {W2963146695},
 pages = {1--33},
 title = {Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression},
 url = {http://jmlr.org/papers/v18/17-409.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-416,
 abstract = {User engagement in online social networking depends critically on the level of social activity in the corresponding platform--the number of online actions, such as posts, shares or replies, taken by their users. Can we design data-driven algorithms to increase social activity? At a user level, such algorithms may increase activity by helping users decide when to take an action to be more likely to be noticed by their peers. At a network level, they may increase activity by incentivizing a few influential users to take more actions, which in turn will trigger additional actions by other users. In this paper, we model social activity using the framework of marked temporal point processes, derive an alternate representation of these processes using stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, develop two efficient online algorithms with provable guarantees to steer social activity both at a user and at a network level. In doing so, we establish a previously unexplored connection between optimal control of jump SDEs and doubly stochastic marked temporal point processes, which is of independent interest. Finally, we experiment both with synthetic and real data gathered from Twitter and show that our algorithms consistently steer social activity more effectively than the state of the art.},
 author = {Ali Zarezade and Abir De and Utkarsh Upadhyay and Hamid R. Rabiee and Manuel Gomez-Rodriguez},
 journal = {Journal of Machine Learning Research},
 number = {205},
 openalex = {W2963694980},
 pages = {1--35},
 title = {Steering Social Activity: A Stochastic Optimal Control Point Of View},
 url = {http://jmlr.org/papers/v18/17-416.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-423,
 abstract = {Distributed learning is an effective way to analyze big data. In distributed regression, a typical approach is to divide the big data into multiple blocks, apply a base regression algorithm on each of them, and then simply average the output functions learnt from these blocks. Since the average process will decrease the variance, not the bias, bias correction is expected to improve the learning performance if the base regression algorithm is a biased one. Regularization kernel network is an effective and widely used method for nonlinear regression analysis. In this paper we will investigate a bias corrected version of regularization kernel network. We derive the error bounds when it is applied to a single data set and when it is applied as a base algorithm in distributed regression. We show that, under certain appropriate conditions, the optimal learning rates can be reached in both situations.},
 author = {Zheng-Chu Guo and Lei Shi and Qiang Wu},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W2962987628},
 pages = {1--25},
 title = {Learning Theory of Distributed Regression with Bias Corrected Regularization Kernel Network},
 url = {http://jmlr.org/papers/v18/17-423.html},
 volume = {18},
 year = {2017}
}

@article{JMLR:v18:17-434,
 author = {Andrew C. Heusser and Kirsten Ziman and Lucy L. W. Owen and Jeremy R. Manning},
 journal = {Journal of Machine Learning Research},
 number = {152},
 openalex = {W2801900386},
 pages = {1--6},
 title = {HyperTools: a Python Toolbox for Gaining Geometric Insights into High-Dimensional Data},
 url = {http://jmlr.org/papers/v18/17-434.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-445,
 abstract = {We propose a method for estimating coefficients in multivariate regression when there is a clustering structure to the response variables. The proposed method includes a fusion penalty, to shrink the difference in fitted values from responses in the same cluster, and an L1 penalty for simultaneous variable selection and estimation. The method can be used when the grouping structure of the response variables is known or unknown. When the clustering structure is unknown the method will simultaneously estimate the clusters of the response and the regression coefficients. Theoretical results are presented for the penalized least squares case, including asymptotic results allowing for p &gt;&gt; n. We extend our method to the setting where the responses are binomial variables. We propose a coordinate descent algorithm for both the normal and binomial likelihood, which can easily be extended to other generalized linear model (GLM) settings. Simulations and data examples from business operations and genomics are presented to show the merits of both the least squares and binomial methods.},
 author = {Bradley S. Price and Ben Sherwood},
 journal = {Journal of Machine Learning Research},
 number = {232},
 openalex = {W2950266077},
 pages = {1--39},
 title = {A Cluster Elastic Net for Multivariate Regression},
 url = {http://jmlr.org/papers/v18/17-445.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-448,
 author = {Avanti Athreya and Donniell E. Fishkind and Minh Tang and Carey E. Priebe and Youngser Park and Joshua T. Vogelstein and Keith Levin and Vince Lyzinski and Yichen Qin and Daniel L Sussman},
 journal = {Journal of Machine Learning Research},
 number = {226},
 openalex = {W2963512140},
 pages = {1--92},
 title = {Statistical inference on random dot product graphs: a survey},
 url = {http://jmlr.org/papers/v18/17-448.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-457,
 abstract = {In this paper we study the convergence of online gradient descent algorithms in reproducing kernel Hilbert spaces (RKHSs) without regularization. We establish a sufficient condition and a necessary condition for the convergence of excess generalization errors in expectation. A sufficient condition for the almost sure convergence is also given. With high probability, we provide explicit convergence rates of the excess generalization errors for both averaged iterates and the last iterate, which in turn also imply convergence rates with probability one. To our best knowledge, this is the first high-probability convergence rate for the last iterate of online gradient descent algorithms without strong convexity. Without any boundedness assumptions on iterates, our results are derived by a novel use of two measures of the algorithm's one-step progress, respectively by generalization errors and by distances in RKHSs, where the variances of the involved martingales are cancelled out by the descent property of the algorithm.},
 author = {Yunwen Lei and Lei Shi and Zheng-Chu Guo},
 journal = {Journal of Machine Learning Research},
 number = {171},
 openalex = {W2963386061},
 pages = {1--33},
 title = {Convergence of Unregularized Online Learning Algorithms},
 url = {http://jmlr.org/papers/v18/17-457.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-468,
 abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic (AD), also called algorithmic or simply auto-diff, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names dynamic computational graphs and differentiable programming. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main techniques and their interrelationships, we aim to bring clarity to the usage of the terms autodiff, automatic differentiation, and symbolic differentiation as these are encountered more and more in machine learning settings.},
 author = {Atilim Gunes Baydin and Barak A. Pearlmutter and Alexey Andreyevich Radul and Jeffrey Mark Siskind},
 journal = {Journal of Machine Learning Research},
 number = {153},
 openalex = {W2962727772},
 pages = {1--43},
 title = {Automatic differentiation in machine learning: a survey},
 url = {http://jmlr.org/papers/v18/17-468.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-492,
 abstract = {Maximum mean discrepancy (MMD), also called energy distance or N-distance in statistics and Hilbert-Schmidt independence criterion (HSIC), specifically distance covariance in statistics, are among the most popular and successful approaches to quantify the difference and independence of random variables, respectively. Thanks to their kernel-based foundations, MMD and HSIC are applicable on a wide variety of domains. Despite their tremendous success, quite little is known about when HSIC characterizes independence and when MMD with tensor product kernel can discriminate probability distributions. In this paper, we answer these questions by studying various notions of characteristic property of the tensor product kernel.},
 author = {Zolt{{\'a}}n Szab{{\'o}} and Bharath K. Sriperumbudur},
 journal = {Journal of Machine Learning Research},
 number = {233},
 openalex = {W2963026013},
 pages = {1--29},
 title = {Characteristic and Universal Tensor Product Kernels},
 url = {http://jmlr.org/papers/v18/17-492.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-514,
 abstract = {Feature selection technique is a knowledge discovery tool which provides an understanding of the problem through the analysis of the most relevant features. Feature selection aims at building better classifier by listing significant features which also helps in reducing computational overload. Due to existing high throughput technologies and their recent advancements are resulting in high dimensional data due to which feature selection is being treated as handy and mandatory in such datasets. This actually questions the interpretability and stability of traditional feature selection algorithms. The high correlation in features frequently produces multiple equally optimal signatures, which makes traditional feature selection method unstable and thus leading to instability which reduces the confidence of selected features. Stability is the robustness of the feature preferences it produces to perturbation of training samples. Stability indicates the reproducibility power of the feature selection method. High stability of the feature selection algorithm is equally important as the high classification accuracy when evaluating feature selection performance. In this paper, we provide an overview of feature selection techniques and instability of the feature selection algorithm. We also present some of the solutions which can handle the different source of instability.},
 author = {Sarah Nogueira and Konstantinos Sechidis and Gavin Brown},
 journal = {Journal of Machine Learning Research},
 number = {174},
 openalex = {W2955612236},
 pages = {1--54},
 title = {Stability of feature selection algorithm: A review},
 url = {http://jmlr.org/papers/v18/17-514.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-527,
 abstract = {It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality.},
 author = {Alan Morningstar and Roger G. Melko},
 journal = {Journal of Machine Learning Research},
 number = {163},
 openalex = {W2749945115},
 pages = {1--17},
 title = {Deep Learning the Ising Model Near Criticality},
 url = {http://jmlr.org/papers/v18/17-527.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-563,
 abstract = {Causal models communicate our assumptions about causes and effects in real-world phe- nomena. Often the interest lies in the identification of the effect of an action which means deriving an expression from the observed probability distribution for the interventional distribution resulting from the action. In many cases an identifiability algorithm may return a complicated expression that contains variables that are in fact unnecessary. In practice this can lead to additional computational burden and increased bias or inefficiency of estimates when dealing with measurement error or missing data. We present graphical criteria to detect variables which are redundant in identifying causal effects. We also provide an improved version of a well-known identifiability algorithm that implements these criteria.},
 author = {Santtu Tikka and Juha Karvanen},
 journal = {Journal of Machine Learning Research},
 number = {194},
 openalex = {W2809027207},
 pages = {1--23},
 title = {Enhancing Identification of Causal Effects by Pruning},
 url = {http://jmlr.org/papers/v18/17-563.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-632,
 author = {Hiroyuki Kasai},
 journal = {Journal of Machine Learning Research},
 number = {215},
 openalex = {W2808678810},
 pages = {1--5},
 title = {SGDLibrary: A MATLAB library for stochastic optimization algorithms},
 url = {http://jmlr.org/papers/v18/17-632.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-636,
 abstract = {We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with---or outperform---other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code.},
 author = {Jacob Schreiber},
 journal = {Journal of Machine Learning Research},
 number = {164},
 openalex = {W4297806271},
 pages = {1--6},
 title = {Pomegranate: fast and flexible probabilistic modeling in python},
 url = {http://jmlr.org/papers/v18/17-636.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-653,
 abstract = {The continuous dynamical system approach to deep learning is explored in order to devise alternative frameworks for training algorithms. Training is recast as a control problem and this allows us to formulate necessary optimality conditions in continuous time using the Pontryagin's maximum principle (PMP). A modification of the method of successive approximations is then used to solve the PMP, giving rise to an alternative training algorithm for deep learning. This approach has the advantage that rigorous error estimates and convergence results can be established. We also show that it may avoid some pitfalls of gradient-based methods, such as slow convergence on flat landscapes near saddle points. Furthermore, we demonstrate that it obtains favorable initial convergence rate per-iteration, provided Hamiltonian maximization can be efficiently carried out - a step which is still in need of improvement. Overall, the approach opens up new avenues to attack problems associated with deep learning, such as trapping in slow manifolds and inapplicability of gradient-based methods for discrete trainable variables.},
 author = {Qianxiao Li and Long Chen and Cheng Tai and Weinan E},
 journal = {Journal of Machine Learning Research},
 number = {165},
 openalex = {W2963656894},
 pages = {1--29},
 title = {Maximum Principle Based Algorithms for Deep Learning},
 url = {http://jmlr.org/papers/v18/17-653.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-716,
 abstract = {We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm produces rule lists with optimal training performance, according to the regularized empirical risk, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. Our results indicate that it is possible to construct optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary risk prediction tool on data from Broward County, Florida, but that are completely interpretable. This framework is a novel alternative to CART and other decision tree methods for interpretable modeling.},
 author = {Elaine Angelino and Nicholas Larus-Stone and Daniel Alabi and Margo Seltzer and Cynthia Rudin},
 journal = {Journal of Machine Learning Research},
 number = {234},
 openalex = {W2606882704},
 pages = {1--78},
 title = {Learning Certifiably Optimal Rule Lists for Categorical Data},
 url = {http://jmlr.org/papers/v18/17-716.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-748,
 abstract = {We introduce a generic scheme for accelerating gradient-based optimization methods in the sense of Nesterov. The approach, called Catalyst, builds upon the inexact accelerated proximal point algorithm for minimizing a convex objective function, and consists of approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. One of the keys to achieve acceleration in theory and in practice is to solve these sub-problems with appropriate accuracy by using the right stopping criterion and the right warm-start strategy. We give practical guidelines to use Catalyst and present a comprehensive analysis of its global complexity. We show that Catalyst applies to a large class of algorithms, including gradient descent, block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG, MISO/Finito, and their proximal variants. For all of these methods, we establish faster rates using the Catalyst acceleration, for strongly convex and non-strongly convex objectives. We conclude with extensive experiments showing that acceleration is useful in practice, especially for ill-conditioned problems.},
 author = {Hongzhou Lin and Julien Mairal and Zaid Harchaoui},
 journal = {Journal of Machine Learning Research},
 number = {212},
 openalex = {W2963474881},
 pages = {1--54},
 title = {Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice},
 url = {http://jmlr.org/papers/v18/17-748.html},
 volume = {18},
 year = {2018}
}

@article{JMLR:v18:17-755,
 author = {Maik D{{\"o}}ring and L{{\'a}}szl{{\'o}} Gy{{\"o}}rfi and Harro Walk},
 journal = {Journal of Machine Learning Research},
 number = {227},
 openalex = {W2884334374},
 pages = {1--16},
 title = {Rate of Convergence of $k$-Nearest-Neighbor Classification Rule},
 url = {http://jmlr.org/papers/v18/17-755.html},
 volume = {18},
 year = {2018}
}
