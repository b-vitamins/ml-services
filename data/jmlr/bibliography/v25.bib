@article{JMLR:v25:18-566,
 abstract = {While machine learning is traditionally a resource intensive task, embedded systems, autonomous navigation, and the vision of the Internet of Things fuel the interest in resource-efficient approaches. These approaches aim for a carefully chosen trade-off between performance and resource consumption in terms of computation and energy. The development of such approaches is among the major challenges in current machine learning research and key to ensure a smooth transition of machine learning technology from a scientific environment with virtually unlimited computing resources into everyday's applications. In this article, we provide an overview of the current state of the art of machine learning techniques facilitating these real-world requirements. In particular, we focus on resource-efficient inference based on deep neural networks (DNNs), the predominant machine learning models of the past decade. We give a comprehensive overview of the vast literature that can be mainly split into three non-mutually exclusive categories: (i) quantized neural networks, (ii) network pruning, and (iii) structural efficiency. These techniques can be applied during training or as post-processing, and they are widely used to reduce the computational demands in terms of memory footprint, inference speed, and energy efficiency. We also briefly discuss different concepts of embedded hardware for DNNs and their compatibility with machine learning techniques as well as potential for energy and latency reduction. We substantiate our discussion with experiments on well-known benchmark data sets using compression techniques (quantization, pruning) for a set of resource-constrained embedded systems, such as CPUs, GPUs and FPGAs. The obtained results highlight the difficulty of finding good trade-offs between resource efficiency and prediction quality.},
 author = {Wolfgang Roth and G{{\"u}}nther Schindler and Bernhard Klein and Robert Peharz and Sebastian Tschiatschek and Holger Fr{{\"o}}ning and Franz Pernkopf and Zoubin Ghahramani},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W4311415687},
 pages = {1--51},
 title = {Resource-Efficient Neural Networks for Embedded Systems},
 url = {http://jmlr.org/papers/v25/18-566.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:19-301,
 author = {Jonathan K. Su},
 journal = {Journal of Machine Learning Research},
 number = {1},
 pages = {1--91},
 title = {On Truthing Issues in Supervised Classification},
 url = {http://jmlr.org/papers/v25/19-301.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:19-556,
 abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be highly variable when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates, and convergence diagnostics. The presented Pareto $\hat{k}$ finite sample convergence rate diagnostic is useful for any Monte Carlo estimator.},
 author = {Aki Vehtari and Daniel Simpson and Andrew Gelman and Yuling Yao and Jonah Gabry},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W2279857902},
 pages = {1--58},
 title = {Pareto Smoothed Importance Sampling},
 url = {http://jmlr.org/papers/v25/19-556.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:20-075,
 abstract = {Statistical inference based on lossy or incomplete samples is often needed in research areas such as signal/image processing, medical image storage, remote sensing, signal transmission. In this paper, we propose a nonparametric testing procedure based on samples quantized to $B$ bits through a computationally efficient algorithm. Under mild technical conditions, we establish the asymptotic properties of the proposed test statistic and investigate how the testing power changes as $B$ increases. In particular, we show that if $B$ exceeds a certain threshold, the proposed nonparametric testing procedure achieves the classical minimax rate of testing (Shang and Cheng, 2015) for spline models. We further extend our theoretical investigations to a nonparametric linearity test and an adaptive nonparametric test, expanding the applicability of the proposed methods. Extensive simulation studies {together with a real-data analysis} are used to demonstrate the validity and effectiveness of the proposed tests.},
 author = {Kexuan Li and Ruiqi Liu and Ganggang Xu and Zuofeng Shang},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2912740123},
 pages = {1--68},
 title = {Nonparametric Inference under B-bits Quantization},
 url = {http://jmlr.org/papers/v25/20-075.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-0022,
 abstract = {Undirected probabilistic graphical models represent the conditional dependencies, or Markov properties, of a collection of random variables. Knowing the sparsity of such a graphical model is valuable for modeling multivariate distributions and for efficiently performing inference. While the problem of learning graph structure from data has been studied extensively for certain parametric families of distributions, most existing methods fail to consistently recover the graph structure for non-Gaussian data. Here we propose an algorithm for learning the Markov structure of continuous and non-Gaussian distributions. To characterize conditional independence, we introduce a score based on integrated Hessian information from the joint log-density, and we prove that this score upper bounds the conditional mutual information for a general class of distributions. To compute the score, our algorithm SING estimates the density using a deterministic coupling, induced by a triangular transport map, and iteratively exploits sparse structure in the map to reveal sparsity in the graph. For certain non-Gaussian datasets, we show that our algorithm recovers the graph structure even with a biased approximation to the density. Among other examples, we apply SING to learn the dependencies between the states of a chaotic dynamical system with local interactions.},
 author = {Ricardo Baptista and Youssef Marzouk and Rebecca Morrison and Olivier Zahm},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W4322810898},
 pages = {1--46},
 title = {Learning non-Gaussian graphical models via Hessian scores and triangular transport},
 url = {http://jmlr.org/papers/v25/21-0022.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-0076,
 abstract = {Tight and efficient neural network bounding is crucial to the scaling of neural network verification systems. Many efficient bounding algorithms have been presented recently, but they are often too loose to verify more challenging properties. This is due to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise-linear activations exists, it comes at the cost of exponentially many constraints and currently lacks an efficient customized solver. We alleviate this deficiency by presenting two novel dual algorithms: one operates a subgradient method on a small active set of dual variables, the other exploits the sparsity of Frank-Wolfe type optimizers to incur only a linear memory cost. Both methods recover the strengths of the new relaxation: tightness and a linear separation oracle. At the same time, they share the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time. As a consequence, we can obtain better bounds than off-the-shelf solvers in only a fraction of their running time, attaining significant formal verification speed-ups.},
 author = {Alessandro De Palma and Harkirat Singh Behl and Rudy Bunel and Philip H.S. Torr and M. Pawan Kumar},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W3124533198},
 pages = {1--51},
 title = {Scaling the Convex Barrier with Sparse Dual Algorithms},
 url = {http://jmlr.org/papers/v25/21-0076.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-0233,
 author = {Kayhan Behdin and Rahul Mazumder},
 journal = {Journal of Machine Learning Research},
 number = {36},
 pages = {1--62},
 title = {Sparse NMF with Archetypal Regularization: Computational and Robustness Properties},
 url = {http://jmlr.org/papers/v25/21-0233.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-0264,
 abstract = {In this paper, we study the lower complexity bounds for finite-sum optimization problems, where the objective is the average of $n$ individual component functions. We consider Proximal Incremental First-order (PIFO) algorithms which have access to the gradient and proximal oracles for each component function. To incorporate loopless methods, we also allow PIFO algorithms to obtain the full gradient infrequently. We develop a novel approach to constructing the hard instances, which partitions the tridiagonal matrix of classical examples into $n$ groups. This construction is friendly to the analysis of PIFO algorithms. Based on this construction, we establish the lower complexity bounds for finite-sum minimax optimization problems when the objective is convex-concave or nonconvex-strongly-concave and the class of component functions is $L$-average smooth. Most of these bounds are nearly matched by existing upper bounds up to log factors. We can also derive similar lower bounds for finite-sum minimization problems as previous work under both smoothness and average smoothness assumptions. Our lower bounds imply that proximal oracles for smooth functions are not much more powerful than gradient oracles.},
 author = {Yuze Han and Guangzeng Xie and Zhihua Zhang},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W3139467552},
 pages = {1--86},
 title = {Lower Complexity Bounds of Finite-Sum Optimization Problems: The Results and Construction},
 url = {http://jmlr.org/papers/v25/21-0264.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-0316,
 abstract = {We study distributed estimation of a Gaussian mean under communication constraints in a decision theoretical framework. Minimax rates of convergence, which characterize the tradeoff between the communication costs and statistical accuracy, are established in both the univariate and multivariate settings. Communication-efficient and statistically optimal procedures are developed. In the univariate case, the optimal rate depends only on the total communication budget, so long as each local machine has at least one bit. However, in the multivariate case, the minimax rate depends on the specific allocations of the communication budgets among the local machines. Although optimal estimation of a Gaussian mean is relatively simple in the conventional setting, it is quite involved under the communication constraints, both in terms of the optimal procedure design and lower bound argument. The techniques developed in this paper can be of independent interest. An essential step is the decomposition of the minimax estimation problem into two stages, localization and refinement. This critical decomposition provides a framework for both the lower bound analysis and optimal procedure design.},
 author = {T. Tony Cai and Hongji Wei},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W3000924877},
 pages = {1--63},
 title = {Distributed Gaussian Mean Estimation under Communication Constraints: Optimal Rates and Communication-Efficient Algorithms},
 url = {http://jmlr.org/papers/v25/21-0316.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-0748,
 abstract = {We investigate online convex optimization in non-stationary environments and choose dynamic regret as the performance measure, defined as the difference between cumulative loss incurred by the online algorithm and that of any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path length that essentially reflects the non-stationarity of environments, the state-of-the-art dynamic regret is $\mathcal{O}(\sqrt{T(1+P_T)})$. Although this bound is proved to be minimax optimal for convex functions, in this paper, we demonstrate that it is possible to further enhance the guarantee for some easy problem instances, particularly when online functions are smooth. Specifically, we introduce novel online algorithms that can exploit smoothness and replace the dependence on $T$ in dynamic regret with problem-dependent quantities: the variation in gradients of loss functions, the cumulative loss of the comparator sequence, and the minimum of these two terms. These quantities are at most $\mathcal{O}(T)$ while could be much smaller in benign environments. Therefore, our results are adaptive to the intrinsic difficulty of the problem, since the bounds are tighter than existing results for easy problems and meanwhile safeguard the same rate in the worst case. Notably, our proposed algorithms can achieve favorable dynamic regret with only one gradient per iteration, sharing the same gradient query complexity as the static regret minimization methods. To accomplish this, we introduce the collaborative online ensemble framework. The proposed framework employs a two-layer online ensemble to handle non-stationarity, and uses optimistic online learning and further introduces crucial correction terms to enable effective collaboration within the meta-base two layers, thereby attaining adaptivity. We believe the framework can be useful for broader problems.},
 author = {Peng Zhao and Yu-Jie Zhang and Lijun Zhang and Zhi-Hua Zhou},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W4226507192},
 pages = {1--52},
 title = {Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization},
 url = {http://jmlr.org/papers/v25/21-0748.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-0831,
 abstract = {The alternating direction method of multipliers (ADMM) algorithm is a powerful and flexible tool for complex optimization problems of the form $\min\{f(x)+g(y) : Ax+By=c\}$. ADMM exhibits robust empirical performance across a range of challenging settings including nonsmoothness and nonconvexity of the objective functions $f$ and $g$, and provides a simple and natural approach to the inverse problem of image reconstruction for computed tomography (CT) imaging. From the theoretical point of view, existing results for convergence in the nonconvex setting generally assume smoothness in at least one of the component functions in the objective. In this work, our new theoretical results provide convergence guarantees under a restricted strong convexity assumption without requiring smoothness or differentiability, while still allowing differentiable terms to be treated approximately if needed. We validate these theoretical results empirically, with a simulated example where both $f$ and $g$ are nondifferentiable -- and thus outside the scope of existing theory -- as well as a simulated CT image reconstruction problem.},
 author = {Rina Foygel Barber and Emil Y. Sidky},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W4387113815},
 pages = {1--46},
 title = {Convergence for nonconvex ADMM, with applications to CT imaging},
 url = {http://jmlr.org/papers/v25/21-0831.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1125,
 abstract = {We analyse and explain the increased generalisation performance \latestEdits{of} Iterate Averaging using a Gaussian Process perturbation model between the true and batch risk surface on the high dimensional quadratic. % Based on our theoretical results We derive three phenomena \latestEdits{from our theoretical results:} (1) The importance of combining iterate averaging with large learning rates and regularisation for improved regularisation (2) Justification for less frequent averaging. (3) That we expect adaptive gradient methods to work equally well or better with iterate averaging than their non adaptive counterparts. Inspired by these results\latestEdits{, together with} empirical investigations of the importance of appropriate regularisation for the solution diversity of the iterates, we propose two adaptive algorithms with iterate averaging. \latestEdits{These} give significantly better results than SGD, require less tuning and do not require early stopping or validation set monitoring. We showcase the efficacy of our approach on the CIFAR-10/100, ImageNet and Penn Treebank datasets on a variety of modern and classical network architectures.},
 author = {Diego Granziol and Nicholas P. Baskerville and Xingchen Wan and Samuel Albanie and Stephen Roberts},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W3183224492},
 pages = {1--55},
 title = {Iterative Averaging in the Quest for Best Test Error},
 url = {http://jmlr.org/papers/v25/21-1125.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1132,
 abstract = {The increasing availability of massive data sets poses a series of challenges for machine learning. Prominent among these is the need to learn models under hardware or human resource constraints. In such resource-constrained settings, a simple yet powerful approach is to operate on small subsets of the data. Coresets are weighted subsets of the data that provide approximation guarantees for the optimization objective. However, existing coreset constructions are highly model-specific and are limited to simple models such as linear regression, logistic regression, and $k$-means. In this work, we propose a generic coreset construction framework that formulates the coreset selection as a cardinality-constrained bilevel optimization problem. In contrast to existing approaches, our framework does not require model-specific adaptations and applies to any twice differentiable model, including neural networks. We show the effectiveness of our framework for a wide range of models in various settings, including training non-convex models online and batch active learning.},
 author = {Zal{{\'a}}n Borsos and Mojmír Mutný and Marco Tagliasacchi and Andreas Krause},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W3201895909},
 pages = {1--53},
 title = {Data Summarization via Bilevel Optimization},
 url = {http://jmlr.org/papers/v25/21-1132.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1137,
 author = {Zheng Tracy Ke and Jun S. Liu and Yucong Ma},
 journal = {Journal of Machine Learning Research},
 number = {3},
 pages = {1--67},
 title = {Power of knockoff: The impact of ranking algorithm, augmented design, and symmetric statistic},
 url = {http://jmlr.org/papers/v25/21-1137.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1181,
 author = {Wei Luo and Yeying Zhu and Xuekui Zhang and Lin Lin},
 journal = {Journal of Machine Learning Research},
 number = {86},
 pages = {1--38},
 title = {A Semi-parametric Estimation of Personalized Dose-response Function Using Instrumental Variables},
 url = {http://jmlr.org/papers/v25/21-1181.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1190,
 abstract = {Network Lasso (NL for short) is a methodology for estimating models by simultaneously clustering data samples and fitting the models to the samples. It often succeeds in forming clusters thanks to the geometry of the $\ell_1$-regularizer employed therein, but there might be limitations because of the convexity of the regularizer. This paper focuses on the cluster structure that NL yields and reinforces it by developing a non-convex extension, which we call Network Trimmed Lasso (NTL for short). Specifically, we first study a sufficient condition that guarantees the recovery of the latent cluster structure of NL on the basis of the result of Sun et al. (2021) for Convex Clustering, which is a special case of NL for clustering. Second, we extend NL to NTL to incorporate a cardinality (or, $\ell_0$-)constraint and rewrite the constrained optimization problem defined with the $\ell_0$ norm, a discontinuous function, into an equivalent unconstrained continuous optimization problem. We develop ADMM algorithms to solve NTL and provide its convergence results. Numerical illustrations demonstrate that the non-convex extension provides a more clear-cut cluster structure when NL fails to form clusters without incorporating prior knowledge of the associated parameters.},
 author = {Shotaro Yagishita and Jun-ya Gotoh},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W3112201118},
 pages = {1--42},
 title = {Pursuit of the Cluster Structure of Network Lasso: Recovery Condition and Non-convex Extension},
 url = {http://jmlr.org/papers/v25/21-1190.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1205,
 abstract = {This paper investigates the problem of computing the equilibrium of competitive games, which is often modeled as a constrained saddle-point optimization problem with probability simplex constraints. Despite recent efforts in understanding the last-iterate convergence of extragradient methods in the unconstrained setting, the theoretical underpinnings of these methods in the constrained settings, especially those using multiplicative updates, remain highly inadequate, even when the objective function is bilinear. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, we develop provably efficient extragradient methods to find the quantal response equilibrium (QRE) -- which are solutions to zero-sum two-player matrix games with entropy regularization -- at a linear rate. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent's actions directly. In addition, by controlling the knob of entropy regularization, the proposed algorithms can locate an approximate Nash equilibrium of the unregularized matrix game at a sublinear rate without assuming the Nash equilibrium to be unique. Our methods also lead to efficient policy extragradient algorithms for solving (entropy-regularized) zero-sum Markov games at similar rates. All of our convergence rates are nearly dimension-free, which are independent of the size of the state and action spaces up to logarithm factors, highlighting the positive role of entropy regularization for accelerating convergence.},
 author = {Shicong Cen and Yuting Wei and Yuejie Chi},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W3169452765},
 pages = {1--48},
 title = {Fast Policy Extragradient Methods for Competitive Games with Entropy Regularization},
 url = {http://jmlr.org/papers/v25/21-1205.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1256,
 abstract = {In this work, we present a novel algorithm design methodology that finds the optimal algorithm as a function of inequalities. Specifically, we restrict convergence analyses of algorithms to use a prespecified subset of inequalities, rather than utilizing all true inequalities, and find the optimal algorithm subject to this restriction. This methodology allows us to design algorithms with certain desired characteristics. As concrete demonstrations of this methodology, we find new state-of-the-art accelerated first-order gradient methods using randomized coordinate updates and backtracking line searches.},
 author = {Chanwoo Park and Ernest K. Ryu},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W4286897771},
 pages = {1--66},
 title = {Optimal First-Order Algorithms as a Function of Inequalities},
 url = {http://jmlr.org/papers/v25/21-1256.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1343,
 abstract = {Reinforcement learning is a framework for interactive decision-making with incentives sequentially revealed across time without a system dynamics model. Due to its scaling to continuous spaces, we focus on policy search where one iteratively improves a parameterized policy with stochastic policy gradient (PG) updates. In tabular Markov Decision Problems (MDPs), under persistent exploration and suitable parameterization, global optimality may be obtained. By contrast, in continuous space, the non-convexity poses a pathological challenge as evidenced by existing convergence results being mostly limited to stationarity or arbitrary local extrema. To close this gap, we step towards persistent exploration in continuous space through policy parameterizations defined by distributions of heavier tails defined by tail-index parameter alpha, which increases the likelihood of jumping in state space. Doing so invalidates smoothness conditions of the score function common to PG. Thus, we establish how the convergence rate to stationarity depends on the policy's tail index alpha, a Holder continuity parameter, integrability conditions, and an exploration tolerance parameter introduced here for the first time. Further, we characterize the dependence of the set of local maxima on the tail index through an exit and transition time analysis of a suitably defined Markov chain, identifying that policies associated with Levy Processes of a heavier tail converge to wider peaks. This phenomenon yields improved stability to perturbations in supervised learning, which we corroborate also manifests in improved performance of policy search, especially when myopic and farsighted incentives are misaligned.},
 author = {Amrit Singh Bedi and Anjaly Parayil and Junyu Zhang and Mengdi Wang and Alec Koppel},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W4287119221},
 pages = {1--58},
 title = {On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control},
 url = {http://jmlr.org/papers/v25/21-1343.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1405,
 abstract = {Approximate inference methods like the Laplace method, Laplace approximations and variational methods, amongst others, are popular methods when exact inference is not feasible due to the complexity of the model or the abundance of data. In this paper we propose a hybrid approximate method called Low-Rank Variational Bayes correction (VBC), that uses the Laplace method and subsequently a Variational Bayes correction in a lower dimension, to the joint posterior mean. The cost is essentially that of the Laplace method which ensures scalability of the method, in both model complexity and data size. Models with fixed and unknown hyperparameters are considered, for simulated and real examples, for small and large datasets.},
 author = {Janet van Niekerk and Haavard Rue},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W4321153814},
 pages = {1--25},
 title = {Low-rank variational Bayes correction to the Laplace method},
 url = {http://jmlr.org/papers/v25/21-1405.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1504,
 author = {Shanshan Song and Yuanyuan Lin and Yong Zhou},
 journal = {Journal of Machine Learning Research},
 number = {99},
 pages = {1--36},
 title = {Semi-supervised Inference for Block-wise Missing Data without Imputation},
 url = {http://jmlr.org/papers/v25/21-1504.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:21-1536,
 abstract = {Statistical methods for confidential data are in high demand due to an increase in computational power and changes in privacy law. This article introduces differentially private methods for handling model uncertainty in linear regression models. More precisely, we provide differentially private Bayes factors, posterior probabilities, likelihood ratio statistics, information criteria, and model-averaged estimates. Our methods are asymptotically consistent and easy to run with existing implementations of non-private methods.},
 author = {V{{\'i}}ctor Pe{{\~n}}a and Andr{{\'e}}s F. Barrientos},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W3198348214},
 pages = {1--44},
 title = {Differentially private methods for managing model uncertainty in linear regression models},
 url = {http://jmlr.org/papers/v25/21-1536.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0068,
 abstract = {While momentum-based methods, in conjunction with stochastic gradient descent (SGD), are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In this work, we first show that there exists a convex loss function for which algorithmic stability fails to establish generalization guarantees when SGD with standard heavy-ball momentum (SGDM) is run for multiple epochs. Then, for smooth Lipschitz loss functions, we analyze a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM), and show that it admits an upper-bound on the generalization error. Thus, our results show that machine learning models can be trained for multiple epochs of SGDEM with a guarantee for generalization. Finally, for the special case of strongly convex loss functions, we find a range of momentum such that multiple epochs of standard SGDM, as a special form of SGDEM, also generalizes. Extending our results on generalization, we also develop an upper-bound on the expected true risk, in terms of the number of training steps, the size of the training set, and the momentum parameter. Experimental evaluations verify the consistency between the numerical results and our theoretical bounds and the effectiveness of SGDEM for smooth Lipschitz loss functions.},
 author = {Ali Ramezani-Kebrya and Kimon Antonakopoulos and Volkan Cevher and Ashish Khisti and Ben Liang},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W4394640583},
 pages = {1--56},
 title = {On the Generalization of Stochastic Gradient Descent with Momentum},
 url = {http://jmlr.org/papers/v25/22-0068.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0083,
 abstract = {Quantifying spatial and/or temporal associations in multivariate geolocated data of different types is achievable via spatial random effects in a Bayesian hierarchical model, but severe computational bottlenecks arise when spatial dependence is encoded as a latent Gaussian process (GP) in the increasingly common large scale data settings on which we focus. The scenario worsens in non-Gaussian models because the reduced analytical tractability leads to additional hurdles to computational efficiency. In this article, we introduce Bayesian models of spatially referenced data in which the likelihood or the latent process (or both) are not Gaussian. First, we exploit the advantages of spatial processes built via directed acyclic graphs, in which case the spatial nodes enter the Bayesian hierarchy and lead to posterior sampling via routine Markov chain Monte Carlo (MCMC) methods. Second, motivated by the possible inefficiencies of popular gradient-based sampling approaches in the multivariate contexts on which we focus, we introduce the simplified manifold preconditioner adaptation (SiMPA) algorithm which uses second order information about the target but avoids expensive matrix operations. We demostrate the performance and efficiency improvements of our methods relative to alternatives in extensive synthetic and real world remote sensing and community ecology applications with large scale data at up to hundreds of thousands of spatial locations and up to tens of outcomes. Software for the proposed methods is part of R package 'meshed', available on CRAN.},
 author = {Michele Peruzzi and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W4221162756},
 pages = {1--49},
 title = {Spatial meshing for general Bayesian multivariate models},
 url = {http://jmlr.org/papers/v25/22-0083.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0186,
 abstract = {Search algorithms for the bandit problems are applicable in materials discovery. However, the objectives of the conventional bandit problem are different from those of materials discovery. The conventional bandit problem aims to maximize the total rewards, whereas materials discovery aims to achieve breakthroughs in material properties. The max K-armed bandit (MKB) problem, which aims to acquire the single best reward, matches with the discovery tasks better than the conventional bandit. Thus, here, we propose a search algorithm for materials discovery based on the MKB problem using a pseudo-value of the upper confidence bound of expected improvement of the best reward. This approach is pseudo-guaranteed to be asymptotic oracles that do not depends on the time horizon. In addition, compared with other MKB algorithms, the proposed algorithm has only one hyperparameter, which is advantageous in materials discovery. We applied the proposed algorithm to synthetic problems and molecular-design demonstrations using a Monte Carlo tree search. According to the results, the proposed algorithm stably outperformed other bandit algorithms in the late stage of the search process when the optimal arm of the MKB could not be determined based on its expectation reward.},
 author = {Nobuaki Kikkawa and Hiroshi Ohno},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W4311991882},
 pages = {1--40},
 title = {Materials Discovery using Max K-Armed Bandit},
 url = {http://jmlr.org/papers/v25/22-0186.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0285,
 author = {Raghav Singal and George Michailidis},
 journal = {Journal of Machine Learning Research},
 number = {52},
 pages = {1--71},
 title = {Axiomatic effect propagation in structural causal models},
 url = {http://jmlr.org/papers/v25/22-0285.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0402,
 author = {Ernesto Araya and Guillaume Braun and Hemant Tyagi},
 journal = {Journal of Machine Learning Research},
 number = {5},
 pages = {1--43},
 title = {Seeded Graph Matching for the Correlated Gaussian Wigner Model via the Projected Power Method},
 url = {http://jmlr.org/papers/v25/22-0402.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0416,
 abstract = {Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent. To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties. In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches, including natural language explanations. We hope that our work can help catalyze and guide future research on explainable evaluation metrics and, mediately, also contribute to better and more transparent machine translation systems.},
 author = {Christoph Leiter and Piyawat Lertvittayakumjorn and Marina Fomicheva and Wei Zhao and Yang Gao and Steffen Eger},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W4381797953},
 pages = {1--49},
 title = {Towards Explainable Evaluation Metrics for Machine Translation},
 url = {http://jmlr.org/papers/v25/22-0416.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0487,
 abstract = {Ordinary differential equation (ODE) is an important tool to study the dynamics of a system of biological and physical processes. A central question in ODE modeling is to infer the significance of individual regulatory effect of one signal variable on another. However, building confidence band for ODE with unknown regulatory relations is challenging, and it remains largely an open question. In this article, we construct post-regularization confidence band for individual regulatory function in ODE with unknown functionals and noisy data observations. Our proposal is the first of its kind, and is built on two novel ingredients. The first is a new localized kernel learning approach that combines reproducing kernel learning with local Taylor approximation, and the second is a new de-biasing method that tackles infinite-dimensional functionals and additional measurement errors. We show that the constructed confidence band has the desired asymptotic coverage probability, and the recovered regulatory network approaches the truth with probability tending to one. We establish the theoretical properties when the number of variables in the system can be either smaller or larger than the number of sampling time points, and we study the regime-switching phenomenon. We demonstrate the efficacy of the proposed method through both simulations and illustrations with two data applications.},
 author = {Xiaowu Dai and Lexin Li},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W3210191573},
 pages = {1--51},
 title = {Post-Regularization Confidence Bands for Ordinary Differential Equations},
 url = {http://jmlr.org/papers/v25/22-0487.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0488,
 abstract = {We propose a penalized nonparametric approach to estimating the quantile regression process (QRP) in a nonseparable model using rectifier quadratic unit (ReQU) activated deep neural networks and introduce a novel penalty function to enforce non-crossing of quantile regression curves. We establish the non-asymptotic excess risk bounds for the estimated QRP and derive the mean integrated squared error for the estimated QRP under mild smoothness and regularity conditions. To establish these non-asymptotic risk and estimation error bounds, we also develop a new error bound for approximating $C^s$ smooth functions with $s >0$ and their derivatives using ReQU activated neural networks. This is a new approximation result for ReQU networks and is of independent interest and may be useful in other problems. Our numerical experiments demonstrate that the proposed method is competitive with or outperforms two existing methods, including methods using reproducing kernels and random forests, for nonparametric quantile regression.},
 author = {Guohao Shen and Yuling Jiao and Yuanyuan Lin and Joel L. Horowitz and Jian Huang},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W4286750555},
 pages = {1--75},
 title = {Estimation of Non-Crossing Quantile Regression Process with Deep ReQU Neural Networks},
 url = {http://jmlr.org/papers/v25/22-0488.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0493,
 abstract = {Comparing different AutoML frameworks is notoriously challenging and often done incorrectly. We introduce an open and extensible benchmark that follows best practices and avoids common mistakes when comparing AutoML frameworks. We conduct a thorough comparison of 9 well-known AutoML frameworks across 71 classification and 33 regression tasks. The differences between the AutoML frameworks are explored with a multi-faceted analysis, evaluating model accuracy, its trade-offs with inference time, and framework failures. We also use Bradley-Terry trees to discover subsets of tasks where the relative AutoML framework rankings differ. The benchmark comes with an open-source tool that integrates with many AutoML frameworks and automates the empirical evaluation process end-to-end: from framework installation and resource allocation to in-depth evaluation. The benchmark uses public data sets, can be easily extended with other AutoML frameworks and tasks, and has a website with up-to-date results.},
 author = {Pieter Gijsbers and Marcos L. P. Bueno and Stefan Coors and Erin LeDell and S{{\'e}}bastien Poirier and Janek Thomas and Bernd Bischl and Joaquin Vanschoren},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W4288096884},
 pages = {1--65},
 title = {AMLB: an AutoML Benchmark},
 url = {http://jmlr.org/papers/v25/22-0493.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0667,
 abstract = {Kernel survival analysis models estimate individual survival distributions with the help of a kernel function, which measures the similarity between any two data points. Such a kernel function can be learned using deep kernel survival models. In this paper, we present a new deep kernel survival model called a survival kernet, which scales to large datasets in a manner that is amenable to model interpretation and also theoretical analysis. Specifically, the training data are partitioned into clusters based on a recently developed training set compression scheme for classification and regression called kernel netting that we extend to the survival analysis setting. At test time, each data point is represented as a weighted combination of these clusters, and each such cluster can be visualized. For a special case of survival kernets, we establish a finite-sample error bound on predicted survival distributions that is, up to a log factor, optimal. Whereas scalability at test time is achieved using the aforementioned kernel netting compression strategy, scalability during training is achieved by a warm-start procedure based on tree ensembles such as XGBoost and a heuristic approach to accelerating neural architecture search. On four standard survival analysis datasets of varying sizes (up to roughly 3 million data points), we show that survival kernets are highly competitive compared to various baselines tested in terms of time-dependent concordance index. Our code is available at: https://github.com/georgehc/survival-kernets},
 author = {George H. Chen},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W4283317790},
 pages = {1--78},
 title = {Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee},
 url = {http://jmlr.org/papers/v25/22-0667.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0670,
 author = {Jiyuan Tu and Weidong Liu and Xiaojun Mao},
 journal = {Journal of Machine Learning Research},
 number = {76},
 pages = {1--41},
 title = {Distributed Estimation on Semi-Supervised Generalized Linear Model},
 url = {http://jmlr.org/papers/v25/22-0670.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0673,
 abstract = {Random forests are a popular class of algorithms used for regression and classification. The algorithm introduced by Breiman in 2001 and many of its variants are ensembles of randomized decision trees built from axis-aligned partitions of the feature space. One such variant, called Mondrian forests, was proposed to handle the online setting and is the first class of random forests for which minimax rates were obtained in arbitrary dimension. However, the restriction to axis-aligned splits fails to capture dependencies between features, and random forests that use oblique splits have shown improved empirical performance for many tasks. In this work, we show that a large class of random forests with general split directions also achieve minimax optimal convergence rates in arbitrary dimension. This class includes STIT forests, a generalization of Mondrian forests to arbitrary split directions, as well as random forests derived from Poisson hyperplane tessellations. These are the first results showing that random forest variants with oblique splits can obtain minimax optimality in arbitrary dimension. Our proof technique relies on the novel application of the theory of stationary random tessellations in stochastic geometry to statistical learning theory.},
 author = {Eliza O'Reilly and Ngoc Mai Tran},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W4286967527},
 pages = {1--32},
 title = {Minimax Rates for High-Dimensional Random Tessellation Forests},
 url = {http://jmlr.org/papers/v25/22-0673.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0687,
 abstract = {The low rank MDP has emerged as an important model for studying representation learning and exploration in reinforcement learning. With a known representation, several model-free exploration strategies exist. In contrast, all algorithms for the unknown representation setting are model-based, thereby requiring the ability to model the full dynamics. In this work, we present the first model-free representation learning algorithms for low rank MDPs. The key algorithmic contribution is a new minimax representation learning objective, for which we provide variants with differing tradeoffs in their statistical and computational properties. We interleave this representation learning step with an exploration strategy to cover the state space in a reward-free manner. The resulting algorithms are provably sample efficient and can accommodate general function approximation to scale to complex environments.},
 author = {Aditya Modi and Jinglin Chen and Akshay Krishnamurthy and Nan Jiang and Alekh Agarwal},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W3129266958},
 pages = {1--76},
 title = {Model-free Representation Learning and Exploration in Low-rank MDPs.},
 url = {http://jmlr.org/papers/v25/22-0687.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0719,
 abstract = {Learning operators between infinitely dimensional spaces is an important learning task arising in wide applications in machine learning, imaging science, mathematical modeling and simulations, etc. This paper studies the nonparametric estimation of Lipschitz operators using deep neural networks. Non-asymptotic upper bounds are derived for the generalization error of the empirical risk minimizer over a properly chosen network class. Under the assumption that the target operator exhibits a low dimensional structure, our error bounds decay as the training sample size increases, with an attractive fast rate depending on the intrinsic dimension in our estimation. Our assumptions cover most scenarios in real applications and our results give rise to fast rates by exploiting low dimensional structures of data in operator estimation. We also investigate the influence of network structures (e.g., network width, depth, and sparsity) on the generalization error of the neural network estimator and propose a general suggestion on the choice of network structures to maximize the learning efficiency quantitatively.},
 author = {Hao Liu and Haizhao Yang and Minshuo Chen and Tuo Zhao and Wenjing Liao},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W4226370728},
 pages = {1--67},
 title = {Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces},
 url = {http://jmlr.org/papers/v25/22-0719.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0735,
 abstract = {Over the past two decades, we have seen an exponentially increased amount of point clouds collected with irregular shapes in various areas. Motivated by the importance of solid modeling for point clouds, we develop a novel and efficient smoothing tool based on multivariate splines over the tetrahedral partitions to extract the underlying signal and build up a 3D solid model from the point cloud. The proposed smoothing method can denoise or deblur the point cloud effectively and provide a multi-resolution reconstruction of the actual signal. In addition, it can handle sparse and irregularly distributed point clouds and recover the underlying trajectory. The proposed smoothing and interpolation method also provides a natural way of numerosity data reduction. Furthermore, we establish the theoretical guarantees of the proposed method. Specifically, we derive the convergence rate and asymptotic normality of the proposed estimator and illustrate that the convergence rate achieves the optimal nonparametric convergence rate. Through extensive simulation studies and a real data example, we demonstrate the superiority of the proposed method over traditional smoothing methods in terms of estimation accuracy and efficiency of data reduction.},
 author = {Xinyi Li and Shan Yu and Yueying Wang and Guannan Wang and Li Wang and Ming-Jun Lai},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W4287123163},
 pages = {1--56},
 title = {Nonparametric Regression for 3D Point Cloud Learning},
 url = {http://jmlr.org/papers/v25/22-0735.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0743,
 abstract = {We formalize and study the natural approach of designing convex surrogate loss functions via embeddings, for problems such as classification, ranking, or structured prediction. In this approach, one embeds each of the finitely many predictions (e.g. rankings) as a point in $R^d$, assigns the original loss values to these points, and "convexifies" the loss in some way to obtain a surrogate. We establish a strong connection between this approach and polyhedral (piecewise-linear convex) surrogate losses: every discrete loss is embedded by some polyhedral loss, and every polyhedral loss embeds some discrete loss. Moreover, an embedding gives rise to a consistent link function as well as linear surrogate regret bounds. Our results are constructive, as we illustrate with several examples. In particular, our framework gives succinct proofs of consistency or inconsistency for various polyhedral surrogates in the literature, and for inconsistent surrogates, it further reveals the discrete losses for which these surrogates are consistent. We go on to show additional structure of embeddings, such as the equivalence of embedding and matching Bayes risks, and the equivalence of various notions of non-redudancy. Using these results, we establish that indirect elicitation, a necessary condition for consistency, is also sufficient when working with polyhedral surrogates.},
 author = {Jessie Finocchiaro and Rafael M. Frongillo and Bo Waggoner},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W4283757707},
 pages = {1--60},
 title = {An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates},
 url = {http://jmlr.org/papers/v25/22-0743.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0796,
 abstract = {We present a new class of Langevin based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler's polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in neural networks. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms.},
 author = {Dong-Young Lim and Sotirios Sabanis},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W4287165198},
 pages = {1--52},
 title = {Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks},
 url = {http://jmlr.org/papers/v25/22-0796.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0801,
 abstract = {Because of the widespread use of black box prediction methods such as random forests and neural nets, there is renewed interest in developing methods for quantifying variable importance as part of the broader goal of interpretable prediction. A popular approach is to define a variable importance parameter - known as LOCO (Leave Out COvariates) - based on dropping covariates from a regression model. This is essentially a nonparametric version of R-squared. This parameter is very general and can be estimated nonparametrically, but it can be hard to interpret because it is affected by correlation between covariates. We propose a method for mitigating the effect of correlation by defining a modified version of LOCO. This new parameter is difficult to estimate nonparametrically, but we show how to estimate it using semiparametric models.},
 author = {Isabella Verdinelli and Larry Wasserman},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W4307435429},
 pages = {1--27},
 title = {Decorrelated Variable Importance},
 url = {http://jmlr.org/papers/v25/22-0801.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0810,
 abstract = {In this paper, we tackle a significant challenge in PCA: heterogeneity. When data are collected from different sources with heterogeneous trends while still sharing some congruency, it is critical to extract shared knowledge while retaining the unique features of each source. To this end, we propose personalized PCA (PerPCA), which uses mutually orthogonal global and local principal components to encode both unique and shared features. We show that, under mild conditions, both unique and shared features can be identified and recovered by a constrained optimization problem, even if the covariance matrices are immensely different. Also, we design a fully federated algorithm inspired by distributed Stiefel gradient descent to solve the problem. The algorithm introduces a new group of operations called generalized retractions to handle orthogonality constraints, and only requires global PCs to be shared across sources. We prove the linear convergence of the algorithm under suitable assumptions. Comprehensive numerical experiments highlight PerPCA's superior performance in feature extraction and prediction from heterogeneous datasets. As a systematic approach to decouple shared and unique features from heterogeneous datasets, PerPCA finds applications in several tasks, including video segmentation, topic extraction, and feature clustering.},
 author = {Naichen Shi and Raed Al Kontar},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W4286233820},
 pages = {1--82},
 title = {Personalized PCA: Decoupling Shared and Unique Features},
 url = {http://jmlr.org/papers/v25/22-0810.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0816,
 abstract = {We introduce robust principal component analysis from a data matrix in which the entries of its columns have been corrupted by permutations, termed Unlabeled Principal Component Analysis (UPCA). Using algebraic geometry, we establish that UPCA is a well-defined algebraic problem in the sense that the only matrices of minimal rank that agree with the given data are row-permutations of the ground-truth matrix, arising as the unique solutions of a polynomial system of equations. Further, we propose an efficient two-stage algorithmic pipeline for UPCA suitable for the practically relevant case where only a fraction of the data have been permuted. Stage-I employs outlier-robust PCA methods to estimate the ground-truth column-space. Equipped with the column-space, Stage-II applies recent methods for unlabeled sensing to restore the permuted data. Allowing for missing entries on top of permutations in UPCA leads to the problem of unlabeled matrix completion, for which we derive theory and algorithms of similar flavor. Experiments on synthetic data, face images, educational and medical records reveal the potential of our algorithms for applications such as data privatization and record linkage.},
 author = {Yunzhen Yao and Liangzu Peng and Manolis C. Tsakiris},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W4387558946},
 pages = {1--38},
 title = {Unlabeled Principal Component Analysis and Matrix Completion},
 url = {http://jmlr.org/papers/v25/22-0816.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0832,
 abstract = {We analyze a stochastic approximation algorithm for decision-dependent problems, wherein the data distribution used by the algorithm evolves along the iterate sequence. The primary examples of such problems appear in performative prediction and its multiplayer extensions. We show that under mild assumptions, the deviation between the average iterate of the algorithm and the solution is asymptotically normal, with a covariance that clearly decouples the effects of the gradient noise and the distributional shift. Moreover, building on the work of H\'ajek and Le Cam, we show that the asymptotic performance of the algorithm with averaging is locally minimax optimal.},
 author = {Joshua Cutler and Mateo D{{\'i}}az and Dmitriy Drusvyatskiy},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W4285070064},
 pages = {1--49},
 title = {Stochastic Approximation with Decision-Dependent Distributions: Asymptotic Normality and Optimality},
 url = {http://jmlr.org/papers/v25/22-0832.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0846,
 abstract = {The study of loss function distributions is critical to characterize a model's behaviour on a given machine learning problem. For example, while the quality of a model is commonly determined by the average loss assessed on a testing set, this quantity does not reflect the existence of the true mean of the loss distribution. Indeed, the finiteness of the statistical moments of the loss distribution is related to the thickness of its tails, which are generally unknown. Since typical cross-validation schemes determine a family of testing loss distributions conditioned on the training samples, the total loss distribution must be recovered by marginalizing over the space of training sets. As we show in this work, the finiteness of the sampling procedure negatively affects the reliability and efficiency of classical tail estimation methods from the Extreme Value Theory, such as the Peaks-Over-Threshold approach. In this work we tackle this issue by developing a novel general theory for estimating the tails of marginal distributions, when there exists a large variability between locations of the individual conditional distributions underlying the marginal. To this end, we demonstrate that under some regularity conditions, the shape parameter of the marginal distribution is the maximum tail shape parameter of the family of conditional distributions. We term this estimation approach as Cross Tail Estimation (CTE). We test cross-tail estimation in a series of experiments on simulated and real data, showing the improved robustness and quality of tail estimation as compared to classical approaches, and providing evidence for the relationship between overfitting and loss distribution tail thickness.},
 author = {Etrit Haxholli and Marco Lorenzi},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W4315606860},
 pages = {1--47},
 title = {On Tail Decay Rate Estimation of Loss Function Distributions},
 url = {http://jmlr.org/papers/v25/22-0846.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0891,
 abstract = {Invariant and equivariant networks are useful in learning data with symmetry, including images, sets, point clouds, and graphs. In this paper, we consider invariant and equivariant networks for symmetries of finite groups. Invariant and equivariant networks have been constructed by various researchers using Reynolds operators. However, Reynolds operators are computationally expensive when the order of the group is large because they use the sum over the whole group, which poses an implementation difficulty. To overcome this difficulty, we consider representing the Reynolds operator as a sum over a subset instead of a sum over the whole group. We call such a subset a Reynolds design, and an operator defined by a sum over a Reynolds design a reductive Reynolds operator. For example, in the case of a graph with $n$ nodes, the computational complexity of the reductive Reynolds operator is reduced to $O(n^2)$, while the computational complexity of the Reynolds operator is $O(n!)$. We construct learning models based on the reductive Reynolds operator called equivariant and invariant Reynolds networks (ReyNets) and prove that they have universal approximation property. Reynolds designs for equivariant ReyNets are derived from combinatorial observations with Young diagrams, while Reynolds designs for invariant ReyNets are derived from invariants called Reynolds dimensions defined on the set of invariant polynomials. Numerical experiments show that the performance of our models is comparable to state-of-the-art methods.},
 author = {Akiyoshi Sannai and Makoto Kawano and Wataru Kumagai},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W3205662036},
 pages = {1--36},
 title = {Equivariant and Invariant Reynolds Networks},
 url = {http://jmlr.org/papers/v25/22-0891.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-0988,
 abstract = {We introduce two new classes of measures of information for statistical experiments which generalise and subsume $\phi$-divergences, integral probability metrics, $\mathfrak{N}$-distances (MMD), and $(f,\Gamma)$ divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem, thus extending the variational $\phi$-divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical data processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization.},
 author = {Robert C. Williamson and Zac Cranko},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W4288054881},
 pages = {1--53},
 title = {Information Processing Equalities and the Information-Risk Bridge},
 url = {http://jmlr.org/papers/v25/22-0988.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1038,
 author = {Kuang-Yao Lee and Lexin Li and Bing Li},
 journal = {Journal of Machine Learning Research},
 number = {78},
 pages = {1--48},
 title = {Functional Directed Acyclic Graphs},
 url = {http://jmlr.org/papers/v25/22-1038.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1112,
 author = {Wasim Huleihel and Yehonathan Refael},
 journal = {Journal of Machine Learning Research},
 number = {64},
 pages = {1--40},
 title = {Mathematical Framework for Online Social Media Auditing},
 url = {http://jmlr.org/papers/v25/22-1112.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1120,
 abstract = {In this paper we study the computation of the nonparametric maximum likelihood estimator (NPMLE) in multivariate mixture models. Our first approach discretizes this infinite dimensional convex optimization problem by fixing the support points of the NPMLE and optimizing over the mixture proportions. In this context we propose, leveraging the sparsity of the solution, an efficient and scalable semismooth Newton based augmented Lagrangian method (ALM). Our algorithm beats the state-of-the-art methods~\cite{koenker2017rebayes, kim2020fast} and can handle $n \approx 10^6$ data points with $m \approx 10^4$ support points. Our second procedure, which combines the expectation-maximization (EM) algorithm with the ALM approach above, allows for joint optimization of both the support points and the probability weights. For both our algorithms we provide formal results on their (superlinear) convergence properties. The computed NPMLE can be immediately used for denoising the observations in the framework of empirical Bayes. We propose new denoising estimands in this context along with their consistent estimates. Extensive numerical experiments are conducted to illustrate the effectiveness of our methods. In particular, we employ our procedures to analyze two astronomy data sets: (i) Gaia-TGAS Catalog~\cite{anderson2018improving} containing $n \approx 1.4 \times 10^6$ data points in two dimensions, and (ii) the $d=19$ dimensional data set from the APOGEE survey~\cite{majewski2017apache} with $n \approx 2.7 \times 10^4$.},
 author = {Yangjing Zhang and Ying Cui and Bodhisattva Sen and Kim-Chuan Toh},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W4292227181},
 pages = {1--46},
 title = {On Efficient and Scalable Computation of the Nonparametric Maximum Likelihood Estimator in Mixture Models},
 url = {http://jmlr.org/papers/v25/22-1120.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1170,
 abstract = {Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of the cover tree data structure, which is of independent interest. We additionally propose an alternative sparse approximation for regression with a Gaussian likelihood which trades off a small amount of performance to further improve stability. We provide illustrative examples showing the relationship between stability of calculations and predictive performance of inducing point methods on spatial tasks.},
 author = {Alexander Terenin and David R. Burt and Artem Artemev and Seth Flaxman and Mark van der Wilk and Carl Edward Rasmussen and Hong Ge},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W4306706988},
 pages = {1--36},
 title = {Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees},
 url = {http://jmlr.org/papers/v25/22-1170.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1197,
 abstract = {Machine learning systems are often applied to data that is drawn from a different distribution than the training distribution. Recent work has shown that for a variety of classification and signal reconstruction problems, the out-of-distribution performance is strongly linearly correlated with the in-distribution performance. If this relationship or more generally a monotonic one holds, it has important consequences. For example, it allows to optimize performance on one distribution as a proxy for performance on the other. In this paper, we study conditions under which a monotonic relationship between the performances of a model on two distributions is expected. We prove an exact asymptotic linear relation for squared error and a monotonic relation for misclassification error for ridge-regularized general linear models under covariate shift, as well as an approximate linear relation for linear inverse problems.},
 author = {Daniel LeJeune and Jiayu Liu and Reinhard Heckel},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W4320343183},
 pages = {1--37},
 title = {Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization},
 url = {http://jmlr.org/papers/v25/22-1197.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1198,
 abstract = {We analyze the complexity of sampling from a class of heavy-tailed distributions by discretizing a natural class of It\^o diffusions associated with weighted Poincar\'e inequalities. Based on a mean-square analysis, we establish the iteration complexity for obtaining a sample whose distribution is $\epsilon$ close to the target distribution in the Wasserstein-2 metric. In this paper, our results take the mean-square analysis to its limits, i.e., we invariably only require that the target density has finite variance, the minimal requirement for a mean-square analysis. To obtain explicit estimates, we compute upper bounds on certain moments associated with heavy-tailed targets under various assumptions. We also provide similar iteration complexity results for the case where only function evaluations of the unnormalized target density are available by estimating the gradients using a Gaussian smoothing technique. We provide illustrative examples based on the multivariate $t$-distribution.},
 author = {Ye He and Tyler Farghly and Krishnakumar Balasubramanian and Murat A. Erdogdu},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W4323066295},
 pages = {1--44},
 title = {Mean-Square Analysis of Discretized Itô Diffusions for Heavy-tailed Sampling},
 url = {http://jmlr.org/papers/v25/22-1198.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1251,
 abstract = {Recommendation algorithms have become increasingly important in many online platforms such as online education, TikTok, YouTube Shorts, advertising platforms, etc. Multiarmed bandit (MAB) [2] is a classic problem which can model these recommendation systems. Each arm in MAB corresponds to a specific type of item in the recommendation system. The recommendation of an item of the <tex xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">$i\text{th}$</tex> type is regarded as a pull of arm <tex xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">$a_{i}$</tex> . Taking recommending short videos as an example, each arm <tex xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">$a_{i}$</tex> represents a class of similar videos (e.g. videos from the same dancer). For simplicity, we assume the reward is 1 if the user likes the recommended item and is 0 otherwise. In a traditional MAB problem, the learner can continue to play the arms with the goal of maximizing the average reward, which either assumes a single user stays in the system for a long period of time or assumes the learner is recommending a single item to each user with a large number of users. While this traditional MAB formulation models recommendation systems such as online advertising well, there are new recommendation systems that are significantly different from these traditional models. In these new recommendation systems, such as TikTok or ALEKS, the learner continuously recommends videos/contents to a user, and the user, other than like or dislike the item, may abandon the system if the recommended items cannot engage the user, and come back later. For example, a user watches TikTok or YouTube Shorts for some period of time, where the duration depends on how interesting/engaging the videos are, then leaves the systems, and comes back later.},
 author = {Zixian Yang and Xin Liu and Lei Ying},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W4281753576},
 pages = {1--55},
 title = {Exploration. Exploitation, and Engagement in Multi-Armed Bandits with Abandonment},
 url = {http://jmlr.org/papers/v25/22-1251.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1296,
 abstract = {In this note, we study how neural networks with a single hidden layer and ReLU activation interpolate data drawn from a radially symmetric distribution with target labels 1 at the origin and 0 outside the unit ball, if no labels are known inside the unit ball. With weight decay regularization and in the infinite neuron, infinite data limit, we prove that a unique radially symmetric minimizer exists, whose weight decay regularizer and Lipschitz constant grow as $d$ and $\sqrt{d}$ respectively. We furthermore show that the weight decay regularizer grows exponentially in $d$ if the label $1$ is imposed on a ball of radius $\varepsilon$ rather than just at the origin. By comparison, a neural networks with two hidden layers can approximate the target function without encountering the curse of dimensionality.},
 author = {Stephan Wojtowytsch},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W4294783007},
 pages = {1--49},
 title = {Optimal bump functions for shallow ReLU networks: Weight decay, depth separation and the curse of dimensionality},
 url = {http://jmlr.org/papers/v25/22-1296.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1312,
 abstract = {Data augmentation (DA) is a powerful workhorse for bolstering performance in modern machine learning. Specific augmentations like translations and scaling in computer vision are traditionally believed to improve generalization by generating new (artificial) data from the same distribution. However, this traditional viewpoint does not explain the success of prevalent augmentations in modern machine learning (e.g. randomized masking, cutout, mixup), that greatly alter the training data distribution. In this work, we develop a new theoretical framework to characterize the impact of a general class of DA on underparameterized and overparameterized linear model generalization. Our framework reveals that DA induces implicit spectral regularization through a combination of two distinct effects: a) manipulating the relative proportion of eigenvalues of the data covariance matrix in a training-data-dependent manner, and b) uniformly boosting the entire spectrum of the data covariance matrix through ridge regression. These effects, when applied to popular augmentations, give rise to a wide variety of phenomena, including discrepancies in generalization between over-parameterized and under-parameterized regimes and differences between regression and classification tasks. Our framework highlights the nuanced and sometimes surprising impacts of DA on generalization, and serves as a testbed for novel augmentation design.},
 author = {Chi-Heng Lin and Chiraag Kaushik and Eva L. Dyer and Vidya Muthukumar},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W4305007587},
 pages = {1--85},
 title = {The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective},
 url = {http://jmlr.org/papers/v25/22-1312.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1317,
 abstract = {Reciprocity, or the tendency of individuals to mirror behavior, is a key measure that describes information exchange in a social network. Users in social networks tend to engage in different levels of reciprocal behavior. Differences in such behavior may indicate the existence of communities that reciprocate links at varying rates. In this paper, we develop methodology to model the diverse reciprocal behavior in growing social networks. In particular, we present a preferential attachment model with heterogeneous reciprocity that imitates the attraction users have for popular users, plus the heterogeneous nature by which they reciprocate links. We compare Bayesian and frequentist model fitting techniques for large networks, as well as computationally efficient variational alternatives. Cases where the number of communities are known and unknown are both considered. We apply the presented methods to the analysis of a Facebook wallpost network where users have non-uniform reciprocal behavior patterns. The fitted model captures the heavy-tailed nature of the empirical degree distributions in the Facebook data and identifies multiple groups of users that differ in their tendency to reply to and receive responses to wallposts.},
 author = {Daniel Cirkovic and Tiandong Wang},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W4386081073},
 pages = {1--40},
 title = {Modeling Random Networks with Heterogeneous Reciprocity},
 url = {http://jmlr.org/papers/v25/22-1317.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1347,
 abstract = {Lipschitz-constrained neural networks have several advantages over unconstrained ones and can be applied to a variety of problems, making them a topic of attention in the deep learning community. Unfortunately, it has been shown both theoretically and empirically that they perform poorly when equipped with ReLU activation functions. By contrast, neural networks with learnable 1-Lipschitz linear splines are known to be more expressive. In this paper, we show that such networks correspond to global optima of a constrained functional optimization problem that consists of the training of a neural network composed of 1-Lipschitz linear layers and 1-Lipschitz freeform activation functions with second-order total-variation regularization. Further, we propose an efficient method to train these neural networks. Our numerical experiments show that our trained networks compare favorably with existing 1-Lipschitz neural architectures.},
 author = {Stanislas Ducotterd and Alexis Goujon and Pakshal Bohra and Dimitris Perdios and Sebastian Neumayer and Michael Unser},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W4307786837},
 pages = {1--30},
 title = {Improving Lipschitz-Constrained Neural Networks by Learning Activation Functions},
 url = {http://jmlr.org/papers/v25/22-1347.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1389,
 abstract = {Recent works have demonstrated a double descent phenomenon in over-parameterized learning. Although this phenomenon has been investigated by recent works, it has not been fully understood in theory. In this paper, we investigate the multiple descent phenomenon in a class of multi-component prediction models. We first consider a ''double random feature model'' (DRFM) concatenating two types of random features, and study the excess risk achieved by the DRFM in ridge regression. We calculate the precise limit of the excess risk under the high dimensional framework where the training sample size, the dimension of data, and the dimension of random features tend to infinity proportionally. Based on the calculation, we further theoretically demonstrate that the risk curves of DRFMs can exhibit triple descent. We then provide a thorough experimental study to verify our theory. At last, we extend our study to the ''multiple random feature model'' (MRFM), and show that MRFMs ensembling $K$ types of random features may exhibit $(K+1)$-fold descent. Our analysis points out that risk curves with a specific number of descent generally exist in learning multi-component prediction models.},
 author = {Xuran Meng and Jianfeng Yao and Yuan Cao},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W4292955350},
 pages = {1--49},
 title = {Multiple Descent in the Multiple Random Feature Model},
 url = {http://jmlr.org/papers/v25/22-1389.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1392,
 author = {Mathis Chagneux and Elisabeth Gassiat and Pierre Gloaguen and Sylvain Le Corff},
 journal = {Journal of Machine Learning Research},
 number = {28},
 pages = {1--33},
 title = {Additive smoothing error in backward variational inference for general state-space models},
 url = {http://jmlr.org/papers/v25/22-1392.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:22-1396,
 abstract = {We propose a new method for estimating the minimizer $\boldsymbol{x}^*$ and the minimum value $f^*$ of a smooth and strongly convex regression function $f$ from the observations contaminated by random noise. Our estimator $\boldsymbol{z}_n$ of the minimizer $\boldsymbol{x}^*$ is based on a version of the projected gradient descent with the gradient estimated by a regularized local polynomial algorithm. Next, we propose a two-stage procedure for estimation of the minimum value $f^*$ of regression function $f$. At the first stage, we construct an accurate enough estimator of $\boldsymbol{x}^*$, which can be, for example, $\boldsymbol{z}_n$. At the second stage, we estimate the function value at the point obtained in the first stage using a rate optimal nonparametric procedure. We derive non-asymptotic upper bounds for the quadratic risk and optimization error of $\boldsymbol{z}_n$, and for the risk of estimating $f^*$. We establish minimax lower bounds showing that, under certain choice of parameters, the proposed algorithms achieve the minimax optimal rates of convergence on the class of smooth and strongly convex functions.},
 author = {Arya Akhavan and Davit Gogolashvili and Alexandre B. Tsybakov},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W4310515070},
 pages = {1--37},
 title = {Estimating the minimizer and the minimum value of a regression function under passive design},
 url = {http://jmlr.org/papers/v25/22-1396.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0038,
 abstract = {Probabilistic forecasting consists of stating a probability distribution for a future outcome based on past observations. In meteorology, ensembles of physics-based numerical models are run to get such distribution. Usually, performance is evaluated with scoring rules, functions of the forecast distribution and the observed outcome. With some scoring rules, calibration and sharpness of the forecast can be assessed at the same time. In deep learning, generative neural networks parametrize distributions on high-dimensional spaces and easily allow sampling by transforming draws from a latent variable. Conditional generative networks additionally constrain the distribution on an input variable. In this manuscript, we perform probabilistic forecasting with conditional generative networks trained to minimize scoring rule values. In contrast to Generative Adversarial Networks (GANs), no discriminator is required and training is stable. We perform experiments on two chaotic models and a global dataset of weather observations; results are satisfactory and better calibrated than what achieved by GANs.},
 author = {Lorenzo Pacchiardi and Rilwan A. Adewoyin and Peter Dueben and Ritabrata Dutta},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W4226301677},
 pages = {1--64},
 title = {Probabilistic Forecasting with Conditional Generative Networks via
  Scoring Rule Minimization},
 url = {http://jmlr.org/papers/v25/23-0038.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0044,
 abstract = {In the context of sketching for compressive mixture modeling, we revisit existing proofs of the Restricted Isometry Property of sketching operators with respect to certain mixtures models. After examining the shortcomings of existing guarantees, we propose an alternative analysis that circumvents the need to assume importance sampling when drawing random Fourier features to build random sketching operators. Our analysis is based on new deterministic bounds on the restricted isometry constant that depend solely on the set of frequencies used to define the sketching operator; then we leverage these bounds to establish concentration inequalities for random sketching operators that lead to the desired RIP guarantees. Our analysis also opens the door to theoretical guarantees for structured sketching with frequencies associated to fast random linear operators.},
 author = {Ayoub Belhadji and Rémi Gribonval},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W4310647065},
 pages = {1--68},
 title = {Revisiting RIP guarantees for sketching operators on mixture models},
 url = {http://jmlr.org/papers/v25/23-0044.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0062,
 abstract = {In this work we undertake a thorough study of the non-asymptotic properties of the vanilla generative adversarial networks (GANs). We prove an oracle inequality for the Jensen-Shannon (JS) divergence between the underlying density $\mathsf{p}^*$ and the GAN estimate with a significantly better statistical error term compared to the previously known results. The advantage of our bound becomes clear in application to nonparametric density estimation. We show that the JS-divergence between the GAN estimate and $\mathsf{p}^*$ decays as fast as $(\log{n}/n)^{2\beta/(2\beta + d)}$, where $n$ is the sample size and $\beta$ determines the smoothness of $\mathsf{p}^*$. This rate of convergence coincides (up to logarithmic factors) with minimax optimal for the considered class of densities.},
 author = {Nikita Puchkin and Sergey Samsonov and Denis Belomestny and Eric Moulines and Alexey Naumov},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W3128042839},
 pages = {1--47},
 title = {Rates of convergence for density estimation with generative adversarial networks},
 url = {http://jmlr.org/papers/v25/23-0062.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0119,
 author = {Matthias K{{\"o}}nig and Annelot W. Bosman and Holger H. Hoos and Jan N. van Rijn},
 journal = {Journal of Machine Learning Research},
 number = {12},
 pages = {1--53},
 title = {Critically Assessing the State of the Art in Neural Network Verification},
 url = {http://jmlr.org/papers/v25/23-0119.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0121,
 author = {Vasilii Feofanov and Emilie Devijver and Massih-Reza Amini},
 journal = {Journal of Machine Learning Research},
 number = {104},
 pages = {1--47},
 title = {Multi-class Probabilistic Bounds for Majority Vote Classifiers with Partially Labeled Data},
 url = {http://jmlr.org/papers/v25/23-0121.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0188,
 abstract = {Selecting the number of topics in LDA models is considered to be a difficult task, for which alternative approaches have been proposed. The performance of the recently developed singular Bayesian information criterion (sBIC) is evaluated and compared to the performance of alternative model selection criteria. The sBIC is a generalization of the standard BIC that can be implemented to singular statistical models. The comparison is based on Monte Carlo simulations and carried out for several alternative settings, varying with respect to the number of topics, the number of documents and the size of documents in the corpora. Performance is measured using different criteria which take into account the correct number of topics, but also whether the relevant topics from the DGPs are identified. Practical recommendations for LDA model selection in applications are derived.},
 author = {Victor Bystrov and Viktoriia Naboka-Krell and Anna Staszewska-Bystrova and Peter Winker},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W4313443081},
 pages = {1--30},
 title = {Choosing the Number of Topics in LDA Models -- A Monte Carlo Comparison of Selection Criteria},
 url = {http://jmlr.org/papers/v25/23-0188.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0220,
 abstract = {We propose new limiting dynamics for stochastic gradient descent in the small learning rate regime called stochastic modified flows. These SDEs are driven by a cylindrical Brownian motion and improve the so-called stochastic modified equations by having regular diffusion coefficients and by matching the multi-point statistics. As a second contribution, we introduce distribution dependent stochastic modified flows which we prove to describe the fluctuating limiting dynamics of stochastic gradient descent in the small learning rate - infinite width scaling regime.},
 author = {Benjamin Gess and Sebastian Kassing and Vitalii Konarovskyi},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W4321013734},
 pages = {1--27},
 title = {Stochastic Modified Flows, Mean-Field Limits and Dynamics of Stochastic Gradient Descent},
 url = {http://jmlr.org/papers/v25/23-0220.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0237,
 author = {Stefan Ankirchner and Stefan Perko},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W4390786676},
 pages = {1--55},
 title = {A comparison of continuous-time approximations to stochastic gradient descent},
 url = {http://jmlr.org/papers/v25/23-0237.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0286,
 abstract = {Both supervised and unsupervised machine learning algorithms have been used to learn partition-based index structures for approximate nearest neighbor (ANN) search. Existing supervised algorithms formulate the learning task as finding a partition in which the nearest neighbors of a training set point belong to the same partition element as the point itself, so that the nearest neighbor candidates can be retrieved by naive lookup or backtracking search. We formulate candidate set selection in ANN search directly as a multilabel classification problem where the labels correspond to the nearest neighbors of the query point, and interpret the partitions as partitioning classifiers for solving this task. Empirical results suggest that the natural classifier based on this interpretation leads to strictly improved performance when combined with any unsupervised or supervised partitioning strategy. We also prove a sufficient condition for consistency of a partitioning classifier for ANN search, and illustrate the result by verifying this condition for chronological $k$-d trees.},
 author = {Ville Hyv{{\"o}}nen and Elias J{{\"a}}{{\"a}}saari and Teemu Roos},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W4288091037},
 pages = {1--51},
 title = {A Multilabel Classification Framework for Approximate Nearest Neighbor Search},
 url = {http://jmlr.org/papers/v25/23-0286.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0295,
 abstract = {We consider the problem of learning a Gaussian graphical model in the case where the observations come from two dependent groups sharing the same variables. We focus on a family of coloured Gaussian graphical models specifically suited for the paired data problem. Commonly, graphical models are ordered by the submodel relationship so that the search space is a lattice, called the model inclusion lattice. We introduce a novel order between models, named the twin order. We show that, embedded with this order, the model space is a lattice that, unlike the model inclusion lattice, is distributive. Furthermore, we provide the relevant rules for the computation of the neighbours of a model. The latter are more efficient than the same operations in the model inclusion lattice, and are then exploited to achieve a more efficient exploration of the search space. These results can be applied to improve the efficiency of both greedy and Bayesian model search procedures. Here we implement a stepwise backward elimination procedure and evaluate its performance by means of simulations. Finally, the procedure is applied to learn a brain network from fMRI data where the two groups correspond to the left and right hemispheres, respectively.},
 author = {Alberto Roverato and Dung Ngoc Nguyen},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W4324107080},
 pages = {1--41},
 title = {Exploration of the search space of Gaussian graphical models for paired data},
 url = {http://jmlr.org/papers/v25/23-0295.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0314,
 abstract = {Imitation from observation is the framework of learning tasks by observing demonstrated state-only trajectories. Recently, adversarial approaches have achieved significant performance improvements over other methods for imitating complex behaviors. However, these adversarial imitation algorithms often require many demonstration examples and learning iterations to produce a policy that is successful at imitating a demonstrator's behavior. This high sample complexity often prohibits these algorithms from being deployed on physical robots. In this paper, we propose an algorithm that addresses the sample inefficiency problem by utilizing ideas from trajectory centric reinforcement learning algorithms. We test our algorithm and conduct experiments using an imitation task on a physical robot arm and its simulated version in Gazebo and will show the improvement in learning rate and efficiency.},
 author = {Dahuin Jung and Hyungyu Lee and Sungroh Yoon},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W2952165569},
 pages = {1--32},
 title = {Sample-efficient Adversarial Imitation Learning from Observation},
 url = {http://jmlr.org/papers/v25/23-0314.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0347,
 abstract = {In this article we consider the estimation of static parameters for partially observed diffusion process with discrete-time observations over a fixed time interval. In particular, we assume that one must time-discretize the partially observed diffusion process and work with the model with bias and consider maximizing the resulting log-likelihood. Using a novel double randomization scheme, based upon Markovian stochastic approximation we develop a new method to unbiasedly estimate the static parameters, that is, to obtain the maximum likelihood estimator with no time discretization bias. Under assumptions we prove that our estimator is unbiased and investigate the method in several numerical examples, showing that it can empirically out-perform existing unbiased methodology.},
 author = {Jeremy Heng and Jeremie Houssineau and Ajay Jasra},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W4386908032},
 pages = {1--66},
 title = {Unbiased Parameter Estimation for Partially Observed Diffusions},
 url = {http://jmlr.org/papers/v25/23-0347.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0356,
 abstract = {We show that the error achievable using physics-informed neural networks for solving systems of differential equations can be substantially reduced when these networks are trained using meta-learned optimization methods rather than to using fixed, hand-crafted optimizers as traditionally done. We choose a learnable optimization method based on a shallow multi-layer perceptron that is meta-trained for specific classes of differential equations. We illustrate meta-trained optimizers for several equations of practical relevance in mathematical physics, including the linear advection equation, Poisson's equation, the Korteweg--de Vries equation and Burgers' equation. We also illustrate that meta-learned optimizers exhibit transfer learning abilities, in that a meta-trained optimizer on one differential equation can also be successfully deployed on another differential equation.},
 author = {Alex Bihlo},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W4327485940},
 pages = {1--26},
 title = {Improving physics-informed neural networks with meta-learned optimization},
 url = {http://jmlr.org/papers/v25/23-0356.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0371,
 abstract = {This paper aims to develop a Newton-type method to solve a class of nonconvex composite programs. In particular, the nonsmooth part is possibly nonconvex. To tackle the nonconvexity, we develop a notion of strong prox-regularity which is related to the singleton property, Lipschitz continuity, and monotonicity of the associated proximal operator, and we verify it in various classes of functions, including weakly convex functions, indicator functions of proximally smooth sets, and two specific sphere-related nonconvex nonsmooth functions. In this case, the problem class we are concerned with covers smooth optimization problems on manifold and certain composite optimization problems on manifold. For the latter, the proposed algorithm is the first second-order type method. Combining with the semismoothness of the proximal operator, we design a projected semismooth Newton method to find a root of the natural residual induced by the proximal gradient method. Since the corresponding natural residual may not be globally monotone, an extra projection is added on the usual semismooth Newton step and new criteria are proposed for the switching between the projected semismooth Newton step and the proximal step. The global convergence is then established under the strong prox-regularity. Based on the BD regularity condition, we establish local superlinear convergence. Numerical experiments demonstrate the effectiveness of our proposed method compared with state-of-the-art ones.},
 author = {Jiang Hu and Kangkang Deng and Jiayuan Wu and Quanzheng Li},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W4323927318},
 pages = {1--32},
 title = {A projected semismooth Newton method for a class of nonconvex composite programs with strong prox-regularity},
 url = {http://jmlr.org/papers/v25/23-0371.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0413,
 abstract = {Learning anticipation in Multi-Agent Reinforcement Learning (MARL) is a reasoning paradigm where agents anticipate the learning steps of other agents to improve cooperation among themselves. As MARL uses gradient-based optimization, learning anticipation requires using Higher-Order Gradients (HOG), with so-called HOG methods. Existing HOG methods are based on policy parameter anticipation, i.e., agents anticipate the changes in policy parameters of other agents. Currently, however, these existing HOG methods have only been applied to differentiable games or games with small state spaces. In this work, we demonstrate that in the case of non-differentiable games with large state spaces, existing HOG methods do not perform well and are inefficient due to their inherent limitations related to policy parameter anticipation and multiple sampling stages. To overcome these problems, we propose Off-Policy Action Anticipation (OffPA2), a novel framework that approaches learning anticipation through action anticipation, i.e., agents anticipate the changes in actions of other agents, via off-policy sampling. We theoretically analyze our proposed OffPA2 and employ it to develop multiple HOG methods that are applicable to non-differentiable games with large state spaces. We conduct a large set of experiments and illustrate that our proposed HOG methods outperform the existing ones regarding efficiency and performance.},
 author = {Ariyan Bighashdel and Daan de Geus and Pavol Jancura and Gijs Dubbelman},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W4362655535},
 pages = {1--31},
 title = {Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning},
 url = {http://jmlr.org/papers/v25/23-0413.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0439,
 author = {Yifei He and Runxiang Cheng and Gargi Balasubramaniam and Yao-Hung Hubert Tsai and Han Zhao},
 journal = {Journal of Machine Learning Research},
 number = {47},
 pages = {1--39},
 title = {Efficient Modality Selection in Multimodal Learning},
 url = {http://jmlr.org/papers/v25/23-0439.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0446,
 abstract = {We propose data thinning, an approach for splitting an observation into two or more independent parts that sum to the original observation, and that follow the same distribution as the original observation, up to a (known) scaling of a parameter. This very general proposal is applicable to any convolution-closed distribution, a class that includes the Gaussian, Poisson, negative binomial, gamma, and binomial distributions, among others. Data thinning has a number of applications to model selection, evaluation, and inference. For instance, cross-validation via data thinning provides an attractive alternative to the usual approach of cross-validation via sample splitting, especially in settings in which the latter is not applicable. In simulations and in an application to single-cell RNA-sequencing data, we show that data thinning can be used to validate the results of unsupervised learning approaches, such as k-means clustering and principal components analysis, for which traditional sample splitting is unattractive or unavailable.},
 author = {Anna Neufeld and Ameer Dharamshi and Lucy L. Gao and Daniela Witten},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W4317548315},
 pages = {1--35},
 title = {Data thinning for convolution-closed distributions},
 url = {http://jmlr.org/papers/v25/23-0446.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0450,
 abstract = {A complete structure-preserving learning scheme for single-input/single-output (SISO) linear port-Hamiltonian systems is proposed. The construction is based on the solution, when possible, of the unique identification problem for these systems, in ways that reveal fundamental relationships between classical notions in control theory and crucial properties in the machine learning context, like structure-preservation and expressive power. In the canonical case, it is shown that the set of uniquely identified systems can be explicitly characterized as a smooth manifold endowed with global Euclidean coordinates, which allows concluding that the parameter complexity necessary for the replication of the dynamics is only $O(n)$ and not $O(n^2)$, as suggested by the standard parametrization of these systems. Furthermore, it is shown that linear port-Hamiltonian systems can be learned while remaining agnostic about the dimension of the underlying data-generating system. Numerical experiments show that this methodology can be used to efficiently estimate linear port-Hamiltonian systems out of input-output realizations, making the contributions in this paper the first example of a structure-preserving machine learning paradigm for linear port-Hamiltonian systems based on explicit representations of this model category.},
 author = {Juan-Pablo Ortega and Daiying Yin},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W4361230975},
 pages = {1--56},
 title = {Learnability of Linear Port-Hamiltonian Systems},
 url = {http://jmlr.org/papers/v25/23-0450.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0456,
 abstract = {Adversarial training is one of the most popular methods for training methods robust to adversarial attacks, however, it is not well-understood from a theoretical perspective. We prove and existence, regularity, and minimax theorems for adversarial surrogate risks. Our results explain some empirical observations on adversarial robustness from prior work and suggest new directions in algorithm development. Furthermore, our results extend previously known existence and minimax theorems for the adversarial classification risk to surrogate risks.},
 author = {Natalie S. Frank and Jonathan Niles-Weed},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W4283320637},
 pages = {1--41},
 title = {Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification},
 url = {http://jmlr.org/papers/v25/23-0456.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0488,
 abstract = {The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL), and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPPO and provides a general template for cooperative MARL algorithmic designs. We prove that all algorithms derived from HAML inherently enjoy monotonic improvement of joint return and convergence to Nash Equilibrium. As its natural outcome, HAML validates more novel algorithms in addition to HATRPO and HAPPO, including HAA2C, HADDPG, and HATD3, which generally outperform their existing MA-counterparts. We comprehensively test HARL algorithms on six challenging benchmarks and demonstrate their superior effectiveness and stability for coordinating heterogeneous agents compared to strong baselines such as MAPPO and QMIX.},
 author = {Yifan Zhong and Jakub Grudzien Kuba and Xidong Feng and Siyi Hu and Jiaming Ji and Yaodong Yang},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W4366731969},
 pages = {1--67},
 title = {Heterogeneous-Agent Reinforcement Learning},
 url = {http://jmlr.org/papers/v25/23-0488.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0549,
 abstract = {In supervised learning, the regularization path is sometimes used as a convenient theoretical proxy for the optimization path of gradient descent initialized from zero. In this paper, we study a modification of the regularization path for infinite-width 2-layer ReLU neural networks with nonzero initial distribution of the weights at different scales. By exploiting a link with unbalanced optimal-transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite-dimensional convex counterpart. We formulate the corresponding functional-optimization problem and investigate its main properties. In particular, we show that, as the scale of the initialization ranges between $0$ and $+\infty$, the associated path interpolates continuously between the so-called kernel and rich regimes. Numerical experiments confirm that, in our setting, the scaling path and the final states of the optimization path behave similarly, even beyond these extreme points.},
 author = {Sebastian Neumayer and Lénaïc Chizat and Michael Unser},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W4362508498},
 pages = {1--24},
 title = {On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks},
 url = {http://jmlr.org/papers/v25/23-0549.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0570,
 abstract = {In this study we evaluate 32 unsupervised anomaly detection algorithms on 52 real-world multivariate tabular datasets, performing the largest comparison of unsupervised anomaly detection algorithms to date. On this collection of datasets, the $k$-thNN (distance to the $k$-nearest neighbor) algorithm significantly outperforms the most other algorithms. Visualizing and then clustering the relative performance of the considered algorithms on all datasets, we identify two clear clusters: one with ``local'' datasets, and another with ``global'' datasets. ``Local'' anomalies occupy a region with low density when compared to nearby samples, while ``global'' occupy an overall low density region in the feature space. On the local datasets the $k$NN ($k$-nearest neighbor) algorithm comes out on top. On the global datasets, the EIF (extended isolation forest) algorithm performs the best. Also taking into consideration the algorithms' computational complexity, a toolbox with these three unsupervised anomaly detection algorithms suffices for finding anomalies in this representative collection of multivariate datasets. By providing access to code and datasets, our study can be easily reproduced and extended with more algorithms and/or datasets.},
 author = {Roel Bouman and Zaharah Bukhsh and Tom Heskes},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W4367701135},
 pages = {1--34},
 title = {Unsupervised anomaly detection algorithms on real-world data: how many do we need?},
 url = {http://jmlr.org/papers/v25/23-0570.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0572,
 author = {Runzhong Wang and Ziao Guo and Wenzheng Pan and Jiale Ma and Yikai Zhang and Nan Yang and Qi Liu and Longxuan Wei and Hanxue Zhang and Chang Liu and Zetian Jiang and Xiaokang Yang and Junchi Yan},
 journal = {Journal of Machine Learning Research},
 number = {33},
 pages = {1--7},
 title = {Pygmtools: A Python Graph Matching Toolkit},
 url = {http://jmlr.org/papers/v25/23-0572.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0576,
 abstract = {In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.},
 author = {Nachuan Xiao and Xiaoyin Hu and Xin Liu and Kim-Chuan Toh},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W4375957515},
 pages = {1--53},
 title = {Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees},
 url = {http://jmlr.org/papers/v25/23-0576.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0636,
 author = {Moritz Wolter and Felix Blanke and Jochen Garcke and Charles Tapley Hoyt},
 journal = {Journal of Machine Learning Research},
 number = {80},
 pages = {1--7},
 title = {ptwt - The PyTorch Wavelet Toolbox},
 url = {http://jmlr.org/papers/v25/23-0636.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0645,
 abstract = {Sparsity of a learning solution is a desirable feature in machine learning. Certain reproducing kernel Banach spaces (RKBSs) are appropriate hypothesis spaces for sparse learning methods. The goal of this paper is to understand what kind of RKBSs can promote sparsity for learning solutions. We consider two typical learning models in an RKBS: the minimum norm interpolation (MNI) problem and the regularization problem. We first establish an explicit representer theorem for solutions of these problems, which represents the extreme points of the solution set by a linear combination of the extreme points of the subdifferential set, of the norm function, which is data-dependent. We then propose sufficient conditions on the RKBS that can transform the explicit representation of the solutions to a sparse kernel representation having fewer terms than the number of the observed data. Under the proposed sufficient conditions, we investigate the role of the regularization parameter on sparsity of the regularized solutions. We further show that two specific RKBSs: the sequence space $\ell_1(\mathbb{N})$ and the measure space can have sparse representer theorems for both MNI and regularization models.},
 author = {Rui Wang and Yuesheng Xu and Mingsong Yan},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W4377864500},
 pages = {1--45},
 title = {Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces},
 url = {http://jmlr.org/papers/v25/23-0645.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0661,
 abstract = {We consider estimating a low-dimensional parameter in an estimating equation involving high-dimensional nuisances that depend on the parameter. A central example is the efficient estimating equation for the (local) quantile treatment effect ((L)QTE) in causal inference, which involves as a nuisance the covariate-conditional cumulative distribution function evaluated at the quantile to be estimated. Debiased machine learning (DML) is a data-splitting approach to estimating high-dimensional nuisances using flexible machine learning methods, but applying it to problems with parameter-dependent nuisances is impractical. For (L)QTE, DML requires we learn the whole covariate-conditional cumulative distribution function. We instead propose localized debiased machine learning (LDML), which avoids this burdensome step and needs only estimate nuisances at a single initial rough guess for the parameter. For (L)QTE, LDML involves learning just two regression functions, a standard task for machine learning methods. We prove that under lax rate conditions our estimator has the same favorable asymptotic behavior as the infeasible estimator that uses the unknown true nuisances. Thus, LDML notably enables practically-feasible and theoretically-grounded efficient estimation of important quantities in causal inference such as (L)QTEs when we must control for many covariates and/or flexible relationships, as we demonstrate in empirical studies.},
 author = {Nathan Kallus and Xiaojie Mao and Masatoshi Uehara},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W3084410516},
 pages = {1--59},
 title = {Localized Debiased Machine Learning: Efficient Inference on Quantile Treatment Effects and Beyond},
 url = {http://jmlr.org/papers/v25/23-0661.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0680,
 abstract = {Mitigating the climate crisis requires a rapid transition towards lower-carbon energy. Catalyst materials play a crucial role in the electrochemical reactions involved in numerous industrial processes key to this transition, such as renewable energy storage and electrofuel synthesis. To reduce the energy spent on such activities, we must quickly discover more efficient catalysts to drive electrochemical reactions. Machine learning (ML) holds the potential to efficiently model materials properties from large amounts of data, accelerating electrocatalyst design. The Open Catalyst Project OC20 dataset was constructed to that end. However, ML models trained on OC20 are still neither scalable nor accurate enough for practical applications. In this paper, we propose task-specific innovations applicable to most architectures, enhancing both computational efficiency and accuracy. This includes improvements in (1) the graph creation step, (2) atom representations, (3) the energy prediction head, and (4) the force prediction head. We describe these contributions, referred to as PhAST, and evaluate them thoroughly on multiple architectures. Overall, PhAST improves energy MAE by 4 to 42$\%$ while dividing compute time by 3 to 8$\times$ depending on the targeted task/model. PhAST also enables CPU training, leading to 40$\times$ speedups in highly parallelized settings. Python package: \url{https://phast.readthedocs.io}.},
 author = {Alexandre Duval and Victor Schmidt and Santiago Miret and Yoshua Bengio and Alex Hern{{\'a}}ndez-Garc{{\'i}}a and David Rolnick},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W4309870673},
 pages = {1--26},
 title = {PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design},
 url = {http://jmlr.org/papers/v25/23-0680.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0698,
 abstract = {Performance of optimization on quadratic problems sensitively depends on the low-lying part of the spectrum. For large (effectively infinite-dimensional) problems, this part of the spectrum can often be naturally represented or approximated by power law distributions, resulting in power law convergence rates for iterative solutions of these problems by gradient-based algorithms. In this paper, we propose a new spectral condition providing tighter upper bounds for problems with power law optimization trajectories. We use this condition to build a complete picture of upper and lower bounds for a wide range of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy Ball, and Conjugate Gradients -- with an emphasis on the underlying schedules of learning rate and momentum. In particular, we demonstrate how an optimally accelerated method, its schedule, and convergence upper bound can be obtained in a unified manner for a given shape of the spectrum. Also, we provide first proofs of tight lower bounds for convergence rates of Steepest Descent and Conjugate Gradients under spectral power laws with general exponents. Our experiments show that the obtained convergence bounds and acceleration strategies are not only relevant for exactly quadratic optimization problems, but also fairly accurate when applied to the training of neural networks.},
 author = {Maksim Velikanov and Dmitry Yarotsky},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W4221165655},
 pages = {1--78},
 title = {Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions},
 url = {http://jmlr.org/papers/v25/23-0698.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0708,
 abstract = {We develop a notion of projections between sets of probability measures using the geometric properties of the 2-Wasserstein space. It is designed for general multivariate probability measures, is computationally efficient to implement, and provides a unique solution in regular settings. The idea is to work on regular tangent cones of the Wasserstein space using generalized geodesics. Its structure and computational properties make the method applicable in a variety of settings, from causal inference to the analysis of object data. An application to estimating causal effects yields a generalization of the notion of synthetic controls to multivariate data with individual-level heterogeneity, as well as a way to estimate optimal weights jointly over all time periods.},
 author = {Florian Gunsilius and Meng Hsuan Hsieh and Myung Jin Lee},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W4289446299},
 pages = {1--41},
 title = {Tangential Wasserstein Projections},
 url = {http://jmlr.org/papers/v25/23-0708.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0740,
 author = {Jiaming Xu and Hanjing Zhu},
 journal = {Journal of Machine Learning Research},
 number = {94},
 pages = {1--83},
 title = {Overparametrized Multi-layer Neural Networks: Uniform Concentration of Neural Tangent Kernel and Convergence of Stochastic Gradient Descent},
 url = {http://jmlr.org/papers/v25/23-0740.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0777,
 abstract = {Learning interpretable representations of neural dynamics at a population level is a crucial first step to understanding how observed neural activity relates to perception and behavior. Models of neural dynamics often focus on either low-dimensional projections of neural activity, or on learning dynamical systems that explicitly relate to the neural state over time. We discuss how these two approaches are interrelated by considering dynamical systems as representative of flows on a low-dimensional manifold. Building on this concept, we propose a new decomposed dynamical system model that represents complex non-stationary and nonlinear dynamics of time series data as a sparse combination of simpler, more interpretable components. Our model is trained through a dictionary learning procedure, where we leverage recent results in tracking sparse vectors over time. The decomposed nature of the dynamics is more expressive than previous switched approaches for a given number of parameters and enables modeling of overlapping and non-stationary dynamics. In both continuous-time and discrete-time instructional examples we demonstrate that our model can well approximate the original system, learn efficient representations, and capture smooth transitions between dynamical modes, focusing on intuitive low-dimensional non-stationary linear and nonlinear systems. Furthermore, we highlight our model's ability to efficiently capture and demix population dynamics generated from multiple independent subnetworks, a task that is computationally impractical for switched models. Finally, we apply our model to neural "full brain" recordings of C. elegans data, illustrating a diversity of dynamics that is obscured when classified into discrete states.},
 author = {Noga Mudrik and Yenho Chen and Eva Yezerets and Christopher J. Rozell and Adam S. Charles},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W4281627263},
 pages = {1--44},
 title = {Decomposed Linear Dynamical Systems (dLDS) for learning the latent components of neural dynamics},
 url = {http://jmlr.org/papers/v25/23-0777.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0802,
 abstract = {Policy learning is an important component of many real-world learning systems. A major challenge in policy learning is how to adapt efficiently to unseen environments or tasks. Recently, it has been suggested to exploit invariant conditional distributions to learn models that generalize better to unseen environments. However, assuming invariance of entire conditional distributions (which we call full invariance) may be too strong of an assumption in practice. In this paper, we introduce a relaxation of full invariance called effect-invariance (e-invariance for short) and prove that it is sufficient, under suitable assumptions, for zero-shot policy generalization. We also discuss an extension that exploits e-invariance when we have a small sample from the test environment, enabling few-shot policy generalization. Our work does not assume an underlying causal graph or that the data are generated by a structural causal model; instead, we develop testing procedures to test e-invariance directly from data. We present empirical results using simulated data and a mobile health intervention dataset to demonstrate the effectiveness of our approach.},
 author = {Sorawit Saengkyongam and Niklas Pfister and Predrag Klasnja and Susan Murphy and Jonas Peters},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W4381571245},
 pages = {1--36},
 title = {Effect-Invariant Mechanisms for Policy Generalization},
 url = {http://jmlr.org/papers/v25/23-0802.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0811,
 author = {Rui Qiu and Zhou Yu and Ruoqing Zhu},
 journal = {Journal of Machine Learning Research},
 number = {107},
 pages = {1--69},
 title = {Random Forest Weighted Local Fr{{\'e}}chet Regression with Random Objects},
 url = {http://jmlr.org/papers/v25/23-0811.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0866,
 abstract = {In this paper, we provide a strategy to determine the eigenvalue decay rate (EDR) of a large class of kernel functions defined on a general domain rather than $\mathbb S^{d}$. This class of kernel functions include but are not limited to the neural tangent kernel associated with neural networks with different depths and various activation functions. After proving that the dynamics of training the wide neural networks uniformly approximated that of the neural tangent kernel regression on general domains, we can further illustrate the minimax optimality of the wide neural network provided that the underground truth function $f\in [\mathcal H_{\mathrm{NTK}}]^{s}$, an interpolation space associated with the RKHS $\mathcal{H}_{\mathrm{NTK}}$ of NTK. We also showed that the overfitted neural network can not generalize well. We believe our approach for determining the EDR of kernels might be also of independent interests.},
 author = {Yicheng Li and Zixiong Yu and Guhan Chen and Qian Lin},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W4372273247},
 pages = {1--47},
 title = {On the Eigenvalue Decay Rates of a Class of Neural-Network Related Kernel Functions Defined on General Domains},
 url = {http://jmlr.org/papers/v25/23-0866.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0870,
 abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
 author = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W4307079201},
 pages = {1--53},
 title = {Scaling Instruction-Finetuned Language Models},
 url = {http://jmlr.org/papers/v25/23-0870.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0893,
 abstract = {We introduce a sufficient graphical model by applying the recently developed nonlinear sufficient dimension reduction techniques to the evaluation of conditional independence. The graphical model is nonparametric in nature, as it does not make distributional assumptions such as the Gaussian or copula Gaussian assumptions. However, unlike a fully nonparametric graphical model, which relies on the high-dimensional kernel to characterize conditional independence, our graphical model is based on conditional independence given a set of sufficient predictors with a substantially reduced dimension. In this way we avoid the curse of dimensionality that comes with a high-dimensional kernel. We develop the population-level properties, convergence rate, and variable selection consistency of our estimate. By simulation comparisons and an analysis of the DREAM 4 Challenge data set, we demonstrate that our method outperforms the existing methods when the Gaussian or copula Gaussian assumptions are violated, and its performance remains excellent in the high-dimensional setting.},
 author = {Bing Li and Kyongwon Kim},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W4383989149},
 pages = {1--64},
 title = {On Sufficient Graphical Models},
 url = {http://jmlr.org/papers/v25/23-0893.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0912,
 abstract = {This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $3N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, albeit with slightly increased constants. Significantly, we establish that the (width,$\,$depth) scaling factors that appeared in the previous result can be further reduced from $(3,2)$ to $(1,1)$ if $\varrho$ falls within a specific subset of $\mathscr{A}$. This subset includes activation functions such as $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, and $\mathtt{Mish}$.},
 author = {Shijun Zhang and Jianfeng Lu and Hongkai Zhao},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W4384389978},
 pages = {1--39},
 title = {Deep Network Approximation: Beyond ReLU to Diverse Activation Functions},
 url = {http://jmlr.org/papers/v25/23-0912.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0970,
 abstract = {Causal discovery aims at revealing causal relations from observational data, which is a fundamental task in science and engineering. We describe $\textit{causal-learn}$, an open-source Python library for causal discovery. This library focuses on bringing a comprehensive collection of causal discovery methods to both practitioners and researchers. It provides easy-to-use APIs for non-specialists, modular building blocks for developers, detailed documentation for learners, and comprehensive methods for all. Different from previous packages in R or Java, $\textit{causal-learn}$ is fully developed in Python, which could be more in tune with the recent preference shift in programming languages within related communities. The library is available at https://github.com/py-why/causal-learn.},
 author = {Yujia Zheng and Biwei Huang and Wei Chen and Joseph Ramsey and Mingming Gong and Ruichu Cai and Shohei Shimizu and Peter Spirtes and Kun Zhang},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W4385474105},
 pages = {1--8},
 title = {Causal-learn: Causal Discovery in Python},
 url = {http://jmlr.org/papers/v25/23-0970.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-0985,
 abstract = {Kernel-based tests provide a simple yet effective framework that use the theory of reproducing kernel Hilbert spaces to design non-parametric testing procedures. In this paper we propose new theoretical tools that can be used to study the asymptotic behaviour of kernel-based tests in several data scenarios, and in many different testing problems. Unlike current approaches, our methods avoid using lengthy $U$ and $V$ statistics expansions and limit theorems, that commonly appear in the literature, and works directly with random functionals on Hilbert spaces. Therefore, our framework leads to a much simpler and clean analysis of kernel tests, only requiring mild regularity conditions. Furthermore, we show that, in general, our analysis cannot be improved by proving that the regularity conditions required by our methods are both sufficient and necessary. To illustrate the effectiveness of our approach we present a new kernel-test for the conditional independence testing problem, as well as new analyses for already known kernel-based tests.},
 author = {Tamara Fern{{\'a}}ndez and Nicol{{\'a}}s Rivera},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W4294534214},
 pages = {1--40},
 title = {A general framework for the analysis of kernel-based tests},
 url = {http://jmlr.org/papers/v25/23-0985.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1015,
 abstract = {Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce "deterministic ADVI" (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the "sample average approximation" (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior covariances via linear response (LR). In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform well with relatively few samples even in very high dimensions, though we also show that such favorable results cannot extend to variational approximations that are too expressive relative to mean-field ADVI. We show on a variety of real-world problems that DADVI reliably finds good solutions with default settings (unlike ADVI) and, together with LR covariances, is typically faster and more accurate than standard ADVI.},
 author = {Ryan Giordano and Martin Ingram and Tamara Broderick},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W4365516763},
 pages = {1--39},
 title = {Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box},
 url = {http://jmlr.org/papers/v25/23-1015.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1027,
 abstract = {QDax is an open-source library with a streamlined and modular API for Quality-Diversity (QD) optimization algorithms in Jax. The library serves as a versatile tool for optimization purposes, ranging from black-box optimization to continuous control. QDax offers implementations of popular QD, Neuroevolution, and Reinforcement Learning (RL) algorithms, supported by various examples. All the implementations can be just-in-time compiled with Jax, facilitating efficient execution across multiple accelerators, including GPUs and TPUs. These implementations effectively demonstrate the framework's flexibility and user-friendliness, easing experimentation for research purposes. Furthermore, the library is thoroughly documented and tested with 95\% coverage.},
 author = {Felix Chalumeau and Bryan Lim and Rapha{{\"e}}l Boige and Maxime Allard and Luca Grillotti and Manon Flageat and Valentin Mac{{\'e}} and Guillaume Richard and Arthur Flajolet and Thomas Pierrot and Antoine Cully},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W4385682158},
 pages = {1--16},
 title = {QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration},
 url = {http://jmlr.org/papers/v25/23-1027.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1042,
 abstract = {Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares. Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this global minimum, when given a test prompt of labeled examples from a new prediction task, the transformer achieves prediction error competitive with the best linear predictor over the test prompt distribution. We additionally characterize the robustness of the trained transformer to a variety of distribution shifts and show that although a number of shifts are tolerated, shifts in the covariate distribution of the prompts are not. Motivated by this, we consider a generalized ICL setting where the covariate distributions can vary across prompts. We show that although gradient flow succeeds at finding a global minimum in this setting, the trained transformer is still brittle under mild covariate shifts. We complement this finding with experiments on large, nonlinear transformer architectures which we show are more robust under covariate shifts.},
 author = {Ruiqi Zhang and Spencer Frei and Peter L. Bartlett},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W4381252822},
 pages = {1--55},
 title = {Trained Transformers Learn Linear Models In-Context},
 url = {http://jmlr.org/papers/v25/23-1042.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1073,
 author = {Pan Zhou and Xingyu Xie and Zhouchen Lin and Kim-Chuan Toh and Shuicheng Yan},
 journal = {Journal of Machine Learning Research},
 number = {83},
 pages = {1--74},
 title = {Win: Weight-Decay-Integrated Nesterov Acceleration for  Faster Network Training},
 url = {http://jmlr.org/papers/v25/23-1073.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1086,
 abstract = {Imagine a learner L who tries to infer a hidden concept from a collection of observations. Building on the work [4] of Ferri et al., we assume the learner to be parameterized by priors P(c) and by c-conditional likelihoods P(z|c) where c ranges over all concepts in a given class C and z ranges over all observations in an observation set Z. L is called a MAP-learner (resp. an MLE-learner) if it thinks of a collection S of observations as a random sample and returns the concept with the maximum a-posteriori probability (resp. the concept which maximizes the c-conditional likelihood of S). Depending on whether L assumes that S is obtained from ordered or unordered sampling resp. from sampling with or without replacement, we can distinguish four different sampling modes. Given a target concept c in C, a teacher for a MAP-learner L aims at finding a smallest collection of observations that causes L to return c. This approach leads in a natural manner to various notions of a MAP- or MLE-teaching dimension of a concept class C. Our main results are: We show that this teaching model has some desirable monotonicity properties. We clarify how the four sampling modes are related to each other. As for the (important!) special case, where concepts are subsets of a domain and observations are 0,1-labeled examples, we obtain some additional results. First of all, we characterize the MAP- and MLE-teaching dimension associated with an optimally parameterized MAP-learner graph-theoretically. From this central result, some other ones are easy to derive. It is shown, for instance, that the MLE-teaching dimension is either equal to the MAP-teaching dimension or exceeds the latter by 1. It is shown furthermore that these dimensions can be bounded from above by the so-called antichain number, the VC-dimension and related combinatorial parameters. Moreover they can be computed in polynomial time.},
 author = {Hans Ulrich Simon and Jan Arne Telle},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W4384112247},
 pages = {1--34},
 title = {MAP- and MLE-Based Teaching},
 url = {http://jmlr.org/papers/v25/23-1086.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1225,
 abstract = {To characterize the function space explored by neural networks (NNs) is an important aspect of learning theory. In this work, noticing that a multi-layer NN generates implicitly a hierarchy of reproducing kernel Hilbert spaces (RKHSs) - named a neural Hilbert ladder (NHL) - we define the function space as an infinite union of RKHSs, which generalizes the existing Barron space theory of two-layer NNs. We then establish several theoretical properties of the new space. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning an NHL with a controlled complexity measure. Third, we derive a non-Markovian dynamics of random fields that governs the evolution of the NHL which is induced by the training of multi-layer NNs in an infinite-width mean-field limit. Fourth, we show examples of depth separation in NHLs under the ReLU activation function. Finally, we perform numerical experiments to illustrate the feature learning aspect of NN training through the lens of NHLs.},
 author = {Zhengdao Chen},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W4383180670},
 pages = {1--65},
 title = {Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space},
 url = {http://jmlr.org/papers/v25/23-1225.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1257,
 abstract = {Supervised learning aims to train a classifier under the assumption that training and test data are from the same distribution. To ease the above assumption, researchers have studied a more realistic setting: out-of-distribution (OOD) detection, where test data may come from classes that are unknown during training (i.e., OOD data). Due to the unavailability and diversity of OOD data, good generalization ability is crucial for effective OOD detection algorithms, and corresponding learning theory is still an open problem. To study the generalization of OOD detection, this paper investigates the probably approximately correct (PAC) learning theory of OOD detection that fits the commonly used evaluation metrics in the literature. First, we find a necessary condition for the learnability of OOD detection. Then, using this condition, we prove several impossibility theorems for the learnability of OOD detection under some scenarios. Although the impossibility theorems are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we next give several necessary and sufficient conditions to characterize the learnability of OOD detection in some practical scenarios. Lastly, we offer theoretical support for representative OOD detection works based on our OOD theory.},
 author = {Zhen Fang and Yixuan Li and Feng Liu and Bo Han and Jie Lu},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W4394663910},
 pages = {1--83},
 title = {On the Learnability of Out-of-distribution Detection},
 url = {http://jmlr.org/papers/v25/23-1257.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1318,
 abstract = {Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.},
 author = {Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W4378105483},
 pages = {1--52},
 title = {Scaling Speech Technology to 1,000+ Languages},
 url = {http://jmlr.org/papers/v25/23-1318.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1360,
 abstract = {In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we recover a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. In particular, the fast rate bound is equivalent to the Seeger--Langford bound. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. This technique is both simpler and more general than previous approaches optimizing over a grid on the parameters' space. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.},
 author = {Borja Rodr{{\'i}}guez-G{{\'a}}lvez and Ragnar Thobaben and Mikael Skoglund},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W4381714197},
 pages = {1--43},
 title = {More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity},
 url = {http://jmlr.org/papers/v25/23-1360.html},
 volume = {25},
 year = {2024}
}

@article{JMLR:v25:23-1415,
 abstract = {Reinforcement learning (RL) on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of Markov decision process (MDP) homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual control tasks from the DeepMind Control Suite. Our method's ability to utilize MDP homomorphisms for representation learning leads to improved performance, and the visualizations of the latent space clearly demonstrate the structure of the learned abstraction.},
 author = {Prakash Panangaden and Sahand Rezaei-Shoshtari and Rosie Zhao and David Meger and Doina Precup},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W4376864530},
 pages = {1--57},
 title = {Policy Gradient Methods in the Presence of Symmetries and State Abstractions},
 url = {http://jmlr.org/papers/v25/23-1415.html},
 volume = {25},
 year = {2024}
}
