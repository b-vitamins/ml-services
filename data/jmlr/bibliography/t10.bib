@article{JMLR:v14:malgireddy13a,
 abstract = {We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-specified activities (or gestures) in a video sequence, analogous to the use of filler models for keyword detection in speech processing. We demonstrate the robustness of our classification model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach.},
 author = {Manavender R. Malgireddy and Ifeoma Nwogu and Venu Govindaraju},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2098376331},
 pages = {2189--2212},
 title = {Language-Motivated Approaches to Action Recognition},
 url = {http://jmlr.org/papers/v14/malgireddy13a.html},
 volume = {14},
 year = {2013}
}

@article{JMLR:v14:roussos13a,
 abstract = {We propose the novel approach of dynamic affine-invariant shape-appearance model (Aff-SAM) and employ it for handshape classification and sign recognition in sign language (SL) videos. Aff-SAM offers a compact and descriptive representation of hand configurations as well as regularized model-fitting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand’s shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by affine transformations, accounting for 3D hand pose changes and improving model’s compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an affine signer adaptation component at the visual level, without requiring training from scratch a new singer-specific model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston-University-400 continuous SL corpus demonstrate improvements on handshape classification when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff-SAM to multiple signers providing promising results.},
 author = {Anastasios Roussos and Stavros Theodorakis and Vassilis Pitsikalis and Petros Maragos},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W2113512957},
 pages = {1627--1663},
 title = {Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos},
 url = {http://jmlr.org/papers/v14/roussos13a.html},
 volume = {14},
 year = {2013}
}
