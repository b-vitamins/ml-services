@article{JMLR:v13:aho12a,
 abstract = {Methods for learning decision rules are being successfully applied to many problem domains, in particular when understanding and interpretation of the learned model is necessary. In many real life ...},
 author = {Timo Aho and Bernard {\v{Z}}enko and Sa{\v{s}}o D{\v{z}}eroski and Tapio Elomaa},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W3006991679},
 pages = {2367--2407},
 title = {Multi-target regression with rule ensembles},
 url = {http://jmlr.org/papers/v13/aho12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:ailon12a,
 abstract = {Given a set V of n elements we wish to linearly order them given pairwise preference labels which may be non-transitive (due to irrationality or arbitrary noise).

The goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible. Our performance is measured by two parameters: The number of disagreements (loss) and the query complexity (number of pairwise preference labels). Our algorithm adaptively queries at most O(e-6n log5 n) preference labels for a regret of e times the optimal loss. As a function of n, this is asymptotically better than standard (non-adaptive) learning bounds achievable for the same problem.

Our main result takes us a step closer toward settling an open problem posed by learning-to-rank (from pairwise information) theoreticians and practitioners: What is a provably correct way to sample preference labels? To further show the power and practicality of our solution, we analyze a typical test case in which a large margin linear relaxation is used for efficiently solving the simpler learning problems in our decomposition.},
 author = {Nir Ailon},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2122350398},
 pages = {137--164},
 title = {An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity},
 url = {http://jmlr.org/papers/v13/ailon12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:anandkumar12a,
 abstract = {We consider the problem of high-dimensional Gaussian graphical model selection. We identify a set of graphs for which an efficient estimation algorithm exists, and this algorithm is based on thresholding of empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of samples n = Ω(Jmin-2 log p), where p is the number of variables and Jmin is the minimum (absolute) edge potential of the graphical model. The sufficient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic necessary conditions on the number of samples required for sparsistency.},
 author = {Animashree Anandkumar and Vincent Y.F. Tan and Furong Huang and Alan S. Willsky},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W2121044254},
 pages = {2293--2337},
 title = {High-dimensional Gaussian graphical model selection: walk summability and local separation criterion},
 url = {http://jmlr.org/papers/v13/anandkumar12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:azar12a,
 abstract = {In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the infinite-horizon Markov decision processes. DPP is an incremental algorithm that forces a gradual change in policy update. This allows us to prove finite-iteration and asymptotic l∞-norm performance-loss bounds in the presence of approximation/ estimation error which depend on the average accumulated error as opposed to the standard bounds which are expressed in terms of the supremum of the errors. The dependency on the average error is important in problems with limited number of samples per iteration, for which the average of the errors can be significantly smaller in size than the supremum of the errors. Based on these theoretical results, we prove that a sampling-based variant of DPP (DPP-RL) asymptotically converges to the optimal policy. Finally, we illustrate numerically the applicability of these results on some benchmark problems and compare the performance of the approximate variants of DPP with some existing reinforcement learning (RL) methods.},
 author = {Mohammad Gheshlaghi Azar and Vicen{\c{c}} G{{\'o}}mez and Hilbert J. Kappen},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W2962901215},
 pages = {3207--3245},
 title = {Dynamic policy programming},
 url = {http://jmlr.org/papers/v13/azar12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:barbu12a,
 abstract = {Prediction markets are used in real life to predict outcomes of interest such as presidential elections. This paper presents a mathematical theory of artificial prediction markets for supervised learning of conditional probability estimators. The artificial prediction market is a novel method for fusing the prediction information of features or trained classifiers, where the fusion result is the contract price on the possible outcomes. The market can be trained online by updating the participants' budgets using training examples. Inspired by the real prediction markets, the equations that govern the market are derived from simple and reasonable assumptions. Efficient numerical algorithms are presented for solving these equations. The obtained artificial prediction market is shown to be a maximum likelihood estimator. It generalizes linear aggregation, existent in boosting and random forest, as well as logistic regression and some kernel methods. Furthermore, the market mechanism allows the aggregation of specialized classifiers that participate only on specific instances. Experimental comparisons show that the artificial prediction markets often outperform random forest and implicit online learning on synthetic data and real UCI data sets. Moreover, an extensive evaluation for pelvic and abdominal lymph node detection in CT data shows that the prediction market improves adaboost's detection rate from 79.6% to 81.2% at 3 false positives/volume.},
 author = {Adrian Barbu and Nathan Lay},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2962849298},
 pages = {2177--2204},
 title = {An introduction to artificial prediction markets for classification},
 url = {http://jmlr.org/papers/v13/barbu12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:ben-tal12a,
 abstract = {In this paper we study the problem of designing SVM classifiers when the kernel matrix, K, is affected by uncertainty. Specifically K is modeled as a positive affine combination of given positive semi definite kernels, with the coefficients ranging in a norm-bounded uncertainty set. We treat the problem using the Robust Optimization methodology. This reduces the uncertain SVM problem into a deterministic conic quadratic problem which can be solved in principle by a polynomial time Interior Point (IP) algorithm. However, for large-scale classification problems, IP methods become intractable and one has to resort to first-order gradient type methods. The strategy we use here is to reformulate the robust counterpart of the uncertain SVM problem as a saddle point problem and employ a special gradient scheme which works directly on the convex-concave saddle function. The algorithm is a simplified version of a general scheme due to Juditski and Nemirovski (2011). It achieves an O(1/T2) reduction of the initial error after T iterations. A comprehensive empirical study on both synthetic data and real-world protein structure data sets show that the proposed formulations achieve the desired robustness, and the saddle point based algorithm outperforms the IP method significantly.},
 author = {Aharon Ben-Tal and Sahely Bhadra and Chiranjib Bhattacharyya and Arkadi Nemirovski},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W2137578134},
 pages = {2923--2954},
 title = {Efficient methods for robust classification under uncertainty in kernel matrices},
 url = {http://jmlr.org/papers/v13/ben-tal12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:benbouzid12a,
 abstract = {The MULTIBOOST package provides a fast C++ implementation of multi-class/multi-label/multitask boosting algorithms. It is based on ADABOOST.MH but it also implements popular cascade classifiers and FILTERBOOST. The package contains common multi-class base learners (stumps, trees, products, Haar filters). Further base learners and strong learners following the boosting paradigm can be easily implemented in a flexible framework.},
 author = {Djalel Benbouzid and R{{\'o}}bert Busa-Fekete and Norman Casagrande and Fran{\c{c}}ois-David Collin and Bal{{\'a}}zs K{{\'e}}gl},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W2184241488},
 pages = {549--553},
 title = {MULTIBOOST: a multi-purpose boosting package},
 url = {http://jmlr.org/papers/v13/benbouzid12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:bergstra12a,
 abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for...},
 author = {James Bergstra and Yoshua Bengio},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2998993395},
 pages = {281--305},
 title = {Random search for hyper-parameter optimization},
 url = {http://jmlr.org/papers/v13/bergstra12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:biau12a,
 abstract = {Random forests are a scheme proposed by Leo Breiman in the 2000's for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing in...},
 author = {G{{\'e}}rard Biau},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W2997833137},
 pages = {1063--1095},
 title = {Analysis of a random forests model},
 url = {http://jmlr.org/papers/v13/biau12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:brodersen12a,
 abstract = {Classification algorithms are frequently used on data with a natural hierarchical structure. For instance, classifiers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classification outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (fixed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classification accuracy is complemented by inference on the balanced accuracy, which avoids inflated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods usingMCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classification studies.},
 author = {Kay H. Brodersen and Christoph Mathys and Justin R. Chumbley and Jean Daunizeau and Cheng Soon Ong and Joachim M. Buhmann and Klaas E. Stephan},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W2149835029},
 pages = {3133--3176},
 title = {Bayesian mixed-effects inference on classification performance in hierarchical data sets},
 url = {http://jmlr.org/papers/v13/brodersen12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:brown12a,
 abstract = {We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation. This is in response to the question: what are the implicit statistical assumptions of feature selection criteria based on mutual information?. To answer this, we adopt a different strategy than is usual in the feature selection literature--instead of trying to define a criterion, we derive one, directly from a clearly specified objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a definition of feature 'relevancy' and 'redundancy', our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and flexibility with small data samples.},
 author = {Gavin Brown and Adam Pocock and Ming-Jie Zhao and Mikel Luj{{\'a}}n},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W2156504490},
 pages = {27--66},
 title = {Conditional likelihood maximisation: a unifying framework for information theoretic feature selection},
 url = {http://jmlr.org/papers/v13/brown12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:brueckner12a,
 abstract = {The standard assumption of identically distributed training and test data is violated when the test data are generated in response to the presence of a predictive model. This becomes apparent, for example, in the context of email spam filtering. Here, email service providers employ spam filters, and spam senders engineer campaign templates to achieve a high rate of successful deliveries despite the filters. We model the interaction between the learner and the data generator as a static game in which the cost functions of the learner and the data generator are not necessarily antagonistic. We identify conditions under which this prediction game has a unique Nash equilibrium and derive algorithms that find the equilibrial prediction model. We derive two instances, the Nash logistic regression and the Nash support vector machine, and empirically explore their properties in a case study on email spam filtering.},
 author = {Michael Br{{\"u}}ckner and Christian Kanzow and Tobias Scheffer},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W2097860933},
 pages = {2617--2654},
 title = {Static prediction games for adversarial learning problems},
 url = {http://jmlr.org/papers/v13/brueckner12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:brunner12a,
 abstract = {Pairwise classification is the task to predict whether the examples a,b of a pair (a,b) belong to the same class or to different classes. In particular, interclass generalization problems can be tr...},
 author = {Carl Brunner and Andreas Fischer and Klaus Luig and Thorsten Thies},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W3020357583},
 pages = {2279--2292},
 title = {Pairwise support vector machines and their application to large scale problems},
 url = {http://jmlr.org/papers/v13/brunner12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:chai12a,
 abstract = {Gaussian process prior with an appropriate likelihood function is a flexible non-parametric model for a variety of learning tasks. One important and standard task is multi-class classification, which is the categorization of an item into one of several fixed classes. A usual likelihood function for this is the multinomial logistic likelihood function. However, exact inference with this model has proved to be difficult because high-dimensional integrations are required. In this paper, we propose a variational approximation to this model, and we describe the optimization of the variational parameters. Experiments have shown our approximation to be tight. In addition, we provide data-independent bounds on the marginal likelihood of the model, one of which is shown to be much tighter than the existing variational mean-field bound in the experiments. We also derive a proper lower bound on the predictive likelihood that involves the Kullback-Leibler divergence between the approximating and the true posterior. We combine our approach with a recently proposed sparse approximation to give a variational sparse approximation to the Gaussian process multi-class model. We also derive criteria which can be used to select the inducing set, and we show the effectiveness of these criteria over random selection in an experiment.},
 author = {Kian Ming A. Chai},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2139036986},
 pages = {1745--1808},
 title = {Variational multinomial logit gaussian process},
 url = {http://jmlr.org/papers/v13/chai12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:chen12a,
 abstract = {In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative filtering. SVDFeature is designed to efficiently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efficient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years.},
 author = {Tianqi Chen and Weinan Zhang and Qiuxia Lu and Kailong Chen and Zhao Zheng and Yong Yu},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W21207210},
 pages = {3619--3622},
 title = {SVDFeature: a toolkit for feature-based collaborative filtering},
 url = {http://jmlr.org/papers/v13/chen12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:chiang12a,
 abstract = {In machine translation, discriminative models have almost entirely supplanted the classical noisy-channel model, but are standardly trained using a method that is reliable only in low-dimensional spaces. Two strands of research have tried to adapt more scalable discriminative training methods to machine translation: the first uses log-linear probability models and either maximum likelihood or minimum risk, and the other uses linear models and large-margin methods. Here, we provide an overview of the latter. We compare several learning algorithms and describe in detail some novel extensions suited to properties of the translation task: no single correct output, a large space of structured outputs, and slow inference. We present experimental results on a large-scale Arabic-English translation task, demonstrating large gains in translation accuracy.},
 author = {David Chiang},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W2158614781},
 pages = {1159--1187},
 title = {Hope and fear for discriminative training of statistical translation models},
 url = {http://jmlr.org/papers/v13/chiang12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:cooper12a,
 abstract = {This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from b...},
 author = {Helen Cooper and Eng-Jon Ong and Nicolas Pugeault and Richard Bowden},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W3027573219},
 pages = {2205--2231},
 title = {Sign language recognition using sub-units},
 url = {http://jmlr.org/papers/v13/cooper12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:cortes12a,
 abstract = {This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination s...},
 author = {Corinna Cortes and Mehryar Mohri and Afshin Rostamizadeh},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W3003414992},
 pages = {795--828},
 title = {Algorithms for learning kernels based on centered alignment},
 url = {http://jmlr.org/papers/v13/cortes12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:crammer12a,
 abstract = {Confidence-weighted online learning is a generalization of margin-based learning of linear classifiers in which the margin constraint is replaced by a probabilistic constraint based on a distribution over classifier weights that is updated online as examples are observed. The distribution captures a notion of confidence on classifier weights, and in some cases it can also be interpreted as replacing a single learning rate by adaptive per-weight rates. Confidence-weighted learning was motivated by the statistical properties of natural-language classification tasks, where most of the informative features are relatively rare. We investigate several versions of confidence-weighted learning that use a Gaussian distribution over weight vectors, updated at each observed example to achieve high probability of correct classification for the example. Empirical evaluation on a range of text-categorization tasks show that our algorithms improve over other state-of-the-art online and batch methods, learn faster in the online setting, and lead to better classifier combination for a type of distributed training commonly used in cloud computing.},
 author = {Koby Crammer and Mark Dredze and Fernando Pereira},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2160648405},
 pages = {1891--1926},
 title = {Confidence-weighted linear classification for text categorization},
 url = {http://jmlr.org/papers/v13/crammer12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:dekel12a,
 abstract = {Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem.},
 author = {Ofer Dekel and Ran Gilad-Bachrach and Ohad Shamir and Lin Xiao},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2130062883},
 pages = {165--202},
 title = {Optimal distributed online prediction using mini-batches},
 url = {http://jmlr.org/papers/v13/dekel12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:dekel12b,
 abstract = {We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on...},
 author = {Ofer Dekel and Claudio Gentile and Karthik Sridharan},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W3014787225},
 pages = {2655--2697},
 title = {Selective sampling and active learning from single and multiple teachers},
 url = {http://jmlr.org/papers/v13/dekel12b.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:desmedt12a,
 abstract = {Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classifiers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern.},
 author = {Tom De Smedt and Walter Daelemans},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2110453538},
 pages = {2063--2067},
 title = {Pattern for python},
 url = {http://jmlr.org/papers/v13/desmedt12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:do12a,
 abstract = {Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efficient convex optimizers with nice guarantees such as no local optima....},
 author = {Trinh Minh Tri Do and Thierry Arti{{\`e}}res},
 journal = {Journal of Machine Learning Research},
 number = {114},
 openalex = {W3017095504},
 pages = {3539--3583},
 title = {Regularized bundle methods for convex and non-convex risks},
 url = {http://jmlr.org/papers/v13/do12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:drineas12a,
 abstract = {The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystrom-based low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n × d matrix A, with n ≫ d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd logn) time, as opposed to the O(nd2) time required by the naive algorithm that involves computing an orthogonal basis for the range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n ≈ d, and the extension to streaming environments.},
 author = {Petros Drineas and Malik Magdon-Ismail and Michael W. Mahoney and David P. Woodruff},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W2616345629},
 pages = {3475--3506},
 title = {Fast approximation of matrix coherence and statistical leverage},
 url = {http://jmlr.org/papers/v13/drineas12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:el-yaniv12a,
 abstract = {We discover a strong relation between two known learning models: stream-based active learning and perfect selective classification (an extreme case of 'classification with a reject option'). For these models, restricted to the realizable case, we show a reduction of active learning to selective classification that preserves fast rates. Applying this reduction to recent results for selective classification, we derive exponential target-independent label complexity speedup for actively learning general (non-homogeneous) linear classifiers when the data distribution is an arbitrary high dimensional mixture of Gaussians. Finally, we study the relation between the proposed technique and existing label complexity measures, including teaching dimension and disagreement coefficient.},
 author = {Ran El-Yaniv and Yair Wiener},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W2108428725},
 pages = {255--279},
 title = {Active learning via perfect selective classification},
 url = {http://jmlr.org/papers/v13/el-yaniv12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:fortin12a,
 abstract = {DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license.},
 author = {F{{\'e}}lix-Antoine Fortin and Fran{\c{c}}ois-Michel De Rainville and Marc-Andr{{\'e}} Gardner and Marc Parizeau and Christian Gagn{{\'e}}},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W2109042184},
 pages = {2171--2175},
 title = {DEAP: evolutionary algorithms made easy},
 url = {http://jmlr.org/papers/v13/fortin12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:frank12a,
 abstract = {Conventional clustering methods typically assume that each data item belongs to a single cluster. This assumption does not hold in general. In order to overcome this limitation, we propose a generative method for clustering vectorial data, where each object can be assigned to multiple clusters. Using a deterministic annealing scheme, our method decomposes the observed data into the contributions of individual clusters and infers their parameters.Experiments on synthetic Boolean data show that our method achieves higher accuracy in the source parameter estimation and superior cluster stability compared to state-of-the-art approaches. We also apply our method to an important problem in computer security known as role mining. Experiments on real-world access control data show performance gains in generalization to new employees against other multi-assignment methods. In challenging situations with high noise levels, our approach maintains its good performance, while alternative state-of-the-art techniques lack robustness.},
 author = {Mario Frank and Andreas P. Streich and David Basin and Joachim M. Buhmann},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2150442700},
 pages = {459--489},
 title = {Multi-assignment clustering for Boolean data},
 url = {http://jmlr.org/papers/v13/frank12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:genovese12a,
 abstract = {We find the minimax rate of convergence in Hausdorff distance for estimating a manifold M of dimension d embedded in RD given a noisy sample from the manifold. Under certain conditions, we show that the optimal rate of convergence is n-2/(2+d). Thus, the minimax rate depends only on the dimension of the manifold, not on the dimension of the space in which M is embedded.},
 author = {Christopher Genovese and Marco Perone-Pacifico and Isabella Verdinelli and Larry Wasserman},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W2962914462},
 pages = {1263--1291},
 title = {Minimax manifold estimation},
 url = {http://jmlr.org/papers/v13/genovese12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:genovese12b,
 abstract = {The lasso is an important method for sparse, high-dimensional regression problems, with efficient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, finding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points.

Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases.

In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefficients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the fixed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the fixed design, noise free, random coefficients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefficients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric.

In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study.},
 author = {Christopher R. Genovese and Jiashun Jin and Larry Wasserman and Zhigang Yao},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2122992239},
 pages = {2107--2143},
 title = {A comparison of the lasso and marginal regression},
 url = {http://jmlr.org/papers/v13/genovese12b.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:gillis12a,
 abstract = {Nonnegative matrix factorization (NMF) has become a very popular technique in machine learning because it automatically extracts meaningful features through a sparse and part-based representation. However, NMF has the drawback of being highly ill-posed, that is, there typically exist many different but equivalent factorizations. In this paper, we introduce a completely new way to obtaining more well-posed NMF problems whose solutions are sparser. Our technique is based on the preprocessing of the nonnegative input data matrix, and relies on the theory of M-matrices and the geometric interpretation of NMF. This approach provably leads to optimal and sparse solutions under the separability assumption of Donoho and Stodden (NIPS, 2003), and, for rank-three matrices, makes the number of exact factorizations finite. We illustrate the effectiveness of our technique on several image datasets.},
 author = {Nicolas Gillis},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W4298427861},
 pages = {3349--3386},
 title = {Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing},
 url = {http://jmlr.org/papers/v13/gillis12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:gould12a,
 abstract = {We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data flows.},
 author = {Stephen Gould},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W148361626},
 pages = {3533--3537},
 title = {DARWIN: a framework for machine learning and computer vision research and development},
 url = {http://jmlr.org/papers/v13/gould12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:grau12a,
 abstract = {Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classifiers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples.},
 author = {Jan Grau and Jens Keilwagen and Andr{{\'e}} Gohr and Berit Haldemann and Stefan Posch and Ivo Grosse},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W771098},
 pages = {1967--1971},
 title = {Jstacs: a java framework for statistical analysis and classification of biological sequences},
 url = {http://jmlr.org/papers/v13/grau12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:gretton12a,
 abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).We present two distribution free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
 author = {Arthur Gretton and Karsten M. Borgwardt and Malte J. Rasch and Bernhard Sch{{\"o}}lkopf and Alexander Smola},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W2212660284},
 pages = {723--773},
 title = {A kernel two-sample test},
 url = {http://jmlr.org/papers/v13/gretton12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:gutmann12a,
 abstract = {We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.},
 author = {Michael U. Gutmann and Aapo Hyv{{\"a}}rinen},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W2138204974},
 pages = {307--361},
 title = {Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics},
 url = {http://jmlr.org/papers/v13/gutmann12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:hanneke12a,
 abstract = {We study the theoretical advantages of active learning over passive learning. Specifically, we prove that, in noise-free classifier learning for VC classes, any passive learning algorithm can be transformed into an active learning algorithm with asymptotically strictly superior label complexity for all nontrivial target functions and distributions. We further provide a general characterization of the magnitudes of these improvements in terms of a novel generalization of the disagreement coefficient. We also extend these results to active learning in the presence of label noise, and find that even under broad classes of noise distributions, we can typically guarantee strict improvements over the known results for passive learning.},
 author = {Steve Hanneke},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W1807800941},
 pages = {1469--1587},
 title = {Activized Learning: Transforming Passive to Active with Improved Label Complexity},
 url = {http://jmlr.org/papers/v13/hanneke12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:hauser12a,
 abstract = {The investigation of directed acyclic graphs (DAGs) encoding the same Markov property, that is the same conditional independence relations of multivariate observational distributions, has a long tradition; many algorithms exist for model selection and structure learning in Markov equivalence classes. In this paper, we extend the notion of Markov equivalence of DAGs to the case of interventional distributions arising from multiple intervention experiments. We show that under reasonable assumptions on the intervention experiments, interventionalMarkov equivalence defines a finer partitioning of DAGs than observational Markov equivalence and hence improves the identifiability of causal models. We give a graph theoretic criterion for two DAGs being Markov equivalent under interventions and show that each interventional Markov equivalence class can, analogously to the observational case, be uniquely represented by a chain graph called interventional essential graph (also known as CPDAG in the observational case). These are key insights for deriving a generalization of the Greedy Equivalence Search algorithm aimed at structure learning from interventional data. This new algorithm is evaluated in a simulation study.},
 author = {Alain Hauser and Peter B{{\"u}}hlmann},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W2103659055},
 pages = {2409--2464},
 title = {Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs},
 url = {http://jmlr.org/papers/v13/hauser12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:hazan12a,
 abstract = {We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efficient and are Hannan-consistent in both the full information and partial feedback settings.},
 author = {Elad Hazan and Satyen Kale},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W169739242},
 pages = {2903--2922},
 title = {Online submodular minimization},
 url = {http://jmlr.org/papers/v13/hazan12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:helmbold12a,
 abstract = {This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.},
 author = {David P. Helmbold and Philip M. Long},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W2616625443},
 pages = {2145--2170},
 title = {On the necessity of irrelevant variables},
 url = {http://jmlr.org/papers/v13/helmbold12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:hennig12a,
 abstract = {Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.},
 author = {Philipp Hennig and Christian J. Schuler},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W1693986406},
 pages = {1809--1837},
 title = {Entropy search for information-efficient global optimization},
 url = {http://jmlr.org/papers/v13/hennig12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:hernandez-orallo12a,
 abstract = {Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method.},
 author = {Jos{{\'e}} Hern{{\'a}}ndez-Orallo and Peter Flach and C{{\`e}}sar Ferri},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2155822698},
 pages = {2813--2869},
 title = {A unified view of performance metrics: translating threshold choice into expected classification loss},
 url = {http://jmlr.org/papers/v13/hernandez-orallo12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:ho12a,
 abstract = {Support vector regression (SVR) and support vector classification (SVC) are popular learning techniques, but their use with kernels is often time consuming. Recently, linear SVC without kernels has...},
 author = {Chia-Hua Ho and Chih-Jen Lin},
 journal = {Journal of Machine Learning Research},
 number = {107},
 openalex = {W3014462869},
 pages = {3323--3348},
 title = {Large-scale linear support vector regression},
 url = {http://jmlr.org/papers/v13/ho12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:huang12a,
 abstract = {We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identified with ne...},
 author = {Gary B. Huang and Andrew Kae and Carl Doersch and Erik Learned-Miller},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W3157352245},
 pages = {363--387},
 title = {Bounding the probability of error for high precision optical character recognition},
 url = {http://jmlr.org/papers/v13/huang12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:huang12b,
 abstract = {The ℓ1-penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1-penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1-penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results.},
 author = {Jian Huang and Cun-Hui Zhang},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W1924642174},
 pages = {1839--1864},
 title = {Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications.},
 url = {http://jmlr.org/papers/v13/huang12b.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:hyttinen12a,
 abstract = {Identifying cause-effect relationships between variables of interest is a central problem in science. Given a set of experiments we describe a procedure that identifies linear models that may contain cycles and latent variables. We provide a detailed description of the model family, full proofs of the necessary and sufficient conditions for identifiability, a search algorithm that is complete, and a discussion of what can be done when the identifiability conditions are not satisfied. The algorithm is comprehensively tested in simulations, comparing it to competing algorithms in the literature. Furthermore, we adapt the procedure to the problem of cellular network inference, applying it to the biologically realistic data of the DREAMchallenges. The paper provides a full theoretical foundation for the causal discovery procedure first presented by Eberhardt et al. (2010) and Hyttinen et al. (2010).},
 author = {Antti Hyttinen and Frederick Eberhardt and Patrik O. Hoyer},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W2116616491},
 pages = {3387--3439},
 title = {Learning linear cyclic causal models with latent variables},
 url = {http://jmlr.org/papers/v13/hyttinen12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:jain12a,
 abstract = {Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework-that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction.},
 author = {Prateek Jain and Brian Kulis and Jason V. Davis and Inderjit S. Dhillon},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W2158602558},
 pages = {519--547},
 title = {Metric and kernel learning using a linear transformation},
 url = {http://jmlr.org/papers/v13/jain12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:kakade12a,
 abstract = {There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate.

Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning.},
 author = {Sham M. Kakade and Shai Shalev-Shwartz and Ambuj Tewari},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W2170912114},
 pages = {1865--1890},
 title = {Regularization techniques for learning with matrices},
 url = {http://jmlr.org/papers/v13/kakade12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:kim12a,
 abstract = {Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufficient conditions for model selection consistency are provided. Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufficient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently. Results of simulation studies as well as real data analysis are given to illustrate that finite sample performances of consistent model selection criteria can be quite different.},
 author = {Yongdai Kim and Sunghoon Kwon and Hosik Choi},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W160307228},
 pages = {1037--1057},
 title = {Consistent model selection criteria on high dimensions},
 url = {http://jmlr.org/papers/v13/kim12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:kim12b,
 abstract = {We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical $M$-estimation. We interpret the KDE based on a radial, positive semi-definite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via $M$-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efficiently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufficient conditions are given for kernelized IRWLS to converge to the global minimizer of the $M$-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the influence function, and experimental results for density estimation and anomaly detection.},
 author = {JooSeuk Kim and Clayton D. Scott},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W2950495522},
 pages = {2529--2565},
 title = {Robust Kernel Density Estimation},
 url = {http://jmlr.org/papers/v13/kim12b.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:kiraly12a,
 abstract = {We propose a novel algebraic algorithmic framework for dealing with probability distributions represented by their cumulants such as the mean and covariance matrix. As an example, we consider the unsupervised learning problem of finding the subspace on which several probability distributions agree. Instead of minimizing an objective function involving the estimated cumulants, we show that by treating the cumulants as elements of the polynomial ring we can directly solve the problem, at a lower computational cost and with higher accuracy. Moreover, the algebraic viewpoint on probability distributions allows us to invoke the theory of algebraic geometry, which we demonstrate in a compact proof for an identifiability criterion.},
 author = {Franz J. Kir{{\'a}}ly and Paul von B{{\"u}}nau and Frank C. Meinecke and Duncan A.J. Blythe and Klaus-Robert M{{\"u}}ller},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W1753618630},
 pages = {855--903},
 title = {Algebraic geometric comparison of probability distributions},
 url = {http://jmlr.org/papers/v13/kiraly12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:kloft12a,
 author = {Marius Kloft and Gilles Blanchard},
 journal = {Journal of Machine Learning Research},
 number = {80},
 pages = {2465--2502},
 title = {On the Convergence Rate of <i>l</i><sub>p</sub>-Norm Multiple Kernel Learning},
 url = {http://jmlr.org/papers/v13/kloft12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:kloft12b,
 abstract = {Security issues are crucial in a number of machine learning applications, especially in scenarios dealing with human activity rather than natural phenomena (e.g., information ranking, spam detectio...},
 author = {Marius Kloft and Pavel Laskov},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W3016149881},
 pages = {3681--3724},
 title = {Security analysis of online centroid anomaly detection},
 url = {http://jmlr.org/papers/v13/kloft12b.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:konidaris12a,
 abstract = {We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.},
 author = {George Konidaris and Ilya Scheidwasser and Andrew Barto},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W2097498341},
 pages = {1333--1371},
 title = {Transfer in reinforcement learning via shared features},
 url = {http://jmlr.org/papers/v13/konidaris12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:kumar12a,
 author = {Sanjiv Kumar and Mehryar Mohri and Ameet Talwalkar},
 journal = {Journal of Machine Learning Research},
 number = {34},
 pages = {981--1006},
 title = {Sampling Methods for the Nystr&#246;m Method},
 url = {http://jmlr.org/papers/v13/kumar12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:lang12a,
 abstract = {A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E3 and R-MAX algorithms. Efficient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efficiency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efficient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques.},
 author = {Tobias Lang and Marc Toussaint and Kristian Kersting},
 journal = {Journal of Machine Learning Research},
 number = {119},
 openalex = {W2119521930},
 pages = {3725--3768},
 title = {Exploration in relational domains for model-based reinforcement learning},
 url = {http://jmlr.org/papers/v13/lang12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:larochelle12a,
 abstract = {Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning.},
 author = {Hugo Larochelle and Michael Mandel and Razvan Pascanu and Yoshua Bengio},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W2105577021},
 pages = {643--669},
 title = {Learning algorithms for the classification restricted Boltzmann machine},
 url = {http://jmlr.org/papers/v13/larochelle12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:lawrence12a,
 abstract = {We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random fields (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter fitting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random field via the graphical lasso.},
 author = {Neil D. Lawrence},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W1548207260},
 pages = {1609--1638},
 title = {A unifying probabilistic perspective for spectral dimensionality reduction: insights and new models},
 url = {http://jmlr.org/papers/v13/lawrence12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:lazaric12a,
 abstract = {In this paper, we report a performance bound for the widely used least-squares policy iteration (LSPI) algorithm. We first consider the problem of policy evaluation in reinforcement learning, that is, learning the value function of a fixed policy, using the least-squares temporal-difference (LSTD) learning method, and report finite-sample analysis for this algorithm. To do so, we first derive a bound on the performance of the LSTD solution evaluated at the states generated by the Markov chain and used by the algorithm to learn an estimate of the value function. This result is general in the sense that no assumption is made on the existence of a stationary distribution for the Markov chain. We then derive generalization bounds in the case when the Markov chain possesses a stationary distribution and is b-mixing. Finally, we analyze how the error at each policy evaluation step is propagated through the iterations of a policy iteration method, and derive a performance bound for the LSPI algorithm.},
 author = {Alessandro Lazaric and Mohammad Ghavamzadeh and R{{\'e}}mi Munos},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W2130599357},
 pages = {3041--3074},
 title = {Finite-Sample Analysis of Least-Squares Policy Iteration},
 url = {http://jmlr.org/papers/v13/lazaric12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:lee12a,
 abstract = {Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a low-dimensional manifold of parameter space along which the regularizer is smooth. (When an l1 regularizer is used to induce sparsity in the solution, for example, this manifold is defined by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a local phase that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identification property and to illustrate the effectiveness of this approach.},
 author = {Sangkyun Lee and Stephen J. Wright},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W2135499062},
 pages = {1705--1744},
 title = {Manifold identification in dual averaging for regularized stochastic online learning},
 url = {http://jmlr.org/papers/v13/lee12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:lee12b,
 abstract = {Recommendation systems are important business applications with significant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms.},
 author = {Joonseok Lee and Mingxuan Sun and Guy Lebanon},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W2132555563},
 pages = {2699--2703},
 title = {PREA: personalized recommendation algorithms toolkit},
 url = {http://jmlr.org/papers/v13/lee12b.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:liu12a,
 abstract = {We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ* + e where e is the noise vector following a Gaussian distribution N(0,σ2I), how to recover the signal (or parameter vector) β* when the signal is sparse?

The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively refines the target signal β*. We show that if X obeys a certain condition, then with a large probability the difference between the solution β estimated by the proposed method and the true solution β* measured in terms of the lp norm (p ≥ 1) is bounded as ||β-β*||p ≤ (C(s-N)1/p√log m + Δ)σ, where C is a constant, s is the number of nonzero entries in β*, the risk of the oracle estimator Δ is independent of m and is much smaller than the first term, and N is the number of entries of β* larger than a certain value in the order of O(σ√log m). The proposed method improves the estimation bound of the standard Dantzig selector approximately from Cs1/p √logmσ to C(s-N)1/p√logmσ where the value N depends on the number of large entries in β*. When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case.},
 author = {Ji Liu and Peter Wonka and Jieping Ye},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W2160106744},
 pages = {1189--1219},
 title = {A multi-stage framework for Dantzig selector and LASSO},
 url = {http://jmlr.org/papers/v13/liu12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:lizotte12a,
 abstract = {We present a general and detailed development of an algorithm for finite-horizon fitted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions. We also present an example of how our methods can be used to construct a real-world decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the field of evidence-based clinical decision support.},
 author = {Daniel J. Lizotte and Michael Bowling and Susan A. Murphy},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W2167062306},
 pages = {3253--3295},
 title = {Linear Fitted-Q Iteration with Multiple Reward Functions.},
 url = {http://jmlr.org/papers/v13/lizotte12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:lui12a,
 abstract = {Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. Th...},
 author = {Yui Man Lui},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W3010264834},
 pages = {3297--3321},
 title = {Human gesture recognition on product manifolds},
 url = {http://jmlr.org/papers/v13/lui12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:ly12a,
 abstract = {A hybrid dynamical system is a mathematical model suitable for describing an extensive spectrum of multi-modal, time-series behaviors, ranging from bouncing balls to air traffic controllers. This paper describes multi-modal symbolic regression (MMSR): a learning algorithm to construct non-linear symbolic representations of discrete dynamical systems with continuous mappings from unlabeled, time-series data. MMSR consists of two subalgorithms--clustered symbolic regression, a method to simultaneously identify distinct behaviors while formulating their mathematical expressions, and transition modeling, an algorithm to infer symbolic inequalities that describe binary classification boundaries. These subalgorithms are combined to infer hybrid dynamical systems as a collection of apt, mathematical expressions. MMSR is evaluated on a collection of four synthetic data sets and outperforms other multi-modal machine learning approaches in both accuracy and interpretability, even in the presence of noise. Furthermore, the versatility of MMSR is demonstrated by identifying and inferring classical expressions of transistor modes from recorded measurements.},
 author = {Daniel L. Ly and Hod Lipson},
 journal = {Journal of Machine Learning Research},
 number = {115},
 openalex = {W2122179032},
 pages = {3585--3618},
 title = {Learning symbolic representations of hybrid dynamical systems},
 url = {http://jmlr.org/papers/v13/ly12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:mahdavi12a,
 abstract = {In this paper we propose a framework for solving constrained online convex optimization problem. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set $\mathcal{K}$ from which the decisions are made. While for simple shapes (e.g. Euclidean ball) the projection is straightforward, for arbitrary complex sets this is the main computational challenge and may be inefficient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring decisions belong to $\mathcal{K}$ for all rounds, we only require that the constraints which define the set $\mathcal{K}$ be satisfied in the long run. We show that our framework can be utilized to solve a relaxed version of online learning with side constraints addressed in \cite{DBLP:conf/colt/MannorT06} and \cite{DBLP:conf/aaai/KvetonYTM08}. By turning the problem into an online convex-concave optimization problem, we propose an efficient algorithm which achieves $\tilde{\mathcal{O}}(\sqrt{T})$ regret bound and $\tilde{\mathcal{O}}(T^{3/4})$ bound for the violation of constraints. Then we modify the algorithm in order to guarantee that the constraints are satisfied in the long run. This gain is achieved at the price of getting $\tilde{\mathcal{O}}(T^{3/4})$ regret bound. Our second algorithm is based on the Mirror Prox method \citep{nemirovski-2005-prox} to solve variational inequalities which achieves $\tilde{\mathcal{\mathcal{O}}}(T^{2/3})$ bound for both regret and the violation of constraints when the domain $\K$ can be described by a finite number of linear constraints. Finally, we extend the result to the setting where we only have partial access to the convex set $\mathcal{K}$ and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our first algorithm.},
 author = {Mehrdad Mahdavi and Rong Jin and Tianbao Yang},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W1520045492},
 pages = {2503--2528},
 title = {Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints},
 url = {http://jmlr.org/papers/v13/mahdavi12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:mahoney12a,
 abstract = {The second eigenvalue of the Laplacian matrix and its associated eigenvector are fundamental features of an undirected graph, and as such they have found widespread use in scientific computing, machine learning, and data analysis. In many applications, however, graphs that arise have several local regions of interest, and the second eigenvector will typically fail to provide information fine-tuned to each local region. In this paper, we introduce a locally-biased analogue of the second eigenvector, and we demonstrate its usefulness at highlighting local properties of data graphs in a semi-supervised manner. To do so, we first view the second eigenvector as the solution to a constrained optimization problem, and we incorporate the local information as an additional constraint; we then characterize the optimal solution to this new problem and show that it can be interpreted as a generalization of a Personalized PageRank vector; and finally, as a consequence, we show that the solution can be computed in nearly-linear time. In addition, we show that this locally-biased vector can be used to compute an approximation to the best partition near an input seed set in a manner analogous to the way in which the second eigenvector of the Laplacian can be used to obtain an approximation to the best partition in the entire input graph. Such a primitive is useful for identifying and refining clusters locally, as it allows us to focus on a local region of interest in a semi-supervised manner. Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to finding locally-biased sparse cuts around an input vertex seed set in social and information networks.},
 author = {Michael W. Mahoney and Lorenzo Orecchia and Nisheeth K. Vishnoi},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W2162148078},
 pages = {2339--2365},
 title = {A local spectral method for graphs: with applications to improving graph partitions and exploring data graphs locally},
 url = {http://jmlr.org/papers/v13/mahoney12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:maillard12a,
 abstract = {We investigate a method for regression that makes use of a randomly generated subspace GP ⊂ F (of finite dimension P) of a given large (possibly infinite) dimensional function space F, for example, L2([0,1]d;R). GP is defined as the span of P random features that are linear combinations of a basis functions of F weighted by random Gaussian i.i.d. coefficients. We show practical motivation for the use of this approach, detail the link that this random projections method share with RKHS and Gaussian objects theory and prove, both in deterministic and random design, approximation error bounds when searching for the best regression function in GP rather than in F, and derive excess risk bounds for a specific regression algorithm (least squares regression in GP). This paper stresses the motivation to study such methods, thus the analysis developed is kept simple for explanations purpose and leaves room for future developments.},
 author = {Odalric-Ambrym Maillard and R{{\'e}}mi Munos},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W2164013269},
 pages = {2735--2772},
 title = {Linear Regression with Random Projections},
 url = {http://jmlr.org/papers/v13/maillard12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:martinez12a,
 abstract = {In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion-the continuous and the categorical model. The continuous model defines each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classifiers, each tuned to a specific emotion category. This model explains, among other findings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difficult time justifying this latter finding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justifies the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly configural. According to this model, the major task for the classification of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in studies of human perception, social interactions and disorders.},
 author = {Aleix Martinez and Shichuan Du},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W2122361883},
 pages = {1589--1608},
 title = {A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives},
 url = {http://jmlr.org/papers/v13/martinez12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:maurer12a,
 abstract = {We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an infinite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels.},
 author = {Andreas Maurer and Massimiliano Pontil},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2116343548},
 pages = {671--690},
 title = {Structured sparsity and generalization},
 url = {http://jmlr.org/papers/v13/maurer12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:may12a,
 abstract = {In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest.

In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour.

We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout.},
 author = {Benedict C. May and Nathan Korda and Anthony Lee and David S. Leslie},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W1618543586},
 pages = {2069--2106},
 title = {Optimistic Bayesian sampling in contextual-bandit problems},
 url = {http://jmlr.org/papers/v13/may12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:mazumder12a,
 abstract = {We consider the sparse inverse covariance regularization problem or graphical lasso with regularization parameter $ρ$. Suppose the co- variance graph formed by thresholding the entries of the sample covariance matrix at $ρ$ is decomposed into connected components. We show that the vertex-partition induced by the thresholded covariance graph is exactly equal to that induced by the estimated concentration graph. This simple rule, when used as a wrapper around existing algorithms, leads to enormous performance gains. For large values of $ρ$, our proposal splits a large graphical lasso problem into smaller tractable problems, making it possible to solve an otherwise infeasible large scale graphical lasso problem.},
 author = {Rahul Mazumder and Trevor Hastie},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W2950215113},
 pages = {781--794},
 title = {Exact covariance thresholding into connected components for large-scale Graphical Lasso},
 url = {http://jmlr.org/papers/v13/mazumder12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:minsker12a,
 abstract = {We present a new active learning algorithm based on nonparametric estimators of the regression function. Our investigation provides probabilistic bounds for the rates of convergence of the generalization error achievable by proposed method over a broad class of underlying distributions. We also prove minimax lower bounds which show that the obtained rates are almost tight.},
 author = {Stanislav Minsker},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W2963885708},
 pages = {67--90},
 title = {Plug-in Approach to Active Learning},
 url = {http://jmlr.org/papers/v13/minsker12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:mohan12a,
 abstract = {The problem of minimizing the rank of a matrix subject to affine constraints has applications in several areas including machine learning, and is known to be NP-hard. A tractable relaxation for thi...},
 author = {Karthik Mohan and Maryam Fazel},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W3023386002},
 pages = {3441--3473},
 title = {Iterative reweighted algorithms for matrix rank minimization},
 url = {http://jmlr.org/papers/v13/mohan12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:nayak12a,
 abstract = {We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is first transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences.},
 author = {Sunita Nayak and Kester Duncan and Sudeep Sarkar and Barbara Loeding},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W2168068018},
 pages = {2589--2615},
 title = {Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs},
 url = {http://jmlr.org/papers/v13/nayak12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:negahban12a,
 abstract = {We consider the matrix completion problem under a form of row/column weighted entrywise sampling, including the case of uniform entrywise sampling as a special case. We analyze the associated random observation operator, and prove that with high probability, it satisfies a form of restricted strong convexity with respect to weighted Frobenius norm. Using this property, we obtain as corollaries a number of error bounds on matrix completion in the weighted Frobenius norm under noisy sampling and for both exact and near low-rank matrices. Our results are based on measures of the "spikiness" and "low-rankness" of matrices that are less restrictive than the incoherence conditions imposed in previous work. Our technique involves an $M$-estimator that includes controls on both the rank and spikiness of the solution, and we establish non-asymptotic error bounds in weighted Frobenius norm for recovering matrices lying with $\ell_q$-"balls" of bounded spikiness. Using information-theoretic methods, we show that no algorithm can achieve better estimates (up to a logarithmic factor) over these same sets, showing that our conditions on matrices and associated rates are essentially optimal.},
 author = {Sahand Negahban and Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W2111297856},
 pages = {1665--1697},
 title = {Restricted strong convexity and weighted matrix completion: Optimal bounds with noise},
 url = {http://jmlr.org/papers/v13/negahban12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:nelson12a,
 abstract = {Classifiers are often used to detect miscreant activities. We study how an adversary can systematically query a classifier to elicit information that allows the attacker to evade detection while incurring a near-minimal cost of modifying their intended malfeasance. We generalize the theory of Lowd and Meek (2005) to the family of convex-inducing classifiers that partition their feature space into two sets, one of which is convex. We present query algorithms for this family that construct undetected instances of approximately minimal cost using only polynomially-many queries in the dimension of the space and in the level of approximation. Our results demonstrate that nearoptimal evasion can be accomplished for this family without reverse engineering the classifier's decision boundary. We also consider general lp costs and show that near-optimal evasion on the family of convex-inducing classifiers is generally efficient for both positive and negative convexity for all levels of approximation if p = 1.},
 author = {Blaine Nelson and Benjamin I. P. Rubinstein and Ling Huang and Anthony D. Joseph and Steven J. Lee and Satish Rao and J. D. Tygar},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W2151773168},
 pages = {1293--1332},
 title = {Query strategies for evading convex-inducing classifiers},
 url = {http://jmlr.org/papers/v13/nelson12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:nickisch12a,
 abstract = {The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean field. Scalable and efficient inference in fully-connected undirected graphical models or Markov random fields with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX files to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classification as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit.},
 author = {Hannes Nickisch},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W2165748376},
 pages = {1699--1703},
 title = {glm-ie: generalised linear models inference & estimation toolbox},
 url = {http://jmlr.org/papers/v13/nickisch12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:orabona12a,
 abstract = {In recent years there has been a lot of interest in designing principled classification algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach.

Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufficient to reach good solutions. Experiments on standard benchmark databases support our claims.},
 author = {Francesco Orabona and Luo Jie and Barbara Caputo},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W2132333011},
 pages = {227--253},
 title = {Multi kernel learning with online-batch optimization},
 url = {http://jmlr.org/papers/v13/orabona12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:park12a,
 abstract = {This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper.},
 author = {Chiwoo Park and Jianhua Z. Huang and Yu Ding},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W38425204},
 pages = {775--779},
 title = {GPLP: a local and parallel computation toolbox for Gaussian process regression},
 url = {http://jmlr.org/papers/v13/park12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:parrado12a,
 abstract = {This paper presents the prior PAC-Bayes bound and explores its capabilities as a tool to provide tight predictions of SVMs' generalization. The computation of the bound involves estimating a prior of the distribution of classifiers from the available data, and then manipulating this prior in the usual PAC-Bayes generalization bound. We explore two alternatives: to learn the prior from a separate data set, or to consider an expectation prior that does not need this separate data set. The prior PAC-Bayes bound motivates two SVM-like classification algorithms, prior SVM and ν-prior SVM, whose regularization term pushes towards the minimization of the prior PAC-Bayes bound. The experimental work illustrates that the new bounds can be significantly tighter than the original PAC-Bayes bound when applied to SVMs, and among them the combination of the prior PAC-Bayes bound and the prior SVM algorithm gives the tightest bound.},
 author = {Emilio Parrado-Hern{{\'a}}ndez and Amiran Ambroladze and John Shawe-Taylor and Shiliang Sun},
 journal = {Journal of Machine Learning Research},
 number = {112},
 openalex = {W2130464645},
 pages = {3507--3531},
 title = {PAC-bayes bounds with data dependent priors},
 url = {http://jmlr.org/papers/v13/parrado12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:piccolo12a,
 abstract = {Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classification analyses in a systematic yet flexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net.},
 author = {Stephen R. Piccolo and Lewis J. Frey},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2118017676},
 pages = {555--559},
 title = {ML-Flex: a flexible toolbox for performing classification analyses in parallel},
 url = {http://jmlr.org/papers/v13/piccolo12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:qin12a,
 abstract = {We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm which incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty $l_1/l_2$-norm and the $l_1/l_\infty$-norm. We propose a unified framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efficiently solved. As the core building-block of this framework, we develop new algorithms using an alternating partial-linearization/splitting technique, and we prove that the accelerated versions of these algorithms require $O(\frac{1}{\sqrt{\epsilon}})$ iterations to obtain an $\epsilon$-optimal solution. To demonstrate the efficiency and relevance of our algorithms, we test them on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms.},
 author = {Zhiwei Qin and Donald Goldfarb},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W2963198961},
 pages = {1435--1468},
 title = {Structured Sparsity via Alternating Direction Methods},
 url = {http://jmlr.org/papers/v13/qin12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:ramsahai12a,
 abstract = {Conditional independence relations involving latent variables do not necessarily imply observable independences. They may imply inequality constraints on observable parameters and causal bounds, which can be used for falsification and identification. The literature on computing such constraints often involve a deterministic underlying data generating process in a counterfactual framework. If an analyst is ignorant of the nature of the underlying mechanisms then they may wish to use a model which allows the underlying mechanisms to be probabilistic. A method of computation for a weaker model without any determinism is given here and demonstrated for the instrumental variable model, though applicable to other models. The approach is based on the analysis of mappings with convex polytopes in a decision theoretic framework and can be implemented in readily available polyhedral computation software. Well known constraints and bounds are replicated in a probabilistic model and novel ones are computed for instrumental variable models without non-deterministic versions of the randomization, exclusion restriction and monotonicity assumptions respectively.},
 author = {Roland R. Ramsahai},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W1541738043},
 pages = {829--848},
 title = {Causal bounds and observable constraints for non-deterministic models},
 url = {http://jmlr.org/papers/v13/ramsahai12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:raskutti12a,
 abstract = {Sparse additive models are families of d-variate functions with the additive decomposition f* = Σj∈S fj*, where S is an unknown subset of cardinality s < d. In this paper, we consider the case where each univariate component function fj* lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f* based on kernels combined with l1-type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2(P) and L2(Pn) norms over the class Fd,s,H of sparse additive models with each univariate function fj* in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L2(P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much faster estimation rates are possible for any sparsity s = Ω(√n), showing that global boundedness is a significant restriction in the high-dimensional setting.},
 author = {Garvesh Raskutti and Martin J. Wainwright and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W2153542182},
 pages = {389--427},
 title = {Minimax-optimal rates for sparse additive models over kernel classes via convex programming},
 url = {http://jmlr.org/papers/v13/raskutti12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:raykar12a,
 abstract = {With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Since we do not have control over the quality of the annotators, very often the annotations can be dominated by spammers, defined as annotators who assign labels randomly without actually looking at the instance. Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels. In this paper we propose an empirical Bayesian algorithm called SpEMthat iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators. The algorithm is motivated by defining a spammer score that can be used to rank the annotators. Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a significantly smaller number of annotators.},
 author = {Vikas C. Raykar and Shipeng Yu},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2119830539},
 pages = {491--518},
 title = {Eliminating spammers and ranking annotators for crowdsourced labeling tasks},
 url = {http://jmlr.org/papers/v13/raykar12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:rejchel12a,
 abstract = {The problem of ranking is to predict or to guess the ordering between objects on the basis of their observed features. In this paper we consider ranking estimators that minimize the empirical convex risk. We prove generalization bounds for the excess risk of such estimators with rates that are faster than 1/√n. We apply our results to commonly used ranking algorithms, for instance boosting or support vector machines. Moreover, we study the performance of considered estimators on real data sets.},
 author = {Wojciech Rejchel},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W1572386387},
 pages = {1373--1392},
 title = {On ranking and generalization bounds},
 url = {http://jmlr.org/papers/v13/rejchel12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:rieck12a,
 abstract = {Strings and sequences are ubiquitous in many areas of data analysis. However, only few learning methods can be directly applied to this form of data. We present Sally, a tool for embedding strings in vector spaces that allows for applying a wide range of learning methods to string data. Sally implements a generalized form of the bag-of-words model, where strings are mapped to a vector space that is spanned by a set of string features, such as words or n-grams of words. The implementation of Sally builds on efficient string algorithms and enables processing millions of strings and features. The tool supports several data formats and is capable of interfacing with common learning environments, such as Weka, Shogun, Matlab, or Pylab. Sally has been successfully applied for learning with natural language text, DNA sequences and monitored program behavior.},
 author = {Konrad Rieck and Christian Wressnegger and Alexander Bikadorov},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W2131182686},
 pages = {3247--3251},
 title = {Sally: a tool for embedding strings in vector spaces},
 url = {http://jmlr.org/papers/v13/rieck12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:rinaldo12a,
 abstract = {High density clusters can be characterized by the connected components of a level set L(λ) = {x : p(x) > λ} of the underlying probability density function p generating the data, at some appropriate level λ ≥ 0. The complete hierarchical clustering can be characterized by a cluster tree T = ∪λ L(λ). In this paper, we study the behavior of a density level set estimate L(λ) and cluster tree estimate T based on a kernel density estimator with kernel bandwidth h. We define two notions of instability to measure the variability of L(λ) and T as a function of h, and investigate the theoretical properties of these instability measures.},
 author = {Alessandro Rinaldo and Aarti Singh and Rebecca Nugent and Larry Wasserman},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W1584683589},
 pages = {905--948},
 title = {Stability of density-based clustering},
 url = {http://jmlr.org/papers/v13/rinaldo12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:rubinstein12a,
 abstract = {The Sample Compression Conjecture of Littlestone & Warmuth has remained unsolved for a quarter century. While maximum classes (concept classes meeting Sauer's Lemma with equality) can be compressed, the compression of general concept classes reduces to compressing maximal classes (classes that cannot be expanded without increasing VC dimension). Two promising ways forward are: embedding maximal classes into maximum classes with at most a polynomial increase to VC dimension, and compression via operating on geometric representations. This paper presents positive results on the latter approach and a first negative result on the former, through a systematic investigation of finite maximum classes. Simple arrangements of hyperplanes in hyperbolic space are shown to represent maximum classes, generalizing the corresponding Euclidean result. We show that sweeping a generic hyperplane across such arrangements forms an unlabeled compression scheme of size VC dimension and corresponds to a special case of peeling the one-inclusion graph, resolving a recent conjecture of Kuzmin & Warmuth. A bijection between finite maximum classes and certain arrangements of piecewise-linear (PL) hyperplanes in either a ball or Euclidean space is established. Finally we show that d-maximum classes corresponding to PL-hyperplane arrangements in Rd have cubical complexes homeomorphic to a d-ball, or equivalently complexes that are manifolds with boundary. A main result is that PL arrangements can be swept by a moving hyperplane to unlabeled d-compress any finite maximum class, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary is that some d-maximal classes cannot be embedded into any maximum class of VC-dimension d+k, for any constant k. The construction of the PL sweeping involves Pachner moves on the one-inclusion graph, corresponding to moves of a hyperplane across the intersection of d other hyperplanes. This extends the well known Pachner moves for triangulations to cubical complexes.},
 author = {Benjamin I.P. Rubinstein and J. Hyam Rubinstein},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W2154955030},
 pages = {1221--1261},
 title = {A geometric approach to sample compression},
 url = {http://jmlr.org/papers/v13/rubinstein12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:sabato12a,
 abstract = {In the supervised learning setting termed Multiple-Instance Learning (MIL), the examples are bags of instances, and the bag label is a function of the labels of its instances. Typically, this function is the Boolean OR. The learner observes a sample of bags and the bag labels, but not the instance labels that determine the bag labels. The learner is then required to emit a classification rule for bags based on the sample. MIL has numerous applications, and many heuristic algorithms have been used successfully on this problem, each adapted to specific settings or applications. In this work we provide a unified theoretical analysis for MIL, which holds for any underlying hypothesis class, regardless of a specific application or problem domain. We show that the sample complexity of MIL is only poly-logarithmically dependent on the size of the bag, for any underlying hypothesis class. In addition, we introduce a new PAC-learning algorithm for MIL, which uses a regular supervised learning algorithm as an oracle. We prove that efficient PAC-learning for MIL can be generated from any efficient non-MIL supervised learning algorithm that handles one-sided error. The computational complexity of the resulting algorithm is only polynomially dependent on the bag size.},
 author = {Sivan Sabato and Naftali Tishby},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W1790119552},
 pages = {2999--3039},
 title = {Multi-instance learning with any hypothesis class},
 url = {http://jmlr.org/papers/v13/sabato12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:salman12a,
 abstract = {We describe a quantum algorithm for computing the intersection of two sets and its application to associative memory. The algorithm is based on a modification of Grover's quantum search algorithm (Grover, 1996). We present algorithms for pattern retrieval, pattern completion, and pattern correction. We show that the quantum associative memory can store an exponential number of memories and retrieve them in sub-exponential time. We prove that this model has advantages over known classical associative memories as well as previously proposed quantum models.},
 author = {Tamer Salman and Yoram Baram},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W66231363},
 pages = {3177--3206},
 title = {Quantum set intersection and its application to associative memory},
 url = {http://jmlr.org/papers/v13/salman12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:schnitzer12a,
 abstract = {'Hubness' has recently been identified as a general problem of high dimensional data spaces, manifesting itself in the emergence of objects, so-called hubs, which tend to be among the k nearest nei...},
 author = {Dominik Schnitzer and Arthur Flexer and Markus Schedl and Gerhard Widmer},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W3019839503},
 pages = {2871--2902},
 title = {Local and global scaling reduce hubs in space},
 url = {http://jmlr.org/papers/v13/schnitzer12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:shalit12a,
 abstract = {When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches to minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low-rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is costly to compute, and so is the projection operator that approximates it, we describe another retraction that can be computed efficiently. It has run time and memory complexity of O((n+m)k) for a rank-k matrix of dimension m×n, when using an online procedure with rank-one gradients. We use this algorithm, LORETA, to learn a matrix-form similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive-aggressive approach in a factorized model, and also improves over a full model trained on pre-selected features using the same memory requirements. We further adapt LORETA to learn positive semi-definite low-rank matrices, providing an online algorithm for low-rank metric learning. LORETA also shows consistent improvement over standard weakly supervised methods in a large (1600 classes and 1 million images, using ImageNet) multilabel image classification task.},
 author = {Uri Shalit and Daphna Weinshall and Gal Chechik},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W2121667911},
 pages = {429--458},
 title = {Online learning in the embedded manifold of low-rank matrices},
 url = {http://jmlr.org/papers/v13/shalit12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:shen12a,
 abstract = {The success of many machine learning and pattern recognition methods relies heavily upon the identification of an appropriate distance metric on the input data. It is often beneficial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed BoostMetric, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive definite. Semidefinite programming is often used to enforce this constraint, but does not scale well and easy to implement. BoostMetric is instead based on the observation that any positive semidefinite matrix can be decomposed into a linear combination of trace-one rank-one matrices. BoostMetric thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting methods are easy to implement, efficient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semidefinite matrix with trace and rank being one rather than a classifier or regressor. Experiments on various datasets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classification accuracy and running time.},
 author = {Chunhua Shen and Junae Kim and Lei Wang and Anton van den Hengel},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W2951838859},
 pages = {1007--1036},
 title = {Positive Semidefinite Metric Learning Using Boosting-like Algorithms},
 url = {http://jmlr.org/papers/v13/shen12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:skolidis12a,
 abstract = {We propose a novel model for meta-generalisation, that is, performing prediction on novel tasks based on information from multiple different but related tasks. The model is based on two coupled Gaussian processes with structured covariance function; one model performs predictions by learning a constrained covariance function encapsulating the relations between the various training tasks, while the second model determines the similarity of new tasks to previously seen tasks. We demonstrate empirically on several real and synthetic data sets both the strengths of the approach and its limitations due to the distributional assumptions underpinning it.},
 author = {Grigorios Skolidis and Guido Sanguinetti},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W2112005937},
 pages = {691--721},
 title = {A case study on meta-generalising: a Gaussian processes approach},
 url = {http://jmlr.org/papers/v13/skolidis12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:snoek12a,
 abstract = {While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which find latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can find representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it finds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically significant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available.},
 author = {Jasper Snoek and Ryan P. Adams and Hugo Larochelle},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W2135306627},
 pages = {2567--2588},
 title = {Nonparametric guidance of autoencoder representations using label information},
 url = {http://jmlr.org/papers/v13/snoek12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:solnon12a,
 abstract = {In this paper we study the kernel multiple ridge regression framework, which we refer to as multi-task regression, using penalization techniques. The theoretical analysis of this problem shows that the key element appearing for an optimal calibration is the covariance matrix of the noise between the different tasks. We present a new algorithm to estimate this covariance matrix, based on the concept of minimal penalty, which was previously used in the single-task regression framework to estimate the variance of the noise. We show, in a non-asymptotic setting and under mild assumptions on the target function, that this estimator converges towards the covariance matrix. Then plugging this estimator into the corresponding ideal penalty leads to an oracle inequality. We illustrate the behavior of our algorithm on synthetic examples.},
 author = {Matthieu Solnon and Sylvain Arlot and Francis Bach},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W2953300141},
 pages = {2773--2812},
 title = {Multi-task Regression using Minimal Penalties},
 url = {http://jmlr.org/papers/v13/solnon12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:song12a,
 abstract = {We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criteri...},
 author = {Le Song and Alex Smola and Arthur Gretton and Justin Bedo and Karsten Borgwardt},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W3003633917},
 pages = {1393--1434},
 title = {Feature selection via dependence maximization},
 url = {http://jmlr.org/papers/v13/song12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:su12a,
 abstract = {Assessing treatment effects in observational studies is a multifaceted problem that not only involves heterogeneous mechanisms of how the treatment or cause is exposed to subjects, known as propensity, but also differential causal effects across sub-populations. We introduce a concept termed the facilitating score to account for both the confounding and interacting impacts of covariates on the treatment effect. Several approaches for estimating the facilitating score are discussed. In particular, we put forward a machine learning method, called causal inference tree (CIT), to provide a piecewise constant approximation of the facilitating score. With interpretable rules, CIT splits data in such a way that both the propensity and the treatment effect become more homogeneous within each resultant partition. Causal inference at different levels can be made on the basis of CIT. Together with an aggregated grouping procedure, CIT stratifies data into strata where causal effects can be conveniently assessed within each. Besides, a feasible way of predicting individual causal effects (ICE) is made available by aggregating ensemble CIT models. Both the stratified results and the estimated ICE provide an assessment of heterogeneity of causal effects and can be integrated for estimating the average causal effect (ACE). Mean square consistency of CIT is also established. We evaluate the performance of proposed methods with simulations and illustrate their use with the NSW data in Dehejia and Wahba (1999) where the objective is to assess the impact of a labor training program, the National SupportedWork (NSW) demonstration, on post-intervention earnings.},
 author = {Xiaogang Su and Joseph Kang and Juanjuan Fan and Richard A. Levine and Xin Yan},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W162576350},
 pages = {2955--2994},
 title = {Facilitating score and causal inference trees for large observational studies},
 url = {http://jmlr.org/papers/v13/su12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:tahan12a,
 abstract = {This paper proposes several novel methods, based on machine learning, to detect malware in executable files without any need for preprocessing, such as unpacking or disassembling. The basic method (Mal-ID) is a new static (form-based) analysis methodology that uses common segment analysis in order to detect malware files. By using common segment analysis, Mal-ID is able to discard malware parts that originate from benign code. In addition, Mal-ID uses a new kind of feature, termed meta-feature, to better capture the properties of the analyzed segments. Rather than using the entire file, as is usually the case with machine learning based techniques, the new approach detects malware on the segment level. This study also introduces two Mal-ID extensions that improve the Mal-ID basic method in various aspects. We rigorously evaluated Mal-ID and its two extensions with more than ten performance measures, and compared them to the highly rated boosted decision tree method under identical settings. The evaluation demonstrated that Mal-ID and the two Mal-ID extensions outperformed the boosted decision tree method in almost all respects. In addition, the results indicated that by extracting meaningful features, it is sufficient to employ one simple detection rule for classifying executable files.},
 author = {Gil Tahan and Lior Rokach and Yuval Shahar},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W62298501},
 pages = {949--979},
 title = {Mal-ID: automatic malware detection using common segment analysis and meta-features},
 url = {http://jmlr.org/papers/v13/tahan12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:tamar12a,
 abstract = {In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent's knowledge. Our method relies on a novel definition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem.},
 author = {Aviv Tamar and Dotan Di Castro and Ron Meir},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W2134110604},
 pages = {1927--1966},
 title = {Integrating a partial model into model free reinforcement learning},
 url = {http://jmlr.org/papers/v13/tamar12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:telgarsky12a,
 abstract = {Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: - Weak learnability aids the whole loss family: for any ε&gt;0, O(ln(1/ε)) iterations suffice to produce a predictor with empirical risk ε-close to the infimum; - The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O(ln(1/ε)); - Arbitrary instances may be decomposed into the above two, granting rate O(1/ε), with a matching lower bound provided for the logistic loss.},
 author = {Matus Telgarsky},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W2963002936},
 pages = {561--606},
 title = {A Primal-Dual Convergence Analysis of Boosting},
 url = {http://jmlr.org/papers/v13/telgarsky12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:tsamardinos12a,
 abstract = {We present methods able to predict the presence and strength of conditional and unconditional dependencies (correlations) between two variables Y and Z never jointly measured on the same samples, based on multiple data sets measuring a set of common variables. The algorithms are specializations of prior work on learning causal structures from overlapping variable sets. This problem has also been addressed in the field of statistical matching. The proposed methods are applied to a wide range of domains and are shown to accurately predict the presence of thousands of dependencies. Compared against prototypical statistical matching algorithms and within the scope of our experiments, the proposed algorithms make predictions that are better correlated with the sample estimates of the unknown parameters on test data ; this is particularly the case when the number of commonly measured variables is low.

The enabling idea behind the methods is to induce one or all causal models that are simultaneously consistent with (fit) all available data sets and prior knowledge and reason with them. This allows constraints stemming from causal assumptions (e.g., Causal Markov Condition, Faithfulness) to propagate. Several methods have been developed based on this idea, for which we propose the unifying name Integrative Causal Analysis (INCA). A contrived example is presented demonstrating the theoretical potential to develop more general methods for co-analyzing heterogeneous data sets. The computational experiments with the novel methods provide evidence that causally-inspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference. Code, scripts, and data are available at www.mensxmachina.org.},
 author = {Ioannis Tsamardinos and Sofia Triantafillou and Vincenzo Lagani},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W2164549187},
 pages = {1097--1157},
 title = {Towards integrative causal analysis of heterogeneous data sets and studies},
 url = {http://jmlr.org/papers/v13/tsamardinos12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:vanerven12a,
 abstract = {Mixability of a loss characterizes fast rates in the online learning setting of prediction with expert advice. The determination of the mixability constant for binary losses is straight forward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses. We then extend this result to multiclass proper losses where there are few existing results. We show that mixability is governed by the maximum eigenvalue of the Hessian of the Bayes risk, relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses, we also show how to carry the results across to improper losses.},
 author = {Tim van Erven and Mark D. Reid and Robert C. Williamson},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2288816805},
 pages = {1639--1663},
 title = {Mixability is Bayes Risk Curvature Relative to Log Loss},
 url = {http://jmlr.org/papers/v13/vanerven12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:verstraeten12a,
 abstract = {Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several crossvalidation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http: //organic.elis.ugent.be/oger.},
 author = {David Verstraeten and Benjamin Schrauwen and Sander Dieleman and Philemon Brakel and Pieter Buteneers and Dejan Pecevski},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W165885910},
 pages = {2995--2998},
 title = {Oger: modular learning architectures for large-scale sequential processing},
 url = {http://jmlr.org/papers/v13/verstraeten12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:voevodski12a,
 abstract = {Given a point set S and an unknown metric d on S, we study the problem of efficiently partitioning S into k clusters while querying few distances between the points. In our model we assume that we have access to one versus all queries that given a point s ∈ S return the distances between s and all other points. We show that given a natural assumption about the structure of the instance, we can efficiently find an accurate clustering using only O(k) distance queries. Our algorithm uses an active selection strategy to choose a small set of points that we call landmarks, and considers only the distances between landmarks and other points to produce a clustering. We use our procedure to cluster proteins by sequence similarity. This setting nicely fits our model because we can use a fast sequence database search program to query a sequence against an entire data set. We conduct an empirical study that shows that even though we query a small fraction of the distances between the points, we produce clusterings that are close to a desired clustering given by manual classification.},
 author = {Konstantin Voevodski and Maria-Florina Balcan and Heiko R{{\"o}}glin and Shang-Hua Teng and Yu Xia},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W2097979671},
 pages = {203--225},
 title = {Active clustering of biological sequences},
 url = {http://jmlr.org/papers/v13/voevodski12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:wang12a,
 abstract = {We consider the problem of parsing human poses and recognizing their actions in static images with part-based models. Most previous work in part-based models only considers rigid parts (e.g., torso...},
 author = {Yang Wang and Duan Tran and Zicheng Liao and David Forsyth},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W3005794849},
 pages = {3075--3102},
 title = {Discriminative hierarchical part-based models for human parsing and action recognition},
 url = {http://jmlr.org/papers/v13/wang12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:wang12b,
 abstract = {Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Specifically, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efficiency both in time and space during training and prediction.},
 author = {Zhuang Wang and Koby Crammer and Slobodan Vucetic},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2095895508},
 pages = {3103--3131},
 title = {Breaking the curse of kernelization: budgeted stochastic gradient descent for large-scale SVM training},
 url = {http://jmlr.org/papers/v13/wang12b.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:xue12a,
 abstract = {The varying-coefficient model is flexible and powerful for modeling the dynamic changes of regression coefficients. It is important to identify significant covariates associated with response varia...},
 author = {Lan Xue and Annie Qu},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W3000391082},
 pages = {1973--1998},
 title = {Variable selection in high-dimensional varying-coefficient models with global optimality},
 url = {http://jmlr.org/papers/v13/xue12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:yan12a,
 abstract = {Sparsity-inducing multiple kernel Fisher discriminant analysis (MK-FDA) has been studied in the literature. Building on recent advances in non-sparse multiple kernel learning (MKL), we propose a non-sparse version of MK-FDA, which imposes a general lp norm regularisation on the kernel weights. We formulate the associated optimisation problem as a semi-infinite program (SIP), and adapt an iterative wrapper algorithm to solve it. We then discuss, in light of latest advances in MKL optimisation techniques, several reformulations and optimisation strategies that can potentially lead to significant improvements in the efficiency and scalability of MK-FDA. We carry out extensive experiments on six datasets from various application areas, and compare closely the performance of lp MK-FDA, fixed norm MK-FDA, and several variants of SVM-based MKL (MK-SVM). Our results demonstrate that lp MK-FDA improves upon sparse MK-FDA in many practical situations. The results also show that on image categorisation problems, lp MK-FDA tends to outperform its SVM counterpart. Finally, we also discuss the connection between (MK-)FDA and (MK-)SVM, under the unified framework of regularised kernel machines.},
 author = {Fei Yan and Josef Kittler and Krystian Mikolajczyk and Atif Tahir},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2156505030},
 pages = {607--642},
 title = {Non-sparse multiple kernel fisher discriminant analysis},
 url = {http://jmlr.org/papers/v13/yan12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:ying12a,
 abstract = {The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML...},
 author = {Yiming Ying and Peng Li},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2998119097},
 pages = {1--26},
 title = {Distance metric learning with eigenvalue optimization},
 url = {http://jmlr.org/papers/v13/ying12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:yuan12a,
 abstract = {Recently, Yuan et al. (2010) conducted a comprehensive comparison on software for L1-regularized classification. They concluded that a carefully designed coordinate descent implementation CDN is the fastest among state-of-the-art solvers. In this paper, we point out that CDN is less competitive on loss functions that are expensive to compute. In particular, CDN for logistic regression is much slower than CDN for SVM because the logistic loss involves expensive exp/log operations.

In optimization, Newton methods are known to have fewer iterations although each iteration costs more. Because solving the Newton sub-problem is independent of the loss calculation, this type of methods may surpass CDN under some circumstances. In L1-regularized classification, GLMNET by Friedman et al. is already a Newton-type method, but experiments in Yuan et al. (2010) indicated that the existing GLMNET implementation may face difficulties for some largescale problems. In this paper, we propose an improved GLMNET to address some theoretical and implementation issues. In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution. By a careful design to adjust the effort for each iteration, our method is efficient for both loosely or strictly solving the optimization problem. Experiments demonstrate that our improved GLMNET ismore efficient than CDN for L1-regularized logistic regression.},
 author = {Guo-Xun Yuan and Chia-Hua Ho and Chih-Jen Lin},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W2205688503},
 pages = {1999--2030},
 title = {An improved GLMNET for L1-regularized logistic regression},
 url = {http://jmlr.org/papers/v13/yuan12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zeng12a,
 abstract = {Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models. The current version includes BP algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future. Interested users may also extend BP algorithms for learning more complicated topic models. The source codes are freely available under the GNU General Public Licence, Version 1.0 at https://mloss.org/software/view/399/.},
 author = {Jia Zeng},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2951681883},
 pages = {2233--2236},
 title = {A Topic Modeling Toolbox Using Belief Propagation},
 url = {http://jmlr.org/papers/v13/zeng12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zhang12a,
 abstract = {This paper studies the construction of a refinement kernel for a given operator-valued reproducing kernel such that the vector-valued reproducing kernel Hilbert space of the refinement kernel contains that of the given kernel as a subspace. The study is motivated from the need of updating the current operator-valued reproducing kernel in multi-task learning when underfitting or overfitting occurs. Numerical simulations confirm that the established refinement kernel method is able to meet this need. Various characterizations are provided based on feature maps and vector-valued integral representations of operator-valued reproducing kernels. Concrete examples of refining translation invariant and finite Hilbert-Schmidt operator-valued reproducing kernels are provided. Other examples include refinement of Hessian of scalar-valued translation-invariant kernels and transformation kernels. Existence and properties of operator-valued reproducing kernels preserved during the refinement process are also investigated.},
 author = {Haizhang Zhang and Yuesheng Xu and Qinghui Zhang},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2122176428},
 pages = {91--136},
 title = {Refinement of operator-valued reproducing kernels},
 url = {http://jmlr.org/papers/v13/zhang12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zhang12b,
 abstract = {In this paper we propose a novel framework for the construction of sparsity-inducing priors. In particular, we define such priors as a mixture of exponential power distributions with a generalized inverse Gaussian density (EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and the special cases include Gaussian scale mixtures and Laplace scale mixtures. Furthermore, Laplace scale mixtures can subserve a Bayesian framework for sparse learning with nonconvex penalization. The densities of EP-GIG can be explicitly expressed. Moreover, the corresponding posterior distribution also follows a generalized inverse Gaussian distribution. These properties lead us to EM algorithms for Bayesian sparse learning. We show that these algorithms bear an interesting resemblance to iteratively re-weighted $\ell_2$ or $\ell_1$ methods. In addition, we present two extensions for grouped variable selection and logistic regression.},
 author = {Zhihua Zhang and Shusen Wang and Dehua Liu and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W2951185564},
 pages = {2031--2061},
 title = {EP-GIG Priors and Applications in Bayesian Sparse Learning},
 url = {http://jmlr.org/papers/v13/zhang12b.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zhang12c,
 abstract = {Support vector machines (SVMs) naturally embody sparseness due to their use of hinge loss functions. However, SVMs can not directly estimate conditional class probabilities. In this paper we propose and study a family of coherence functions, which are convex and differentiable, as surrogates of the hinge function. The coherence function is derived by using the maximum-entropy principle and is characterized by a temperature parameter. It bridges the hinge function and the logit function in logistic regression. The limit of the coherence function at zero temperature corresponds to the hinge function, and the limit of the minimizer of its expected error is the minimizer of the expected error of the hinge loss. We refer to the use of the coherence function in large-margin classification as C-learning, and we present efficient coordinate descent algorithms for the training of regularized ${\cal C}$-learning models.},
 author = {Zhihua Zhang and Dehua Liu and Guang Dai and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W2953375029},
 pages = {2705--2734},
 title = {Coherence Functions with Applications in Large-Margin Classification Methods},
 url = {http://jmlr.org/papers/v13/zhang12c.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zhang12d,
 abstract = {A Support Vector Method for multivariate performance measures was recently introduced by Joachims (2005). The underlying optimization problem is currently solved using cutting plane methods such as SVM-Perf and BMRM. One can show that these algorithms converge to an eta accurate solution in O(1/Lambda*e) iterations, where lambda is the trade-off parameter between the regularizer and the loss function. We present a smoothing strategy for multivariate performance scores, in particular precision/recall break-even point and ROCArea. When combined with Nesterov's accelerated gradient algorithm our smoothing strategy yields an optimization algorithm which converges to an eta accurate solution in O(min{1/e,1/sqrt(lambda*e)}) iterations. Furthermore, the cost per iteration of our scheme is the same as that of SVM-Perf and BMRM. Empirical evaluation on a number of publicly available datasets shows that our method converges significantly faster than cutting plane methods without sacrificing generalization ability.},
 author = {Xinhua Zhang and Ankan Saha and S.V.N. Vishwanathan},
 journal = {Journal of Machine Learning Research},
 number = {117},
 openalex = {W3021959806},
 pages = {3623--3680},
 title = {Smoothing Multivariate Performance Measures},
 url = {http://jmlr.org/papers/v13/zhang12d.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zhao12a,
 abstract = {We describe an R package named huge which provides easy-to-use functions for estimating high dimensional undirected graphs from data. This package implements recent results in the literature, including Friedman et al. (2007), Liu et al. (2009, 2012) and Liu et al. (2010). Compared with the existing graph estimation package glasso, the huge package provides extra features: (1) instead of using Fortan, it is written in C, which makes the code more portable and easier to modify; (2) besides fitting Gaussian graphical models, it also provides functions for fitting high dimensional semiparametric Gaussian copula models; (3) more functions like data-dependent model selection, data generation and graph visualization; (4) a minor convergence problem of the graphical lasso algorithm is corrected; (5) the package allows the user to apply both lossless and lossy screening rules to scale up large-scale problems, making a tradeoff between computational and statistical efficiency.},
 author = {Tuo Zhao and Han Liu and Kathryn Roeder and John Lafferty and Larry Wasserman},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2125156589},
 pages = {1059--1062},
 title = {The huge Package for High-dimensional Undirected Graph Estimation in R},
 url = {http://jmlr.org/papers/v13/zhao12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zhu12a,
 abstract = {A supervised topic model can use side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective functions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet allocation (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) under a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or regression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, especially for classification.},
 author = {Jun Zhu and Amr Ahmed and Eric P. Xing},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W2133568543},
 pages = {2237--2278},
 title = {MedLDA: maximum margin supervised topic models},
 url = {http://jmlr.org/papers/v13/zhu12a.html},
 volume = {13},
 year = {2012}
}

@article{JMLR:v13:zitnik12a,
 abstract = {NIMFA is an open-source Python library that provides a unified interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA's component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks.},
 author = {Marinka {\v{Z}}itnik and Bla{\v{z}} Zupan},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W1823022514},
 pages = {849--853},
 title = {NIMFA: a python library for nonnegative matrix factorization},
 url = {http://jmlr.org/papers/v13/zitnik12a.html},
 volume = {13},
 year = {2012}
}
