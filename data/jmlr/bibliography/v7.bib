@article{JMLR:v7:abbeel06a,
 abstract = {We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.},
 author = {Pieter Abbeel and Daphne Koller and Andrew Y. Ng},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W2147071755},
 pages = {1743--1788},
 title = {Learning Factor Graphs in Polynomial Time and Sample Complexity},
 url = {http://jmlr.org/papers/v7/abbeel06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:angluin06a,
 abstract = {We consider the problem of learning a hypergraph using edge-detecting queries. In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(24rm · poly(r,logn)) queries with high probability. The queries can be made in O(min(2r (log m+r)2, (log m+r)3)) rounds. We also give an algorithm that learns an almost uniform hypergraph of dimension r using O(2O((1+Δ/2)r) · m1+Δ/2 · poly(log n)) queries with high probability, where Δ is the difference between the maximum and the minimum edge sizes. This upper bound matches our lower bound of Ω((m/(1+Δ/2))1+Δ/2) for this class of hypergraphs in terms of dependence on m. The queries can also be made in O((1+Δ) · min(2r (log m+r)2, (log m+r)3)) rounds.},
 author = {Dana Angluin and Jiang Chen},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W3023154460},
 pages = {2215--2236},
 title = {Learning a Hidden Hypergraph},
 url = {http://jmlr.org/papers/v7/angluin06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:bach06a,
 abstract = {Receiver Operating Characteristic (ROC) curves are a standard way to display the performance of a set of binary classifiers for all feasible ratios of the costs associated with false positives and false negatives. For linear classifiers, the set of classifiers is typically obtained by training once, holding constant the estimated slope and then varying the intercept to obtain a parameterized set of classifiers whose performances can be plotted in the ROC plane. We consider the alternative of varying the asymmetry of the cost function used for training. We show that the ROC curve obtained by varying both the intercept and the asymmetry, and hence the slope, always outperforms the ROC curve obtained by varying only the intercept. In addition, we present a path-following algorithm for the support vector machine (SVM) that can compute efficiently the entire ROC curve, and that has the same computational complexity as training a single classifier. Finally, we provide a theoretical analysis of the relationship between the asymmetric cost model assumed when training a classifier and the cost model assumed in applying the classifier. In particular, we show that the mismatch between the step function used for testing and its convex upper bounds, usually used for training, leads to a provable and quantifiable difference around extreme asymmetries.},
 author = {Francis R. Bach and David Heckerman and Eric Horvitz},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W2096536283},
 pages = {1713--1741},
 title = {Considering Cost Asymmetry in Learning Classifiers},
 url = {http://jmlr.org/papers/v7/bach06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:bach06b,
 abstract = {Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram.},
 author = {Francis R. Bach and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2129116669},
 pages = {1963--2001},
 title = {Learning Spectral Clustering, With Application To Speech Separation},
 url = {http://jmlr.org/papers/v7/bach06b.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:barber06a,
 abstract = {We introduce a method for approximate smoothed inference in a class of switching linear dynamical systems, based on a novel form of Gaussian Sum smoother. This class includes the switching Kalman 'Filter' and the more general case of switch transitions dependent on the continuous latent state. The method improves on the standard Kim smoothing approach by dispensing with one of the key approximations, thus making fuller use of the available future information.

Whilst the central assumption required is projection to a mixture of Gaussians, we show that an additional conditional independence assumption results in a simpler but accurate alternative. Our method consists of a single Forward and Backward Pass and is reminiscent of the standard smoothing 'correction' recursions in the simpler linear dynamical system. The method is numerically stable and compares favourably against alternative approximations, both in cases where a single mixture component provides a good posterior approximation, and where a multimodal approximation is required.},
 author = {David Barber},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W2172160462},
 pages = {2515--2540},
 title = {Expectation Correction for Smoothed Inference in Switching Linear Dynamical Systems},
 url = {http://jmlr.org/papers/v7/barber06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:begleiter06a,
 abstract = {We present worst case bounds for the learning rate of a known prediction method that is based on hierarchical applications of binary context tree weighting (CTW) predictors. A heuristic application of this approach that relies on Huffman's alphabet decomposition is known to achieve state-of-the-art performance in prediction and lossless compression benchmarks. We show that our new bound for this heuristic is tighter than the best known performance guarantees for prediction and lossless compression algorithms in various settings. This result substantiates the efficiency of this hierarchical method and provides a compelling explanation for its practical success. In addition, we present the results of a few experiments that examine other possibilities for improving the multi-alphabet prediction performance of CTW-based algorithms.},
 author = {Ron Begleiter and Ran El-Yaniv},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W2172293472},
 pages = {379--411},
 title = {Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition},
 url = {http://jmlr.org/papers/v7/begleiter06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:belkin06a,
 abstract = {We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that in...},
 author = {Mikhail Belkin and Partha Niyogi and Vikas Sindhwani},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W2997701990},
 pages = {2399--2434},
 title = {Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples},
 url = {http://jmlr.org/papers/v7/belkin06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:bergkvist06a,
 abstract = {We consider an optimization problem in probabilistic inference: Given n hypotheses Hj, m possible observations Ok, their conditional probabilities pkj, and a particular Ok, select a possibly small subset of hypotheses excluding the true target only with some error probability e. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m+n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound e. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming.},
 author = {Anders Bergkvist and Peter Damaschke and Marcel L{{\"u}}thi},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W2148299450},
 pages = {1339--1355},
 title = {Linear Programs for Hypotheses Selection in Probabilistic Inference Models},
 url = {http://jmlr.org/papers/v7/bergkvist06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:bhatnagar06a,
 abstract = {We study the problem of long-run average cost control of Markov chains conditioned on a rare event. In a related recent work, a simulation based algorithm for estimating performance measures associated with a Markov chain conditioned on a rare event has been developed. We extend ideas from this work and develop an adaptive algorithm for obtaining, online, optimal control policies conditioned on a rare event. Our algorithm uses three timescales or step-size schedules. On the slowest timescale, a gradient search algorithm for policy updates that is based on one-simulation simultaneous perturbation stochastic approximation (SPSA) type estimates is used. Deterministic perturbation sequences obtained from appropriate normalized Hadamard matrices are used here. The fast timescale recursions compute the conditional transition probabilities of an associated chain by obtaining solutions to the multiplicative Poisson equation (for a given policy estimate). Further, the risk parameter associated with the value function for a given policy estimate is updated on a timescale that lies in between the two scales above. We briefly sketch the convergence analysis of our algorithm and present a numerical application in the setting of routing multiple flows in communication networks.},
 author = {Shalabh Bhatnagar and Vivek S. Borkar and Madhukar Akarapu},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W2167019781},
 pages = {1937--1962},
 title = {A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events},
 url = {http://jmlr.org/papers/v7/bhatnagar06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:bickel06a,
 abstract = {We give a review of various aspects of boosting, clarifying the issues through a few simple results, and relate our work and that of others to the minimax paradigm of statistics. We consider the population version of the boosting algorithm and prove its convergence to the Bayes classifier as a corollary of a general result about Gauss-Southwell optimization in Hilbert space. We then investigate the algorithmic convergence of the sample version, and give bounds to the time until perfect separation of the sample. We conclude by some results on the statistical optimality of the L2 boosting.},
 author = {Peter J. Bickel and Ya'acov Ritov and Alon Zakai},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W2107008825},
 pages = {705--732},
 title = {Some Theory for Generalized Boosting Algorithms},
 url = {http://jmlr.org/papers/v7/bickel06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:blanchard06a,
 abstract = {Finding components of high-dimensional data is an important preprocessing step for efficient information processing. This article proposes a new linear method to identify the non-Gaussian subspace within a very general semi-parametric framework. Our proposed method, called NGCA (non-Gaussian component analysis), is based on a linear operator which, to any arbitrary nonlinear (smooth) function, associates a vector belonging to the low dimensional target subspace, up to an estimation error. By applying this operator to a family of different nonlinear functions, one obtains a family of different vectors lying in a vicinity of the target space. As a final step, the target space itself is estimated by applying PCA to this family of vectors. We show that this procedure is consistent in the sense that the estimaton error tends to zero at a parametric rate, uniformly over the family, Numerical examples demonstrate the usefulness of our method.},
 author = {Gilles Blanchard and Motoaki Kawanabe and Masashi Sugiyama and Vladimir Spokoiny and Klaus-Robert M{{\"u}}ller},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W2127770723},
 pages = {247--282},
 title = {In Search of Non-Gaussian Components of a High-Dimensional Distribution},
 url = {http://jmlr.org/papers/v7/blanchard06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:bratko06a,
 abstract = {Spam filtering poses a special problem in text categorization, of which the defining characteristic is that filters face an active adversary, which constantly attempts to evade filtering. Since spam evolves continuously and most practical applications are based on online user feedback, the task calls for fast, incremental and robust learning algorithms. In this paper, we investigate a novel approach to spam filtering based on adaptive statistical data compression models. The nature of these models allows them to be employed as probabilistic text classifiers based on character-level or binary sequences. By modeling messages as sequences, tokenization and other error-prone preprocessing steps are omitted altogether, resulting in a method that is very robust. The models are also fast to construct and incrementally updateable. We evaluate the filtering performance of two different compression algorithms; dynamic Markov compression and prediction by partial matching. The results of our empirical evaluation indicate that compression models outperform currently established spam filters, as well as a number of methods proposed in previous studies.},
 author = {Andrej Bratko and Gordon V. Cormack and Bogdan Filipi{\v{c}} and Thomas R. Lynam and Bla{\v{z}} Zupan},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W2104290684},
 pages = {2673--2698},
 title = {Spam Filtering Using Statistical Data Compression Models},
 url = {http://jmlr.org/papers/v7/bratko06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:braun06a,
 abstract = {The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to infinity. We derive probabilistic finite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reflecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a significant improvement over existing non-scaling bounds.},
 author = {Mikio L. Braun},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W2143053860},
 pages = {2303--2328},
 title = {Accurate Error Bounds for the Eigenvalues of the Kernel Matrix},
 url = {http://jmlr.org/papers/v7/braun06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:buehlmann06a,
 abstract = {We propose Sparse Boosting (the SparseL2Boost algorithm), a variant on boosting with the squared error loss. SparseL2Boost yields sparser solutions than the previously proposed L2Boosting by minimizing some penalized L2-loss functions, the FPE model selection criteria, through small-step gradient descent. Although boosting may give already relatively sparse solutions, for example corresponding to the soft-thresholding estimator in orthogonal linear models, there is sometimes a desire for more sparseness to increase prediction accuracy and ability for better variable selection: such goals can be achieved with SparseL2Boost.

We prove an equivalence of SparseL2Boost to Breiman's nonnegative garrote estimator for orthogonal linear models and demonstrate the generic nature of SparseL2Boost for nonparametric interaction modeling. For an automatic selection of the tuning parameter in SparseL2Boost we propose to employ the gMDL model selection criterion which can also be used for early stopping of L2Boosting. Consequently, we can select between SparseL2Boost and L2Boosting by comparing their gMDL scores.},
 author = {Peter B{{\"u}}hlmann and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W2912273251},
 pages = {1001--1024},
 title = {Sparse Boosting},
 url = {http://jmlr.org/papers/v7/buehlmann06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:caponnetto06a,
 abstract = {We study some stability properties of algorithms which minimize (or almost-minimize) empirical error over Donsker classes of functions. We show that, as the number n of samples grows, the L2-diameter of the set of almost-minimizers of empirical error with tolerance ξ(n)=o(n-1/2) converges to zero in probability. Hence, even in the case of multiple minimizers of expected error, as n increases it becomes less and less likely that adding a sample (or a number of samples) to the training set will result in a large jump to a new hypothesis. Moreover, under some assumptions on the entropy of the class, along with an assumption of Komlos-Major-Tusnady type, we derive a power rate of decay for the diameter of almost-minimizers. This rate, through an application of a uniform ratio limit inequality, is shown to govern the closeness of the expected errors of the almost-minimizers. In fact, under the above assumptions, the expected errors of almost-minimizers become closer with a rate strictly faster than n-1/2.},
 author = {Andrea Caponnetto and Alexander Rakhlin},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2114067803},
 pages = {2565--2583},
 title = {Stability Properties of Empirical Risk Minimization over Donsker Classes},
 url = {http://jmlr.org/papers/v7/caponnetto06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:castelo06a,
 abstract = {Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data.},
 author = {Robert Castelo and Alberto Roverato},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W2104346261},
 pages = {2621--2650},
 title = {A Robust Procedure For Gaussian Graphical Model Search From Microarray Data With p Larger Than n},
 url = {http://jmlr.org/papers/v7/castelo06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:castillo06a,
 abstract = {This paper introduces a learning method for two-layer feedforward neural networks based on sensitivity analysis, which uses a linear training algorithm for each of the two layers. First, random values are assigned to the outputs of the first layer; later, these initial values are updated based on sensitivity formulas, which use the weights in each of the layers; the process is repeated until convergence. Since these weights are learnt solving a linear system of equations, there is an important saving in computational time. The method also gives the local sensitivities of the least square errors with respect to input and output data, with no extra computational cost, because the necessary information becomes available without extra calculations. This method, called the Sensitivity-Based Linear Learning Method, can also be used to provide an initial set of weights, which significantly improves the behavior of other learning algorithms. The theoretical basis for the method is given and its performance is illustrated by its application to several examples in which it is compared with several learning algorithms and well known data sets. The results have shown a learning speed generally faster than other existing methods. In addition, it can be used as an initialization tool for other well known methods with significant improvements.},
 author = {Enrique Castillo and Bertha Guijarro-Berdi{{\~n}}as and Oscar Fontenla-Romero and Amparo Alonso-Betanzos},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W2135719924},
 pages = {1159--1182},
 title = {A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis},
 url = {http://jmlr.org/papers/v7/castillo06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:centeno06a,
 abstract = {In this paper we consider a novel Bayesian interpretation of Fisher's discriminant analysis. We relate Rayleigh's coefficient to a noise model that minimises a cost based on the most probable class centres and that abandons the 'regression to the labels' assumption used by other algorithms. Optimisation of the noise model yields a direction of discrimination equivalent to Fisher's discriminant, and with the incorporation of a prior we can apply Bayes' rule to infer the posterior distribution of the direction of discrimination. Nonetheless, we argue that an additional constraining distribution has to be included if sensible results are to be obtained. Going further, with the use of a Gaussian process prior we show the equivalence of our model to a regularised kernel Fisher's discriminant. A key advantage of our approach is the facility to determine kernel parameters and the regularisation coefficient through the optimisation of the marginal log-likelihood of the data. An added bonus of the new formulation is that it enables us to link the regularisation coefficient with the generalisation error.},
 author = {Tonatiuh Pe{{\~n}}a Centeno and Neil D. Lawrence},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2107381584},
 pages = {455--491},
 title = {Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis},
 url = {http://jmlr.org/papers/v7/centeno06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:cesa-bianchi06a,
 abstract = {We study the problem of classifying data in a given taxonomy when classifications associated with multiple and/or partial paths are allowed. We introduce a new algorithm that incrementally learns a...},
 author = {Nicol{{\'o}} Cesa-Bianchi and Claudio Gentile and Luca Zaniboni},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W3015740509},
 pages = {31--54},
 title = {Incremental Algorithms for Hierarchical Classification},
 url = {http://jmlr.org/papers/v7/cesa-bianchi06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:cesa-bianchi06b,
 abstract = {A selective sampling algorithm is a learning algorithm for classification that, based on the past observed data, decides whether to ask the label of each new instance to be classified. In this paper, we introduce a general technique for turning linear-threshold classification algorithms from the general additive family into randomized selective sampling algorithms. For the most popular algorithms in this family we derive mistake bounds that hold for individual sequences of examples. These bounds show that our semi-supervised algorithms can achieve, on average, the same accuracy as that of their fully supervised counterparts, but using fewer labels. Our theoretical results are corroborated by a number of experiments on real-world textual data. The outcome of these experiments is essentially predicted by our theoretical results: Our selective sampling algorithms tend to perform as well as the algorithms receiving the true label after each classification, while observing in practice substantially fewer labels.},
 author = {Nicol{{\'o}} Cesa-Bianchi and Claudio Gentile and Luca Zaniboni},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W2111145173},
 pages = {1205--1230},
 title = {Worst-Case Analysis of Selective Sampling for Linear Classification},
 url = {http://jmlr.org/papers/v7/cesa-bianchi06b.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:chang06a,
 abstract = {In this paper, we propose a number of adaptive prototype learning (APL) algorithms. They employ the same algorithmic scheme to determine the number and location of prototypes, but differ in the use...},
 author = {Fu Chang and Chin-Chin Lin and Chi-Jen Lu},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W3004791276},
 pages = {2125--2148},
 title = {Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies},
 url = {http://jmlr.org/papers/v7/chang06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:chen06a,
 abstract = {The consistency of classification algorithm plays a central role in statistical learning theory. A consistent algorithm guarantees us that taking more samples essentially suffices to roughly reconstruct the unknown distribution. We consider the consistency of ERM scheme over classes of combinations of very simple rules (base classifiers) in multiclass classification. Our approach is, under some mild conditions, to establish a quantitative relationship between classification errors and convex risks. In comparison with the related previous work, the feature of our result is that the conditions are mainly expressed in terms of the differences between some values of the convex function.},
 author = {Di-Rong Chen and Tao Sun},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W2119851445},
 pages = {2435--2447},
 title = {Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss},
 url = {http://jmlr.org/papers/v7/chen06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:climer06a,
 abstract = {Given a matrix of values in which the rows correspond to objects and the columns correspond to features of the objects, rearrangement clustering is the problem of rearranging the rows of the matrix...},
 author = {Sharlee Climer and Weixiong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W3006032487},
 pages = {919--943},
 title = {Rearrangement Clustering: Pitfalls, Remedies, and Applications},
 url = {http://jmlr.org/papers/v7/climer06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:collobert06a,
 abstract = {We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the first time a highly scalable ...},
 author = {Ronan Collobert and Fabian Sinz and Jason Weston and L{{\'e}}on Bottou},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W3005100505},
 pages = {1687--1712},
 title = {Large Scale Transductive SVMs},
 url = {http://jmlr.org/papers/v7/collobert06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:crammer06a,
 abstract = {We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This unified view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any fixed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.},
 author = {Koby Crammer and Ofer Dekel and Joseph Keshet and Shai Shalev-Shwartz and Yoram Singer},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2160218441},
 pages = {551--585},
 title = {Online Passive-Aggressive Algorithms},
 url = {http://jmlr.org/papers/v7/crammer06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:debie06a,
 abstract = {The rise of convex programming has changed the face of many research fields in recent years, machine learning being one of the ones that benefitted the most. A very recent developement, the relaxation of combinatorial problems to semi-definite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning.

In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade off computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example.},
 author = {Tijl De Bie and Nello Cristianini},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2133026689},
 pages = {1409--1436},
 title = {Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems},
 url = {http://jmlr.org/papers/v7/debie06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:decampos06a,
 abstract = {We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented.},
 author = {Luis M. de Campos},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W2142390772},
 pages = {2149--2187},
 title = {A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests},
 url = {http://jmlr.org/papers/v7/decampos06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:demsar06a,
 abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on mult...},
 author = {Janez Dem{\v{s}}ar},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2997546679},
 pages = {1--30},
 title = {Statistical Comparisons of Classifiers over Multiple Data Sets},
 url = {http://jmlr.org/papers/v7/demsar06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:ekdahl06a,
 abstract = {In many pattern recognition/classification problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classifier is expected to have worse performance, here measured by the probability of correct classification. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classification when compared to the optimal classifier. An example of an approximation is the Naive Bayes classifier. We show that the performance of the Naive Bayes depends on the degree of functional dependence between the features and labels. We provide a sufficient condition for zero loss of performance, too.},
 author = {Magnus Ekdahl and Timo Koski},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W2161272368},
 pages = {2449--2480},
 title = {Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation},
 url = {http://jmlr.org/papers/v7/ekdahl06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:evendar06a,
 abstract = {We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O((n/e2)log(1/δ)) times to find an e-optimal arm with probability of at least 1-δ. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over e-greedy Q-learning.},
 author = {Eyal Even-Dar and Shie Mannor and Yishay Mansour},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W2147967768},
 pages = {1079--1105},
 title = {Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems},
 url = {http://jmlr.org/papers/v7/evendar06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:fumera06a,
 abstract = {In recent years anti-spam filters have become necessary tools for Internet service providers to face up to the continuously growing spam phenomenon. Current server-side anti-spam filters are made u...},
 author = {Giorgio Fumera and Ignazio Pillai and Fabio Roli},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W3000705115},
 pages = {2699--2720},
 title = {Spam Filtering Based On The Analysis Of Text Information Embedded Into Images},
 url = {http://jmlr.org/papers/v7/fumera06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:gardner06a,
 abstract = {This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a .one-shot. manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training.},
 author = {Andrew B. Gardner and Abba M. Krieger and George Vachtsevanos and Brian Litt},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2170014674},
 pages = {1025--1044},
 title = {One-Class Novelty Detection for Seizure Analysis from Intracranial EEG},
 url = {http://jmlr.org/papers/v7/gardner06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:glasmachers06a,
 abstract = {Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variab...},
 author = {Tobias Glasmachers and Christian Igel},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W3023975262},
 pages = {1437--1466},
 title = {Maximum-Gain Working Set Selection for SVMs},
 url = {http://jmlr.org/papers/v7/glasmachers06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:goldberg06a,
 abstract = {A classical approach in multi-class pattern classification is the following. Estimate the probability distributions that generated the observations for each label class, and then label new instances by applying the Bayes classifier to the estimated distributions. That approach provides more useful information than just a class label; it also provides estimates of the conditional distribution of class labels, in situations where there is class overlap.

We would like to know whether it is harder to build accurate classifiers via this approach, than by techniques that may process all data with distinct labels together. In this paper we make that question precise by considering it in the context of PAC learnability. We propose two restrictions on the PAC learning framework that are intended to correspond with the above approach, and consider their relationship with standard PAC learning. Our main restriction of interest leads to some interesting algorithms that show that the restriction is not stronger (more restrictive) than various other well-known restrictions on PAC learning. An alternative slightly milder restriction turns out to be almost equivalent to unrestricted PAC learning.},
 author = {Paul W. Goldberg},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2148360652},
 pages = {283--306},
 title = {Some Discriminant-Based PAC Algorithms},
 url = {http://jmlr.org/papers/v7/goldberg06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:hamerly06a,
 abstract = {An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efficient means by which they may est...},
 author = {Greg Hamerly and Erez Perelman and Jeremy Lau and Brad Calder and Timothy Sherwood},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W3012338770},
 pages = {343--378},
 title = {Using Machine Learning to Guide Architecture Simulation},
 url = {http://jmlr.org/papers/v7/hamerly06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:heiler06a,
 abstract = {We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone progr...},
 author = {Matthias Heiler and Christoph Schn{{\"o}}rr},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W3015755214},
 pages = {1385--1407},
 title = {Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming},
 url = {http://jmlr.org/papers/v7/heiler06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:huang06a,
 abstract = {The Bradley-Terry model for obtaining individual skill from paired comparisons has been popular in many areas. In machine learning, this model is related to multi-class probability estimates by coupling all pairwise classification results. Error correcting output codes (ECOC) are a general framework to decompose a multi-class problem to several binary problems. To obtain probability estimates under this framework, this paper introduces a generalized Bradley-Terry model in which paired individual comparisons are extended to paired team comparisons. We propose a simple algorithm with convergence proofs to solve the model and obtain individual skill. Experiments on synthetic and re al data demonstrate that the algorithm is useful for obtaining multi-class probability estimates. Moreover, we discuss four extensions of the proposed model: 1) weighted individual skill, 2) home-field advantage, 3) ties, and 4) comparisons with more than two teams.},
 author = {Tzu-Kuo Huang and Ruby C. Weng and Chih-Jen Lin},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2107496976},
 pages = {85--115},
 title = {Generalized Bradley-Terry Models and Multi-Class Probability Estimates},
 url = {http://jmlr.org/papers/v7/huang06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:hush06a,
 abstract = {We describe polynomial--time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classifiers. These algorithms employ a two--stage process where the first stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an approximate dual solution with accuracy (2(2Km)1/2+8(λ)1/2)-2 λ ep2 to an approximate primal solution with accuracy ep where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the first stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ-rate certifying decomposition algorithms we establish the optimality of τ = 1/(n-1). In addition we extend the recent τ = 1/(n-1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n-1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ-rate certifying property of these algorithms to produce new stopping rules that are computationally efficient and that guarantee a specified accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classification problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2) bound on the overall run time for the first stage. Combining the first and second stages gives an overall run time of O(n2(ck + 1)) where ck is an upper bound on the computation to perform a kernel evaluation. Pseudocode is presented for a complete algorithm that inputs an accuracy ep and produces an approximate solution that satisfies this accuracy in low order polynomial time. Experiments are included to illustrate the new stopping rules and to compare the Simon and composite decomposition algorithms.},
 author = {Don Hush and Patrick Kelly and Clint Scovel and Ingo Steinwart},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2101087512},
 pages = {733--769},
 title = {QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines},
 url = {http://jmlr.org/papers/v7/hush06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:jonsson06a,
 abstract = {We present Variable Influence Structure Analysis, or VISA, an algorithm that performs hierarchical decomposition of factored Markov decision processes. VISA uses a dynamic Bayesian network model of actions, and constructs a causal graph that captures relationships between state variables. In tasks with sparse causal graphs VISA exploits structure by introducing activities that cause the values of state variables to change. The result is a hierarchy of activities that together represent a solution to the original task. VISA performs state abstraction for each activity by ignoring irrelevant state variables and lower-level activities. In addition, we describe an algorithm for constructing compact models of the activities introduced. State abstraction and compact activity models enable VISA to apply efficient algorithms to solve the stand-alone subtask associated with each activity. Experimental results show that the decomposition introduced by VISA can significantly accelerate construction of an optimal, or near-optimal, policy.},
 author = {Anders Jonsson and Andrew Barto},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W2153668164},
 pages = {2259--2301},
 title = {Causal Graph Based Decomposition of Factored MDPs},
 url = {http://jmlr.org/papers/v7/jonsson06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:kaempke06a,
 abstract = {Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best fit isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost flow, linear assignment relaxations and related graph algorithms.},
 author = {Thomas K&#228;mpke},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W1607562333},
 pages = {2065--2086},
 title = {Distance Patterns in Structural Similarity},
 url = {http://jmlr.org/papers/v7/kaempke06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:keerthi06a,
 abstract = {Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classification speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily finds a set of kernel basis functions of a specified maximum size (dmax) to approximate the SVM primal cost function well; (3) it is efficient and roughly scales as O(ndmax2) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors.},
 author = {S. Sathiya Keerthi and Olivier Chapelle and Dennis DeCoste},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W2137285073},
 pages = {1493--1515},
 title = {Building Support Vector Machines with Reduced Classifier Complexity},
 url = {http://jmlr.org/papers/v7/keerthi06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:kim06a,
 abstract = {This paper proposes a general probabilistic framework for shape-based modeling and classification of waveform data. A segmental hidden Markov model (HMM) is used to characterize waveform shape and shape variation is captured by adding random effects to the segmental model. The resulting probabilistic framework provides a basis for learning of waveform models from data as well as parsing and recognition of new waveforms. Expectation-maximization (EM) algorithms are derived and investigated for fitting such models to data. In particular, the expectation conditional maximization either (ECME) algorithm is shown to provide significantly faster convergence than a standard EM procedure. Experimental results on two real-world data sets demonstrate that the proposed approach leads to improved accuracy in classification and segmentation when compared to alternatives such as Euclidean distance matching, dynamic time warping, and segmental HMMs without random effects.},
 author = {Seyoung Kim and Padhraic Smyth},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W2132350041},
 pages = {945--969},
 title = {Segmental Hidden Markov Models with Random Effects for Waveform Modeling},
 url = {http://jmlr.org/papers/v7/kim06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:kitzelmann06a,
 abstract = {We describe an approach to the inductive synthesis of recursive equations from input/output-examples which is based on the classical two-step approach to induction of functional Lisp programs of Summers (1977). In a first step, I/O-examples are rewritten to traces which explain the outputs given the respective inputs based on a datatype theory. These traces can be integrated into one conditional expression which represents a non-recursive program. In a second step, this initial program term is generalized into recursive equations by searching for syntactical regularities in the term. Our approach extends the classical work in several aspects. The most important extensions are that we are able to induce a set of recursive equations in one synthesizing step, the equations may contain more than one recursive call, and additionally needed parameters are automatically introduced.},
 author = {Emanuel Kitzelmann and Ute Schmid},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2144936864},
 pages = {429--454},
 title = {Inductive Synthesis of Functional Programs: An Explanation Based Generalization Approach},
 url = {http://jmlr.org/papers/v7/kitzelmann06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:klivans06a,
 abstract = {We consider two well-studied problems regarding attribute efficient learning: learning decision lists and learning parity functions. First, we give an algorithm for learning decision lists of length k over n variables using 2 ${\widetilde{O}}^{(k^{1/3})}{\rm log}n$ examples and time $n{\widetilde{O}}^{(k^{1/3})}{\rm log}n$ . This is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a lower bound due to Beigel for decision lists and gives an essentially optimal tradeoff between polynomial threshold function degree and weight. Second, we give an algorithm for learning an unknown parity function on k out of n variables using O(n 1 − − 1/k) examples in poly(n) time. For k=o(log n) this yields the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity. We also give a simple algorithm for learning an unknown size-k parity using O(k log n) examples in n k/2 time, which improves on the naive n k time bound of exhaustive search.},
 author = {Adam R. Klivans and Rocco A. Servedio},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W2163361804},
 pages = {587--602},
 title = {Toward Attribute Efficient Learning of Decision Lists and Parities},
 url = {http://jmlr.org/papers/v7/klivans06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:kok06a,
 abstract = {In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcement-learning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efficient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences.},
 author = {Jelle R. Kok and Nikos Vlassis},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W2110906765},
 pages = {1789--1828},
 title = {Collaborative Multiagent Reinforcement Learning by Payoff Propagation},
 url = {http://jmlr.org/papers/v7/kok06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:kolter06a,
 abstract = {We describe the use of machine learning and data mining to detect and classify malicious executables as they appear in the wild. We gathered 1,971 benign and 1,651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the ROC curve of 0.996. Results suggest that our methodology will scale to larger collections of executables. We also evaluated how well the methods classified executables based on the function of their payload, such as opening a backdoor and mass-mailing. Areas under the ROC curve for detecting payload function were in the neighborhood of 0.9, which were smaller than those for the detection task. However, we attribute this drop in performance to fewer training examples and to the challenge of obtaining properly labeled examples, rather than to a failing of the methodology or to some inherent difficulty of the classification task. Finally, we applied detectors to 291 malicious executables discovered after we gathered our original collection, and boosted decision trees achieved a true-positive rate of 0.98 for a desired false-positive rate of 0.05. This result is particularly important, for it suggests that our methodology could be used as the basis for an operational system for detecting previously undiscovered malicious executables.},
 author = {J. Zico Kolter and Marcus A. Maloof},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W2121749752},
 pages = {2721--2744},
 title = {Learning to Detect and Classify Malicious Executables in the Wild},
 url = {http://jmlr.org/papers/v7/kolter06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:langley06a,
 abstract = {In this paper, we propose a new representation for physical control -- teleoreactive logic programs -- along with an interpreter that uses them to achieve goals. In addition, we present a new learning method that acquires recursive forms of these structures from traces of successful problem solving. We report experiments in three different domains that demonstrate the generality of this approach. In closing, we review related work on learning complex skills and discuss directions for future research on this topic.},
 author = {Pat Langley and Dongkyu Choi},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W2104115362},
 pages = {493--518},
 title = {Learning Recursive Control Programs from Problem Solving},
 url = {http://jmlr.org/papers/v7/langley06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:laskov06a,
 abstract = {Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efficient incremental SVM learning, with the...},
 author = {Pavel Laskov and Christian Gehl and Stefan Kr{{\"u}}ger and Klaus-Robert M{{\"u}}ller},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W3007818867},
 pages = {1909--1936},
 title = {Incremental Support Vector Learning: Analysis, Implementation and Applications},
 url = {http://jmlr.org/papers/v7/laskov06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:lecue06a,
 abstract = {In this paper we prove the optimality of an aggregation procedure. We prove lower bounds for aggregation of model selection type of $M$ density estimators for the Kullback-Leiber divergence (KL), the Hellinger's distance and the $L\_1$-distance. The lower bound, with respect to the KL distance, can be achieved by the on-line type estimate suggested, among others, by Yang (2000). Combining these results, we state that $\log M/n$ is an optimal rate of aggregation in the sense of Tsybakov (2003), where $n$ is the sample size.},
 author = {Guillaume Lecu{{\'e}}},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W2951267856},
 pages = {971--981},
 title = {Lower bounds and aggregation in density estimation},
 url = {http://jmlr.org/papers/v7/lecue06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:lippert06a,
 author = {Ross A. Lippert and Ryan M. Rifkin},
 journal = {Journal of Machine Learning Research},
 number = {30},
 pages = {855--876},
 title = {Infinite-Ï Limits For Tikhonov Regularization},
 url = {http://jmlr.org/papers/v7/lippert06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:liu06a,
 abstract = {This paper is about non-approximate acceleration of high-dimensional nonparametric operations such as k nearest neighbor classifiers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly find the data points close to the query, but merely need to answer questions about the properties of that set of data points. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. This is applicable to many algorithms in nonparametric statistics, memory-based learning and kernel-based learning. But for clarity, this paper c oncentrates on pure k-NN classification. We introduce new ball-tree algorithms that on real-world data sets give accelerations from 2-fold to 100-fold compared against highly optimized traditional ball-tree-based k-NN . These results include data sets with up to 10 6 dimensions and 10 5 records, and demonstrate non-trivial speed-ups},
 author = {Ting Liu and Andrew W. Moore and Alexander Gray},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W2108406823},
 pages = {1135--1158},
 title = {New Algorithms for Efficient High-Dimensional Nonparametric Classification},
 url = {http://jmlr.org/papers/v7/liu06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:malioutov06a,
 abstract = {We present a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose the correlation between each pair of variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlation coefficients. This representation holds for a large class of Gaussian graphical models which we call walk-summable. We give a precise characterization of this class of models, and relate it to other classes including diagonally dominant, attractive, non-frustrated, and pairwise-normalizable. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. The walk-sum perspective leads to a better understanding of Gaussian belief propagation and to stronger results for its convergence in loopy graphs.},
 author = {Dmitry M. Malioutov and Jason K. Johnson and Alan S. Willsky},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2111521164},
 pages = {2031--2064},
 title = {Walk-Sums and Belief Propagation in Gaussian Graphical Models},
 url = {http://jmlr.org/papers/v7/malioutov06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:mangasarian06a,
 abstract = {Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional finite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classification problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.},
 author = {Olvi L. Mangasarian},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2164933608},
 pages = {1517--1530},
 title = {Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization},
 url = {http://jmlr.org/papers/v7/mangasarian06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:maurer06a,
 abstract = {We give dimension-free and data-dependent bounds for linear multi-task learning where a common linear operator is chosen to preprocess data for a vector of task specific linear-thresholding classifiers. The complexity penalty of multi-task learning is bounded by a simple expression involving the margins of the task-specific classifiers, the Hilbert-Schmidt norm of the selected preprocessor and the Hilbert-Schmidt norm of the covariance operator for the total mixture of all task distributions, or, alternatively, the Frobenius norm of the total Gramian matrix for the data-dependent version. The results can be compared to state-of-the-art results on linear single-task learning.},
 author = {Andreas Maurer},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2156267734},
 pages = {117--139},
 title = {Bounds for Linear Multi-Task Learning},
 url = {http://jmlr.org/papers/v7/maurer06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:meinshausen06a,
 abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression,...},
 author = {Nicolai Meinshausen},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W3000577762},
 pages = {983--999},
 title = {Quantile Regression Forests},
 url = {http://jmlr.org/papers/v7/meinshausen06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:micchelli06a,
 abstract = {In this paper we investigate conditions on the features of a continuous kernel so that it may approximate an arbitrary continuous target function uniformly on any compact subset of the input space. A number of concrete examples are given of kernels with this universal approximating property.},
 author = {Charles A. Micchelli and Yuesheng Xu and Haizhang Zhang},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W2913243980},
 pages = {2651--2667},
 title = {Universal Kernels},
 url = {http://jmlr.org/papers/v7/micchelli06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:MLOPT-intro06a,
 abstract = {The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.},
 author = {Kristin P. Bennett and Emilio Parrado-Hern{{\'a}}ndez},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W2170905826},
 pages = {1265--1281},
 title = {The Interplay of Optimization and Machine Learning Research},
 url = {http://jmlr.org/papers/v7/MLOPT-intro06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:MLSEC-intro06a,
 abstract = {The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems.},
 author = {Philip K. Chan and Richard P. Lippmann},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W2105136475},
 pages = {2669--2672},
 title = {Machine Learning for Computer Security},
 url = {http://jmlr.org/papers/v7/MLSEC-intro06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:moser06a,
 abstract = {Kernels are two-placed functions that can be interpreted as inner products in some Hilbert space. It is this property which makes kernels predestinated to carry linear models of learning, optimization or classification strategies over to non-linear variants. Following this idea, various kernel-based methods like support vector machines or kernel principal component analysis have been conceived which prove to be successful for machine learning, data mining and computer vision applications. When applying a kernel-based method a central question is the choice and the design of the kernel function. This paper provides a novel view on kernels based on fuzzy-logical concepts which allows to incorporate prior knowledge in the design process. It is demonstrated that kernels mapping to the unit interval with constant one in its diagonal can be represented by a commonly used fuzzy-logical formula for representing fuzzy rule bases. This means that a great class of kernels can be represented by fuzzy-logical concepts. Apart from this result, which only guarantees the existence of such a representation, constructive examples are presented and the relation to unlabeled learning is pointed out.},
 author = {Bernhard Moser},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W2168756514},
 pages = {2603--2620},
 title = {On Representing and Generating Kernels by Fuzzy Equivalence Relations},
 url = {http://jmlr.org/papers/v7/moser06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:mukherjee06a,
 abstract = {We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efficient implementation with respect to both memory and time.},
 author = {Sayan Mukherjee and Ding-Xuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W2097462699},
 pages = {519--549},
 title = {Learning Coordinate Covariances via Gradients},
 url = {http://jmlr.org/papers/v7/mukherjee06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:mukherjee06b,
 abstract = {We introduce an algorithm that simultaneously estimates a classification function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to find salient v...},
 author = {Sayan Mukherjee and Qiang Wu},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W3004943848},
 pages = {2481--2514},
 title = {Estimation of Gradients and Coordinate Covariation in Classification},
 url = {http://jmlr.org/papers/v7/mukherjee06b.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:munos06a,
 abstract = {We study a variance reduction technique for Monte Carlo estimation of functionals in Markov chains. The method is based on designing sequential control variates using successive approximations of the function of interest V. Regular Monte Carlo estimates have a variance of O(1/N), where N is the number of sample trajectories of the Markov chain. Here, we obtain a geometric variance reduction O(ρN) (with ρ<1) up to a threshold that depends on the approximation error V-AV, where A is an approximation operator linear in the values. Thus, if V belongs to the right approximation space (i.e. AV=V), the variance decreases geometrically to zero.

An immediate application is value function estimation in Markov chains, which may be used for policy evaluation in a policy iteration algorithm for solving Markov Decision Processes.

Another important domain, for which variance reduction is highly needed, is gradient estimation, that is computing the sensitivity ∂αV of the performance measure V with respect to some parameter α of the transition probabilities. For example, in policy parametric optimization, computing an estimate of the policy gradient is required to perform a gradient optimization method.

We show that, using two approximations for the value function and the gradient, a geometric variance reduction is also achieved, up to a threshold that depends on the approximation errors of both of those representations.},
 author = {R{{\'e}}mi Munos},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W2112192511},
 pages = {413--427},
 title = {Geometric Variance Reduction in Markov Chains: Application to Value Function and Gradient Estimation},
 url = {http://jmlr.org/papers/v7/munos06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:munos06b,
 abstract = {Policy search is a method for approximately solving an optimal control problem by performing a parametric optimization search in a given class of parameterized policies. In order to process a local optimization technique, such as a gradient method, we wish to evaluate the sensitivity of the performance measure with respect to the policy parameters, the so-called policy gradient. This paper is concerned with the estimation of the policy gradient for continuous-time, deterministic state dynamics, in a reinforcement learning framework, that is, when the decision maker does not have a model of the state dynamics.

We show that usual likelihood ratio methods used in discrete-time, fail to proceed the gradient because they are subject to variance explosion when the discretization time-step decreases to 0. We describe an alternative approach based on the approximation of the pathwise derivative, which leads to a policy gradient estimate that converges almost surely to the true gradient when the time-step tends to 0. The underlying idea starts with the derivation of an explicit representation of the policy gradient using pathwise derivation. This derivation makes use of the knowledge of the state dynamics. Then, in order to estimate the gradient from the observable data only, we use a stochastic policy to discretize the continuous deterministic system into a stochastic discrete process, which enables to replace the unknown coefficients by quantities that solely depend on known data. We prove the almost sure convergence of this estimate to the true policy gradient when the discretization time-step goes to zero.

The method is illustrated on two target problems, in discrete and continuous control spaces.},
 author = {R{{\'e}}mi Munos},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W2143803054},
 pages = {771--791},
 title = {Policy Gradient in Continuous Time},
 url = {http://jmlr.org/papers/v7/munos06b.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:niculescu06a,
 abstract = {The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure.

We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context specific independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several specific classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data.

We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to first learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented.},
 author = {Radu Stefan Niculescu and Tom M. Mitchell and R. Bharat Rao},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W2121278962},
 pages = {1357--1383},
 title = {Bayesian Network Learning with Parameter Constraints},
 url = {http://jmlr.org/papers/v7/niculescu06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:olsson06a,
 abstract = {We apply a type of generative modelling to the problem of blind source separation in which prior knowledge about the latent source signals, such as time-varying auto-correlation and quasi-periodicity, are incorporated into a linear state-space model. In simulations, we show that in terms of signal-to-error ratio, the sources are inferred more accurately as a result of the inclusion of strong prior knowledge. We explore different schemes of maximum-likelihood optimization for the purpose of learning the model parameters. The Expectation Maximization algorithm, which is often considered the standard optimization method in this context, results in slow convergence when the noise variance is small. In such scenarios, quasi-Newton optimization yields substantial improvements in a range of signal to noise ratios. We analyze the performance of the methods on convolutive mixtures of speech signals.},
 author = {Rasmus Kongsgaard Olsson and Lars Kai Hansen},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W2158429562},
 pages = {2585--2602},
 title = {Linear State-Space Models for Blind Source Separation},
 url = {http://jmlr.org/papers/v7/olsson06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:passerini06a,
 abstract = {We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.},
 author = {Andrea Passerini and Paolo Frasconi and Luc De Raedt},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W2151168698},
 pages = {307--342},
 title = {Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting},
 url = {http://jmlr.org/papers/v7/passerini06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:peer06a,
 abstract = {In recent years, there has been a growing interest in applying Bayesian networks and their extensions to reconstruct regulatory networks from gene expression data. Since the gene expression domain involves a large number of variables and a limited number of samples, it poses both computational and statistical challenges to Bayesian network learning algorithms. Here we define a constrained family of Bayesian network structures suitable for this domain and devise an efficient search algorithm that utilizes these structural constraints to find high scoring networks from data. Interestingly, under reasonable assumptions on the underlying probability distribution, we can provide performance guarantees on our algorithm. Evaluation on real data from yeast and mouse, demonstrates that our method cannot only reconstruct a high quality model of the yeast regulatory network, but is also the first method to scale to the complexity of mammalian networks and successfully reconstructs a reasonable model over thousands of variables.},
 author = {Dana Pe'er and Amos Tanay and Aviv Regev},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W2107923541},
 pages = {167--189},
 title = {MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals},
 url = {http://jmlr.org/papers/v7/peer06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:porta06a,
 abstract = {We propose a novel approach to optimize Partially Observable Markov Decisions Processes (POMDPs) defined on continuous spaces. To date, most algorithms for model-based POMDPs are restricted to discrete states, actions, and observations, but many real-world problems such as, for instance, robot navigation, are naturally defined on continuous spaces. In this work, we demonstrate that the value function for continuous POMDPs is convex in the beliefs over continuous state spaces, and piecewise-linear convex for the particular case of discrete observations and actions but still continuous states. We also demonstrate that continuous Bellman backups are contracting and isotonic ensuring the monotonic convergence of value-iteration algorithms. Relying on those properties, we extend the algorithm, originally developed for discrete POMDPs, to work in continuous state spaces by representing the observation, transition, and reward models using Gaussian mixtures, and the beliefs using Gaussian mixtures or particle sets. With these representations, the integrals that appear in the Bellman backup can be computed in closed form and, therefore, the algorithm is computationally feasible. Finally, we further extend to deal with continuous action and observation sets by designing effective sampling approaches.},
 author = {Josep M. Porta and Nikos Vlassis and Matthijs T.J. Spaan and Pascal Poupart},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W2144088174},
 pages = {2329--2367},
 title = {Point-Based Value Iteration for Continuous POMDPs},
 url = {http://jmlr.org/papers/v7/porta06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:raghavan06a,
 abstract = {We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is significant potential in improving classifier performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion of the most relevant features (over 50% in our experiments). We find that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves labeling features and documents which significantly accelerates standard active learning in our simulation experiments. Feature feedback can complement traditional active learning in applications such as news filtering, e-mail classification, and personalization, where the human teacher can have significant knowledge on the relevance of features.},
 author = {Hema Raghavan and Omid Madani and Rosie Jones},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W2166353350},
 pages = {1655--1686},
 title = {Active Learning with Feedback on Features and Instances},
 url = {http://jmlr.org/papers/v7/raghavan06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:ross06a,
 abstract = {Many perceptual models and theories hinge on treating objects as a collection of constituent parts. When applying these approaches to data, a fundamental problem arises: how can we determine what are the parts? We attack this problem using learning, proposing a form of generative latent factor model, in which each data dimension is allowed to select a different factor or part as its explanation. This approach permits a range of variations that posit different models for the appearance of a part. Here we provide the details for two such models: a discrete and a continuous one. Further, we show that this latent factor model can be extended hierarchically to account for correlations between the appearances of different parts. This permits modelling of data consisting of multiple categories, and learning these categories simultaneously with the parts when they are unobserved. Experiments demonstrate the ability to learn parts-based representations, and categories, of facial images and user-preference data.},
 author = {David A. Ross and Richard S. Zemel},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W2138137770},
 pages = {2369--2397},
 title = {Learning Parts-Based Representations of Data},
 url = {http://jmlr.org/papers/v7/ross06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:rousu06a,
 abstract = {We present a kernel-based algorithm for hierarchical text classification where the documents are allowed to belong to more than one category at a time. The classification model is a variant of the Maximum Margin Markov Network framework, where the classification hierarchy is represented as a Markov tree equipped with an exponential family defined on the edges. We present an efficient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set.

Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classification hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efficient as training independent SVM-light classifiers for each node. The algorithm's predictive accuracy was found to be competitive with other recently introduced hierarchical multi-category or multilabel classification learning algorithms.},
 author = {Juho Rousu and Craig Saunders and Sandor Szedmak and John Shawe-Taylor},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W2135140174},
 pages = {1601--1626},
 title = {Kernel-Based Learning of Hierarchical Multilabel Classification Models},
 url = {http://jmlr.org/papers/v7/rousu06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:roverato06a,
 abstract = {This paper deals with chain graph models under alternative AMP interpretation. A new representative of an AMP Markov equivalence class, called the largest deflagged graph, is proposed. The representative is based on revealed internal structure of the AMP Markov equivalence class. More specifically, the AMP Markov equivalence class decomposes into finer strong equivalence classes and there exists a distinguished strong equivalence class among those forming the AMP Markov equivalence class. The largest deflagged graph is the largest chain graph in that distinguished strong equivalence class. A composed graphical procedure to get the largest deflagged graph on the basis of any AMP Markov equivalent chain graph is presented. In general, the largest deflagged graph differs from the AMP essential graph, which is another representative of the AMP Markov equivalence class.},
 author = {Alberto Roverato and Milan Studen&#253;},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W2126408332},
 pages = {1045--1078},
 title = {A Graphical Representation of Equivalence Classes of AMP Chain Graphs},
 url = {http://jmlr.org/papers/v7/roverato06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:ryabko06a,
 abstract = {In this work we consider the task of relaxing the i.i.d. assumption in pattern recognition (or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples (pairs of objects and labels). We consider the case of deterministically defined labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold.

We find a broad class of learning algorithms for which estimations of the probability of the classification error achieved under the classical i.i.d. assumption can be generalized to the similar estimates for case of conditionally i.i.d. examples.},
 author = {Daniil Ryabko},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2121796365},
 pages = {645--664},
 title = {Pattern Recognition for Conditionally Independent Data},
 url = {http://jmlr.org/papers/v7/ryabko06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:sahbi06a,
 abstract = {We introduce a computational design for pattern detection based on a tree-structured of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly finer subsets. The hierarchy is traversed coarse-to-fine and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a which balances overall error and computation.

Initially, SVMs are constructed for each cell with no constraints. This network is then perturbed, cell by cell, into another network, which is in two ways: first, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives.

When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free is already faster and more accurate than applying a single pose-specific SVM many times. The graded promotes very rapid processing of background regions while maintaining the discriminatory power of the free network.},
 author = {Hichem Sahbi and Donald Geman},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W2169625877},
 pages = {2087--2123},
 title = {A Hierarchy of Support Vector Machines for Pattern Detection},
 url = {http://jmlr.org/papers/v7/sahbi06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:scheinberg06a,
 abstract = {We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is bas...},
 author = {Katya Scheinberg},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W3011124486},
 pages = {2237--2257},
 title = {An Efficient Implementation of an Active Set Method for SVMs},
 url = {http://jmlr.org/papers/v7/scheinberg06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:schmitt06a,
 abstract = {Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-the-best searches for a sufficiently good ordering of cues (or features) in a task where objects are to be compared lexicographically. We investigate the computational complexity of finding optimal cue permutations for lexicographic strategies and prove that the problem is NP-complete. It follows that no efficient (that is, polynomial-time) algorithm computes optimal solutions, unless P=NP. We further analyze the complexity of approximating optimal cue permutations for lexicographic strategies. We show that there is no efficient algorithm that approximates the optimum to within any constant factor, unless P=NP.

The results have implications for the complexity of learning lexicographic strategies from examples. They show that learning them in polynomial time within the model of agnostic probably approximately correct (PAC) learning is impossible, unless RP=NP. We further consider greedy approaches for building lexicographic strategies and determine upper and lower bounds for the performance ratio of simple algorithms. Moreover, we present a greedy algorithm that performs provably better than take-the-best. Tight bounds on the sample complexity for learning lexicographic strategies are also given in this article.},
 author = {Michael Schmitt and Laura Martignon},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W2109110363},
 pages = {55--83},
 title = {On the Complexity of Learning Lexicographic Strategies},
 url = {http://jmlr.org/papers/v7/schmitt06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:schraudolph06a,
 abstract = {This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefficient vector but an element of the RKHS. We derive efficient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efficient online multiclass classification. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size.},
 author = {S. V. N. Vishwanathan and Nicol N. Schraudolph and Alex J. Smola},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W2123107849},
 pages = {1107--1133},
 title = {Step Size Adaptation in Reproducing Kernel Hilbert Space},
 url = {http://jmlr.org/papers/v7/schraudolph06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:scott06a,
 abstract = {Given a probability measure P and a reference measure μ, one is often interested in the minimum μ-measure set with P-measure at least α. Minimum volume sets of this type summarize the regions of greatest probability mass of P, and are useful for detecting anomalies and constructing confidence regions. This paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to P. Other than these samples, no other information is available regarding P, but the reference measure μ is assumed to be known. We introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classification. As in classification, we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn. Thus we obtain finite sample size performance bounds in terms of VC dimension and related quantities. We also demonstrate strong universal consistency and an oracle inequality. Estimators based on histograms and dyadic partitions illustrate the proposed rules.},
 author = {Clayton D. Scott and Robert D. Nowak},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W2139155312},
 pages = {665--704},
 title = {Learning Minimum Volume Sets},
 url = {http://jmlr.org/papers/v7/scott06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:shalev-shwartz06a,
 abstract = {We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by defining a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a significantly smaller size and prove that it attains the same minimum. We then describe an efficient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron defined by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show significant improvements in run time over a state of the art interior-point algorithm.},
 author = {Shai Shalev-Shwartz and Yoram Singer},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W2118353660},
 pages = {1567--1599},
 title = {Efficient Learning of Label Ranking by Soft Projections onto Polyhedra},
 url = {http://jmlr.org/papers/v7/shalev-shwartz06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:shimizu06a,
 abstract = {In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identification from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-specified time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artificially generated data and real-world data.},
 author = {Shohei Shimizu and Patrik O. Hoyer and Aapo Hyv&#228;rinen and Antti Kerminen},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W2151226328},
 pages = {2003--2030},
 title = {A Linear Non-Gaussian Acyclic Model for Causal Discovery},
 url = {http://jmlr.org/papers/v7/shimizu06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:shivaswamy06a,
 abstract = {We propose a novel second order cone programming formulation for designing robust classifiers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classification and regression problems. Experiments show that the proposed formulations outperform imputation.},
 author = {Pannagadatta K. Shivaswamy and Chiranjib Bhattacharyya and Alexander J. Smola},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W2147609908},
 pages = {1283--1314},
 title = {Second Order Cone Programming Approaches for Handling Missing and Uncertain Data},
 url = {http://jmlr.org/papers/v7/shivaswamy06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:silva06a,
 abstract = {We describe anytime search procedures that (1) find disjoint subsets of recorded variables for which the members of each subset are d-separated by a single common unrecorded cause, if such exists; (2) return information about the causal relations among the latent factors so identified. We prove the procedure is point-wise consistent assuming (a) the causal relations can be represented by a directed acyclic graph (DAG) satisfying the Markov Assumption and the Faithfulness Assumption; (b) unrecorded variables are not caused by recorded variables; and (c) dependencies are linear. We compare the procedure with standard approaches over a variety of simulated structures and sample sizes, and illustrate its practical value with brief studies of social science data sets. Finally, we consider generalizations for non-linear systems.},
 author = {Ricardo Silva and Richard Scheine and Clark Glymour and Peter Spirtes},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W2137099275},
 pages = {191--246},
 title = {Learning the Structure of Linear Latent Variable Models},
 url = {http://jmlr.org/papers/v7/silva06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:singliar06a,
 abstract = {We develop a new component analysis framework, the Noisy-Or Component Analyzer (NOCA), that targets high-dimensional binary data. NOCA is a probabilistic latent variable model that assumes the expression of observed high-dimensional binary data is driven by a small number of hidden binary sources combined via noisy-or units. The component analysis procedure is equivalent to learning of NOCA parameters. Since the classical EM formulation of the NOCA learning problem is intractable, we develop its variational approximation. We test the NOCA framework on two problems: (1) a synthetic image-decomposition problem and (2) a co-citation data analysis problem for thousands of CiteSeer documents. We demonstrate good performance of the new model on both problems. In addition, we contrast the model to two mixture-based latent-factor models: the probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA). Differing assumptions underlying these models cause them to discover different types of structure in co-citation data, thus illustrating the benefit of NOCA in building our understanding of high-dimensional data sets.},
 author = {Tom{{\'a}}{\v{s}} &#352;ingliar and Milo{\v{s}} Hauskrecht},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W1545527235},
 pages = {2189--2213},
 title = {Noisy-OR Component Analysis and its Application to Link Analysis},
 url = {http://jmlr.org/papers/v7/singliar06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:sonnenburg06a,
 abstract = {While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of ke...},
 author = {S{{\"o}}ren Sonnenburg and Gunnar R{{\"a}}tsch and Christin Sch{{\"a}}fer and Bernhard Sch{{\"o}}lkopf},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W2997532932},
 pages = {1531--1565},
 title = {Large Scale Multiple Kernel Learning},
 url = {http://jmlr.org/papers/v7/sonnenburg06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:spratling06a,
 abstract = {In order to perform object recognition it is necessary to learn representations of the underlying components of images. Such components correspond to objects, object-parts, or features. Non-negative matrix factorisation is a generative model that has been specifically proposed for finding such meaningful representations of image data, through the use of non-negativity constraints on the factors. This article reports on an empirical investigation of the performance of non-negative matrix factorisation algorithms. It is found that such algorithms need to impose additional constraints on the sparseness of the factors in order to successfully deal with occlusion. However, these constraints can themselves result in these algorithms failing to identify image components under certain conditions. In contrast, a recognition model (a competitive learning neural network algorithm) reliably and accurately learns representations of elementary image features without such constraints.},
 author = {Michael W. Spratling},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2117824861},
 pages = {793--815},
 title = {Learning Image Components for Object Recognition},
 url = {http://jmlr.org/papers/v7/spratling06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:sugiyama06a,
 abstract = {The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly specified. In many practical situations, however, this assumption may not be fulfilled. Recently, active learning methods using importance-weighted least-squares learning have been proposed, which are shown to be robust against misspecification of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be fine-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods.},
 author = {Masashi Sugiyama},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2114338449},
 pages = {141--166},
 title = {Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error},
 url = {http://jmlr.org/papers/v7/sugiyama06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:takeuchi06a,
 abstract = {In regression, the desired estimate of y|x is not always given by a conditional mean, although this is most common. Sometimes one wants to obtain a good estimate that satisfies the property that a proportion, τ, of y|x, will be below the estimate. For τ = 0.5 this is an estimate of the median. What might be called median regression, is subsumed under the term quantile regression. We present a nonparametric version of a quantile estimator, which can be obtained by solving a simple quadratic programming problem and provide uniform convergence statements and bounds on the quantile property of our estimator. Experimental results show the feasibility of the approach and competitiveness of our method with existing ones. We discuss several types of extensions including an approach to solve the quantile crossing problems, as well as a method to incorporate prior qualitative knowledge such as monotonicity constraints.},
 author = {Ichiro Takeuchi and Quoc V. Le and Timothy D. Sears and Alexander J. Smola},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W2167250202},
 pages = {1231--1264},
 title = {Nonparametric Quantile Estimation},
 url = {http://jmlr.org/papers/v7/takeuchi06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:taskar06a,
 abstract = {We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex flow, depending on the structure of the problem. We show that this approach provides a memory-efficient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.},
 author = {Ben Taskar and Simon Lacoste-Julien and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2100564780},
 pages = {1627--1653},
 title = {Structured Prediction, Dual Extragradient and Bregman Projections},
 url = {http://jmlr.org/papers/v7/taskar06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:vert06a,
 abstract = {We determine the asymptotic behaviour of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to infinity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held fixed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classification error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the first time to be a consistent density level set estimator.},
 author = {R{{\'e}}gis Vert and Jean-Philippe Vert},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W2137233422},
 pages = {817--854},
 title = {Consistency and Convergence Rates of One-Class SVMs and Related Algorithms},
 url = {http://jmlr.org/papers/v7/vert06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:wainwright06a,
 abstract = {Consider the problem of joint parameter estimation and prediction in a Markov random field: that is, the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the wrong model even in the infinite data limit) is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious benefit of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product.},
 author = {Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2096920988},
 pages = {1829--1859},
 title = {Estimating the Wrong Graphical Model: Benefits in the Computation-Limited Setting},
 url = {http://jmlr.org/papers/v7/wainwright06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:watanabe06a,
 abstract = {Bayesian learning has been widely used and proved to be effective in many data modeling problems. However, computations involved in it require huge costs and generally cannot be performed exactly. ...},
 author = {Kazuho Watanabe and Sumio Watanabe},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W2996794101},
 pages = {625--644},
 title = {Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation},
 url = {http://jmlr.org/papers/v7/watanabe06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:whiteson06a,
 abstract = {Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This paper investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efficient individual learning. This method evolves individuals that are better able to learn. We present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This paper also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. We evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the field of autonomic computing. The results demonstrate that evolutionary function approximation can significantly improve the performance of TD methods and on-line evolutionary computation can significantly improve evolutionary methods. This paper also presents additional tests that offer insight into what factors can make neural network function approximation difficult in practice.},
 author = {Shimon Whiteson and Peter Stone},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W2116339921},
 pages = {877--917},
 title = {Evolutionary Function Approximation for Reinforcement Learning},
 url = {http://jmlr.org/papers/v7/whiteson06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:wright06a,
 abstract = {Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of traffic as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identified using only the features that remain intact after encryption---namely packet size, timing, and direction. We first present what we believe to be the first exploratory look at protocol identification in encrypted tunnels which carry traffic from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identification in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classifiers achieve accuracy greater than 90% for several protocols in aggregate traffic, and, for most protocols, greater than 80% when making fine-grained classifications on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%.},
 author = {Charles V. Wright and Fabian Monrose and Gerald M. Masson},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2134632326},
 pages = {2745--2769},
 title = {On Inferring Application Protocol Behaviors in Encrypted Network Traffic},
 url = {http://jmlr.org/papers/v7/wright06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:wu06a,
 abstract = {Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classifier, whose key component is a weight vector in a feature space implicitly int...},
 author = {Mingrui Wu and Bernhard Sch{{\"o}}lkopf and G{{\"o}}khan Bakir},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W3005803528},
 pages = {603--624},
 title = {A Direct Method for Building Sparse Kernel Learning Algorithms},
 url = {http://jmlr.org/papers/v7/wu06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:yanover06a,
 abstract = {The problem of finding the most probable (MAP) configuration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for finding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems.

In this paper we compare tree-reweighted belief propagation (TRBP) and powerful general-purpose LP solvers (CPLEX) on relaxations of real-world graphical models from the fields of computer vision and computational biology. We find that TRBP almost always finds the solution significantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can find the MAP configurations in a matter of minutes for a large range of real world problems.},
 author = {Chen Yanover and Talya Meltzer and Yair Weiss},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2143524933},
 pages = {1887--1907},
 title = {Linear Programming Relaxations and Belief Propagation -- An Empirical Study},
 url = {http://jmlr.org/papers/v7/yanover06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:ye06a,
 abstract = {Dimensionality reduction is an important pre-processing step in many applications. Linear discriminant analysis (LDA) is a classical statistical approach for supervised dimensionality reduction. It...},
 author = {Jieping Ye and Tao Xiong},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W3018586685},
 pages = {1183--1204},
 title = {Computational and Theoretical Analysis of Null Space and Orthogonal Linear Discriminant Analysis},
 url = {http://jmlr.org/papers/v7/ye06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:zanni06a,
 abstract = {Parallel software for solving the quadratic program arising in training support vector machines for classification problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-of-the-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes.},
 author = {Luca Zanni and Thomas Serafini and Gaetano Zanghirati},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W2150062669},
 pages = {1467--1492},
 title = {Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems},
 url = {http://jmlr.org/papers/v7/zanni06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:zhang06a,
 abstract = {An ensemble is a group of learning models that jointly solve a problem. However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in effectiveness. The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. This subset selection problem is a combinatorial optimization problem and thus finding the exact optimal solution is computationally prohibitive. Various heuristic methods have been developed to obtain an approximate solution. However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. By applying semi-definite programming (SDP) as a solution technique, we are able to get better approximate solutions. Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. Its application in a classifier-sharing study also demonstrates the effectiveness of the method.},
 author = {Yi Zhang and Samuel Burer and W. Nick Street},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W2127082287},
 pages = {1315--1338},
 title = {Ensemble Pruning Via Semi-definite Programming},
 url = {http://jmlr.org/papers/v7/zhang06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:zhao06a,
 abstract = {Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes.

In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation.

This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are irrepresentable (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result.},
 author = {Peng Zhao and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W2150940164},
 pages = {2541--2563},
 title = {On Model Selection Consistency of Lasso},
 url = {http://jmlr.org/papers/v7/zhao06a.html},
 volume = {7},
 year = {2006}
}

@article{JMLR:v7:zhou06a,
 abstract = {In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overfitting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-fit in the limit of infinite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features.},
 author = {Jing Zhou and Dean P. Foster and Robert A. Stine and Lyle H. Ungar},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W2155648952},
 pages = {1861--1885},
 title = {Streamwise feature selection},
 url = {http://jmlr.org/papers/v7/zhou06a.html},
 volume = {7},
 year = {2006}
}
