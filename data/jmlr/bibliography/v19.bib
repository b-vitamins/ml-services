@article{JMLR:v19:13-538,
 abstract = {We study \emph{TV regularization}, a widely used technique for eliciting structured sparsity. In particular, we propose efficient algorithms for computing prox-operators for $\ell_p$-norm TV. The most important among these is $\ell_1$-norm TV, for whose prox-operator we present a new geometric analysis which unveils a hitherto unknown connection to taut-string methods. This connection turns out to be remarkably useful as it shows how our geometry guided implementation results in efficient weighted and unweighted 1D-TV solvers, surpassing state-of-the-art methods. Our 1D-TV solvers provide the backbone for building more complex (two or higher-dimensional) TV solvers within a modular proximal optimization approach. We review the literature for an array of methods exploiting this strategy, and illustrate the benefits of our modular design through extensive suite of experiments on (i) image denoising, (ii) image deconvolution, (iii) four variants of fused-lasso, and (iv) video denoising. To underscore our claims and permit easy reproducibility, we provide all the reviewed and our new TV solvers in an easy to use multi-threaded C++, Matlab and Python library.},
 author = {Alvaro Barbero and Suvrit Sra},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W311729721},
 pages = {1--82},
 title = {Modular proximal optimization for multidimensional total-variation regularization},
 url = {http://jmlr.org/papers/v19/13-538.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:14-033,
 abstract = {Markov blanket (Mb) and Markov boundary (MB) are two key concepts in Bayesian networks (BNs). In this paper, we study the problem of Mb and MB for multiple variables. First, we show that Mb possess...},
 author = {Xu-Qing Liu and Xin-Sheng Liu},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W3087217272},
 pages = {1--50},
 title = {Markov blanket and Markov boundary of multiple variables},
 url = {http://jmlr.org/papers/v19/14-033.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:15-020,
 abstract = {Scalable machine learning over big data is an important problem that is receiving a lot of attention in recent years. On popular distributed environments such as Hadoop running on a cluster of comm...},
 author = {Dhruv Mahajan and Nikunj Agrawal and S. Sathiya Keerthi and Sundararajan Sellamanickam and Leon Bottou},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W2964024720},
 pages = {1--37},
 title = {An efficient distributed learning algorithm based on effective local functional approximations},
 url = {http://jmlr.org/papers/v19/15-020.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:15-493,
 abstract = {We propose a new class of semiparametric exponential family graphical models for the analysis of high dimensional mixed data. Different from the existing mixed graphical models, we allow the nodewise conditional distributions to be semiparametric generalized linear models with unspecified base measure functions. Thus, one advantage of our method is that it is unnecessary to specify the type of each node and the method is more convenient to apply in practice. Under the proposed model, we consider both problems of parameter estimation and hypothesis testing in high dimensions. In particular, we propose a symmetric pairwise score test for the presence of a single edge in the graph. Compared to the existing methods for hypothesis tests, our approach takes into account of the symmetry of the parameters, such that the inferential results are invariant with respect to the different parametrizations of the same edge. Thorough numerical simulations and a real data example are provided to back up our results.},
 author = {Zhuoran Yang and Yang Ning and Han Liu},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W1653729022},
 pages = {1--59},
 title = {On Semiparametric Exponential Family Graphical Models},
 url = {http://jmlr.org/papers/v19/15-493.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:15-498,
 abstract = {The present work aims at deriving theoretical guaranties on the behavior of some cross-validation procedures applied to the $k$-nearest neighbors ($k$NN) rule in the context of binary classification. Here we focus on the leave-$p$-out cross-validation (L$p$O) used to assess the performance of the $k$NN classifier. Remarkably this L$p$O estimator can be efficiently computed in this context using closed-form formulas derived by \cite{CelisseMaryHuard11}. We describe a general strategy to derive moment and exponential concentration inequalities for the L$p$O estimator applied to the $k$NN classifier. Such results are obtained first by exploiting the connection between the L$p$O estimator and U-statistics, and second by making an intensive use of the generalized Efron-Stein inequality applied to the L$1$O estimator. One other important contribution is made by deriving new quantifications of the discrepancy between the L$p$O estimator and the classification error/risk of the $k$NN classifier. The optimality of these bounds is discussed by means of several lower bounds as well as simulation experiments.},
 author = {Alain Celisse and Tristan Mary-Huard},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W2761605154},
 pages = {1--54},
 title = {Theoretical analysis of cross-validation for estimating the risk of the k-nearest neighbor classifier},
 url = {http://jmlr.org/papers/v19/15-498.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-210,
 abstract = {The existence of singularities often affects the learning dynamics in feedforward neural networks. In this paper, based on theoretical analysis results, we numerically analyze the learning dynamics of radial basis function (RBF) networks near singularities to understand to what extent singularities influence the learning dynamics. First, we show the explicit expression of the Fisher information matrix for RBF networks. Second, we demonstrate through numerical simulations that the singularities have a significant impact on the learning dynamics of RBF networks. Our results show that overlap singularities mainly have influence on the low dimensional RBF networks and elimination singularities have a more significant impact to the learning processes than overlap singularities in both low and high dimensional RBF networks, whereas the plateau phenomena are mainly caused by the elimination singularities. The results can also be the foundation to investigate the singular learning dynamics in deep feedforward neural networks.},
 author = {Weili Guo and Haikun Wei and Yew-Soon Ong and Jaime Rubio Hervas and Junsheng Zhao and Hai Wang and Kanjian Zhang},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2893706220},
 pages = {1--39},
 title = {Numerical analysis near singularities in RBF networks},
 url = {http://jmlr.org/papers/v19/16-210.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-225,
 abstract = {We propose a two-stage penalized least squares method to build large systems of structural equations based on the instrumental variables view of the classical two-stage least squares method. We show that, with large numbers of endogenous and exogenous variables, the system can be constructed via consistent estimation of a set of conditional expectations at the first stage, and consistent selection of regulatory effects at the second stage. While the consistent estimation at the first stage can be obtained via the ridge regression, the adaptive lasso is employed at the second stage to achieve the consistent selection. The resultant estimates of regulatory effects enjoy the oracle properties. This method is computationally fast and allows for parallel implementation. We demonstrate its effectiveness via simulation studies and real data analysis.},
 author = {Chen Chen and Min Ren and Min Zhang and Dabao Zhang},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W2963107416},
 pages = {1--34},
 title = {A Two-Stage Penalized Least Squares Method for Constructing Large Systems of Structural Equations},
 url = {http://jmlr.org/papers/v19/16-225.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-241,
 abstract = {Minibatching is a very well studied and highly popular technique in supervised learning, used by practitioners due to its ability to accelerate training through better utilization of parallel processing power and reduction of stochastic variance. Another popular technique is importance sampling--a strategy for preferential sampling of more important examples also capable of accelerating the training process. However, despite considerable effort by the community in these areas, and due to the inherent technical difficulty of the problem, there is virtually no existing work combining the power of importance sampling with the strength of minibatching. In this paper we propose the first practical importance sampling for minibatches and give simple and rigorous complexity analysis of its performance. We illustrate on synthetic problems that for training data of certain properties, our sampling can lead to several orders of magnitude improvement in training time. We then test the new sampling on several popular data sets, and show that the improvement can reach an order of magnitude.},
 author = {Dominik Csiba and Peter Richt{{\'a}}rik},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W2963589953},
 pages = {1--21},
 title = {Importance sampling for minibatches},
 url = {http://jmlr.org/papers/v19/16-241.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-291,
 abstract = {Kernel mean embeddings have become a popular tool in machine learning. They map probability measures to functions in a reproducing kernel Hilbert space. The distance between two mapped measures defines a semi-distance over the probability measures known as the maximum mean discrepancy (MMD). Its properties depend on the underlying kernel and have been linked to three fundamental concepts of the kernel literature: universal, characteristic and strictly positive definite kernels.

The contributions of this paper are three-fold. First, by slightly extending the usual definitions of universal, characteristic and strictly positive definite kernels, we show that these three concepts are essentially equivalent. Second, we give the first complete characterization of those kernels whose associated MMD-distance metrizes the weak convergence of probability measures. Third, we show that kernel mean embeddings can be extended from probability measures to generalized measures called Schwartz-distributions and analyze a few properties of these distribution embeddings.},
 author = {Carl-Johann Simon-Gabriel and Bernhard Sch{{\"o}}lkopf},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W2962818398},
 pages = {1--29},
 title = {Kernel distribution embeddings: universal kernels, characteristic kernels and kernel metrics on distributions},
 url = {http://jmlr.org/papers/v19/16-291.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-349,
 abstract = {We study the density estimation problem with observations generated by certain dynamical systems that admit a unique underlying invariant Lebesgue density. Observations drawn from dynamical systems are not independent and moreover, usual mixing concepts may not be appropriate for measuring the dependence among these observations. By employing the $\mathcal{C}$-mixing concept to measure the dependence, we conduct statistical analysis on the consistency and convergence of the kernel density estimator. Our main results are as follows: First, we show that with properly chosen bandwidth, the kernel density estimator is universally consistent under $L_1$-norm; Second, we establish convergence rates for the estimator with respect to several classes of dynamical systems under $L_1$-norm. In the analysis, the density function $f$ is only assumed to be Hölder continuous which is a weak assumption in the literature of nonparametric density estimation and also more realistic in the dynamical system context. Last but not least, we prove that the same convergence rates of the estimator under $L_\infty$-norm and $L_1$-norm can be achieved when the density function is Hölder continuous, compactly supported and bounded. The bandwidth selection problem of the kernel density estimator for dynamical system is also discussed in our study via numerical simulations.},
 author = {Hanyuan Hang and Ingo Steinwart and Yunlong Feng and Johan A.K. Suykens},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W2963457386},
 pages = {1--49},
 title = {Kernel Density Estimation for Dynamical Systems},
 url = {http://jmlr.org/papers/v19/16-349.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-412,
 abstract = {For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of rank aggregation, for the Plackett-Luce model, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded offs against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data. Further, the proposed generalized rank-breaking algorithm involves set-wise comparisons as opposed to traditional pairwise comparisons. The maximum likelihood estimate of pairwise comparisons is computed efficiently using the celebrated minorization maximization algorithm (Hunter, 2004). To compute the pseudo-maximum likelihood estimate of the set-wise comparisons, we provide a generalization of the minorization maximization algorithm and give guarantees on its convergence.},
 author = {Ashish Khetan and Sewoong Oh},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2898679161},
 pages = {1--42},
 title = {Generalized rank-breaking: computational and statistical tradeoffs},
 url = {http://jmlr.org/papers/v19/16-412.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-432,
 abstract = {Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.},
 author = {Mateo Rojas-Carulla and Bernhard Sch{{\"o}}lkopf and Richard Turner and Jonas Peters},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W2891765548},
 pages = {1--34},
 title = {Invariant models for causal transfer learning},
 url = {http://jmlr.org/papers/v19/16-432.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-465,
 abstract = {We prove that stochastic gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider.},
 author = {Moritz Hardt and Tengyu Ma and Benjamin Recht},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W2964297418},
 pages = {1--44},
 title = {Gradient Descent Learns Linear Dynamical Systems},
 url = {http://jmlr.org/papers/v19/16-465.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-474,
 abstract = {One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent "absent levels" problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question's level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's random forests FORTRAN code and the randomForest R package (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model's performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found.},
 author = {Timothy C. Au},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W2626134199},
 pages = {1--30},
 title = {Random Forests, Decision Trees, and Categorical Predictors: The "Absent Levels" Problem},
 url = {http://jmlr.org/papers/v19/16-474.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-515,
 abstract = {When performing regression on a data set with p variables, it is often of interest to go beyond using main linear effects and include interactions as products between individual variables. For smal...},
 author = {Gian-Andrea Thanei and Nicolai Meinshausen and Rajen D. Shah},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2962965258},
 pages = {1--42},
 title = {The xyz algorithm for fast interaction search in high-dimensional data},
 url = {http://jmlr.org/papers/v19/16-515.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-534,
 abstract = {We introduce the submodularity ratio as a measure of how close to submodular a set function f is. We show that when f has submodularity ratio γ, the greedy algorithm for maximizing f provides a (1 - e-γ)-approximation. Furthermore, when γ is bounded away from 0, the greedy algorithm for minimum submodular cover also provides essentially an O(log n) approximation for a universe of n elements.

As a main application of this framework, we study the problem of selecting a subset of k random variables from a large set, in order to obtain the best linear prediction of another variable of interest. We analyze the performance of widely used greedy heuristics; in particular, by showing that the submodularity ratio is lower-bounded by the smallest 2k- sparse eigenvalue of the covariance matrix, we obtain the strongest known approximation guarantees for the Forward Regression and Orthogonal Matching Pursuit algorithms.

As a second application, we analyze greedy algorithms for the dictionary selection problem, and significantly improve the previously known guarantees. Our theoretical analysis is complemented by experiments on real-world and synthetic data sets; in particular, we focus on an analysis of how tight various spectral parameters and the submodularity ratio are in terms of predicting the performance of the greedy algorithms.},
 author = {Abhimanyu Das and David Kempe},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W2894187248},
 pages = {1--34},
 title = {Approximate submodularity and its applications: subset selection, sparse approximation and dictionary selection},
 url = {http://jmlr.org/papers/v19/16-534.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-554,
 abstract = {We consider a model selection problem in high-dimensional binary Markov random fields. The usefulness of the Ising model in studying systems of complex interactions has been confirmed in many paper...},
 author = {Blazej Miasojedow and Wojciech Rejchel},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W2998369233},
 pages = {1--26},
 title = {Sparse estimation in Ising model via penalized Monte Carlo methods},
 url = {http://jmlr.org/papers/v19/16-554.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-569,
 author = {Nicole M{{\"u}}cke and Gilles Blanchard},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W2898975352},
 pages = {1--29},
 title = {Parallelizing Spectrally Regularized Kernel Algorithms},
 url = {http://jmlr.org/papers/v19/16-569.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:16-656,
 abstract = {Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike exist- ing models, an HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning an HASMM from the EHR data is achieved via a novel forward- filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward- filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient's continuous-time state trajectory. We demonstrate the di- agnostic and prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center.},
 author = {Ahmed M. Alaa and Mihaela van der Schaar},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2583977184},
 pages = {1--62},
 title = {A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference},
 url = {http://jmlr.org/papers/v19/16-656.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-006,
 abstract = {We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where p < n but p/n is not close to zero. We consider ordinary least-squares as well as robust ...},
 author = {Noureddine El Karoui and Elizabeth Purdom},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2894361858},
 pages = {1--66},
 title = {Can We Trust the Bootstrap in High-dimensions? The Case of Linear Models},
 url = {http://jmlr.org/papers/v19/17-006.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-016,
 abstract = {In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bf G}radient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an $\epsilon$-optimal solution with a lower complexity than the SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the $\epsilon$-level set and the optimal set {multiplied by a logarithmic factor}. Moreover, we show the advantages of RSG over SG in solving three different families of convex optimization problems. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property in the $\epsilon$-sublevel set, RSG has an $O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity. (c) For the problems that admit a local Kurdyka-Łojasiewicz property with a power constant of $\beta\in[0,1)$, RSG has an $O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the $\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-Łojasiewicz property, more generally local error bound conditions) to develop the improved convergence of RSG. { We also develop a practical variant of RSG enjoying faster convergence than the SG method, which can be run without knowing the involved parameters in the local error bound condition.} We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression, classification and matrix completion.},
 author = {Tianbao Yang and Qihang Lin},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2964347994},
 pages = {1--33},
 title = {RSG: Beating Subgradient Method without Smoothness and Strong Convexity},
 url = {http://jmlr.org/papers/v19/17-016.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-025,
 abstract = {We present upper and lower bounds for the prediction error of the Lasso. For the case of random Gaussian design, we show that under mild conditions the prediction error of the Lasso is up to smaller order terms dominated by the prediction error of its noiseless counterpart. We then provide exact expressions for the prediction error of the latter, in terms of compatibility constants. Here, we assume the active components of the underlying regression function satisfy some betamin condition. For the case of fixed design, we provide upper and lower bounds, again in terms of compatibility constants. As an example, we give an up to a logarithmic term tight bound for the least squares estimator with total variation penalty.},
 author = {Sara van de Geer},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W2964317129},
 pages = {1--48},
 title = {On Tight Bounds for the Lasso},
 url = {http://jmlr.org/papers/v19/17-025.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-042,
 abstract = {This paper presents a new approach for Gaussian process (GP) regression for large datasets. The approach involves partitioning the regression input domain into multiple local regions with a different local GP model fitted in each region. Unlike existing local partitioned GP approaches, we introduce a technique for patching together the local GP models nearly seamlessly to ensure that the local GP models for two neighboring regions produce nearly the same response prediction and prediction error variance on the boundary between the two regions. This largely mitigates the well-known discontinuity problem that degrades the boundary accuracy of existing local partitioned GP methods. Our main innovation is to represent the continuity conditions as additional pseudo-observations that the differences between neighboring GP responses are identically zero at an appropriately chosen set of boundary input locations. To predict the response at any input location, we simply augment the actual response observations with the pseudo-observations and apply standard GP prediction methods to the augmented data. In contrast to heuristic continuity adjustments, this has an advantage of working within a formal GP framework, so that the GP-based predictive uncertainty quantification remains valid. Our approach also inherits a sparse block-like structure for the sample covariance matrix, which results in computationally efficient closed-form expressions for the predictive mean and variance. In addition, we provide a new spatial partitioning scheme based on a recursive space partitioning along local principal component directions, which makes the proposed approach applicable for regression domains having more than two dimensions. Using three spatial datasets and three higher dimensional datasets, we investigate the numerical performance of the approach and compare it to several state-of-the-art approaches.},
 author = {Chiwoo Park and Daniel Apley},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W4299604521},
 pages = {1--43},
 title = {Patchwork Kriging for Large-scale Gaussian Process Regression},
 url = {http://jmlr.org/papers/v19/17-042.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-084,
 abstract = {Divide-and-conquer based methods for Bayesian inference provide a general approach for tractable posterior inference when the sample size is large. These methods divide the data into smaller subset...},
 author = {Sanvesh Srivastava and Cheng Li and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W2964276553},
 pages = {1--35},
 title = {Scalable Bayes via Barycenter in Wasserstein Space},
 url = {http://jmlr.org/papers/v19/17-084.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-112,
 abstract = {Learning a low-rank matrix from missing and corrupted observations is a fundamental problem in many machine learning applications. However, the role of side information in low-rank matrix learning ...},
 author = {Kai-Yang Chiang and Inderjit S. Dhillon and Cho-Jui Hsieh},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W2914720434},
 pages = {1--35},
 title = {Using Side Information to Reliably Learn Low-Rank Matrices from Missing and Corrupted Observations},
 url = {http://jmlr.org/papers/v19/17-112.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-128,
 abstract = {Given matrices $X,Y \in R^{n \times K}$ and $S \in R^{K \times K}$ with positive elements, this paper proposes an algorithm fastRG to sample a sparse matrix $A$ with low rank expectation $E(A) = XSY^T$ and independent Poisson elements. This allows for quickly sampling from a broad class of stochastic blockmodel graphs (degree-corrected, mixed membership, overlapping) all of which are specific parameterizations of the generalized random product graph model defined in Section 2.2. The basic idea of fastRG is to first sample the number of edges $m$ and then sample each edge. The key insight is that because of the the low rank expectation, it is easy to sample individual edges. The naive "element-wise" algorithm requires $O(n^2)$ operations to generate the $n\times n$ adjacency matrix $A$. In sparse graphs, where $m = O(n)$, ignoring log terms, fastRG runs in time $O(n)$. An implementation in fastRG is available on github. A computational experiment in Section 2.4 simulates graphs up to $n=10,000,000$ nodes with $m = 100,000,000$ edges. For example, on a graph with $n=500,000$ and $m = 5,000,000$, fastRG runs in less than one second on a 3.5 GHz Intel i5.},
 author = {Karl Rohe and Jun Tao and Xintian Han and Norbert Binkiewicz},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W2950831917},
 pages = {1--13},
 title = {A note on quickly sampling a sparse matrix with low rank expectation},
 url = {http://jmlr.org/papers/v19/17-128.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-131,
 abstract = {Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience retention and experience sampling. We refer to the combination as experience selection. We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about the characteristics of the control problem at hand to choose the appropriate experience replay strategy.},
 author = {Tim de Bruin and Jens Kober and Karl Tuyls and Robert Babu{\v{s}}ka},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W2892979040},
 pages = {1--56},
 title = {Experience selection in deep reinforcement learning for control},
 url = {http://jmlr.org/papers/v19/17-131.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-144,
 abstract = {We show a Talagrand-type concentration inequality for Multi-Task Learning (MTL), using which we establish sharp excess risk bounds for MTL in terms of distribution- and data-dependent versions of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for norm regularized as well as strongly convex hypothesis classes, which applies not only to MTL but also to the standard i.i.d. setting. Combining both results, one can now easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including---as we demonstrate---Schatten-norm, group-norm, and graph-regularized MTL. The derived bounds reflect a relationship akeen to a conservation law of asymptotic convergence rates. This very relationship allows for trading off slower rates w.r.t. the number of tasks for faster rates with respect to the number of available samples per task, when compared to the rates obtained via a traditional, global Rademacher analysis.},
 author = {Niloofar Yousefi and Yunwen Lei and Marius Kloft and Mansooreh Mollaghasemi and Georgios C. Anagnostopoulos},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W2963225057},
 pages = {1--47},
 title = {Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning},
 url = {http://jmlr.org/papers/v19/17-144.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-165,
 author = {Jayadev Acharya and Moein Falahatgar and Ashkan Jafarpour and Alon Orlitsky and Ananda Theertha Suresh},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W2910103617},
 pages = {1--31},
 title = {Maximum Selection and Sorting with Adversarial Comparators},
 url = {http://jmlr.org/papers/v19/17-165.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-179,
 author = {Ivo F. D. Oliveira and Nir Ailon and Ori Davidov},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2914704832},
 pages = {1--29},
 title = {A New and Flexible Approach to the Analysis of Paired Comparison Data},
 url = {http://jmlr.org/papers/v19/17-179.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-194,
 author = {Jian Huang and Yuling Jiao and Yanyan Liu and Xiliang Lu},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2893246539},
 pages = {1--37},
 title = {A Constructive Approach to $L_0$ Penalized Regression},
 url = {http://jmlr.org/papers/v19/17-194.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-218,
 author = {Leland Bybee and Yves Atchad{{\'e}}},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W2892750663},
 pages = {1--38},
 title = {Change-Point Computation for Large Graphical Models: A Scalable Algorithm for Gaussian Graphical Models with Change-Points},
 url = {http://jmlr.org/papers/v19/17-218.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-244,
 abstract = {We propose a new iteratively reweighted least squares (IRLS) algorithm for the recovery of a matrix X ∈ Cd1×d2 of rank r ≪ min(d1, d2) from incomplete linear observations, solving a sequence of low complexity linear problems. The easily implementable algorithm, which we call harmonic mean iteratively reweighted least squares (HM-IRLS), optimizes a non-convex Schatten-p quasi-norm penalization to promote low-rankness and carries three major strengths, in particular for the matrix completion setting. First, we observe a remarkable global convergence behavior of the algorithm's iterates to the low-rank matrix for relevant, interesting cases, for which any other state-of-the-art optimization approach fails the recovery. Secondly, HM-IRLS exhibits an empirical recovery probability close to 1 even for a number of measurements very close to the theoretical lower bound r(d1+d2-r), i.e., already for significantly fewer linear observations than any other tractable approach in the literature. Thirdly, HM-IRLS exhibits a locally superlinear rate of convergence (of order 2 - p) if the linear observations fulfill a suitable null space property. While for the first two properties we have so far only strong empirical evidence, we prove the third property as our main theoretical result.},
 author = {Christian K{{\"u}}mmerle and Juliane Sigl},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W2902112793},
 pages = {1--49},
 title = {Harmonic mean iteratively reweighted least squares for low-rank matrix recovery},
 url = {http://jmlr.org/papers/v19/17-244.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-283,
 abstract = {We consider off-policy temporal-difference (TD) learning in discounted Markov decision processes, where the goal is to evaluate a policy in a model-free way by using observations of a state process generated without executing the policy. To curb the high variance issue in off-policy TD learning, we propose a new scheme of setting the $$\lambda $$ parameters of TD, based on generalized Bellman equations. Our scheme is to set $$\lambda $$ according to the eligibility trace iterates calculated in TD, thereby easily keeping these traces in a desired bounded range. Compared to prior works, this scheme is more direct and flexible, and allows much larger $$\lambda $$ values for off-policy TD learning with bounded traces. Using Markov chain theory, we prove the ergodicity of the joint state-trace process under nonrestrictive conditions, and we show that associated with our scheme is a generalized Bellman equation (for the policy to be evaluated) that depends on both $$\lambda $$ and the unique invariant probability measure of the state-trace process. These results not only lead immediately to a characterization of the convergence behavior of least-squares based implementation of our scheme, but also prepare the ground for further analysis of gradient-based implementations.},
 author = {Huizhen Yu and A. Rupam Mahmood and Richard S. Sutton},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W2606786028},
 pages = {1--49},
 title = {On Generalized Bellman Equations and Temporal-Difference Learning},
 url = {http://jmlr.org/papers/v19/17-283.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-285,
 abstract = {Quadratic discriminant analysis (QDA) is a standard tool for classification due to its simplicity and flexibility. Because the number of its parameters scales quadratically with the number of the variables, QDA is not practical, however, when the dimensionality is relatively large. To address this, we propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional data. Formulated in a simple and coherent framework, DA-QDA aims to directly estimate the key quantities in the Bayes discriminant function including quadratic interactions and a linear index of the variables for classification. Under appropriate sparsity assumptions, we establish consistency results for estimating the interactions and the linear index, and further demonstrate that the misclassification rate of our procedure converges to the optimal Bayes risk, even when the dimensionality is exponentially high with respect to the sample size. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed for finding interactions, which is much faster than its competitor in the literature. The promising performance of DA-QDA is illustrated via extensive simulation studies and the analysis of four real datasets.},
 author = {Binyan Jiang and Xiangyu Wang and Chenlei Leng},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W2963551774},
 pages = {1--37},
 title = {A direct approach for sparse quadratic discriminant analysis},
 url = {http://jmlr.org/papers/v19/17-285.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-291,
 abstract = {In this article, we study the question of the statistical convergence of the 1-dimensional Mapper to its continuous analogue, the Reeb graph. We show that the Mapper is an optimal estimator of the Reeb graph, which gives, as a byproduct, a method to automatically tune its parameters and compute confidence regions on its topological features, such as its loops and flares. This allows to circumvent the issue of testing a large grid of parameters and keeping the most stable ones in the brute-force setting, which is widely used in visualization, clustering and feature selection with the Mapper.},
 author = {Mathieu Carri{{\`e}}re and Bertrand Michel and Steve Oudot},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W2621305858},
 pages = {1--39},
 title = {Statistical Analysis and Parameter Selection for Mapper},
 url = {http://jmlr.org/papers/v19/17-291.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-295,
 abstract = {We present a Distributionally Robust Optimization (DRO) approach to estimate a robustified regression plane in a linear regression setting, when the observed samples are potentially contaminated with adversarially corrupted outliers. Our approach mitigates the impact of outliers by hedging against a family of probability distributions on the observed data, some of which assign very low probabilities to the outliers. The set of distributions under consideration are close to the empirical distribution in the sense of the Wasserstein metric. We show that this DRO formulation can be relaxed to a convex optimization problem which encompasses a class of models. By selecting proper norm spaces for the Wasserstein metric, we are able to recover several commonly used regularized regression models. We provide new insights into the regularization term and give guidance on the selection of the regularization coefficient from the standpoint of a confidence region. We establish two types of performance guarantees for the solution to our formulation under mild conditions. One is related to its out-of-sample behavior (prediction bias), and the other concerns the discrepancy between the estimated and true regression planes (estimation bias). Extensive numerical results demonstrate the superiority of our approach to a host of regression models, in terms of the prediction and estimation accuracies. We also consider the application of our robust learning procedure to outlier detection, and show that our approach achieves a much higher AUC (Area Under the ROC Curve) than M-estimation (Huber, 1964, 1973).},
 author = {Ruidi Chen and Ioannis Ch. Paschalidis},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W2893687496},
 pages = {1--48},
 title = {A Robust Learning Approach for Regression Models Based on Distributionally Robust Optimization.},
 url = {http://jmlr.org/papers/v19/17-295.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-329,
 abstract = {Many of the recent trajectory optimization algorithms alternate between linear approximation of the system dynamics around the mean trajectory and conservative policy update. One way of constraining the policy change is by bounding the Kullback-Leibler (KL) divergence between successive policies. These approaches already demonstrated great experimental success in challenging problems such as end-to-end control of physical systems. However, the linear approximation of the system dynamics can introduce a bias in the policy update and prevent convergence to the optimal policy. In this article, we propose a new model-free trajectory-based policy optimization algorithm with guaranteed monotonic improvement. The algorithm backpropagates a local, quadratic and time-dependent \qfunc~learned from trajectory data instead of a model of the system dynamics. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics. We experimentally demonstrate on highly non-linear control tasks the improvement in performance of our algorithm in comparison to approaches linearizing the system dynamics. In order to show the monotonic improvement of our algorithm, we additionally conduct a theoretical analysis of our policy update scheme to derive a lower bound of the change in policy return between successive iterations.},
 author = {Riad Akrour and Abbas Abdolmaleki and Hany Abdulsamad and Jan Peters and Gerhard Neumann},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W2811292668},
 pages = {1--25},
 title = {Model-Free Trajectory-based Policy Optimization with Monotonic Improvement},
 url = {http://jmlr.org/papers/v19/17-329.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-345,
 author = {Luc Leh{{\'e}}ricy},
 journal = {Journal of Machine Learning Research},
 number = {39},
 pages = {1--46},
 title = {State-by-state Minimax Adaptive Estimation for Nonparametric Hidden {M}arkov Models},
 url = {http://jmlr.org/papers/v19/17-345.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-361,
 abstract = {This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover's distance in reference to the classical earth mover's distance. We develop two generic schemes that we respectively call the alternate scaling algorithm and the non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. These schemes are based on Dykstra's algorithm with alternate Bregman projections, and further exploit the Newton-Raphson method when applied to separable divergences. We enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our proposed framework and discuss the inherent specificities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate the merits of our methods with experiments using synthetic data to illustrate the effect of different regularizers and penalties on the solutions, as well as real-world data for a pattern recognition application to audio scene classification.},
 author = {Arnaud Dessein and Nicolas Papadakis and Jean-Luc Rouas},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2538817717},
 pages = {1--53},
 title = {Regularized Optimal Transport and the Rot Mover's Distance},
 url = {http://jmlr.org/papers/v19/17-361.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-370,
 author = {Yixin Fang and Jinfeng Xu and Lei Yang},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W2912349752},
 pages = {1--21},
 title = {Online Bootstrap Confidence Intervals for the Stochastic Gradient Descent Estimator},
 url = {http://jmlr.org/papers/v19/17-370.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-374,
 abstract = {Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features.},
 author = {Jarno Lintusaari and Henri Vuollekoski and Antti Kangasr{{\"a}}{{\"a}}si{{\"o}} and Kusti Skytén and Marko J{{\"a}}rvenp{{\"a}}{{\"a}} and Pekka Marttinen and Michael U. Gutmann and Aki Vehtari and Jukka Corander and Samuel Kaski},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2953009816},
 pages = {1--7},
 title = {ELFI: Engine for Likelihood-Free Inference},
 url = {http://jmlr.org/papers/v19/17-374.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-383,
 abstract = {Binary, or one-bit, representations of data arise naturally in many applications, and are appealing in both hardware implementations and algorithm design. In this work, we study the problem of data classification from binary data and propose a framework with low computation and resource costs. We illustrate the utility of the proposed approach through stylized and realistic numerical experiments, and provide a theoretical analysis for a simple case. We hope that our framework and analysis will serve as a foundation for studying similar types of approaches.},
 author = {Deanna Needell and Rayan Saab and Tina Woolf},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W2734202545},
 pages = {1--30},
 title = {Simple Classification using Binary Data},
 url = {http://jmlr.org/papers/v19/17-383.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-402,
 author = {Dolev Raviv and Tamir Hazan and Margarita Osadchy},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W2913450167},
 pages = {1--30},
 title = {Hinge-Minimax Learner for the Ensemble of Hyperplanes},
 url = {http://jmlr.org/papers/v19/17-402.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-404,
 abstract = {We consider the problem of streaming kernel regression, when the observations arrive sequentially and the goal is to recover the underlying mean function, assumed to belong to an RKHS. The variance of the noise is not assumed to be known. In this context, we tackle the problem of tuning the regularization parameter adaptively at each time step, while maintaining tight confidence bounds estimates on the value of the mean function at each point. To this end, we first generalize existing results for finite-dimensional linear regression with fixed regularization and known variance to the kernel setup with a regularization parameter allowed to be a measurable function of past observations. Then, using appropriate self-normalized inequalities we build upper and lower bound estimates for the variance, leading to Bersntein-like concentration bounds. The later is used in order to define the adaptive regularization. The bounds resulting from our technique are valid uniformly over all observation points and all time steps, and are compared against the literature with numerical experiments. Finally, the potential of these tools is illustrated by an application to kernelized bandits, where we revisit the Kernel UCB and Kernel Thompson Sampling procedures, and show the benefits of the novel adaptive kernel tuning strategy.},
 author = {Audrey Durand and Odalric-Ambrym Maillard and Joelle Pineau},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W2963188341},
 pages = {1--34},
 title = {Streaming kernel regression with provably adaptive mean, variance, and regularization},
 url = {http://jmlr.org/papers/v19/17-404.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-421,
 abstract = {This article provides an original understanding of the behavior of a class of graph-oriented semi-supervised learning algorithms in the limit of large and numerous data. It is demonstrated that the intuition at the root of these methods collapses in this limit and that, as a result, most of them become inconsistent. Corrective measures and a new data-driven parametrization scheme are proposed along with a theoretical analysis of the asymptotic performances of the resulting approach. A surprisingly close behavior between theoretical performances on Gaussian mixture models and on real datasets is also illustrated throughout the article, thereby suggesting the importance of the proposed analysis for dealing with practical data. As a result, significant performance gains are observed on practical data classification using the proposed parametrization.},
 author = {Xiaoyi Mai},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W2963720057},
 pages = {1--27},
 title = {A random matrix analysis and improvement of semi-supervised learning for large dimensional data},
 url = {http://jmlr.org/papers/v19/17-421.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-436,
 abstract = {We consider the problem of outlier rejection in single subspace learning. Classical approaches work directly with a low-dimensional representation of the subspace. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement. We pose this problem as an l1-minimization problem on the sphere and show that, under certain conditions on the distribution of the data, any global minimizer of this non-convex problem gives a vector orthogonal to the subspace. Moreover, we show that such a vector can still be found by relaxing the non-convex problem with a sequence of linear programs. Experiments on synthetic and real data show that the proposed approach, which we call Dual Principal Component Pursuit (DPCP), outperforms state-of-the art methods, especially in the case of high-dimensional subspaces.},
 author = {Manolis C. Tsakiris and Ren{{\'e}} Vidal},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W2220473304},
 pages = {1--50},
 title = {Dual Principal Component Pursuit},
 url = {http://jmlr.org/papers/v19/17-436.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-444,
 abstract = {With ever growing data volume and model size, an error-tolerant, communication efficient, yet versatile distributed algorithm has become vital for the success of many large-scale machine learning applications. In this work we propose m-PAPG, an implementation of the flexible proximal gradient algorithm in model parallel systems equipped with the partially asynchronous communication protocol. The worker machines communicate asynchronously with a controlled staleness bound $s$ and operate at different frequencies. We characterize various convergence properties of m-PAPG: 1) Under a general non-smooth and non-convex setting, we prove that every limit point of the sequence generated by m-PAPG is a critical point of the objective function; 2) Under an error bound condition, we prove that the function value decays linearly for every $s$ steps; 3) Under the Kurdyka-${\L}$ojasiewicz inequality, we prove that the sequences generated by m-PAPG converge to the same critical point, provided that a proximal Lipschitz condition is satisfied.},
 author = {Yi Zhou and Yingbin Liang and Yaoliang Yu and Wei Dai and Eric P. Xing},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2949950906},
 pages = {1--32},
 title = {Distributed Proximal Gradient Algorithm for Partially Asynchronous Computer Clusters},
 url = {http://jmlr.org/papers/v19/17-444.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-473,
 abstract = {Robust PCA is a widely used statistical procedure to recover a underlying low-rank matrix with grossly corrupted observations. This work considers the problem of robust PCA as a nonconvex optimization problem on the manifold of low-rank matrices, and proposes two algorithms (for two versions of retractions) based on manifold optimization. It is shown that, with a proper designed initialization, the proposed algorithms are guaranteed to converge to the underlying low-rank matrix linearly. Compared with a previous work based on the Burer-Monterio decomposition of low-rank matrices, the proposed algorithms reduce the dependence on the conditional number of the underlying low-rank matrix theoretically. Simulations and real data examples confirm the competitive performance of our method.},
 author = {Teng Zhang and Yi Yang},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W2761260651},
 pages = {1--39},
 title = {Robust PCA by Manifold Optimization},
 url = {http://jmlr.org/papers/v19/17-473.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-511,
 abstract = {Neural Information Processing Systems (NIPS) is a top-tier annual conference in machine learning. The 2016 edition of the conference comprised more than 2,400 paper submissions, 3,000 reviewers, and 8,000 attendees. This represents a growth of nearly 40% in terms of submissions, 96% in terms of reviewers, and over 100% in terms of attendees as compared to the previous year. The massive scale as well as rapid growth of the conference calls for a thorough quality assessment of the peer-review process and novel means of improvement. In this paper, we analyze several aspects of the data collected during the review process, including an experiment investigating the efficacy of collecting ordinal rankings from reviewers. Our goal is to check the soundness of the review process, and provide insights that may be useful in the design of the review process of subsequent conferences.},
 author = {Nihar B. Shah and Behzad Tabibian and Krikamol Muandet and Isabelle Guyon and Ulrike von Luxburg},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W2752524112},
 pages = {1--34},
 title = {Design and Analysis of the NIPS 2016 Review Process},
 url = {http://jmlr.org/papers/v19/17-511.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-513,
 author = {Tor Lattimore},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W2894130846},
 pages = {1--32},
 title = {Refining the Confidence Level for Optimistic Bandit Strategies},
 url = {http://jmlr.org/papers/v19/17-513.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-537,
 abstract = {Although neural networks are routinely and successfully trained in practice using simple gradient-based methods, most existing theoretical results are negative, showing that learning such networks is difficult, in a worst-case sense over all data distributions. In this paper, we take a more nuanced view, and consider whether specific assumptions on the niceness of the input distribution, or niceness of the target function (e.g. in terms of smoothness, non-degeneracy, incoherence, random choice of parameters etc.), are sufficient to guarantee learnability using gradient-based methods. We provide evidence that neither class of assumptions alone is sufficient: On the one hand, for any member of a class of nice target functions, there are difficult input distributions. On the other hand, we identify a family of simple target functions, which are di_cult to learn even if the input distribution is nice. To prove our results, we develop some tools which may be of independent interest, such as extending Fourier-based hardness techniques developed in the context of statistical queries (Blum et al., 1994), from the Boolean cube to Euclidean space and to more general classes of functions.},
 author = {Ohad Shamir},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W2963325933},
 pages = {1--29},
 title = {Distribution-specific hardness of learning neural networks},
 url = {http://jmlr.org/papers/v19/17-537.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-558,
 abstract = {We propose a short-term sparse portfolio optimization (SSPO) system based on alternating direction method of multipliers (ADMM). Although some existing strategies have also exploited sparsity, they either constrain the quantity of the portfolio change or aim at the long-term portfolio optimization. Very few of them are dedicated to constructing sparse portfolios for the short-term portfolio optimization, which will be complemented by the proposed SSPO. SSPO concentrates wealth on a small proportion of assets that have good increasing potential according to some empirical financial principles, so as to maximize the cumulative wealth for the whole investment. We also propose a solving algorithm based on ADMM to handle the l1-regularization term and the self-financing constraint simultaneously. As a significant improvement in the proposed ADMM, we have proven that its augmented Lagrangian has a saddle point, which is the foundation of the iterative formulae of ADMM but is seldom addressed by other sparsity strategies. Extensive experiments on 5 benchmark data sets from real-world stock markets show that SSPO outperforms other state-of-the-art systems in thorough evaluations, withstands reasonable transaction costs and runs fast. Thus it is suitable for real-world financial environments.},
 author = {Zhao-Rong Lai and Pei-Yi Yang and Liangda Fang and Xiaotian Wu},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W2914085915},
 pages = {1--28},
 title = {Short-term sparse portfolio optimization based on alternating direction method of multipliers},
 url = {http://jmlr.org/papers/v19/17-558.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-573,
 abstract = {There has been considerable interest in making Bayesian inference more scalable. In big data settings, most literature focuses on reducing the computing time per iteration, with less focused on reducing the number of iterations needed in Markov chain Monte Carlo (MCMC). This article focuses on data augmentation MCMC (DA-MCMC), a widely used technique. DA-MCMC samples tend to become highly autocorrelated in large data samples, due to a miscalibration problem in which conditional posterior distributions given augmented data are too concentrated. This makes it necessary to collect very long MCMC paths to obtain acceptably low MC error. To combat this inefficiency, we propose a family of calibrated data augmentation algorithms, which appropriately adjust the variance of conditional posterior distributions. A Metropolis-Hastings step is used to eliminate bias in the stationary distribution of the resulting sampler. Compared to existing alternatives, this approach can dramatically reduce MC error by reducing autocorrelation and increasing the effective number of DA-MCMC samples per computing time. The approach is simple and applicable to a broad variety of existing data augmentation algorithms, and we focus on three popular models: probit, logistic and Poisson log-linear. Dramatic gains in computational efficiency are shown in applications.},
 author = {Leo L. Duan and James E. Johndrow and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W2949636293},
 pages = {1--34},
 title = {Scaling up Data Augmentation MCMC via Calibration},
 url = {http://jmlr.org/papers/v19/17-573.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-607,
 abstract = {When tracking user-specific online activities, each user's preference is revealed in the form of choices and comparisons. For example, a user's purchase history is a record of her choices, i.e. which item was chosen among a subset of offerings. A user's preferences can be observed either explicitly as in movie ratings or implicitly as in viewing times of news articles. Given such individualized ordinal data in the form of comparisons and choices, we address the problem of collaboratively learning representations of the users and the items. The learned features can be used to predict a user's preference of an unseen item to be used in recommendation systems. This also allows one to compute similarities among users and items to be used for categorization and search. Motivated by the empirical successes of the MultiNomial Logit (MNL) model in marketing and transportation, and also more recent successes in word embedding and crowdsourced image embedding, we pose this problem as learning the MNL model parameters that best explain the data. We propose a convex relaxation for learning the MNL model, and show that it is minimax optimal up to a logarithmic factor by comparing its performance to a fundamental lower bound. This characterizes the minimax sample complexity of the problem, and proves that the proposed estimator cannot be improved upon other than by a logarithmic factor. Further, the analysis identifies how the accuracy depends on the topology of sampling via the spectrum of the sampling graph. This provides a guideline for designing surveys when one can choose which items are to be compared. This is accompanied by numerical simulations on synthetic and real data sets, confirming our theoretical predictions.},
 author = {Sahand Negahban and Sewoong Oh and Kiran K. Thekumparampil and Jiaming Xu},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W2963380955},
 pages = {1--95},
 title = {Learning from Comparisons and Choices},
 url = {http://jmlr.org/papers/v19/17-607.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-624,
 abstract = {We consider goodness-of-fit tests with i.i.d. samples generated from a categorical distribution $(p_1,...,p_k)$. For a given $(q_1,...,q_k)$, we test the null hypothesis whether $p_j=q_{π(j)}$ for some label permutation $π$. The uncertainty of label permutation implies that the null hypothesis is composite instead of being singular. In this paper, we construct a testing procedure using statistics that are defined as indefinite integrals of some symmetric polynomials. This method is aimed directly at the invariance of the problem, and avoids the need of matching the unknown labels. The asymptotic distribution of the testing statistic is shown to be chi-squared, and its power is proved to be nearly optimal under a local alternative hypothesis. Various degenerate structures of the null hypothesis are carefully analyzed in the paper. A two-sample version of the test is also studied.},
 author = {Chao Gao},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W2754811153},
 pages = {1--50},
 title = {Goodness-of-Fit Tests for Random Partitions via Symmetric Polynomials},
 url = {http://jmlr.org/papers/v19/17-624.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-646,
 abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
 author = {Alessandro Achille and Stefano Soatto},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W2902227449},
 pages = {1--34},
 title = {Emergence of Invariance and Disentanglement in Deep Representations},
 url = {http://jmlr.org/papers/v19/17-646.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-650,
 abstract = {As datasets continue to increase in size and multi-core computer architectures are developed, asynchronous parallel optimization algorithms become more and more essential to the field of Machine Learning. Unfortunately, conducting the theoretical analysis asynchronous methods is difficult, notably due to the introduction of delay and inconsistency in inherently sequential algorithms. Handling these issues often requires resorting to simplifying but unrealistic assumptions. Through a novel perspective, we revisit and clarify a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms, and propose a simplification of the recently introduced "perturbed iterate" framework that resolves it. We demonstrate the usefulness of our new framework by analyzing three distinct asynchronous parallel incremental optimization algorithms: Hogwild (asynchronous SGD), KROMAGNON (asynchronous SVRG) and ASAGA, a novel asynchronous parallel version of the incremental gradient algorithm SAGA that enjoys fast linear convergence rates. We are able to both remove problematic assumptions and obtain better theoretical results. Notably, we prove that ASAGA and KROMAGNON can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions. We present results of an implementation on a 40-core architecture illustrating the practical speedups as well as the hardware overhead. Finally, we investigate the overlap constant, an ill-understood but central quantity for the theoretical analysis of asynchronous parallel algorithms. We find that it encompasses much more complexity than suggested in previous work, and often is order-of-magnitude bigger than traditionally thought.},
 author = {Remi Leblond and Fabian Pedregosa and Simon Lacoste-Julien},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W2783257164},
 pages = {1--68},
 title = {Improved asynchronous parallel optimization analysis for stochastic incremental methods},
 url = {http://jmlr.org/papers/v19/17-650.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-670,
 abstract = {Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior inference technique that is increasingly popular due to its fast runtimes on large-scale data sets. However, even when MFVB ...},
 author = {Ryan Giordano and Tamara Broderick and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W2963736577},
 pages = {1--49},
 title = {Covariances, Robustness, and Variational Bayes},
 url = {http://jmlr.org/papers/v19/17-670.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-684,
 abstract = {We develop an approximate formula for evaluating a cross-validation estimator of predictive likelihood for multinomial logistic regression regularized by an $\ell_1$-norm. This allows us to avoid repeated optimizations required for literally conducting cross-validation; hence, the computational time can be significantly reduced. The formula is derived through a perturbative approach employing the largeness of the data size and the model dimensionality. An extension to the elastic net regularization is also addressed. The usefulness of the approximate formula is demonstrated on simulated data and the ISOLET dataset from the UCI machine learning repository.},
 author = {Tomoyuki Obuchi and Yoshiyuki Kabashima},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2769350044},
 pages = {1--30},
 title = {Accelerating Cross-Validation in Multinomial Logistic Regression with $\ell_1$-Regularization},
 url = {http://jmlr.org/papers/v19/17-684.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-693,
 abstract = {Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such profiles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called \textit{SampLinUCB}, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings.},
 author = {Sylvain Lamprier and Thibault Gisselbrecht and Patrick Gallinari},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W2903099987},
 pages = {1--40},
 title = {Profile-Based Bandit with Unknown Profiles},
 url = {http://jmlr.org/papers/v19/17-693.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-701,
 abstract = {The difficulty of multi-class classification generally increases with the number of classes. Using data from a subset of the classes, can we predict how well a classifier will scale with an increased number of classes? Under the assumptions that the classes are sampled identically and independently from a population, and that the classifier is based on independently learned scoring functions, we show that the expected accuracy when the classifier is trained on k classes is the (k-1)st moment of a certain distribution that can be estimated from data. We present an unbiased estimation method based on the theory, and demonstrate its application on a facial recognition example.},
 author = {Charles Zheng and Rakesh Achanta and Yuval Benjamini},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W2963741170},
 pages = {1--30},
 title = {Extrapolating Expected Accuracies for Large Multi-Class Problems},
 url = {http://jmlr.org/papers/v19/17-701.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-704,
 author = {Bin Dai and Yu Wang and John Aston and Gang Hua and David Wipf},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W2899379230},
 pages = {1--42},
 title = {Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models},
 url = {http://jmlr.org/papers/v19/17-704.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-735,
 abstract = {This paper proposes a new approach to construct high quality space-filling sample designs. First, we propose a novel technique to quantify the space-filling property and optimally trade-off uniformity and randomness in sample designs in arbitrary dimensions. Second, we connect the proposed metric (defined in the spatial domain) to the objective measure of the design performance (defined in the spectral domain). This connection serves as an analytic framework for evaluating the qualitative properties of space-filling designs in general. Using the theoretical insights provided by this spatial-spectral analysis, we derive the notion of optimal space-filling designs, which we refer to as space-filling spectral designs. Third, we propose an efficient estimator to evaluate the space-filling properties of sample designs in arbitrary dimensions and use it to develop an optimization framework to generate high quality space-filling designs. Finally, we carry out a detailed performance comparison on two different applications in 2 to 6 dimensions: a) image reconstruction and b) surrogate modeling on several benchmark optimization functions and an inertial confinement fusion (ICF) simulation code. We demonstrate that the propose spectral designs significantly outperform existing approaches especially in high dimensions.},
 author = {Bhavya Kailkhura and Jayaraman J. Thiagarajan and Charvi Rastogi and Pramod K. Varshney and Peer-Timo Bremer},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W2964250531},
 pages = {1--46},
 title = {A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms},
 url = {http://jmlr.org/papers/v19/17-735.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-740,
 author = {Zeyi Wen and Jiashuai Shi and Qinbin Li and Bingsheng He and Jian Chen},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2782213427},
 pages = {1--5},
 title = {ThunderSVM: A Fast SVM Library on GPUs and CPUs},
 url = {http://jmlr.org/papers/v19/17-740.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-747,
 abstract = {We investigate the low-dimensional structure of deterministic transformations between random variables, i.e., transport maps between probability measures. In the context of statistics and machine learning, these transformations can be used to couple a tractable "reference" measure (e.g., a standard Gaussian) with a target measure of interest. Direct simulation from the desired measure can then be achieved by pushing forward reference samples through the map. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of low-dimensional couplings, induced by transport maps that are sparse and/or decomposable. Our analysis not only facilitates the construction of transformations in high-dimensional settings, but also suggests new inference methodologies for continuous non-Gaussian graphical models. For instance, in the context of nonlinear state-space models, we describe new variational algorithms for filtering, smoothing, and sequential parameter inference. These algorithms can be understood as the natural generalization---to the non-Gaussian case---of the square-root Rauch-Tung-Striebel Gaussian smoother.},
 author = {Alessio Spantini and Daniele Bigoni and Youssef Marzouk},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2963849628},
 pages = {1--71},
 title = {Inference via low-dimensional couplings},
 url = {http://jmlr.org/papers/v19/17-747.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-759,
 abstract = {DFG, 318763901, Approximative Bayes’sche Schatzung und Modellauswahl fur stochastische Differentialgleichungen (A06)},
 author = {Christian Donner and Manfred Opper},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W2887480292},
 pages = {1--34},
 title = {Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes},
 url = {http://jmlr.org/papers/v19/17-759.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-777,
 abstract = {We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. To begin, we establish the condition under which the fundamental assumption in synthetic control-like approaches holds, i.e. when the linear relationship between the treatment unit and the donor pool prevails in both the pre- and post-intervention periods. We provide the first finite sample analysis for a broader class of models, the Latent Variable Model, in contrast to Factor Models previously considered in the literature. Further, we show that our de-noising procedure accurately imputes missing entries, producing a consistent estimator of the underlying signal matrix provided $p = \Omega( T^{-1 + \zeta})$ for some $\zeta > 0$; here, $p$ is the fraction of observed data and $T$ is the time interval of interest. Under the same setting, we prove that the mean-squared-error (MSE) in our prediction estimation scales as $O(\sigma^2/p + 1/\sqrt{T})$, where $\sigma^2$ is the noise variance. Using a data aggregation method, we show that the MSE can be made as small as $O(T^{-1/2+\gamma})$ for any $\gamma \in (0, 1/2)$, leading to a consistent estimator. We also introduce a Bayesian framework to quantify the model uncertainty through posterior probabilities. Our experiments, using both real-world and synthetic datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method.},
 author = {Muhammad Amjad and Devavrat Shah and Dennis Shen},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W2964204340},
 pages = {1--51},
 title = {Robust Synthetic Control},
 url = {http://jmlr.org/papers/v19/17-777.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:17-781,
 abstract = {We study the following basic machine learning task: Given a fixed set of input points in ℝd for a linear regression problem, we wish to predict a hidden response value for each of the points. We can only afford to attain the responses for a small subset of the points that are then used to construct linear predictions for all points in the dataset. The performance of the predictions is evaluated by the total square loss on all responses (the attained as well as the remaining hidden ones). We show that a good approximate solution to this least squares problem can be obtained from just dimension d many responses by using a joint sampling technique called volume sampling. Moreover, the least squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal solution based on all n responses. This unbiasedness is a desirable property that is not shared by other common subset selection techniques.

Motivated by these basic properties, we develop a theoretical framework for studying volume sampling, resulting in a number of new matrix expectation equalities and statistical guarantees which are of importance not only to least squares regression but also to numerical linear algebra in general. Our methods also lead to a regularized variant of volume sampling, and we propose the first efficient algorithm for volume sampling which makes this technique a practical tool in the machine learning toolbox. Finally, we provide experimental evidence which confirms our theoretical findings.},
 author = {Micha{{\l}} Derezi{{\'n}}ski and Manfred K. Warmuth},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2963214110},
 pages = {1--39},
 title = {Reverse iterative volume sampling for linear regression},
 url = {http://jmlr.org/papers/v19/17-781.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-009,
 abstract = {This paper deals with inference and prediction for multiple correlated time series, where one has also the choice of using a candidate pool of contemporaneous predictors for each target series. Starting with a structural model for the time-series, Bayesian tools are used for model fitting, prediction, and feature selection, thus extending some recent work along these lines for the univariate case. The Bayesian paradigm in this multivariate setting helps the model avoid overfitting as well as capture correlations among the multiple time series with the various state components. The model provides needed flexibility to choose a different set of components and available predictors for each target series. The cyclical component in the model can handle large variations in the short term, which may be caused by external shocks. We run extensive simulations to investigate properties such as estimation accuracy and performance in forecasting. We then run an empirical study with one-step-ahead prediction on the max log return of a portfolio of stocks that involve four leading financial institutions. Both the simulation studies and the extensive empirical study confirm that this multivariate model outperforms three other benchmark models, viz. a model that treats each target series as independent, the autoregressive integrated moving average model with regression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model.},
 author = {Jinwen Qiu and S. Rao Jammalamadaka and Ning Ning},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2783124434},
 pages = {1--33},
 title = {Multivariate Bayesian Structural Time Series Model},
 url = {http://jmlr.org/papers/v19/18-009.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-015,
 abstract = {Recent research has shown the potential utility of deep Gaussian processes. These deep structures are probability distributions, designed through hierarchical construction, which are conditionally ...},
 author = {Matthew M. Dunlop and Mark A. Girolami and Andrew M. Stuart and Aretha L. Teckentrup},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W2963935178},
 pages = {1--46},
 title = {How Deep Are Deep Gaussian Processes},
 url = {http://jmlr.org/papers/v19/18-015.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-020,
 abstract = {A new class of non-homogeneous state-affine systems is introduced for use in reservoir computing. Sufficient conditions are identified that guarantee first, that the associated reservoir computers with linear readouts are causal, time-invariant, and satisfy the fading memory property and second, that a subset of this class is universal in the category of fading memory filters with stochastic almost surely uniformly bounded inputs. This means that any discrete-time filter that satisfies the fading memory property with random inputs of that type can be uniformly approximated by elements in the non-homogeneous state-affine family.},
 author = {Lyudmila Grigoryeva and Juan-Pablo Ortega},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W2963627589},
 pages = {1--40},
 title = {Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems},
 url = {http://jmlr.org/papers/v19/18-020.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-046,
 abstract = {A long-standing problem at the interface of artificial intelligence and applied mathematics is to devise an algorithm capable of achieving human level or even superhuman proficiency in transforming observed data into predictive mathematical models of the physical world. In the current era of abundance of data and advanced machine learning capabilities, the natural question arises: How can we automatically uncover the underlying laws of physics from high-dimensional data generated from experiments? In this work, we put forth a deep learning approach for discovering nonlinear partial differential equations from scattered and potentially noisy observations in space and time. Specifically, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the effectiveness of our approach for several benchmark problems spanning a number of scientific domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers', Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schrödinger, and Navier-Stokes equations.},
 author = {Maziar Raissi},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W2963112935},
 pages = {1--24},
 title = {Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations},
 url = {http://jmlr.org/papers/v19/18-046.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-088,
 abstract = {In solving hard computational problems, semidefinite program (SDP) relaxations often play an important role because they come with a guarantee of optimality. Here, we focus on a popular semidefinite relaxation of K-means clustering which yields the same solution as the non-convex original formulation for well segregated datasets. We report an unexpected finding: when data contains (greater than zero-dimensional) manifolds, the SDP solution captures such geometrical structures. Unlike traditional manifold embedding techniques, our approach does not rely on manually defining a kernel but rather enforces locality via a nonnegativity constraint. We thus call our approach NOnnegative MAnifold Disentangling, or NOMAD. To build an intuitive understanding of its manifold learning capabilities, we develop a theoretical analysis of NOMAD on idealized datasets. While NOMAD is convex and the globally optimal solution can be found by generic SDP solvers with polynomial time complexity, they are too slow for modern datasets. To address this problem, we analyze a non-convex heuristic and present a new, convex and yet efficient, algorithm, based on the conditional gradient method. Our results render NOMAD a versatile, understandable, and powerful tool for manifold learning.},
 author = {Mariano Tepper and Anirvan M. Sengupta and Dmitri Chklovskii},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W2964316122},
 pages = {1--30},
 title = {Clustering is semidefinitely not that hard: Nonnegative SDP for manifold disentangling},
 url = {http://jmlr.org/papers/v19/18-088.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-100,
 author = {Tom Ronan and Shawn Anastasio and Zhijie Qi and Pedro Henrique S. Vieira Tavares and Roman Sloutsky and Kristen M. Naegle},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2892560236},
 pages = {1--6},
 title = {OpenEnsembles: A Python Resource for Ensemble Clustering},
 url = {http://jmlr.org/papers/v19/18-100.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-113,
 abstract = {Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert's plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert.},
 author = {Adrian {\v{S}}o{\v{s}}i{{\'c}} and Elmar Rueckert and Jan Peters and Abdelhak M. Zoubir and Heinz Koeppl},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W2790798766},
 pages = {1--45},
 title = {Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling},
 url = {http://jmlr.org/papers/v19/18-113.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-117,
 abstract = {We introduce an agglomerative hierarchical clustering (AHC) framework which is generic, efficient and effective. Our approach embeds a sub-family of Lance-Williams (LW) clusterings and relies on inner-products instead of squared Euclidean distances. We carry out a constrained bottom-up merging procedure on a sparsified normalized inner-product matrix. Our method is named SNK-AHC for Sparsified Normalized Kernel matrix based AHC. SNK-AHC is more scalable than the classic dissimilarity matrix based AHC. It can also produce better results when clusters have arbitrary shapes. Artificial and real-world benchmarks are used to exemplify these points. From a theoretical standpoint, SNK-AHC provides another interpretation of the classic techniques which relies on the concept of weighted penalized similarities. The di_erences between group average, Mcquitty, centroid, median and Ward, can be explained by their distinct averaging strategies for aggregating clusters inter-similarities and intra-similarities. Other features of SNK-AHC are examined. We provide sufficient conditions in order to have monotonic dendrograms, we elaborate a stored data matrix approach for centroid and median, we underline the diagonal translation invariance property of group average, Mcquitty and Ward and we show to what extent SNK-AHC can determine the number of clusters.},
 author = {Julien Ah-Pine},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W2899429778},
 pages = {1--43},
 title = {An Efficient and Effective Generic Agglomerative Hierarchical Clustering Approach},
 url = {http://jmlr.org/papers/v19/18-117.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-158,
 abstract = {We propose and analyze two new MCMC sampling algorithms, the Vaidya walk and the John walk, for generating samples from the uniform distribution over a polytope. Both random walks are sampling algorithms derived from interior point methods. The former is based on volumetric-logarithmic barrier introduced by Vaidya whereas the latter uses John's ellipsoids. We show that the Vaidya walk mixes in significantly fewer steps than the logarithmic-barrier based Dikin walk studied in past work. For a polytope in $\mathbb{R}^d$ defined by $n &gt;d$ linear constraints, we show that the mixing time from a warm start is bounded as $\mathcal{O}(n^{0.5}d^{1.5})$, compared to the $\mathcal{O}(nd)$ mixing time bound for the Dikin walk. The cost of each step of the Vaidya walk is of the same order as the Dikin walk, and at most twice as large in terms of constant pre-factors. For the John walk, we prove an $\mathcal{O}(d^{2.5}\cdot\log^4(n/d))$ bound on its mixing time and conjecture that an improved variant of it could achieve a mixing time of $\mathcal{O}(d^2\cdot\text{polylog}(n/d))$. Additionally, we propose variants of the Vaidya and John walks that mix in polynomial time from a deterministic starting point. The speed-up of the Vaidya walk over the Dikin walk are illustrated in numerical examples.},
 author = {Yuansi Chen and Raaz Dwivedi and Martin J. Wainwright and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W2765848362},
 pages = {1--86},
 title = {Fast MCMC sampling algorithms on polytopes},
 url = {http://jmlr.org/papers/v19/18-158.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-160,
 abstract = {Seglearn is an open-source python package for machine learning time series or sequences using a sliding window segmentation approach. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. This package is compatible with scikit-learn and is listed under scikit-learn Related Projects. The package depends on numpy, scipy, and scikit-learn. Seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage.},
 author = {David M. Burns and Cari M. Whyne},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W2962970382},
 pages = {1--7},
 title = {Seglearn: A Python Package for Learning Sequences and Time Series},
 url = {http://jmlr.org/papers/v19/18-160.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-188,
 abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.},
 author = {Daniel Soudry and Elad Hoffer and Mor Shpigel Nacson and Suriya Gunasekar and Nathan Srebro},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W2911742574},
 pages = {1--57},
 title = {The implicit bias of gradient descent on separable data},
 url = {http://jmlr.org/papers/v19/18-188.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-195,
 abstract = {In learning theory, the VC dimension of a concept class C is the most common way to measure its “richness.” A fundamental result says that the number of examples needed to learn an unknown target concept c∈C under an unknown distribution D, is tightly determined by the VC dimension d of the concept class C. Specifically, in the PAC model
Θ(dϵ+log(1/δ)ϵ)
examples are necessary and sufficient for a learner to output, with probability 1−δ, a hypothesis h that is ϵ-close to the target concept c (measured under D). In the related agnostic model, where the samples need not come from a c∈C, we know that
Θ(dϵ2+log(1/δ)ϵ2)
examples are necessary and sufficient to output an hypothesis h∈C whose error is at most ϵ worse than the error of the best concept in C. Here we analyze quantum sample complexity, where each example is a coherent quantum state. This model was introduced by Bshouty and Jackson (1999), who showed that quantum examples are more powerful than classical examples in some fixed-distribution settings. However, Atici and Servedio (2005), improved by Zhang (2010), showed that in the PAC setting (where the learner has to succeed for every distribution), quantum examples cannot be much more powerful: the required number of quantum examples is
Ω(d1−ηϵ+d+log(1/δ)ϵ) for arbitrarily small constant η>0.
Our main result is that quantum and classical sample complexity are in fact equal up to constant factors in both the PAC and agnostic models. We give two proof approaches. The first is a fairly simple information-theoretic argument that yields the above two classical bounds and yields the same bounds for quantum sample complexity up to a log(d/ϵ) factor. We then give a second approach that avoids the log-factor loss, based on analyzing the behavior of the “Pretty Good Measurement” on the quantum state-identification problems that correspond to learning. This shows classical and quantum sample complexity are equal up to constant factors for every concept class C.},
 author = {Srinivasan Arunachalam and Ronald de Wolf},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2964328760},
 pages = {1--36},
 title = {Optimal quantum sample complexity of learning algorithms},
 url = {http://jmlr.org/papers/v19/18-195.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-251,
 abstract = {Scikit-multiflow is a multi-output/multi-label and stream data mining framework for the Python programming language. Conceived to serve as a platform to encourage democratization of stream learning research, it provides multiple state of the art methods for stream learning, stream generators and evaluators. scikit-multiflow builds upon popular open source frameworks including scikit-learn, MOA and MEKA. Development follows the FOSS principles and quality is enforced by complying with PEP8 guidelines and using continuous integration and automatic testing. The source code is publicly available at https://github.com/scikit-multiflow/scikit-multiflow.},
 author = {Jacob Montiel and Jesse Read and Albert Bifet and Talel Abdessalem},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W2847284300},
 pages = {1--5},
 title = {Scikit-Multiflow: A Multi-output Streaming Framework},
 url = {http://jmlr.org/papers/v19/18-251.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-264,
 abstract = {In 1984, Johnson and Lindenstrauss proved that any finite set of data in a high-dimensional space can be projected to a lower-dimensional space while preserving the pairwise Euclidean distance between points up to a bounded relative error. If the desired dimension of the image is too small, however, Kane, Meka, and Nelson (2011) and Jayram and Woodruff (2013) independently proved that such a projection does not exist. In this paper, we provide a precise asymptotic threshold for the dimension of the image, above which, there exists a projection preserving the Euclidean distance, but, below which, there does not exist such a projection.},
 author = {Michael Burr and Shuhong Gao and Fiona Knoll},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2964031261},
 pages = {1--22},
 title = {Optimal Bounds for Johnson-Lindenstrauss Transformations},
 url = {http://jmlr.org/papers/v19/18-264.html},
 volume = {19},
 year = {2018}
}

@article{JMLR:v19:18-416,
 abstract = {Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). Such methods are usually described by a large number of parameters or hyper parameters - a price that one needs to pay for elasticity. The very number of parameters makes models hard to understand.

This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes. Each explainer is a technique for exploration of a black box model. Presented approaches are model-agnostic, what means that they extract useful information from any predictive method irrespective of its internal structure. Each explainer is linked with a specific aspect of a model. Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable.

Every explainer presented here works for a single model or for a collection of models. In the latter case, models can be compared against each other. Such comparison helps to find strengths and weaknesses of different models and gives additional tools for model validation. Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended.},
 author = {Przemyslaw Biecek},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W2963852406},
 pages = {1--5},
 title = {DALEX: explainers for complex predictive models in R},
 url = {http://jmlr.org/papers/v19/18-416.html},
 volume = {19},
 year = {2018}
}
