@article{JMLR:v15:agarwal14a,
 abstract = {We present a system and a set of techniques for learning linear predictors with convex losses on terascale datasets, with trillions of features, {The number of features here refers to the number of non-zero entries in the data matrix.} billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature (as of 2011 when our experiments were conducted). We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.},
 author = {Alekh Agarwal and Oliveier Chapelle and Miroslav Dud\'{i}k and John Langford},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W2157462866},
 pages = {1111--1133},
 title = {A Reliable Effective Terascale Linear Learning System},
 url = {http://jmlr.org/papers/v15/agarwal14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:agarwal14b,
 abstract = {The problem of bipartite ranking, where instances are labeled positive or negative and the goal is to learn a scoring function that minimizes the probability of mis-ranking a pair of positive and negative instances (or equivalently, that maximizes the area under the ROC curve), has been widely studied in recent years. A dominant theoretical and algorithmic framework for the problem has been to reduce bipartite ranking to pairwise classification; in particular, it is well known that the bipartite ranking regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for classification problems. Recently, Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of the regret associated with balanced versions of the standard (non-pairwise) logistic and exponential losses. In this paper, we show that such (nonpairwise) surrogate regret bounds for bipartite ranking can be obtained in terms of a broad class of proper (composite) losses that we term as strongly proper. Our proof technique is much simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses as special cases. An important consequence is that standard algorithms minimizing a (non-pairwise) strongly proper loss, such as logistic regression and boosting algorithms (assuming a universal function class and appropriate regularization), are in fact consistent for bipartite ranking; moreover, our results allow us to quantify the bipartite ranking regret in terms of the corresponding surrogate regret. We also obtain tighter surrogate bounds under certain low-noise conditions via a recent result of Clemencon and Robbiano (2011).},
 author = {Shivani Agarwal},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W2141789531},
 pages = {1653--1674},
 title = {Surrogate regret bounds for bipartite ranking via strongly proper losses},
 url = {http://jmlr.org/papers/v15/agarwal14b.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:ailon14a,
 abstract = {The disagreement coeffcient of Hanneke has become a central data independent invariant in proving active learning rates. It has been shown in various ways that a concept class with low complexity together with a bound on the disagreement coeffcient at an optimal solution allows active learning rates that are superior to passive learning ones.

We present a different tool for pool based active learning which follows from the existence of a certain uniform version of low disagreement coeffcient, but is not equivalent to it. In fact, we present two fundamental active learning problems of significant interest for which our approach allows nontrivial active learning bounds. However, any general purpose method relying on the disagreement coeffcient bounds only, fails to guarantee any useful bounds for these problems. The applications of interest are: Learning to rank from pairwise preferences, and clustering with side information (a.k.a. semi-supervised clustering).

The tool we use is based on the learner's ability to compute an estimator of the difference between the loss of any hypothesis and some fixed hypothesis to within an absolute error of at most e times the disagreement measure (l1 distance) between the two hypotheses. We prove that such an estimator implies the existence of a learning algorithm which, at each iteration, reduces its in-class excess risk to within a constant factor. Each iteration replaces the current pivotal hypothesis with the minimizer of the estimated loss difference function with respect to the previous pivotal hypothesis. The label complexity essentially becomes that of computing this estimator.},
 author = {Nir Ailon and Ron Begleiter and Esther Ezra},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W2399210510},
 pages = {885--920},
 title = {Active Learning Using Smooth Relative Regret Approximations with Applications},
 url = {http://jmlr.org/papers/v15/ailon14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:alain14a,
 abstract = {What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data-generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.},
 author = {Guillaume Alain and Yoshua Bengio},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W2614634292},
 pages = {3743--3773},
 title = {What regularized auto-encoders learn from the data-generating distribution},
 url = {http://jmlr.org/papers/v15/alain14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:anandkumar14a,
 abstract = {Community detection is the task of detecting hidden communities from observed interactions. Guaranteed community detection has so far been mostly limited to models with non-overlapping communities such as the stochastic block model. In this paper, we remove this restriction, and provide guaranteed community detection for a family of probabilistic network models with overlapping communities, termed as the mixed membership Dirichlet model, first introduced by Airoldi et al. (2008). This model allows for nodes to have fractional memberships in multiple communities and assumes that the community memberships are drawn from a Dirichlet distribution. Moreover, it contains the stochastic block model as a special case. We propose a unified approach to learning these models via a tensor spectral decomposition method. Our estimator is based on low-order moment tensor of the observed network, consisting of 3-star counts. Our learning method is fast and is based on simple linear algebraic operations, e.g., singular value decomposition and tensor power iterations. We provide guaranteed recovery of community memberships and model parameters and present a careful finite sample analysis of our learning method. As an important special case, our results match the best known scaling requirements for the (homogeneous) stochastic block model.},
 author = {Animashree  An and kumar and Rong Ge and Daniel Hsu and Sham M. Kakade},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2963300816},
 pages = {2239--2312},
 title = {A tensor approach to learning mixed membership community models},
 url = {http://jmlr.org/papers/v15/anandkumar14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:anandkumar14b,
 abstract = {This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models--including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation--which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.},
 author = {Animashree Anandkumar and Rong Ge and Daniel Hsu and Sham M. Kakade and Matus Telgarsky},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W2105724942},
 pages = {2773--2832},
 title = {Tensor decompositions for learning latent variable models},
 url = {http://jmlr.org/papers/v15/anandkumar14b.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:aravkin14a,
 abstract = {We study a simple linear regression problem for grouped variables; we are interested in methods which jointly perform estimation and variable selection, that is, that automatically set to zero groups of variables in the regression vector. The Group Lasso (GLasso), a well known approach used to tackle this problem which is also a special case of Multiple Kernel Learning (MKL), boils down to solving convex optimization problems. On the other hand, a Bayesian approach commonly known as Sparse Bayesian Learning (SBL), a version of which is the well known Automatic Relevance Determination (ARD), lead to nonconvex problems. In this paper we discuss the relation between ARD (and a penalized version which we call PARD) and Glasso, and study their asymptotic properties in terms of the Mean Squared Error in estimating the unknown parameter. The theoretical arguments developed here are independent of the correctness of the prior models and clarify the advantages of PARD over GLasso.},
 author = {Aleksandr Aravkin and James V. Burke and Alessandro Chiuso and Gianluigi Pillonetto},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W2098729277},
 pages = {217--252},
 title = {Convex vs non-convex estimators for regression and sparse estimation: the mean squared error properties of ARD and GLasso},
 url = {http://jmlr.org/papers/v15/aravkin14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:archer14a,
 abstract = {We consider the problem of estimating Shannon's entropy $H$ from discrete data, in cases where the number of possible symbols is unknown or even countably infinite. The Pitman-Yor process, a generalization of Dirichlet process, provides a tractable prior distribution over the space of countably infinite discrete distributions, and has found major applications in Bayesian non-parametric statistics and machine learning. Here we show that it also provides a natural family of priors for Bayesian entropy estimation, due to the fact that moments of the induced posterior distribution over $H$ can be computed analytically. We derive formulas for the posterior mean (Bayes' least squares estimate) and variance under Dirichlet and Pitman-Yor process priors. Moreover, we show that a fixed Dirichlet or Pitman-Yor process prior implies a narrow prior distribution over $H$, meaning the prior strongly determines the entropy estimate in the under-sampled regime. We derive a family of continuous mixing measures such that the resulting mixture of Pitman-Yor processes produces an approximately flat prior over $H$. We show that the resulting Pitman-Yor Mixture (PYM) entropy estimator is consistent for a large class of distributions. We explore the theoretical properties of the resulting estimator, and show that it performs well both in simulation and in application to real data.},
 author = {Evan Archer and Il Memming Park and Jonathan W. Pillow},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W2951863790},
 pages = {2833--2868},
 title = {Bayesian Entropy Estimation for Countable Discrete Distributions},
 url = {http://jmlr.org/papers/v15/archer14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:bach14a,
 abstract = {In this paper, we consider supervised learning problems such as logistic regression and study the stochastic gradient method with averaging, in the usual stochastic approximation setting where observations are used only once. We show that after $N$ iterations, with a constant step-size proportional to $1/R^2 \sqrt{N}$ where $N$ is the number of observations and $R$ is the maximum norm of the observations, the convergence rate is always of order $O(1/\sqrt{N})$, and improves to $O(R^2 / \mu N)$ where $\mu$ is the lowest eigenvalue of the Hessian at the global optimum (when this eigenvalue is greater than $R^2/\sqrt{N}$). Since $\mu$ does not need to be known in advance, this shows that averaged stochastic gradient is adaptive to \emph{unknown local} strong convexity of the objective function. Our proof relies on the generalized self-concordance properties of the logistic loss and thus extends to all generalized linear models with uniformly bounded features.},
 author = {Francis Bach},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W4299569908},
 pages = {595--627},
 title = {Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression},
 url = {http://jmlr.org/papers/v15/bach14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:balcan14a,
 abstract = {One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm achieves better performance than other hierarchical algorithms in the presence of noise.},
 author = {Maria-Florina Balcan and Yingyu Liang and Pramod Gupta},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W2962962407},
 pages = {4011--4051},
 title = {Robust Hierarchical Clustering},
 url = {http://jmlr.org/papers/v15/balcan14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:boumal14a,
 abstract = {Optimization on manifolds is a rapidly developing branch of nonlinear optimization. Its focus is on problems where the smooth geometry of the search space can be leveraged to design efficient numerical algorithms. In particular, optimization on manifolds is well-suited to deal with rank and orthogonality constraints. Such structured constraints appear pervasively in machine learning applications, including low-rank matrix completion, sensor network localization, camera network registration, independent component analysis, metric learning, dimensionality reduction and so on. The Manopt toolbox, available at www.manopt.org, is a user-friendly, documented piece of software dedicated to simplify experimenting with state of the art Riemannian optimization algorithms. We aim particularly at reaching practitioners outside our field.},
 author = {Nicolas Boumal and Bamdev Mishra and P.-A. Absil and Rodolphe Sepulchre},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W2167623372},
 pages = {1455--1459},
 title = {Manopt, a Matlab toolbox for optimization on manifolds},
 url = {http://jmlr.org/papers/v15/boumal14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:chen14a,
 abstract = {This paper considers the problem of clustering a partially observed unweighted graph--i.e., one where for some node pairs we know there is an edge between them, for some others we know there is no edge, and for the remaining we do not know whether or not there is an edge. We want to organize the nodes into disjoint clusters so that there is relatively dense (observed) connectivity within clusters, and sparse across clusters.

We take a novel yet natural approach to this problem, by focusing on finding the clustering that minimizes the number of disagreements--i.e., the sum of the number of (observed) missing edges within clusters, and (observed) present edges across clusters. Our algorithm uses convex optimization; its basis is a reduction of disagreement minimization to the problem of recovering an (unknown) low-rank matrix and an (unknown) sparse matrix from their partially observed sum. We evaluate the performance of our algorithm on the classical Planted Partition/Stochastic Block Model. Our main theorem provides sufficient conditions for the success of our algorithm as a function of the minimum cluster size, edge density and observation probability; in particular, the results characterize the tradeoff between the observation probability and the edge density gap. When there are a constant number of clusters of equal size, our results are optimal up to logarithmic factors.},
 author = {Yudong Chen and Ali Jalali and Sujay Sanghavi and Huan Xu},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W2614891641},
 pages = {2213--2238},
 title = {Clustering partially observed graphs via convex optimization},
 url = {http://jmlr.org/papers/v15/chen14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:chiang14a,
 abstract = {The study of social networks is a burgeoning research area. However, most existing work deals with networks that simply encode whether relationships exist or not. In contrast, relationships in signed networks can be positive ("like", "trust") or negative ("dislike", "distrust"). The theory of social balance shows that signed networks tend to conform to some local patterns that, in turn, induce certain global characteristics. In this paper, we exploit both local as well as global aspects of social balance theory for two fundamental problems in the analysis of signed networks: sign prediction and clustering. Motivated by local patterns of social balance, we first propose two families of sign prediction methods: measures of social imbalance (MOIs), and supervised learning using high order cycles (HOCs). These methods predict signs of edges based on triangles and \ell-cycles for relatively small values of \ell. Interestingly, by examining measures of social imbalance, we show that the classic Katz measure, which is used widely in unsigned link prediction, actually has a balance theoretic interpretation when applied to signed networks. Furthermore, motivated by the global structure of balanced networks, we propose an effective low rank modeling approach for both sign prediction and clustering. For the low rank modeling approach, we provide theoretical performance guarantees via convex relaxations, scale it up to large problem sizes using a matrix factorization based algorithm, and provide extensive experimental validation including comparisons with local approaches. Our experimental results indicate that, by adopting a more global viewpoint of balance structure, we get significant performance and computational gains in prediction and clustering tasks on signed networks. Our work therefore highlights the usefulness of the global aspect of balance theory for the analysis of signed networks.},
 author = {Kai-Yang Chiang and Cho-Jui Hsieh and Nagarajan Natarajan and Inderjit S. Dhillon and Ambuj Tewari},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W2950615402},
 pages = {1177--1213},
 title = {Prediction and Clustering in Signed Networks: A Local to Global Perspective},
 url = {http://jmlr.org/papers/v15/chiang14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:chin14a,
 abstract = {Track 2 in KDD Cup 2013 aims at determining duplicated authors in a data set from Microsoft Academic Search. This type of problems appears in many large-scale applications that compile information from different sources. This paper describes our solution developed at National Taiwan University to win the first prize of the competition. We propose an effective name matching framework and realize two implementations. An important strategy in our approach is to consider Chinese and non-Chinese names separately because of their different naming conventions. Post-processing including merging results of two predictions further boosts the performance. Our approach achieves F1-score 0.99202 on the private leader board, while 0.99195 on the public leader board.},
 author = {Wei-Sheng Chin and Yong Zhuang and Yu-Chin Juan and Felix Wu and Hsiao-Yu Tung and Tong Yu and Jui-Pin Wang and Cheng-Xia Chang and Chun-Pai Yang and Wei-Cheng Chang and Kuan-Hao Huang and Tzu-Ming Kuo and Shan-Wei Lin and Young-San Lin and Yu-Chen Lu and Yu-Chuan Su and Cheng-Kuang Wei and Tu-Chun Yin and Chun-Liang Li and Ting-Wei Lin and Cheng-Hao Tsai and Shou-De Lin and Hsuan-Tien Lin and Chih-Jen Lin},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W2112564748},
 pages = {3037--3064},
 title = {Effective string processing and matching for author disambiguation},
 url = {http://jmlr.org/papers/v15/chin14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:claesen14a,
 abstract = {EnsembleSVM is a free software package containing efficient routines to perform ensemble learning with support vector machine (SVM) base models. It currently offers ensemble methods based on binary SVM models. Our implementation avoids duplicate storage and evaluation of support vectors which are shared between constituent models. Experimental results show that using ensemble approaches can drastically reduce training complexity while maintaining high predictive accuracy. The EnsembleSVM software package is freely available online at http://esat.kuleuven.be/stadius/ensemblesvm.},
 author = {Marc Claesen and Frank De Smet and Johan A.K. Suykens and Bart De Moor},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2166619467},
 pages = {141--145},
 title = {EnsembleSVM: A Library for Ensemble Learning Using Support Vector Machines},
 url = {http://jmlr.org/papers/v15/claesen14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:cohen14a,
 abstract = {We introduce a spectral learning algorithm for latent-variable PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides statistically consistent parameter estimates. Our result rests on three theorems: the first gives a tensor form of the inside-outside algorithm for PCFGs; the second shows that the required tensors can be estimated directly from training examples where hidden-variable values are missing; the third gives a PAC-style convergence bound for the estimation method.},
 author = {Shay B. Cohen and Karl Stratos and Michael Collins and Dean P. Foster and Lyle Ungar},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W2158896774},
 pages = {2399--2449},
 title = {Spectral learning of latent-variable PCFGs: algorithms and sample complexity},
 url = {http://jmlr.org/papers/v15/cohen14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:colombo14a,
 abstract = {We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The first step of all these algorithms consists of the adjacency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modifications of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modifications in simulation studies and on a yeast gene expression data set. We show that our modifications yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.},
 author = {Diego Colombo and Marloes H. Maathuis},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W2134240743},
 pages = {3921--3962},
 title = {Order-independent constraint-based causal structure learning},
 url = {http://jmlr.org/papers/v15/colombo14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:couckuyt14a,
 abstract = {When analyzing data from computationally expensive simulation codes, surrogate modeling methods are firmly established as facilitators for design space exploration, sensitivity analysis, visualization and optimization. Kriging is a popular surrogate modeling technique used for the Design and Analysis of Computer Experiments (DACE). Hence, the past decade Kriging has been the subject of extensive research and many extensions have been proposed, e.g., co-Kriging, stochastic Kriging, blind Kriging, etc. However, few Kriging implementations are publicly available and tailored towards scientists and engineers. Furthermore, no Kriging toolbox exists that unifies several Kriging flavors. This paper addresses this need by presenting an efficient object-oriented Kriging implementation and several Kriging extensions, providing a flexible and easily extendable framework to test and implement new Kriging flavors while reusing as much code as possible.},
 author = {Ivo Couckuyt and Tom Dhaene and Piet Demeester},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2151794096},
 pages = {3183--3186},
 title = {ooDACE toolbox: a flexible object-oriented Kriging implementation},
 url = {http://jmlr.org/papers/v15/couckuyt14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:couprie14a,
 abstract = {This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on handcrafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. Using a frame by frame labeling, we obtain nearly state-of-the-art performance on the NYU-v2 depth data set with an accuracy of 64.5%. We then show that the labeling can be further improved by exploiting the temporal consistency in the video sequence of the scene. To that goal, we present a method producing temporally consistent superpixels from a streaming video. Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real-time applications. We illustrate the labeling of indoor scenes in video sequences that could be processed in real-time using appropriate hardware such as an FPGA.},
 author = {Camille Couprie and Cl{{\'e}}ment Farabet and Laurent Najman and Yann LeCun},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W126953435},
 pages = {3489--3511},
 title = {Convolutional Nets and Watershed Cuts for Real-Time Semantic Labeling of RGBD Videos},
 url = {http://jmlr.org/papers/v15/couprie14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:coviello14a,
 abstract = {The hidden Markov model (HMM) is a widely-used generative model that copes with sequential data, assuming that each observation is conditioned on the state of a hidden Markov chain. In this paper, we derive a novel algorithm to HMMs based on the hierarchical EM (HEM) algorithm. The proposed algorithm i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a cluster center, that is, a novel HMM that is representative for the group, in a manner that is consistent with the underlying generative model of the HMM. To cope with intractable inference in the E-step, the HEM algorithm is formulated as a variational optimization problem, and efficiently solved for the HMM case by leveraging an appropriate variational approximation. The benefits of the proposed algorithm, which we call variational HEM (VHEM), are demonstrated on several tasks involving time-series data, such as hierarchical clustering of motion capture sequences, and automatic annotation and retrieval of music and of online hand-writing data, showing improvements over current methods. In particular, our variational HEM algorithm effectively leverages large amounts of data when learning annotation models by using an efficient hierarchical estimation procedure, which reduces learning times and memory requirements, while improving model robustness through better regularization.},
 author = {Emanuele Coviello and Antoni B. Chan and Gert R.G. Lanckriet},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W2963805875},
 pages = {697--747},
 title = {Clustering hidden Markov models with variational HEM},
 url = {http://jmlr.org/papers/v15/coviello14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:cuong14a,
 abstract = {Dependencies among neighboring labels in a sequence are important sources of information for sequence labeling and segmentation. However, only first-order dependencies, which are dependencies between adjacent labels or segments, are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we give effcient inference algorithms to handle high-order dependencies between labels or segments in conditional random fields, under the assumption that the number of distinct label patterns used in the features is small. This leads to effcient learning algorithms for these conditional random fields. We show experimentally that exploiting high-order dependencies can lead to substantial performance improvements for some problems, and we discuss conditions under which high-order features can be effective.},
 author = {Nguyen Viet Cuong and Nan Ye and Wee Sun Lee and Hai Leong Chieu},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2165073960},
 pages = {981--1009},
 title = {Conditional random field with high-order dependencies for sequence labeling and segmentation},
 url = {http://jmlr.org/papers/v15/cuong14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:cuturi14a,
 abstract = {Optimal transport distances have been used for more than a decade in machine learning to compare histograms of features. They have one parameter: the ground metric, which can be any metric between the features themselves. As is the case for all parameterized distances, optimal transport distances can only prove useful in practice when this parameter is carefully chosen. To date, the only option available to practitioners to set the ground metric parameter was to rely on a priori knowledge of the features, which limited considerably the scope of application of optimal transport distances. We propose to lift this limitation and consider instead algorithms that can learn the ground metric using only a training set of labeled histograms. We call this approach ground metric learning. We formulate the problem of learning the ground metric as the minimization of the difference of two convex polyhedral functions over a convex set of metric matrices. We follow the presentation of our algorithms with promising experimental results which show that this approach is useful both for retrieval and binary/multiclass classification tasks.},
 author = {Marco Cuturi and David Avis},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W2125483582},
 pages = {533--564},
 title = {Ground metric learning},
 url = {http://jmlr.org/papers/v15/cuturi14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:dann14a,
 abstract = {Policy evaluation is an essential step in most reinforcement learning approaches. It yields a value function, the quality assessment of states for a given policy, which can be used in a policy improvement step. Since the late 1980s, this research area has been dominated by temporal-difference (TD) methods due to their data-efficiency. However, core issues such as stability guarantees in the off-policy scenario, improved sample efficiency and probabilistic treatment of the uncertainty in the estimates have only been tackled recently, which has led to a large number of new approaches.

This paper aims at making these new developments accessible in a concise overview, with foci on underlying cost functions, the off-policy scenario as well as on regularization in high dimensional feature spaces. By presenting the first extensive, systematic comparative evaluations comparing TD, LSTD, LSPE, FPKF, the residual-gradient algorithm, Bellman residual minimization, GTD, GTD2 and TDC, we shed light on the strengths and weaknesses of the methods. Moreover, we present alternative versions of LSTD and LSPE with drastically improved off-policy performance.},
 author = {Christoph Dann and Gerhard Neumann and Jan Peters},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W2121703796},
 pages = {809--883},
 title = {Policy evaluation with temporal differences: a survey and comparison},
 url = {http://jmlr.org/papers/v15/dann14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:delgado14a,
 abstract = {We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking,...},
 author = {Manuel Fern{{\'a}}ndez-Delgado and Eva Cernadas and Sen{{\'e}}n Barro and Dinani Amorim},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W2998768810},
 pages = {3133--3181},
 title = {Do we need hundreds of classifiers to solve real world classification problems},
 url = {http://jmlr.org/papers/v15/delgado14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:desautels14a,
 abstract = {How can we take advantage of opportunities for experimental parallelization in exploration-exploitation tradeoffs? In many experimental scenarios, it is often desirable to execute experiments simultaneously or in batches, rather than only performing one at a time. Additionally, observations may be both noisy and expensive. We introduce Gaussian Process Batch Upper Confidence Bound (GP-BUCB), an upper confidence bound-based algorithm, which models the reward function as a sample from a Gaussian process and which can select batches of experiments to run in parallel. We prove a general regret bound for GP-BUCB, as well as the surprising result that for some common kernels, the asymptotic average regret can be made independent of the batch size. The GP-BUCB algorithm is also applicable in the related case of a delay between initiation of an experiment and observation of its results, for which the same regret bounds hold. We also introduce Gaussian Process Adaptive Upper Confidence Bound (GP-AUCB), a variant of GP-BUCB which can exploit parallelism in an adaptive manner. We evaluate GP-BUCB and GP-AUCB on several simulated and real data sets. These experiments show that GP-BUCB and GP-AUCB are competitive with state-of-the-art heuristics.},
 author = {Thomas Desautels and Andreas Krause and Joel W. Burdick},
 journal = {Journal of Machine Learning Research},
 number = {119},
 openalex = {W2616619952},
 pages = {4053--4103},
 title = {Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization},
 url = {http://jmlr.org/papers/v15/desautels14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:dhurandhar14a,
 abstract = {In this paper, we propose an approach for learning regression models efficiently in an environment where multiple features and data-points are added incrementally in a multistep process. At each step, any finite number of features maybe added and hence, the setting is not amenable to low rank updates. We show that our approach is not only efficient and optimal for ordinary least squares, weighted least squares, generalized least squares and ridge regression, but also more generally for generalized linear models and lasso regression that use iterated re-weighted least squares for maximum likelihood estimation. Our approach instantiated to linear settings has close relations to the partitioned matrix inversion mechanism based on Schur's complement. For arbitrary regression methods, even a relaxation of the approach is no worse than using the model from the previous step or using a model that learns on the additional features and optimizes the residual of the model at the previous step. Such problems are commonplace in complex manufacturing operations consisting of hundreds of steps, where multiple measurements are taken at each step to monitor the quality of the final product. Accurately predicting if the finished product will meet specifications at each or, at least, important intermediate steps can be extremely useful in enhancing productivity. We further validate our claims through experiments on synthetic and real industrial data sets.},
 author = {Amit Dhur and har and Marek Petrik},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W2117819897},
 pages = {2607--2627},
 title = {Efficient and accurate methods for updating generalized linear models with multiple feature additions},
 url = {http://jmlr.org/papers/v15/dhurandhar14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:doliwa14a,
 abstract = {This paper is concerned with various combinatorial parameters of classes that can be learned from a small set of examples. We show that the recursive teaching dimension, recently introduced by Zilles et al. (2008), is strongly connected to known complexity notions in machine learning, e.g., the self-directed learning complexity and the VC-dimension. To the best of our knowledge these are the first results unveiling such relations between teaching and query learning as well as between teaching and the VC-dimension. It will turn out that for many natural classes the RTD is upper-bounded by the VCD, e.g., classes of VC-dimension 1, intersection-closed classes and finite maximum classes. However, we will also show that there are certain (but rare) classes for which the recursive teaching dimension exceeds the VC-dimension. Moreover, for maximum classes, the combinatorial structure induced by the RTD, called teaching plan, is highly similar to the structure of sample compression schemes. Indeed one can transform any repetition-free teaching plan for a maximum class C into an unlabeled sample compression scheme for C and vice versa, while the latter is produced by (i) the corner-peeling algorithm of Rubinstein and Rubinstein (2012) and (ii) the tail matching algorithm of Kuzmin and Warmuth (2007).},
 author = {Thorsten Doliwa and Gaojian Fan and Hans Ulrich Simon and Sandra Zilles},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W2146250995},
 pages = {3107--3131},
 title = {Recursive teaching dimension, VC-dimension and sample compression},
 url = {http://jmlr.org/papers/v15/doliwa14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:doppa14a,
 abstract = {We consider a framework for structured prediction based on search in the space of complete structured outputs. Given a structured input, an output is produced by running a time-bounded search procedure guided by a learned cost function, and then returning the least cost output uncovered during the search. This framework can be instantiated for a wide range of search spaces and search procedures, and easily incorporates arbitrary structured-prediction loss functions. In this paper, we make two main technical contributions. First, we describe a novel approach to automatically defining an effective search space over structured outputs, which is able to leverage the availability of powerful classification learning algorithms. In particular, we define the limited-discrepancy search space and relate the quality of that space to the quality of learned classifiers. We also define a sparse version of the search space to improve the effciency of our overall approach. Second, we give a generic cost function learning approach that is applicable to a wide range of search procedures. The key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function. Our experiments on six benchmark domains show that a small amount of search in limited discrepancy search space is often sufficient for significantly improving on state-of-the-art structured-prediction performance. We also demonstrate significant speed improvements for our approach using sparse search spaces with little or no loss in accuracy.},
 author = {Janardhan Rao Doppa and Alan Fern and Prasad Tadepalli},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W2096006868},
 pages = {1317--1350},
 title = {Structured prediction via output space search},
 url = {http://jmlr.org/papers/v15/doppa14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:dubout14a,
 abstract = {Classical boosting algorithms, such as AdaBoost, build a strong classifier without concern for the computational cost. Some applications, in particular in computer vision, may involve millions of training examples and very large feature spaces. In such contexts, the training time of off-the-shelf boosting algorithms may become prohibitive. Several methods exist to accelerate training, typically either by sampling the features or the examples used to train the weak learners. Even if some of these methods provide a guaranteed speed improvement, they offer no insurance of being more efficient than any other, given the same amount of time.

The contributions of this paper are twofold: (1) a strategy to better deal with the increasingly common case where features come from multiple sources (for example, color, shape, texture, etc., in the case of images) and therefore can be partitioned into meaningful subsets; (2) new algorithms which balance at every boosting iteration the number of weak learners and the number of training examples to look at in order to maximize the expected loss reduction. Experiments in image classification and object recognition on four standard computer vision data sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.},
 author = {Charles Dubout and Francois Fleuret},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W2171380303},
 pages = {1431--1453},
 title = {Adaptive sampling for large scale boosting},
 url = {http://jmlr.org/papers/v15/dubout14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:durante14a,
 abstract = {In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such time-varying smoothness is not accounted for, one can obtain misleading inferences and predictions, with oversmoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to mis-calibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time, allowing locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions evolving in time through nested Gaussian processes and linearly related to the observed data with a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application.},
 author = {Daniele Durante and Bruno Scarpa and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W2120362738},
 pages = {1493--1522},
 title = {Locally adaptive factor processes for multivariate time series},
 url = {http://jmlr.org/papers/v15/durante14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:fabisch14a,
 abstract = {We consider the problem of learning skills that are versatilely applicable. One popular approach for learning such skills is contextual policy search in which the individual tasks are represented a...},
 author = {Alexander Fabisch and Jan Hendrik Metzen},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W3005112600},
 pages = {3371--3399},
 title = {Active contextual policy search},
 url = {http://jmlr.org/papers/v15/fabisch14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:feldman14a,
 abstract = {We present a multi-task learning approach to jointly estimate the means of multiple independent distributions from samples. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the individual task's sample averages. We derive the optimal amount of regularization for the two task case for the minimum risk estimator and a minimax estimator, and show that the optimal amount of regularization can be practically estimated without cross-validation. We extend the practical estimators to an arbitrary number of tasks. Simulations and real data experiments demonstrate the advantage of the proposed MTA estimators over standard averaging and James-Stein estimation.},
 author = {Sergey Feldman and Maya R. Gupta and Bela A. Frigyik},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W2153912717},
 pages = {3621--3662},
 title = {Revisiting Stein's paradox: multi-task averaging},
 url = {http://jmlr.org/papers/v15/feldman14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:fournierviger14a,
 abstract = {We present SPMF, an open-source data mining library offering implementations of more than 55 data mining algorithms. SPMF is a cross-platform library implemented in Java, specialized for discovering patterns in transaction and sequence databases such as frequent itemsets, association rules and sequential patterns. The source code can be integrated in other Java programs. Moreover, SPMF offers a command line interface and a simple graphical interface for quick testing. The source code is available under the GNU General Public License, version 3. The website of the project offers several resources such as documentation with examples of how to run each algorithm, a developer's guide, performance comparisons of algorithms, data sets, an active forum, a FAQ and a mailing list.},
 author = {Philippe Fournier-Viger and Antonio Gomariz and Ted Gueniche and Azadeh Soltani and Cheng-Wei Wu and Vincent S. Tseng},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W2126046032},
 pages = {3569--3573},
 title = {SPMF: a Java open-source pattern mining library},
 url = {http://jmlr.org/papers/v15/fournierviger14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:foxroberts14a,
 abstract = {Reliable semi-supervised learning, where a small amount of labelled data is complemented by a large body of unlabelled data, has been a long-standing goal of the machine learning community. However...},
 author = {Patrick Fox-Roberts and Edward Rosten},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W3032500228},
 pages = {367--443},
 title = {Unbiased generative semi-supervised learning},
 url = {http://jmlr.org/papers/v15/foxroberts14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:geist14a,
 abstract = {In the framework of Markov Decision Processes, off-policy learning, that is the problem of learning a linear approximation of the value function of some fixed policy from one trajectory possibly generated by some other policy. We briefly review on-policy learning algorithms of the literature (gradient-based and least-squares-based), adopting a unified algorithmic view. Then, we highlight a systematic approach for adapting them to off-policy learning with eligibility traces. This leads to some known algorithms - off-policy LSTD(\lambda), LSPE(\lambda), TD(\lambda), TDC/GQ(\lambda) - and suggests new extensions - off-policy FPKF(\lambda), BRM(\lambda), gBRM(\lambda), GTD2(\lambda). We describe a comprehensive algorithmic derivation of all algorithms in a recursive and memory-efficent form, discuss their known convergence properties and illustrate their relative empirical behavior on Garnet problems. Our experiments suggest that the most standard algorithms on and off-policy LSTD(\lambda)/LSPE(\lambda) - and TD(\lambda) if the feature space dimension is too large for a least-squares approach - perform the best.},
 author = {Matthieu Geist and Bruno Scherrer},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2953189830},
 pages = {289--333},
 title = {Off-policy Learning with Eligibility Traces: A Survey},
 url = {http://jmlr.org/papers/v15/geist14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:gentile14a,
 abstract = {We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show O(T1/2 log T) regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on diverse real-world multilabel data sets, often obtaining comparable performance.},
 author = {Claudio Gentile and Francesco Orabona},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W2117847193},
 pages = {2451--2487},
 title = {On multilabel classification and ranking with bandit feedback},
 url = {http://jmlr.org/papers/v15/gentile14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:gillian14a,
 abstract = {The Gesture Recognition Toolkit is a cross-platform open-source C++ library designed to make real-time machine learning and gesture recognition more accessible for non-specialists. Emphasis is placed on ease of use, with a consistent, minimalist design that promotes accessibility while supporting flexibility and customization for advanced users. The toolkit features a broad range of classification and regression algorithms and has extensive support for building real-time systems. This includes algorithms for signal processing, feature extraction and automatic gesture spotting.},
 author = {Nicholas Gillian and Joseph A. Paradiso},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W2151889438},
 pages = {3483--3487},
 title = {The Gesture Recognition Toolkit},
 url = {http://jmlr.org/papers/v15/gillian14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:gillis14a,
 abstract = {Nonnegative matrix factorization (NMF) has been shown recently to be tractable under the separability assumption, under which all the columns of the input data matrix belong to the convex cone generated by only a few of these columns. Bittorf, Recht, R\'e and Tropp (`Factoring nonnegative matrices with linear programs', NIPS 2012) proposed a linear programming (LP) model, referred to as Hottopixx, which is robust under any small perturbation of the input matrix. However, Hottopixx has two important drawbacks: (i) the input matrix has to be normalized, and (ii) the factorization rank has to be known in advance. In this paper, we generalize Hottopixx in order to resolve these two drawbacks, that is, we propose a new LP model which does not require normalization and detects the factorization rank automatically. Moreover, the new LP model is more flexible, significantly more tolerant to noise, and can easily be adapted to handle outliers and other noise models. Finally, we show on several synthetic datasets that it outperforms Hottopixx while competing favorably with two state-of-the-art methods.},
 author = {Nicolas Gillis and Robert Luce},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W4295253269},
 pages = {1249--1280},
 title = {Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization},
 url = {http://jmlr.org/papers/v15/gillis14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:goussies14a,
 abstract = {Decision forests are an increasingly popular tool in computer vision problems. Their advantages include high computational efficiency, state-of-the-art accuracy and multi-class support. In this paper, we present a novel method for transfer learning which uses decision forests, and we apply it to recognize gestures and characters. We introduce two mechanisms into the decision forest framework in order to transfer knowledge from the source tasks to a given target task. The first one is mixed information gain, which is a data-based regularizer. The second one is label propagation, which infers the manifold structure of the feature space. We show that both of them are important to achieve higher accuracy. Our experiments demonstrate improvements over traditional decision forests in the ChaLearn Gesture Challenge and MNIST data set. They also compare favorably against other state-of-the-art classifiers.},
 author = {Norberto A. Goussies and Sebasti{{\'a}}n Ubalde and Marta Mejail},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W2115595637},
 pages = {3847--3870},
 title = {Transfer Learning Decision Forests for Gesture Recognition},
 url = {http://jmlr.org/papers/v15/goussies14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:gupta14a,
 abstract = {Classification problems with thousands or more classes often have a large range of class-confusabilities, and we show that the more-confusable classes add more noise to the empirical loss that is minimized during training. We propose an online solution that reduces the effect of highly confusable classes in training the classifier parameters, and focuses the training on pairs of classes that are easier to differentiate at any given time in the training. We also show that the adagrad method, recently proposed for automatically decreasing step sizes for convex stochastic gradient descent, can also be profitably applied to the nonconvex joint training of supervised dimensionality reduction and linear classifiers as done in Wsabie. Experiments on ImageNet benchmark data sets and proprietary image recognition problems with 15,000 to 97,000 classes show substantial gains in classification accuracy compared to one-vs-all linear SVMs and Wsabie.},
 author = {Maya R. Gupta and Samy Bengio and Jason Weston},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W2151166364},
 pages = {1461--1492},
 title = {Training highly multiclass classifiers},
 url = {http://jmlr.org/papers/v15/gupta14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:hamilton14a,
 abstract = {Predictive state representations (PSRs) offer an expressive framework for modelling partially observable systems. By compactly representing systems as functions of observable quantities, the PSR learning approach avoids using local-minima prone expectation-maximization and instead employs a globally optimal moment-based algorithm. Moreover, since PSRs do not require a predetermined latent state structure as an input, they offer an attractive framework for model-based reinforcement learning when agents must plan without a priori access to a system model. Unfortunately, the expressiveness of PSRs comes with significant computational cost, and this cost is a major factor inhibiting the use of PSRs in applications. In order to alleviate this shortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSR learning approach combines recent advancements in dimensionality reduction, incremental matrix decomposition, and compressed sensing. We show how this approach provides a principled avenue for learning accurate approximations of PSRs, drastically reducing the computational costs associated with learning while also providing effective regularization. Going further, we propose a planning framework which exploits these learned models. And we show that this approach facilitates model-learning and planning in large complex partially observable domains, a task that is infeasible without the principled use of compression.},
 author = {William Hamilton and Mahdi Milani Fard and Joelle Pineau},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W2159136633},
 pages = {3575--3619},
 title = {Efficient learning and planning with compressed predictive states},
 url = {http://jmlr.org/papers/v15/hamilton14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:hansen14a,
 abstract = {In many applications, one has side information, e.g., labels that are provided in a semi-supervised manner, about a specific target region of a large data set, and one wants to perform machine learning and data analysis tasks "nearby" that prespecified target region. For example, one might be interested in the clustering structure of a data graph near a prespecified "seed set" of nodes, or one might be interested in finding partitions in an image that are near a prespecified "ground truth" set of pixels. Locally-biased problems of this sort are particularly challenging for popular eigenvector-based machine learning and data analysis tools. At root, the reason is that eigenvectors are inherently global quantities, thus limiting the applicability of eigenvector-based methods in situations where one is interested in very local properties of the data. In this paper, we address this issue by providing a methodology to construct semi-supervised eigenvectors of a graph Laplacian, and we illustrate how these locally-biased eigenvectors can be used to perform locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized directions of maximum variance, conditioned on being well-correlated with an input seed set of nodes that is assumed to be provided in a semi-supervised manner. We show that these semi-supervised eigenvectors can be computed quickly as the solution to a system of linear equations; and we also describe several variants of our basic method that have improved scaling properties. We provide several empirical examples demonstrating how these semi-supervised eigenvectors can be used to perform locally-biased learning; and we discuss the relationship between our results and recent machine learning algorithms that use global eigenvectors of the graph Laplacian.},
 author = {Toke J. Hansen and Michael W. Mahoney},
 journal = {Journal of Machine Learning Research},
 number = {114},
 openalex = {W1877845655},
 pages = {3871--3914},
 title = {Semi-supervised Eigenvectors for Large-scale Locally-biased Learning},
 url = {http://jmlr.org/papers/v15/hansen14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:hazan14a,
 abstract = {We give novel algorithms for stochastic strongly-convex optimization in the gradient oracle model which return a O(1/T)-approximate solution after T iterations. The first algorithm is deterministic, and achieves this rate via gradient updates and historical averaging. The second algorithm is randomized, and is based on pure gradient steps with a random step size.

his rate of convergence is optimal in the gradient oracle model. This improves upon the previously known best rate of O(log(T/T), which was obtained by applying an online strongly-convex optimization algorithm with regret O(log(T)) to the batch setting.

We complement this result by proving that any algorithm has expected regret of (log(T)) in the online stochastic strongly-convex optimization setting. This shows that any online-to-batch conversion is inherently suboptimal for stochastic strongly-convex optimization. This is the first formal evidence that online convex optimization is strictly more difficult than batch stochastic convex optimization.},
 author = {Elad Hazan and Satyen Kale},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2154682027},
 pages = {2489--2512},
 title = {Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
 url = {http://jmlr.org/papers/v15/hazan14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:henniges14a,
 abstract = {We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image. We show that the object parameters can be learned from an unlabeled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. Tractable approximations can be derived, however, by applying a truncated variational approach to Expectation Maximization (EM). In numerical experiments it is shown that these approximations recover the underlying set of object parameters including data noise and sparsity. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating components. The studied approach demonstrates that the multiple-causes generative approach can be generalized to extract occluding components, which links research on occlusion to the field of sparse coding approaches.},
 author = {Marc Henniges and Richard E. Turner and Maneesh Sahani and Julian Eggert and J{{\"o}}rg L{{\"u}}cke},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W2097676679},
 pages = {2689--2722},
 title = {Efficient occlusive components analysis},
 url = {http://jmlr.org/papers/v15/henniges14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:hoffman14a,
 abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size e and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more effciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter e on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient turnkey samplers.},
 author = {Matthew D. Hoffman and Andrew Gelman},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W2963977107},
 pages = {1593--1623},
 title = {The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo},
 url = {http://jmlr.org/papers/v15/hoffman14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:hoi14a,
 abstract = {LIBOL is an open-source library for large-scale online learning, which consists of a large family of efficient and scalable state-of-the-art online learning algorithms for large-scale online classification tasks. We have offered easy-to-use command-line tools and examples for users and developers, and also have made comprehensive documents available for both beginners and advanced users. LIBOL is not only a machine learning toolbox, but also a comprehensive experimental platform for conducting online learning research.},
 author = {Steven C.H. Hoi and Jialei Wang and Peilin Zhao},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2119393863},
 pages = {495--499},
 title = {LIBOL: a library for online learning algorithms},
 url = {http://jmlr.org/papers/v15/hoi14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:hsieh14a,
 abstract = {The l1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to recent state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and present experimental results using synthetic and real-world application data that demonstrate the considerable improvements in performance of our method when compared to previous methods.},
 author = {Cho-Jui Hsieh and M{{\'a}}ty{{\'a}}s A. Sustik and Inderjit S. Dhillon and Pradeep Ravikumar},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W2131242330},
 pages = {2911--2947},
 title = {QUIC: quadratic approximation for sparse inverse covariance estimation},
 url = {http://jmlr.org/papers/v15/hsieh14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:huang14a,
 abstract = {The ramp loss is a robust but non-convex loss for classification. Compared with other non-convex losses, a local minimum of the ramp loss can be effectively found. The effectiveness of local search comes from the piecewise linearity of the ramp loss. Motivated by the fact that the l1-penalty is piecewise linear as well, the l1-penalty is applied for the ramp loss, resulting in a ramp loss linear programming support vector machine (ramp-LPSVM). The proposed ramp-LPSVM is a piecewise linear minimization problem and the related optimization techniques are applicable. Moreover, the l1-penalty can enhance the sparsity. In this paper, the corresponding misclassification error and convergence behavior are discussed. Generally, the ramp loss is a truncated hinge loss. Therefore ramp-LPSVM possesses some similar properties as hinge loss SVMs. A local minimization algorithm and a global search strategy are discussed. The good optimization capability of the proposed algorithms makes ramp-LPSVM perform well in numerical experiments: the result of ramp-LPSVM is more robust than that of hinge SVMs and is sparser than that of ramp-SVM, which consists of the ||  || k-penalty and the ramp loss.},
 author = {Xiaolin Huang and Lei Shi and Johan A.K. Suykens},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W2107976320},
 pages = {2185--2211},
 title = {Ramp loss linear programming support vector machine},
 url = {http://jmlr.org/papers/v15/huang14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:jackson14a,
 abstract = {In a very strong positive result for passive learning algorithms, Bshouty et al. showed that DNF expressions are efficiently learnable in the uniform random walk model. It is natural to ask whether the more expressive class of thresholds of parities (TOP) can also be learned efficiently in this model, since both DNF and TOP are efficiently uniform-learnable from queries. However, the time bounds of the algorithms of Bshouty et al. are exponential for TOP. We present a new approach to weak parity learning that leads to quasi-efficient uniform random walk learnability of TOP. We also introduce a more general random walk model and give two positive results in this new model: DNF is efficiently learnable and juntas are efficiently agnostically learnable.},
 author = {Jeffrey C. Jackson and Karl Wimmer},
 journal = {Journal of Machine Learning Research},
 number = {112},
 openalex = {W109458800},
 pages = {3815--3846},
 title = {New results for random walk learning},
 url = {http://jmlr.org/papers/v15/jackson14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:janzamin14a,
 abstract = {Fitting high-dimensional data involves a delicate tradeoff between faithful representation and the use of sparse models. Too often, sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data. In this paper, we present a novel framework incorporating sparsity in different domains. We decompose the observed covariance matrix into a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse independence model (with a sparse covariance matrix). Our framework incorporates sparse covariance and sparse precision estimation as special cases and thus introduces a richer class of high-dimensional models. We characterize sufficient conditions for identifiability of the two models, viz., Markov and independence models. We propose an efficient decomposition method based on a modification of the popular l1-penalized maximum-likelihood estimator (l1-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples n scales as n = (d2 log p), where p is the number of variables and d is the maximum node degree in the Markov model. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation.},
 author = {Majid Janzamin and Animashree Anandkumar},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W2614599440},
 pages = {1549--1591},
 title = {High-dimensional covariance decomposition into sparse Markov and independence models},
 url = {http://jmlr.org/papers/v15/janzamin14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:javanmard14a,
 abstract = {Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p- values for these models.

We consider here high-dimensional linear regression problem, and propose an efficient algorithm for constructing confidence intervals and p-values. The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power.

Our approach is based on constructing a 'de-biased' version of regularized M-estimators. The new construction improves over recent work in the field in that it does not assume a special structure on the design matrix. We test our method on synthetic data and a high-throughput genomic data set about riboflavin production rate, made publicly available by Buhlmann et al. (2014).},
 author = {Adel Javanmard and Andrea Montanari},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W2128235479},
 pages = {2869--2909},
 title = {Confidence intervals and hypothesis testing for high-dimensional regression},
 url = {http://jmlr.org/papers/v15/javanmard14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:jin14a,
 abstract = {Consider a linear model Y = X+z, where X has n rows and p columns and z - N(0, In). We assume both p and n are large, including the case of p  n. The unknown signal vector  is assumed to be sparse in the sense that only a small fraction of its components is nonzero. The goal is to identify such nonzero coordinates (i.e., variable selection).

We are primarily interested in the regime where signals are both rare and weak so that successful variable selection is challenging but is still possible. We assume the Gram matrix G = XX is sparse in the sense that each row has relatively few large entries (diagonals of G are normalized to 1). The sparsity of G naturally induces the sparsity of the so-called Graph of Strong Dependence (GOSD). The key insight is that there is an interesting interplay between the signal sparsity and graph sparsity: in a broad context, the signals decompose into many small-size components of GOSD that are disconnected to each other.

We propose Graphlet Screening for variable selection. This is a two-step Screen and Clean procedure, where in the first step, we screen subgraphs of GOSD with sequential 2-tests, and in the second step, we clean with penalized MLE. The main methodological innovation is to use GOSD to guide both the screening and cleaning processes.

For any variable selection procedure , we measure its performance by the Hamming distance between the sign vectors of  and , and assess the optimality by the minimax Hamming distance. Compared with more stringent criteria such as exact support recovery or oracle property, which demand strong signals, the Hamming distance criterion is more appropriate for weak signals since it naturally allows a small fraction of errors.

We show that in a broad class of situations, Graphlet Screening achieves the optimal rate of convergence in terms of the Hamming distance. Unlike Graphlet Screening, well-known procedures such as the L0/L1-penalization methods do not utilize local graphic structure for variable selection, so they generally do not achieve the optimal rate of convergence, even in very simple settings and even if the tuning parameters are ideally set.

The the presented algorithm is implemented as R-CRAN package ScreenClean and in matlab (available at http://www.stat.cmu.edu/~jiashun/Research/software/GS-matlab/).},
 author = {Jiashun Jin and Cun-Hui Zhang and Qi Zhang},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W2158391286},
 pages = {2723--2772},
 title = {Optimality of graphlet screening in high dimensional variable selection},
 url = {http://jmlr.org/papers/v15/jin14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:judah14a,
 author = {Kshitij Judah and Alan P. Fern and Thomas G. Dietterich and Prasad Tadepalli},
 journal = {Journal of Machine Learning Research},
 number = {120},
 pages = {4105--4143},
 title = {Active Imitation Learning: Formal and Practical Reductions to I.I.D. Learning},
 url = {http://jmlr.org/papers/v15/judah14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:jylanki14a,
 abstract = {We propose a novel approach for nonlinear regression using a two-layer neural network (NN) model structure with sparsity-favoring hierarchical priors on the network weights. We present an expectation propagation (EP) approach for approximate integration over the posterior distribution of the weights, the hierarchical scale parameters of the priors, and the residual scale. Using a factorized posterior approximation we derive a computationally effcient algorithm, whose complexity scales similarly to an ensemble of independent sparse linear models. The approach enables flexible definition of weight priors with different sparseness properties such as independent Laplace priors with a common scale parameter or Gaussian automatic relevance determination (ARD) priors with different relevance parameters for all inputs. The approach can be extended beyond standard activation functions and NN model structures to form flexible nonlinear predictors from multiple sparse linear models. The effects of the hierarchical priors and the predictive performance of the algorithm are assessed using both simulated and real-world data. Comparisons are made to two alternative models with ARD priors: a Gaussian process with a NN covariance function and marginal maximum a posteriori estimates of the relevance parameters, and a NN with Markov chain Monte Carlo integration over all the unknown model parameters.},
 author = {Pasi Jyl{{\"a}}nki and Aapo Nummenmaa and Aki Vehtari},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W2099479976},
 pages = {1849--1901},
 title = {Expectation propagation for neural networks with sparsity-promoting priors},
 url = {http://jmlr.org/papers/v15/jylanki14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:kolar14a,
 abstract = {Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data. For example, they are often used to explore complex systems where connections between entities are not well understood, such as in functional brain networks or genetic networks. Existing methods for estimating structure of undirected graphical models focus on scenarios where each node represents a scalar random variable, such as a binary neural activation state or a continuous mRNA abundance measurement, even though in many real world problems, nodes can represent multivariate variables with much richer meanings, such as whole images, text documents, or multi-view feature vectors. In this paper, we propose a new principled framework for estimating the structure of undirected graphical models from such multivariate (or multi-attribute) nodal data. The structure of a graph is inferred through estimation of non-zero partial canonical correlation between nodes. Under a Gaussian model, this strategy is equivalent to estimating conditional independencies between random vectors represented by the nodes and it generalizes the classical problem of covariance selection (Dempster, 1972). We relate the problem of estimating non-zero partial canonical correlations to maximizing a penalized Gaussian likelihood objective and develop a method that efficiently maximizes this objective. Extensive simulation studies demonstrate the effectiveness of the method under various conditions. We provide illustrative applications to uncovering gene regulatory networks from gene and protein profiles, and uncovering brain connectivity graph from positron emission tomography data. Finally, we provide sufficient conditions under which the true graphical structure can be recovered correctly.},
 author = {Mladen Kolar and Han Liu and Eric P. Xing},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W2162926179},
 pages = {1713--1750},
 title = {Graph Estimation From Multi-attribute Data},
 url = {http://jmlr.org/papers/v15/kolar14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:konecny14a,
 abstract = {The purpose of this paper is to describe one-shot-learning gesture recognition systems developed on the \textit{ChaLearn Gesture Dataset}. We use RGB and depth images and combine appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) for parallel temporal segmentation and recognition. The Quadratic-Chi distance family is used to measure differences between histograms to capture cross-bin relationships. We also propose a new algorithm for trimming videos --- to remove all the unimportant frames from videos. We present two methods that use combination of HOG-HOF descriptors together with variants of Dynamic Time Warping technique. Both methods outperform other published methods and help narrow down the gap between human performance and algorithms on this task. The code has been made publicly available in the MLOSS repository.},
 author = {Jakub Konecny and Michal Hagara},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W4301810716},
 pages = {2513--2532},
 title = {One-Shot-Learning Gesture Recognition using HOG-HOF Features},
 url = {http://jmlr.org/papers/v15/konecny14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:lan14a,
 abstract = {We develop a new model and algorithms for machine learning-based learning analytics, which estimate a learner's knowledge of the concepts underlying a domain, and content analytics, which estimate the relationships among a collection of questions and those concepts. Our model represents the probability that a learner provides the correct response to a question in terms of three factors: their understanding of a set of underlying concepts, the concepts involved in each question, and each question's intrinsic difficulty. We estimate these factors given the graded responses to a collection of questions. The underlying estimation problem is ill-posed in general, especially when only a subset of the questions are answered. The key observation that enables a well-posed solution is the fact that typical educational domains of interest involve only a small number of key concepts. Leveraging this observation, we develop both a bi-convex maximum-likelihood-based solution and a Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem. We also incorporate user-defined tags on questions to facilitate the interpretability of the estimated factors. Experiments with synthetic and real-world data demonstrate the efficacy of our approach. Finally, we make a connection between SPARFA and noisy, binary-valued (1-bit) dictionary learning that is of independent interest.},
 author = {Andrew S. Lan and Andrew E. Waters and Christoph Studer and Richard G. Baraniuk},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W2126917700},
 pages = {1959--2008},
 title = {Sparse factor analysis for learning and content analytics},
 url = {http://jmlr.org/papers/v15/lan14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:lecci14a,
 abstract = {A metric graph is a 1-dimensional stratified metric space consisting of vertices and edges or loops glued together. Metric graphs can be naturally used to represent and model data that take the form of noisy filamentary structures, such as street maps, neurons, networks of rivers and galaxies. We consider the statistical problem of reconstructing the topology of a metric graph embedded in RD from a random sample. We derive lower and upper bounds on the minimax risk for the noiseless case and tubular noise case. The upper bound is based on the reconstruction algorithm given in Aanjaneya et al. (2012).},
 author = {Fabrizio Lecci and Aless and ro Rinaldo and Larry Wasserman},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W2963169159},
 pages = {3425--3446},
 title = {Statistical analysis of metric graph reconstruction},
 url = {http://jmlr.org/papers/v15/lecci14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:lember14a,
 abstract = {Motivated by the unceasing interest in hidden Markov models (HMMs), this paper reexamines hidden path inference in these models, using primarily a risk-based framework. While the most common maximum a posteriori (MAP), or Viterbi, path estimator and the minimum error, or Posterior Decoder (PD) have long been around, other path estimators, or decoders, have been either only hinted at or applied more recently and in dedicated applications generally unfamiliar to the statistical learning community. Over a decade ago, however, a family of algorithmically defined decoders aiming to hybridize the two standard ones was proposed elsewhere. The present paper gives a careful analysis of this hybridization approach, identifies several problems and issues with it and other previously proposed approaches, and proposes practical resolutions of those. Furthermore, simple modifications of the classical criteria for hidden path recognition are shown to lead to a new class of decoders. Dynamic programming algorithms to compute these decoders in the usual forward-backward manner are presented. A particularly interesting subclass of such estimators can be also viewed as hybrids of the MAP and PD estimators. Similar to previously proposed MAP-PD hybrids, the new class is parameterized by a small number of tunable parameters. Unlike their algorithmic predecessors, the new risk-based decoders are more clearly interpretable, and, most importantly, work out-of-the box in practice, which is demonstrated on some real bioinformatics tasks and data. Some further generalizations and applications are discussed in the conclusion.},
 author = {J{{\"u}}ri Lember and Alexey A. Koloydenko},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2128889245},
 pages = {1--58},
 title = {Bridging Viterbi and posterior decoding: a generalized risk approach to hidden path inference based on hidden Markov models},
 url = {http://jmlr.org/papers/v15/lember14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:lin14a,
 abstract = {We adapt the alternating linearization method for proximal decomposition to structured regularization problems, in particular, to the generalized lasso problems. The method is related to two well-known operator splitting methods, the Douglas-Rachford and the Peaceman-Rachford method, but it has descent properties with respect to the objective function. This is achieved by employing a special update test, which decides whether it is beneficial to make a Peaceman-Rachford step, any of the two possible Douglas-Rachford steps, or none. The convergence mechanism of the method is related to that of bundle methods of nonsmooth optimization. We also discuss implementation for very large problems, with the use of specialized algorithms and sparse data structures. Finally, we present numerical results for several synthetic and real-world examples, including a three-dimensional fused lasso problem, which illustrate the scalability, efficacy, and accuracy of the method.},
 author = {Xiaodong Lin and Minh Pham and Andrzej Ruszczy\'{n}ski},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2138998322},
 pages = {3447--3481},
 title = {Alternating linearization for structured regularization problems},
 url = {http://jmlr.org/papers/v15/lin14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:lindsten14a,
 abstract = {Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining the two main tools used for Monte Carlo statistical inference: sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We present a novel PMCMC algorithm that we refer to as particle Gibbs with ancestor sampling (PGAS). PGAS provides the data analyst with an off-the-shelf class of Markov kernels that can be used to simulate the typically high-dimensional and highly autocorrelated state trajectory in a state-space model. The ancestor sampling procedure enables fast mixing of the PGAS kernel even when using seemingly few particles in the underlying SMC sampler. This is important as it can significantly reduce the computational burden that is typically associated with using SMC. PGAS is conceptually similar to the existing PG with backward simulation (PGBS) procedure. Instead of using separate forward and backward sweeps as in PGBS, however, we achieve the same effect in a single forward sweep. This makes PGAS well suited for addressing inference problems not only in state-space models, but also in models with more complex dependencies, such as non-Markovian, Bayesian nonparametric, and general probabilistic graphical models.},
 author = {Fredrik Lindsten and Michael I. Jordan and Thomas B. Sch{{\"o}}n},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W4300009125},
 pages = {2145--2184},
 title = {Particle Gibbs with Ancestor Sampling},
 url = {http://jmlr.org/papers/v15/lindsten14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:loh14a,
 abstract = {We establish a new framework for statistical estimation of directed acyclic graphs (DAGs) when data are generated from a linear, possibly non-Gaussian structural equation model. Our framework consists of two parts: (1) inferring the moralized graph from the support of the inverse covariance matrix; and (2) selecting the best-scoring graph amongst DAGs that are consistent with the moralized graph. We show that when the error variances are known or estimated to close enough precision, the true DAG is the unique minimizer of the score computed using the reweighted squared l_2-loss. Our population-level results have implications for the identifiability of linear SEMs when the error covariances are specified up to a constant multiple. On the statistical side, we establish rigorous conditions for high-dimensional consistency of our two-part algorithm, defined in terms of a "gap" between the true DAG and the next best candidate. Finally, we demonstrate that dynamic programming may be used to select the optimal DAG in linear time when the treewidth of the moralized graph is bounded.},
 author = {Po-Ling Loh and Peter B{{\"u}}hlmann},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W2109605413},
 pages = {3065--3105},
 title = {High-dimensional learning of linear causal networks via inverse covariance estimation},
 url = {http://jmlr.org/papers/v15/loh14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:lowd14a,
 abstract = {Most existing algorithms for learning Markov network structure either are limited to learning interactions among few variables or are very slow, due to the large space of possible structures. In this paper, we propose three new methods for using decision trees to learn Markov network structures. The advantage of using decision trees is that they are very fast to learn and can represent complex interactions among many variables. The first method, DTSL, learns a decision tree to predict each variable and converts each tree into a set of conjunctive features that define the Markov network structure. The second, DT-BLM, builds on DTSL by using it to initialize a search-based Markov network learning algorithm recently proposed by Davis and Domingos (2010). The third, DT+L1, combines the features learned by DTSL with those learned by an L1-regularized logistic regression method (L1) proposed by Ravikumar et al. (2009). In an extensive empirical evaluation on 20 data sets, DTSL is comparable to L1 and significantly faster and more accurate than two other baselines. DT-BLM is slower than DTSL, but obtains slightly higher accuracy. DT+L1 combines the strengths of DTSL and L1 to perform significantly better than either of them with only a modest increase in training time.},
 author = {Daniel Lowd and Jesse Davis},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2166946383},
 pages = {501--532},
 title = {Improving Markov network structure learning using decision trees},
 url = {http://jmlr.org/papers/v15/lowd14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:lu14a,
 abstract = {Learning preference distributions is a critical problem in many areas (e.g., recommender systems, IR, social choice). However, many existing learning and inference methods impose restrictive assumptions on the form of user preferences that can be admitted as evidence. We relax these restrictions by considering as data arbitrary pairwise comparisons of alternatives, which represent the fundamental building blocks of ordinal rankings. We develop the first algorithms for learning Mallows models (and mixtures thereof) from pairwise comparison data. At the heart of our technique is a new algorithm, the generalized repeated insertion model (GRIM), which allows sampling from arbitrary ranking distributions, and conditional Mallows models in particular. While we show that sampling from a Mallows model with pairwise evidence is computationally difficult in general, we develop approximate samplers that are exact for many important special cases|and have provable bounds with pairwise evidence--and derive algorithms for evaluating log-likelihood, learning Mallows mixtures, and non-parametric estimation. Experiments on real-world data sets demonstrate the effectiveness of our approach.},
 author = {Tyler Lu and Craig Boutilier},
 journal = {Journal of Machine Learning Research},
 number = {117},
 openalex = {W2114523181},
 pages = {3963--4009},
 title = {Effective sampling and learning for mallows models with pairwise-preference data},
 url = {http://jmlr.org/papers/v15/lu14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:lyzinski14a,
 abstract = {Graph matching is an important problem in machine learning and pattern recognition. Herein, we present theoretical and practical results on the consistency of graph matching for estimating a latent alignment function between the vertex sets of two graphs, as well as subsequent algorithmic implications when the latent alignment is partially observed. In the correlated Erdos-Renyi graph setting, we prove that graph matching provides a strongly consistent estimate of the latent alignment in the presence of even modest correlation. We then investigate a tractable, restricted-focus version of graph matching, which is only concerned with adjacency involving vertices in a partial observation of the latent alignment; we prove that a logarithmic number of vertices whose alignment is known is sufficient for this restricted-focus version of graph matching to yield a strongly consistent estimate of the latent alignment of the remaining vertices. We show how Frank-Wolfe methodology for approximate graph matching, when there is a partially observed latent alignment, inherently incorporates this restricted-focus graph matching. Lastly, we illustrate the relationship between seeded graph matching and restricted-focus graph matching by means of an illuminating example from human connectomics.},
 author = {Vince Lyzinski and Donniell E. Fishkind and Carey E. Priebe},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W2252481082},
 pages = {3693--3720},
 title = {Seeded graph matching for correlated Erds-Rnyi graphs},
 url = {http://jmlr.org/papers/v15/lyzinski14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:mannor14a,
 abstract = {Approachability has become a standard tool in analyzing learning algorithms in the adversarial online learning setup. We develop a variant of approachability for games where there is ambiguity in t...},
 author = {Shie Mannor and Vianney Perchet and Gilles Stoltz},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W3085605451},
 pages = {3247--3295},
 title = {Set-valued approachability and online learning with partial monitoring},
 url = {http://jmlr.org/papers/v15/mannor14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:martinezcantin14a,
 abstract = {BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization is sample efficient by building a posterior distribution to capture the evidence and prior knowledge for the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave.},
 author = {Ruben Martinez-Cantin},
 journal = {Journal of Machine Learning Research},
 number = {115},
 openalex = {W4295725036},
 pages = {3915--3919},
 title = {BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits},
 url = {http://jmlr.org/papers/v15/martinezcantin14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:miller14a,
 abstract = {In many applications, a finite mixture is a natural model, but it can be difficult to choose an appropriate number of components. To circumvent this choice, investigators are increasingly turning to Dirichlet process mixtures (DPMs), and Pitman-Yor process mixtures (PYMs), more generally. While these models may be well-suited for Bayesian density estimation, many investigators are using them for inferences about the number of components, by considering the posterior on the number of components represented in the observed data. We show that this posterior is not consistent --- that is, on data from a finite mixture, it does not concentrate at the true number of components. This result applies to a large class of nonparametric mixtures, including DPMs and PYMs, over a wide variety of families of component distributions, including essentially all discrete families, as well as continuous exponential families satisfying mild regularity conditions (such as multivariate Gaussians).},
 author = {Jeffrey W. Miller and Matthew T. Harrison},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W2950797261},
 pages = {3333--3370},
 title = {Inconsistency of Pitman-Yor process mixtures for the number of components},
 url = {http://jmlr.org/papers/v15/miller14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:mizutani14a,
 abstract = {We present a numerical algorithm for nonnegative matrix factorization (NMF) problems under noisy separability. An NMF problem under separability can be stated as one of finding all vertices of the convex hull of data points. The research interest of this paper is to find the vectors as close to the vertices as possible in a situation in which noise is added to the data points. Our algorithm is designed to capture the shape of the convex hull of data points by using its enclosing ellipsoid. We show that the algorithm has correctness and robustness properties from theoretical and practical perspectives; correctness here means that if the data points do not contain any noise, the algorithm can find the vertices of their convex hull; robustness means that if the data points contain noise, the algorithm can find the near-vertices. Finally, we apply the algorithm to document clustering, and report the experimental results.},
 author = {Tomohiko Mizutani},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W2114760097},
 pages = {1011--1039},
 title = {Ellipsoidal rounding for nonnegative matrix factorization under noisy separability},
 url = {http://jmlr.org/papers/v15/mizutani14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:mohan14a,
 abstract = {We consider the problem of estimating high-dimensional Gaussian graphical models corresponding to a single set of variables under several distinct conditions. This problem is motivated by the task of recovering transcriptional regulatory networks on the basis of gene expression data containing heterogeneous samples, such as different disease states, multiple species, or different developmental stages. We assume that most aspects of the conditional dependence networks are shared, but that there are some structured differences between them. Rather than assuming that similarities and differences between networks are driven by individual edges, we take a node-based approach, which in many cases provides a more intuitive interpretation of the network differences. We consider estimation under two distinct assumptions: (1) differences between the K networks are due to individual nodes that are perturbed across conditions, or (2) similarities among the K networks are due to the presence of common hub nodes that are shared across all K networks. Using a row-column overlap norm penalty function, we formulate two convex optimization problems that correspond to these two assumptions. We solve these problems using an alternating direction method of multipliers algorithm, and we derive a set of necessary and sufficient conditions that allows us to decompose the problem into independent subproblems so that our algorithm can be scaled to high-dimensional settings. Our proposal is illustrated on synthetic data, a webpage data set, and a brain cancer gene expression data set.},
 author = {Karthik Mohan and Palma London and Maryam Fazel and Daniela Witten and Su-In Lee},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W2166794017},
 pages = {445--488},
 title = {Node-Based Learning of Multiple Gaussian Graphical Models.},
 url = {http://jmlr.org/papers/v15/mohan14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:moore14a,
 abstract = {Clinical research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index of the electroencephalogram as the controlled variable. These controllers have evolved to yield patient-specific anesthesia, which is associated with improved patient outcomes. Despite progress, the problem of patient-specific anesthesia remains unsolved. A variety of factors confound good control, including variations in human physiology, imperfect measures of drug effect, and delayed, hysteretic response to drug delivery. Reinforcement learning (RL) appears to be uniquely equipped to overcome these challenges; however, the literature offers no precedent for RL in anesthesia. To begin exploring the role RL might play in improving anesthetic care, we investigated the method's application in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to performance metrics reported in the anesthesia literature, RL demonstrated patient-specific control marked by improved accuracy and stability. Furthermore, these results suggest that RL may be considered a viable alternative for solving other difficult closed-loop control problems in medicine. More rigorous clinical study, beyond the confines of controlled human volunteer studies, is needed to substantiate these findings.},
 author = {Brett L Moore and Larry D Pyeatt and Vivekan and Kulkarni and Periklis Panousis and Kevin Padrez and Anthony G Doufas},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2103535498},
 pages = {655--696},
 title = {Reinforcement learning for closed-loop propofol anesthesia: a study in human volunteers},
 url = {http://jmlr.org/papers/v15/moore14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:mueller14a,
 abstract = {Structured prediction methods have become a central tool for many machine learning applications. While more and more algorithms are developed, only very few implementations are available.

PyStruct aims at providing a general purpose implementation of standard structured prediction methods, both for practitioners and as a baseline for researchers. It is written in Python and adapts paradigms and types from the scientific Python community for seamless integration with other projects.},
 author = {Andreas C. M{{\"u}}ller and Sven Behnke},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W181529752},
 pages = {2055--2060},
 title = {PyStruct: learning structured prediction in python},
 url = {http://jmlr.org/papers/v15/mueller14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:nandan14a,
 abstract = {Applications of non-linear kernel Support Vector Machines (SVMs) to large datasets is seriously hampered by its excessive training time. We propose a modification, called the approximate extreme points support vector machine (AESVM), that is aimed at overcoming this burden. Our approach relies on conducting the SVM optimization over a carefully selected subset, called the representative set, of the training dataset. We present analytical results that indicate the similarity of AESVM and SVM solutions. A linear time algorithm based on convex hulls and extreme points is used to compute the representative set in kernel space. Extensive computational experiments on nine datasets compared AESVM to LIBSVM \citep{LIBSVM}, CVM \citep{Tsang05}, BVM \citep{Tsang07}, LASVM \citep{Bordes05}, $\text{SVM}^{\text{perf}}$ \citep{Joachims09}, and the random features method \citep{rahimi07}. Our AESVM implementation was found to train much faster than the other methods, while its classification accuracy was similar to that of LIBSVM in all cases. In particular, for a seizure detection dataset, AESVM training was almost $10^3$ times faster than LIBSVM and LASVM and more than forty times faster than CVM and BVM. Additionally, AESVM also gave competitively fast classification times.},
 author = {Manu Nandan and Pramod P. Khargonekar and Sachin S. Talathi},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W4297939357},
 pages = {59--98},
 title = {Fast SVM training using approximate extreme points},
 url = {http://jmlr.org/papers/v15/nandan14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:nguyendinh14a,
 abstract = {Crowdsourcing is a promising way to reduce the effort of collecting annotations for training gesture recognition systems. Crowdsourced annotations suffer from noise such as mislabeling, or inaccu...},
 author = {Long-Van Nguyen-Dinh and Alberto Calatroni and Gerhard Tr\"{o}ster},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W3024112198},
 pages = {3187--3220},
 title = {Robust online gesture recognition with crowdsourced annotations},
 url = {http://jmlr.org/papers/v15/nguyendinh14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:nishihara14a,
 abstract = {Probabilistic models are conceptually powerful tools for finding structure in data, but their practical effectiveness is often limited by our ability to perform inference in them. Exact inference is frequently intractable, so approximate inference is often performed using Markov chain Monte Carlo (MCMC). To achieve the best possible results from MCMC, we want to effciently simulate many steps of a rapidly mixing Markov chain which leaves the target distribution invariant. Of particular interest in this regard is how to take advantage of multi-core computing to speed up MCMC-based inference, both to improve mixing and to distribute the computational load. In this paper, we present a parallelizable Markov chain Monte Carlo algorithm for effciently sampling from continuous probability distributions that can take advantage of hundreds of cores. This method shares information between parallel Markov chains to build a scale-location mixture of Gaussians approximation to the density function of the target distribution. We combine this approximation with a recently developed method known as elliptical slice sampling to create a Markov chain with no step-size parameters that can mix rapidly without requiring gradient or curvature computations.},
 author = {Robert Nishihara and Iain Murray and Ryan P. Adams},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W2142819099},
 pages = {2087--2112},
 title = {Parallel MCMC with generalized elliptical slice sampling},
 url = {http://jmlr.org/papers/v15/nishihara14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:oentaryo14a,
 abstract = {Click fraud-the deliberate clicking on advertisements with no real interest on the product or service offered-is one of the most daunting problems in online advertising. Building an effective fraud detection method is thus pivotal for online advertising businesses. We organized a Fraud Detection in Mobile Advertising (FDMA) 2012 Competition, opening the opportunity for participants to work on real-world fraud data from BuzzCity Pte. Ltd., a global mobile advertising company based in Singapore. In particular, the task is to identify fraudulent publishers who generate illegitimate clicks, and distinguish them from normal publishers. The competition was held from September 1 to September 30, 2012, attracting 127 teams from more than 15 countries. The mobile advertising data are unique and complex, involving heterogeneous information, noisy patterns with missing values, and highly imbalanced class distribution. The competition results provide a comprehensive study on the usability of data mining-based fraud detection approaches in practical setting. Our principal findings are that features derived from fine-grained time-series analysis are crucial for accurate fraud detection, and that ensemble methods offer promising solutions to highly-imbalanced nonlinear classification tasks with mixed variable types and noisy/missing patterns. The competition data remain available for further studies at http://palanteer.sis.smu.edu.sg/fdma2012/.},
 author = {Richard Oentaryo and Ee-Peng Lim and Michael Finegold and David Lo and Feida Zhu and Clifton Phua and Eng-Yeow Cheu and Ghim-Eng Yap and Kelvin Sim and Minh Nhut Nguyen and Kasun Perera and Bijay Neupane and Mustafa Faisal and Zeyar Aung and Wei Lee Woon and Wei Chen and Dhaval Patel and Daniel Berrar},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W2148086182},
 pages = {99--140},
 title = {Detecting click fraud in online advertising: a data mining approach},
 url = {http://jmlr.org/papers/v15/oentaryo14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:osting14a,
 abstract = {Given a graph where vertices represent alternatives and arcs represent pairwise comparison data, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. Our goal in this paper is to develop a method for collecting data for which the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximizes the informativeness of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding multigraphs with large algebraic connectivity. This reduction of the data collection problem to graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating data set and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. As another application, we study the 2011-12 NCAA football schedule and propose schedules with the same number of games which are significantly more informative. Using spectral clustering methods to identify highly-connected communities within the division, we argue that the NCAA could improve its notoriously poor rankings by simply scheduling more out-of-conference games.},
 author = {Braxton Osting and Christoph Brune and Stanley J.  Osher},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W1884597034},
 pages = {2981--3012},
 title = {Optimal data collection for informative rankings expose well-connected graphs},
 url = {http://jmlr.org/papers/v15/osting14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:pang14a,
 abstract = {We develop an R package FASTCLIME for solving a family of regularized linear programming (LP) problems. Our package efficiently implements the parametric simplex algorithm, which provides a scalabl...},
 author = {Haotian Pang and Han Liu and Robert V and erbei},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W3114411530},
 pages = {489--493},
 title = {The fastclime package for linear programming and large-scale precision matrix estimation in R},
 url = {http://jmlr.org/papers/v15/pang14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:peters14a,
 abstract = {We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (regression with subsequent independence test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation.},
 author = {Jonas Peters and Joris M. Mooij and Dominik Janzing and Bernhard Sch{{\"o}}lkopf},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W2132547334},
 pages = {2009--2053},
 title = {Causal discovery with continuous additive noise models},
 url = {http://jmlr.org/papers/v15/peters14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:raskutti14a,
 abstract = {Early stopping is a form of regularization based on choosing when to stop running an iterative algorithm. Focusing on non-parametric regression in a reproducing kernel Hilbert space, we analyze the early stopping strategy for a form of gradient-descent applied to the least-squares loss function. We propose a data-dependent stopping rule that does not involve hold-out or cross-validation data, and we prove upper bounds on the squared error of the resulting function estimate, measured in either the L2(P) and L2(Pn) norm. These upper bounds lead to minimax-optimal rates for various kernel classes, including Sobolev smoothness classes and other forms of reproducing kernel Hilbert spaces. We show through simulation that our stopping rule compares favorably to two other stopping rules, one based on hold-out data and the other based on Stein's unbiased risk estimate. We also establish a tight connection between our early stopping strategy and the solution path of a kernel ridge regression estimator.},
 author = {Garvesh Raskutti and Martin J. Wainwright and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W1869625623},
 pages = {335--366},
 title = {Early stopping and non-parametric regression: an optimal data-dependent stopping rule},
 url = {http://jmlr.org/papers/v15/raskutti14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:reece14a,
 abstract = {Latent force models (LFM) are principled approaches to incorporating solutions to differential equations within non-parametric inference methods. Unfortunately, the development and application of LFMs can be inhibited by their computational cost, especially when closed-form solutions for the LFM are unavailable, as is the case in many real world problems where these latent forces exhibit periodic behaviour. Given this, we develop a new sparse representation of LFMs which considerably improves their computational efficiency, as well as broadening their applicability, in a principled way, to domains with periodic or near periodic latent forces. Our approach uses a linear basis model to approximate one generative model for each periodic force. We assume that the latent forces are generated from Gaussian process priors and develop a linear basis model which fully expresses these priors. We apply our approach to model the thermal dynamics of domestic buildings and show that it is effective at predicting day-ahead temperatures within the homes. We also apply our approach within queueing theory in which quasi-periodic arrival rates are modelled as latent forces. In both cases, we demonstrate that our approach can be implemented efficiently using state-space methods which encode the linear dynamic systems via LFMs. Further, we show that state estimates obtained using periodic latent force models can reduce the root mean squared error to 17% of that from non-periodic models and 27% of the nearest rival approach which is the resonator model (Sarkka et al., 2012; Hartikainen et al., 2012).},
 author = {Steven Reece and Siddhartha Ghosh and Alex Rogers and Stephen Roberts and Nicholas R. Jennings},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2117146921},
 pages = {2337--2397},
 title = {Efficient state-space inference of periodic latent force models},
 url = {http://jmlr.org/papers/v15/reece14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:richard14a,
 abstract = {In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices. On the adjacency matrix it takes into account both sparsity and low rank properties and on the VAR it encodes the sparsity. The analysis involves oracle inequalities that illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank. The estimate is computed efficiently using proximal methods, and evaluated through numerical experiments.},
 author = {Emile Richard and St{{\'e}}phane Ga{{\"i}}ffas and Nicolas Vayatis},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W2580322410},
 pages = {565--593},
 title = {Link prediction in graphs with autoregressive features},
 url = {http://jmlr.org/papers/v15/richard14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:rooij14a,
 abstract = {Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that guarantees constant regret in the stochastic setting, but has terrible performance for worst-case data. Other hedging strategies have better worst-case guarantees but may perform much worse than FTL if the data are not maximally adversarial. We introduce the FlipFlop algorithm, which is the first method that provably combines the best of both worlds. As part of our construction, we develop AdaHedge, which is a new way of dynamically tuning the learning rate in Hedge without using the doubling trick. AdaHedge refines a method by Cesa-Bianchi, Mansour and Stoltz (2007), yielding slightly improved worst-case guarantees. By interleaving AdaHedge and FTL, the FlipFlop algorithm achieves regret within a constant factor of the FTL regret, without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance; moreover, unlike earlier methods, both have the intuitive property that the issued weights are invariant under rescaling and translation of the losses. The losses are also allowed to be negative, in which case they may be interpreted as gains.},
 author = {Steven de Rooij and Tim van Erven and Peter D. Gr{{\"u}}nwald and Wouter M. Koolen},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2149107760},
 pages = {1281--1316},
 title = {Follow the Leader If You Can, Hedge If You Must},
 url = {http://jmlr.org/papers/v15/rooij14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:ruiz14a,
 abstract = {The analysis of comorbidity is an open and complex research field in the branch of psychiatry, where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications. In this paper, we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them. To this end, we use the large amount of information collected in the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database and propose to model these data using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the discrete nature of the data, we first need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. We also provide a variational inference algorithm for this model, which provides a complementary (and less expensive in terms of computational complexity) alternative to the Gibbs sampler allowing us to deal with a larger number of data. Finally, we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the NESARC database.},
 author = {Francisco J. R. Ruiz and Isabel Valera and Carlos Blanco and Fern and o Perez-Cruz},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W2949511756},
 pages = {1215--1247},
 title = {Bayesian nonparametric comorbidity analysis of psychiatric disorders},
 url = {http://jmlr.org/papers/v15/ruiz14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:saberian14a,
 abstract = {The problem of learning classifier cascades is considered. A new cascade boosting algorithm, fast cascade boosting (FCBoost), is proposed. FCBoost is shown to have a number of interesting propertie...},
 author = {Mohammad Saberian and Nuno Vasconcelos},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W3010693587},
 pages = {2569--2605},
 title = {Boosting algorithms for detector cascade learning},
 url = {http://jmlr.org/papers/v15/saberian14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:shah14a,
 abstract = {Finding interactions between variables in large and high-dimensional datasets is often a serious computational challenge. Most approaches build up interaction sets incrementally, adding variables in a greedy fashion. The drawback is that potentially informative high-order interactions may be overlooked. Here, we propose at an alternative approach for classification problems with binary predictor variables, called Random Intersection Trees. It works by starting with a maximal interaction that includes all variables, and then gradually removing variables if they fail to appear in randomly chosen observations of a class of interest. We show that informative interactions are retained with high probability, and the computational complexity of our procedure is of order $p^\kappa$ for a value of $\kappa$ that can reach values as low as 1 for very sparse data; in many more general settings, it will still beat the exponent $s$ obtained when using a brute force search constrained to order $s$ interactions. In addition, by using some new ideas based on min-wise hash schemes, we are able to further reduce the computational cost. Interactions found by our algorithm can be used for predictive modelling in various forms, but they are also often of interest in their own right as useful characterisations of what distinguishes a certain class from others.},
 author = {Rajen Dinesh Shah and Nicolai Meinshausen},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W4297774857},
 pages = {629--654},
 title = {Random Intersection Trees},
 url = {http://jmlr.org/papers/v15/shah14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:shamir14a,
 abstract = {Trace-norm regularization is a widely-used and successful approach for collaborative filtering and matrix completion. However, previous learning guarantees require strong assumptions, such as a uniform distribution over the matrix entries. In this paper, we bridge this gap by providing such guarantees, under much milder assumptions which correspond to matrix completion as performed in practice. In fact, we claim that previous difficulties partially stemmed from a mismatch between the standard learning-theoretic modeling of matrix completion, and its practical application. Our results also shed some light on the issue of matrix completion with bounded models, which enforce predictions to lie within a certain range. In particular, we provide experimental and theoretical evidence that such models lead to a modest yet significant improvement.},
 author = {Ohad Shamir and Shai Shalev-Shwartz},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W2104404561},
 pages = {3401--3423},
 title = {Matrix completion with the trace norm: learning, bounding, and transducing},
 url = {http://jmlr.org/papers/v15/shamir14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:sheikh14a,
 abstract = {We study inference and learning based on a sparse coding model with 'spike-and-slab' prior. As in standard sparse coding, the model used assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse prior such as a Laplace distribution, we study the application of a more flexible 'spike-and-slab' distribution which models the absence or presence of a source's contribution independently of its strength if it contributes. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: a novel truncated EM approach and, for comparison, an approach based on standard factored variational distributions. The truncated approach can be regarded as a variational approach with truncated posteriors as variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of 'spike-and-slab' priors for the corresponding data domains. Furthermore, we find that the truncated EM approach improves on the standard factored approach in source separation tasks--which hints to biases introduced by assuming posterior independence in the factored variational approach. Likewise, on a standard benchmark for image denoising, we find that the truncated EM approach improves on the factored variational approach. While the performance of the factored approach saturates with increasing numbers of hidden dimensions, the performance of the truncated approach improves the state-of-the-art for higher noise levels.},
 author = {Abdul-Saboor Sheikh and Jacquelyn A. Shelton and J{{\"o}}rg L{{\"u}}cke},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W2141522908},
 pages = {2653--2687},
 title = {A truncated EM approach for spike-and-slab sparse coding},
 url = {http://jmlr.org/papers/v15/sheikh14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:shimizu14a,
 abstract = {Several existing methods have been shown to consistently estimate causal direction assuming linear or some form of nonlinear relationship and no latent confounders. However, the estimation results could be distorted if either assumption is violated. We develop an approach to determining the possible causal direction between two observed variables when latent confounding variables are present. We first propose a new linear non-Gaussian acyclic structural equation model with individual-specific effects that are sometimes the source of confounding. Thus, modeling individual-specific effects as latent variables allows latent confounding to be considered. We then propose an empirical Bayesian approach for estimating possible causal direction using the new model. We demonstrate the effectiveness of our method using artificial and real-world data.},
 author = {Shohei Shimizu and Kenneth Bollen},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W2143372885},
 pages = {2629--2652},
 title = {Bayesian Estimation of Causal Direction in Acyclic Structural Equation Models with Individual-specific Confounder Variables and Non-Gaussian Distributions.},
 url = {http://jmlr.org/papers/v15/shimizu14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:slivkins14a,
 abstract = {In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence of choices. In each round it chooses from a time-invariant set of alternatives and receives the payoff associated with this alternative. While the case of small strategy sets is by now well-understood, a lot of recent work has focused on MAB problems with exponentially or infinitely large strategy sets, where one needs to assume extra structure in order to make the problem tractable. In particular, recent literature considered information on similarity between arms.

We consider similarity information in the setting of contextual bandits, a natural extension of the basic MAB problem where before each round an algorithm is given the context--a hint about the payoffs in this round. Contextual bandits are directly motivated by placing advertisements on web pages, one of the crucial problems in sponsored search. A particularly simple way to represent similarity information in the contextual bandit setting is via a similarity distance between the context-arm pairs which bounds from above the difference between the respective expected payoffs.

Prior work on contextual bandits with similarity uses uniform partitions of the similarity space, so that each context-arm pair is approximated by the closest pair in the partition. Algorithms based on uniform partitions disregard the structure of the payoffs and the context arrivals, which is potentially wasteful. We present algorithms that are based on adaptive partitions, and take advantage of benign payoffs and context arrivals without sacrificing the worst-case performance. The central idea is to maintain a finer partition in high-payoff regions of the similarity space and in popular regions of the context space. Our results apply to several other settings, e.g., MAB with constrained temporal change (Slivkins and Upfal, 2008) and sleeping bandits (Kleinberg et al., 2008a).},
 author = {Aleks and rs Slivkins},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2148434045},
 pages = {2533--2568},
 title = {Contextual bandits with similarity information},
 url = {http://jmlr.org/papers/v15/slivkins14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:sprekeler14a,
 abstract = {We present and test an extension of slow feature analysis as a novel approach to nonlinear blind source separation. The algorithm relies on temporal correlations and iteratively reconstructs a set of statistically independent sources from arbitrary nonlinear instantaneous mixtures. Simulations show that it is able to invert a complicated nonlinear mixture of two audio signals with a high reliability. The algorithm is based on a mathematical analysis of slow feature analysis for the case of input data that are generated from statistically independent sources.},
 author = {Henning Sprekeler and Tiziano Zito and Laurenz Wiskott},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2169314529},
 pages = {921--947},
 title = {An extension of slow feature analysis for nonlinear blind source separation},
 url = {http://jmlr.org/papers/v15/sprekeler14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:srivastava14a,
 abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
 author = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2095705004},
 pages = {1929--1958},
 title = {Dropout: a simple way to prevent neural networks from overfitting},
 url = {http://jmlr.org/papers/v15/srivastava14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:srivastava14b,
 abstract = {A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a unified representation that fuses modalities together. We find that this representation is useful for classification and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and filling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model significantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains.},
 author = {Nitish Srivastava and Ruslan Salakhutdinov},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W2164587673},
 pages = {2949--2980},
 title = {Multimodal Learning with Deep Boltzmann Machines},
 url = {http://jmlr.org/papers/v15/srivastava14b.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:staedler14a,
 abstract = {We propose a novel and efficient algorithm for maximizing the observed log-likelihood of a multivariate normal data matrix with missing values. We show that our procedure, based on iteratively regressing the missing on the observed variables, generalizes the standard EM algorithm by alternating between different complete data spaces and performing the E-Step incrementally. In this non-standard setup we prove numerical convergence to a stationary point of the observed log-likelihood. For high-dimensional data, where the number of variables may greatly exceed sample size, we perform regularization using a Lasso-type penalty. This introduces sparsity in the regression coefficients used for imputation, permits fast computation and warrants competitive performance in terms of estimating the missing entries. We show on simulated and real data that the new method often improves upon other modern imputation techniques such as k-nearest neighbors imputation, nuclear norm minimization or a penalized likelihood approach with an l1-penalty on the concentration matrix.},
 author = {Nicolas St{{\"a}}dler and Daniel J. Stekhoven and Peter B{{\"u}}hlmann},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W2152589362},
 pages = {1903--1928},
 title = {Pattern alternating maximization algorithm for missing data in high-dimensional problems},
 url = {http://jmlr.org/papers/v15/staedler14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:szabo14a,
 abstract = {We present ITE (information theoretical estimators) a free and open source, multi-platform, Matlab/Octave toolbox that is capable of estimating many different variants of entropy, mutual information, divergence, association measures, cross quantities, and kernels on distributions. Thanks to its highly modular design, ITE supports additionally (i) the combinations of the estimation techniques, (ii) the easy construction and embedding of novel information theoretical estimators, and (iii) their immediate application in information theoretical optimization problems. ITE also includes a prototype application in a central problem class of signal processing, independent subspace analysis and its extensions.},
 author = {Zolt{{\'a}}n Szab{{\'o}}},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W1522345630},
 pages = {283--287},
 title = {Information Theoretical Estimators Toolbox},
 url = {http://jmlr.org/papers/v15/szabo14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:tan14a,
 abstract = {In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on Big Data, and then reformulate it as a convex semi-infinite programming (SIP) problem. To address the SIP, we propose an efficient feature generating paradigm. Different from traditional gradient-based approaches that conduct optimization on all input features, the proposed paradigm iteratively activates a group of features, and solves a sequence of multiple kernel learning (MKL) subproblems. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm is guaranteed to converge globally under mild conditions, and can achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures, and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world data sets of tens of million data points with O(1014) features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training effciency.},
 author = {Mingkui Tan and Ivor W. Tsang and Li Wang},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W2131101925},
 pages = {1371--1429},
 title = {Towards ultrahigh dimensional feature selection for big data},
 url = {http://jmlr.org/papers/v15/tan14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:tan14b,
 abstract = {We consider the problem of learning a high-dimensional graphical model in which certain hub nodes are highly-connected to many other nodes. Many authors have studied the use of an l1 penalty in order to learn a sparse graph in high-dimensional setting. However, the l1 penalty implicitly assumes that each edge is equally likely and independent of all other edges. We propose a general framework to accommodate more realistic networks with hub nodes, using a convex formulation that involves a row-column overlap norm penalty. We apply this general framework to three widely-used probabilistic graphical models: the Gaussian graphical model, the covariance graph model, and the binary Ising model. An alternating direction method of multipliers algorithm is used to solve the corresponding convex optimization problems. On synthetic data, we demonstrate that our proposed framework outperforms competitors that do not explicitly model hub nodes. We illustrate our proposal on a webpage data set and a gene expression data set.},
 author = {Kean Ming Tan and Palma London and Karthik Mohan and Su-In Lee and Maryam Fazel and Daniela Witten},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W2951608831},
 pages = {3297--3331},
 title = {Learning Graphical Models With Hubs},
 url = {http://jmlr.org/papers/v15/tan14b.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:tziortziotis14a,
 abstract = {This paper proposes an online tree-based Bayesian approach for reinforcement learning. For inference, we employ a generalised context tree model. This defines a distribution on multivariate Gaussian piecewise-linear models, which can be updated in closed form. The tree structure itself is constructed using the cover tree method, which remains efficient in high dimensional spaces. We combine the model with Thompson sampling and approximate dynamic programming to obtain effective exploration policies in unknown environments. The flexibility and computational simplicity of the model render it suitable for many reinforcement learning problems in continuous state spaces. We demonstrate this in an experimental comparison with a Gaussian process model, a linear model and simple least squares policy iteration.},
 author = {Nikolaos Tziortziotis and Christos Dimitrakakis and Konstantinos Blekas},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W2105474305},
 pages = {2313--2335},
 title = {Cover tree Bayesian reinforcement learning},
 url = {http://jmlr.org/papers/v15/tziortziotis14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:vandenoord14a,
 abstract = {Recent results have shown that Gaussian mixture models (GMMs) are remarkably good at density modeling of natural image patches, especially given their simplicity. In terms of log likelihood on real-valued data they are comparable with the best performing techniques published, easily outperforming more advanced ones, such as deep belief networks. They can be applied to various image processing tasks, such as image denoising, deblurring and inpainting, where they improve on other generic prior methods, such as sparse coding and field of experts. Based on this we propose the use of another, even richer mixture model based image prior: the Student-t mixture model (STM). We demonstrate that it convincingly surpasses GMMs in terms of log likelihood, achieving performance competitive with the state of the art in image patch modeling. We apply both the GMM and STM to the task of lossy and lossless image compression, and propose efficient coding schemes that can easily be extended to other unsupervised machine learning models. Finally, we show that the suggested techniques outperform JPEG, with results comparable to or better than JPEG 2000.},
 author = {A{\"a}ron van den Oord and Benjamin Schrauwen},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W115742922},
 pages = {2061--2086},
 title = {The student-t mixture as a natural image patch prior with application to image compression},
 url = {http://jmlr.org/papers/v15/vandenoord14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:vandermaaten14a,
 abstract = {The paper investigates the acceleration of t-SNE--an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots--using two tree-based algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE embeddings in O(N log N). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.},
 author = {Laurens van der Maaten},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W1875842236},
 pages = {3221--3245},
 title = {Accelerating t-SNE using tree-based algorithms},
 url = {http://jmlr.org/papers/v15/vandermaaten14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:vanlaarhoven14a,
 abstract = {We investigate properties that intuitively ought to be satisfied by graph clustering quality functions, that is, functions that assign a score to a clustering of a graph. Graph clustering, also known as network community detection, is often performed by optimizing such a function. Two axioms tailored for graph clustering quality functions are introduced, and the four axioms introduced in previous work on distance based clustering are reformulated and generalized for the graph setting. We show that modularity, a standard quality function for graph clustering, does not satisfy all of these six properties. This motivates the derivation of a new family of quality functions, adaptive scale modularity, which does satisfy the proposed axioms. Adaptive scale modularity has two parameters, which give greater flexibility in the kinds of clusterings that can be found. Standard graph clustering quality functions, such as normalized cut and unnormalized cut, are obtained as special cases of adaptive scale modularity.

In general, the results of our investigation indicate that the considered axiomatic framework covers existing 'good' quality functions for graph clustering, and can be used to derive an interesting new family of quality functions.},
 author = {Twan van Laarhoven and Elena Marchiori},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2100985998},
 pages = {193--215},
 title = {Axioms for graph clustering quality functions},
 url = {http://jmlr.org/papers/v15/vanlaarhoven14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:vanmoffaert14a,
 abstract = {Many real-world problems involve the optimization of multiple, possibly conflicting objectives. Multi-objective reinforcement learning (MORL) is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, in essence, one for each objective. MORL is the process of learning policies that optimize multiple criteria simultaneously. In this paper, we present a novel temporal difference learning algorithm that integrates the Pareto dominance relation into a reinforcement learning approach. This algorithm is a multi-policy algorithm that learns a set of Pareto dominating policies in a single run. We name this algorithm Pareto Q-learning and it is applicable in episodic environments with deterministic as well as stochastic transition functions. A crucial aspect of Pareto Q-learning is the updating mechanism that bootstraps sets of Q-vectors. One of our main contributions in this paper is a mechanism that separates the expected immediate reward vector from the set of expected future discounted reward vectors. This decomposition allows us to update the sets and to exploit the learned policies consistently throughout the state space. To balance exploration and exploitation during learning, we also propose three set evaluation mechanisms. These three mechanisms evaluate the sets of vectors to accommodate for standard action selection strategies, such as e-greedy. More precisely, these mechanisms use multi-objective evaluation principles such as the hypervolume measure, the cardinality indicator and the Pareto dominance relation to select the most promising actions. We experimentally validate the algorithm on multiple environments with two and three objectives and we demonstrate that Pareto Q-learning outperforms current state-of-the-art MORL algorithms with respect to the hypervolume of the obtained policies. We note that (1) Pareto Q-learning is able to learn the entire Pareto front under the usual assumption that each state-action pair is sufficiently sampled, while (2) not being biased by the shape of the Pareto front. Furthermore, (3) the set evaluation mechanisms provide indicative measures for local action selection and (4) the learned policies can be retrieved throughout the state and action space.},
 author = {Kristof Van Moffaert and Ann Now{{\'e}}},
 journal = {Journal of Machine Learning Research},
 number = {107},
 openalex = {W2186820913},
 pages = {3663--3692},
 title = {Multi-objective reinforcement learning using sets of pareto dominating policies},
 url = {http://jmlr.org/papers/v15/vanmoffaert14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:vats14a,
 abstract = {An undirected graphical model is a joint probability distribution defined on an undirected graph G*, where the vertices in the graph index a collection of random variables and the edges encode conditional independence relationships among random variables. The undirected graphical model selection (UGMS) problem is to estimate the graph G* given observations drawn from the undirected graphical model. This paper proposes a framework for decomposing the UGMS problem into multiple subproblems over clusters and subsets of the separators in a junction tree. The junction tree is constructed using a graph that contains a superset of the edges in G*. We highlight three main properties of using junction trees for UGMS. First, different regularization parameters or different UGMS algorithms can be used to learn different parts of the graph. This is possible since the subproblems we identify can be solved independently of each other. Second, under certain conditions, a junction tree based UGMS algorithm can produce consistent results with fewer observations than the usual requirements of existing algorithms. Third, both our theoretical and experimental results show that the junction tree framework does a significantly better job at finding the weakest edges in a graph than existing methods. This property is a consequence of both the first and second properties. Finally, we note that our framework is independent of the choice of the UGMS algorithm and can be used as a wrapper around standard UGMS algorithms for more accurate graph estimation.},
 author = {Divyanshu Vats and Robert D. Nowak},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2162672359},
 pages = {147--191},
 title = {A junction tree framework for undirected graphical model selection},
 url = {http://jmlr.org/papers/v15/vats14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:volkovs14a,
 abstract = {In this paper we present a general treatment of the preference aggregation problem, in which multiple preferences over objects must be combined into a single consensus ranking. We consider two instances of this problem: unsupervised aggregation where no information about a target ranking is available, and supervised aggregation where ground truth preferences are provided. For each problem class we develop novel learning methods that are applicable to a wide range of preference types. Specifically, for unsupervised aggregation we introduce the Multinomial Preference model (MPM) which uses a multinomial generative process to model the observed preferences. For the supervised problem we develop a supervised extension for MPM and then propose two fully supervised models. The first model employs SVD factorization to derive effective item features, transforming the aggregation problems into a learning-to-rank one. The second model aims to eliminate the costly SVD factorization and instantiates a probabilistic CRF framework, deriving unary and pairwise potentials directly from the observed preferences. Using a probabilistic framework allows us to directly optimize the expectation of any target metric, such as NDCG or ERR. All the proposed models operate on pairwise preferences and can thus be applied to a wide range of preference types. We empirically validate the models on rank aggregation and collaborative filtering data sets and demonstrate superior empirical accuracy.},
 author = {Maksims N. Volkovs and Richard S. Zemel},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W2103850933},
 pages = {1135--1176},
 title = {New learning methods for supervised and unsupervised preference aggregation},
 url = {http://jmlr.org/papers/v15/volkovs14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:vonluxburg14a,
 abstract = {In machine learning, a popular tool to analyze the structure of graphs is the hitting time and the commute distance (resistance distance). For two vertices u and v, the hitting time Huv is the expected time it takes a random walk to travel from u to v. The commute distance is its symmetrized version Cuv = Huv +Hvu. In our paper we study the behavior of hitting times and commute distances when the number n of vertices in the graph tends to infinity. We focus on random geometric graphs (e-graphs, kNN graphs and Gaussian similarity graphs), but our results also extend to graphs with a given expected degree distribution or Erdos-Renyi graphs with planted partitions. We prove that in these graph families, the suitably rescaled hitting time Huv converges to 1/dv and the rescaled commute time to 1/du+1=dv where du and dv denote the degrees of vertices u and v. In these cases, hitting and commute times do not provide information about the structure of the graph, and their use is discouraged in many machine learning applications.},
 author = {Ulrike von Luxburg and Agnes Radl and Matthias Hein},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2143724577},
 pages = {1751--1798},
 title = {Hitting and commute times in large random neighborhood graphs},
 url = {http://jmlr.org/papers/v15/vonluxburg14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:wade14a,
 abstract = {Flexible covariate-dependent density estimation can be achieved by modelling the joint density of the response and covariates as a Dirichlet process mixture. An appealing aspect of this approach is that computations are relatively easy. In this paper, we examine the predictive performance of these models with an increasing number of covariates. Even for a moderate number of covariates, we find that the likelihood for x tends to dominate the posterior of the latent random partition, degrading the predictive performance of the model. To overcome this, we suggest using a different nonparametric prior, namely an enriched Dirichlet process. Our proposal maintains a simple allocation rule, so that computations remain relatively simple. Advantages are shown through both predictive equations and examples, including an application to diagnosis Alzheimer's disease.},
 author = {Sara Wade and David B. Dunson and Sonia Petrone and Lorenzo Trippa},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W2128823902},
 pages = {1041--1071},
 title = {Improving prediction from dirichlet process mixtures via enrichment},
 url = {http://jmlr.org/papers/v15/wade14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:waegeman14a,
 abstract = {The F-measure, which has originally been introduced in information retrieval, is nowadays routinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction. Optimizing this measure is a statistically and computationally challenging problem, since no closed-form solution exists. Adopting a decision-theoretic perspective, this article provides a formal and experimental analysis of different approaches for maximizing the F-measure. We start with a Bayes-risk analysis of related loss functions, such as Hamming loss and subset zero-one loss, showing that optimizing such losses as a surrogate of the F-measure leads to a high worst-case regret. Subsequently, we perform a similar type of analysis for F-measure maximizing algorithms, showing that such algorithms are approximate, while relying on additional assumptions regarding the statistical distribution of the binary response variables. Furthermore, we present a new algorithm which is not only computationally efficient but also Bayes-optimal, regardless of the underlying distribution. To this end, the algorithm requires only a quadratic (with respect to the number of binary responses) number of parameters of the joint distribution. We illustrate the practical performance of all analyzed methods by means of experiments with multi-label classification problems.},
 author = {Willem Waegeman and Krzysztof Dembczy{\'n}ski and Arkadiusz Jachnik and Weiwei Cheng and Eyke H{{\"u}}llermeier},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W1864426711},
 pages = {3513--3568},
 title = {On the bayes-optimality of F-measure maximizers},
 url = {http://jmlr.org/papers/v15/waegeman14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:wager14a,
 abstract = {We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2012) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B on the order of n^{1.5} bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B on the order of n replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies.},
 author = {Stefan Wager and Trevor Hastie and Bradley Efron},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W4302799641},
 pages = {1625--1651},
 title = {Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife},
 url = {http://jmlr.org/papers/v15/wager14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:wand14a,
 abstract = {Fully simplified expressions for Multivariate Normal updates in non-conjugate variational message passing approximate inference schemes are obtained. The simplicity of these expressions means that the updates can be achieved very effciently. Since the Multivariate Normal family is the most common for approximating the joint posterior density function of a continuous parameter vector, these fully simplified updates are of great practical benefit.},
 author = {Matt P. W and },
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W2109848216},
 pages = {1351--1369},
 title = {Fully simplified multivariate normal updates in non-conjugate variational message passing},
 url = {http://jmlr.org/papers/v15/wand14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:wang14a,
 abstract = {In many machine learning problems such as the dual form of SVM, the objective function to be minimized is convex but not strongly convex. This fact causes difficulties in obtaining the complexity of some commonly used optimization algorithms. In this paper, we proved the global linear convergence on a wide range of algorithms when they are applied to some non-strongly convex problems. In particular, we are the first to prove O(log(1/e)) time complexity of cyclic coordinate descent methods on dual problems of support vector classification and regression.},
 author = {Po-Wei Wang and Chih-Jen Lin},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W2117669906},
 pages = {1523--1548},
 title = {Iteration complexity of feasible descent methods for convex optimization},
 url = {http://jmlr.org/papers/v15/wang14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:wang14b,
 author = {Zhan Wang and Sandra Paterlini and Fuchang Gao and Yuhong Yang},
 journal = {Journal of Machine Learning Research},
 number = {50},
 pages = {1675--1711},
 title = {Adaptive Minimax Regression Estimation over Sparse $\ell_q$-Hulls},
 url = {http://jmlr.org/papers/v15/wang14b.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:wierstra14a,
 abstract = {This paper presents natural evolution strategies (NES), a novel algorithm for performing real-valued dasiablack boxpsila function optimization: optimizing an unknown objective function where algorithm-selected function measurements constitute the only information accessible to the method. Natural evolution strategies search the fitness landscape using a multivariate normal distribution with a self-adapting mutation matrix to generate correlated mutations in promising regions. NES shares this property with covariance matrix adaption (CMA), an evolution strategy (ES) which has been shown to perform well on a variety of high-precision optimization tasks. The natural evolution strategies algorithm, however, is simpler, less ad-hoc and more principled. Self-adaptation of the mutation matrix is derived using a Monte Carlo estimate of the natural gradient towards better expected fitness. By following the natural gradient instead of the dasiavanillapsila gradient, we can ensure efficient update steps while preventing early convergence due to overly greedy updates, resulting in reduced sensitivity to local suboptima. We show NES has competitive performance with CMA on unimodal tasks, while outperforming it on several multimodal tasks that are rich in deceptive local optima.},
 author = {Daan Wierstra and Tom Schaul and Tobias Glasmachers and Yi Sun and Jan Peters and J\"{u}rgen Schmidhuber},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W2166160300},
 pages = {949--980},
 title = {Natural Evolution Strategies},
 url = {http://jmlr.org/papers/v15/wierstra14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:wilson14a,
 abstract = {Recently, Bayesian Optimization (BO) has been used to successfully optimize parametric policies in several challenging Reinforcement Learning (RL) applications. BO is attractive for this problem because it exploits Bayesian prior information about the expected return and exploits this knowledge to select new policies to execute. Effectively, the BO framework for policy search addresses the exploration-exploitation tradeoff. In this work, we show how to more effectively apply BO to RL by exploiting the sequential trajectory information generated by RL agents. Our contributions can be broken into two distinct, but mutually beneficial, parts. The first is a new Gaussian process (GP) kernel for measuring the similarity between policies using trajectory data generated from policy executions. This kernel can be used in order to improve posterior estimates of the expected return thereby improving the quality of exploration. The second contribution, is a new GP mean function which uses learned transition and reward functions to approximate the surface of the objective. We show that the model-based approach we develop can recover from model inaccuracies when good transition and reward models cannot be learned. We give empirical results in a standard set of RL benchmarks showing that both our model-based and model-free approaches can speed up learning compared to competing methods. Further, we show that our contributions can be combined to yield synergistic improvement in some domains.},
 author = {Aaron Wilson and Alan Fern and Prasad Tadepalli},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W2145957964},
 pages = {253--282},
 title = {Using trajectory data to improve bayesian optimization for reinforcement learning},
 url = {http://jmlr.org/papers/v15/wilson14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:wipf14a,
 abstract = {Blind deconvolution involves the estimation of a sharp signal or image given only a blurry observation. Because this problem is fundamentally ill-posed, strong priors on both the sharp image and blur kernel are required to regularize the solution space. While this naturally leads to a standard MAP estimation framework, performance is compromised by unknown trade-off parameter settings, optimization heuristics, and convergence issues stemming from non-convexity and/or poor prior selections. To mitigate some of these problems, a number of authors have recently proposed substituting a variational Bayesian (VB) strategy that marginalizes over the high-dimensional image space leading to better estimates of the blur kernel. However, the underlying cost function now involves both integrals with no closed-form solution and complex, function-valued arguments, thus losing the transparency of MAP. Beyond standard Bayesian-inspired intuitions, it thus remains unclear by exactly what mechanism these methods are able to operate, rendering understanding, improvements and extensions more difficult. To elucidate these issues, we demonstrate that the VB methodology can be recast as an unconventional MAP problem with a very particular penalty/prior that couples the image, blur kernel, and noise level in a principled way. This unique penalty has a number of useful characteristics pertaining to relative concavity, local minima avoidance, and scale-invariance that allow us to rigorously explain the success of VB including its existing implementational heuristics and approximations. It also provides strict criteria for choosing the optimal image prior that, perhaps counter-intuitively, need not reflect the statistics of natural scenes. In so doing we challenge the prevailing notion of why VB is successful for blind deconvolution while providing a transparent platform for introducing enhancements.},
 author = {David Wipf and Haichao Zhang},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W2953310097},
 pages = {3775--3814},
 title = {Revisiting Bayesian Blind Deconvolution},
 url = {http://jmlr.org/papers/v15/wipf14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:wu14a,
 abstract = {With the development of data acquisition equipment, more and more modalities become available for gesture recognition. However, there still exist two critical issues for multimodal gesture recognit...},
 author = {Jiaxiang Wu and Jian Cheng},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W3004734689},
 pages = {3013--3036},
 title = {Bayesian co-boosting for multi-modal gesture recognition},
 url = {http://jmlr.org/papers/v15/wu14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:xu14a,
 abstract = {Machine learning algorithms have successfully entered industry through many real-world applications (e.g., search engines and product recommendations). In these applications, the test-time CPU cost must be budgeted and accounted for. In this paper, we examine two main components of the test-time CPU cost, classifier evaluation cost and feature extraction cost, and show how to balance these costs with the classifier accuracy. Since the computation required for feature extraction dominates the test-time cost of a classifier in these settings, we develop two algorithms to efficiently balance the performance with the test-time cost. Our first contribution describes how to construct and optimize a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. Our second contribution is a natural reduction of the tree of classifiers into a cascade. The cascade is particularly useful for class-imbalanced data sets as the majority of instances can be early-exited out of the cascade when the algorithm is sufficiently confident in its prediction. Because both approaches only compute features for inputs that benefit from them the most, we find our trained classifiers lead to high accuracies at a small fraction of the computational cost.},
 author = {Zhixiang (Eddie) Xu and Matt J. Kusner and Kilian Q. Weinberger and Minmin Chen and Olivier Chapelle},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W2128466927},
 pages = {2113--2144},
 title = {Classifier cascades and trees for minimizing feature evaluation cost},
 url = {http://jmlr.org/papers/v15/xu14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:yamazaki14a,
 abstract = {Hierarchical statistical models are widely employed in information science and data engineering. The models consist of two types of variables: observable variables that represent the given data and latent variables for the unobservable labels. An asymptotic analysis of the models plays an important role in evaluating the learning process; the result of the analysis is applied not only to theoretical but also to practical situations, such as optimal model selection and active learning. There are many studies of generalization errors, which measure the prediction accuracy of the observable variables. However, the accuracy of estimating the latent variables has not yet been elucidated. For a quantitative evaluation of this, the present paper formulates distribution-based functions for the errors in the estimation of the latent variables. The asymptotic behavior is analyzed for both the maximum likelihood and the Bayes methods.},
 author = {Keisuke Yamazaki},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W2141674052},
 pages = {3721--3742},
 title = {Asymptotic accuracy of distribution-based estimation of latent variables},
 url = {http://jmlr.org/papers/v15/yamazaki14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:zhang14a,
 abstract = {We study the basic problem of robust subspace recovery. That is, we assume a data set that some of its points are sampled around a fixed subspace and the rest of them are spread in the whole ambient space, and we aim to recover the fixed underlying subspace. We first estimate "robust inverse sample covariance" by solving a convex minimization procedure; we then recover the subspace by the bottom eigenvectors of this matrix (their number correspond to the number of eigenvalues close to 0). We guarantee exact subspace recovery under some conditions on the underlying data. Furthermore, we propose a fast iterative algorithm, which linearly converges to the matrix minimizing the convex problem. We also quantify the effect of noise and regularization and discuss many other practical and theoretical issues for improving the subspace recovery in various settings. When replacing the sum of terms in the convex energy function (that we minimize) with the sum of squares of terms, we obtain that the new minimizer is a scaled version of the inverse sample covariance (when exists). We thus interpret our minimizer and its subspace (spanned by its bottom eigenvectors) as robust versions of the empirical inverse covariance and the PCA subspace respectively. We compare our method with many other algorithms for robust PCA on synthetic and real data sets and demonstrate state-of-the-art speed and accuracy.},
 author = {Teng Zhang and Gilad Lerman},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2099953425},
 pages = {749--808},
 title = {A Novel M-Estimator for Robust PCA},
 url = {http://jmlr.org/papers/v15/zhang14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:zhu14a,
 abstract = {Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max-margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the augment-and-collapse Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time effciency. The classification performance is also improved over competitors on binary, multi-class and multi-label classification tasks.},
 author = {Jun Zhu and Ning Chen and Hugh Perkins and Bo Zhang},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W2100931213},
 pages = {1073--1110},
 title = {Gibbs max-margin topic models with data augmentation},
 url = {http://jmlr.org/papers/v15/zhu14a.html},
 volume = {15},
 year = {2014}
}

@article{JMLR:v15:zhu14b,
 abstract = {Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark data sets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community.},
 author = {Jun Zhu and Ning Chen and Eric P. Xing},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W2963641944},
 pages = {1799--1847},
 title = {Bayesian inference with posterior regularization and applications to infinite latent SVMs},
 url = {http://jmlr.org/papers/v15/zhu14b.html},
 volume = {15},
 year = {2014}
}
