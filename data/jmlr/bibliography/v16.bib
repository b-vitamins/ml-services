@article{JMLR:v16:addarioberry15a,
 abstract = {In this paper we explore maximal deviations of large random structures from their typical behavior. We introduce a model for a high-dimensional random graph process and ask analogous questions to those of Vapnik and Chervonenkis for deviations of averages: how "rich" does the process have to be so that one sees atypical behavior. In particular, we study a natural process of Erdős-Rényi random graphs indexed by unit vectors in $\mathbb{R}^d$. We investigate the deviations of the process with respect to three fundamental properties: clique number, chromatic number, and connectivity. In all cases we establish upper and lower bounds for the minimal dimension $d$ that guarantees the existence of "exceptional directions" in which the random graph behaves atypically with respect to the property. For each of the three properties, four theorems are established, to describe upper and lower bounds for the threshold dimension in the subcritical and supercritical regimes.},
 author = {Louigi Addario-Berry and Shankar Bhamidi and S{{\'e}}bastien Bubeck and Luc Devroye and G{{\'a}}bor Lugosi and Roberto Imbuzeiro Oliveira},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W1880169967},
 pages = {1893--1922},
 title = {Exceptional rotations of random graphs: a VC theory},
 url = {http://jmlr.org/papers/v16/addarioberry15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:ailon15a,
 abstract = {This paper investigates graph clustering under the planted partition model in the presence of small clusters. Traditional results dictate that for an algorithm to provably correctly recover the underlying clusters, all clusters must be sufficiently large--in particular, the cluster sizes need to be Ω(√n), where n is the number of nodes of the graph. We show that this is not really a restriction: by a refined analysis of a convex-optimization-based recovery approach, we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to provably recover almost all clusters via a strategy: we recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often after large clusters are learned (and removed). We expect that the idea of iterative peeling--that is, sequentially identifying a subset of the clusters and reducing the problem to a smaller one|is useful more broadly beyond the specific implementations (based on convex optimization) used in this paper.},
 author = {Nir Ailon and Yudong Chen and Huan Xu},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W225218192},
 pages = {455--490},
 title = {Iterative and active graph clustering using trace norm minimization without cluster size constraints},
 url = {http://jmlr.org/papers/v16/ailon15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:anandkumar15a,
 abstract = {Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. Our sufficient conditions for identifiability involve a novel set of expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allow for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition.},
 author = {Animashree An and kumar and Daniel Hsu and Majid Janzamin and Sham Kakade},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W2149375393},
 pages = {2643--2694},
 title = {When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity},
 url = {http://jmlr.org/papers/v16/anandkumar15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:aragam15a,
 abstract = {We develop a penalized likelihood estimation framework to estimate the structure of Gaussian Bayesian networks from observational data. In contrast to recent methods which accelerate the learning problem by restricting the search space, our main contribution is a fast algorithm for score-based structure learning which does not restrict the search space in any way and works on high-dimensional datasets with thousands of variables. Our use of concave regularization, as opposed to the more popular $\ell_0$ (e.g. BIC) penalty, is new. Moreover, we provide theoretical guarantees which generalize existing asymptotic results when the underlying distribution is Gaussian. Most notably, our framework does not require the existence of a so-called faithful DAG representation, and as a result the theory must handle the inherent nonidentifiability of the estimation problem in a novel way. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the R language. Based on these experiments, we show that our algorithm is significantly faster than other competing methods while obtaining higher sensitivity with comparable false discovery rates for high-dimensional data. In particular, the total runtime for our method to generate a solution path of 20 estimates for DAGs with 8000 nodes is around one hour.},
 author = {Bryon Aragam and Qing Zhou},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W2963581809},
 pages = {2273--2328},
 title = {Concave Penalized Estimation of Sparse Gaussian Bayesian Networks},
 url = {http://jmlr.org/papers/v16/aragam15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:barbero15a,
 abstract = {In this work we address the Ev-SVM model proposed by Perez-Cruz et al. as an extension of the traditional v support vector classification model (v-SVM). Through an enhancement of the range of admissible values for the regularization parameter v, the Ev-SVM has been shown to be able to produce a wider variety of decision functions, giving rise to a better adaptability to the data. However, while a clear and intuitive geometric interpretation can be given for the v-SVM model as a nearest-point problem in reduced convex hulls (RCH-NPP), no previous work has been made in developing such intuition for the Ev-SVM model. In this paper we show how Ev-SVM can be reformulated as a geometrical problem that generalizes RCH-NPP, providing new insights into this model. Under this novel point of view, we propose the RapMinos algorithm, able to solve Ev-SVM more efficiently than the current methods. Furthermore, we show how RapMinos is able to address the Ev-SVM model for any choice of regularization norm lp ≥1 seamlessly, which further extends the SVM model flexibility beyond the usual Ev-SVM models.},
 author = {{{\'A}}lvaro Barbero and Akiko Takeda and Jorge L{{\'o}}pez},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W2125795421},
 pages = {323--369},
 title = {Geometric intuition and algorithms for Ev-SVM},
 url = {http://jmlr.org/papers/v16/barbero15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:basu15a,
 abstract = {The problem of estimating high-dimensional network models arises naturally in the analysis of many physical, biological and socio-economic systems. Examples include stock price fluctuations in financial markets and gene regulatory networks representing effects of regulators (transcription factors) on regulated genes in genetics. We aim to learn the structure of the network over time employing the framework of Granger causal models under the assumptions of sparsity of its edges and inherent grouping structure among its nodes. We introduce a thresholded variant of the Group Lasso estimator for discovering Granger causal interactions among the nodes of the network. Asymptotic results on the consistency of the new estimation procedure are developed. The performance of the proposed methodology is assessed through an extensive set of simulation studies and comparisons with existing techniques.},
 author = {Sumanta Basu and Ali Shojaie and George Michailidis},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W2952309330},
 pages = {417--453},
 title = {Network Granger Causality with Inherent Grouping Structure},
 url = {http://jmlr.org/papers/v16/basu15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:bellec15a,
 abstract = {We derive oracle inequalities for the problems of isotonic and convex regression using the combination of Q-aggregation procedure and sparsity pattern aggregation. This improves upon the previous results including the oracle inequalities for the constrained least squares estimator. One of the improvements is that our oracle inequalities are sharp, i.e., with leading constant 1. It allows us to obtain bounds for the minimax regret thus accounting for model misspecification, which was not possible based on the previous results. Another improvement is that we obtain oracle inequalities both with high probability and in expectation.},
 author = {Pierre C. Bellec and Alexandre B. Tsybakov},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2962703606},
 pages = {1879--1892},
 title = {Sharp oracle bounds for monotone and convex regression through aggregation},
 url = {http://jmlr.org/papers/v16/bellec15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:berend15a,
 abstract = {We revisit, from a statistical learning perspective, the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Naive Bayes weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. We derive optimality results for our estimates and also establish some structural characterizations. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. Several challenging open problems are posed, and experimental results are provided to illustrate the theory.},
 author = {Daniel Berend and Aryeh Kontorovich},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W1930161385},
 pages = {1519--1545},
 title = {A finite sample analysis of the Naive Bayes classifier},
 url = {http://jmlr.org/papers/v16/berend15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:bernstein15a,
 abstract = {Blackwell's theory of approachability provides fundamental results for repeated games with vector-valued payoffs, which have been usefully applied in the theory of learning in games, and in devising online learning algorithms in the adversarial setup. A target set S is approachable by a player (the agent) in such a game if he can ensure that the average payoff vector converges to S, no matter what the opponent does. Blackwell provided two equivalent conditions for a convex set to be approachable. Standard approachability algorithms rely on the primal condition, which is a geometric separation condition, and essentially require to compute at each stage a projection direction from a certain point to S. Here we introduce an approachability algorithm that relies on Blackwell's dual condition, which requires the agent to have a feasible response to each mixed action of the opponent, namely a mixed action such that the expected payoff vector belongs to S. Thus, rather than projections, the proposed algorithm relies on computing the response to a certain action of the opponent at each stage. We demonstrate the utility of the proposed approach by applying it to certain generalizations of the classical regret minimization problem, which incorporate side constraints, reward-to-cost criteria, and so-called global cost functions. In these extensions, computation of the projection is generally complex while the response is readily obtainable.},
 author = {Andrey Bernstein and Nahum Shimkin},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W2166616772},
 pages = {747--773},
 title = {Response-based approachability with applications to generalized no-regret problems},
 url = {http://jmlr.org/papers/v16/bernstein15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:bontempi15a,
 abstract = {The relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference. Recent results in the ChaLearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in Markov indistinguishable configurations thanks to data driven approaches. This paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with $n>2$ variables. The approach relies on the asymmetry of some conditional (in)dependence relations between the members of the Markov blankets of two variables causally connected. Our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for $n>2$ variate distributions.},
 author = {Gianluca Bontempi and Maxime Flauder},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W4301130270},
 pages = {2437--2457},
 title = {From dependency to causality: a machine learning approach},
 url = {http://jmlr.org/papers/v16/bontempi15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:bubenik15a,
 abstract = {We define a new topological summary for data that we call the persistence landscape. Since this summary lies in a vector space, it is easy to combine with tools from statistics and machine learning...},
 author = {Peter Bubenik},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W3015234645},
 pages = {77--102},
 title = {Statistical topological data analysis using persistence landscapes},
 url = {http://jmlr.org/papers/v16/bubenik15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:cano15a,
 abstract = {JCLEC-Classification is a usable and extensible open source library for genetic programming classification algorithms. It houses implementations of rule-based methods for classification based on genetic programming, supporting multiple model representations and providing to users the tools to implement any classifier easily. The software is written in Java and it is available from http://jclec.sourceforge.net/classification under the GPL license.},
 author = {Alberto Cano and Jos{{\'e}} Mar{{\'i}}a Luna and Amelia Zafra and Sebasti{{\'a}}n Ventura},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2100683190},
 pages = {491--494},
 title = {A classification module for genetic programming algorithms in JCLEC},
 url = {http://jmlr.org/papers/v16/cano15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:carpentier15a,
 abstract = {We consider the problem of stratified sampling for Monte Carlo integration of a random variable. We model this problem in a K-armed bandit, where the arms represent the K strata. The goal is to estimate the integral mean, that is a weighted average of the mean values of the arms. The learner is allowed to sample the variable n times, but it can decide on-line which stratum to sample next. We propose an UCB-type strategy that samples the arms according to an upper bound on their estimated standard deviations. We compare its performance to an ideal sample allocation that knows the standard deviations of the arms. For sub-Gaussian arm distributions, we provide bounds on the total regret: a distribution-dependent bound of order poly(λmin-1)O(n-3/2)1 that depends on a measure of the disparity λmin of the per stratum variances and a distribution-free bound poly(K)O(n-7/6) that does not. We give similar, but somewhat sharper bounds on a proxy of the regret. The problem-independent bound for this proxy matches its recent minimax lower bound in terms of n up to a log n factor.},
 author = {Alexandra Carpentier and Remi Munos and Andr{{\'a}}s Antos},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2182034283},
 pages = {2231--2271},
 title = {Adaptive strategy for stratified Monte Carlo sampling},
 url = {http://jmlr.org/papers/v16/carpentier15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:chazal15a,
 abstract = {Computational topology has recently seen an important development toward data analysis, giving birth to Topological Data Analysis. Persistent homology appears as a fundamental tool in this field. We show that the use of persistent homology can be naturally considered in general statistical frameworks . We establish convergence rates of persistence diagrams associated to data randomly sampled from any compact metric space to a well defined limit diagram encoding the topological features of the support of the measure from which the data have been sampled. Our approach relies on a recent and deep stability result for persistence that allows to relate our problem to support estimation problems (with respect to the Gromov-Hausdorff distance). Some numerical experiments are performed in various contexts to illustrate our results.},
 author = {Fr{{\'e}}d{{\'e}}ric Chazal and Marc Glisse and Catherine Labru{{\`e}}re and Bertrand Michel},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W2100117429},
 pages = {3603--3635},
 title = {Convergence Rates for Persistence Diagram Estimation in Topological Data Analysis},
 url = {http://jmlr.org/papers/v16/chazal15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:chen15a,
 abstract = {In crowd labeling, a large amount of unlabeled data instances are outsourced to a crowd of workers. Workers will be paid for each label they provide, but the labeling requester usually has only a limited amount of the budget. Since data instances have different levels of labeling difficulty and workers have different reliability, it is desirable to have an optimal policy to allocate the budget among all instance-worker pairs such that the overall labeling accuracy is maximized. We consider categorical labeling tasks and formulate the budget allocation problem as a Bayesian Markov decision process (MDP), which simultaneously conducts learning and decision making. Using the dynamic programming (DP) recurrence, one can obtain the optimal allocation policy. However, DP quickly becomes computationally intractable when the size of the problem increases. To solve this challenge, we propose a computationally efficient approximate policy, called optimistic knowledge gradient policy. Our MDP is a quite general framework, which applies to both pull crowdsourcing marketplaces with homogeneous workers and push marketplaces with heterogeneous workers. It can also incorporate the contextual information of instances when they are available. The experiments on both simulated and real data show that the proposed policy achieves a higher labeling accuracy than other existing policies at the same budget level.},
 author = {Xi Chen and Qihang Lin and Dengyong Zhou},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2104759087},
 pages = {1--46},
 title = {Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling},
 url = {http://jmlr.org/papers/v16/chen15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:chen15b,
 abstract = {Matrix completion, i.e., the exact and provable recovery of a low-rank matrix from a small subset of its elements, is currently only known to be possible if the matrix satisfies a restrictive struc...},
 author = {Yudong Chen and Srinadh Bhojanapalli and Sujay Sanghavi and Rachel Ward},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W3015031642},
 pages = {2999--3034},
 title = {Completing any low-rank matrix, provably},
 url = {http://jmlr.org/papers/v16/chen15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:chen15c,
 abstract = {Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. They have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized Stacked Linear Denoising Autoencoder (mSLDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSLDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters -- in fact, the linear formulation gives rise to a closed-form solution. Consequently, mSLDA, which can be implemented in only 20 lines of MATLAB™, is about two orders of magnitude faster than a corresponding SDA. Furthermore, the representations learnt by mSLDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.},
 author = {Minmin Chen and Kilian Q. Weinberger and Zhixiang (Eddie) Xu and Fei Sha},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W2281948289},
 pages = {3849--3875},
 title = {Marginalizing stacked linear denoising autoencoders},
 url = {http://jmlr.org/papers/v16/chen15c.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:cunningham15a,
 abstract = {Linear dimensionality reduction methods are a cornerstone of analyzing high dimensional data, due to their simple geometric interpretations and typically attractive computational properties. These methods capture many data features of interest, such as covariance, dynamical structure, correlation between data sets, input-output relationships, and margin between data classes. Methods have been developed with a variety of names and motivations in many fields, and perhaps as a result the connections between all these methods have not been highlighted. Here we survey methods from this disparate literature as optimization programs over matrix manifolds. We discuss principal component analysis, factor analysis, linear multidimensional scaling, Fisher's linear discriminant analysis, canonical correlations analysis, maximum autocorrelation factors, slow feature analysis, sufficient dimensionality reduction, undercomplete independent component analysis, linear regression, distance metric learning, and more. This optimization framework gives insight to some rarely discussed shortcomings of well-known methods, such as the suboptimality of certain eigenvector solutions. Modern techniques for optimization over matrix manifolds enable a generic linear dimensionality reduction solver, which accepts as input data and an objective to be optimized, and returns, as output, an optimal low-dimensional projection of the data. This simple optimization framework further allows straightforward generalizations and novel variants of classical methods, which we demonstrate here by creating an orthogonal-projection canonical correlations analysis. More broadly, this survey and generic solver suggest that linear dimensionality reduction can move toward becoming a blackbox, objective-agnostic numerical technology.},
 author = {John P. Cunningham and Zoubin Ghahramani},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W4297817021},
 pages = {2859--2900},
 title = {Linear Dimensionality Reduction: Survey, Insights, and Generalizations},
 url = {http://jmlr.org/papers/v16/cunningham15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:curtin15a,
 abstract = {Numerous machine learning algorithms contain pairwise statistical problems at their core---that is, tasks that require computations over all pairs of input points if implemented naively. Often, tree structures are used to solve these problems efficiently. Dual-tree algorithms can efficiently solve or approximate many of these problems. Using cover trees, rigorous worst-case runtime guarantees have been proven for some of these algorithms. In this paper, we present a problem-independent runtime guarantee for any dual-tree algorithm using the cover tree, separating out the problem-dependent and the problem-independent elements. This allows us to just plug in bounds for the problem-dependent elements to get runtime guarantees for dual-tree algorithms for any pairwise statistical problem without re-deriving the entire proof. We demonstrate this plug-and-play procedure for nearest-neighbor search and approximate kernel density estimation to get improved runtime guarantees. Under mild assumptions, we also present the first linear runtime guarantee for dual-tree based range search.},
 author = {Ryan R. Curtin and Dongryeol Lee and William B. March and Parikshit Ram},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W4293482517},
 pages = {3269--3297},
 title = {Plug-and-play dual-tree algorithm runtime analysis},
 url = {http://jmlr.org/papers/v16/curtin15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:daniely15a,
 abstract = {We study the sample complexity of multiclass prediction in several learning settings. For the PAC setting our analysis reveals a surprising phenomenon: In sharp contrast to binary classification, we show that there exist multiclass hypothesis classes for which some Empirical Risk Minimizers (ERM learners) have lower sample complexity than others. Furthermore, there are classes that are learnable by some ERM learners, while other ERM learners will fail to learn them. We propose a principle for designing good ERM learners, and use this principle to prove tight bounds on the sample complexity of learning symmetric multiclass hypothesis classes--classes that are invariant under permutations of label names. We further provide a characterization of mistake and regret bounds for multiclass learning in the online setting and the bandit setting, using new generalizations of Littlestone's dimension.},
 author = {Amit Daniely and Sivan Sabato and Shai Ben-David and Shai Shalev-Shwartz},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W2402090481},
 pages = {2377--2404},
 title = {Multiclass Learnability and the ERM principle.},
 url = {http://jmlr.org/papers/v16/daniely15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:dasilva15a,
 abstract = {Parallel Graphics Processing Unit (GPU) implementations of GP have appeared in the literature using three main methodologies: (i) compilation, which generates the individuals in GPU code and requires compilation; (ii) pseudo-assembly, which generates the individuals in an intermediary assembly code and also requires compilation; and (iii) interpretation, which interprets the codes. This paper proposes a new methodology that uses the concepts of quantum computing and directly handles the GPU machine code instructions. Our methodology utilizes a probabilistic representation of an individual to improve the global search capability. In addition, the evolution in machine code eliminates both the overhead of compiling the code and the cost of parsing the program during evaluation. We obtained up to 2.74 trillion GP operations per second for the 20-bit Boolean Multiplexer benchmark. We also compared our approach with the other three GPU-based acceleration methodologies implemented for quantum-inspired linear GP. Significant gains in performance were obtained.},
 author = {Cleomar Pereira da Silva and Douglas Mota Dias and Cristiana Bentes and Marco Aur{{\'e}}lio Cavalcanti Pacheco and Le and ro Fontoura Cupertino},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W619811882},
 pages = {673--712},
 title = {Evolving GPU Machine Code},
 url = {http://jmlr.org/papers/v16/dasilva15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:dhillon15a,
 abstract = {Spectral learning algorithms have recently become popular in data-rich domains, driven in part by recent advances in large scale randomized SVD, and in spectral estimation of Hidden Markov Models. Extensions of these methods lead to statistical estimation algorithms which are not only fast, scalable, and useful on real data sets, but are also provably correct. Following this line of research, we propose four fast and scalable spectral algorithms for learning word embeddings -- low dimensional real vectors (called Eigenwords) that capture the meaning of words from their context. All the proposed algorithms harness the multi-view nature of text data i.e. the left and right context of each word, are fast to train and have strong theoretical properties. Some of the variants also have lower sample complexity and hence higher statistical power for rare words. We provide theory which establishes relationships between these algorithms and optimality criteria for the estimates they provide. We also perform thorough qualitative and quantitative evaluation of Eigenwords showing that simple linear approaches give performance comparable to or superior than the state-of-the-art non-linear deep learning based methods.},
 author = {Paramveer S. Dhillon and Dean P.  Foster and Lyle H. Ungar},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W2183945364},
 pages = {3035--3078},
 title = {Eigenwords: spectral word embeddings},
 url = {http://jmlr.org/papers/v16/dhillon15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:fearnley15a,
 abstract = {A recent body of experimental literature has studied empirical game-theoretical analysis, in which we have partial knowledge of a game, consisting of observations of a subset of the pure-strategy profiles and their associated payoffs to players. The aim is to find an exact or approximate Nash equilibrium of the game, based on these observations. It is usually assumed that the strategy profiles may be chosen in an on-line manner by the algorithm. We study a corresponding computational learning model, and the query complexity of learning equilibria for various classes of games. We give basic results for bimatrix and graphical games. Our focus is on symmetric network congestion games. For directed acyclic networks, we can learn the cost functions (and hence compute an equilibrium) while querying just a small fraction of pure-strategy profiles. For the special case of parallel links, we have the stronger result that an equilibrium can be identified while only learning a small fraction of the cost values.},
 author = {John Fearnley and Martin Gairing and Paul W. Goldberg and Rahul Savani},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W2568433471},
 pages = {1305--1344},
 title = {Learning equilibria of games via payoff queries},
 url = {http://jmlr.org/papers/v16/fearnley15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:feldman15a,
 abstract = {We consider the problem of approximating and learning disjunctions (or equivalently, conjunctions) on symmetric distributions over $\{0,1\}^n$. Symmetric distributions are distributions whose PDF is invariant under any permutation of the variables. We give a simple proof that for every symmetric distribution $\mathcal{D}$, there exists a set of $n^{O(\log{(1/ε)})}$ functions $\mathcal{S}$, such that for every disjunction $c$, there is function $p$, expressible as a linear combination of functions in $\mathcal{S}$, such that $p$ $ε$-approximates $c$ in $\ell_1$ distance on $\mathcal{D}$ or $\mathbf{E}_{x \sim \mathcal{D}}[ |c(x)-p(x)|] \leq ε$. This directly gives an agnostic learning algorithm for disjunctions on symmetric distributions that runs in time $n^{O( \log{(1/ε)})}$. The best known previous bound is $n^{O(1/ε^4)}$ and follows from approximation of the more general class of halfspaces (Wimmer, 2010). We also show that there exists a symmetric distribution $\mathcal{D}$, such that the minimum degree of a polynomial that $1/3$-approximates the disjunction of all $n$ variables is $\ell_1$ distance on $\mathcal{D}$ is $Ω( \sqrt{n})$. Therefore the learning result above cannot be achieved via $\ell_1$-regression with a polynomial basis used in most other agnostic learning algorithms. Our technique also gives a simple proof that for any product distribution $\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of degree $O(\log{(1/ε)})$ such that $p$ $ε$-approximates $c$ in $\ell_1$ distance on $\mathcal{D}$. This was first proved by Blais et al. (2008) via a more involved argument.},
 author = {Vitaly Feldman and Pravesh Kothari},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W2950489634},
 pages = {3455--3467},
 title = {Agnostic Learning of Disjunctions on Symmetric Distributions},
 url = {http://jmlr.org/papers/v16/feldman15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:feng15a,
 abstract = {Within the statistical learning framework, this paper studies the regression model associated with the correntropy induced losses. The correntropy, as a similarity measure, has been frequently employed in signal processing and pattern recognition. Motivated by its empirical successes, this paper aims at presenting some theoretical understanding towards the maximum correntropy criterion in regression problems. Our focus in this paper is two-fold: first, we are concerned with the connections between the regression model associated with the correntropy induced loss and the least squares regression model. Second, we study its convergence property. A learning theory analysis which is centered around the above two aspects is conducted. From our analysis, we see that the scale parameter in the loss function balances the convergence rates of the regression model and its robustness. We then make some efforts to sketch a general view on robust loss functions when being applied into the learning for regression problems. Numerical experiments are also implemented to verify the effectiveness of the model.},
 author = {Yunlong Feng and Xiaolin Huang and Lei Shi and Yuning Yang and Johan A.K. Suykens},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W1903303072},
 pages = {993--1034},
 title = {Learning with the maximum correntropy criterion induced losses for regression},
 url = {http://jmlr.org/papers/v16/feng15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:fox15a,
 abstract = {Capturing predictor-dependent correlations amongst the elements of a multivariate response vector is fundamental to numerous applied domains, including neuroscience, epidemiology, and finance. Although there is a rich literature on methods for allowing the variance in a univariate regression model to vary with predictors, relatively little has been done in the multivariate case. As a motivating example, we consider the Google Flu Trends data set, which provides indirect measurements of influenza incidence at a large set of locations over time (our predictor). To accurately characterize temporally evolving influenza incidence across regions, it is important to develop statistical methods for a time-varying covariance matrix. Importantly, the locations provide a redundant set of measurements and do not yield a sparse nor static spatial dependence structure. We propose to reduce dimensionality and induce a flexible Bayesian nonparametric covariance regression model by relating these location-specific trajectories to a lower-dimensional subspace through a latent factor model with predictor-dependent factor loadings. These loadings are in terms of a collection of basis functions that vary nonparametrically over the predictor space. Such low-rank approximations are in contrast to sparse precision assumptions, and are appropriate in a wide range of applications. Our formulation aims to address three challenges: scaling to large p domains, coping with missing values, and allowing an irregular grid of observations. The model is shown to be highly exible, while leading to a computationally feasible implementation via Gibbs sampling. The ability to scale to large p domains and cope with missing values is fundamental in analyzing the Google Flu Trends data.},
 author = {Emily B. Fox and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W1537539030},
 pages = {2501--2542},
 title = {Bayesian nonparametric covariance regression},
 url = {http://jmlr.org/papers/v16/fox15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:gammerman15a,
 abstract = {Preview this article: Preface to the special issue: Queering borders: Language, sexuality and migration, Page 1 of 1 < Previous page | Next page > /docserver/preview/fulltext/jls.3.1.01mur-1.gif},
 author = {Alex Gammerman and Vladimir Vovk},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W1967498427},
 pages = {1677--1681},
 title = {Preface to the special issue},
 url = {http://jmlr.org/papers/v16/gammerman15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:gammerman15b,
 abstract = {This introduction to Alexey Chervonenkis’s bibliography, which is published next in this issue, mainly consists of historical notes. The bibliography is doubtless incomplete, and it is just a first step in compiling more comprehensive ones. En route we also give some basic information about Alexey as a researcher and person; for further details, see, e.g., the short biography (Editors, 2015) in the Chervonenkis Festschrift. In this introduction, the numbers in square brackets refer to Chervonenkis’s bibliography, and the author/year citations refer to the list of references at the end of this introduction. Alexey Chervonenkis was born in Moscow in 1938. In 1955 he became a student at the MIPT, Moscow Institute of Physics and Technology (Faculty 1, Radio Engineering, nowadays Radio Engineering and Cybernetics). As part of his course of studies at the MIPT, he was attached to a laboratory at the ICS (the Institute of Control Sciences, called the Institute of Automation and Remote Control at the time), an institution in a huge system known as the Soviet Academy of Sciences.},
 author = {Alex Gammerman and Vladimir Vovk},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W1824309071},
 pages = {2051--2066},
 title = {Alexey Chervonenkis's bibliography: introductory comments},
 url = {http://jmlr.org/papers/v16/gammerman15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:gammerman15c,
 abstract = {This bibliography does not contain Alexey’s patents (he has at least two), technical reports, unpublished manuscripts, and collections edited by him. NA indicates that a journal paper was not assigned to a volume; e.g., it is common for Russian journals (such as Проблемы управления and, in some years, Автоматика и телемеханика) not to have volumes, and also to have pages numbered separately inside each issue. All papers published by Alexey before 2001 (and afterwards in the case of papers whose original language was Russian) have author lists ordered according to the Cyrillic alphabetic order; for other papers the order may reflect the authors’ contributions (people who contributed most tend to be listed first) and administrative positions (bosses tend to be listed last). The bibliography is given by the year of the original publication (which may be different from the year of the English translation, always given first when available).},
 author = {Alex Gammerman and Vladimir Vovk},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W2132116238},
 pages = {2067--2080},
 title = {Alexey Chervonenkis's bibliography},
 url = {http://jmlr.org/papers/v16/gammerman15c.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:garcia15a,
 abstract = {Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performan...},
 author = {Javier Garc{{\'i}}a and Fern and o Fern{{\'a}}ndez},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W3003931103},
 pages = {1437--1480},
 title = {A comprehensive survey on safe reinforcement learning},
 url = {http://jmlr.org/papers/v16/garcia15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:geramifard15a,
 abstract = {RLPy is an object-oriented reinforcement learning software package with a focus on value-function-based methods using linear function approximation and discrete actions. The framework was designed for both educational and research purposes. It provides a rich library of fine-grained, easily exchangeable components for learning agents (e.g., policies or representations of value functions), facilitating recently increased specialization in reinforcement learning. RLPy is written in Python to allow fast prototyping, but is also suitable for large-scale experiments through its built-in support for optimized numerical libraries and parallelization. Code profiling, domain visualizations, and data analysis are integrated in a self-contained package available under the Modified BSD License at http://github.com/rlpy/rlpy. All of these properties allow users to compare various reinforcement learning algorithms with little effort.},
 author = {Alborz Geramifard and Christoph Dann and Robert H. Klein and William Dabney and Jonathan P. How},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W2130935555},
 pages = {1573--1578},
 title = {RLPy: a value-function-based reinforcement learning framework for education and research},
 url = {http://jmlr.org/papers/v16/geramifard15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:germain15a,
 abstract = {We propose an extensive analysis of the behavior of majority votes in binary classification. In particular, we introduce a risk bound for majority votes, called the C-bound, that takes into account the average quality of the voters and their average disagreement. We also propose an extensive PAC-Bayesian analysis that shows how the C-bound can be estimated from various observations contained in the training data. The analysis intends to be self-contained and can be used as introductory material to PAC-Bayesian statistical learning theory. It starts from a general PAC-Bayesian perspective and ends with uncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leibler divergence and others allow kernel functions to be used as voters (via the sample compression setting). Finally, out of the analysis, we propose the MinCq learning algorithm that basically minimizes the C-bound. MinCq reduces to a simple quadratic program. Aside from being theoretically grounded, MinCq achieves state-of-the-art performance, as shown in our extensive empirical comparison with both AdaBoost and the Support Vector Machine.},
 author = {Pascal Germain and Alexandre Lacasse and Francois Laviolette and Mario March and Jean-Francis Roy},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W2163187564},
 pages = {787--860},
 title = {Risk bounds for the majority vote: from a PAC-Bayesian analysis to a learning algorithm},
 url = {http://jmlr.org/papers/v16/germain15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:gyorfi15a,
 abstract = {An estimate of the second moment of the regression function is introduced. Its asymptotic normality is proved such that the asymptotic variance depends neither on the dimension of the observation vector, nor on the smoothness properties of the regression function. The asymptotic variance is given explicitly.},
 author = {L{{\'a}}szl{{\'o}} Gy{{\"o}}rfi and Harro Walk},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W1649659002},
 pages = {1863--1877},
 title = {On the asymptotic normality of an estimate of a regression functional},
 url = {http://jmlr.org/papers/v16/gyorfi15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:han15a,
 abstract = {The vector autoregressive (VAR) model is a powerful tool in modeling complex time series and has been exploited in many fields. However, fitting high dimensional VAR model poses some unique challenges: On one hand, the dimensionality, caused by modeling a large number of time series and higher order autoregressive processes, is usually much higher than the time series length; On the other hand, the temporal dependence structure in the VAR model gives rise to extra theoretical challenges. In high dimensions, one popular approach is to assume the transition matrix is sparse and fit the VAR model using the "least squares" method with a lasso-type penalty. In this manuscript, we propose an alternative way in estimating the VAR model. The main idea is, via exploiting the temporal dependence structure, to formulate the estimating problem into a linear program. There is instant advantage for the proposed approach over the lasso-type estimators: The estimation equation can be decomposed into multiple sub-equations and accordingly can be efficiently solved in a parallel fashion. In addition, our method brings new theoretical insights into the VAR model analysis. So far the theoretical results developed in high dimensions (e.g., Song and Bickel (2011) and Kock and Callot (2012)) mainly pose assumptions on the design matrix of the formulated regression problems. Such conditions are indirect about the transition matrices and not transparent. In contrast, our results show that the operator norm of the transition matrices plays an important role in estimation accuracy. We provide explicit rates of convergence for both estimation and prediction. In addition, we provide thorough experiments on both synthetic and real-world equity data to show that there are empirical advantages of our method over the lasso-type estimators in both parameter estimation and forecasting.},
 author = {Fang Han and Huanran Lu and Han Liu},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W1836500409},
 pages = {3115--3150},
 title = {A Direct Estimation of High Dimensional Stationary Vector Autoregressions},
 url = {http://jmlr.org/papers/v16/han15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:hanneke15a,
 abstract = {This work establishes distribution-free upper and lower bounds on the minimax label complexity of active learning with general hypothesis classes, under various noise models. The results reveal a number of surprising facts. In particular, under the noise model of Tsybakov (2004), the minimax label complexity of active learning with a VC class is always asymptotically smaller than that of passive learning, and is typically signi_cantly smaller than the best previously-published upper bounds in the active learning literature. In high-noise regimes, it turns out that all active learning problems of a given VC dimension have roughly the same minimax label complexity, which contrasts with well-known results for bounded noise. In low-noise regimes, we find that the label complexity is well-characterized by a simple combinatorial complexity measure we call the star number. Interestingly, we find that almost all of the complexity measures previously explored in the active learning literature have worst-case values exactly equal to the star number. We also propose new active learning strategies that nearly achieve these minimax label complexities.},
 author = {Steve Hanneke and Liu Yang},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W2964242659},
 pages = {3487--3602},
 title = {Minimax analysis of active learning},
 url = {http://jmlr.org/papers/v16/hanneke15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:hastie15a,
 abstract = {The matrix-completion problem has attracted a lot of attention, largely as a result of the celebrated Netflix competition. Two popular approaches for solving the problem are nuclear-norm-regularized matrix approximation (Candes and Tao, 2009, Mazumder, Hastie and Tibshirani, 2010), and maximum-margin matrix factorization (Srebro, Rennie and Jaakkola, 2005). These two procedures are in some cases solving equivalent problems, but with quite different algorithms. In this article we bring the two approaches together, leading to an efficient algorithm for large matrix factorization and completion that outperforms both of these. We develop a software package "softImpute" in R for implementing our approaches, and a distributed version for very large matrices using the "Spark" cluster programming environment.},
 author = {Trevor Hastie and Rahul Mazumder and Jason D. Lee and Reza Zadeh},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W2949419207},
 pages = {3367--3402},
 title = {Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares},
 url = {http://jmlr.org/papers/v16/hastie15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:he15a,
 abstract = {When learning a directed acyclic graph (DAG) model via observational data, one generally cannot identify the underlying DAG, but can potentially obtain a Markov equivalence class. The size (the number of DAGs) of a Markov equivalence class is crucial to infer causal effects or to learn the exact causal DAG via further interventions. Given a set of Markov equivalence classes, the distribution of their sizes is a key consideration in developing learning methods. However, counting the size of an equivalence class with many vertices is usually computationally infeasible, and the existing literature reports the size distributions only for equivalence classes with ten or fewer vertices.

In this paper, we develop a method to compute the size of a Markov equivalence class. We first show that there are five types of Markov equivalence classes whose sizes can be formulated as five functions of the number of vertices respectively. Then we introduce a new concept of a rooted sub-class. The graph representations of rooted subclasses of a Markov equivalence class are used to partition this class recursively until the sizes of all rooted subclasses can be computed via the five functions. The proposed size counting is efficient for Markov equivalence classes of sparse DAGs with hundreds of vertices. Finally, we explore the size and edge distributions of Markov equivalence classes and find experimentally that, in general, (1) most Markov equivalence classes are half completed and their average sizes are small, and (2) the sizes of sparse classes grow approximately exponentially with the numbers of vertices.},
 author = {Yangbo He and Jinzhu Jia and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W2280205447},
 pages = {2589--2609},
 title = {Counting and exploring sizes of Markov equivalence classes of directed acyclic graphs},
 url = {http://jmlr.org/papers/v16/he15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:heaton15a,
 abstract = {This paper introduces the Encog library for Java and C#, a scalable, adaptable, multiplatform machine learning framework that was 1st released in 2008. Encog allows a variety of machine learning models to be applied to datasets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient multithreaded code to reduce training time by exploiting modern multicore processors. The current version of Encog can be downloaded from http://www.encog.org.},
 author = {Jeff Heaton},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W4300124873},
 pages = {1243--1247},
 title = {Encog: Library of Interchangeable Machine Learning Models for Java and C#},
 url = {http://jmlr.org/papers/v16/heaton15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:helmbold15a,
 abstract = {Dropout is a simple but effective technique for learning in neural networks and other settings. A sound theoretical understanding of dropout is needed to determine when dropout should be applied and how to use it most effectively. In this paper we continue the exploration of dropout as a regularizer pioneered by Wager et al. We focus on linear classification where a convex proxy to the misclassification loss (i.e. the logistic loss used in logistic regression) is minimized. We show: • when the dropout-regularized criterion has a unique minimizer, • when the dropout-regularization penalty goes to infinity with the weights, and when it remains bounded, • that the dropout regularization can be non-monotonic as individual weights increase from 0, and • that the dropout regularization penalty may not be convex. This last point is particularly surprising because the combination of dropout regularization with any convex loss proxy is always a convex function.

In order to contrast dropout regularization with L2 regularization, we formalize the notion of when different random sources of data are more compatible with different regularizers. We then exhibit distributions that are provably more compatible with dropout regularization than L2 regularization, and vice versa. These sources provide additional insight into how the inductive biases of dropout and L2 regularization differ. We provide some similar results for L1 regularization.},
 author = {David P. Helmbold and Philip M. Long},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W2963724148},
 pages = {3403--3454},
 title = {On the inductive bias of dropout},
 url = {http://jmlr.org/papers/v16/helmbold15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:herbster15a,
 abstract = {We study the problem of predicting online the labeling of a graph. We consider a novel setting for this problem in which, in addition to observing vertices and labels on the graph, we also observe a sequence of just vertices on a second graph. A latent labeling of the second graph selects one of K labelings to be active on the first graph. We propose a polynomial time algorithm for online prediction in this setting and derive a mistake bound for the algorithm. The bound is controlled by the geometric cut of the observed and latent labelings, as well as the resistance diameters of the graphs. When specialized to multitask prediction and online switching problems the bound gives new and sharper results under certain conditions.},
 author = {Mark Herbster and Stephen Pasteris and Massimiliano Pontil},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2190548589},
 pages = {2003--2022},
 title = {Predicting a switching sequence of graph labelings},
 url = {http://jmlr.org/papers/v16/herbster15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:hermans15a,
 abstract = {Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers.},
 author = {Michiel Hermans and Miguel C. Soriano and Joni Dambre and Peter Bienstman and Ingo Fischer},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W2953341488},
 pages = {2081--2097},
 title = {Photonic Delay Systems as Machine Learning Implementations},
 url = {http://jmlr.org/papers/v16/hermans15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:honda15a,
 abstract = {In this paper we consider a stochastic multiarmed bandit problem. It is known in this problem that Deterministic Minimum Empirical Divergence (DMED) policy achieves the asymptotic theoretical bound...},
 author = {Junya Honda and Akimichi Takemura},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W3005005431},
 pages = {3721--3756},
 title = {Non-asymptotic analysis of a new bandit algorithm for semi-bounded rewards},
 url = {http://jmlr.org/papers/v16/honda15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:honorio15a,
 abstract = {We consider learning, from strictly behavioral data, the structure and parameters of linear influence games (LIGs), a class of parametric graphical games introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic inference (CSI): Making inferences from causal interventions on stable behavior in strategic settings. Applications include the identification of the most influential individuals in large (social) networks. Such tasks can also support policy-making analysis. Motivated by the computational work on LIGs, we cast the learning problem as maximum-likelihood estimation (MLE) of a generative model defined by pure-strategy Nash equilibria (PSNE). Our simple formulation uncovers the fundamental interplay between goodness-of-fit and model complexity: good models capture equilibrium behavior within the data while controlling the true number of equilibria, including those unobserved. We provide a generalization bound establishing the sample complexity for MLE in our framework. We propose several algorithms including convex loss minimization (CLM) and sigmoidal approximations. We prove that the number of exact PSNE in LIGs is small, with high probability; thus, CLM is sound. We illustrate our approach on synthetic data and real-world U.S. congressional voting records. We briefly discuss our learning framework's generality and potential applicability to general graphical games.},
 author = {Jean Honorio and Luis Ortiz},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W1807347157},
 pages = {1157--1210},
 title = {Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data},
 url = {http://jmlr.org/papers/v16/honorio15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:horta15a,
 abstract = {Similarity measures for comparing clusterings is an important component, e.g., of evaluating clustering algorithms, for consensus clustering, and for clustering stability assessment. These measures have been studied for over 40 years in the domain of exclusive hard clusterings (exhaustive and mutually exclusive object sets). In the past years, the literature has proposed measures to handle more general clusterings (e.g., fuzzy/probabilistic clusterings). This paper provides an overview of these new measures and discusses their drawbacks. We ultimately develop a corrected-for-chance measure (13AGRI) capable of comparing exclusive hard, fuzzy/probabilistic, non-exclusive hard, and possibilistic clusterings. We prove that 13AGRI and the adjusted Rand index (ARI, by Hubert and Arabie) are equivalent in the exclusive hard domain. The reported experiments show that only 13AGRI could provide both a fine-grained evaluation across clusterings with different numbers of clusters and a constant evaluation between random clusterings, showing all the four desirable properties considered here. We identified a high correlation between 13AGRI applied to fuzzy clusterings and ARI applied to hard exclusive clusterings over 14 real data sets from the UCI repository, which corroborates the validity of 13AGRI fuzzy clustering evaluation. 13AGRI also showed good results as a clustering stability statistic for solutions produced by the expectation maximization algorithm for Gaussian mixture. Implementation and supplementary figures can be found at http://sn.im/25a9h8u.},
 author = {Danilo Horta and Ricardo J.G.B. Campello},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W2275525390},
 pages = {2949--2997},
 title = {Comparing hard and overlapping clusterings},
 url = {http://jmlr.org/papers/v16/horta15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:hothorn15a,
 abstract = {The R package partykit provides a flexible toolkit for learning, representing, summarizing, and visualizing a wide range of tree-structured regression and classification models. The functionality encompasses: (a) basic infrastructure for representing trees (inferred by any algorithm) so that unified print/plot/predict methods are available; (b) dedicated methods for trees with constant fits in the leaves (or terminal nodes) along with suitable coercion functions to create such trees (e.g., by rpart, RWeka, PMML); (c) a reimplementation of conditional inference trees (ctree, originally provided in the party package); (d) an extended reimplementation of model-based recursive partitioning (mob, also originally in party) along with dedicated methods for trees with parametric models in the leaves. Here, a brief overview of the package and its design is given while more detailed discussions of items (a)--(d) are available in vignettes accompanying the package.},
 author = {Torsten Hothorn and Achim Zeileis},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W3121876541},
 pages = {3905--3909},
 title = {partykit: A Modular Toolkit for Recursive Partytioning in R},
 url = {http://jmlr.org/papers/v16/hothorn15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:huang15a,
 abstract = {We introduce an online tensor decomposition based approach for two latent variable modeling problems namely, (1) community detection, in which we learn the latent communities that the social actors in social networks belong to, and (2) topic modeling, in which we infer hidden topics of text articles. We consider decomposition of moment tensors using stochastic gradient descent. We conduct optimization of multilinear operations in SGD and avoid directly forming the tensors, to save computational and storage costs. We present optimized algorithm in two platforms. Our GPU-based implementation exploits the parallelism of SIMD architectures to allow for maximum speed-up by a careful optimization of storage and data transfer, whereas our CPU-based implementation uses efficient sparse matrix computations and is suitable for large sparse data sets. For the community detection problem, we demonstrate accuracy and computational efficiency on Facebook, Yelp and DBLP data sets, and for the topic modeling problem, we also demonstrate good performance on the New York Times data set. We compare our results to the state-of-the-art algorithms such as the variational method, and report a gain of accuracy and a gain of several orders of magnitude in the execution time.},
 author = {Furong Huang and U. N. Niranjan and Mohammad Umar Hakeem and Animashree An and kumar},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W2963396025},
 pages = {2797--2835},
 title = {Online tensor methods for learning latent variable models},
 url = {http://jmlr.org/papers/v16/huang15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:janzing15a,
 abstract = {According to a recently stated 'independence postulate', the distribution Pcause contains no information about the conditional Peffect|cause while Peffect may contain information about Pcause|effect. Since semi-supervised learning (SSL) attempts to exploit information from PX to assist in predicting Y from X, it should only work in anticausal direction, i.e., when Y is the cause and X is the effect. In causal direction, when X is the cause and Y the effect, unlabelled x-values should be useless. To shed light on this asymmetry, we study a deterministic causal relation Y = f(X) as recently assayed in Information-Geometric Causal Inference (IGCI). Within this model, we discuss two options to formalize the independence of PX and f as an orthogonality of vectors in appropriate inner product spaces. We prove that unlabelled data help for the problem of interpolating a monotonically increasing function if and only if the orthogonality conditions are violated - which we only expect for the anticausal direction. Here, performance of SSL and its supervised baseline analogue is measured in terms of two different loss functions: first, the mean squared error and second the surprise in a Bayesian prediction scenario.},
 author = {Dominik Janzing and Bernhard Sch{{\"o}}lkopf},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W1845812186},
 pages = {1923--1948},
 title = {Semi-supervised interpolation in an anticausal learning scenario},
 url = {http://jmlr.org/papers/v16/janzing15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:jawanpuria15a,
 abstract = {This paper generalizes the framework of Hierarchical Kernel Learning (HKL) and illustrates its utility in the domain of rule learning. HKL involves Multiple Kernel Learning over a set of given base kernels assumed to be embedded on a directed acyclic graph. This paper proposes a two-fold generalization of HKL: the first is employing a generic l1/lp block-norm regularizer (ρ ∈ (1, 2]) that alleviates a key limitation of the HKL formulation. The second is a generalization to the case of multi-class, multi-label and more generally, multi-task applications. The main technical contribution of this work is the derivation of a highly specialized partial dual of the proposed generalized HKL formulation and an efficient mirror descent based active set algorithm for solving it. Importantly, the generic regularizer enables the proposed formulation to be employed in the Rule Ensemble Learning (REL) where the goal is to construct an ensemble of conjunctive propositional rules. Experiments on benchmark REL data sets illustrate the efficacy of the proposed generalizations.},
 author = {Pratik Jawanpuria and Jagarlapudi Saketha Nath and Ganesh Ramakrishnan},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W344268215},
 pages = {617--652},
 title = {Generalized hierarchical kernel learning},
 url = {http://jmlr.org/papers/v16/jawanpuria15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:jiang15a,
 abstract = {This paper proposes a novel multi-layered gesture recognition method with Kinect. We explore the essential linguistic characters of gestures: the components concurrent character and the sequential ...},
 author = {Feng Jiang and Shengping Zhang and Shen Wu and Yang Gao and Debin Zhao},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W3008201418},
 pages = {227--254},
 title = {Multi-layered gesture recognition with Kinect},
 url = {http://jmlr.org/papers/v16/jiang15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:jorgensen15a,
 abstract = {We study reproducing kernels, and associated reproducing kernel Hilbert spaces (RKHSs) $\mathscr{H}$ over infinite, discrete and countable sets $V$. In this setting we analyze in detail the distributions of the corresponding Dirac point-masses of $V$. Illustrations include certain models from neural networks: An Extreme Learning Machine (ELM) is a neural network-configuration in which a hidden layer of weights are randomly sampled, and where the object is then to compute resulting output. For RKHSs $\mathscr{H}$ of functions defined on a prescribed countable infinite discrete set $V$, we characterize those which contain the Dirac masses $δ_{x}$ for all points $x$ in $V$. Further examples and applications where this question plays an important role are: (i) discrete Brownian motion-Hilbert spaces, i.e., discrete versions of the Cameron-Martin Hilbert space; (ii) energy-Hilbert spaces corresponding to graph-Laplacians where the set $V$ of vertices is then equipped with a resistance metric; and finally (iii) the study of Gaussian free fields.},
 author = {Palle Jorgensen and Feng Tian},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W2963834606},
 pages = {3079--3114},
 title = {Discrete reproducing kernel Hilbert spaces: Sampling and distribution of Dirac-masses},
 url = {http://jmlr.org/papers/v16/jorgensen15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:kiraly15a,
 abstract = {We present a novel algebraic combinatorial view on low-rank matrix completion based on studying relations between a few entries with tools from algebraic geometry and matroid theory. The intrinsic locality of the approach allows for the treatment of single entries in a closed theoretical and practical framework. More specifically, apart from introducing an algebraic combinatorial theory of low-rank matrix completion, we present probability-one algorithms to decide whether a particular entry of the matrix can be completed. We also describe methods to complete that entry from a few others, and to estimate the error which is incurred by any method completing that entry. Furthermore, we show how known results on matrix completion and their sampling assumptions can be related to our new perspective and interpreted in terms of a completability phase transition.},
 author = {Franz J.Kir{{\'a}}ly and Louis Theran and Ryota Tomioka},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W2963823663},
 pages = {1391--1436},
 title = {The Algebraic Combinatorial Approach for Low-Rank Matrix Completion},
 url = {http://jmlr.org/papers/v16/kiraly15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:kirichenko15a,
 abstract = {In this paper we provide theoretical support for the so-called Sigmoidal Gaussian Cox Process approach to learning the intensity of an inhomogeneous Poisson process on a d- dimensional domain. This method was proposed by Adams, Murray and MacKay (ICML, 2009), who developed a tractable computational approach and showed in simulation and real data experiments that it can work quite satisfactorily. The results presented in the present paper provide theoretical underpinning of the method. In particular, we show how to tune the priors on the hyper parameters of the model in order for the procedure to automatically adapt to the degree of smoothness of the unknown intensity, and to achieve optimal convergence rates.},
 author = {Alisa Kirichenko and Harry van Zanten},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W1510966913},
 pages = {2909--2919},
 title = {Optimality of Poisson processes intensity learning with Gaussian processes},
 url = {http://jmlr.org/papers/v16/kirichenko15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:koltchinskii15a,
 abstract = {The density matrices are positively semi-definite Hermitian matrices of unit trace that describe the state of a quantum system. The goal of the paper is to develop minimax lower bounds on error rates of estimation of low rank density matrices in trace regression models used in quantum state tomography (in particular, in the case of Pauli measurements) with explicit dependence of the bounds on the rank and other complexity parameters. Such bounds are established for several statistically relevant distances, including quantum versions of Kullback-Leibler divergence (relative entropy distance) and of Hellinger distance (so called Bures distance), and Schatten p-norm distances. Sharp upper bounds and oracle inequalities for least squares estimator with von Neumann entropy penalization are obtained showing that minimax lower bounds are attained (up to logarithmic factors) for these distances.},
 author = {Vladimir Koltchinskii and Dong Xia},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W2963441353},
 pages = {1757--1792},
 title = {Optimal estimation of low rank density matrices},
 url = {http://jmlr.org/papers/v16/koltchinskii15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:krueger15a,
 abstract = {With the increasing size of today's data sets, finding the right parameter configuration in model selection via cross-validation can be an extremely time-consuming task. In this paper we propose an improved cross-validation procedure which uses nonparametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating underperforming candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the capability of the full cross-validation. Theoretical considerations underline the statistical power of our procedure. The experimental evaluation shows that our method reduces the computation time by a factor of up to 120 compared to a full cross-validation with a negligible impact on the accuracy.},
 author = {Tammo Krueger and Danny Panknin and Mikio Braun},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W1880342431},
 pages = {1103--1155},
 title = {Fast Cross-Validation via Sequential Testing},
 url = {http://jmlr.org/papers/v16/krueger15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:lee15a,
 abstract = {Estimation of inverse covariance matrices, known as precision matrices, is important in various areas of statistical analysis. In this article, we consider estimation of multiple precision matrices sharing some common structures. In this setting, estimating each precision matrix separately can be suboptimal as it ignores potential common structures. This article proposes a new approach to parameterize each precision matrix as a sum of common and unique components and estimate multiple precision matrices in a constrained l1 minimization framework. We establish both estimation and selection consistency of the proposed estimator in the high dimensional setting. The proposed estimator achieves a faster convergence rate for the common structure in certain cases. Our numerical examples demonstrate that our new estimator can perform better than several existing methods in terms of the entropy loss and Frobenius loss. An application to a glioblastoma cancer data set reveals some interesting gene networks across multiple cancer subtypes.},
 author = {Wonyul Lee and Yufeng Liu},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W1958662094},
 pages = {1035--1062},
 title = {Joint Estimation of Multiple Precision Matrices with Common Structures.},
 url = {http://jmlr.org/papers/v16/lee15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:li15a,
 abstract = {This paper describes an R package named flare, which implements a family of new high dimensional regression methods (LAD Lasso, SQRT Lasso, lq Lasso, and Dantzig selector) and their extensions to s...},
 author = {Xingguo Li and Tuo Zhao and Xiaoming Yuan and Han Liu},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W3008266953},
 pages = {553--557},
 title = {The flare package for high dimensional linear regression and precision matrix estimation in R},
 url = {http://jmlr.org/papers/v16/li15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:li15b,
 abstract = {The track 1 problem in KDD Cup 2013 is to discriminate between papers confirmed by the given authors from the other deleted papers. This paper describes the winning solution of team National Taiwan University for track 1 of KDD Cup 2013. First, we conduct the feature engineering to transform the various provided text information into 97 features. Second, we train classification and ranking models using these features. Last, we combine our individual models to boost the performance by using results on the internal validation set and the official Valid set. Some effective post-processing techniques have also been proposed. Our solution achieves 0.98259 MAP score and ranks the first place on the private leaderboard of Test set.},
 author = {Chun-Liang Li and Yu-Chuan Su and Ting-Wei Lin and Cheng-Hao Tsai and Wei-Cheng Chang and Kuan-Hao Huang and Tzu-Ming Kuo and Shan-Wei Lin and Young-San Lin and Yu-Chen Lu and Chun-Pai Yang and Cheng-Xia Chang and Wei-Sheng Chin and Yu-Chin Juan and Hsiao-Yu Tung and Jui-Pin Wang and Cheng-Kuang Wei and Felix Wu and Tu-Chun Yin and Tong Yu and Yong Zhuang and Shou-de Lin and Hsuan-Tien Lin and Chih-Jen Lin},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W2152313299},
 pages = {2921--2947},
 title = {Combination of feature engineering and ranking models for paper-author identification in KDD Cup 2013},
 url = {http://jmlr.org/papers/v16/li15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:lin15a,
 abstract = {A relaxed randomized Kaczmarz algorithm is investigated in a least squares regression setting by a learning theory approach. When the sampling values are accurate and the regression function (condi...},
 author = {Junhong Lin and Ding-Xuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W3017548433},
 pages = {3341--3365},
 title = {Learning theory of randomized Kaczmarz algorithm},
 url = {http://jmlr.org/papers/v16/lin15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:lin15b,
 abstract = {This paper investigates the Euler's elastica (EE) model for high-dimensional supervised learning problems in a function approximation framework. In 1744 Euler introduced the elastica energy for a 2D curve on modeling torsion-free thin elastic rods. Together with its degenerate form of total variation (TV), Euler's elastica has been successfully applied to low-dimensional data processing such as image denoising and image inpainting in the last two decades. Our motivation is to apply Euler's elastica to high-dimensional supervised learning problems. To this end, a supervised learning problem is modeled as an energy functional minimization under a new geometric regularization scheme, where the energy is composed of a squared loss and an elastica penalty. The elastica penalty aims at regularizing the approximated function by heavily penalizing large gradients and high curvature values on all level curves. We take a computational PDE approach to minimize the energy functional. By using variational principles, the energy minimization problem is transformed into an Euler-Lagrange PDE. However, this PDE is usually high-dimensional and can not be directly handled by common low-dimensional solvers. To circumvent this difficulty, we use radial basis functions (RBF) to approximate the target function, which reduces the optimization problem to finding the linear coefficients of these basis functions. Some theoretical properties of this new model, including the existence and uniqueness of solutions and universal consistency, are analyzed. Extensive experiments have demonstrated the effectiveness of the proposed model for binary classification, multi-class classification, and regression tasks.},
 author = {Tong Lin and Hanlin Xue and Ling Wang and Bo Huang and Hongbin Zha},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W2282127401},
 pages = {3637--3686},
 title = {Supervised learning via Euler's Elastica models},
 url = {http://jmlr.org/papers/v16/lin15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:liu15a,
 abstract = {We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate (1/K) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is O(n1/2) in unconstrained optimization and O(n1/4) in the separable-constrained case, where n is the number of variables. We describe results from implementation on 40-core processors.},
 author = {Ji Liu and Stephen J. Wright and Christopher R{{\'e}} and Victor Bittorf and Srikrishna Sridhar},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2616811598},
 pages = {285--322},
 title = {An asynchronous parallel stochastic coordinate descent algorithm},
 url = {http://jmlr.org/papers/v16/liu15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:liu15b,
 abstract = {We propose a calibrated multivariate regression method named CMR for fitting high dimensional multivariate regression models. Compared with existing methods, CMR calibrates regularization for each regression task with respect to its noise level so that it simultaneously attains improved finite-sample performance and tuning insensitiveness. Theoretically, we provide sufficient conditions under which CMR achieves the optimal rate of convergence in parameter estimation. Computationally, we propose an efficient smoothed proximal gradient algorithm with a worst-case numerical rate of convergence O(1/ϵ), where ϵ is a pre-specified accuracy of the objective function value. We conduct thorough numerical simulations to illustrate that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR to solve a brain activity prediction problem and find that it is as competitive as a handcrafted model created by human experts. The R package camel implementing the proposed method is available on the Comprehensive R Archive Network http://cran.r-project.org/web/packages/camel/.},
 author = {Han Liu and Lie Wang and Tuo Zhao},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W1921133930},
 pages = {1579--1606},
 title = {Calibrated Multivariate Regression with Application to Neural Semantic Basis Discovery.},
 url = {http://jmlr.org/papers/v16/liu15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:loh15a,
 abstract = {We provide novel theoretical results regarding local optima of regularized M-estimators, allowing for nonconvexity in both loss and penalty functions. Under restricted strong convexity on the loss and suitable regularity conditions on the penalty, we prove that any stationary point of the composite objective function will lie within statistical precision of the underlying parameter vector. Our theory covers many nonconvex objective functions of interest, including the corrected Lasso for errors-in-variables linear models; regression for generalized linear models with nonconvex penalties such as SCAD, MCP, and capped-l 1; and high-dimensional graphical model estimation. We quantify statistical accuracy by providing bounds on the l1-, l2-, and prediction error between stationary points and the population-level optimum. We also propose a simple modification of composite gradient descent that may be used to obtain a near-global optimum within statistical precision estat in log(1/estat) steps, which is the fastest possible rate of any first-order method. We provide simulation studies illustrating the sharpness of our theoretical results.},
 author = {Po-Ling Loh and Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2616050959},
 pages = {559--616},
 title = {Regularized M-estimators with nonconvexity: statistical and algorithmic theory for local optima},
 url = {http://jmlr.org/papers/v16/loh15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:lopezpaz15a,
 abstract = {We are interested in learning causal relationships between pairs of random variables, purely from observational data. To effectively address this task, the state-of-the-art relies on strong assumptions regarding the mechanisms mapping causes to effects, such as invertibility or the existence of additive noise, which only hold in limited situations. On the contrary, this short paper proposes to learn how to perform causal inference directly from data, and without the need of feature engineering. In particular, we pose causality as a kernel mean embedding classification problem, where inputs are samples from arbitrary probability distributions on pairs of random variables, and labels are types of causal relationships. We validate the performance of our method on synthetic and real-world data against the state-of-the-art. Moreover, we submitted our algorithm to the ChaLearn's "Fast Causation Coefficient Challenge" competition, with which we won the fastest code prize and ranked third in the overall leaderboard.},
 author = {David Lopez-Paz and Krikamol Mu and et and Benjamin Recht},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W2950405949},
 pages = {2901--2907},
 title = {The Randomized Causation Coefficient},
 url = {http://jmlr.org/papers/v16/lopezpaz15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:lowd15a,
 abstract = {The Libra Toolkit is a collection of algorithms for learning and inference with discrete probabilistic models, including Bayesian networks, Markov networks, dependency networks, and sum-product networks. Compared to other toolkits, Libra places a greater emphasis on learning the structure of tractable models in which exact inference is efficient. It also includes a variety of algorithms for learning graphical models in which inference is potentially intractable, and for performing exact and approximate inference. Libra is released under a 2-clause BSD license to encourage broad use in academia and industry.},
 author = {Daniel Lowd and Amirmohammad Rooshenas},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W2963458184},
 pages = {2459--2463},
 title = {The Libra Toolkit for Probabilistic Models},
 url = {http://jmlr.org/papers/v16/lowd15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:ma15a,
 abstract = {One popular method for dealing with large-scale data sets is sampling. For example, by using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. This method has been successful in improving computational efficiency of algorithms for matrix problems such as least-squares approximation, least absolute deviations approximation, and low-rank matrix approximation. Existing work has focused on algorithmic issues such as worst-case running times and numerical issues associated with providing high-quality implementations, but none of it addresses statistical aspects of this method. In this paper, we provide a simple yet effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms. A detailed empirical evaluation of existing leverage-based methods as well as these two new methods is carried out on both synthetic and real data sets. The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance.},
 author = {Ping Ma and Michael W. Mahoney and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W2615055317},
 pages = {861--911},
 title = {A Statistical Perspective on Algorithmic Leveraging},
 url = {http://jmlr.org/papers/v16/ma15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:mackey15a,
 abstract = {If learning methods are to scale to the massive sizes of modern data sets, it is essential for the field of machine learning to embrace parallel and distributed computing. Inspired by the recent development of matrix factorization methods with rich theory but poor computational complexity and by the relative ease of mapping matrices onto distributed architectures, we introduce a scalable divide-and-conquer framework for noisy matrix factorization. We present a thorough theoretical analysis of this framework in which we characterize the statistical errors introduced by the divide step and control their magnitude in the conquer step, so that the overall algorithm enjoys high-probability estimation guarantees comparable to those of its base algorithm. We also present experiments in collaborative filtering and video background modeling that demonstrate the near-linear to superlinear speed-ups attainable with this approach.},
 author = {Lester Mackey and Ameet Talwalkar and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2962715745},
 pages = {913--960},
 title = {Distributed matrix completion and robust factorization},
 url = {http://jmlr.org/papers/v16/mackey15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:martins15a,
 author = {Andr{{\'e}} F. T. Martins and M{{\'a}}rio A. T. Figueiredo and Pedro M. Q. Aguiar and Noah A. Smith and Eric P. Xing},
 journal = {Journal of Machine Learning Research},
 number = {16},
 pages = {495--545},
 title = {AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Models},
 url = {http://jmlr.org/papers/v16/martins15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:marusic15a,
 abstract = {We consider the complexity of equivalence and learning for multiplicity tree automata, i.e., weighted tree automata with weights in a field. We first show that the equivalence problem for multiplicity tree automata is logspace equivalent to polynomial identity testing. Secondly, we consider the problem of learning multiplicity tree automata in Angluin’s exact learning model. Here we give lower bounds on the number of queries, both for the case of an arbitrary and a fixed underlying field. We also present a learning algorithm in which trees are represented succinctly as DAGs. Assuming a Teacher that represents counterexamples as succinctly as possible, our algorithm uses exponentially fewer queries than the best previously known procedure, leaving only a polynomial gap with the above-mentioned lower bound. Moreover, fixing the alphabet rank, the query complexity of our algorithm matches the lower bound up to a constant factor.},
 author = {Ines Marusic and James Worrell},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W144407630},
 pages = {2465--2500},
 title = {Complexity of Equivalence and Learning for Multiplicity Tree Automata},
 url = {http://jmlr.org/papers/v16/marusic15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:masnadi15a,
 abstract = {Regularization is commonly used in classifier design, to assure good generalization. Classical regularization enforces a cost on classifier complexity, by constraining parameters. This is usually combined with a margin loss, which favors large-margin decision rules. A novel and unified view of this architecture is proposed, by showing that margin losses act as regularizers of posterior class probabilities, in a way that amplifies classical parameter regularization. The problem of controlling the regularization strength of a margin loss is considered, using a decomposition of the loss in terms of a link and a binding function. The link function is shown to be responsible for the regularization strength of the loss, while the binding function determines its outlier robustness. A large class of losses is then categorized into equivalence classes of identical regularization strength or outlier robustness. It is shown that losses in the same regularization class can be parameterized so as to have tunable regularization strength. This parameterization is finally used to derive boosting algorithms with loss regularization (BoostLR). Three classes of tunable regularization losses are considered in detail. Canonical losses can implement all regularization behaviors but have no flexibility in terms of outlier modeling. Shrinkage losses support equally parameterized link and binding functions, leading to boosting algorithms that implement the popular shrinkage procedure. This offers a new explanation for shrinkage as a special case of loss-based regularization. Finally, α-tunable losses enable the independent parameterization of link and binding functions, leading to boosting algorithms of great exibility. This is illustrated by the derivation of an algorithm that generalizes both AdaBoost and LogitBoost, behaving as either one when that best suits the data to classify. Various experiments provide evidence of the benefits of probability regularization for both classification and estimation of posterior class probabilities.},
 author = {Hamed Masnadi-Shirazi and Nuno Vasconcelos},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W2276848200},
 pages = {2751--2795},
 title = {A view of margin losses as regularizers of probability estimates},
 url = {http://jmlr.org/papers/v16/masnadi15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:mokhtari15a,
 abstract = {Global convergence of an online (stochastic) limited memory version of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method for solving optimization problems with stochastic objectives that arise in large scale machine learning is established. Lower and upper bounds on the Hessian eigenvalues of the sample functions are shown to suffice to guarantee that the curvature approximation matrices have bounded determinants and traces, which, in turn, permits establishing convergence to optimal arguments with probability 1. Experimental evaluation on a search engine advertising problem showcase reductions in convergence time relative to stochastic gradient descent algorithms.},
 author = {Aryan Mokhtari and Alej and ro Ribeiro},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W1592294486},
 pages = {3151--3181},
 title = {Global convergence of online limited memory BFGS},
 url = {http://jmlr.org/papers/v16/mokhtari15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:montufar15a,
 abstract = {We describe discrete restricted Boltzmann machines: probabilistic graphical models with bipartite interactions between visible and hidden discrete variables. Examples are binary restricted Boltzmann machines and discrete naive Bayes models. We detail the inference functions and distributed representations arising in these models in terms of configurations of projected products of simplices and normal fans of products of simplices. We bound the number of hidden variables, depending on the cardinalities of their state spaces, for which these models can approximate any probability distribution on their visible states to any given accuracy. In addition, we use algebraic methods and coding theory to compute their dimension.},
 author = {Guido Mont{{\'u}}far and Jason Morton},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2614986750},
 pages = {653--672},
 title = {Discrete restricted Boltzmann machines},
 url = {http://jmlr.org/papers/v16/montufar15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:montufar15b,
 abstract = {Conditional restricted Boltzmann machines are undirected stochastic neural networks with a layer of input and output units connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the output units given the states of the input units, parameterized by interaction weights and biases. We address the representational power of these models, proving results on their ability to represent conditional Markov random fields and conditional distributions with restricted supports, the minimal size of universal approximators, the maximal model approximation errors, and on the dimension of the set of representable conditional distributions. We contribute new tools for investigating conditional probability models, which allow us to improve the results that can be derived from existing work on restricted Boltzmann machine probability models.},
 author = {Guido Mont{{\'u}}far and Nihat Ay and Keyan Ghazi-Zahedi},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2963177263},
 pages = {2405--2436},
 title = {Geometry and expressive power of conditional restricted Boltzmann machines},
 url = {http://jmlr.org/papers/v16/montufar15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:morales15a,
 abstract = {SAMOA (SCALABLE ADVANCED MASSIVE ONLINE ANALYSIS) is a platform for mining big data streams. It provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Storm, S4, and Samza. samoa is written in Java, is open source, and is available at http://samoa-project.net under the Apache Software License version 2.0.},
 author = {Gianmarco De Francisci Morales and Albert Bifet},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W2104564392},
 pages = {149--153},
 title = {SAMOA: scalable advanced massive online analysis},
 url = {http://jmlr.org/papers/v16/morales15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:moreno15a,
 abstract = {Crowdsourcing has been proven to be an effective and efficient tool to annotate large data-sets. User annotations are often noisy, so methods to combine the annotations to produce reliable estimates of the ground truth are necessary. We claim that considering the existence of clusters of users in this combination step can improve the performance. This is especially important in early stages of crowdsourcing implementations, where the number of annotations is low. At this stage there is not enough information to accurately estimate the bias introduced by each annotator separately, so we have to resort to models that consider the statistical links among them. In addition, finding these clusters is interesting in itself as knowing the behavior of the pool of annotators allows implementing efficient active learning strategies. Based on this, we propose in this paper two new fully unsupervised models based on a Chinese restaurant process (CRP) prior and a hierarchical structure that allows inferring these groups jointly with the ground truth and the properties of the users. Efficient inference algorithms based on Gibbs sampling with auxiliary variables are proposed. Finally, we perform experiments, both on synthetic and real databases, to show the advantages of our models over state-of-the-art algorithms.},
 author = {Pablo G. Moreno and Antonio Artes-Rodriguez and Yee Whye Teh and Fern and o Perez-Cruz},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W1839751919},
 pages = {1607--1627},
 title = {Bayesian nonparametric crowdsourcing},
 url = {http://jmlr.org/papers/v16/moreno15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:moroshko15a,
 abstract = {The goal of a learner in standard online learning, is to have the cumulative loss not much larger compared with the best-performing function from some fixed class. Numerous algorithms were shown to...},
 author = {Edward Moroshko and Nina Vaits and Koby Crammer},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W3017031875},
 pages = {1481--1517},
 title = {Second-order non-stationary online learning for regression},
 url = {http://jmlr.org/papers/v16/moroshko15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:nakajima15a,
 abstract = {Having shown its good performance in many applications, variational Bayesian (VB) learning is known to be one of the best tractable approximations to Bayesian learning. However, its performance was not well understood theoretically. In this paper, we clarify the behavior of VB learning in probabilistic PCA (or fully-observed matrix factorization). More specifically, we establish a necessary and sufficient condition for perfect dimensionality (or rank) recovery in the large-scale limit when the matrix size goes to infinity. Our result theoretically guarantees the performance of VB-PCA. At the same time, it also reveals the conservative nature of VB learning--it offers a low false positive rate at the expense of low sensitivity. By contrasting with an alternative dimensionality selection method, we characterize VB learning in PCA. In our analysis, we obtain bounds of the noise variance estimator, and a new and simple analytic-form solution for the other parameters, which themselves are useful for implementation of VB-PCA.},
 author = {Shinichi Nakajima and Ryota Tomioka and Masashi Sugiyama and S. Derin Babacan},
 journal = {Journal of Machine Learning Research},
 number = {114},
 openalex = {W2276728651},
 pages = {3757--3811},
 title = {Condition for perfect dimensionality recovery by variational Bayesian PCA},
 url = {http://jmlr.org/papers/v16/nakajima15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:neumann15a,
 abstract = {We introduce pyGPs, an object-oriented implementation of Gaussian processes (gps) for machine learning. The library provides a wide range of functionalities reaching from simple gp specification via mean and covariance and gp inference to more complex implementations of hyperparameter optimization, sparse approximations, and graph based learning. Using Python we focus on usability for both users and researchers. Our main goal is to offer a user-friendly and flexible implementation of GPs for machine learning.},
 author = {Marion Neumann and Shan Huang and Daniel E. Marthaler and Kristian Kersting},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W2277766262},
 pages = {2611--2616},
 title = {pyGPs: a Python library for Gaussian process regression and classification},
 url = {http://jmlr.org/papers/v16/neumann15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:nikulin15a,
 abstract = {In this paper we formulate in general terms an approach to prove strong consistency of the Empirical Risk Minimisation inductive principle applied to the prototype or distance based clustering. This approach was motivated by the Divisive Information-Theoretic Feature Clustering model in probabilistic space with Kullback-Leibler divergence, which may be regarded as a special case within the Clustering Minimisation framework.},
 author = {Vladimir Nikulin},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W1536072320},
 pages = {775--785},
 title = {Strong consistency of the prototype based clustering in probabilistic space},
 url = {http://jmlr.org/papers/v16/nikulin15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:ovcharov15a,
 abstract = {To discuss the existence and uniqueness of proper scoring rules one needs to extend the associated entropy functions as sublinear functions to the conic hull of the prediction set. In some natural function spaces, such as the Lebesgue $L^p$-spaces over $\mathbb R^d$, the positive cones have empty interior. Entropy functions defined on such cones have only directional derivatives. Certain entropies may be further extended continuously to open cones in normed spaces containing signed densities. The extended densities are Gâteaux differentiable except on a negligible set and have everywhere continuous subgradients due to the supporting hyperplane theorem. We introduce the necessary framework from analysis and algebra that allows us to give an affirmative answer to the titular question of the paper. As a result of this, we give a formal sense in which entropy functions have uniquely associated proper scoring rules. We illustrate our framework by studying the derivatives and subgradients of the following three prototypical entropies: Shannon entropy, Hyvarinen entropy, and quadratic entropy.},
 author = {Evgeni Y. Ovcharov},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W2950039555},
 pages = {2207--2230},
 title = {Existence and Uniqueness of Proper Scoring Rules},
 url = {http://jmlr.org/papers/v16/ovcharov15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:pati15a,
 abstract = {In Bayesian nonparametric models, Gaussian processes provide a popular prior choice for regression function estimation. Existing literature on the theoretical investigation of the resulting posterior distribution almost exclusively assume a fixed design for covariates. The only random design result we are aware of (van der Vaart and van Zanten, 2011) assumes the assigned Gaussian process to be supported on the smoothness class specified by the true function with probability one. This is a fairly restrictive assumption as it essentially rules out the Gaussian process prior with a squared exponential kernel when modeling rougher functions. In this article, we show that an appropriate rescaling of the above Gaussian process leads to a rate-optimal posterior distribution even when the covariates are independently realized from a known density on a compact set. The proofs are based on deriving sharp concentration inequalities for frequentist kernel estimators; the results might be of independent interest.},
 author = {Debdeep Pati and Anirban Bhattacharya and Guang Cheng},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W1758216174},
 pages = {2837--2851},
 title = {Optimal Bayesian estimation in random covariate design with a rescaled Gaussian process prior},
 url = {http://jmlr.org/papers/v16/pati15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:pitsikalis15a,
 abstract = {We present a new framework for multimodal gesture recognition that is based on a multiple hypotheses rescoring fusion scheme. We specifically deal with a demanding Kinect-based multimodal data set,...},
 author = {Vassilis Pitsikalis and Athanasios Katsamanis and Stavros Theodorakis and Petros Maragos},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W2997426117},
 pages = {255--284},
 title = {Multimodal gesture recognition via multiple hypotheses rescoring},
 url = {http://jmlr.org/papers/v16/pitsikalis15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:plumb15a,
 author = {Gregory Plumb and Deepti Pachauri and Risi Kondor and Vikas Singh},
 journal = {Journal of Machine Learning Research},
 number = {107},
 pages = {3469--3473},
 title = {SnFFT: A Julia Toolkit for Fourier Analysis of Functions over Permutations},
 url = {http://jmlr.org/papers/v16/plumb15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:pokarowski15a,
 author = {Piotr Pokarowski and Jan Mielniczuk},
 journal = {Journal of Machine Learning Research},
 number = {29},
 pages = {961--992},
 title = {Combined l1 and Greedy l0 Penalized Least Squares for Linear Model Selection},
 url = {http://jmlr.org/papers/v16/pokarowski15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:pourhabib15a,
 abstract = {We propose an algorithm for two-class classification problems when the training data are imbalanced. This means the number of training instances in one of the classes is so low that the conventional classification algorithms become ineffective in detecting the minority class. We present a modification of the kernel Fisher discriminant analysis such that the imbalanced nature of the problem is explicitly addressed in the new algorithm formulation. The new algorithm exploits the properties of the existing minority points to learn the effects of other minority data points, had they actually existed. The algorithm proceeds iteratively by employing the learned properties and conditional sampling in such a way that it generates sufficient artificial data points for the minority set, thus enhancing the detection probability of the minority class. Implementing the proposed method on a number of simulated and real data sets, we show that our proposed method performs competitively compared to a set of alternative state-of-the-art imbalanced classification algorithms.},
 author = {Arash Pourhabib and Bani K. Mallick and Yu Ding},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W2276069960},
 pages = {2695--2724},
 title = {Absent data generating classifier for imbalanced class sizes},
 url = {http://jmlr.org/papers/v16/pourhabib15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:prasse15a,
 abstract = {This paper addresses the problem of inferring a regular expression from a given set of strings that resembles, as closely as possible, the regular expression that a human expert would have written ...},
 author = {Paul Prasse and Christoph Sawade and Niels L and wehr and Tobias Scheffer},
 journal = {Journal of Machine Learning Research},
 number = {112},
 openalex = {W3006287016},
 pages = {3687--3720},
 title = {Learning to identify concise regular expressions that describe email campaigns},
 url = {http://jmlr.org/papers/v16/prasse15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:qiao15a,
 abstract = {Classification is an important topic in statistics and machine learning with great potential in many real applications. In this paper, we investigate two popular large-margin classification methods...},
 author = {Xingye Qiao and Lingsong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W3010187774},
 pages = {1547--1572},
 title = {Flexible high-dimensional classification machines and their asymptotic properties},
 url = {http://jmlr.org/papers/v16/qiao15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:qiu15a,
 abstract = {A low-rank transformation learning framework for subspace clustering and classification is proposed here. Many high-dimensional data, such as face images and motion sequences, approximately lie in ...},
 author = {Qiang Qiu and Guillermo Sapiro},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W3016464806},
 pages = {187--225},
 title = {Learning transformations for clustering and classification},
 url = {http://jmlr.org/papers/v16/qiu15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:rakhlin15a,
 abstract = {We consider the problem of sequential prediction and provide tools to study the minimax value of the associated game. Classical statistical learning theory provides several useful complexity measures to study learning with i.i.d. data. Our proposed sequential complexities can be seen as extensions of these measures to the sequential setting. The developed theory is shown to yield precise learning guarantees for the problem of sequential prediction. In particular, we show necessary and sufficient conditions for online learnability in the setting of supervised learning. Several examples show the utility of our framework: we can establish learnability without having to exhibit an explicit online learning algorithm.},
 author = {Alexander Rakhlin and Karthik Sridharan and Ambuj Tewari},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2963482921},
 pages = {155--186},
 title = {Online learning via sequential complexities},
 url = {http://jmlr.org/papers/v16/rakhlin15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:ravanbakhsh15a,
 abstract = {We introduce an efficient message passing scheme for solving Constraint Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and directly produce a single satisfying assignment. Our first CSP solver, called Perturbed Belief Propagation, smoothly interpolates two well-known inference procedures; it starts as BP and ends as a Gibbs sampler, which produces a single sample from the set of solutions. Moreover we apply a similar perturbation scheme to SP to produce another CSP solver, Perturbed Survey Propagation. Experimental results on random and real-world CSPs show that Perturbed BP is often more successful and at the same time tens to hundreds of times more efficient than standard BP guided decimation. Perturbed BP also compares favorably with state-of-the-art SP-guided decimation, which has a computational complexity that generally scales exponentially worse than our method (w.r.t. the cardinality of variable domains and constraints). Furthermore, our experiments with random satisfiability and coloring problems demonstrate that Perturbed SP can outperform SP-guided decimation, making it the best incomplete random CSP-solver in difficult regimes.},
 author = {Siamak Ravanbakhsh and Russell  Greiner},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2962897345},
 pages = {1249--1274},
 title = {Perturbed message passing for constraint satisfaction problems},
 url = {http://jmlr.org/papers/v16/ravanbakhsh15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:ryan15a,
 abstract = {Semi-supervised learning approaches are trained using the full training (labeled) data and available testing (unlabeled) data. Demonstrations of the value of training with unlabeled data typically depend on a smoothness assumption relating the conditional expectation to high density regions of the marginal distribution and an inherent missing completely at random assumption for the labeling. So-called covariate shift poses a challenge for many existing semi-supervised or supervised learning techniques. Covariate shift models allow the marginal distributions of the labeled and unlabeled feature data to differ, but the conditional distribution of the response given the feature data is the same. An example of this occurs when a complete labeled data sample and then an unlabeled sample are obtained sequentially, as it would likely follow that the distributions of the feature data are quite different between samples. The value of using unlabeled data during training for the elastic net is justified geometrically in such practical covariate shift problems. The approach works by obtaining adjusted coefficients for unlabeled prediction which recalibrate the supervised elastic net to compromise: (i) maintaining elastic net predictions on the labeled data with (ii) shrinking unlabeled predictions to zero. Our approach is shown to dominate linear supervised alternatives on unlabeled response predictions when the unlabeled feature data are concentrated on a low dimensional manifold away from the labeled data and the true coefficient vector emphasizes directions away from this manifold. Large variance of the supervised predictions on the unlabeled set is reduced more than the increase in squared bias when the unlabeled responses are expected to be small, so an improved compromise within the bias-variance tradeoff is the rationale for this performance improvement. Performance is validated on simulated and real data.},
 author = {Kenneth Joseph Ryan and Mark Vere Culp},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W2286239176},
 pages = {3183--3217},
 title = {On semi-supervised linear regression in covariate shift problems},
 url = {http://jmlr.org/papers/v16/ryan15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:sabato15a,
 abstract = {We consider the problem of learning a non-negative linear classifier with a l1-norm of at most k, and a fixed threshold, under the hinge-loss. This problem generalizes the problem of learning a k-monotone disjunction. We prove that we can learn efficiently in this setting, at a rate which is linear in both k and the size of the threshold, and that this is the best possible rate. We provide an efficient online learning algorithm that achieves the optimal rate, and show that in the batch case, empirical risk minimization achieves this rate as well. The rates we show are tighter than the uniform convergence rate, which grows with k2.},
 author = {Sivan Sabato and Shai Shalev-Shwartz and Nathan Srebro and Daniel Hsu and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W1819308084},
 pages = {1275--1304},
 title = {Learning sparse low-threshold linear classifiers},
 url = {http://jmlr.org/papers/v16/sabato15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:santhanam15a,
 abstract = {Motivated by problems in insurance, our task is to predict finite upper bounds on a future draw from an unknown distribution p over natural numbers. We can only use past observations generated independently and identically distributed according to p. While p is unknown, it is known to belong to a given collection P of probability distributions on the natural numbers.

The support of the distributions p ∈ P may be unbounded, and the prediction game goes on for infinitely many draws. We are allowed to make observations without predicting upper bounds for some time. But we must, with probability 1, start and then continue to predict upper bounds after a finite time irrespective of which p ∈ P governs the data.

If it is possible, without knowledge of p and for any prescribed confidence however close to 1, to come up with a sequence of upper bounds that is never violated over an infinite time window with confidence at least as big as prescribed, we say the model class P is insurable.

We completely characterize the insurability of any class P of distributions over natural numbers by means of a condition on how the neighborhoods of distributions in P should be, one that is both necessary and sufficient.},
 author = {Narayana Santhanam and Venkat Anantharam},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W1645893680},
 pages = {2329--2355},
 title = {Agnostic insurability of model classes},
 url = {http://jmlr.org/papers/v16/santhanam15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:scherrer15a,
 abstract = {Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of the well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analysis that unify those for approximate policy and value iteration. We develop the finite-sample analysis of these algorithms, which highlights the influence of their parameters. In the classification-based version of the algorithm (CBMPI), the analysis shows that MPI's main parameter controls the balance between the estimation error of the classifier and the overall value function approximation. We illustrate and evaluate the behavior of these new algorithms in the Mountain Car and Tetris problems. Remarkably, in Tetris, CBMPI outperforms the existing DP approaches by a large margin, and competes with the current state-of-the-art methods while using fewer samples.},
 author = {Bruno Scherrer and Mohammad Ghavamzadeh and Victor Gabillon and Boris Lesner and Matthieu Geist},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W1889629917},
 pages = {1629--1676},
 title = {Approximate modified policy iteration and its application to the game of Tetris},
 url = {http://jmlr.org/papers/v16/scherrer15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:schnass15a,
 abstract = {This paper presents the first theoretical results showing that stable identification of overcomplete $μ$-coherent dictionaries $Φ\in \mathbb{R}^{d\times K}$ is locally possible from training signals with sparsity levels $S$ up to the order $O(μ^{-2})$ and signal to noise ratios up to $O(\sqrt{d})$. In particular the dictionary is recoverable as the local maximum of a new maximisation criterion that generalises the K-means criterion. For this maximisation criterion results for asymptotic exact recovery for sparsity levels up to $O(μ^{-1})$ and stable recovery for sparsity levels up to $O(μ^{-2})$ as well as signal to noise ratios up to $O(\sqrt{d})$ are provided. These asymptotic results translate to finite sample size recovery results with high probability as long as the sample size $N$ scales as $O(K^3dS \tilde \varepsilon^{-2})$, where the recovery precision $\tilde \varepsilon$ can go down to the asymptotically achievable precision. Further, to actually find the local maxima of the new criterion, a very simple Iterative Thresholding and K (signed) Means algorithm (ITKM), which has complexity $O(dKN)$ in each iteration, is presented and its local efficiency is demonstrated in several experiments.},
 author = {Karin Schnass},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W2963337051},
 pages = {1211--1242},
 title = {Local Identification of Overcomplete Dictionaries},
 url = {http://jmlr.org/papers/v16/schnass15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:shamir15a,
 abstract = {We provide a tight sample complexity bound for learning bounded-norm linear predictors with respect to the squared loss. Our focus is on an agnostic PAC-style setting, where no assumptions are made on the data distribution beyond boundedness. This contrasts with existing results in the literature, which rely on other distributional assumptions, refer to specific parameter settings, or use other performance measures.},
 author = {Ohad Shamir},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W349434659},
 pages = {3475--3486},
 title = {The sample complexity of learning linear predictors with the squared loss},
 url = {http://jmlr.org/papers/v16/shamir15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:statnikov15a,
 abstract = {Discovery of causal relations from data is a fundamental objective of several scientific disciplines. Most causal discovery algorithms that use observational data can infer causality only up to a statistical equivalency class, thus leaving many causal relations undetermined. In general, complete identification of causal relations requires experimentation to augment discoveries from observational data. This has led to the recent development of several methods for active learning of causal networks that utilize both observational and experimental data in order to discover causal networks. In this work, we focus on the problem of discovering local causal pathways that contain only direct causes and direct effects of the target variable of interest and propose new discovery methods that aim to minimize the number of required experiments, relax common sufficient discovery assumptions in order to increase discovery accuracy, and scale to high-dimensional data with thousands of variables. We conduct a comprehensive evaluation of new and existing methods with data of dimensionality up to 1,000,000 variables. We use both artificially simulated networks and in-silico gene transcriptional networks that model the characteristics of real gene expression data.},
 author = {Alexander Statnikov and Sisi Ma and Mikael Henaff and Nikita Lytkin and Efstratios Efstathiadis and Eric R. Peskin and Constantin F. Aliferis},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2266375225},
 pages = {3219--3267},
 title = {Ultra-scalable and efficient methods for hybrid observational and experimental local causal pathway discovery},
 url = {http://jmlr.org/papers/v16/statnikov15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:sunehag15a,
 abstract = {In this article, we present a top-down theoretical study of general reinforcement learning agents. We begin with rational agents with unlimited resources and then move to a setting where an agent can only maintain a limited number of hypotheses and optimizes plans over a horizon much shorter than what the agent designer actually wants. We axiomatize what is rational in such a setting in a manner that enables optimism, which is important to achieve systematic explorative behavior. Then, within the class of agents deemed rational, we achieve convergence and finite-error bounds. Such results are desirable since they imply that the agent learns well from its experiences, but the bounds do not directly guarantee good performance and can be achieved by agents doing things one should obviously not. Good performance cannot in fact be guaranteed for any agent in fully general settings. Our approach is to design agents that learn well from experience and act rationally. We introduce a framework for general reinforcement learning agents based on rationality axioms for a decision function and an hypothesis-generating function designed so as to achieve guarantees on the number errors. We will consistently use an optimistic decision function but the hypothesis-generating function needs to change depending on what is known/assumed. We investigate a number of natural situations having either a frequentist or Bayesian flavor, deterministic or stochastic environments and either finite or countable hypothesis class. Further, to achieve sufficiently good bounds as to hold promise for practical success we introduce a notion of a class of environments being generated by a set of laws. None of the above has previously been done for fully general reinforcement learning environments.},
 author = {Peter Sunehag and Marcus Hutter},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W1876044947},
 pages = {1345--1390},
 title = {Rationality, optimism and guarantees in general reinforcement learning},
 url = {http://jmlr.org/papers/v16/sunehag15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:swaminathan15a,
 abstract = {We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem (Bottou et al., 2013) through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. In analogy to the Structural Risk Minimization principle of Wapnik and Tscherwonenkis (1979), these constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method--called Policy Optimizer for Exponential Models (POEM)--for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. The effectiveness and efficiency of POEM is evaluated on several simulated multi-label classification problems, as well as on a real-world information retrieval problem. The empirical results show that the CRM objective implemented in POEM provides improved robustness and generalization performance compared to the state-of-the-art.},
 author = {Adith Swaminathan and Thorsten Joachims},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W1835900096},
 pages = {1731--1755},
 title = {Batch learning from logged bandit feedback through counterfactual risk minimization},
 url = {http://jmlr.org/papers/v16/swaminathan15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:taleghan15a,
 abstract = {In a simulator-defined MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efficient exploration and an improved confidence interval that enables earlier termination with probabilistic guarantees. We prove that the heuristics and the confidence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved confidence intervals and the new search heuristics yield reductions of between 8% and 47% in the number of simulator calls required to reach near-optimal policies.},
 author = {Majid Alkaee Taleghan and Thomas G. Dietterich and Mark Crowley and Kim Hall and H. Jo Albers},
 journal = {Journal of Machine Learning Research},
 number = {117},
 openalex = {W2274670401},
 pages = {3877--3903},
 title = {PAC optimal MDP planning with application to invasive species management},
 url = {http://jmlr.org/papers/v16/taleghan15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:thomann15a,
 abstract = {We propose some axioms for hierarchical clustering of probability measures and investigate their ramifications. The basic idea is to let the user stipulate the clusters for some elementary measures. This is done without the need of any notion of metric, similarity or dissimilarity. Our main results then show that for each suitable choice of user-defined clustering on elementary measures we obtain a unique notion of clustering on a large set of distributions satisfying a set of additivity and continuity axioms. We illustrate the developed theory by numerous examples including some with and some without a density.},
 author = {Philipp Thomann and Ingo Steinwart and Nico Schmid},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W4293771442},
 pages = {1949--2002},
 title = {Towards an Axiomatic Approach to Hierarchical Clustering of Measures},
 url = {http://jmlr.org/papers/v16/thomann15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:thon15a,
 abstract = {Stochastic multiplicity automata (SMA) are weighted finite automata that generalize probabilistic automata. They have been used in the context of probabilistic grammatical inference. Observable operator models (OOMs) are a generalization of hidden Markov models, which in turn are models for discrete-valued stochastic processes and are used ubiquitously in the context of speech recognition and bio-sequence modeling. Predictive state representations (PSRs) extend OOMs to stochastic input-output systems and are employed in the context of agent modeling and planning.

We present SMA, OOMs, and PSRs under the common framework of sequential systems, which are an algebraic characterization of multiplicity automata, and examine the precise relationships between them. Furthermore, we establish a unified approach to learning such models from data. Many of the learning algorithms that have been proposed can be understood as variations of this basic learning scheme, and several turn out to be closely related to each other, or even equivalent.},
 author = {Michael Thon and Herbert Jaeger},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2117421927},
 pages = {103--147},
 title = {Links between multiplicity automata, observable operator models and predictive state representations: a unified learning framework},
 url = {http://jmlr.org/papers/v16/thon15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:tibshirani15a,
 abstract = {Forward stagewise regression follows a very simple strategy for constructing a sequence of sparse regression estimates: it starts with all coefficients equal to zero, and iteratively updates the coefficient (by a small amount e) of the variable that achieves the maximal absolute inner product with the current residual. This procedure has an interesting connection to the lasso: under some conditions, it is known that the sequence of forward stagewise estimates exactly coincides with the lasso path, as the step size e goes to zero. Furthermore, essentially the same equivalence holds outside of least squares regression, with the minimization of a differentiable convex loss function subject to an l1 norm constraint (the stagewise algorithm now updates the coefficient corresponding to the maximal absolute component of the gradient).

Even when they do not match their l1-constrained analogues, stagewise estimates provide a useful approximation, and are computationally appealing. Their success in sparse modeling motivates the question: can a simple, effective strategy like forward stagewise be applied more broadly in other regularization settings, beyond the l1 norm and sparsity? The current paper is an attempt to do just this. We present a general framework for stagewise estimation, which yields fast algorithms for problems such as group-structured learning, matrix completion, image denoising, and more.},
 author = {Ryan J. Tibshirani},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W1365889},
 pages = {2543--2588},
 title = {A general framework for fast stagewise algorithms},
 url = {http://jmlr.org/papers/v16/tibshirani15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:trandihn15a,
 abstract = {We propose a variable metric framework for minimizing the sum of a self-concordant function and a possibly non-smooth convex function, endowed with an easily computable proximal operator. We theoretically establish the convergence of our framework without relying on the usual Lipschitz gradient assumption on the smooth part. An important highlight of our work is a new set of analytic step-size selection and correction procedures based on the structure of the problem. We describe concrete algorithmic instances of our framework for several interesting applications and demonstrate them numerically on both synthetic and real data.},
 author = {Quoc Tran-Dinh and Anastasios Kyrillidis and Volkan Cevher},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W1824074430},
 pages = {371--416},
 title = {Composite self-concordant minimization},
 url = {http://jmlr.org/papers/v16/trandihn15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:triantafillou15a,
 abstract = {Scientific practice typically involves repeatedly studying a system, each time trying to unravel a different perspective. In each study, the scientist may take measurements under different experimental conditions (interventions, manipulations, perturbations) and measure different sets of quantities (variables). The result is a collection of heterogeneous data sets coming from different data distributions. In this work, we present algorithm COmbINE, which accepts a collection of data sets over overlapping variable sets under different experimental conditions; COmbINE then outputs a summary of all causal models indicating the invariant and variant structural characteristics of all models that simultaneously fit all of the input data sets. COmbINE converts estimated dependencies and independencies in the data into path constraints on the data-generating causal model and encodes them as a SAT instance. The algorithm is sound and complete in the sample limit. To account for conflicting constraints arising from statistical errors, we introduce a general method for sorting constraints in order of confidence, computed as a function of their corresponding p-values. In our empirical evaluation, COmbINE outperforms in terms of efficiency the only pre-existing similar algorithm; the latter additionally admits feedback cycles, but does not admit conflicting constraints which hinders the applicability on real data. As a proof-of-concept, COmbINE is employed to co-analyze 4 real, mass-cytometry data sets measuring phosphorylated protein concentrations of overlapping protein sets under 3 different interventions.},
 author = {Sofia Triantafillou and Ioannis Tsamardinos},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W2207421686},
 pages = {2147--2205},
 title = {Constraint-based causal discovery from multiple interventions over overlapping variable sets},
 url = {http://jmlr.org/papers/v16/triantafillou15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:vanerven15a,
 abstract = {The speed with which a learning algorithm converges as it is presented with more data is a central problem in machine learning -- a fast rate of convergence means less data is needed for the same level of performance. The pursuit of fast rates in online and statistical learning has led to the discovery of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for 'proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that under surprisingly weak assumptions both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov margin condition, both of which have played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying conditions thus provide a substantial step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.},
 author = {Tim van Erven and Peter D. Gr{{\"u}}nwald and Nishant A. Mehta and Mark D. Reid and Robert C. Williamson},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W1831843218},
 pages = {1793--1861},
 title = {Fast rates in statistical and online learning},
 url = {http://jmlr.org/papers/v16/vanerven15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:vapnik15a,
 abstract = {This paper presents direct settings and rigorous solutions of the main Statistical Inference problems. It shows that rigorous solutions require solving multidimensional Fredholm integral equations of the first kind in the situation where not only the right-hand side of the equation is an approximation, but the operator in the equation is also defined approximately. Using Stefanuyk-Vapnik theory for solving such ill-posed operator equations, constructive methods of empirical inference are introduced. These methods are based on a new concept called V-matrix. This matrix captures geometric properties of the observation data that are ignored by classical statistical methods.},
 author = {Vladimir Vapnik and Rauf Izmailov},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W1854718785},
 pages = {1683--1730},
 title = {V-matrix method of solving statistical inference problems},
 url = {http://jmlr.org/papers/v16/vapnik15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:vapnik15b,
 abstract = {This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classification of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for significantly accelerating the speed of Student's learning using privileged information: (1) correction of Student's concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer.},
 author = {Vladimir Vapnik and Rauf Izmailov},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W2173379916},
 pages = {2023--2049},
 title = {Learning using privileged information: similarity control and knowledge transfer},
 url = {http://jmlr.org/papers/v16/vapnik15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:varando15a,
 abstract = {Bayesian network classifiers are a powerful machine learning tool. In order to evaluate the expressive power of these models, we compute families of polynomials that sign-represent decision functions induced by Bayesian network classifiers. We prove that those families are linear combinations of products of Lagrange basis polynomials. In absence of V-structures in the predictor sub-graph, we are also able to prove that this family of polynomials does indeed characterize the specific classifier considered. We then use this representation to bound the number of decision functions representable by Bayesian network classifiers with a given structure.},
 author = {Gherardo Var and o and Concha Bielza and Pedro Larranaga},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W2134283720},
 pages = {2725--2749},
 title = {Decision boundary for discrete Bayesian network classifiers},
 url = {http://jmlr.org/papers/v16/varando15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:wang15a,
 abstract = {Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no exact screening rule for group Lasso. We have evaluated our screening rule using many real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso.},
 author = {Jie Wang and Peter Wonka and Jieping Ye},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W2097620183},
 pages = {1063--1101},
 title = {Lasso Screening Rules via Dual Polytope Projection},
 url = {http://jmlr.org/papers/v16/wang15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:wang15b,
 abstract = {A new method is proposed for estimating derivatives of a nonparametric regression function. By applying Taylor expansion technique to a derived symmetric difference sequence, we obtain a sequence of approximate linear regression representation in which the derivative is just the intercept term. Using locally weighted least squares, we estimate the derivative in the linear regression model. The estimator has less bias in both valleys and peaks of the true derivative function. For the special case of a domain with equispaced design points, the asymptotic bias and variance are derived; consistency and asymptotic normality are established. In simulations our estimators have less bias and mean square error than its main competitors, especially second order derivative estimator.},
 author = {WenWu Wang and Lu Lin},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W2275812670},
 pages = {2617--2641},
 title = {Derivative estimation based on difference sequence via locally weighted least squares regression},
 url = {http://jmlr.org/papers/v16/wang15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:watanabe15a,
 abstract = {The normalized maximum likelihood distribution achieves minimax coding (log-loss) regret given a fixed sample size, or horizon, n. It generally requires that n be known in advance. Furthermore, extracting the sequential predictions from the normalized maximum likelihood distribution is computationally infeasible for most statistical models. Several computationally feasible alternative strategies have been devised. We characterize the achievability of asymptotic minimaxity by horizon-dependent and horizon-independent strategies. We prove that no horizon-independent strategy can be asymptotically minimax in the multinomial case. A weaker result is given in the general case subject to a condition on the horizon-dependence of the normalized maximum likelihood. Motivated by these negative results, we demonstrate that an easily implementable Bayes mixture based on a conjugate Dirichlet prior with a simple dependency on n achieves asymptotic minimaxity for all sequences, simplifying earlier similar proposals. Our numerical experiments for the Bernoulli model demonstrate improved finite-sample performance by a number of novel horizon-dependent and horizon-independent algorithms.},
 author = {Kazuho Watanabe and Teemu Roos},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2219485572},
 pages = {2357--2375},
 title = {Achievability of asymptotic minimax regret by horizon-dependent and horizon-independent strategies},
 url = {http://jmlr.org/papers/v16/watanabe15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:weninger15a,
 abstract = {In this article, we introduce CURRENNT, an open-source parallel implementation of deep recurrent neural networks (RNNs) supporting graphics processing units (GPUs) through NVIDIA's Computed Unified Device Architecture (CUDA). CURRENNT supports uni- and bidirectional RNNs with Long Short-Term Memory (LSTM) memory cells which overcome the vanishing gradient problem. To our knowledge, CURRENNT is the first publicly available parallel implementation of deep LSTM-RNNs. Benchmarks are given on a noisy speech recognition task from the 2013 2nd CHiME Speech Separation and Recognition Challenge, where LSTM-RNNs have been shown to deliver best performance. In the result, double digit speedups in bidirectional LSTM training are achieved with respect to a reference single-threaded CPU implementation. CURRENNT is available under the GNU General Public License from http://sourceforge.net/p/currennt.},
 author = {Felix Weninger},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W304834817},
 pages = {547--551},
 title = {Introducing CURRENNT: the Munich open-source CUDA recurrent neural network toolkit},
 url = {http://jmlr.org/papers/v16/weninger15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:wiener15a,
 abstract = {We introduce a new and improved characterization of the label complexity of disagreement-based active learning, in which the leading quantity is the version space compression set size. This quantity is defined as the size of the smallest subset of the training data that induces the same version space. We show various applications of the new characterization, including a tight analysis of CAL and refined label complexity bounds for linear separators under mixtures of Gaussians and axis-aligned rectangles under product densities. The version space compression set size, as well as the new characterization of the label complexity, can be naturally extended to agnostic learning problems, for which we show new speedup results for two well known active learning algorithms.},
 author = {Yair Wiener and Steve Hanneke and Ran El-Yaniv},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2963974045},
 pages = {713--745},
 title = {A compression technique for analyzing disagreement-based active learning},
 url = {http://jmlr.org/papers/v16/wiener15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:yan15a,
 abstract = {In multi-response regression, pursuit of two different types of structures is essential to battle the curse of dimensionality. In this paper, we seek a sparsest decomposition representation of a pa...},
 author = {Qi Yan and Jieping Ye and Xiaotong Shen},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W3021777356},
 pages = {47--75},
 title = {Simultaneous pursuit of sparseness and rank structures for matrix decomposition},
 url = {http://jmlr.org/papers/v16/yan15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:yang15a,
 abstract = {Undirected graphical models, or Markov networks, are a popular class of statistical models, used in a wide variety of applications. Popular instances of this class include Gaussian graphical models and Ising models. In many settings, however, it might not be clear which subclass of graphical models to use, particularly for non-Gaussian and non-categorical data. In this paper, we consider a general sub-class of graphical models where the node-wise conditional distributions arise from exponential families. This allows us to derive multivariate graphical model distributions from univariate exponential family distributions, such as the Poisson, negative binomial, and exponential distributions. Our key contributions include a class of M-estimators to fit these graphical model distributions; and rigorous statistical analysis showing that these M-estimators recover the true graphical model structure exactly, with high probability. We provide examples of genomic and proteomic networks learned via instances of our class of graphical models derived from Poisson and exponential distributions.},
 author = {Eunho Yang and Pradeep Ravikumar and Genevera I. Allen and Zhandong Liu},
 journal = {Journal of Machine Learning Research},
 number = {115},
 openalex = {W2952350108},
 pages = {3813--3847},
 title = {On Graphical Models via Univariate Exponential Family Distributions},
 url = {http://jmlr.org/papers/v16/yang15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:zhang15a,
 abstract = {CEKA is a software package for developers and researchers to mine the wisdom of crowds. It makes the entire knowledge discovery procedure much easier, including analyzing qualities of workers, simulating labeling behaviors, inferring true class labels of instances, filtering and correcting mislabeled instances (noise), building learning models and evaluating them. It integrates a set of state-of-the-art inference algorithms, a set of general noise handling algorithms, and abundant functions for model training and evaluation. CEKA is written in Java with core classes being compatible with the well-known machine learning tool WEKA, which makes the utilization of the functions in WEKA much easier.},
 author = {Jing Zhang and Victor S. Sheng and Bryce A. Nicholson and Xindong Wu},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W2285341298},
 pages = {2853--2858},
 title = {CEKA: a tool for mining the wisdom of crowds},
 url = {http://jmlr.org/papers/v16/zhang15a.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:zhang15b,
 author = {Jian Zhang and Chao Liu},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W2485273068},
 pages = {2099--2145},
 title = {Linearly Constrained Minimum Variance Beamforming},
 url = {http://jmlr.org/papers/v16/zhang15b.html},
 volume = {16},
 year = {2015}
}

@article{JMLR:v16:zhang15d,
 abstract = {We establish optimal convergence rates for a decomposition-based scalable approach to kernel ridge regression. The method is simple to describe: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent kernel ridge regression estimator for each subset, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all N samples. Our two main theorems establish that despite the computational speed-up, statistical optimality is retained: as long as m is not too large, the partition-based estimator achieves the statistical minimax rate over all estimators using the set of N samples. As concrete examples, our theory guarantees that the number of processors m may grow nearly linearly for finite-rank kernels and Gaussian kernels and polynomially in N for Sobolev spaces, which in turn allows for substantial reductions in computational cost. We conclude with experiments on both simulated data and a music-prediction task that complement our theoretical results, exhibiting the computational and statistical benefits of our approach.},
 author = {Yuchen Zhang and John Duchi and Martin Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W1847890728},
 pages = {3299--3340},
 title = {Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates},
 url = {http://jmlr.org/papers/v16/zhang15d.html},
 volume = {16},
 year = {2015}
}
