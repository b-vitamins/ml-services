@article{JMLR:v10:abeel09a,
 abstract = {Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classifiers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license.},
 author = {Thomas Abeel and Yves Van de Peer and Yvan Saeys},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W1760882240},
 pages = {931--934},
 title = {Java-ML: A Machine Learning Library},
 url = {http://jmlr.org/papers/v10/abeel09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:abernethy09a,
 abstract = {We present a general approach for collaborative filtering (CF) using spectral regularization to learn linear operators from "users" to the "objects" they rate. Recent low-rank type matrix completion approaches to CF are shown to be special cases. However, unlike existing regularization based CF methods, our approach can be used to also incorporate information such as attributes of the users or the objects -- a limitation of existing regularization based CF methods. We then provide novel representer theorems that we use to develop new estimation methods. We provide learning algorithms based on low-rank decompositions, and test them on a standard CF dataset. The experiments indicate the advantages of generalizing the existing regularization based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach.},
 author = {Jacob Abernethy and Francis Bach and Theodoros Evgeniou and Jean-Philippe Vert},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W2116413942},
 pages = {803--826},
 title = {A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization},
 url = {http://jmlr.org/papers/v10/abernethy09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:agarwal09a,
 abstract = {The problem of ranking, in which the goal is to learn a real-valued ranking function that induces a ranking or ordering over an instance space, has recently gained much attention in machine learning. We study generalization properties of ranking algorithms using the notion of algorithmic stability; in particular, we derive generalization bounds for ranking algorithms that have good stability properties. We show that kernel-based ranking algorithms that perform regularization in a reproducing kernel Hilbert space have such stability properties, and therefore our bounds can be applied to these algorithms; this is in contrast with generalization bounds based on uniform convergence, which in many cases cannot be applied to these algorithms. Our results generalize earlier results that were derived in the special setting of bipartite ranking (Agarwal and Niyogi, 2005) to a more general setting of the ranking problem that arises frequently in applications.},
 author = {Shivani Agarwal and Partha Niyogi},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W2144064004},
 pages = {441--474},
 title = {Generalization Bounds for Ranking Algorithms via Algorithmic Stability},
 url = {http://jmlr.org/papers/v10/agarwal09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:angluin09a,
 abstract = {We define a model of learning probabilistic acyclic circuits using value injection queries, in which fixed values are assigned to an arbitrary subset of the wires and the value on the single output wire is observed. We adapt the approach of using test paths from the Circuit Builder algorithm (Angluin et al., 2009) to show that there is a polynomial time algorithm that uses value injection queries to learn acyclic Boolean probabilistic circuits of constant fan-in and log depth. We establish upper and lower bounds on the attenuation factor for general and transitively reduced Boolean probabilistic circuits of test paths versus general experiments. We give computational evidence that a polynomial time learning algorithm using general value injection experiments may not do much better than one using test paths. For probabilistic circuits with alphabets of size three or greater, we show that the test path lemmas (Angluin et al., 2009, 2008b) fail utterly. To overcome this obstacle, we introduce function injection queries, in which the values on a wire may be mapped to other values rather than just to themselves or constants, and prove a generalized test path lemma for this case.},
 author = {Dana Angluin and James Aspnes and Jiang Chen and David Eisenstat and Lev Reyzin},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W2150479827},
 pages = {1881--1911},
 title = {Learning Acyclic Probabilistic Circuits Using Test Paths},
 url = {http://jmlr.org/papers/v10/angluin09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:argyriou09a,
 abstract = {We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the inner product then the learned vector is a linear combination of the input data. This result, known as the {\em representer theorem}, is at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, thereby completing the characterization of kernel methods based on regularization. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufficient condition for these class of matrix regularizers and highlight them with some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing function.},
 author = {Andreas Argyriou and Charles A. Micchelli and Massimiliano Pontil},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W4302236756},
 pages = {2507--2529},
 title = {When is there a representer theorem? Vector versus matrix regularizers},
 url = {http://jmlr.org/papers/v10/argyriou09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:arlot09a,
 abstract = {Penalization procedures often suffer from their dependence on multiplying factors, whose optimal values are either unknown or hard to estimate from the data. We propose a completely data-driven calibration algorithm for this parameter in the least-squares regression framework, without assuming a particular shape for the penalty. Our algorithm relies on the concept of minimal penalty, recently introduced by Birge and Massart (2007) in the context of penalized least squares for Gaussian homoscedastic regression. On the positive side, the minimal penalty can be evaluated from the data themselves, leading to a data-driven estimation of an optimal penalty which can be used in practice; on the negative side, their approach heavily relies on the homoscedastic Gaussian nature of their stochastic framework. The purpose of this paper is twofold: stating a more general heuristics for designing a data-driven penalty (the slope heuristics) and proving that it works for penalized least-squares regression with a random design, even for heteroscedastic non-Gaussian data. For technical reasons, some exact mathematical results will be proved only for regressogram bin-width selection. This is at least a first step towards further results, since the approach and the method that we use are indeed general.},
 author = {Sylvain Arlot and Pascal Massart},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W2126750886},
 pages = {245--279},
 title = {Data-driven calibration of penalties for least-squares regression},
 url = {http://jmlr.org/papers/v10/arlot09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:bickel09a,
 abstract = {We address classification problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution---problems also referred t...},
 author = {Steffen Bickel and Michael Br{{\"u}}ckner and Tobias Scheffer},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W3007501395},
 pages = {2137--2155},
 title = {Discriminative Learning Under Covariate Shift},
 url = {http://jmlr.org/papers/v10/bickel09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:blanchard09a,
 abstract = {In the context of multiple hypothesis testing, the proportion Ï€0 of true null hypotheses in the pool of hypotheses to test often plays a crucial role, although it is generally unknown a priori. A testing procedure using an implicit or explicit estimate of this quantity in order to improve its efficency is called adaptive. In this paper, we focus on the issue of false discovery rate (FDR) control and we present new adaptive multiple testing procedures with control of the FDR. In a first part, assuming independence of the p-values, we present two new procedures and give a unified review of other existing adaptive procedures that have provably controlled FDR. We report extensive simulation results comparing these procedures and testing their robustness when the independence assumption is violated. The new proposed procedures appear competitive with existing ones. The overall best, though, is reported to be Storey's estimator, albeit for a specific parameter setting that does not appear to have been considered before. In a second part, we propose adaptive versions of step-up procedures that have provably controlled FDR under positive dependence and unspecified dependence of the p-values, respectively. In the latter case, while simulations only show an improvement over non-adaptive procedures in limited situations, these are to our knowledge among the first theoretically founded adaptive multiple testing procedures that control the FDR when the p-values are not independent.},
 author = {Gilles Blanchard and &Eacute;tienne Roquain},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W2147752708},
 pages = {2837--2871},
 title = {Adaptive false discovery rate control under independence and dependence},
 url = {http://jmlr.org/papers/v10/blanchard09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:bordes09a,
 abstract = {The SGD-QN algorithm is a stochastic gradient descent algorithm that makes careful use of second-order information and splits the parameter update into independently scheduled components. Thanks to this design, SGD-QN iterates nearly as fast as a first-order stochastic gradient descent but requires less iterations to achieve the same accuracy. This algorithm won the Wild Track of the first PASCAL Large Scale Learning Challenge (Sonnenburg et al., 2008).},
 author = {Antoine Bordes and L{{\'e}}on Bottou and Patrick Gallinari},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W2137515395},
 pages = {1737--1754},
 title = {SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent},
 url = {http://jmlr.org/papers/v10/bordes09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:boulle09a,
 abstract = {With the rapid growth of computer storage capacities, available data and demand for scoring models both follow an increasing trend, sharper than that of the processing power. However, the main limi...},
 author = {Marc Boull{{\'e}}},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W3106451500},
 pages = {1367--1385},
 title = {A Parameter-Free Classification Method for Large Scale Learning},
 url = {http://jmlr.org/papers/v10/boulle09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:bromberg09a,
 abstract = {We address the problem of improving the reliability of independence-based causal discovery algorithms that results from the execution of statistical independence tests on small data sets, which typically have low reliability. We model the problem as a knowledge base containing a set of independence facts that are related through Pearl's well-known axioms. Statistical tests on finite data sets may result in errors in these tests and inconsistencies in the knowledge base. We resolve these inconsistencies through the use of an instance of the class of defeasible logics called argumentation, augmented with a preference function, that is used to reason about and possibly correct errors in these tests. This results in a more robust conditional independence test, called an argumentative independence test. Our experimental evaluation shows clear positive improvements in the accuracy of argumentative over purely statistical tests. We also demonstrate significant improvements on the accuracy of causal structure discovery from the outcomes of independence tests both on sampled data from randomly generated causal models and on real-world data sets.},
 author = {Facundo Bromberg and Dimitris Margaritis},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W2153409221},
 pages = {301--340},
 title = {Improving the Reliability of Causal Discovery from Small Data Sets Using Argumentation},
 url = {http://jmlr.org/papers/v10/bromberg09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:brunskill09a,
 abstract = {To quickly achieve good performance, reinforcement-learning algorithms for acting in large continuous-valued domains must use a representation that is both sufficiently powerful to capture important domain characteristics, and yet simultaneously allows generalization, or sharing, among experiences. Our algorithm balances this tradeoff by using a stochastic, switching, parametric dynamics representation. We argue that this model characterizes a number of significant, real-world domains, such as robot navigati on across varying terrain. We prove that this representational assumption allows our algorithm to be probably approximately correct with a sample complexity that scales polynomially with all problem-specific quantities including the state-space dimension. We also explicitly incorporate the error introduced by approximate planning in our sample complexity bounds, in contrast to prior Probably Approximately Correct (PAC) Markov Decision Processes (MDP) approaches, which typically assume the estimated MDP can be solved exactly. Our experimental results on constructing plans for driving to work using real car trajectory data, as well as a small robot experiment on navigating varying terrain, demonstrate that our dynamics representation enables us to capture real-world dynamics in a sufficient manner to produce good performance.},
 author = {Emma Brunskill and Bethany R. Leffler and Lihong Li and Michael L. Littman and Nicholas Roy},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W2162206751},
 pages = {1955--1988},
 title = {Provably Efficient Learning with Typed Parametric Models},
 url = {http://jmlr.org/papers/v10/brunskill09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:bubeck09a,
 abstract = {Clustering is often formulated as a discrete optimization problem. The objective is to find, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the finite data set has been sampled from some underlying space, the goal is not to find the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal, and instead can lead to inconsistency. We construct examples which provably have this behavior. As in the case of supervised learning, the cure is to restrict the size of the function classes under consideration. For appropriate small function classes we can prove very general consistency theorems for clustering optimization schemes. As one particular algorithm for clustering with a restricted function space we introduce nearest neighbor clustering. Similar to the k-nearest neighbor classifier in supervised learning, this algorithm can be seen as a general baseline algorithm to minimize arbitrary clustering objective functions. We prove that it is statistically consistent for all commonly used clustering objective functions.},
 author = {S{{\'e}}bastien Bubeck and Ulrike von Luxburg},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2171813245},
 pages = {657--698},
 title = {Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions},
 url = {http://jmlr.org/papers/v10/bubeck09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:chen09a,
 abstract = {This paper reviews and extends the field of similarity-based classification, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classification problems. Specifically, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning.},
 author = {Yihua Chen and Eric K. Garcia and Maya R. Gupta and Ali Rahimi and Luca Cazzanti},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W2115933183},
 pages = {747--776},
 title = {Similarity-based Classification: Concepts and Algorithms},
 url = {http://jmlr.org/papers/v10/chen09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:chen09b,
 abstract = {Nearest neighbor graphs are widely used in data mining and machine learning. A brute-force method to compute the exact kNN graph takes Î˜(dn2) time for n data points in the d dimensional Euclidean s...},
 author = {Jie Chen and Haw-ren Fang and Yousef Saad},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W3005838368},
 pages = {1989--2012},
 title = {Fast Approximate kNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection},
 url = {http://jmlr.org/papers/v10/chen09b.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:dasgupta09a,
 abstract = {We start by showing that in an active learning setting, the Perceptron algorithm needs $\Omega(\frac{1}{\epsilon ^{2}})$ labels to learn linear separators within generalization error Îµ. We then present a simple selective sampling algorithm for this problem, which combines a modification of the perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error Îµ after asking for just ${\tilde O}(d log \frac{1}{\epsilon})$ labels. This exponential improvement over the usual sample complexity of supervised learning has previously been demonstrated only for the computationally more complex query-by-committee algorithm.},
 author = {Sanjoy Dasgupta and Adam Tauman Kalai and Claire Monteleoni},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W2143521632},
 pages = {281--299},
 title = {Analysis of Perceptron-Based Active Learning},
 url = {http://jmlr.org/papers/v10/dasgupta09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:delcoz09a,
 abstract = {Nondeterministic classifiers are defined as those allowed to predict more than one class for some entries from an input space. Given that the true class should be included in predictions and the number of classes predicted should be as small as possible, these kind of classifiers can be considered as Information Retrieval (IR) procedures. In this paper, we propose a family of IR loss functions to measure the performance of nondeterministic learners. After discussing such measures, we derive an algorithm for learning optimal nondeterministic hypotheses. Given an entry from the input space, the algorithm requires the posterior probabilities to compute the subset of classes with the lowest expected loss. From a general point of view, nondeterministic classifiers provide an improvement in the proportion of predictions that include the true class compared to their deterministic counterparts; the price to be paid for this increase is usually a tiny proportion of predictions with more than one class. The paper includes an extensive experimental study using three deterministic learners to estimate posterior probabilities: a multiclass Support Vector Machine (SVM), a Logistic Regression, and a Naive Bayes. The data sets considered comprise both UCI multi-class learning tasks and microarray expressions of different kinds of cancer. We successfully compare nondeterministic classifiers with other alternative approaches. Additionally, we shall see how the quality of posterior probabilities (measured by the Brier score) determines the goodness of nondeterministic predictions.},
 author = {Juan Jos{{\'e}} del Coz and Jorge D{{\'i}}ez and Antonio Bahamonde},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W2124232193},
 pages = {2273--2293},
 title = {Learning Nondeterministic Classifiers},
 url = {http://jmlr.org/papers/v10/delcoz09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:drton09a,
 abstract = {In recursive linear models, the multivariate normal joint distribution of all variables exhibits a dependence structure induced by a recursive (or acyclic) system of linear structural equations. These linear models have a long tradition and appear in seemingly unrelated regressions, structural equation modelling, and approaches to causal inference. They are also related to Gaussian graphical models via a classical representation known as a path diagram. Despite the models' long history, a number of problems remain open. In this paper, we address the problem of computing maximum likelihood estimates in the subclass of `bow-free' recursive linear models. The term `bow-free' refers to the condition that the errors for variables $i$ and $j$ be uncorrelated if variable $i$ occurs in the structural equation for variable $j$. We introduce a new algorithm, termed Residual Iterative Conditional Fitting (RICF), that can be implemented using only least squares computations. In contrast to existing algorithms, RICF has clear convergence properties and finds parameter estimates in closed form whenever possible.},
 author = {Mathias Drton and Michael Eichler and Thomas S. Richardson},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W2122634476},
 pages = {2329--2348},
 title = {Computing maximum likelihood estimates in recursive linear models with correlated errors},
 url = {http://jmlr.org/papers/v10/drton09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:duchi09a,
 abstract = {We describe, analyze, and experiment with a framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we first perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the first phase. This view yields a simple yet effective algorithm that can be used for batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as l1. We derive concrete and very simple algorithms for minimization of loss functions with l1, l2, l22, and lâˆž regularization. We also show how to construct efficient algorithms for mixed-norm l1/lq regularization. We further extend the algorithms and give efficient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in a series of experiments with synthetic and natural data sets.},
 author = {John Duchi and Yoram Singer},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W2164301055},
 pages = {2899--2934},
 title = {Efficient Online and Batch Learning Using Forward Backward Splitting},
 url = {http://jmlr.org/papers/v10/duchi09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:dugas09a,
 abstract = {Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in its two arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of Lipschitz functions with these and other properties. We apply this new class of functions to the task of modelling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.},
 author = {Charles Dugas and Yoshua Bengio and Fran{\c{c}}ois B{{\'e}}lisle and Claude Nadeau and Ren{{\'e}} Garcia},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W2168977355},
 pages = {1239--1262},
 title = {Incorporating Functional Knowledge in Neural Networks},
 url = {http://jmlr.org/papers/v10/dugas09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:escalante09a,
 abstract = {This paper proposes the application of particle swarm optimization (PSO) to the problem of full model selection, FMS, for classification tasks. FMS is defined as follows: given a pool of preprocessing methods, feature selection and learning algorithms, to select the combination of these that obtains the lowest classification error for a given data set; the task also includes the selection of hyperparameters for the considered methods. This problem generates a vast search space to be explored, well suited for stochastic optimization techniques. FMS can be applied to any classification domain as it does not require domain knowledge. Different model types and a variety of algorithms can be considered under this formulation. Furthermore, competitive yet simple models can be obtained with FMS. We adopt PSO for the search because of its proven performance in different problems and because of its simplicity, since neither expensive computations nor complicated operations are needed. Interestingly, the way the search is guided allows PSO to avoid overfitting to some extend. Experimental results on benchmark data sets give evidence that the proposed approach is very effective, despite its simplicity. Furthermore, results obtained in the framework of a model selection challenge show the competitiveness of the models selected with PSO, compared to models selected with other techniques that focus on a single algorithm and that use domain knowledge.},
 author = {Hugo Jair Escalante and Manuel Montes and Luis Enrique Sucar},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2106614481},
 pages = {405--440},
 title = {Particle Swarm Model Selection},
 url = {http://jmlr.org/papers/v10/escalante09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:esposito09a,
 abstract = {The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels.

In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity. We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always finds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus confirming it as a sound replacement.},
 author = {Roberto Esposito and Daniele P. Radicioni},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W2141894578},
 pages = {1851--1880},
 title = {CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning},
 url = {http://jmlr.org/papers/v10/esposito09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:fan09a,
 abstract = {Variable selection in high-dimensional space characterizes many contemporary problems in scientific discovery and decision making. Many frequently-used techniques are based on independence screening; examples include correlation ranking (Fan and Lv, 2008) or feature selection using a two-sample t-test in high-dimensional classification (Tibshirani et al., 2003). Within the context of the linear model, Fan and Lv (2008) showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision, called iteratively sure independent screening (ISIS), is needed when the features are marginally unrelated but jointly related to the response variable. In this paper, we extend ISIS, without explicit definition of residuals, to a general pseudo-likelihood framework, which includes generalized linear models as a special case. Even in the least-squares setting, the new method improves ISIS by allowing feature deletion in the iterative process. Our technique allows us to select important features in high-dimensional classification where the popularly used two-sample t-method fails. A new technique is introduced to reduce the false selection rate in the feature screening stage. Several simulated and two real data examples are presented to illustrate the methodology.},
 author = {Jianqing Fan and Richard Samworth and Yichao Wu},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W2118047185},
 pages = {2013--2038},
 title = {Ultrahigh dimensional feature selection: beyond the linear model.},
 url = {http://jmlr.org/papers/v10/fan09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:feldman09a,
 abstract = {We study the properties of the agnostic learning framework of Haussler (1992) and Kearns, Schapire, and Sellie (1994). In particular, we address the question: is there any situation in which membership queries are useful in agnostic learning?

Our results show that the answer is negative for distribution-independent agnostic learning and positive for agnostic learning with respect to a specific marginal distribution. Namely, we give a simple proof that any concept class learnable agnostically by a distribution-independent algorithm with access to membership queries is also learnable agnostically without membership queries. This resolves an open problem posed by Kearns et al. (1994). For agnostic learning with respect to the uniform distribution over {0,1}n we show a concept class that is learnable with membership queries but computationally hard to learn from random examples alone (assuming that one-way functions exist).},
 author = {Vitaly Feldman},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W2099362736},
 pages = {163--182},
 title = {On The Power of Membership Queries in Agnostic Learning},
 url = {http://jmlr.org/papers/v10/feldman09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:ferrer09a,
 abstract = {We present a method for training support vector machine (SVM)-based classification systems for combination with other classification systems designed for the same task. Ideally, a new system should...},
 author = {Luciana Ferrer and Kemal S{{\"o}}nmez and Elizabeth Shriberg},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W3083543088},
 pages = {2079--2114},
 title = {An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems},
 url = {http://jmlr.org/papers/v10/ferrer09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:foster09a,
 abstract = {The use of Gaussian processes can be an effective approach to prediction in a supervised learning environment. For large data sets, the standard Gaussian process approach requires solving very larg...},
 author = {Leslie Foster and Alex Waagen and Nabeela Aijaz and Michael Hurley and Apolonio Luis and Joel Rinsky and Chandrika Satyavolu and Michael J. Way and Paul Gazis and Ashok Srivastava},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W3012228619},
 pages = {857--882},
 title = {Stable and Efficient Gaussian Process Calculations},
 url = {http://jmlr.org/papers/v10/foster09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:franc09a,
 abstract = {We have developed an optimized cutting plane algorithm (OCA) for solving large-scale risk minimization problems. We prove that the number of iterations OCA requires to converge to a e precise solut...},
 author = {Vojt&#x011B;ch Franc and S{{\"o}}ren Sonnenburg},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W2998583931},
 pages = {2157--2192},
 title = {Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization},
 url = {http://jmlr.org/papers/v10/franc09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:ghanty09a,
 abstract = {In this paper we propose a new multilayer classifier architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classification module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classification module we use support vector machines (SVMs)---here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classification module classifies the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classification module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classification module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been verified with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed.},
 author = {Pradip Ghanty and Samrat Paul and Nikhil R. Pal},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2120239875},
 pages = {591--622},
 title = {NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM},
 url = {http://jmlr.org/papers/v10/ghanty09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:goedertier09a,
 abstract = {Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it ...},
 author = {Stijn Goedertier and David Martens and Jan Vanthienen and Bart Baesens},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W3085412222},
 pages = {1305--1340},
 title = {Robust Process Discovery with Artificial Negative Events},
 url = {http://jmlr.org/papers/v10/goedertier09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:gorissen09a,
 abstract = {Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type.},
 author = {Dirk Gorissen and Tom Dhaene and Filip De Turck},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W2140046357},
 pages = {2039--2078},
 title = {Evolutionary Model Type Selection for Global Surrogate Modeling},
 url = {http://jmlr.org/papers/v10/gorissen09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:greenshtein09a,
 abstract = {We consider the problem of classification using high dimensional features' space. In a paper by Bickel and Levina (2004), it is recommended to use naive-Bayes classifiers, that is, to treat the features as if they are statistically independent.

Consider now a sparse setup, where only a few of the features are for classification. Fan and Fan (2008), suggested a variable selection and classification method, called FAIR. The FAIR method improves the design of naive-Bayes classifiers in sparse setups. The improvement is due to reducing the noise in estimating the features' means. This reduction is since that only the means of a few selected variables should be estimated.

We also consider the design of naive Bayes classifiers. We show that a good alternative to variable selection is estimation of the means through a certain non parametric empirical Bayes procedure. In sparse setups the empirical Bayes implicitly performs an efficient variable selection. It also adapts very well to non sparse setups, and has the advantage of making use of the information from many weakly informative variables, which variable selection type of classification procedures give up on using.

We compare our method with FAIR and other classification methods in simulation for sparse and non sparse setups, and in real data examples involving classification of normal versus malignant tissues based on microarray data.},
 author = {Eitan Greenshtein and Junyong Park},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W2121687865},
 pages = {1687--1704},
 title = {Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification},
 url = {http://jmlr.org/papers/v10/greenshtein09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:gunawardana09a,
 abstract = {Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform diff...},
 author = {Asela Gunawardana and Guy Shani},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2998206837},
 pages = {2935--2962},
 title = {A Survey of Accuracy Evaluation Metrics of Recommendation Tasks},
 url = {http://jmlr.org/papers/v10/gunawardana09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:hausser09a,
 abstract = {We present a procedure for effective estimation of entropy and mutual information from small-sample data, and apply it to the problem of inferring high-dimensional gene association networks. Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator.},
 author = {Jean Hausser and Korbinian Strimmer},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W2949177869},
 pages = {1469--1484},
 title = {Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks},
 url = {http://jmlr.org/papers/v10/hausser09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:hellerstein09a,
 abstract = {A Boolean function f is correlation immune if each input variable is independent of the output, under the uniform distribution on inputs. For example, the parity function is correlation immune. We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. We address this problem in two different contexts. First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). We present theoretical results revealing both the capabilities and limitations of skewing. Second, we explore the problem of identifying relevant variables in the Product Distribution Choice (PDC) learning model, a model in which the learner can choose product distributions and obtain examples from them. We prove a lemma establishing a property of Boolean functions that may be of independent interest. Using this lemma, we give two new algorithms for finding relevant variables of correlation immune functions in the PDC model.},
 author = {Lisa Hellerstein and Bernard Rosell and Eric Bach and Soumya Ray and David Page},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W2166031440},
 pages = {2375--2411},
 title = {Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions},
 url = {http://jmlr.org/papers/v10/hellerstein09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:helmbold09a,
 abstract = {We give an algorithm for the on-line learning of permutations. The algorithm maintains its uncertainty about the target permutation as a doubly stochastic weight matrix, and makes predictions using an efficient method for decomposing the weight matrix into a convex combination of permutations. The weight matrix is updated by multiplying the current matrix entries by exponential factors, and an iterative procedure is needed to restore double stochasticity. Even though the result of this procedure does not have a closed form, a new analysis approach allows us to prove an optimal (up to small constant factors) bound on the regret of our algorithm. This regret bound is significantly better than that of either Kalai and Vempala's more efficient Follow the Perturbed Leader algorithm or the computationally expensive method of explicitly representing each permutation as an expert.},
 author = {David P. Helmbold and Manfred K. Warmuth},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W3022720367},
 pages = {1705--1736},
 title = {Learning Permutations with Exponential Weights},
 url = {http://jmlr.org/papers/v10/helmbold09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:hoefling09a,
 abstract = {We consider the problems of estimating the parameters as well as the structure of binary-valued Markov networks. For maximizing the penalized log-likelihood, we implement an approximate procedure based on the pseudo-likelihood of Besag (1975) and generalize it to a fast exact algorithm. The exact algorithm starts with the pseudo-likelihood solution and then adjusts the pseudo-likelihood criterion so that each additional iterations moves it closer to the exact solution. Our results show that this procedure is faster than the competing exact method proposed by Lee, Ganapathi, and Koller (2006a). However, we also find that the approximate pseudo-likelihood as well as the approaches of Wainwright et al. (2006), when implemented using the coordinate descent procedure of Friedman, Hastie, and Tibshirani (2008b), are much faster than the exact methods, and only slightly less accurate.},
 author = {Holger H{{\"o}}fling and Robert Tibshirani},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W2111380642},
 pages = {883--906},
 title = {Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods.},
 url = {http://jmlr.org/papers/v10/hoefling09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:hu09a,
 abstract = {Learning algorithms are based on samples which are often drawn independently from an identical distribution (i.i.d.). In this paper we consider a different setting with samples drawn according to a non-identical sequence of probability distributions. Each time a sample is drawn from a different distribution. In this setting we investigate a fully online learning algorithm associated with a general convex loss function and a reproducing kernel Hilbert space (RKHS). Error analysis is conducted under the assumption that the sequence of marginal distributions converges polynomially in the dual of a Holder space. For regression with least square or insensitive loss, learning rates are given in both the RKHS norm and the L2 norm. For classification with hinge loss and support vector machine q-norm loss, rates are explicitly stated with respect to the excess misclassification error.},
 author = {Ting Hu and Ding-Xuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W2103195393},
 pages = {2873--2898},
 title = {Online Learning with Samples Drawn from Non-identical Distributions},
 url = {http://jmlr.org/papers/v10/hu09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:huang09a,
 abstract = {Permutations are ubiquitous in many real-world problems, such as voting, ranking, and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities, and typical compact and factorized probability distribution representations, such as graphical models, cannot capture the mutual exclusivity constraints associated with permutations. In this paper, we use the low-frequency terms of a Fourier decomposition to represent distributions over permutations compactly. We present Kronecker conditioning, a novel approach for maintaining and updating these distributions directly in the Fourier domain, allowing for polynomial time bandlimited approximations. Low order Fourier-based approximations, however, may lead to functions that do not correspond to valid distributions. To address this problem, we present a quadratic program defined directly in the Fourier domain for projecting the approximation onto a relaxation of the polytope of legal marginal distributions. We demonstrate the effectiveness of our approach on a real camera-based multi-person tracking scenario.},
 author = {Jonathan Huang and Carlos Guestrin and Leonidas Guibas},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2119883797},
 pages = {997--1070},
 title = {Fourier Theoretic Probabilistic Inference over Permutations},
 url = {http://jmlr.org/papers/v10/huang09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:jain09a,
 author = {Brijnesh J. Jain and Klaus Obermayer},
 journal = {Journal of Machine Learning Research},
 number = {93},
 pages = {2667--2714},
 title = {Structure Spaces},
 url = {http://jmlr.org/papers/v10/jain09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:jiang09a,
 abstract = {The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding's inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding's inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with l1-loss, and ARX model with variable selection for sign classification, which uses both lagged responses and exogenous predictors.},
 author = {Wenxin Jiang},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W2102537236},
 pages = {977--996},
 title = {On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality},
 url = {http://jmlr.org/papers/v10/jiang09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:kanamori09a,
 abstract = {We address the problem of estimating the ratio of two probability density functions, which is often referred to as the importance. The importance values can be used for various succeeding tasks suc...},
 author = {Takafumi Kanamori and Shohei Hido and Masashi Sugiyama},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W3022547535},
 pages = {1391--1445},
 title = {A Least-squares Approach to Direct Importance Estimation},
 url = {http://jmlr.org/papers/v10/kanamori09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:kang09a,
 abstract = {A linear causal model with correlated errors, represented by a DAG with bi-directed edges, can be tested by the set of conditional independence relations implied by the model. A global Markov property specifies, by the d-separation criterion, the set of all conditional independence relations holding in any model associated with a graph. A local Markov property specifies a much smaller set of conditional independence relations which will imply all other conditional independence relations which hold under the global Markov property. For DAGs with bi-directed edges associated with arbitrary probability distributions, a local Markov property is given in Richardson (2003) which may invoke an exponential number of conditional independencies. In this paper, we show that for a class of linear structural equation models with correlated errors, there is a local Markov property which will invoke only a linear number of conditional independence relations. For general linear models, we provide a local Markov property that often invokes far fewer conditional independencies than that in Richardson (2003). The results have applications in testing linear structural equation models with correlated errors.},
 author = {Changsung Kang and Jin Tian},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W2166856700},
 pages = {41--70},
 title = {Markov Properties for Linear Causal Models with Correlated Errors},
 url = {http://jmlr.org/papers/v10/kang09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:king09a,
 abstract = {There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.},
 author = {Davis E. King},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2115252128},
 pages = {1755--1758},
 title = {Dlib-ml: A Machine Learning Toolkit},
 url = {http://jmlr.org/papers/v10/king09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:klivans09a,
 abstract = {We give new algorithms for learning halfspaces in the challenging malicious noise model, where an adversary may corrupt both the labels and the underlying distribution of examples. Our algorithms can tolerate malicious noise rates exponentially larger than previous work in terms of the dependence on the dimension n, and succeed for the fairly broad class of all isotropic log-concave distributions.

We give poly(n, 1/e)-time algorithms for solving the following problems to accuracy e: 
Learning origin-centered halfspaces in Rn with respect to the uniform distribution on the unit ball with malicious noise rate Î· = Î©(e2 / log(n/e)). (The best previous result was Î©(e / (n log(n/e))1/4).) 

Learning origin-centered halfspaces with respect to any isotropic log-concave distribution on Rn with malicious noise rate Î· = Î©(e3 / log2(n/e)). This is the first efficient algorithm for learning under isotropic log-concave distributions in the presence of malicious noise. 




We also give a poly(n,1/e)-time algorithm for learning origin-centered halfspaces under any isotropic log-concave distribution on Rn in the presence of adversarial label noise at rate Î· = Î©(e3 / log(1/e)). In the adversarial label noise setting (or agnostic model), labels can be noisy, but not example points themselves. Previous results could handle Î· = Î©(e) but had running time exponential in an unspecified function of 1/e.

Our analysis crucially exploits both concentration and anti-concentration properties of isotropic log-concave distributions. Our algorithms combine an iterative outlier removal procedure using Principal Component Analysis together with smooth boosting.},
 author = {Adam R. Klivans and Philip M. Long and Rocco A. Servedio},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W3021023808},
 pages = {2715--2740},
 title = {Learning Halfspaces with Malicious Noise},
 url = {http://jmlr.org/papers/v10/klivans09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:kontorovich09a,
 abstract = {We propose a novel framework for supervised learning of discrete concepts. Since the 1970's, the standard computational primitive has been to find the most consistent hypothesis in a given complexity class. In contrast, in this paper we propose a new basic operation: for each pair of input instances, count how many concepts of bounded complexity contain both of them.

Our approach maps instances to a Hilbert space, whose metric is induced by a universal kernel coinciding with our computational primitive, and identifies concepts with half-spaces. We prove that all concepts are linearly separable under this mapping. Hence, given a labeled sample and an oracle for evaluating the universal kernel, we can efficiently compute a linear classifier (via SVM, for example) and use margin bounds to control its generalization error. Even though exact evaluation of the universal kernel may be infeasible, in various natural situations it is efficiently approximable.

Though our approach is general, our main application is to regular languages. Our approach presents a substantial departure from current learning paradigms and in particular yields a novel method for learning this fundamental concept class. Unlike existing techniques, we make no structural assumptions on the corresponding unknown automata, the string distribution or the completeness of the training set. Instead, given a labeled sample our algorithm outputs a classifier with guaranteed distribution-free generalization bounds; to our knowledge, the proposed framework is the only one capable of achieving the latter. Along the way, we touch upon several fundamental questions in complexity, automata, and machine learning.},
 author = {Leonid (Aryeh) Kontorovich and Boaz Nadler},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W2149652368},
 pages = {1095--1129},
 title = {Universal Kernel-Based Learning with Applications to Regular Languages},
 url = {http://jmlr.org/papers/v10/kontorovich09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:kralj-novak09a,
 abstract = {This paper gives a survey of contrast set mining (CSM), emerging pattern mining (EPM), and subgroup discovery (SD) in a unifying framework named supervised descriptive rule discovery. While all these research areas aim at discovering patterns in the form of rules induced from labeled data, they use different terminology and task definitions, claim to have different goals, claim to use different rule learning heuristics, and use different means for selecting subsets of induced patterns. This paper contributes a novel understanding of these subareas of data mining by presenting a unified terminology, by explaining the apparent differences between the learning tasks as variants of a unique supervised descriptive rule discovery task and by exploring the apparent differences between the approaches. It also shows that various rule learning heuristics used in CSM, EPM and SD algorithms all aim at optimizing a trade off between rule coverage and precision. The commonalities (and differences) between the approaches are showcased on a selection of best known variants of CSM, EPM and SD algorithms. The paper also provides a critical survey of existing supervised descriptive rule discovery visualization methods.},
 author = {Petra Kralj Novak and Nada Lavra&#269; and Geoffrey I. Webb},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W2156821882},
 pages = {377--403},
 title = {Supervised Descriptive Rule Discovery: A Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining},
 url = {http://jmlr.org/papers/v10/kralj-novak09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:kulis09a,
 abstract = {In this paper, we study low-rank matrix nearness problems, with a focus on learning low-rank positive semidefinite (kernel) matrices for machine learning applications. We propose efficient algorithms that scale linearly in the number of data points and quadratically in the rank of the input matrix. Existing algorithms for learning kernel matrices often scale poorly, with running times that are cubic in the number of data points. We employ Bregman matrix divergences as the measures of nearness---these divergences are natural for learning low-rank kernels since they preserve rank as well as positive semidefiniteness. Special cases of our framework yield faster algorithms for various existing learning problems, and experimental results demonstrate that our algorithms can effectively learn both low-rank and full-rank kernel matrices.},
 author = {Brian Kulis and M{{\'a}}ty{{\'a}}s A. Sustik and Inderjit S. Dhillon},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W2097939965},
 pages = {341--376},
 title = {Low-Rank Kernel Learning with Bregman Matrix Divergences},
 url = {http://jmlr.org/papers/v10/kulis09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:kumar09a,
 author = {M. Pawan Kumar and Vladimir Kolmogorov and Philip H.S. Torr},
 journal = {Journal of Machine Learning Research},
 number = {3},
 pages = {71--106},
 title = {An Analysis of Convex Relaxations for MAP Estimation of Discrete MRFs},
 url = {http://jmlr.org/papers/v10/kumar09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:langford09a,
 abstract = {We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular $L_1$-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.},
 author = {John Langford and Lihong Li and Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W2616657226},
 pages = {777--801},
 title = {Sparse Online Learning via Truncated Gradient},
 url = {http://jmlr.org/papers/v10/langford09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:larochelle09a,
 abstract = {Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.},
 author = {Hugo Larochelle and Yoshua Bengio and J{{\'e}}r{{\^o}}me Louradour and Pascal Lamblin},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W2140833774},
 pages = {1--40},
 title = {Exploring Strategies for Training Deep Neural Networks},
 url = {http://jmlr.org/papers/v10/larochelle09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:lehmann09a,
 abstract = {In this paper, we introduce DL-Learner, a framework for learning in description logics and OWL. OWL is the official W3C standard ontology language for the Semantic Web. Concepts in this language can be learned for constructing and maintaining OWL ontologies or for solving problems similar to those in Inductive Logic Programming. DL-Learner includes several learning algorithms, support for different OWL formats, reasoner interfaces, and learning problems. It is a cross-platform framework implemented in Java. The framework allows easy programmatic access and provides a command line interface, a graphical interface as well as a WSDL-based web service.},
 author = {Jens Lehmann},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2160605923},
 pages = {2639--2642},
 title = {DL-Learner: Learning Concepts in Description Logics},
 url = {http://jmlr.org/papers/v10/lehmann09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:li09a,
 abstract = {In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.},
 author = {Junning Li and Z. Jane Wang},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W2097421443},
 pages = {475--514},
 title = {Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm},
 url = {http://jmlr.org/papers/v10/li09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:li09b,
 abstract = {We consider the problem of multi-task reinforcement learning (MTRL) in multiple partially observable stochastic environments. We introduce the regionalized policy representation (RPR) to characteri...},
 author = {Hui Li and Xuejun Liao and Lawrence Carin},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W3094203569},
 pages = {1131--1186},
 title = {Multi-task Reinforcement Learning in Partially Observable Stochastic Environments},
 url = {http://jmlr.org/papers/v10/li09b.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:lin09a,
 abstract = {Inference in Bayesian statistics involves the evaluation of marginal likelihood integrals. We present algebraic algorithms for computing such integrals exactly for discrete data of small sample size. Our methods apply to both uniform priors and Dirichlet priors. The underlying statistical models are mixtures of independent distributions, or, in geometric language, secant varieties of Segre-Veronese varieties.},
 author = {Shaowei Lin and Bernd Sturmfels and Zhiqiang Xu},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W2149301289},
 pages = {1611--1631},
 title = {Marginal Likelihood Integrals for Mixtures of Independence Models},
 url = {http://jmlr.org/papers/v10/lin09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:liu09a,
 abstract = {Recent methods for estimating sparse undirected graphs for real-valued data in high dimensional problems rely heavily on the assumption of normality. We show how to use a semiparametric Gaussian copula---or nonparanormal---for high dimensional inference. Just as additive models extend linear models by replacing linear functions with a set of one-dimensional smooth functions, the nonparanormal extends the normal by transforming the variables by smooth functions. We derive a method for estimating the nonparanormal, study the method's theoretical properties, and show that it works well in many examples.},
 author = {Han Liu and John Lafferty and Larry Wasserman},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W2137892504},
 pages = {2295--2328},
 title = {The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs},
 url = {http://jmlr.org/papers/v10/liu09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:madani09a,
 abstract = {Many learning tasks, such as large-scale text categorization and word prediction, can benefit from efficient training and classification when the number of classes, in addition to instances and features, is large, that is, in the thousands and beyond. We investigate the learning of sparse class indices to address this challenge. An index is a mapping from features to classes. We compare the index-learning methods against other techniques, including one-versus-rest and top-down classification using perceptrons and support vector machines. We find that index learning is highly advantageous for space and time efficiency, at both training and classification times. Moreover, this approach yields similar and at times better accuracies. On problems with hundreds of thousands of instances and thousands of classes, the index is learned in minutes, while other methods can take hours or days.

As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. This constraint is crucial for scalability. Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classification take O(dl log(dl)).},
 author = {Omid Madani and Michael Connor and Wiley Greiner},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W2170674774},
 pages = {2571--2613},
 title = {Learning When Concepts Abound},
 url = {http://jmlr.org/papers/v10/madani09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:maes09a,
 abstract = {In this paper we introduce NIEME, a machine learning library for large-scale classification, regression and ranking. NIEME, relies on the framework of energy-based models (LeCun et al., 2006) which unifies several learning algorithms ranging from simple perceptrons to recent models such as the pegasos support vector machine or l1-regularized maximum entropy models. This framework also unifies batch and stochastic learning which are both seen as energy minimization problems. NIEME, can hence be used in a wide range of situations, but is particularly interesting for large-scale learning tasks where both the examples and the features are processed incrementally. Being able to deal with new incoming features at any time within the learning process is another original feature of the NIEME, toolbox. NIEME, is released under the GPL license. It is efficiently implemented in C++, it works on Linux, Mac OS X and Windows and provides interfaces for C++, Java and Python.},
 author = {Francis Maes},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W114702462},
 pages = {743--746},
 title = {Nieme: Large-Scale Energy-Based Models},
 url = {http://jmlr.org/papers/v10/maes09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:mannor09a,
 abstract = {We study online learning where a decision maker interacts with Nature with the objective of maximizing her long-term average reward subject to some sample path average constraints. We define the reward-in-hindsight as the highest reward the decision maker could have achieved, while satisfying the constraints, had she known Nature's choices in advance. We show that in general the reward-in-hindsight is not attainable. The convex hull of the reward-in-hindsight function is, however, attainable. For the important case of a single constraint, the convex hull turns out to be the highest attainable function. Using a calibrated forecasting rule, we provide an explicit strategy that attains this convex hull. We also measure the performance of heuristic methods based on non-calibrated forecasters in experiments involving a CPU power management problem.},
 author = {Shie Mannor and John N. Tsitsiklis and Jia Yuan Yu},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W2095762034},
 pages = {569--590},
 title = {Online Learning with Sample Path Constraints},
 url = {http://jmlr.org/papers/v10/mannor09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:martins09a,
 abstract = {Positive definite kernels on probability measures have been recently applied to classification problems involving text, images, and other types of structured data. Some of these kernels are related to classic information theoretic quantities, such as (Shannon's) mutual information and the Jensen-Shannon (JS) divergence. Meanwhile, there have been recent advances in nonextensive generalizations of Shannon's information theory. This paper bridges these two trends by introducing nonextensive information theoretic kernels on probability measures, based on new JS-type divergences. These new divergences result from extending the the two building blocks of the classical JS divergence: convexity and Shannon's entropy. The notion of convexity is extended to the wider concept of q-convexity, for which we prove a Jensen q-inequality. Based on this inequality, we introduce Jensen-Tsallis (JT) q-differences, a nonextensive generalization of the JS divergence, and define a k-th order JT q-difference between stochastic processes. We then define a new family of nonextensive mutual information kernels, which allow weights to be assigned to their arguments, and which includes the Boolean, JS, and linear kernels as particular cases. Nonextensive string kernels are also defined that generalize the p-spectrum kernel. We illustrate the performance of these kernels on text categorization tasks, in which documents are modeled both as bags of words and as sequences of characters.},
 author = {Andr{{\'e}} F. T. Martins and Noah A. Smith and Eric P. Xing and Pedro M. Q. Aguiar and M{{\'a}}rio A. T. Figueiredo},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W2160682802},
 pages = {935--975},
 title = {Nonextensive Information Theoretic Kernels on Measures},
 url = {http://jmlr.org/papers/v10/martins09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:mcdowell09a,
 abstract = {Many collective classification (CC) algorithms have been shown to increase accuracy when instances are interrelated. However, CC algorithms must be carefully applied because their use of estimated labels can in some cases decrease accuracy. In this article, we show that managing this label uncertainty through cautious algorithmic behavior is essential to achieving maximal, robust performance. First, we describe cautious inference and explain how four well-known families of CC algorithms can be parameterized to use varying degrees of such caution. Second, we introduce cautious learning and show how it can be used to improve the performance of almost any CC algorithm, with or without cautious inference. We then evaluate cautious inference and learning for the four collective inference families, with three local classifiers and a range of both synthetic and real-world data. We find that cautious learning and cautious inference typically outperform less cautious approaches. In addition, we identify the data characteristics that predict more substantial performance differences. Our results reveal that the degree of caution used usually has a larger impact on performance than the choice of the underlying inference algorithm. Together, these results identify the most appropriate CC algorithms to use for particular task characteristics and explain multiple conflicting findings from prior CC research.},
 author = {Luke K. McDowell and Kalyan Moy Gupta and David W. Aha},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W2100045227},
 pages = {2777--2836},
 title = {Cautious Collective Classification},
 url = {http://jmlr.org/papers/v10/mcdowell09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:newman09a,
 abstract = {We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straightforward mapping of LDA to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled LDA. The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both LDA and HDP, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.},
 author = {David Newman and Arthur Asuncion and Padhraic Smyth and Max Welling},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W2116137244},
 pages = {1801--1828},
 title = {Distributed Algorithms for Topic Models},
 url = {http://jmlr.org/papers/v10/newman09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:orabona09a,
 abstract = {A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound ...},
 author = {Francesco Orabona and Joseph Keshet and Barbara Caputo},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W3003446791},
 pages = {2643--2666},
 title = {Bounded Kernel-Based Online Learning},
 url = {http://jmlr.org/papers/v10/orabona09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:paquet09a,
 abstract = {Bayesian inference is intractable for many interesting models, making deterministic algorithms for approximate inference highly desirable. Unlike stochastic methods, which are exact in the limit, the accuracy of these approaches cannot be reasonably judged. In this paper we show how low order perturbation corrections to an expectation-consistent (EC) approximation can provide the necessary tools to ameliorate inference accuracy, and to give an indication of the quality of approximation without having to resort to Monte Carlo methods. Further comparisons are given with variational Bayes and parallel tempering (PT) combined with thermodynamic integration on a Gaussian mixture model. To obtain practical results we further generalize PT to temper from arbitrary distributions rather than a prior in Bayesian inference.},
 author = {Ulrich Paquet and Ole Winther and Manfred Opper},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W2136403412},
 pages = {1263--1304},
 title = {Perturbation Corrections in Approximate Inference: Mixture Modelling Applications},
 url = {http://jmlr.org/papers/v10/paquet09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:pena09a,
 abstract = {We present a sound and complete graphical criterion for reading dependencies from the minimal undirected independence map G of a graphoid M that satisfies weak transitivity. Here, complete means th...},
 author = {Jose M. Pe{{\~n}}a and Roland Nilsson and Johan Bj{{\"o}}rkegren and Jesper Tegn{{\'e}}r},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W3006422638},
 pages = {1071--1094},
 title = {An Algorithm for Reading Dependencies from the Minimal Undirected Independence Map of a Graphoid that Satisfies Weak Transitivity},
 url = {http://jmlr.org/papers/v10/pena09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:poczos09a,
 abstract = {We introduce novel online Bayesian methods for the identification of a family of noisy recurrent neural networks (RNNs). We present Bayesian active learning techniques for stimulus selection given past experiences. In particular, we consider the unknown parameters as stochastic variables and use A-optimality and D-optimality principles to choose optimal stimuli. We derive myopic cost functions in order to maximize the information gain concerning network parameters at each time step. We also derive the A-optimal and D-optimal estimations of the additive noise that perturbs the dynamical system of the RNN. Here we investigate myopic as well as non-myopic estimations, and study the problem of simultaneous estimation of both the system parameters and the noise. Employing conjugate priors our derivations remain approximation-free and give rise to simple update rules for the online learning of the parameters. The efficiency of our method is demonstrated for a number of selected cases, including the task of controlled independent component analysis.},
 author = {Barnab{{\'a}}s P{{\'o}}czos and Andr{{\'a}}s Lo{\H{o}}rincz},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W2151472571},
 pages = {515--554},
 title = {Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques},
 url = {http://jmlr.org/papers/v10/poczos09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:quadrianto09a,
 abstract = {Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, possibly with known label proportions. This problem occurs in areas like e-commerce, politics, spam filtering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice.},
 author = {Novi Quadrianto and Alex J. Smola and Tib{{\'e}}rio S. Caetano and Quoc V. Le},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W1607038179},
 pages = {2349--2374},
 title = {Estimating Labels from Label Proportions},
 url = {http://jmlr.org/papers/v10/quadrianto09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:raeder09a,
 author = {Troy Raeder and Nitesh V. Chawla},
 journal = {Journal of Machine Learning Research},
 number = {47},
 pages = {1387--1390},
 title = {Model Monitor (M2): Evaluating, Comparing, and Monitoring Models},
 url = {http://jmlr.org/papers/v10/raeder09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:ramon09a,
 abstract = {Algorithms that list graphs such that no two listed graphs are isomorphic, are important building blocks of systems for mining and learning in graphs. Algorithms are already known that solve this problem efficiently for many classes of graphs of restricted topology, such as trees. In this article we introduce the concept of a dense augmentation schema, and introduce an algorithm that can be used to enumerate any class of graphs with polynomial delay, as long as the class of graphs can be described using a monotonic predicate operating on a dense augmentation schema. In practice this means that this is the first enumeration algorithm that can be applied theoretically efficiently in any frequent subgraph mining algorithm, and that this algorithm generalizes to situations beyond the standard frequent subgraph mining setting.},
 author = {Jan Ramon and Siegfried Nijssen},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W2117731643},
 pages = {907--929},
 title = {Polynomial-Delay Enumeration of Monotonic Graph Classes},
 url = {http://jmlr.org/papers/v10/ramon09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:rieger09a,
 abstract = {We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of e- and Î½-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples.},
 author = {Christian Rieger and Barbara Zwicknagl},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W2150695134},
 pages = {2115--2132},
 title = {Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods},
 url = {http://jmlr.org/papers/v10/rieger09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:rosset09a,
 abstract = {Modeling of conditional quantiles requires specification of the quantile being estimated and can thus be viewed as a parameterized predictive modeling problem. Quantile loss is typically used, and it is indeed parameterized by a quantile parameter. In this paper we show how to follow the path of cross validated solutions to regularized kernel quantile regression. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on data. This algorithm allows us to efficiently solve the whole family of bi-level problems.},
 author = {Saharon Rosset},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W2102752746},
 pages = {2473--2505},
 title = {Bi-level path following for cross validated solution of kernel quantile regression},
 url = {http://jmlr.org/papers/v10/rosset09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:rudin09a,
 abstract = {We study boosting algorithms for learning to rank. We give a general margin-based bound for ranking based on covering numbers for the hypothesis space. Our bound suggests that algorithms that maximize the ranking margin will generalize well. We then describe a new algorithm, smooth margin ranking, that precisely converges to a maximum ranking-margin solution. The algorithm is a modification of RankBoost, analogous to approximate coordinate ascent boosting. Finally, we prove that AdaBoost and RankBoost are equally good for the problems of bipartite ranking and classification in terms of their asymptotic behavior on the training set. Under natural conditions, AdaBoost achieves an area under the ROC curve that is equally as good as RankBoost's; furthermore, RankBoost, when given a specific intercept, achieves a misclassification error that is as good as AdaBoost's. This may help to explain the empirical observations made by Cortes and Mohri, and Caruana and Niculescu-Mizil, about the excellent performance of AdaBoost as a bipartite ranking algorithm, as measured by the area under the ROC curve.},
 author = {Cynthia Rudin and Robert E. Schapire},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W2141833813},
 pages = {2193--2232},
 title = {Margin-based Ranking and an Equivalence between AdaBoost and RankBoost},
 url = {http://jmlr.org/papers/v10/rudin09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:rudin09b,
 abstract = {We are interested in supervised ranking algorithms that perform especially well near the top of the ranked list, and are only required to perform sufficiently well on the rest of the list. In this work, we provide a general form of convex objective that gives high-scoring examples more importance. This near the top of the list can be chosen arbitrarily large or small, based on the preference of the user. We choose lp-norms to provide a specific type of push; if the user sets p larger, the objective concentrates harder on the top of the list. We derive a generalization bound based on the p-norm objective, working around the natural asymmetry of the problem. We then derive a boosting-style algorithm for the problem of ranking with a push at the top. The usefulness of the algorithm is illustrated through experiments on repository data. We prove that the minimizer of the algorithm's objective is unique in a specific sense. Furthermore, we illustrate how our objective is related to quality measurements for information retrieval.},
 author = {Cynthia Rudin},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W2121824931},
 pages = {2233--2271},
 title = {The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List},
 url = {http://jmlr.org/papers/v10/rudin09b.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:shah09a,
 abstract = {In this paper, we introduce pebl, a Python library and application for learning Bayesian network structure from data and prior knowledge that provides features unmatched by alternative software packages: the ability to use interventional data, flexible specification of structural priors, modeling with hidden variables and exploitation of parallel processing.},
 author = {Abhik Shah and Peter Woolf},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W2109182932},
 pages = {159--162},
 title = {Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data.},
 url = {http://jmlr.org/papers/v10/shah09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:shahbaba09a,
 abstract = {We introduce a new nonlinear model for classification, in which we model the joint distribution of response variable, y, and covariates, x, non-parametrically using Dirichlet process mixtures. We keep the relationship between y and x linear within each component of the mixture. The overall relationship becomes nonlinear if the mixture contains more than one component, with different regression coefficients. We use simulated data to compare the performance of this new approach to alternative methods such as multinomial logit (MNL) models, decision trees, and support vector machines. We also evaluate our approach on two classification problems: identifying the folding class of protein sequences and detecting Parkinson's disease. Our model can sometimes improve predictive accuracy. Moreover, by grouping observations into sub-populations (i.e., mixture components), our model can sometimes provide insight into hidden structure in the data.},
 author = {Babak Shahbaba and Radford Neal},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W2154036191},
 pages = {1829--1850},
 title = {Nonlinear Models Using Dirichlet Process Mixtures},
 url = {http://jmlr.org/papers/v10/shahbaba09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:shi09a,
 abstract = {We propose hashing to facilitate efficient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs.},
 author = {Qinfeng Shi and James Petterson and Gideon Dror and John Langford and Alex Smola and S.V.N. Vishwanathan},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W2112490010},
 pages = {2615--2637},
 title = {Hash Kernels for Structured Data},
 url = {http://jmlr.org/papers/v10/shi09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:silva09a,
 abstract = {Directed acyclic graphs (DAGs) have been widely used as a representation of conditional independence in machine learning and statistics. Moreover, hidden or latent variables are often an important component of graphical models. However, DAG models suffer from an important limitation: the family of DAGs is not closed under marginalization of hidden variables. This means that in general we cannot use a DAG to represent the independencies over a subset of variables in a larger DAG. Directed mixed graphs (DMGs) are a representation that includes DAGs as a special case, and overcomes this limitation. This paper introduces algorithms for performing Bayesian inference in Gaussian and probit DMG models. An important requirement for inference is the specification of the distribution over parameters of the models. We introduce a new distribution for covariance matrices of Gaussian DMGs. We discuss and illustrate how several Bayesian machine learning tasks can benefit from the principle presented here: the power to model dependencies that are generated from hidden variables, but without necessarily modeling such variables explicitly.},
 author = {Ricardo Silva and Zoubin Ghahramani},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W2127159115},
 pages = {1187--1238},
 title = {The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models},
 url = {http://jmlr.org/papers/v10/silva09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:slobodianik09a,
 abstract = {In the machine learning community, the Bayesian scoring criterion is widely used for model selection problems. One of the fundamental theoretical properties justifying the usage of the Bayesian scoring criterion is its consistency. In this paper we refine this property for the case of binomial Bayesian network models. As a by-product of our derivations we establish strong consistency and obtain the law of iterated logarithm for the Bayesian scoring criterion.},
 author = {Nikolai Slobodianik and Dmitry Zaporozhets and Neal Madras},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2110544382},
 pages = {1511--1526},
 title = {Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks},
 url = {http://jmlr.org/papers/v10/slobodianik09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:strehl09a,
 abstract = {We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These PAC-MDP algorithms include the well-known E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.},
 author = {Alexander L. Strehl and Lihong Li and Michael L. Littman},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W2123447947},
 pages = {2413--2444},
 title = {Reinforcement Learning in Finite MDPs: PAC Analysis},
 url = {http://jmlr.org/papers/v10/strehl09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:su09a,
 abstract = {Subgroup analysis is an integral part of comparative analysis where assessing the treatment effect on a response is of central interest. Its goal is to determine the heterogeneity of the treatment effect across subpopulations. In this paper, we adapt the idea of recursive partitioning and introduce an interaction tree (IT) procedure to conduct subgroup analysis. The IT procedure automatically facilitates a number of objectively defined subgroups, in some of which the treatment effect is found prominent while in others the treatment has a negligible or even negative effect. The standard CART (Breiman et al., 1984) methodology is inherited to construct the tree structure. Also, in order to extract factors that contribute to the heterogeneity of the treatment effect, variable importance measure is made available via random forests of the interaction trees. Both simulated experiments and analysis of census wage data are presented for illustration.},
 author = {Xiaogang Su and Chih-Ling Tsai and Hansheng Wang and David M. Nickerson and Bogong Li},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W3122128060},
 pages = {141--158},
 title = {Subgroup Analysis via Recursive Partitioning},
 url = {http://jmlr.org/papers/v10/su09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:syed09a,
 abstract = {In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved mo...},
 author = {Zeeshan Syed and Piotr Indyk and John Guttag},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W3024631166},
 pages = {1913--1936},
 title = {Learning Approximate Sequential Patterns for Classification},
 url = {http://jmlr.org/papers/v10/syed09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:takacs09a,
 abstract = {The collaborative filtering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thriving subfield of machine learning became po...},
 author = {G{{\'a}}bor Tak{{\'a}}cs and Istv{{\'a}}n Pil{{\'a}}szy and Botty{{\'a}}n N{{\'e}}meth and Domonkos Tikk},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W2997904722},
 pages = {623--656},
 title = {Scalable Collaborative Filtering Approaches for Large Recommender Systems},
 url = {http://jmlr.org/papers/v10/takacs09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:tanner09a,
 abstract = {RL-Glue is a standard, language-independent software package for reinforcement-learning experiments. The standardization provided by RL-Glue facilitates code sharing and collaboration. Code sharing reduces the need to re-engineer tasks and experimental apparatus, both common barriers to comparatively evaluating new ideas in the context of the literature. Our software features a minimalist interface and works with several languages and computing platforms. RL-Glue compatibility can be extended to any programming language that supports network socket communication. RL-Glue has been used to teach classes, to run international competitions, and is currently used by several other open-source software and hardware projects.},
 author = {Brian Tanner and Adam White},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W2103048296},
 pages = {2133--2136},
 title = {RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments},
 url = {http://jmlr.org/papers/v10/tanner09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:taylor09a,
 abstract = {The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.},
 author = {Matthew E. Taylor and Peter Stone},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W2097381042},
 pages = {1633--1685},
 title = {Transfer Learning for Reinforcement Learning Domains: A Survey},
 url = {http://jmlr.org/papers/v10/taylor09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:tuv09a,
 abstract = {Predictive models benefit from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categor...},
 author = {Eugene Tuv and Alexander Borisov and George Runger and Kari Torkkola},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W3022239467},
 pages = {1341--1366},
 title = {Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination},
 url = {http://jmlr.org/papers/v10/tuv09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:vanderweele09a,
 abstract = {Various relationships are shown hold between monotonic effects and weak monotonic effects and the monotonicity of certain conditional expectations. Counterexamples are provided to show that the results do not hold under less restrictive conditions. Monotonic effects are furthermore used to relate signed edges on a causal directed acyclic graph to qualitative effect modification. The theory is applied to an example concerning the direct effect of smoking on cardiovascular disease controlling for hypercholesterolemia. Monotonicity assumptions are used to construct a test for whether there is a variable that confounds the relationship between the mediator, hypercholesterolemia, and the outcome, cardiovascular disease.},
 author = {Tyler J. VanderWeele and James M. Robins},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W2105845192},
 pages = {699--718},
 title = {Properties of Monotonic Effects on Directed Acyclic Graphs},
 url = {http://jmlr.org/papers/v10/vanderweele09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:vovk09a,
 abstract = {We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.},
 author = {Vladimir Vovk and Fedor Zhdanov},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W2126984241},
 pages = {2445--2471},
 title = {Prediction with expert advice for the Brier game},
 url = {http://jmlr.org/papers/v10/vovk09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:vural09a,
 abstract = {Most classification methods assume that the samples are drawn independently and identically from an unknown data generating distribution, yet this assumption is violated in several real life problems. In order to relax this assumption, we consider the case where batches or groups of samples may have internal correlations, whereas the samples from different batches may be considered to be uncorrelated. This paper introduces three algorithms for classifying all the samples in a batch jointly: one based on a probabilistic formulation, and two based on mathematical programming analysis. Experiments on three real-life computer aided diagnosis (CAD) problems demonstrate that the proposed algorithms are significantly more accurate than a naive support vector machine which ignores the correlations among the samples.},
 author = {Volkan Vural and Glenn Fung and Balaji Krishnapuram and Jennifer G. Dy and Bharat Rao},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W2150455632},
 pages = {183--206},
 title = {Using Local Dependencies within Batches to Improve Large Margin Classifiers},
 url = {http://jmlr.org/papers/v10/vural09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:wang09a,
 abstract = {In classification, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difficult to achi...},
 author = {Junhui Wang and Xiaotong Shen and Wei Pan},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W3171106320},
 pages = {719--742},
 title = {On Efficient Large Margin Semisupervised Learning: Method and Theory},
 url = {http://jmlr.org/papers/v10/wang09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:weinberger09a,
 abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.},
 author = {Kilian Q. Weinberger and Lawrence K. Saul},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W2106053110},
 pages = {207--244},
 title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
 url = {http://jmlr.org/papers/v10/weinberger09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:white09a,
 abstract = {Judea Pearl's Causal Model is a rich framework that provides deep insight into the nature of causal relations. As yet, however, the Pearl Causal Model (PCM) has had a lesser impact on economics or econometrics than on other disciplines. This may be due in part to the fact that the PCM is not as well suited to analyzing structures that exhibit features of central interest to economists and econometricians: optimization, equilibrium, and learning. We offer the settable systems framework as an extension of the PCM that permits causal discourse in systems embodying optimization, equilibrium, and learning. Because these are common features of physical, natural, or social systems, our framework may prove generally useful for machine learning. Important features distinguishing the settable system framework from the PCM are its countable dimensionality and the use of partitioning and partition-specific response functions to accommodate the behavior of optimizing and interacting agents and to eliminate the requirement of a unique fixed point for the system. Refinements of the PCM include the settable systems treatment of attributes, the causal role of exogenous variables, and the dual role of variables as causes and responses. A series of closely related machine learning examples and examples from game theory and machine learning with feedback demonstrates some limitations of the PCM and motivates the distinguishing features of settable systems.},
 author = {Halbert White and Karim Chalak},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W1957172765},
 pages = {1759--1799},
 title = {Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning},
 url = {http://jmlr.org/papers/v10/white09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:woodsend09a,
 abstract = {Support vector machines are a powerful machine learning technology, but the training process involves a dense quadratic optimization problem and is computationally challenging. A parallel implement...},
 author = {Kristian Woodsend and Jacek Gondzio},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W3027486538},
 pages = {1937--1953},
 title = {Hybrid MPI/OpenMP Parallel Linear Support Vector Machine Training},
 url = {http://jmlr.org/papers/v10/woodsend09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:xiang09a,
 abstract = {This paper considers binary classification algorithms generated from Tikhonov regularization schemes associated with general convex loss functions and varying Gaussian kernels. Our main goal is to ...},
 author = {Dao-Hong Xiang and Ding-Xuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W3097862731},
 pages = {1447--1468},
 title = {Classification with Gaussians and Convex Loss},
 url = {http://jmlr.org/papers/v10/xiang09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:xu09a,
 abstract = {We continue our recent study on constructing a refinement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the refinement kernel contains that with the original kernel as a subspace. To motivate this study, we first develop a refinement kernel method for learning, which gives an efficient algorithm for updating a learning predictor. Several characterizations of refinement kernels are then presented. It is shown that a nontrivial refinement kernel for a given kernel always exists if the input space has an infinite cardinal number. Refinement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided.},
 author = {Yuesheng Xu and Haizhang Zhang},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W2122925131},
 pages = {107--140},
 title = {Refinement of Reproducing Kernels},
 url = {http://jmlr.org/papers/v10/xu09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:xu09b,
 abstract = {We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classification that explicitly build in protection to noise, and at the same time control overfitting. On the analysis front, the equivalence of robustness and regularization, provides a robust optimization interpretation for the success of regularized SVMs. We use the this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well.},
 author = {Huan Xu and Constantine Caramanis and Shie Mannor},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W2153417333},
 pages = {1485--1510},
 title = {Robustness and Regularization of Support Vector Machines},
 url = {http://jmlr.org/papers/v10/xu09b.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:xu09c,
 abstract = {Beam search is commonly used to help maintain tractability in large search spaces at the expense of completeness and optimality. Here we study supervised learning of linear ranking functions for controlling beam search. The goal is to learn ranking functions that allow for beam search to perform nearly as well as unconstrained search, and hence gain computational efficiency without seriously sacrificing optimality. In this paper, we develop theoretical aspects of this learning problem and investigate the application of this framework to learning in the context of automated planning. We first study the computational complexity of the learning problem, showing that even for exponentially large search spaces the general consistency problem is in NP. We also identify tractable and intractable subclasses of the learning problem, giving insight into the problem structure. Next, we analyze the convergence of recently proposed and modified online learning algorithms, where we introduce several notions of problem margin that imply convergence for the various algorithms. Finally, we present empirical results in automated planning, where ranking functions are learned to guide beam search in a number of benchmark planning domains. The results show that our approach is often able to outperform an existing state-of-the-art planning heuristic as well as a recent approach to learning such heuristics.},
 author = {Yuehua Xu and Alan Fern and Sungwook Yoon},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W2146140624},
 pages = {1571--1610},
 title = {Learning Linear Ranking Functions for Beam Search with Application to Planning},
 url = {http://jmlr.org/papers/v10/xu09c.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:yehezkel09a,
 abstract = {We propose the recursive autonomy identification (RAI) algorithm for constraint-based Bayesian network structure learning. The RAI algorithm learns the structure by sequential application of conditional independence (CI) tests, edge direction and structure decomposition into autonomous sub-structures. The sequence of operations is performed recursively for each autonomous sub-structure while simultaneously increasing the order of the CI test. In comparison to other constraint-based algorithms d-separating structures and then directing the resulted undirected graph, the RAI algorithm combines the two processes from the outset and along the procedure. Thereby, learning a structure using the RAI algorithm requires a smaller number of high order CI tests. This reduces the complexity and run-time as well as increases structural and prediction accuracies as demonstrated in extensive experimentation.},
 author = {Raanan Yehezkel and Boaz Lerner},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W2103547885},
 pages = {1527--1570},
 title = {Bayesian Network Structure Learning by Recursive Autonomy Identification},
 url = {http://jmlr.org/papers/v10/yehezkel09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:zakai09a,
 abstract = {We show that all consistent learning methods---that is, that asymptotically achieve the lowest possible expected loss for any distribution on (X,Y)---are necessarily localizable, by which we mean that they do not significantly change their response at a particular point when we show them only the part of the training set that is close to that point. This is true in particular for methods that appear to be defined in a non-local manner, such as support vector machines in classification and least-squares estimators in regression. Aside from showing that consistency implies a specific form of localizability, we also show that consistency is logically equivalent to the combination of two properties: (1) a form of localizability, and (2) that the method's global mean (over the entire X distribution) correctly estimates the true mean. Consistency can therefore be seen as comprised of two aspects, one local and one global.},
 author = {Alon Zakai and Ya'acov Ritov},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W2153254162},
 pages = {827--856},
 title = {Consistency and Localizability},
 url = {http://jmlr.org/papers/v10/zakai09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:zhang09a,
 abstract = {This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches infinity. The condition is identical to a corresponding condition for Lasso.

Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefficient is larger than a constant times the noise level. In comparison, Lasso may require the coefficients to be larger than O(âˆšs) times the noise level in the worst case, where s is the number of nonzero coefficients.},
 author = {Tong Zhang},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2147329339},
 pages = {555--568},
 title = {On the Consistency of Feature Selection using Greedy Least Squares Regression},
 url = {http://jmlr.org/papers/v10/zhang09a.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:zhang09b,
 abstract = {We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semi-inner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established.},
 author = {Haizhang Zhang and Yuesheng Xu and Jun Zhang},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W2125085449},
 pages = {2741--2775},
 title = {Reproducing Kernel Banach Spaces for Machine Learning},
 url = {http://jmlr.org/papers/v10/zhang09b.html},
 volume = {10},
 year = {2009}
}

@article{JMLR:v10:zhu09a,
 abstract = {The standard maximum margin approach for structured prediction lacks a straightforward probabilistic interpretation of the learning scheme and the prediction rule. Therefore its unique advantages such as dual sparseness and kernel tricks cannot be easily conjoined with the merits of a probabilistic model such as Bayesian regularization, model averaging, and ability to model hidden variables. In this paper, we present a new general framework called maximum entropy discrimination Markov networks (MaxEnDNet, or simply, MEDN), which integrates these two approaches and combines and extends their merits. Major innovations of this approach include: 1) It extends the conventional max-entropy discrimination learning of classification rules to a new structural max-entropy discrimination paradigm of learning a distribution of Markov networks. 2) It generalizes the extant Markov network structured-prediction rule based on a point estimator of model coefficients to an averaging model akin to a Bayesian predictor that integrates over a learned posterior distribution of model coefficients. 3) It admits flexible entropic regularization of the model during learning. By plugging in different prior distributions of the model coefficients, it subsumes the well-known maximum margin Markov networks (M3N) as a special case, and leads to a model similar to an L1-regularized M3N that is simultaneously primal and dual sparse, or other new types of Markov networks. 4) It applies a modular learning algorithm that combines existing variational inference techniques and convex-optimization based M3N solvers as subroutines. Essentially, MEDN can be understood as a jointly maximum likelihood and maximum margin estimate of Markov network. It represents the first successful attempt to combine maximum entropy learning (a dual form of maximum likelihood learning) with maximum margin learning of Markov network for structured input/output problems; and the basic principle can be generalized to learning arbitrary graphical models, such as the generative Bayesian networks or models with structured hidden variables. We discuss a number of theoretical properties of this approach, and show that empirically it outperforms a wide array of competing methods for structured input/output learning on both synthetic and real OCR and web data extraction data sets.},
 author = {Jun Zhu and Eric P. Xing},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W2096989558},
 pages = {2531--2569},
 title = {Maximum Entropy Discrimination Markov Networks},
 url = {http://jmlr.org/papers/v10/zhu09a.html},
 volume = {10},
 year = {2009}
}
