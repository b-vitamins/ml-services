@article{JMLR:v21:13-233,
 author = {Olivier Binette and Debdeep Pati and David B. Dunson},
 journal = {Journal of Machine Learning Research},
 number = {119},
 openalex = {W3081792483},
 pages = {1--26},
 title = {Bayesian Closed Surface Fitting Through Tensor Products},
 url = {http://jmlr.org/papers/v21/13-233.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:13-561,
 abstract = {We consider the skill rating problem for multiplayer games, that is how to infer player skills from game outcomes in multiplayer games. We formulate the problem as a minimization problem arg min s s T ∆s where ∆ is a positive semidefinite matrix and s a real-valued function, of which some entries are the skill values to be inferred and other entries are constrained by the game outcomes. We leverage graph-based semi-supervised learning (SSL) algorithms for this problem. We apply our algorithms on several data sets of multiplayer games and obtain very promising results compared to Elo Duelling (see Elo, 1978) and TrueSkill (see Herbrich et al., 2006). As we leverage graph-based SSL algorithms and because games can be seen as relations between sets of players, we then generalize the approach. For this aim, we introduce a new finite model, called hypernode graph, defined to be a set of weighted binary relations between sets of nodes. We define Laplacians of hy-pernode graphs. Then, we show that the skill rating problem for multiplayer games can be formulated as arg min s s T ∆s where ∆ is the Laplacian of a hypernode graph constructed from a set of games. From a fundamental perspective, we show that hypernode graph Laplacians are symmetric positive semidefinite matrices with constant functions in their null space. We show that problems on hypernode graphs can not be solved with graph constructions and graph kernels. We relate hypernode graphs to signed graphs showing that positive relations between groups can lead to negative relations between individuals.},
 author = {Thomas Ricatte and Rémi Gilleron and Marc Tommasi},
 journal = {Journal of Machine Learning Research},
 number = {48},
 openalex = {W3021965653},
 pages = {1--18},
 title = {Skill Rating for Multiplayer Games Introducing Hypernode Graphs and their Spectral Theory},
 url = {http://jmlr.org/papers/v21/13-561.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:14-241,
 abstract = {Iterative methods for fitting a Gaussian Random Field (GRF) model via maximum likelihood (ML) estimation requires solving a nonconvex optimization problem. The problem is aggravated for anisotropic GRFs where the number of covariance function parameters increases with the dimension. Even evaluation of the likelihood function requires $O(n^3)$ floating point operations, where $n$ denotes the number of data locations. In this paper, we propose a new two-stage procedure to estimate the parameters of second-order stationary GRFs. First, a convex likelihood problem regularized with a weighted $\ell_1$-norm, utilizing the available distance information between observation locations, is solved to fit a sparse precision (inverse covariance) matrix to the observed data. Second, the parameters of the covariance function are estimated by solving a least squares problem. Theoretical error bounds for the solutions of stage I and II problems are provided, and their tightness are investigated.},
 author = {Sam Davanloo Tajbakhsh and Necdet Serhat Aybat and Enrique Del Castillo},
 journal = {Journal of Machine Learning Research},
 number = {217},
 openalex = {W3004439449},
 pages = {1--41},
 title = {On the Theoretical Guarantees for Parameter Estimation of Gaussian Random Field Models: A Sparse Precision Matrix Approach},
 url = {http://jmlr.org/papers/v21/14-241.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:14-368,
 author = {Hoda Eldardiry and Jennifer Neville and Ryan A. Rossi},
 journal = {Journal of Machine Learning Research},
 number = {49},
 openalex = {W3028284342},
 pages = {1--37},
 title = {Ensemble Learning for Relational Data},
 url = {http://jmlr.org/papers/v21/14-368.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:15-114,
 abstract = {We consider the problem of unveiling the implicit network structure of node interactions (such as user interactions in a social network), based only on high-frequency timestamps. Our inference is based on the minimization of the least-squares loss associated with a multivariate Hawkes model, penalized by $\ell_1$ and trace norm of the interaction tensor. We provide a first theoretical analysis for this problem, that includes sparsity and low-rank inducing penalizations. This result involves a new data-driven concentration inequality for matrix martingales in continuous time with observable variance, which is a result of independent interest and a broad range of possible applications since it extends to matrix martingales former results restricted to the scalar case. A consequence of our analysis is the construction of sharply tuned $\ell_1$ and trace-norm penalizations, that leads to a data-driven scaling of the variability of information available for each users. Numerical experiments illustrate the significant improvements achieved by the use of such data-driven penalizations.},
 author = {Emmanuel Bacry and Martin Bompaire and Stéphane Gaïffas and Jean-Francois Muzy},
 journal = {Journal of Machine Learning Research},
 number = {50},
 openalex = {W3027795164},
 pages = {1--32},
 title = {Sparse and low-rank multivariate Hawkes processes},
 url = {http://jmlr.org/papers/v21/15-114.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:15-509,
 abstract = {We consider the problem of jointly estimating multiple inverse covariance matrices from high-dimensional data consisting of distinct classes. An l2-penalized maximum likelihood approach is employed. The suggested approach is flexible and generic, incorporating several other l2-penalized estimators as special cases. In addition, the approach allows specification of target matrices through which prior knowledge may be incorporated and which can stabilize the estimation procedure in high-dimensional settings. The result is a targeted fused ridge estimator that is of use when the precision matrices of the constituent classes are believed to chiefly share the same structure while potentially differing in a number of locations of interest. It has many applications in (multi)factorial study designs. We focus on the graphical interpretation of precision matrices with the proposed estimator then serving as a basis for integrative or meta-analytic Gaussian graphical modeling. Situations are considered in which the classes are defined by data sets and subtypes of diseases. The performance of the proposed estimator in the graphical modeling setting is assessed through extensive simulation experiments. Its practical usability is illustrated by the differential network modeling of 12 large-scale gene expression data sets of diffuse large B-cell lymphoma subtypes. The estimator and its related procedures are incorporated into the R-package rags2ridges.},
 author = {Anders Ellern Bilgrau and Carel F.W. Peeters and Poul Svante Eriksen and Martin Boegsted and Wessel N. van Wieringen},
 journal = {Journal of Machine Learning Research},
 number = {26},
 openalex = {W3013631797},
 pages = {1--52},
 title = {Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes},
 url = {http://jmlr.org/papers/v21/15-509.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:15-900,
 abstract = {Graph-based semi-supervised learning is the problem of propagating labels from a small number of labelled data points to a larger set of unlabelled data. This paper is concerned with the consistency of optimization-based techniques for such problems, in the limit where the labels have small noise and the underlying unlabelled data is well clustered. We study graph-based probit for binary classification, and a natural generalization of this method to multi-class classification using one-hot encoding. The resulting objective function to be optimized comprises the sum of a quadratic form defined through a rational function of the graph Laplacian, involving only the unlabelled data, and a fidelity term involving only the labelled data. The consistency analysis sheds light on the choice of the rational function defining the optimization.},
 author = {Franca Hoffmann and Bamdad Hosseini and Zhi Ren and Andrew M Stuart},
 journal = {Journal of Machine Learning Research},
 number = {186},
 openalex = {W4288322147},
 pages = {1--55},
 title = {Consistency of semi-supervised learning algorithms on graphs: Probit and one-hot methods},
 url = {http://jmlr.org/papers/v21/15-900.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:16-122,
 author = {Alistair Shilton and Sutharshan Rajasegarar and Marimuthu Palaniswami},
 journal = {Journal of Machine Learning Research},
 number = {213},
 openalex = {W3096600740},
 pages = {1--39},
 title = {Multiclass Anomaly Detector: the CS++ Support Vector Machine},
 url = {http://jmlr.org/papers/v21/16-122.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:16-252,
 author = {Kuang-Yao Lee and Tianqi Liu and Bing Li and Hongyu Zhao},
 journal = {Journal of Machine Learning Research},
 number = {51},
 openalex = {W3028349898},
 pages = {1--38},
 title = {Learning Causal Networks via Additive Faithfulness},
 url = {http://jmlr.org/papers/v21/16-252.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:16-311,
 abstract = {We consider learning problems over training sets in which both, the number of training examples and the dimension of the feature vectors, are large. To solve these problems we propose the random parallel stochastic algorithm (RAPSA). We call the algorithm random parallel because it utilizes multiple parallel processors to operate on a randomly chosen subset of blocks of the feature vector. We call the algorithm stochastic because processors choose training subsets uniformly at random. Algorithms that are parallel in either of these dimensions exist, but RAPSA is the first attempt at a methodology that is parallel in both the selection of blocks and the selection of elements of the training set. In RAPSA, processors utilize the randomly chosen functions to compute the stochastic gradient component associated with a randomly chosen block. The technical contribution of this paper is to show that this minimally coordinated algorithm converges to the optimal classifier when the training objective is convex. Moreover, we present an accelerated version of RAPSA (ARAPSA) that incorporates the objective function curvature information by premultiplying the descent direction by a Hessian approximation matrix. We further extend the results for asynchronous settings and show that if the processors perform their updates without any coordination the algorithms are still convergent to the optimal argument. RAPSA and its extensions are then numerically evaluated on a linear estimation problem and a binary image classification task using the MNIST handwritten digit dataset.},
 author = {Aryan Mokhtari and Alec Koppel and Martin Takac and Alejandro Ribeiro},
 journal = {Journal of Machine Learning Research},
 number = {120},
 openalex = {W4300001780},
 pages = {1--51},
 title = {A Class of Parallel Doubly Stochastic Algorithms for Large-Scale Learning},
 url = {http://jmlr.org/papers/v21/16-311.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:16-494,
 author = {Hao Yu and Michael J. Neely},
 journal = {Journal of Machine Learning Research},
 number = {1},
 openalex = {W3008594870},
 pages = {1--24},
 title = {A Low Complexity Algorithm with O(√T) Regret and O(1) Constraint Violations for Online Convex Optimization with Long Term Constraints},
 url = {http://jmlr.org/papers/v21/16-494.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:16-529,
 author = {Tomáš Kocák and Rémi Munos and Branislav Kveton and Shipra Agrawal and Michal Valko},
 journal = {Journal of Machine Learning Research},
 number = {218},
 pages = {1--44},
 title = {Spectral bandits},
 url = {http://jmlr.org/papers/v21/16-529.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:16-543,
 author = {Malte Probst and Franz Rothlauf},
 journal = {Journal of Machine Learning Research},
 number = {78},
 openalex = {W3038317307},
 pages = {1--31},
 title = {Harmless Overfitting: Using Denoising Autoencoders in Estimation of Distribution Algorithms},
 url = {http://jmlr.org/papers/v21/16-543.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:16-639,
 abstract = {In many applications, observed data are influenced by some combination of latent causes. For example, suppose sensors are placed inside a building to record responses such as temperature, humidity, power consumption and noise levels. These random, observed responses are typically affected by many unobserved, latent factors (or features) within the building such as the number of individuals, the turning on and off of electrical devices, power surges, etc. These latent factors are usually present for a contiguous period of time before disappearing; further, multiple factors could be present at a time. This paper develops new probabilistic methodology and inference methods for random object generation influenced by latent features exhibiting temporal persistence. Every datum is associated with subsets of a potentially infinite number of hidden, persistent features that account for temporal dynamics in an observation. The ensuing class of dynamic models constructed by adapting the Indian Buffet Process --- a probability measure on the space of random, unbounded binary matrices --- finds use in a variety of applications arising in operations, signal processing, biomedicine, marketing, image analysis, etc. Illustrations using synthetic and real data are provided.},
 author = {Sinead A. Williamson and Michael Minyi Zhang and Paul Damien},
 journal = {Journal of Machine Learning Research},
 number = {27},
 openalex = {W3015458397},
 pages = {1--24},
 title = {A New Class of Time Dependent Latent Factor Models with Applications},
 url = {http://jmlr.org/papers/v21/16-639.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:17-068,
 abstract = {This paper studies the nonparametric modal regression problem systematically from a statistical learning view. Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its \textit{Bayes} rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols, such as mean regression, median regression, and quantile regression. However, it outperforms these regression models in terms of robustness as shown in our study from a re-descending M-estimation view. This coincides with and in return explains the merits of MCCR on robustness. On the practical side, the implementation issues of modal regression including the computational algorithm and the tuning parameters selection are discussed. Numerical assessments on modal regression are also conducted to verify our findings empirically.},
 author = {Yunlong Feng and Jun Fan and Johan A.K. Suykens},
 journal = {Journal of Machine Learning Research},
 number = {2},
 openalex = {W3007928625},
 pages = {1--35},
 title = {A Statistical Learning Approach to Modal Regression},
 url = {http://jmlr.org/papers/v21/17-068.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:17-123,
 abstract = {The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets from different contexts that elegantly unifies both approaches. JCI is a causal modeling framework rather than a specific algorithm, and it can be implemented using any causal discovery algorithm that can take into account certain background knowledge. JCI can deal with different types of interventions (e.g., perfect, imperfect, stochastic, etc.) in a unified fashion, and does not require knowledge of intervention targets or types in case of interventional data. We explain how several well-known causal discovery algorithms can be seen as addressing special cases of the JCI framework, and we also propose novel implementations that extend existing causal discovery methods for purely observational data to the JCI setting. We evaluate different JCI implementations on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can considerably outperform state-of-the-art causal discovery algorithms.},
 author = {Joris M. Mooij and Sara Magliacane and Tom Claassen},
 journal = {Journal of Machine Learning Research},
 number = {99},
 openalex = {W3039905535},
 pages = {1--108},
 title = {Joint Causal Inference from Multiple Contexts},
 url = {http://jmlr.org/papers/v21/17-123.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:17-231,
 abstract = {Graphical models are ubiquitous tools to describe the interdependence between variables measured simultaneously such as large-scale gene or protein expression data. Gaussian graphical models (GGMs) are well-established tools for probabilistic exploration of dependence structures using precision matrices and they are generated under a multivariate normal joint distribution. However, they suffer from several shortcomings since they are based on Gaussian distribution assumptions. In this article, we propose a Bayesian quantile based approach for sparse estimation of graphs. We demonstrate that the resulting graph estimation is robust to outliers and applicable under general distributional assumptions. Furthermore, we develop efficient variational Bayes approximations to scale the methods for large data sets. Our methods are applied to a novel cancer proteomics data dataset where-in multiple proteomic antibodies are simultaneously assessed on tumor samples using reverse-phase protein arrays (RPPA) technology.},
 author = {Nilabja Guha and Veera Baladandayuthapani and Bani K. Mallick},
 journal = {Journal of Machine Learning Research},
 number = {79},
 openalex = {W3082831675},
 pages = {1--47},
 title = {Quantile Graphical Models: Bayesian Approaches.},
 url = {http://jmlr.org/papers/v21/17-231.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:17-328,
 abstract = {Latent feature modeling allows capturing the latent structure responsible for generating the observed properties of a set of objects. It is often used to make predictions either for new values of interest or missing information in the original data, as well as to perform data exploratory analysis. However, although there is an extensive literature on latent feature models for homogeneous datasets, where all the attributes that describe each object are of the same (continuous or discrete) nature, there is a lack of work on latent feature modeling for heterogeneous databases. In this paper, we introduce a general Bayesian nonparametric latent feature model suitable for heterogeneous datasets, where the attributes describing each object can be either discrete, continuous or mixed variables. The proposed model presents several important properties. First, it accounts for heterogeneous data while keeping the properties of conjugate models, which allow us to infer the model in linear time with respect to the number of objects and attributes. Second, its Bayesian nonparametric nature allows us to automatically infer the model complexity from the data, i.e., the number of features necessary to capture the latent structure in the data. Third, the latent features in the model are binary-valued variables, easing the interpretability of the obtained latent features in data exploratory analysis. We show the flexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets. Moreover, a software package of the GLFM is publicly available for other researcher to use and improve it.},
 author = {Isabel Valera and Melanie F. Pradier and Maria Lomeli and Zoubin Ghahramani},
 journal = {Journal of Machine Learning Research},
 number = {100},
 openalex = {W2625683755},
 pages = {1--49},
 title = {General Latent Feature Models for Heterogeneous Datasets},
 url = {http://jmlr.org/papers/v21/17-328.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:17-360,
 abstract = {Data-driven analysis has been increasingly used in various decision making processes. With more sources, including reviews, news, and photos, that can now be used for data analysis, the authenticity of data sources is in doubt. While previous literature attempted to detect fake data piece by piece, in the current work, we try to capture the fake data sender's strategic behavior to detect the fake data source. Specifically, we model the tension between a data receiver who makes data-driven decisions and a fake data sender who benefits from misleading the receiver. We propose a potentially infinite horizon continuous time game-theoretic model with asymmetric information to capture the fact that the receiver does not initially know the existence of fake data and learns about it during the course of the game. We use point processes to model the data traffic, where each piece of data can occur at any discrete moment in a continuous time flow. We fully solve the model and employ numerical examples to illustrate the players' strategies and payoffs for insights. Specifically, our results show that maintaining some suspicion about the data sources can be very helpful to the data receiver.},
 author = {Xiaofan Li and Andrew B. Whinston},
 journal = {Journal of Machine Learning Research},
 number = {3},
 openalex = {W3008154471},
 pages = {1--26},
 title = {A Model of Fake Data in Data-driven Analysis},
 url = {http://jmlr.org/papers/v21/17-360.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:17-470,
 author = {Zhuang Ma and Zongming Ma and Hongsong Yuan},
 journal = {Journal of Machine Learning Research},
 number = {4},
 openalex = {W3007611469},
 pages = {1--67},
 title = {Universal Latent Space Model Fitting for Large Networks with Edge Covariates},
 url = {http://jmlr.org/papers/v21/17-470.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:17-678,
 abstract = {Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used empirical approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature, but notably not the Hessian).},
 author = {James Martens},
 journal = {Journal of Machine Learning Research},
 number = {146},
 openalex = {W3086499488},
 pages = {1--76},
 title = {New Insights and Perspectives on the Natural Gradient Method},
 url = {http://jmlr.org/papers/v21/17-678.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:17-698,
 author = {Nicolas Garcia Trillos and Zachary Kaplan and Thabo Samakhoana and Daniel Sanz-Alonso},
 journal = {Journal of Machine Learning Research},
 number = {28},
 openalex = {W3013644691},
 pages = {1--47},
 title = {On the consistency of graph-based Bayesian semi-supervised learning and the scalability of sampling algorithms},
 url = {http://jmlr.org/papers/v21/17-698.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:17-788,
 author = {Xin Zhang and Qing Mai and Hui Zou},
 journal = {Journal of Machine Learning Research},
 number = {29},
 openalex = {W3011584860},
 pages = {1--36},
 title = {The Maximum Separation Subspace in Sufficient Dimension Reduction with Categorical Response},
 url = {http://jmlr.org/papers/v21/17-788.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-008,
 abstract = {Tensor Train decomposition is used across many branches of machine learning. We present T3F -- a library for Tensor Train decomposition based on TensorFlow. T3F supports GPU execution, batch processing, automatic differentiation, and versatile functionality for the Riemannian optimization framework, which takes into account the underlying manifold structure to construct efficient optimization methods. The library makes it easier to implement machine learning papers that rely on the Tensor Train decomposition. T3F includes documentation, examples and 94% test coverage.},
 author = {Alexander Novikov and Pavel Izmailov and Valentin Khrulkov and Michael Figurnov and Ivan Oseledets},
 journal = {Journal of Machine Learning Research},
 number = {30},
 openalex = {W3013294074},
 pages = {1--7},
 title = {Tensor Train decomposition on TensorFlow (T3F)},
 url = {http://jmlr.org/papers/v21/18-008.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-012,
 abstract = {We propose expected policy gradients (EPG), which unify stochastic policy gradients (SPG) and deterministic policy gradients (DPG) for reinforcement learning. Inspired by expected sarsa, EPG integrates (or sums) across actions when estimating the gradient, instead of relying only on the action in the sampled trajectory. For continuous action spaces, we first derive a practical result for Gaussian policies and quadratic critics and then extend it to a universal analytical method, covering a broad class of actors and critics, including Gaussian, exponential families, and policies with bounded support. For Gaussian policies, we introduce an exploration method that uses covariance proportional to the matrix exponential of the scaled Hessian of the critic with respect to the actions. For discrete action spaces, we derive a variant of EPG based on softmax policies. We also establish a new general policy gradient theorem, of which the stochastic and deterministic policy gradient theorems are special cases. Furthermore, we prove that EPG reduces the variance of the gradient estimates without requiring deterministic policies and with little computational overhead. Finally, we provide an extensive experimental evaluation of EPG and show that it outperforms existing approaches on multiple challenging control domains.},
 author = {Kamil Ciosek and Shimon Whiteson},
 journal = {Journal of Machine Learning Research},
 number = {52},
 openalex = {W2783932892},
 pages = {1--51},
 title = {Expected Policy Gradients for Reinforcement Learning},
 url = {http://jmlr.org/papers/v21/18-012.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-041,
 abstract = {We study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We first investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that optimal generalization error bounds can be retained for distributed SGM provided that the partition level is not too large. We then extend our results to spectral-regularization algorithms (SRA), including kernel ridge regression (KRR), kernel principal component analysis, and gradient methods. Our results are superior to the state-of-the-art theory. Particularly, our results show that distributed SGM has a smaller theoretical computational complexity, compared with distributed KRR and classic SGM. Moreover, even for non-distributed SRA, they provide the first optimal, capacity-dependent convergence rates, considering the case that the regression function may not be in the RKHS.},
 author = {Junhong Lin and Volkan Cevher},
 journal = {Journal of Machine Learning Research},
 number = {147},
 openalex = {W3086852721},
 pages = {1--63},
 title = {Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral Algorithms},
 url = {http://jmlr.org/papers/v21/18-041.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-042,
 author = {Rafael Frongillo and Andrew Nobel},
 journal = {Journal of Machine Learning Research},
 number = {80},
 openalex = {W3038737518},
 pages = {1--28},
 title = {Memoryless Sequences for General Losses},
 url = {http://jmlr.org/papers/v21/18-042.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-083,
 author = {Yue Liu and Zhuangyan Fang and Yangbo He and Zhi Geng and Chunchen Liu},
 journal = {Journal of Machine Learning Research},
 number = {148},
 openalex = {W3085895377},
 pages = {1--37},
 title = {Local Causal Network Learning for Finding Pairs of Total and Direct Effects},
 url = {http://jmlr.org/papers/v21/18-083.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-085,
 abstract = {We consider the problem of clustering with the longest-leg path distance (LLPD) metric, which is informative for elongated and irregularly shaped clusters. We prove finite-sample guarantees on the performance of clustering with respect to this metric when random samples are drawn from multiple intrinsically low-dimensional clusters in high-dimensional space, in the presence of a large number of high-dimensional outliers. By combining these results with spectral clustering with respect to LLPD, we provide conditions under which the Laplacian eigengap statistic correctly determines the number of clusters for a large class of data sets, and prove guarantees on the labeling accuracy of the proposed algorithm. Our methods are quite general and provide performance guarantees for spectral clustering with any ultrametric. We also introduce an efficient, easy to implement approximation algorithm for the LLPD based on a multiscale analysis of adjacency graphs, which allows for the runtime of LLPD spectral clustering to be quasilinear in the number of data points.},
 author = {Anna Little and Mauro Maggioni and James M. Murphy},
 journal = {Journal of Machine Learning Research},
 number = {6},
 openalex = {W3008855359},
 pages = {1--66},
 title = {Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and Fast Algorithms},
 url = {http://jmlr.org/papers/v21/18-085.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-101,
 author = {Francois Kamper and Sarel J. Steel and Johan A. du Preez},
 journal = {Journal of Machine Learning Research},
 number = {101},
 openalex = {W3040337729},
 pages = {1--42},
 title = {Regularized Gaussian Belief Propagation with Nodes of Arbitrary Size},
 url = {http://jmlr.org/papers/v21/18-101.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-112,
 author = {Emmanuel Abbe and Sanjeev Kulkarni and Eun Jee Lee},
 journal = {Journal of Machine Learning Research},
 number = {31},
 openalex = {W3013608040},
 pages = {1--36},
 title = {Generalized Nonbacktracking Bounds on the Influence},
 url = {http://jmlr.org/papers/v21/18-112.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-141,
 author = {Nikolay Manchev and Michael Spratling},
 journal = {Journal of Machine Learning Research},
 number = {7},
 openalex = {W3008706590},
 pages = {1--33},
 title = {Target Propagation in Recurrent Neural Networks},
 url = {http://jmlr.org/papers/v21/18-141.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-143,
 abstract = {Consider an unknown smooth function $f: [0,1]^d \rightarrow \mathbb{R}$, and say we are given $n$ noisy mod 1 samples of $f$, i.e., $y_i = (f(x_i) + η_i)\mod 1$, for $x_i \in [0,1]^d$, where $η_i$ denotes the noise. Given the samples $(x_i,y_i)_{i=1}^{n}$, our goal is to recover smooth, robust estimates of the clean samples $f(x_i) \bmod 1$. We formulate a natural approach for solving this problem, which works with angular embeddings of the noisy mod 1 samples over the unit circle, inspired by the angular synchronization framework. This amounts to solving a smoothness regularized least-squares problem -- a quadratically constrained quadratic program (QCQP) -- where the variables are constrained to lie on the unit circle. Our approach is based on solving its relaxation, which is a trust-region sub-problem and hence solvable efficiently. We provide theoretical guarantees demonstrating its robustness to noise for adversarial, and random Gaussian and Bernoulli noise models. To the best of our knowledge, these are the first such theoretical results for this problem. We demonstrate the robustness and efficiency of our approach via extensive numerical simulations on synthetic data, along with a simple least-squares solution for the unwrapping stage, that recovers the original samples of $f$ (up to a global shift). It is shown to perform well at high levels of noise, when taking as input the denoised modulo $1$ samples. Finally, we also consider two other approaches for denoising the modulo 1 samples that leverage tools from Riemannian optimization on manifolds, including a Burer-Monteiro approach for a semidefinite programming relaxation of our formulation. For the two-dimensional version of the problem, which has applications in radar interferometry, we are able to solve instances of real-world data with a million sample points in under 10 seconds, on a personal laptop.},
 author = {Mihai Cucuringu and Hemant Tyagi},
 journal = {Journal of Machine Learning Research},
 number = {32},
 openalex = {W2794212473},
 pages = {1--77},
 title = {Provably robust estimation of modulo 1 samples of a smooth function with applications to phase unwrapping},
 url = {http://jmlr.org/papers/v21/18-143.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-144,
 abstract = {DESlib is an open-source python library providing the implementation of several dynamic selection techniques. The library is divided into three modules: (i) \emph{dcs}, containing the implementation of dynamic classifier selection methods (DCS); (ii) \emph{des}, containing the implementation of dynamic ensemble selection methods (DES); (iii) \emph{static}, with the implementation of static ensemble techniques. The library is fully documented (documentation available online on Read the Docs), has a high test coverage (this http URL) and is part of the scikit-learn-contrib supported projects. Documentation, code and examples can be found on its GitHub page: this https URL.},
 author = {Rafael M. O. Cruz and Luiz G. Hafemann and Robert Sabourin and George D. C. Cavalcanti},
 journal = {Journal of Machine Learning Research},
 number = {8},
 openalex = {W3008399030},
 pages = {1--5},
 title = {DESlib: A Dynamic ensemble selection library in Python},
 url = {http://jmlr.org/papers/v21/18-144.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-155,
 abstract = {Cluster analysis is a fundamental tool for pattern discovery of complex heterogeneous data. Prevalent clustering methods mainly focus on vector or matrix-variate data and are not applicable to general-order tensors, which arise frequently in modern scientific and business applications. Moreover, there is a gap between statistical guarantees and computational efficiency for existing tensor clustering solutions due to the nature of their non-convex formulations. In this work, we bridge this gap by developing a provable convex formulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator enjoys stability guarantees and its computational and storage costs are polynomial in the size of the data. We further establish a non-asymptotic error bound for the CoCo estimator, which reveals a surprising "blessing of dimensionality" phenomenon that does not exist in vector or matrix-variate cluster analysis. Our theoretical findings are supported by extensive simulated studies. Finally, we apply the CoCo estimator to the cluster analysis of advertisement click tensor data from a major online company. Our clustering results provide meaningful business insights to improve advertising effectiveness.},
 author = {Eric C. Chi and Brian J. Gaines and Will Wei Sun and Hua Zhou and Jian Yang},
 journal = {Journal of Machine Learning Research},
 number = {214},
 openalex = {W3097519197},
 pages = {1--58},
 title = {Provable Convex Co-clustering of Tensors.},
 url = {http://jmlr.org/papers/v21/18-155.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-156,
 abstract = {Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The obtained results are quite competitive when compared to other recent proposals of the literature.},
 author = {José R. Berrendero and Beatriz Bueno-Larraz and Antonio Cuevas},
 journal = {Journal of Machine Learning Research},
 number = {9},
 openalex = {W3008164074},
 pages = {1--33},
 title = {On Mahalanobis Distance in Functional Settings},
 url = {http://jmlr.org/papers/v21/18-156.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-211,
 author = {Nikitas Rontsis and Michael A. Osborne and Paul J. Goulart},
 journal = {Journal of Machine Learning Research},
 number = {149},
 openalex = {W3084595587},
 pages = {1--26},
 title = {Distributionally Ambiguous Optimization for Batch Bayesian Optimization},
 url = {http://jmlr.org/papers/v21/18-211.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-212,
 abstract = {An ongoing aim of research in multiobjective Bayesian optimization is to extend its applicability to a large number of objectives. While coping with a limited budget of evaluations, recovering the set of optimal compromise solutions generally requires numerous observations and is less interpretable since this set tends to grow larger with the number of objectives. We thus propose to focus on a specific solution originating from game theory, the Kalai-Smorodinsky solution, which possesses attractive properties. In particular, it ensures equal marginal gains over all objectives. We further make it insensitive to a monotonic transformation of the objectives by considering the objectives in the copula space. A novel tailored algorithm is proposed to search for the solution, in the form of a Bayesian optimization algorithm: sequential sampling decisions are made based on acquisition functions that derive from an instrumental Gaussian process prior. Our approach is tested on four problems with respectively four, six, eight, and nine objectives. The method is available in the Rpackage GPGame available on CRAN at https://cran.r-project.org/package=GPGame.},
 author = {Mickael Binois and Victor Picheny and Patrick Taillandier and Abderrahmane Habbal},
 journal = {Journal of Machine Learning Research},
 number = {150},
 openalex = {W3086334476},
 pages = {1--42},
 title = {The Kalai-Smorodinsky solution for many-objective Bayesian optimization},
 url = {http://jmlr.org/papers/v21/18-212.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-216,
 abstract = {Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This article considers the problem of finding a robust policy while taking into account the impact of environment variables. We present alternating optimisation and quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. We also present transferable ALOQ (TALOQ), for settings where simulator inaccuracies lead to difficulty in transferring the learnt policy to the physical system. We show that our algorithms are robust to the presence of significant rare events, which may not be observable under random sampling but play a substantial role in determining the optimal policy. Experimental results across different domains show that our algorithms learn robust policies efficiently.},
 author = {Supratik Paul and Konstantinos Chatzilygeroudis and Kamil Ciosek and Jean-Baptiste Mouret and Michael A. Osborne and Shimon Whiteson},
 journal = {Journal of Machine Learning Research},
 number = {151},
 openalex = {W3086586587},
 pages = {1--31},
 title = {Robust Reinforcement Learning with Bayesian Optimisation and Quadrature},
 url = {http://jmlr.org/papers/v21/18-216.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-223,
 abstract = {Bayesian Optimisation (BO) refers to a suite of techniques for global optimisation of expensive black box functions, which use introspective Bayesian models of the function to efficiently search for the optimum. While BO has been applied successfully in many applications, modern optimisation tasks usher in new challenges where conventional methods fail spectacularly. In this work, we present Dragonfly, an open source Python library for scalable and robust BO. Dragonfly incorporates multiple recently developed methods that allow BO to be applied in challenging real world settings; these include better methods for handling higher dimensional domains, methods for handling multi-fidelity evaluations when cheap approximations of an expensive function are available, methods for optimising over structured combinatorial spaces, such as the space of neural network architectures, and methods for handling parallel evaluations. Additionally, we develop new methodological improvements in BO for selecting the Bayesian model, selecting the acquisition function, and optimising over complex domains with different variable types and additional constraints. We compare Dragonfly to a suite of other packages and algorithms for global optimisation and demonstrate that when the above methods are integrated, they enable significant improvements in the performance of BO. The Dragonfly library is available at dragonfly.github.io.},
 author = {Kirthevasan Kandasamy and Karun Raju Vysyaraju and Willie Neiswanger and Biswajit Paria and Christopher R. Collins and Jeff Schneider and Barnabas Poczos and Eric P. Xing},
 journal = {Journal of Machine Learning Research},
 number = {81},
 openalex = {W4300686955},
 pages = {1--27},
 title = {Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly},
 url = {http://jmlr.org/papers/v21/18-223.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-259,
 author = {Matey Neykov and Zhaoran Wang and Han Liu},
 journal = {Journal of Machine Learning Research},
 number = {121},
 openalex = {W3082601126},
 pages = {1--39},
 title = {Agnostic Estimation for Phase Retrieval},
 url = {http://jmlr.org/papers/v21/18-259.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-348,
 abstract = {Several classification methods assume that the underlying distributions follow tree-structured graphical models. Indeed, trees capture statistical dependencies between pairs of variables, which may be crucial to attain low classification errors. The resulting classifier is linear in the log-transformed univariate and bivariate densities that correspond to the tree edges. In practice, however, observed data may not be well approximated by trees. Yet, motivated by the importance of pairwise dependencies for accurate classification, here we propose to approximate the optimal decision boundary by a sparse linear combination of the univariate and bivariate log-transformed densities. Our proposed approach is semi-parametric in nature: we non-parametrically estimate the univariate and bivariate densities, remove pairs of variables that are nearly independent using the Hilbert-Schmidt independence criteria, and finally construct a linear SVM on the retained log-transformed densities. We demonstrate using both synthetic and real data that our resulting classifier, denoted SLB (Sparse Log-Bivariate density), is competitive with popular classification methods.},
 author = {Yaniv Tenzer and Amit Moscovich and Mary Frances Dorn and Boaz Nadler and Clifford Spiegelman},
 journal = {Journal of Machine Learning Research},
 number = {189},
 openalex = {W3096646790},
 pages = {1--33},
 title = {Beyond Trees: Classification with Sparse Pairwise Dependencies},
 url = {http://jmlr.org/papers/v21/18-348.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-352,
 abstract = {Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine-tune the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization. We bridge this gap by providing theoretical guarantees for the convergence of AdaGrad for smooth, nonconvex functions. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the $\mathcal{O}(\log(N)/\sqrt{N})$ rate in the stochastic setting, and at the optimal $\mathcal{O}(1/N)$ rate in the batch (non-stochastic) setting -- in this sense, our convergence guarantees are 'sharp'. In particular, the convergence of AdaGrad-Norm is robust to the choice of all hyper-parameters of the algorithm, in contrast to stochastic gradient descent whose convergence depends crucially on tuning the step-size to the (generally unknown) Lipschitz smoothness constant and level of stochastic noise on the gradient. Extensive numerical experiments are provided to corroborate our theory; moreover, the experiments suggest that the robustness of AdaGrad-Norm extends to state-of-the-art models in deep learning, without sacrificing generalization.},
 author = {Rachel Ward and Xiaoxia Wu and Leon Bottou},
 journal = {Journal of Machine Learning Research},
 number = {219},
 openalex = {W2946511237},
 pages = {1--30},
 title = {AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
 url = {http://jmlr.org/papers/v21/18-352.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-357,
 abstract = {Motivated by modern applications in which one constructs graphical models based on a very large number of features, this paper introduces a new class of cluster-based graphical models, in which variable clustering is applied as an initial step for reducing the dimension of the feature space. We employ model assisted clustering, in which the clusters contain features that are similar to the same unobserved latent variable. Two different cluster-based Gaussian graphical models are considered: the latent variable graph, corresponding to the graphical model associated with the unobserved latent variables, and the cluster-average graph, corresponding to the vector of features averaged over clusters. Our study reveals that likelihood based inference for the latent graph, not analyzed previously, is analytically intractable. Our main contribution is the development and analysis of alternative estimation and inference strategies, for the precision matrix of an unobservable latent vector $Z$. We replace the likelihood of the data by an appropriate class of empirical risk functions, that can be specialized to the latent graphical model and to the simpler, but under-analyzed, cluster-average graphical model. The estimators thus derived can be used for inference on the graph structure, for instance on edge strength or pattern recovery. Inference is based on the asymptotic limits of the entry-wise estimates of the precision matrices associated with the conditional independence graphs under consideration. While taking the uncertainty induced by the clustering step into account, we establish Berry-Esseen central limit theorems for the proposed estimators. It is noteworthy that, although the clusters are estimated adaptively from the data, the central limit theorems regarding the entries of the estimated graphs are proved under the same conditions one would use if the clusters were known....},
 author = {Carson Eisenach and Florentina Bunea and Yang Ning and Claudiu Dinicu},
 journal = {Journal of Machine Learning Research},
 number = {53},
 openalex = {W3028175940},
 pages = {1--55},
 title = {High-Dimensional Inference for Cluster-Based Graphical Models},
 url = {http://jmlr.org/papers/v21/18-357.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-370,
 abstract = {The problem of accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. Graph kernels have recently emerged as a promising approach to this problem. There are now many kernels, each focusing on different structural aspects of graphs. Here, we present GraKeL, a library that unifies several graph kernels into a common framework. The library is written in Python and adheres to the scikit-learn interface. It is simple to use and can be naturally combined with scikit-learn's modules to build a complete machine learning pipeline for tasks such as graph classification and clustering. The code is BSD licensed and is available at: https://github.com/ysig/ GraKeL.},
 author = {Giannis Siglidis and Giannis Nikolentzos and Stratis Limnios and Christos Giatsidis and Konstantinos Skianis and Michalis Vazirgiannis},
 journal = {Journal of Machine Learning Research},
 number = {54},
 openalex = {W3028283557},
 pages = {1--5},
 title = {GraKeL: A Graph Kernel Library in Python},
 url = {http://jmlr.org/papers/v21/18-370.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-402,
 author = {Eugenio Bargiacchi and Diederik M. Roijers and Ann Nowé},
 journal = {Journal of Machine Learning Research},
 number = {102},
 openalex = {W3045084777},
 pages = {1--12},
 title = {AI-toolbox: A C++ library for reinforcement learning and planning (with Python Bindings)},
 url = {http://jmlr.org/papers/v21/18-402.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-410,
 abstract = {High dimensional piecewise stationary graphical models represent a versatile class for modelling time varying networks arising in diverse application areas, including biology, economics, and social sciences. There has been recent work in offline detection and estimation of regime changes in the topology of sparse graphical models. However, the online setting remains largely unexplored, despite its high relevance to applications in sensor networks and other engineering monitoring systems, as well as financial markets. To that end, this work introduces a novel scalable online algorithm for detecting an unknown number of abrupt changes in the inverse covariance matrix of sparse Gaussian graphical models with small delay. The proposed algorithm is based upon monitoring the conditional log-likelihood of all nodes in the network and can be extended to a large class of continuous and discrete graphical models. We also investigate asymptotic properties of our procedure under certain mild regularity conditions on the graph size, sparsity level, number of samples, and pre- and post-changes in the topology of the network. Numerical works on both synthetic and real data illustrate the good performance of the proposed methodology both in terms of computational and statistical efficiency across numerous experimental settings.},
 author = {Hossein Keshavarz and George Michaildiis and Yves Atchade},
 journal = {Journal of Machine Learning Research},
 number = {82},
 openalex = {W2809433143},
 pages = {1--57},
 title = {Sequential change-point detection in high-dimensional Gaussian graphical models},
 url = {http://jmlr.org/papers/v21/18-410.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-415,
 abstract = {We introduce in this paper a learning paradigm in which the training data is transformed by a diffeomorphic transformation before prediction. The learning algorithm minimizes a cost function evaluating the prediction error on the training set penalized by the distance between the diffeomorphism and the identity. The approach borrows ideas from shape analysis where diffeomorphisms are estimated for shape and image alignment, and brings them in a previously unexplored setting, estimating, in particular diffeomorphisms in much larger dimensions. After introducing the concept and describing a learning algorithm, we present diverse applications, mostly with synthetic examples, demonstrating the potential of the approach, as well as some insight on how it can be improved.},
 author = {Laurent Younes},
 journal = {Journal of Machine Learning Research},
 number = {220},
 openalex = {W2805860188},
 pages = {1--28},
 title = {Diffeomorphic Learning},
 url = {http://jmlr.org/papers/v21/18-415.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-425,
 abstract = {Dual first-order methods are essential techniques for large-scale constrained convex optimization. However, when recovering the primal solutions, we need $T(\epsilon^{-2})$ iterations to achieve an $\epsilon$-optimal primal solution when we apply an algorithm to the non-strongly convex dual problem with $T(\epsilon^{-1})$ iterations to achieve an $\epsilon$-optimal dual solution, where $T(x)$ can be $x$ or $\sqrt{x}$. In this paper, we prove that the iteration complexity of the primal solutions and dual solutions have the same $O\left(\frac{1}{\sqrt{\epsilon}}\right)$ order of magnitude for the accelerated randomized dual coordinate ascent. When the dual function further satisfies the quadratic functional growth condition, by restarting the algorithm at any period, we establish the linear iteration complexity for both the primal solutions and dual solutions even if the condition number is unknown. When applied to the regularized empirical risk minimization problem, we prove the iteration complexity of $O\left(n\log n+\sqrt{\frac{n}{\epsilon}}\right)$ in both primal space and dual space, where $n$ is the number of samples. Our result takes out the $\left(\log \frac{1}{\epsilon}\right)$ factor compared with the methods based on smoothing/regularization or Catalyst reduction. As far as we know, this is the first time that the optimal $O\left(\sqrt{\frac{n}{\epsilon}}\right)$ iteration complexity in the primal space is established for the dual coordinate ascent based stochastic algorithms. We also establish the accelerated linear complexity for some problems with nonsmooth loss, i.e., the least absolute deviation and SVM.},
 author = {Huan Li and Zhouchen Lin},
 journal = {Journal of Machine Learning Research},
 number = {33},
 openalex = {W2811391283},
 pages = {1--45},
 title = {On the Complexity Analysis of the Primal Solutions for the Accelerated Randomized Dual Coordinate Ascent},
 url = {http://jmlr.org/papers/v21/18-425.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-434,
 author = {Bin Gu and Wenhan Xian and Zhouyuan Huo and Cheng Deng and Heng Huang},
 journal = {Journal of Machine Learning Research},
 number = {190},
 openalex = {W3095726154},
 pages = {1--53},
 title = {A Unified q-Memorization Framework for Asynchronous Stochastic Optimization},
 url = {http://jmlr.org/papers/v21/18-434.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-435,
 abstract = {Commonly-used clustering algorithms usually find ellipsoidal, spherical or other regular-structured clusters, but are more challenged when the underlying groups lack formal structure or definition. Syncytial clustering is the name that we introduce for methods that merge groups obtained from standard clustering algorithms in order to reveal complex group structure in the data. Here, we develop a distribution-free fully-automated syncytial clustering algorithm that can be used with $k$-means and other algorithms. Our approach estimates the cumulative distribution function of the normed residuals from an appropriately fit $k$-groups model and calculates the estimated nonparametric overlap between each pair of clusters. Groups with high pairwise overlap are merged as long as the estimated generalized overlap decreases. Our methodology is always a top performer in identifying groups with regular and irregular structures in several datasets and can be applied to datasets with scatter or incomplete records. The approach is also used to identify the distinct kinds of gamma ray bursts in the Burst and Transient Source Experiment 4Br catalog and the distinct kinds of activation in a functional Magnetic Resonance Imaging study.},
 author = {Israel A. Almodóvar-Rivera and Ranjan Maitra},
 journal = {Journal of Machine Learning Research},
 number = {122},
 openalex = {W3046085002},
 pages = {1--54},
 title = {Kernel-estimated Nonparametric Overlap-Based Syncytial Clustering},
 url = {http://jmlr.org/papers/v21/18-435.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-447,
 abstract = {We study finite-sum nonconvex optimization problems, where the objective function is an average of $n$ nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each epoch, our algorithm uses $K+1$ nested reference points to build an semi-stochastic gradient to further reduce its variance in each epoch. For smooth functions, the proposed algorithm converges to an approximate first order stationary point (i.e., $\|\nabla F(\xb)\|_2\leq \epsilon$) within $\tO(n\land \epsilon^{-2}+\epsilon^{-3}\land n^{1/2}\epsilon^{-2})$\footnote{$\tO(\cdot)$ hides the logarithmic factors} number of stochastic gradient evaluations, where $n$ is the number of component functions, and $\epsilon$ is the optimization error. This improves the best known gradient complexity of SVRG $O(n+n^{2/3}\epsilon^{-2})$ and the best gradient complexity of SCSG $O(\epsilon^{-5/3}\land n^{2/3}\epsilon^{-2})$. For gradient dominated functions, our algorithm achieves $\tO(n\land \tau\epsilon^{-1}+\tau\cdot (n^{1/2}\land (\tau\epsilon^{-1})^{1/2})$ gradient complexity, which again beats the existing best gradient complexity $\tO(n\land \tau\epsilon^{-1}+\tau\cdot (n^{1/2}\land (\tau\epsilon^{-1})^{2/3})$ achieved by SCSG. Thorough experimental results on different nonconvex optimization problems back up our theory.},
 author = {Dongruo Zhou and Pan Xu and Quanquan Gu},
 journal = {Journal of Machine Learning Research},
 number = {103},
 openalex = {W3024230214},
 pages = {1--63},
 title = {Stochastic Nested Variance Reduction for Nonconvex Optimization},
 url = {http://jmlr.org/papers/v21/18-447.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-473,
 abstract = {We propose a novel variant of the conjugate gradient algorithm, Kernel Conjugate Gradient (KCG), designed to speed up learning for kernel machines with differentiable loss functions. This approach leads to a better conditioned optimization problem during learning. We establish an upper bound on the number of iterations for KCG that indicates it should require less than the square root of the number of iterations that standard conjugate gradient requires. In practice, for various differentiable kernel learning problems, we find KCG consistently, and significantly, outperforms existing techniques. The algorithm is simple to implement, requires no more computation per iteration than standard approaches, and is well motivated by Reproducing Kernel Hilbert Space (RKHS) theory. We further show that data-structure techniques recently used to speed up kernel machine approaches are well matched to the algorithm by reducing the dominant costs of training: function evaluation and RKHS inner product computation.},
 author = {Simon Bartels and Philipp Hennig},
 journal = {Journal of Machine Learning Research},
 number = {55},
 openalex = {W1605532809},
 pages = {1--42},
 title = {Kernel conjugate gradient for fast kernel machines},
 url = {http://jmlr.org/papers/v21/18-473.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-487,
 author = {Xiao-Tong Yuan and Bo Liu and Lezi Wang and Qingshan Liu and Dimitris N. Metaxas},
 journal = {Journal of Machine Learning Research},
 number = {152},
 openalex = {W3085857577},
 pages = {1--50},
 title = {Dual iterative hard thresholding},
 url = {http://jmlr.org/papers/v21/18-487.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-488,
 abstract = {We present new excess risk bounds for general unbounded loss functions including log loss and squared loss, where the distribution of the losses may be heavy-tailed. The bounds hold for general estimators, but they are optimized when applied to η-generalized Bayesian, MDL, and empirical risk minimization estimators. In the case of log loss, the bounds imply convergence rates for generalized Bayesian inference under misspecification in terms of a generalization of the Hellinger metric as long as the learning rate η is set correctly. For general loss functions, our bounds rely on two separate conditions: the v-GRIP (generalized reversed information projection) conditions, which control the lower tail of the excess loss; and the newly introduced witness condition, which controls the upper tail. The parameter v in the v-GRIP conditions determines the achievable rate and is akin to the exponent in the Tsybakov margin condition and the Bernstein condition for bounded losses, which the v-GRIP conditions generalize; favorable v in combination with small model complexity leads to O(1/n) rates. The witness condition allows us to connect the excess risk to an “annealed” version thereof, by which we generalize several previous results connecting Hellinger and Renyi divergence to KL divergence.},
 author = {Peter D. Grünwald and Nishant A. Mehta},
 journal = {Journal of Machine Learning Research},
 number = {56},
 openalex = {W3027700053},
 pages = {1--80},
 title = {Fast Rates for General Unbounded Loss Functions: From ERM to Generalized Bayes},
 url = {http://jmlr.org/papers/v21/18-488.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-503,
 abstract = {Convolutional neural networks typically consist of many convolutional layers followed by one or more fully connected layers. While convolutional layers map between high-order activation tensors, the fully connected layers operate on flattened activation vectors. Despite empirical success, this approach has notable drawbacks. Flattening followed by fully connected layers discards multilinear structure in the activations and requires many parameters. We address these problems by incorporating tensor algebraic operations that preserve multilinear structure at every layer. First, we introduce Tensor Contraction Layers (TCLs) that reduce the dimensionality of their input while preserving their multilinear structure using tensor contraction. Next, we introduce Tensor Regression Layers (TRLs), which express outputs through a low-rank multilinear mapping from a high-order activation tensor to an output tensor of arbitrary order. We learn the contraction and regression factors end-to-end, and produce accurate nets with fewer parameters. Additionally, our layers regularize networks by imposing low-rank constraints on the activations (TCL) and regression weights (TRL). Experiments on ImageNet show that, applied to VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters compared to fully connected layers by more than 65% while maintaining or increasing accuracy. In addition to the space savings, our approach's ability to leverage topological structure can be crucial for structured data such as MRI. In particular, we demonstrate significant performance improvements over comparable architectures on three tasks associated with the UK Biobank dataset.},
 author = {Jean Kossaifi and Zachary C. Lipton and Arinbjorn Kolbeinsson and Aran Khanna and Tommaso Furlanello and Anima Anandkumar},
 journal = {Journal of Machine Learning Research},
 number = {123},
 openalex = {W3081927587},
 pages = {1--21},
 title = {Tensor Regression Networks},
 url = {http://jmlr.org/papers/v21/18-503.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-514,
 author = {Hang Yu and Songwei Wu and Luyin Xin and Justin Dauwels},
 journal = {Journal of Machine Learning Research},
 number = {124},
 openalex = {W3082331408},
 pages = {1--54},
 title = {Fast Bayesian inference of Sparse Networks with automatic sparsity determination},
 url = {http://jmlr.org/papers/v21/18-514.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-527,
 abstract = {In this paper we study the fundamental problems of maximizing a continuous non-monotone submodular function over the hypercube, both with and without coordinate-wise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first $\frac{1}{2}$-approximation algorithm for continuous submodular function maximization; this approximation factor of $\frac{1}{2}$ is the best possible for algorithms that only query the objective function at polynomially many points. For the special case of DR-submodular maximization, i.e. when the submodular functions is also coordinate wise concave along all coordinates, we provide a different $\frac{1}{2}$-approximation algorithm that runs in quasilinear time. Both of these results improve upon prior work [Bian et al, 2017, Soma and Yoshida, 2017]. 
Our first algorithm uses novel ideas such as reducing the guaranteed approximation problem to analyzing a zero-sum game for each coordinate, and incorporates the geometry of this zero-sum game to fix the value at this coordinate. Our second algorithm exploits coordinate-wise concavity to identify a monotone equilibrium condition sufficient for getting the required approximation guarantee, and hunts for the equilibrium point using binary search. We further run experiments to verify the performance of our proposed algorithms in related machine learning applications.},
 author = {Rad Niazadeh and Tim Roughgarden and Joshua R. Wang},
 journal = {Journal of Machine Learning Research},
 number = {125},
 openalex = {W2803199317},
 pages = {1--31},
 title = {Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization.},
 url = {http://jmlr.org/papers/v21/18-527.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-531,
 abstract = {We study the learnability of sums of independent integer random variables given a bound on the size of the union of their supports. For a A ⊂Z <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">+</sub> ubset A of non-negative integers, a sum of independent random variables with collective support A (called an "A-sum" in this paper) is a distribution S = X <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">1</sub> + ... + X <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">N</sub> where the X <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">i</sub> 's are mutually independent (but not necessarily identically distributed) integer random variables all of whose supports are contained in A. We give two main algorithmic results for learning such distributions: 1) For the case |A|=3, we give an algorithm for learning A-sums to accuracy ε that uses poly(1/ε) samples and runs in time poly(1/ε), independent of N and of the elements of A. 2) For an arbitrary constant k>=4, if A = {a <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">1</sub> ,...,a <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">k</sub> } with 0<;=a <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">1</sub> <; ... <; a <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">k</sub> , we give an algorithm that uses poly(1/ε)*log log a <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">k</sub> samples (independent of N) and runs in time poly(1/ε, log a <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">k</sub> ). We prove an essentially matching lower bound: if |A| = 4, then any algorithm must use Ω(log log a <sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">4</sub> ) samples even for learning to constant accuracy. We also give similar-in-spirit (but quantitatively very different) algorithmic results, and essentially matching lower bounds, for the case in which A is not known to the learner. Our learning algorithms employ new limit theorems which may be of independent interest. Our algorithms and lower bounds together settle the question of how the sample complexity of learning sums of independent integer random variables scales with the elements in the union of their supports, both in the known-support and unknown-support settings. Finally, all our algorithms easily extend to the "semi-agnostic" learning model, in which training data is generated from a distribution that is only c*ε-close to some A-sum for a constant c>0.},
 author = {Anindya De and Philip M. Long and Rocco A. Servedio},
 journal = {Journal of Machine Learning Research},
 number = {221},
 openalex = {W2963009416},
 pages = {1--79},
 title = {Learning Sums of Independent Random Variables with Sparse Collective Support},
 url = {http://jmlr.org/papers/v21/18-531.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-562,
 author = {Xiaoming Yuan and Shangzhi Zeng and Jin Zhang},
 journal = {Journal of Machine Learning Research},
 number = {83},
 openalex = {W3038675262},
 pages = {1--75},
 title = {Discerning the Linear Convergence of ADMM for Structured Convex Optimization through the Lens of Variational Analysis},
 url = {http://jmlr.org/papers/v21/18-562.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-567,
 author = {Zhanrui Cai and Runze Li and Liping Zhu},
 journal = {Journal of Machine Learning Research},
 number = {10},
 openalex = {W3008047081},
 pages = {1--25},
 title = {Online sufficient dimension reduction through sliced inverse regression},
 url = {http://jmlr.org/papers/v21/18-567.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-573,
 abstract = {We study the misclassification error for community detection in general heterogeneous stochastic block models (SBM) with noisy or partial label information. We establish a connection between the misclassification rate and the notion of minimum energy on the local neighborhood of the SBM. We develop an optimally weighted message passing algorithm to reconstruct labels for SBM based on the minimum energy flow and the eigenvectors of a certain Markov transition matrix. The general SBM considered in this paper allows for unequal-size communities, degree heterogeneity, and different connection probabilities among blocks. We focus on how to optimally weigh the message passing to improve misclassification.},
 author = {T. Tony Cai and Tengyuan Liang and Alexander Rakhlin},
 journal = {Journal of Machine Learning Research},
 number = {11},
 openalex = {W3007788152},
 pages = {1--34},
 title = {Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information},
 url = {http://jmlr.org/papers/v21/18-573.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-577,
 abstract = {The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level $\alpha$. This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong, Feng and Li (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.g., logistic regression, support vector machines, random forest) to respect the given type I error upper bound $\alpha$ with high probability, without specific distributional assumptions on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum sample size requirement on class $0$, which is often the more scarce class, such as in rare disease diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA) model and propose a new parametric thresholding algorithm, which does not need the minimum sample size requirements on class $0$ observations and thus is suitable for small sample applications such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed parametric thresholding rules, we propose four LDA-based NP classifiers, for both low- and high-dimensional settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier, where the rate for excess type II error benefits from the explicit parametric model assumption. Furthermore, as NP classifiers involve a sample splitting step of class $0$ observations, we construct a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this adaptive strategy reduces the type II error of these classifiers.},
 author = {Xin Tong and Lucy Xia and Jiacheng Wang and Yang Feng},
 journal = {Journal of Machine Learning Research},
 number = {12},
 openalex = {W3009605419},
 pages = {1--48},
 title = {Neyman-Pearson classification: parametrics and sample size requirement},
 url = {http://jmlr.org/papers/v21/18-577.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-581,
 abstract = {Community detection in large social networks is affected by degree heterogeneity of nodes. The D-SCORE algorithm for directed networks was introduced to reduce this effect by taking the element-wise ratios of the singular vectors of the adjacency matrix before clustering. Meaningful results were obtained for the statistician citation network, but rigorous analysis on its performance was missing. First, this paper establishes theoretical guarantee for this algorithm and its variants for the directed degree-corrected block model (Directed-DCBM). Second, this paper provides significant improvements for the original D-SCORE algorithms by attaching the nodes outside of the community cores using the information of the original network instead of the singular vectors.},
 author = {Zhe Wang and Yingbin Liang and Pengsheng Ji},
 journal = {Journal of Machine Learning Research},
 number = {153},
 openalex = {W3047996775},
 pages = {1--45},
 title = {Spectral Algorithms for Community Detection in Directed Networks},
 url = {http://jmlr.org/papers/v21/18-581.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-595,
 abstract = {Principal component analysis (PCA) is a well-established tool in machine learning and data processing. The principal axes in PCA were shown to be equivalent to the maximum marginal likelihood estimator of the factor loading matrix in a latent factor model for the observed data, assuming that the latent factors are independently distributed as standard normal distributions. However, the independence assumption may be unrealistic for many scenarios such as modeling multiple time series, spatial processes, and functional data, where the outcomes are correlated. In this paper, we introduce the generalized probabilistic principal component analysis (GPPCA) to study the latent factor model for multiple correlated outcomes, where each factor is modeled by a Gaussian process. Our method generalizes the previous probabilistic formulation of PCA (PPCA) by providing the closed-form maximum marginal likelihood estimator of the factor loadings and other parameters. Based on the explicit expression of the precision matrix in the marginal likelihood that we derived, the number of the computational operations is linear to the number of output variables. Furthermore, we also provide the closed-form expression of the marginal likelihood when other covariates are included in the mean structure. We highlight the advantage of GPPCA in terms of the practical relevance, estimation accuracy and computational convenience. Numerical studies of simulated and real data confirm the excellent finite-sample performance of the proposed approach.},
 author = {Mengyang Gu and Weining Shen},
 journal = {Journal of Machine Learning Research},
 number = {13},
 openalex = {W3008242931},
 pages = {1--41},
 title = {Generalized probabilistic principal component analysis of correlated data},
 url = {http://jmlr.org/papers/v21/18-595.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-601,
 abstract = {In this paper, we extend the methodology developed for Support Vector Machines (SVM) using the `2-norm (`2-SVM) to the more general case of `p-norms with p > 1 (`p-SVM). We derive second order cone formulations for the resulting dual and primal problems. The concept of kernel function, widely applied in `2-SVM, is extended to the more general case of `p-norms with p > 1 by defining a new operator called multidimensional kernel. This object gives rise to reformulations of dual problems, in a transformed space of the original data, where the dependence on the original data always appear as homogeneous polynomials. We adapt known solution algorithms to efficiently solve the primal and dual resulting problems and some computational experiments on real-world datasets are presented showing rather good behavior in terms of the accuracy of `p-SVM with p > 1.},
 author = {Victor Blanco and Justo Puerto and Antonio M. Rodriguez-Chia},
 journal = {Journal of Machine Learning Research},
 number = {14},
 openalex = {W3008629075},
 pages = {1--29},
 title = {On lp-Support Vector Machines and Multidimensional Kernels},
 url = {http://jmlr.org/papers/v21/18-601.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-624,
 abstract = {In Path Integral control problems a representation of an optimally controlled dynamical system can be formally computed and serve as a guidepost to learn a parametrized policy. The Path Integral Cross-Entropy (PICE) method tries to exploit this, but is hampered by poor sample efficiency. We propose a model-free algorithm called ASPIC (Adaptive Smoothing of Path Integral Control) that applies an inf-convolution to the cost function to speedup convergence of policy optimization. We identify PICE as the infinite smoothing limit of such technique and show that the sample efficiency problems that PICE suffers disappear for finite levels of smoothing. For zero smoothing this method becomes a greedy optimization of the cost, which is the standard approach in current reinforcement learning. We show analytically and empirically that intermediate levels of smoothing are optimal, which renders the new method superior to both PICE and direct cost-optimization.},
 author = {Dominik Thalmeier and Hilbert J. Kappen and Simone Totaro and Vicenç Gómez},
 journal = {Journal of Machine Learning Research},
 number = {191},
 openalex = {W3097841524},
 pages = {1--37},
 title = {Adaptive Smoothing for Path Integral Control},
 url = {http://jmlr.org/papers/v21/18-624.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-638,
 abstract = {We propose graph-dependent implicit regularisation strategies for distributed stochastic subgradient descent (Distributed SGD) for convex problems in multi-agent learning. Under the standard assumptions of convexity, Lipschitz continuity, and smoothness, we establish statistical learning rates that retain, up to logarithmic terms, centralised statistical guarantees through implicit regularisation (step size tuning and early stopping) with appropriate dependence on the graph topology. Our approach avoids the need for explicit regularisation in decentralised learning problems, such as adding constraints to the empirical risk minimisation rule. Particularly for distributed methods, the use of implicit regularisation allows the algorithm to remain simple, without projections or dual methods. To prove our results, we establish graph-independent generalisation bounds for Distributed SGD that match the centralised setting (using algorithmic stability), and we establish graph-dependent optimisation bounds that are of independent interest. We present numerical experiments to show that the qualitative nature of the upper bounds we derive can be representative of real behaviours.},
 author = {Dominic Richards and Patrick Rebeschini},
 journal = {Journal of Machine Learning Research},
 number = {34},
 openalex = {W3013478627},
 pages = {1--44},
 title = {Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent},
 url = {http://jmlr.org/papers/v21/18-638.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-664,
 abstract = {Decision forests, including Random Forests and Gradient Boosting Trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. Decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. In contrast, many recent extensions to decision forests are based on axis-oblique splits. Unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. We introduce yet another decision forest, called "Sparse Projection Oblique Randomer Forests" (SPORF). SPORF uses very sparse random projections, i.e., linear combinations of a small subset of features. SPORF significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with >100 problems of varying dimension, sample size, and number of classes. To illustrate how SPORF addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. SPORF typically yields improved performance over existing decision forests, while mitigating computational efficiency and scalability and maintaining interpretability. SPORF can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains.},
 author = {Tyler M. Tomita and James Browne and Cencheng Shen and Jaewon Chung and Jesse L. Patsolic and Benjamin Falk and Carey E. Priebe and Jason Yim and Randal Burns and Mauro Maggioni and Joshua T. Vogelstein},
 journal = {Journal of Machine Learning Research},
 number = {104},
 openalex = {W3038692287},
 pages = {1--39},
 title = {Sparse Projection Oblique Randomer Forests},
 url = {http://jmlr.org/papers/v21/18-664.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-668,
 author = {Christiane Görgen and Manuele Leonelli},
 journal = {Journal of Machine Learning Research},
 number = {84},
 openalex = {W3039897977},
 pages = {1--32},
 title = {Model-Preserving Sensitivity Analysis for Families of Gaussian Distributions},
 url = {http://jmlr.org/papers/v21/18-668.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-696,
 abstract = {Minimum Error Entropy (MEE) principle is an important approach in Information Theoretical Learning (ITL). It is widely applied and studied in various fields for its robustness to noise. In this paper, we study a reproducing kernel-based distributed MEE algorithm, DMEE, which is designed to work with both fully supervised data and semi-supervised data. The divide-and-conquer approach is employed, so there is no inter-node communication overhead. Similar as other distributed algorithms, DMEE significantly reduces the computational complexity and memory requirement on single computing nodes. With fully supervised data, our proved learning rates equal the minimax optimal learning rates of the classical pointwise kernel-based regressions. Under the semi-supervised learning scenarios, we show that DMEE exploits unlabeled data effectively, in the sense that first, under the settings with weak regularity assumptions, additional unlabeled data significantly improves the learning rates of DMEE. Second, with sufficient unlabeled data, labeled data can be distributed to many more computing nodes, that each node takes only O(1) labels, without spoiling the learning rates in terms of the number of labels. This conclusion overcomes the saturation phenomenon in unlabeled data size. It parallels a recent results for regularized least squares (Lin and Zhou, 2018), and suggests that an inflation of unlabeled data is a solution to the MEE learning problems with decentralized data source for the concerns of privacy protection. Our work refers to pairwise learning and non-convex loss. The theoretical analysis is achieved by distributed U-statistics and error decomposition techniques in integral operators.},
 author = {Xin Guo and Ting Hu and Qiang Wu},
 journal = {Journal of Machine Learning Research},
 number = {126},
 openalex = {W3082936827},
 pages = {1--31},
 title = {Distributed Minimum Error Entropy Algorithms},
 url = {http://jmlr.org/papers/v21/18-696.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-720,
 abstract = {One of the common tasks in unsupervised learning is dimensionality reduction, where the goal is to find meaningful low-dimensional structures hidden in high-dimensional data. Sometimes referred to as manifold learning, this problem is closely related to the problem of localization, which aims at embedding a weighted graph into a low-dimensional Euclidean space. Several methods have been proposed for localization, and also manifold learning. Nonetheless, the robustness property of most of them is little understood. In this paper, we obtain perturbation bounds for classical scaling and trilateration, which are then applied to derive performance bounds for Isomap, Landmark Isomap, and Maximum Variance Unfolding. A new perturbation bound for procrustes analysis plays a key role.},
 author = {Ery Arias-Castro and Adel Javanmard and Bruno Pelletier},
 journal = {Journal of Machine Learning Research},
 number = {15},
 openalex = {W2896292394},
 pages = {1--37},
 title = {Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration, with Applications to Manifold Learning},
 url = {http://jmlr.org/papers/v21/18-720.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-735,
 author = {Ganggang Xu and Ming Wang and Jiangze Bian and Hui Huang and Timothy R. Burch and Sandro C. Andrade and Jingfei Zhang and Yongtao Guan},
 journal = {Journal of Machine Learning Research},
 number = {192},
 openalex = {W3096034563},
 pages = {1--39},
 title = {Semi-parametric Learning of Structured Temporal Point Processes},
 url = {http://jmlr.org/papers/v21/18-735.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-751,
 abstract = {Curriculum Learning - the idea of teaching by gradually exposing the learner to examples in a meaningful order, from easy to hard, has been investigated in the context of machine learning long ago. Although methods based on this concept have been empirically shown to improve performance of several learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems - linear regression, and binary classification by hinge loss minimization. We show that in both cases, the expected convergence rate decreases monotonically with the ideal difficulty score, in accordance with earlier empirical results. We also prove that when the ideal difficulty score is fixed, the convergence rate is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results bring to term two apparently contradicting heuristics: curriculum learning on the one hand, and hard data mining on the other.},
 author = {Daphna Weinshall and Dan Amir},
 journal = {Journal of Machine Learning Research},
 number = {222},
 openalex = {W3116940672},
 pages = {1--19},
 title = {Theory of Curriculum Learning, with Convex Loss Functions},
 url = {http://jmlr.org/papers/v21/18-751.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-764,
 abstract = {This paper considers stochastic optimization problems for a large class of objective functions, including convex and continuous submodular. Stochastic proximal gradient methods have been widely used to solve such problems; however, their applicability remains limited when the problem dimension is large and the projection onto a convex set is costly. Instead, stochastic conditional gradient methods are proposed as an alternative solution relying on (i) Approximating gradients via a simple averaging technique requiring a single stochastic gradient evaluation per iteration; (ii) Solving a linear program to compute the descent/ascent direction. The averaging technique reduces the noise of gradient approximations as time progresses, and replacing projection step in proximal methods by a linear program lowers the computational complexity of each iteration. We show that under convexity and smoothness assumptions, our proposed method converges to the optimal objective function value at a sublinear rate of $O(1/t^{1/3})$. Further, for a monotone and continuous DR-submodular function and subject to a general convex body constraint, we prove that our proposed method achieves a $((1-1/e)OPT-\eps)$ guarantee with $O(1/\eps^3)$ stochastic gradient computations. This guarantee matches the known hardness results and closes the gap between deterministic and stochastic continuous submodular maximization. Additionally, we obtain $((1/e)OPT -\eps)$ guarantee after using $O(1/\eps^3)$ stochastic gradients for the case that the objective function is continuous DR-submodular but non-monotone and the constraint set is down-closed. By using stochastic continuous optimization as an interface, we provide the first $(1-1/e)$ tight approximation guarantee for maximizing a monotone but stochastic submodular set function subject to a matroid constraint and $(1/e)$ approximation guarantee for the non-monotone case.},
 author = {Aryan Mokhtari and Hamed Hassani and Amin Karbasi},
 journal = {Journal of Machine Learning Research},
 number = {105},
 openalex = {W3040190148},
 pages = {1--49},
 title = {Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization},
 url = {http://jmlr.org/papers/v21/18-764.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-766,
 abstract = {We consider the problem of decomposing a higher-order tensor with binary entries. Such data problems arise frequently in applications such as neuroimaging, recommendation system, topic modeling, and sensor network localization. We propose a multilinear Bernoulli model, develop a rank-constrained likelihood-based estimation method, and obtain the theoretical accuracy guarantees. In contrast to continuous-valued problems, the binary tensor problem exhibits an interesting phase transition phenomenon according to the signal-to-noise ratio. The error bound for the parameter tensor estimation is established, and we show that the obtained rate is minimax optimal under the considered model. Furthermore, we develop an alternating optimization algorithm with convergence guarantees. The efficacy of our approach is demonstrated through both simulations and analyses of multiple data sets on the tasks of tensor completion and clustering.},
 author = {Miaoyan Wang and Lexin Li},
 journal = {Journal of Machine Learning Research},
 number = {154},
 openalex = {W3087642357},
 pages = {1--38},
 title = {Learning from Binary Multiway Data: Probabilistic Tensor Decomposition and its Statistical Optimality},
 url = {http://jmlr.org/papers/v21/18-766.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-772,
 abstract = {An accurate model of a patient's survival can help determine the appropriate treatment for terminal patients. Unfortunately, risk scores (e.g., from Cox Proportional Hazard models) do not provide survival probabilities, single-time probability models (e.g., the Gail model, predicting 5 year probability) only provide for a single time point, and standard Kaplan-Meier survival curves provide only population averages for a large class of patients meaning they are not specific to patients. This motivates an alternative class of tools that can learn a model which provides an survival which gives survival probabilities across all times - such as extensions to the Cox model, Accelerated Failure Time, an extension to Random Survival Forests, and Multi-Task Logistic Regression. This paper first motivates such individual survival distribution (ISD) models, and explains how they differ from standard models. It then discusses ways to evaluate such models - namely Concordance, 1-Calibration, Brier score, and various versions of L1-loss - and then motivates and defines a novel approach D-Calibration, which determines whether a model's probability estimates are meaningful. We also discuss how these measures differ, and use them to evaluate several ISD prediction tools, over a range of survival datasets.},
 author = {Humza Haider and Bret Hoehn and Sarah Davis and Russell Greiner},
 journal = {Journal of Machine Learning Research},
 number = {85},
 openalex = {W3038272954},
 pages = {1--63},
 title = {Effective Ways to Build and Evaluate Individual Survival Distributions},
 url = {http://jmlr.org/papers/v21/18-772.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-786,
 abstract = {We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time -- TreeHist and Bitstogram. In both algorithms, server running time is $\tilde O(n)$ and user running time is $\tilde O(1)$, hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring $O(n^{5/2})$ server time and $O(n^{3/2})$ user time. With a typically large number of participants in local algorithms ($n$ in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.},
 author = {Raef Bassily and Kobbi Nissim and Uri Stemmer and Abhradeep Thakurta},
 journal = {Journal of Machine Learning Research},
 number = {16},
 openalex = {W4293582152},
 pages = {1--42},
 title = {Practical Locally Private Heavy Hitters},
 url = {http://jmlr.org/papers/v21/18-786.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-790,
 abstract = {We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization (QDSFM), which allows to model a number of learning tasks on graphs and hypergraphs. The problem exhibits close ties to decomposable submodular function minimization (DSFM), yet is much more challenging to solve. We approach the problem via a new dual strategy and formulate an objective that can be optimized through a number of double-loop algorithms. The outer-loop uses either random coordinate descent (RCD) or alternative projection (AP) methods, for both of which we prove linear convergence rates. The inner-loop computes projections onto cones generated by base polytopes of the submodular functions, via the modified min-norm-point or Frank-Wolfe algorithm. We also describe two new applications of QDSFM: hypergraph-adapted PageRank and semi-supervised learning. The proposed hypergraph-based PageRank algorithm can be used for local hypergraph partitioning, and comes with provable performance guarantees. For hypergraph-adapted semi-supervised learning, we provide numerical experiments demonstrating the efficiency of our QDSFM solvers and their significant improvements on prediction accuracy when compared to state-of-the-art methods.},
 author = {Pan Li and Niao He and Olgica Milenkovic},
 journal = {Journal of Machine Learning Research},
 number = {106},
 openalex = {W3039339554},
 pages = {1--49},
 title = {Quadratic Decomposable Submodular Function Minimization: Theory and Practice},
 url = {http://jmlr.org/papers/v21/18-790.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-794,
 author = {Fan Ma and Deyu Meng and Xuanyi Dong and Yi Yang},
 journal = {Journal of Machine Learning Research},
 number = {57},
 openalex = {W3027559642},
 pages = {1--38},
 title = {Self-paced Multi-view Co-training},
 url = {http://jmlr.org/papers/v21/18-794.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-800,
 abstract = {Apache Mahout is a library for scalable machine learning (ML) on distributed dataflow systems, offering various implementations of classification, clustering, dimensionality reduction and recommendation algorithms. Mahout was a pioneer in large-scale machine learning in 2008, when it started and targeted MapReduce, which was the predominant abstraction for scalable computing in industry at that time. Mahout has been widely used by leading web companies and is part of several commercial cloud offerings. In recent years, Mahout migrated to a general framework enabling a mix of dataflow programming and linear algebraic computations on backends such as Apache Spark and Apache Flink. This design allows users to execute data preprocessing and model training in a single, unified dataflow system, instead of requiring a complex integration of several specialized systems. Mahout is maintained as a community-driven open source project at the Apache Software Foundation, and is available under https://mahout.apache.org.},
 author = {Robin Anil and Gokhan Capan and Isabel Drost-Fromm and Ted Dunning and Ellen Friedman and Trevor Grant and Shannon Quinn and Paritosh Ranjan and Sebastian Schelter and Özgür Yılmazel},
 journal = {Journal of Machine Learning Research},
 number = {127},
 openalex = {W3047908100},
 pages = {1--6},
 title = {Apache Mahout: Machine Learning on Distributed Dataflow Systems},
 url = {http://jmlr.org/papers/v21/18-800.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-804,
 abstract = {We study the convergence rate of the optimal quantization for a probability measure sequence (µn) n∈N* on R^d converging in the Wasserstein distance in two aspects: the first one is the convergence rate of optimal quantizer x (n) ∈ (R d) K of µn at level K; the other one is the convergence rate of the distortion function valued at x^(n), called the of x^(n). Moreover, we also study the mean performance of the optimal quantization for the empirical measure of a distribution µ with finite second moment but possibly unbounded support. As an application, we show that the mean performance for the empirical measure of the multidimensional normal distribution N (m, Σ) and of distributions with hyper-exponential tails behave like O(log n √ n). This extends the results from [BDL08] obtained for compactly supported distribution. We also derive an upper bound which is sharper in the quantization level K but suboptimal in n by applying results in [FG15].},
 author = {Yating Liu and Gilles Pagès},
 journal = {Journal of Machine Learning Research},
 number = {86},
 openalex = {W3040411779},
 pages = {1--36},
 title = {Convergence Rate of Optimal Quantization and Application to the Clustering Performance of the Empirical Measure},
 url = {http://jmlr.org/papers/v21/18-804.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-807,
 author = {Chong Wu and Gongjun Xu and Xiaotong Shen and Wei Pan},
 journal = {Journal of Machine Learning Research},
 number = {128},
 openalex = {W3081603125},
 pages = {1--67},
 title = {A Regularization-Based Adaptive Test for High-Dimensional GLMs},
 url = {http://jmlr.org/papers/v21/18-807.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-808,
 abstract = {First order optimization algorithms play a major role in large scale machine learning. A new class of methods, called adaptive algorithms, were recently introduced to adjust iteratively the learning rate for each coordinate. Despite great practical success in deep learning, their behavior and performance on more general loss functions are not well understood. In this paper, we derive a non-autonomous system of differential equations, which is the continuous time limit of adaptive optimization methods. We prove global well-posedness of the system and we investigate the numerical time convergence of its forward Euler approximation. We study, furthermore, the convergence of its trajectories and give conditions under which the differential system, underlying all adaptive algorithms, is suitable for optimization. We discuss convergence to a critical point in the non-convex case and give conditions for the dynamics to avoid saddle points and local maxima. For convex and deterministic loss function, we introduce a suitable Lyapunov functional which allow us to study its rate of convergence. Several other properties of both the continuous and discrete systems are briefly discussed. The differential system studied in the paper is general enough to encompass many other classical algorithms (such as Heavy ball and Nesterov's accelerated method) and allow us to recover several known results for these algorithms.},
 author = {Andre Belotto da Silva and Maxime Gazeau},
 journal = {Journal of Machine Learning Research},
 number = {129},
 openalex = {W2898640938},
 pages = {1--42},
 title = {A general system of differential equations to model first order adaptive algorithms.},
 url = {http://jmlr.org/papers/v21/18-808.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-811,
 abstract = {We introduce an extension of the multi-instance learning problem where examples are organized as nested bags of instances (e.g., a document could be represented as a bag of sentences, which in turn are bags of words). This framework can be useful in various scenarios, such as text and image classification, but also supervised learning over graphs. As a further advantage, multi-multi instance learning enables a particular way of interpreting predictions and the decision function. Our approach is based on a special neural network layer, called bag-layer, whose units aggregate bags of inputs of arbitrary size. We prove theoretically that the associated class of functions contains all Boolean functions over sets of sets of instances and we provide empirical evidence that functions of this kind can be actually learned on semi-synthetic datasets. We finally present experiments on text classification, on citation graphs, and social graph data, which show that our model obtains competitive results with respect to accuracy when compared to other approaches such as convolutional networks on graphs, while at the same time it supports a general approach to interpret the learnt model, as well as explain individual predictions.},
 author = {Alessandro Tibo and Manfred Jaeger and Paolo Frasconi},
 journal = {Journal of Machine Learning Research},
 number = {193},
 openalex = {W3094896511},
 pages = {1--60},
 title = {Learning and Interpreting Multi-Multi-Instance Learning Networks},
 url = {http://jmlr.org/papers/v21/18-811.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-813,
 abstract = {We consider the standard model of distributed optimization of a sum of functions F(z)=∑i=1nfi(z) , where node i in a network holds the function fi (z). We allow for a harsh network model characterized by asynchronous updates, message delays, unpredictable message losses, and directed communication among nodes. In this setting, we analyze a modification of the Gradient-Push method for distributed optimization, assuming that (i) node i is capable of generating gradients of its function fi (z) corrupted by zero-mean bounded-support additive noise at each step, (ii) F(z) is strongly convex, and (iii) each fi (z) has Lipschitz gradients. We show that our proposed method asymptotically performs as well as the best bounds on centralized gradient descent that takes steps in the direction of the sum of the noisy gradients of all the functions f1(z), …, fn (z) at each step.},
 author = {Artin Spiridonoff and Alex Olshevsky and Ioannis Ch. Paschalidis},
 journal = {Journal of Machine Learning Research},
 number = {58},
 openalex = {W3026733282},
 pages = {1--47},
 title = {Robust Asynchronous Stochastic Gradient-Push: Asymptotically Optimal and Network-Independent Performance for Strongly Convex Functions},
 url = {http://jmlr.org/papers/v21/18-813.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-814,
 abstract = {We consider the problem of estimating the location of a single change point in a dynamic stochastic block model. We propose two methods of estimating the change point, together with the model parameters. The first employs a least squares criterion function and takes into consideration the full structure of the stochastic block model and is evaluated at each point in time. Hence, as an intermediate step, it requires estimating the community structure based on a clustering algorithm at every time point. The second method comprises of the following two steps: in the first one, a least squares function is used and evaluated at each time point, but ignores the community structures and just considers a random graph generating mechanism exhibiting a change point. Once the change point is identified, in the second step, all network data before and after it are used together with a clustering algorithm to obtain the corresponding community structures and subsequently estimate the generating stochastic block model parameters. A comparison between these two methods is illustrated. Further, for both methods under their respective identifiability and certain additional regularity conditions, we establish rates of convergence and derive the asymptotic distributions of the change point estimators. The results are illustrated on synthetic data.},
 author = {Monika Bhattacharjee and Moulinath Banerjee and George Michailidis},
 journal = {Journal of Machine Learning Research},
 number = {107},
 openalex = {W2904319319},
 pages = {1--59},
 title = {Change Point Estimation in a Dynamic Stochastic Block Model},
 url = {http://jmlr.org/papers/v21/18-814.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-817,
 abstract = {A common divide-and-conquer approach for Bayesian computation with big data is to partition the data, perform local inference for each piece separately, and combine the results to obtain a global posterior approximation. While being conceptually and computationally appealing, this method involves the problematic need to also split the prior for the local inferences; these weakened priors may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma while still retaining the generalizability of the underlying local inference method, we apply the idea of expectation propagation (EP) as a framework for distributed Bayesian inference. The central idea is to iteratively update approximations to the local likelihoods given the state of the other approximations and the prior. The present paper has two roles: we review the steps that are needed to keep EP algorithms numerically stable, and we suggest a general approach, inspired by EP, for approaching data partitioning problems in a way that achieves the computational benefits of parallelism while allowing each local update to make use of relevant information from the other sites. In addition, we demonstrate how the method can be applied in a hierarchical context to make use of partitioning of both data and parameters. The paper describes a general algorithmic framework, rather than a specific algorithm, and presents an example implementation for it.},
 author = {Aki Vehtari and Andrew Gelman and Tuomas Sivula and Pasi Jylänki and Dustin Tran and Swupnil Sahai and Paul Blomstedt and John P. Cunningham and David Schiminovich and Christian P. Robert},
 journal = {Journal of Machine Learning Research},
 number = {17},
 openalex = {W2599969863},
 pages = {1--53},
 title = {Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data},
 url = {http://jmlr.org/papers/v21/18-817.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-843,
 abstract = {Peak detection in genomic data involves segmenting counts of DNA sequence reads aligned to different locations of a chromosome. The goal is to detect peaks with higher counts, and filter out background noise with lower counts. Most existing algorithms for this problem are unsupervised heuristics tailored to patterns in specific data types. We propose a supervised framework for this problem, using optimal changepoint detection models with learned penalty functions. We propose the first dynamic programming algorithm that is guaranteed to compute the optimal solution to changepoint detection problems with constraints between adjacent segment mean parameters. Implementing this algorithm requires the choice of penalty parameter that determines the number of segments that are estimated. We show how the supervised learning ideas of Rigaill et al. (2013) can be used to choose this penalty. We compare the resulting implementation of our algorithm to several baselines in a benchmark of labeled ChIP-seq data sets with two dierent patterns (broad H3K36me3 data and sharp H3K4me3 data). Whereas baseline unsupervised methods only provide accurate peak detection for a single pattern, our supervised method achieves state-of-the-art accuracy in all data sets. The log-linear timings of our proposed dynamic programming algorithm make it scalable to the large genomic data sets that are now common. Our implementation is available in the PeakSegOptimal R package on CRAN.},
 author = {Toby Dylan Hocking and Guillem Rigaill and Paul Fearnhead and Guillaume Bourque},
 journal = {Journal of Machine Learning Research},
 number = {87},
 openalex = {W3038255897},
 pages = {1--40},
 title = {Constrained Dynamic Programming and Supervised Penalty Learning Algorithms for Peak Detection in Genomic Data},
 url = {http://jmlr.org/papers/v21/18-843.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-850,
 abstract = {We study the connections between spectral clustering and the problems of maximum margin clustering, and estimation of the components of level sets of a density function. Specifically, we obtain bounds on the eigenvectors of graph Laplacian matrices in terms of the between cluster separation, and within cluster connectivity. These bounds ensure that the spectral clustering solution converges to the maximum margin clustering solution as the scaling parameter is reduced towards zero. The sensitivity of maximum margin clustering solutions to outlying points is well known, but can be mitigated by first removing such outliers, and applying maximum margin clustering to the remaining points. If outliers are identified using an estimate of the underlying probability density, then the remaining points may be seen as an estimate of a level set of this density function. We show that such an approach can be used to consistently estimate the components of the level sets of a density function under very mild assumptions.},
 author = {David P. Hofmeyr},
 journal = {Journal of Machine Learning Research},
 number = {18},
 openalex = {W2905412960},
 pages = {1--35},
 title = {Connecting Spectral Clustering to Maximum Margins and Level Sets},
 url = {http://jmlr.org/papers/v21/18-850.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-856,
 abstract = {Modern learning algorithms excel at producing accurate but complex models of the data. However, deploying such models in the real-world requires extra care: we must ensure their reliability, robustness, and absence of undesired biases. This motivates the development of models that are equally accurate but can be also easily inspected and assessed beyond their predictive performance. To this end, we introduce contextual explanation networks (CEN)---a class of architectures that learn to predict by generating and utilizing intermediate, simplified probabilistic models. Specifically, CENs generate parameters for intermediate graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain simultaneously. Our approach offers two major advantages: (i) for each prediction valid, instance-specific explanation is generated with no computational overhead and (ii) prediction via explanation acts as a regularizer and boosts performance in data-scarce settings. We analyze the proposed framework theoretically and experimentally. Our results on image and text classification and survival analysis tasks demonstrate that CENs are not only competitive with the state-of-the-art methods but also offer additional insights behind each prediction, that can be valuable for decision support. We also show that while post-hoc methods may produce misleading explanations in certain cases, CENs are consistent and allow to detect such cases systematically.},
 author = {Maruan Al-Shedivat and Avinava Dubey and Eric Xing},
 journal = {Journal of Machine Learning Research},
 number = {194},
 openalex = {W2619325105},
 pages = {1--44},
 title = {Contextual Explanation Networks},
 url = {http://jmlr.org/papers/v21/18-856.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-881,
 author = {Igor Molybog and Ramtin Madani and Javad Lavaei},
 journal = {Journal of Machine Learning Research},
 number = {195},
 openalex = {W3097067517},
 pages = {1--36},
 title = {Conic Optimization for Quadratic Regression Under Sparse Noise},
 url = {http://jmlr.org/papers/v21/18-881.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:18-884,
 abstract = {This work is concerned with the non-negative rank-1 robust principal component analysis (RPCA), where the goal is to recover the dominant non-negative principal components of a data matrix precisely, where a number of measurements could be grossly corrupted with sparse and arbitrary large noise. Most of the known techniques for solving the RPCA rely on convex relaxation methods by lifting the problem to a higher dimension, which significantly increase the number of variables. As an alternative, the well-known Burer-Monteiro approach can be used to cast the RPCA as a non-convex and non-smooth $\ell_1$ optimization problem with a significantly smaller number of variables. In this work, we show that the low-dimensional formulation of the symmetric and asymmetric positive rank-1 RPCA based on the Burer-Monteiro approach has benign landscape, i.e., 1) it does not have any spurious local solution, 2) has a unique global solution, and 3) its unique global solution coincides with the true components. An implication of this result is that simple local search algorithms are guaranteed to achieve a zero global optimality gap when directly applied to the low-dimensional formulation. Furthermore, we provide strong deterministic and probabilistic guarantees for the exact recovery of the true principal components. In particular, it is shown that a constant fraction of the measurements could be grossly corrupted and yet they would not create any spurious local solution.},
 author = {Salar Fattahi and Somayeh Sojoudi},
 journal = {Journal of Machine Learning Research},
 number = {59},
 openalex = {W3027190643},
 pages = {1--51},
 title = {Exact Guarantees on the Absence of Spurious Local Minima for Non-negative Rank-1 Robust Principal Component Analysis},
 url = {http://jmlr.org/papers/v21/18-884.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-021,
 abstract = {Over the past decades, numerous loss functions have been been proposed for a variety of supervised learning tasks, including regression, classification, ranking, and more generally structured prediction. Understanding the core principles and theoretical properties underpinning these losses is key to choose the right loss for the right problem, as well as to create new losses which combine their strengths. In this paper, we introduce Fenchel-Young losses, a generic way to construct a convex loss function for a regularized prediction function. We provide an in-depth study of their properties in a very broad setting, covering all the aforementioned supervised learning tasks, and revealing new connections between sparsity, generalized entropies, and separation margins. We show that Fenchel-Young losses unify many well-known loss functions and allow to create useful new ones easily. Finally, we derive efficient predictive and training algorithms, making Fenchel-Young losses appealing both in theory and practice.},
 author = {Mathieu Blondel and André F.T. Martins and Vlad Niculae},
 journal = {Journal of Machine Learning Research},
 number = {35},
 openalex = {W3010898928},
 pages = {1--69},
 title = {Learning with Fenchel-Young losses},
 url = {http://jmlr.org/papers/v21/19-021.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-027,
 abstract = {We introduce Geomstats, an open-source Python toolbox for computations and statistics on nonlinear manifolds, such as hyperbolic spaces, spaces of symmetric positive definite matrices, Lie groups of transformations, and many more. We provide object-oriented and extensively unit-tested implementations. Among others, manifolds come equipped with families of Riemannian metrics, with associated exponential and logarithmic maps, geodesics and parallel transport. Statistics and learning algorithms provide methods for estimation, clustering and dimension reduction on manifolds. All associated operations are vectorized for batch computation and provide support for different execution backends, namely NumPy, PyTorch and TensorFlow, enabling GPU acceleration. This paper presents the package, compares it with related libraries and provides relevant code examples. We show that Geomstats provides reliable building blocks to foster research in differential geometry and statistics, and to democratize the use of Riemannian geometry in machine learning applications. The source code is freely available under the MIT license at \url{geomstats.ai}.},
 author = {Nina Miolane and Nicolas Guigui and Alice Le Brigant and Johan Mathe and Benjamin Hou and Yann Thanwerdas and Stefan Heyder and Olivier Peltre and Niklas Koep and Hadi Zaatiti and Hatem Hajri and Yann Cabanes and Thomas Gerald and Paul Chauchat and Christian Shewmake and Daniel Brooks and Bernhard Kainz and Claire Donnat and Susan Holmes and Xavier Pennec},
 journal = {Journal of Machine Learning Research},
 number = {223},
 openalex = {W2804935370},
 pages = {1--9},
 title = {Geomstats: A Python Package for Riemannian Geometry in Machine Learning},
 url = {http://jmlr.org/papers/v21/19-027.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-047,
 abstract = {The wavelet scattering transform is an invariant signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks. All transforms may be executed on a GPU (in addition to CPU), offering a considerable speed up over CPU implementations. The package also has a small memory footprint, resulting inefficient memory usage. The source code, documentation, and examples are available undera BSD license at this https URL},
 author = {Mathieu Andreux and Tomás Angles and Georgios Exarchakis and Roberto Leonarduzzi and Gaspar Rochette and Louis Thiry and John Zarka and Stéphane Mallat and Joakim Andén and Eugene Belilovsky and Joan Bruna and Vincent Lostanlen and Muawiz Chaudhary and Matthew J. Hirn and Edouard Oyallon and Sixin Zhang and Carmine Cella and Michael Eickenberg},
 journal = {Journal of Machine Learning Research},
 number = {60},
 openalex = {W2908414239},
 pages = {1--6},
 title = {Kymatio: Scattering Transforms in Python},
 url = {http://jmlr.org/papers/v21/19-047.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-054,
 abstract = {An important problem in the field of Topological Data Analysis is defining topological summaries which can be combined with traditional data analytic tools. In recent work Bubenik introduced the persistence landscape, a stable representation of persistence diagrams amenable to statistical analysis and machine learning tools. In this paper we generalise the persistence landscape to multiparameter persistence modules providing a stable representation of the rank invariant. We show that multiparameter landscapes are stable with respect to the interleaving distance and persistence weighted Wasserstein distance, and that the collection of multiparameter landscapes faithfully represents the rank invariant. Finally we provide example calculations and statistical tests to demonstrate a range of potential applications and how one can interpret the landscapes associated to a multiparameter module.},
 author = {Oliver Vipond},
 journal = {Journal of Machine Learning Research},
 number = {61},
 openalex = {W3026838057},
 pages = {1--38},
 title = {Multiparameter Persistence Landscapes},
 url = {http://jmlr.org/papers/v21/19-054.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-060,
 abstract = {A key question in reinforcement learning is how an intelligent agent can generalize knowledge across different inputs. By generalizing across different inputs, information learned for one input can be immediately reused for improving predictions for another input. Reusing information allows an agent to compute an optimal decision-making strategy using less data. State representation is a key element of the generalization process, compressing a high-dimensional input space into a low-dimensional latent state space. This article analyzes properties of different latent state spaces, leading to new connections between model-based and model-free reinforcement learning. Successor features, which predict frequencies of future observations, form a link between model-based and model-free learning: Learning to predict future expected reward outcomes, a key characteristic of model-based agents, is equivalent to learning successor features. Learning successor features is a form of temporal difference learning and is equivalent to learning to predict a single policy's utility, which is a characteristic of model-free agents. Drawing on the connection between model-based reinforcement learning and successor features, we demonstrate that representations that are predictive of future reward outcomes generalize across variations in both transitions and rewards. This result extends previous work on successor features, which is constrained to fixed transitions and assumes re-learning of the transferred state representation.},
 author = {Lucas Lehnert and Michael L. Littman},
 journal = {Journal of Machine Learning Research},
 number = {196},
 openalex = {W2994987910},
 pages = {1--53},
 title = {Successor Features Combine Elements of Model-Free and Model-based Reinforcement Learning},
 url = {http://jmlr.org/papers/v21/19-060.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-071,
 abstract = {In statistical learning framework with regressions, interactions are the contributions to the response variable from the products of the explanatory variables. In high-dimensional problems, detecting interactions is challenging due to combinatorial complexity and limited data information. We consider detecting interactions by exploring their connections with the principal Hessian matrix. Specifically, we propose a one-step synthetic approach for estimating the principal Hessian matrix by a penalized M-estimator. An alternating direction method of multipliers (ADMM) is proposed to efficiently solve the encountered regularized optimization problem. Based on the sparse estimator, we detect the interactions by identifying its nonzero components. Our method directly targets at the interactions, and it requires no structural assumption on the hierarchy of the interaction effects. We show that our estimator is theoretically valid, computationally efficient, and practically useful for detecting the interactions in a broad spectrum of scenarios.},
 author = {Cheng Yong Tang and Ethan X. Fang and Yuexiao Dong},
 journal = {Journal of Machine Learning Research},
 number = {19},
 openalex = {W2911447028},
 pages = {1--25},
 title = {High-dimensional Interactions Detection with Sparse Principal Hessian Matrix},
 url = {http://jmlr.org/papers/v21/19-071.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-073,
 abstract = {In this paper, we propose a unified view of gradient-based algorithms for stochastic convex composite optimization by extending the concept of estimate sequence introduced by Nesterov. More precisely, we interpret a large class of stochastic optimization methods as procedures that iteratively minimize a surrogate of the objective, which covers the stochastic gradient descent method and variants of the incremental approaches SAGA, SVRG, and MISO/Finito/SDCA. This point of view has several advantages: (i) we provide a simple generic proof of convergence for all of the aforementioned methods; (ii) we naturally obtain new algorithms with the same guarantees; (iii) we derive generic strategies to make these algorithms robust to stochastic noise, which is useful when data is corrupted by small random perturbations. Finally, we propose a new accelerated stochastic gradient descent algorithm and an accelerated SVRG algorithm with optimal complexity that is robust to stochastic noise.},
 author = {Andrei Kulunchakov and Julien Mairal},
 journal = {Journal of Machine Learning Research},
 number = {155},
 openalex = {W2913250055},
 pages = {1--52},
 title = {Estimate Sequences for Stochastic Composite Optimization: Variance Reduction, Acceleration, and Robustness to Noise},
 url = {http://jmlr.org/papers/v21/19-073.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-080,
 abstract = {Dimensionality reduction is a first step of many machine learning pipelines. Two popular approaches are principal component analysis, which projects onto a small number of well chosen but non-interpretable directions, and feature selection, which selects a small number of the original features. Feature selection can be abstracted as a numerical linear algebra problem called the column subset selection problem (CSSP). CSSP corresponds to selecting the best subset of columns of a matrix $X \in \mathbb{R}^{N \times d}$, where \emph{best} is often meant in the sense of minimizing the approximation error, i.e., the norm of the residual after projection of $X$ onto the space spanned by the selected columns. Such an optimization over subsets of $\{1,\dots,d\}$ is usually impractical. One workaround that has been vastly explored is to resort to polynomial-cost, random subset selection algorithms that favor small values of this approximation error. We propose such a randomized algorithm, based on sampling from a projection determinantal point process (DPP), a repulsive distribution over a fixed number $k$ of indices $\{1,\dots,d\}$ that favors diversity among the selected columns. We give bounds on the ratio of the expected approximation error for this DPP over the optimal error of PCA. These bounds improve over the state-of-the-art bounds of \emph{volume sampling} when some realistic structural assumptions are satisfied for $X$. Numerical experiments suggest that our bounds are tight, and that our algorithms have comparable performance with the \emph{double phase} algorithm, often considered to be the practical state-of-the-art. Column subset selection with DPPs thus inherits the best of both worlds: good empirical performance and tight error bounds.},
 author = {Ayoub Belhadji and Rémi Bardenet and Pierre Chainais},
 journal = {Journal of Machine Learning Research},
 number = {197},
 openalex = {W2905635580},
 pages = {1--62},
 title = {A determinantal point process for column subset selection},
 url = {http://jmlr.org/papers/v21/19-080.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-083,
 author = {Junhong Lin and Volkan Cevher},
 journal = {Journal of Machine Learning Research},
 number = {20},
 openalex = {W3009640417},
 pages = {1--44},
 title = {Convergences of Regularized Algorithms and Stochastic Gradient Methods with Random Projections},
 url = {http://jmlr.org/papers/v21/19-083.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-095,
 author = {Zeyi Wen and Hanfeng Liu and Jiashuai Shi and Qinbin Li and Bingsheng He and Jian Chen},
 journal = {Journal of Machine Learning Research},
 number = {108},
 openalex = {W2912265134},
 pages = {1--5},
 title = {ThunderGBM: Fast GBDTs and Random Forests on GPUs},
 url = {http://jmlr.org/papers/v21/19-095.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-1015,
 abstract = {Gaussian processes are distributions over functions that are versatile and mathematically convenient priors in Bayesian modelling. However, their use is often impeded for data with large numbers of observations, $N$, due to the cubic (in $N$) cost of matrix operations used in exact inference. Many solutions have been proposed that rely on $M \ll N$ inducing variables to form an approximation at a cost of $\mathcal{O}(NM^2)$. While the computational cost appears linear in $N$, the true complexity depends on how $M$ must scale with $N$ to ensure a certain quality of the approximation. In this work, we investigate upper and lower bounds on how $M$ needs to grow with $N$ to ensure high quality approximations. We show that we can make the KL-divergence between the approximate model and the exact posterior arbitrarily small for a Gaussian-noise regression model with $M\ll N$. Specifically, for the popular squared exponential kernel and $D$-dimensional Gaussian distributed covariates, $M=\mathcal{O}((\log N)^D)$ suffice and a method with an overall computational cost of $\mathcal{O}(N(\log N)^{2D}(\log\log N)^2)$ can be used to perform inference.},
 author = {David R. Burt and Carl Edward Rasmussen and Mark van der Wilk},
 journal = {Journal of Machine Learning Research},
 number = {131},
 openalex = {W3081084362},
 pages = {1--63},
 title = {Convergence of Sparse Variational Inference in Gaussian Processes Regression},
 url = {http://jmlr.org/papers/v21/19-1015.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-102,
 author = {Tom Rainforth and Adam Golinski and Frank Wood and Sheheryar Zaidi},
 journal = {Journal of Machine Learning Research},
 number = {88},
 openalex = {W3039470607},
 pages = {1--54},
 title = {Target–Aware Bayesian Inference: How to Beat Optimal Conventional Estimators},
 url = {http://jmlr.org/papers/v21/19-102.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-1022,
 abstract = {Stochastic convex optimization problems with expectation constraints (SOECs) are encountered in statistics and machine learning, business, and engineering. In data-rich environments, the SOEC objective and constraints contain expectations defined with respect to large datasets. Therefore, efficient algorithms for solving such SOECs need to limit the fraction of data points that they use, which we refer to as algorithmic data complexity. Recent stochastic first order methods exhibit low data complexity when handling SOECs but guarantee near-feasibility and near-optimality only at convergence. These methods may thus return highly infeasible solutions when heuristically terminated, as is often the case, due to theoretical convergence criteria being highly conservative. This issue limits the use of first order methods in several applications where the SOEC constraints encode implementation requirements. We design a stochastic feasible level set method (SFLS) for SOECs that has low data complexity and emphasizes feasibility before convergence. Specifically, our level-set method solves a root-finding problem by calling a novel first order oracle that computes a stochastic upper bound on the level-set function by extending mirror descent and online validation techniques. We establish that SFLS maintains a high-probability feasible solution at each root-finding iteration and exhibits favorable iteration complexity compared to state-of-the-art deterministic feasible level set and stochastic subgradient methods. Numerical experiments on three diverse applications validate the low data complexity of SFLS relative to the former approach and highlight how SFLS finds feasible solutions with small optimality gaps significantly faster than the latter method.},
 author = {Qihang Lin and Selvaprabu Nadarajah and Negar Soheili and Tianbao Yang},
 journal = {Journal of Machine Learning Research},
 number = {143},
 openalex = {W3082888874},
 pages = {1--45},
 title = {A Data Efficient and Feasible Level Set Method for Stochastic Convex Optimization with Expectation Constraints},
 url = {http://jmlr.org/papers/v21/19-1022.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-1026,
 author = {Andrea Rotnitzky and Ezequiel Smucler},
 journal = {Journal of Machine Learning Research},
 number = {188},
 openalex = {W3085944171},
 pages = {1--86},
 title = {Efficient Adjustment Sets for Population Average Causal Treatment Effect Estimation in Graphical Models},
 url = {http://jmlr.org/papers/v21/19-1026.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-1031,
 abstract = {Kernel techniques are among the most widely-applied and influential tools in machine learning with applications at virtually all areas of the field. To combine this expressive power with computational efficiency numerous randomized schemes have been proposed in the literature, among which probably random Fourier features (RFF) are the simplest and most popular. While RFFs were originally designed for the approximation of kernel values, recently they have been adapted to kernel derivatives, and hence to the solution of large-scale tasks involving function derivatives. Unfortunately, the understanding of the RFF scheme for the approximation of higher-order kernel derivatives is quite limited due to the challenging polynomial growing nature of the underlying function class in the empirical process. To tackle this difficulty, we establish a finite-sample deviation bound for a general class of polynomial-growth functions under α-exponential Orlicz condition on the distribution of the sample. Instantiating this result for RFFs, our finite-sample uniform guarantee implies a.s. convergence with tight rate for arbitrary kernel with α-exponential Orlicz spectrum and any order of derivative.},
 author = {Linda Chamakh and Emmanuel Gobet and Zoltán Szabó},
 journal = {Journal of Machine Learning Research},
 number = {145},
 openalex = {W3082346782},
 pages = {1--37},
 title = {Orlicz Random Fourier Features},
 url = {http://jmlr.org/papers/v21/19-1031.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-1032,
 abstract = {We consider the problem of inferring simplified topological substructures—which we term backbones—in metric and non-metric graphs. Intuitively, these are subgraphs with ‘few’ nodes, multifurcations, and cycles, that model the topology of the original graph well. We present a multistep procedure for inferring these backbones. First, we encode local (geometric) information of each vertex in the original graph by means of the boundary coefficient (BC) to identify ‘core’ nodes in the graph. Next, we construct a forest representation of the graph, termed an f-pine, that connects every node of the graph to a local ‘core’ node. The final backbone is then inferred from the f-pine through CLOF (Constrained Leaves Optimal subForest), a novel graph optimization problem we introduce in this paper. On a theoretical level, we show that CLOF is NP-hard for general graphs. However, we prove that CLOF can be efficiently solved for forest graphs, a surprising fact given that CLOF induces a nontrivial monotone submodular set function maximization problem on tree graphs. This result is the basis of our method for mining backbones in graphs through forest representation. We qualitatively and quantitatively confirm the applicability, effectiveness, and scalability of our method for discovering backbones in a variety of graph-structured data, such as social networks, earthquake locations scattered across the Earth, and high-dimensional cell trajectory data},
 author = {Robin Vandaele and Yvan Saeys and Tijl De Bie},
 journal = {Journal of Machine Learning Research},
 number = {215},
 openalex = {W3095254395},
 pages = {1--68},
 title = {Mining Topological Structure in Graphs through Forest Representations},
 url = {http://jmlr.org/papers/v21/19-1032.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-1035,
 author = {Vijay Arya and Rachel K. E. Bellamy and Pin-Yu Chen and Amit Dhurandhar and Michael Hind and Samuel C. Hoffman and Stephanie Houde and Q. Vera Liao and Ronny Luss and Aleksandra Mojsilović and Sami Mourad and Pablo Pedemonte and Ramya Raghavendra and John T. Richards and Prasanna Sattigeri and Karthikeyan Shanmugam and Moninder Singh and Kush R. Varshney and Dennis Wei and Yunfeng Zhang},
 journal = {Journal of Machine Learning Research},
 number = {130},
 openalex = {W3048102985},
 pages = {1--6},
 title = {AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models},
 url = {http://jmlr.org/papers/v21/19-1035.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-1045,
 author = {Rui Tuo and Wenjia Wang},
 journal = {Journal of Machine Learning Research},
 number = {187},
 openalex = {W3085532080},
 pages = {1--38},
 title = {Kriging Prediction with Isotropic Matern Correlations: Robustness and Experimental Designs},
 url = {http://jmlr.org/papers/v21/19-1045.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-1054,
 abstract = {In this paper, we study the dynamic assortment optimization problem under a finite selling season of length $T$. At each time period, the seller offers an arriving customer an assortment of substitutable products under a cardinality constraint, and the customer makes the purchase among offered products according to a discrete choice model. Most existing work associates each product with a real-valued fixed mean utility and assumes a multinomial logit choice (MNL) model. In many practical applications, feature/contexutal information of products is readily available. In this paper, we incorporate the feature information by assuming a linear relationship between the mean utility and the feature. In addition, we allow the feature information of products to change over time so that the underlying choice model can also be non-stationary. To solve the dynamic assortment optimization under this changing contextual MNL model, we need to simultaneously learn the underlying unknown coefficient and makes the decision on the assortment. To this end, we develop an upper confidence bound (UCB) based policy and establish the regret bound on the order of $\widetilde O(d\sqrt{T})$, where $d$ is the dimension of the feature and $\widetilde O$ suppresses logarithmic dependence. We further established the lower bound $\Omega(d\sqrt{T}/K)$ where $K$ is the cardinality constraint of an offered assortment, which is usually small. When $K$ is a constant, our policy is optimal up to logarithmic factors. In the exploitation phase of the UCB algorithm, we need to solve a combinatorial optimization for assortment optimization based on the learned information. We further develop an approximation algorithm and an efficient greedy heuristic. The effectiveness of the proposed policy is further demonstrated by our numerical studies.},
 author = {Xi Chen and Yining Wang and Yuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {216},
 openalex = {W2898644698},
 pages = {1--44},
 title = {Dynamic Assortment Optimization with Changing Contextual Information},
 url = {http://jmlr.org/papers/v21/19-1054.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-117,
 author = {Miriam R. Elman and Jessica Minnier and Xiaohui Chang and Dongseok Choi},
 journal = {Journal of Machine Learning Research},
 number = {36},
 openalex = {W3015544418},
 pages = {1--23},
 title = {Noise Accumulation in High Dimensional Classification and Total Signal Index},
 url = {http://jmlr.org/papers/v21/19-117.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-120,
 abstract = {We develop an encompassing framework for matching, covariate balancing, and doubly-robust methods for causal inference from observational data called generalized optimal matching (GOM). The framework is given by generalizing a new functional-analytical formulation of optimal matching, giving rise to the class of GOM methods, for which we provide a single unified theory to analyze tractability, consistency, and efficiency. Many commonly used existing methods are included in GOM and, using their GOM interpretation, can be extended to optimally and automatically trade off balance for variance and outperform their standard counterparts. As a subclass, GOM gives rise to kernel optimal matching (KOM), which, as supported by new theoretical and empirical results, is notable for combining many of the positive properties of other methods in one. KOM, which is solved as a linearly-constrained convex-quadratic optimization problem, inherits both the interpretability and model-free consistency of matching but can also achieve the $\sqrt{n}$-consistency of well-specified regression and the efficiency and robustness of doubly robust methods. In settings of limited overlap, KOM enables a very transparent method for interval estimation for partial identification and robust coverage. We demonstrate these benefits in examples with both synthetic and real data},
 author = {Nathan Kallus},
 journal = {Journal of Machine Learning Research},
 number = {62},
 openalex = {W3027012348},
 pages = {1--54},
 title = {Generalized Optimal Matching Methods for Causal Inference},
 url = {http://jmlr.org/papers/v21/19-120.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-123,
 abstract = {We propose a general algorithmic framework for Bayesian model selection. A spike-and-slab Laplacian prior is introduced to model the underlying structural assumption. Using the notion of effective resistance, we derive an EM-type algorithm with closed-form iterations to efficiently explore possible candidates for Bayesian model selection. The deterministic nature of the proposed algorithm makes it more scalable to large-scale and high-dimensional data sets compared with existing stochastic search algorithms. When applied to sparse linear regression, our framework recovers the EMVS algorithm [Rockova and George, 2014] as a special case. We also discuss extensions of our framework using tools from graph algebra to incorporate complex Bayesian models such as biclustering and submatrix localization. Extensive simulation studies and real data applications are conducted to demonstrate the superior performance of our methods over its frequentist competitors such as $\ell_0$ or $\ell_1$ penalization.},
 author = {Youngseok Kim and Chao Gao},
 journal = {Journal of Machine Learning Research},
 number = {109},
 openalex = {W3040128710},
 pages = {1--61},
 title = {Bayesian Model Selection with Graph Structured Sparsity},
 url = {http://jmlr.org/papers/v21/19-123.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-144,
 author = {Haoran Wang and Thaleia Zariphopoulou and Xun Yu Zhou},
 journal = {Journal of Machine Learning Research},
 number = {198},
 openalex = {W3097026541},
 pages = {1--34},
 title = {Reinforcement Learning in Continuous Time and Space: A Stochastic Control Approach},
 url = {http://jmlr.org/papers/v21/19-144.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-152,
 abstract = {In this paper we adopt the familiar sparse, high-dimensional linear regression model and focus on the important but often overlooked task of prediction. In particular, we consider a new empirical Bayes framework that incorporates data in the prior in two ways: one is to center the prior for the non-zero regression coefficients and the other is to provide some additional regularization. We show that, in certain settings, the asymptotic concentration of the proposed empirical Bayes posterior predictive distribution is very fast, and we establish a Bernstein--von Mises theorem which ensures that the derived empirical Bayes prediction intervals achieve the targeted frequentist coverage probability. The empirical prior has a convenient conjugate form, so posterior computations are relatively simple and fast. Finally, our numerical results demonstrate the proposed method's strong finite-sample performance in terms of prediction accuracy, uncertainty quantification, and computation time compared to existing Bayesian methods.},
 author = {Ryan Martin and Yiqi Tang},
 journal = {Journal of Machine Learning Research},
 number = {144},
 openalex = {W3082274400},
 pages = {1--30},
 title = {Empirical priors for prediction in sparse high-dimensional linear regression},
 url = {http://jmlr.org/papers/v21/19-152.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-161,
 abstract = {We study the asymptotic consistency properties of $\alpha$-Renyi approximate posteriors, a class of variational Bayesian methods that approximate an intractable Bayesian posterior with a member of a tractable family of distributions, the member chosen to minimize the $\alpha$-Renyi divergence from the true posterior. Unique to our work is that we consider settings with $\alpha > 1$, resulting in approximations that upperbound the log-likelihood, and consequently have wider spread than traditional variational approaches that minimize the Kullback-Liebler (KL) divergence from the posterior. Our primary result identifies sufficient conditions under which consistency holds, centering around the existence of a 'good' sequence of distributions in the approximating family that possesses, among other properties, the right rate of convergence to a limit distribution. We further characterize the good sequence by demonstrating that a sequence of distributions that converges too quickly cannot be a good sequence. We also extend our analysis to the setting where $\alpha$ equals one, corresponding to the minimizer of the reverse KL divergence, and to models with local latent variables. We also illustrate the existence of good sequence with a number of examples. Our results complement a growing body of work focused on the frequentist properties of variational Bayesian methods.},
 author = {Prateek Jaiswal and Vinayak Rao and Harsha Honnappa},
 journal = {Journal of Machine Learning Research},
 number = {156},
 openalex = {W2912073643},
 pages = {1--42},
 title = {Asymptotic Consistency of $\alpha-$R\'enyi-Approximate Posteriors},
 url = {http://jmlr.org/papers/v21/19-161.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-169,
 author = {Yu Wang and Siqi Wu and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {63},
 openalex = {W3027870051},
 pages = {1--52},
 title = {Unique Sharp Local Minimum in L1-minimization Complete Dictionary Learning},
 url = {http://jmlr.org/papers/v21/19-169.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-173,
 author = {Yuankun Zhang and Heng Lian and Yan Yu},
 journal = {Journal of Machine Learning Research},
 number = {224},
 openalex = {W3118218782},
 pages = {1--25},
 title = {Ultra-High Dimensional Single-Index Quantile Regression},
 url = {http://jmlr.org/papers/v21/19-173.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-181,
 author = {Eugen Pircalabelu and Gerda Claeskens},
 journal = {Journal of Machine Learning Research},
 number = {64},
 openalex = {W3027606374},
 pages = {1--32},
 title = {Community-Based Group Graphical Lasso},
 url = {http://jmlr.org/papers/v21/19-181.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-187,
 abstract = {This paper presents a new open source Python framework for causal discovery from observational data and domain background knowledge, aimed at causal graph and causal mechanism modeling. The 'cdt' package implements the end-to-end approach, recovering the direct dependencies (the skeleton of the causal graph) and the causal relationships between variables. It includes algorithms from the 'Bnlearn' and 'Pcalg' packages, together with algorithms for pairwise causal discovery such as ANM. 'cdt' is available under the MIT License at https://github.com/Diviyan-Kalainathan/CausalDiscoveryToolbox.},
 author = {Diviyan Kalainathan and Olivier Goudet and Ritik Dutta},
 journal = {Journal of Machine Learning Research},
 number = {37},
 openalex = {W2935788686},
 pages = {1--5},
 title = {Causal Discovery Toolbox: Uncover causal relationships in Python},
 url = {http://jmlr.org/papers/v21/19-187.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-198,
 abstract = {We study derivative-free methods for policy optimization over the class of linear policies. We focus on characterizing the convergence rate of a canonical stochastic, two-point, derivative-free method for linear-quadratic systems in which the initial state of the system is drawn at random. In particular, we show that for problems with effective dimension $D$, such a method converges to an $\epsilon$-approximate solution within $\widetilde{\mathcal{O}}(D/\epsilon)$ steps, with multiplicative pre-factors that are explicit lower-order polynomial terms in the curvature parameters of the problem. Along the way, we also derive stochastic zero-order rates for a class of non-convex optimization problems.},
 author = {Dhruv Malik and Ashwin Pananjady and Kush Bhatia and Koulik Khamaru and Peter L. Bartlett and Martin J. Wainwright},
 journal = {Journal of Machine Learning Research},
 number = {21},
 openalex = {W2904304778},
 pages = {1--51},
 title = {Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems},
 url = {http://jmlr.org/papers/v21/19-198.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-218,
 abstract = {Sum-of-norms clustering is a method for assigning $n$ points in $\mathbb{R}^d$ to $K$ clusters, $1\le K\le n$, using convex optimization. Recently, Panahi et proved that sum-of-norms clustering is guaranteed to recover a mixture of Gaussians under the restriction that the number of samples is not too large. The purpose of this note is to lift this restriction, i.e., show that sum-of-norms clustering with equal weights can recover a mixture of Gaussians even as the number of samples tends to infinity. Our proof relies on an interesting characterization of clusters computed by sum-of-norms clustering that was developed inside a proof of the agglomeration conjecture by Chiquet et al. Because we believe this theorem has independent interest, we restate and reprove the Chiquet et result herein.},
 author = {Tao Jiang and Stephen Vavasis and Chen Wen Zhai},
 journal = {Journal of Machine Learning Research},
 number = {225},
 openalex = {W3116965578},
 pages = {1--16},
 title = {Recovery of a Mixture of Gaussians by Sum-of-Norms Clustering},
 url = {http://jmlr.org/papers/v21/19-218.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-222,
 author = {Tui H. Nolan and Marianne Menictas and Matt P. Wand},
 journal = {Journal of Machine Learning Research},
 number = {157},
 openalex = {W3086756919},
 pages = {1--62},
 title = {Streamlined Variational Inference with Higher Level Random Effects},
 url = {http://jmlr.org/papers/v21/19-222.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-232,
 abstract = {It is commonplace to encounter heterogeneous or nonstationary data, of which the underlying generating process changes across domains or over time. Such a distribution shift feature presents both challenges and opportunities for causal discovery. In this paper, we develop a framework for causal discovery from such data, called Constraint-based causal Discovery from heterogeneous/NOnstationary Data (CD-NOD), to find causal skeleton and directions and estimate the properties of mechanism changes. First, we propose an enhanced constraint-based procedure to detect variables whose local mechanisms change and recover the skeleton of the causal structure over observed variables. Second, we present a method to determine causal orientations by making use of independent changes in the data distribution implied by the underlying causal model, benefiting from information carried by changing distributions. After learning the causal structure, next, we investigate how to efficiently estimate the driving force of the nonstationarity of a causal mechanism. That is, we aim to extract from data a low-dimensional representation of changes. The proposed methods are nonparametric, with no hard restrictions on data distributions and causal mechanisms, and do not rely on window segmentation. Furthermore, we find that data heterogeneity benefits causal structure identification even with particular types of confounders. Finally, we show the connection between heterogeneity/nonstationarity and soft intervention in causal discovery. Experimental results on various synthetic and real-world data sets (task-fMRI and stock market data) are presented to demonstrate the efficacy of the proposed methods.},
 author = {Biwei Huang and Kun Zhang and Jiji Zhang and Joseph Ramsey and Ruben Sanchez-Romero and Clark Glymour and Bernhard Schölkopf},
 journal = {Journal of Machine Learning Research},
 number = {89},
 openalex = {W2921112192},
 pages = {1--53},
 title = {Causal Discovery from Heterogeneous/Nonstationary Data},
 url = {http://jmlr.org/papers/v21/19-232.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-239,
 abstract = {High dimensional data often contain multiple facets, and several clustering patterns can co-exist under different variable subspaces, also known as the views. While multi-view clustering algorithms were proposed, the uncertainty quantification remains difficult --- a particular challenge is in the high complexity of estimating the cluster assignment probability under each view, and sharing information among views. In this article, we propose an approximate Bayes approach --- treating the similarity matrices generated over the views as rough first-stage estimates for the co-assignment probabilities; in its Kullback-Leibler neighborhood, we obtain a refined low-rank matrix, formed by the pairwise product of simplex coordinates. Interestingly, each simplex coordinate directly encodes the cluster assignment uncertainty. For multi-view clustering, we let each view draw a parameterization from a few candidates, leading to dimension reduction. With high model flexibility, the estimation can be efficiently carried out as a continuous optimization problem, hence enjoys gradient-based computation. The theory establishes the connection of this model to a random partition distribution under multiple views. Compared to single-view clustering approaches, substantially more interpretable results are obtained when clustering brains from a human traumatic brain injury study, using high-dimensional gene expression data. KEY WORDS: Co-regularized Clustering, Consensus, PAC-Bayes, Random Cluster Graph, Variable Selection},
 author = {Leo L. Duan},
 journal = {Journal of Machine Learning Research},
 number = {38},
 openalex = {W2922708282},
 pages = {1--25},
 title = {Latent Simplex Position Model: High Dimensional Multi-view Clustering with Uncertainty Quantification},
 url = {http://jmlr.org/papers/v21/19-239.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-245,
 abstract = {This paper investigates asymptotic behaviors of gradient descent algorithms (particularly accelerated gradient descent and stochastic gradient descent) in the context of stochastic optimization arising in statistics and machine learning where objective functions are estimated from available data. We show that these algorithms can be computationally modeled by continuous-time ordinary or stochastic differential equations. We establish gradient flow central limit theorems to describe the limiting dynamic behaviors of these computational algorithms and the large-sample performances of the related statistical procedures, as the number of algorithm iterations and data size both go to infinity, where the gradient flow central limit theorems are governed by some linear ordinary or stochastic differential equations like time-dependent Ornstein-Uhlenbeck processes. We illustrate that our study can provide a novel unified framework for a joint computational and statistical asymptotic analysis, where the computational asymptotic analysis studies dynamic behaviors of these algorithms with the time (or the number of iterations in the algorithms), the statistical asymptotic analysis investigates large sample behaviors of the statistical procedures (like estimators and classifiers) that the algorithms are applied to compute, and in fact the statistical procedures are equal to the limits of the random sequences generated from these iterative algorithms as the number of iterations goes to infinity. The joint analysis results based on the obtained gradient flow central limit theorems can identify four factors - learning rate, batch size, gradient covariance, and Hessian - to derive new theory regarding the local minima found by stochastic gradient descent for solving non-convex optimization problems.},
 author = {Yazhen Wang and Shang Wu},
 journal = {Journal of Machine Learning Research},
 number = {199},
 openalex = {W2769464037},
 pages = {1--103},
 title = {Asymptotic Analysis via Stochastic Differential Equations of Gradient Descent Algorithms in Statistical and Computational Paradigms},
 url = {http://jmlr.org/papers/v21/19-245.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-246,
 author = {Yu Liu and Kris De Brabanter},
 journal = {Journal of Machine Learning Research},
 number = {65},
 openalex = {W3027685816},
 pages = {1--45},
 title = {Smoothed Nonparametric Derivative Estimation using Weighted Difference Quotients},
 url = {http://jmlr.org/papers/v21/19-246.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-247,
 abstract = {In this paper, we consider high-dimensional nonconvex square-root-loss regression problems and introduce a proximal majorization-minimization (PMM) algorithm for these problems. Our key idea for making the proposed PMM to be efficient is to develop a sparse semismooth Newton method to solve the corresponding subproblems. By using the Kurdyka-Łojasiewicz property exhibited in the underlining problems, we prove that the PMM algorithm converges to a d-stationary point. We also analyze the oracle property of the initial subproblem used in our algorithm. Extensive numerical experiments are presented to demonstrate the high efficiency of the proposed PMM algorithm.},
 author = {Peipei Tang and Chengjing Wang and Defeng Sun and Kim-Chuan Toh},
 journal = {Journal of Machine Learning Research},
 number = {226},
 openalex = {W3118064088},
 pages = {1--38},
 title = {A sparse semismooth Newton based proximal majorization-minimization algorithm for nonconvex square-root-loss regression problems},
 url = {http://jmlr.org/papers/v21/19-247.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-248,
 abstract = {We propose a new stochastic first-order algorithmic framework to solve stochastic composite nonconvex optimization problems that covers both finite-sum and expectation settings. Our algorithms rely on the SARAH estimator introduced in (Nguyen et al, 2017) and consist of two steps: a proximal gradient and an averaging step making them different from existing nonconvex proximal-type algorithms. The algorithms only require an average smoothness assumption of the nonconvex objective term and additional bounded variance assumption if applied to expectation problems. They work with both constant and adaptive step-sizes, while allowing single sample and mini-batches. In all these cases, we prove that our algorithms can achieve the best-known complexity bounds. One key step of our methods is new constant and adaptive step-sizes that help to achieve desired complexity bounds while improving practical performance. Our constant step-size is much larger than existing methods including proximal SVRG schemes in the single sample case. We also specify the algorithm to the non-composite case that covers existing state-of-the-arts in terms of complexity bounds. Our update also allows one to trade-off between step-sizes and mini-batch sizes to improve performance. We test the proposed algorithms on two composite nonconvex problems and neural networks using several well-known datasets.},
 author = {Nhan H. Pham and Lam M. Nguyen and Dzung T. Phan and Quoc Tran-Dinh},
 journal = {Journal of Machine Learning Research},
 number = {110},
 openalex = {W3039770199},
 pages = {1--48},
 title = {ProxSARAH: An Efficient Algorithmic Framework for Stochastic Composite Nonconvex Optimization},
 url = {http://jmlr.org/papers/v21/19-248.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-253,
 abstract = {In this paper, we study the Empirical Risk Minimization (ERM) problem in the non-interactive Local Differential Privacy (LDP) model. Previous research on this problem \citep{smith2017interaction} indicates that the sample complexity, to achieve error $\alpha$, needs to be exponentially depending on the dimensionality $p$ for general loss functions. In this paper, we make two attempts to resolve this issue by investigating conditions on the loss functions that allow us to remove such a limit. In our first attempt, we show that if the loss function is $(\infty, T)$-smooth, by using the Bernstein polynomial approximation we can avoid the exponential dependency in the term of $\alpha$. We then propose player-efficient algorithms with $1$-bit communication complexity and $O(1)$ computation cost for each player. The error bound of these algorithms is asymptotically the same as the original one. With some additional assumptions, we also give an algorithm which is more efficient for the server. In our second attempt, we show that for any $1$-Lipschitz generalized linear convex loss function, there is an $(\epsilon, \delta)$-LDP algorithm whose sample complexity for achieving error $\alpha$ is only linear in the dimensionality $p$. Our results use a polynomial of inner product approximation technique. Finally, motivated by the idea of using polynomial approximation and based on different types of polynomial approximations, we propose (efficient) non-interactive locally differentially private algorithms for learning the set of k-way marginal queries and the set of smooth queries.},
 author = {Di Wang and Marco Gaboardi and Adam Smith and Jinhui Xu},
 journal = {Journal of Machine Learning Research},
 number = {200},
 openalex = {W3096794223},
 pages = {1--39},
 title = {Empirical Risk Minimization in the Non-interactive Local Model of Differential Privacy},
 url = {http://jmlr.org/papers/v21/19-253.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-260,
 abstract = {We consider the problem of learning causal models from observational data generated by linear non-Gaussian acyclic causal models with latent variables. Without considering the effect of latent variables, one usually infers wrong causal relationships among the observed variables. Under faithfulness assumption, we propose a method to check whether there exists a causal path between any two observed variables. From this information, we can obtain the causal order among them. The next question is then whether or not the causal effects can be uniquely identified as well. It can be shown that causal effects among observed variables cannot be identified uniquely even under the assumptions of faithfulness and non-Gaussianity of exogenous noises. However, we will propose an efficient method to identify the set of all possible causal effects that are compatible with the observational data. Furthermore, we present some structural conditions on the causal graph under which we can learn causal effects among observed variables uniquely. We also provide necessary and sufficient graphical conditions for unique identification of the number of variables in the system. Experiments on synthetic data and real-world data show the effectiveness of our proposed algorithm on learning causal models.},
 author = {Saber Salehkaleybar and AmirEmad Ghassami and Negar Kiyavash and Kun Zhang},
 journal = {Journal of Machine Learning Research},
 number = {39},
 openalex = {W3014036758},
 pages = {1--24},
 title = {Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables},
 url = {http://jmlr.org/papers/v21/19-260.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-264,
 abstract = {We develop a pivotal test to assess the statistical significance of the feature variables in a single-layer feedforward neural network regression model. We propose a gradient-based test statistic and study its asymptotics using nonparametric techniques. Under technical conditions, the limiting distribution is given by a mixture of chi-square distributions. The tests enable one to discern the impact of individual variables on the prediction of a neural network. The test statistic can be used to rank variables according to their influence. Simulation results illustrate the computational efficiency and the performance of the test. An empirical application to house price valuation highlights the behavior of the test using actual data.},
 author = {Enguerrand Horel and Kay Giesecke},
 journal = {Journal of Machine Learning Research},
 number = {227},
 openalex = {W4288580299},
 pages = {1--29},
 title = {Significance Tests for Neural Networks},
 url = {http://jmlr.org/papers/v21/19-264.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-265,
 author = {Haishan Ye and Luo Luo and Zhihua Zhang},
 journal = {Journal of Machine Learning Research},
 number = {142},
 openalex = {W3082625493},
 pages = {1--37},
 title = {Nesterov's Acceleration For Approximate Newton},
 url = {http://jmlr.org/papers/v21/19-265.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-276,
 abstract = {Graph learning from data represents a canonical problem that has received substantial attention in the literature. However, insufficient work has been done in incorporating prior structural knowledge onto the learning of underlying graphical models from data. Learning a graph with a specific structure is essential for interpretability and identification of the relationships among data. Useful structured graphs include the multi-component graph, bipartite graph, connected graph, sparse graph, and regular graph. In general, structured graph learning is an NP-hard combinatorial problem, therefore, designing a general tractable optimization method is extremely challenging. In this paper, we introduce a unified graph learning framework lying at the integration of Gaussian graphical models and spectral graph theory. To impose a particular structure on a graph, we first show how to formulate the combinatorial constraints as an analytical property of the graph matrix. Then we develop an optimization framework that leverages graph learning with specific structures via spectral constraints on graph matrices. The proposed algorithms are provably convergent, computationally efficient, and practically amenable for numerous graph-based tasks. Extensive numerical experiments with both synthetic and real data sets illustrate the effectiveness of the proposed algorithms. The code for all the simulations is made available as an open source repository.},
 author = {Sandeep Kumar and Jiaxi Ying and José Vinícius de M. Cardoso and Daniel P. Palomar},
 journal = {Journal of Machine Learning Research},
 number = {22},
 openalex = {W2937507957},
 pages = {1--60},
 title = {A Unified Framework for Structured Graph Learning via Spectral Constraints},
 url = {http://jmlr.org/papers/v21/19-276.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-277,
 abstract = {In many areas, practitioners need to analyze large datasets that challenge conventional single-machine computing. To scale up data analysis, distributed and parallel computing approaches are increasingly needed. 
Here we study a fundamental and highly important problem in this area: How to do ridge regression in a distributed computing environment? Ridge regression is an extremely popular method for supervised learning, and has several optimality properties, thus it is important to study. We study one-shot methods that construct weighted combinations of ridge regression estimators computed on each machine. By analyzing the mean squared error in a high dimensional random-effects model where each predictor has a small effect, we discover several new phenomena. 
1. Infinite-worker limit: The distributed estimator works well for very large numbers of machines, a phenomenon we call infinite-worker limit. 
2. Optimal weights: The optimal weights for combining local estimators sum to more than unity, due to the downward bias of ridge. Thus, all averaging methods are suboptimal. 
We also propose a new Weighted ONe-shot DistributEd Ridge regression (WONDER) algorithm. We test WONDER in simulation studies and using the Million Song Dataset as an example. There it can save at least 100x in computation time, while nearly preserving test accuracy.},
 author = {Edgar Dobriban and Yue Sheng},
 journal = {Journal of Machine Learning Research},
 number = {66},
 openalex = {W3027670001},
 pages = {1--52},
 title = {WONDER: Weighted One-shot Distributed Ridge Regression in High Dimensions},
 url = {http://jmlr.org/papers/v21/19-277.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-290,
 abstract = {Tree data are ubiquitous because they model a large variety of situations, e.g., the architecture of plants, the secondary structure of RNA, or the hierarchy of XML files. Nevertheless, the analysis of these non-Euclidean data is difficult per se. In this paper, we focus on the subtree kernel that is a convolution kernel for tree data introduced by Vishwanathan and Smola in the early 2000's. More precisely, we investigate the influence of the weight function from a theoretical perspective and in real data applications. We establish on a 2-classes stochastic model that the performance of the subtree kernel is improved when the weight of leaves vanishes, which motivates the definition of a new weight function, learned from the data and not fixed by the user as usually done. To this end, we define a unified framework for computing the subtree kernel from ordered or unordered trees, that is particularly suitable for tuning parameters. We show through eight real data classification problems the great efficiency of our approach, in particular for small datasets, which also states the high importance of the weight function. Finally, a visualization tool of the significant features is derived.},
 author = {Romain Azaïs and Florian Ingels},
 journal = {Journal of Machine Learning Research},
 number = {67},
 openalex = {W2936284547},
 pages = {1--36},
 title = {The Weight Function in the Subtree Kernel is Decisive},
 url = {http://jmlr.org/papers/v21/19-290.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-299,
 abstract = {We study bipartite community detection in networks, or more generally the network biclustering problem. We present a fast two-stage procedure based on spectral initialization followed by the application of a pseudo-likelihood classifier twice. Under mild regularity conditions, we establish the weak consistency of the procedure (i.e., the convergence of the misclassification rate to zero) under a general bipartite stochastic block model. We show that the procedure is optimal in the sense that it achieves the optimal convergence rate that is achievable by a biclustering oracle, adaptively over the whole class, up to constants. This is further formalized by deriving a minimax lower bound over a class of biclustering problems. The optimal rate we obtain sharpens some of the existing results and generalizes others to a wide regime of average degree growth, from sparse networks with average degrees growing arbitrarily slowly to fairly dense networks with average degrees of order $\sqrt{n}$. As a special case, we recover the known exact recovery threshold in the $\log n$ regime of sparsity. To obtain the consistency result, as part of the provable version of the algorithm, we introduce a sub-block partitioning scheme that is also computationally attractive, allowing for distributed implementation of the algorithm without sacrificing optimality. The provable algorithm is derived from a general class of pseudo-likelihood biclustering algorithms that employ simple EM type updates. We show the effectiveness of this general class by numerical simulations.},
 author = {Zhixin Zhou and Arash A. Amini},
 journal = {Journal of Machine Learning Research},
 number = {40},
 openalex = {W3011766796},
 pages = {1--68},
 title = {Optimal Bipartite Network Clustering},
 url = {http://jmlr.org/papers/v21/19-299.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-307,
 abstract = {Decision trees are flexible models that are well suited for many statistical regression problems. In a Bayesian framework for regression trees, Markov Chain Monte Carlo (MCMC) search algorithms are required to generate samples of tree models according to their posterior probabilities. The critical component of such an MCMC algorithm is to construct good Metropolis-Hastings steps for updating the tree topology. However, such algorithms frequently suffering from local mode stickiness and poor mixing. As a result, the algorithms are slow to converge. Hitherto, authors have primarily used discrete-time birth/death mechanisms for Bayesian (sums of) regression tree models to explore the model space. These algorithms are efficient only if the acceptance rate is high which is not always the case. Here we overcome this issue by developing a new search algorithm which is based on a continuous-time birth-death Markov process. This search algorithm explores the model space by jumping between parameter spaces corresponding to different tree structures. In the proposed algorithm, the moves between models are always accepted which can dramatically improve the convergence and mixing properties of the MCMC algorithm. We provide theoretical support of the algorithm for Bayesian regression tree models and demonstrate its performance.},
 author = {Reza Mohammadi and Matthew Pratola and Maurits Kaptein},
 journal = {Journal of Machine Learning Research},
 number = {201},
 openalex = {W4288365976},
 pages = {1--26},
 title = {Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models},
 url = {http://jmlr.org/papers/v21/19-307.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-318,
 abstract = {Structure learning of Bayesian networks has always been a challenging problem. Nowadays, massive-size networks with thousands or more of nodes but fewer samples frequently appear in many areas. We develop a divide-and-conquer framework, called partition-estimation-fusion (PEF), for structure learning of such big networks. The proposed method first partitions nodes into clusters, then learns a subgraph on each cluster of nodes, and finally fuses all learned subgraphs into one Bayesian network. The PEF method is designed in a flexible way so that any structure learning method may be used in the second step to learn a subgraph structure as either a DAG or a CPDAG. In the clustering step, we adapt the hierarchical clustering method to automatically choose a proper number of clusters. In the fusion step, we propose a novel hybrid method that sequentially add edges between subgraphs. Extensive numerical experiments demonstrate the competitive performance of our PEF method, in terms of both speed and accuracy compared to existing methods. Our method can improve the accuracy of structure learning by 20% or more, while reducing running time up to two orders-of-magnitude.},
 author = {Jiaying Gu and Qing Zhou},
 journal = {Journal of Machine Learning Research},
 number = {158},
 openalex = {W2941343371},
 pages = {1--31},
 title = {Learning big Gaussian Bayesian networks: partition, estimation, and fusion},
 url = {http://jmlr.org/papers/v21/19-318.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-322,
 abstract = {Treating neural network inputs and outputs as random variables, we characterize the structure of neural networks that can be used to model data that are invariant or equivariant under the action of a compact group. Much recent research has been devoted to encoding invariance under symmetry transformations into neural network architectures, in an effort to improve the performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised settings. By considering group invariance from the perspective of probabilistic symmetry, we establish a link between functional and probabilistic symmetry, and obtain generative functional representations of probability distributions that are invariant or equivariant under the action of a compact group. Our representations completely characterize the structure of neural networks that can be used to model such distributions and yield a general program for constructing invariant stochastic or deterministic neural networks. We demonstrate that examples from the recent literature are special cases, and develop the details of the general program for exchangeable sequences and arrays.},
 author = {Benjamin Bloem-Reddy and { Yee Whye } Teh},
 journal = {Journal of Machine Learning Research},
 number = {90},
 openalex = {W2909878113},
 pages = {1--61},
 title = {Probabilistic symmetries and invariant neural networks},
 url = {http://jmlr.org/papers/v21/19-322.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-327,
 abstract = {Stochastic gradient Langevin dynamics (SGLD) is a fundamental algorithm in stochastic optimization. Recent work by Zhang et al. [2017] presents an analysis for the hitting time of SGLD for the first and second order stationary points. The proof in Zhang et al. [2017] is a two-stage procedure through bounding the Cheeger's constant, which is rather complicated and leads to loose bounds. In this paper, using intuitions from stochastic differential equations, we provide a direct analysis for the hitting times of SGLD to the first and second order stationary points. Our analysis is straightforward. It only relies on basic linear algebra and probability theory tools. Our direct analysis also leads to tighter bounds comparing to Zhang et al. [2017] and shows the explicit dependence of the hitting time on different factors, including dimensionality, smoothness, noise strength, and step size effects. Under suitable conditions, we show that the hitting time of SGLD to first-order stationary points can be dimension-independent. Moreover, we apply our analysis to study several important online estimation problems in machine learning, including linear regression, matrix factorization, and online PCA.},
 author = {Xi Chen and Simon S. Du and Xin T. Tong},
 journal = {Journal of Machine Learning Research},
 number = {68},
 openalex = {W3028236308},
 pages = {1--41},
 title = {On Stationary-Point Hitting Time and Ergodicity of Stochastic Gradient Langevin Dynamics},
 url = {http://jmlr.org/papers/v21/19-327.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-339,
 author = {Frederik Heber and Žofia Trst’anová and Benedict Leimkuhler},
 journal = {Journal of Machine Learning Research},
 number = {228},
 openalex = {W3116727811},
 pages = {1--33},
 title = {Posterior sampling strategies based on discretized stochastic differential equations for machine learning applications},
 url = {http://jmlr.org/papers/v21/19-339.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-346,
 abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
 author = {Shakir Mohamed and Mihaela Rosca and Michael Figurnov and Andriy Mnih},
 journal = {Journal of Machine Learning Research},
 number = {132},
 openalex = {W3082278101},
 pages = {1--62},
 title = {Monte Carlo Gradient Estimation in Machine Learning},
 url = {http://jmlr.org/papers/v21/19-346.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-347,
 author = {Morteza Ashraphijuo and Xiaodong Wang},
 journal = {Journal of Machine Learning Research},
 number = {69},
 openalex = {W3026653732},
 pages = {1--36},
 title = {Union of Low-Rank Tensor Spaces: Clustering and Completion},
 url = {http://jmlr.org/papers/v21/19-347.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-348,
 author = {Edesio Alcobaça and Felipe Siqueira and Adriano Rivolli and Luís P. F. Garcia and Jefferson T. Oliva and André C. P. L. F. de Carvalho},
 journal = {Journal of Machine Learning Research},
 number = {111},
 openalex = {W3044965819},
 pages = {1--5},
 title = {MFE: Towards reproducible meta-feature extraction},
 url = {http://jmlr.org/papers/v21/19-348.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-359,
 author = {Yao Ma and Alex Olshevsky and Csaba Szepesvari and Venkatesh Saligrama},
 journal = {Journal of Machine Learning Research},
 number = {133},
 openalex = {W2803815016},
 pages = {1--36},
 title = {Gradient descent for sparse rank-one matrix completion for crowd-sourced aggregation of sparsely interacting workers},
 url = {http://jmlr.org/papers/v21/19-359.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-362,
 abstract = {Although multivariate count data are routinely collected in many application areas, there is surprisingly little work developing flexible models for characterizing their dependence structure. This is particularly true when interest focuses on inferring the conditional independence graph. In this article, we propose a new class of pairwise Markov random field-type models for the joint distribution of a multivariate count vector. By employing a novel type of transformation, we avoid restricting to non-negative dependence structures or inducing other restrictions through truncations. Taking a Bayesian approach to inference, we choose a Dirichlet process prior for the distribution of a random effect to induce great flexibility in the specification. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed for posterior computation. We prove various theoretical properties, including posterior consistency, and show that our COunt Nonparametric Graphical Analysis (CONGA) approach has good performance relative to competitors in simulation studies. The methods are motivated by an application to neuron spike count data in mice.},
 author = {Arkaprava Roy and David B Dunson},
 journal = {Journal of Machine Learning Research},
 number = {229},
 openalex = {W2908008786},
 pages = {1--21},
 title = {Nonparametric graphical model for counts},
 url = {http://jmlr.org/papers/v21/19-362.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-368,
 author = {Yan Ru Pei and Haik Manukian and Massimiliano Di Ventra},
 journal = {Journal of Machine Learning Research},
 number = {159},
 openalex = {W3086762529},
 pages = {1--55},
 title = {Generating Weighted MAX-2-SAT Instances with Frustrated Loops: an RBM Case Study},
 url = {http://jmlr.org/papers/v21/19-368.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-383,
 abstract = {Probabilistic graphical models provide a flexible yet parsimonious framework for modeling dependencies among nodes in networks. There is a vast literature on parameter estimation and consistent model selection for graphical models. However, in many of the applications, scientists are also interested in quantifying the uncertainty associated with the estimated parameters and selected models, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for statistical inference on edge parameters in pairwise graphical models based on generalized Hyvärinen scoring rule. Hyvärinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form, which is a common problem for graphical models, including Ising models and truncated Gaussian graphical models. Our estimator allows us to perform statistical inference for general graphical models whereas the existing works mostly focus on statistical inference for Gaussian graphical models where finding normalizing constant is computationally tractable. Under mild conditions that are typically assumed in the literature for consistent estimation, we prove that our proposed estimator is $\sqrt{n}$-consistent and asymptotically normal, which allows us to construct confidence intervals and build hypothesis tests for edge parameters. Moreover, we show how our proposed method can be applied to test hypotheses that involve a large number of model parameters simultaneously. We illustrate validity of our estimator through extensive simulation studies on a diverse collection of data-generating processes.},
 author = {Ming Yu and Varun Gupta and Mladen Kolar},
 journal = {Journal of Machine Learning Research},
 number = {91},
 openalex = {W2945860003},
 pages = {1--51},
 title = {Simultaneous Inference for Pairwise Graphical Models with Generalized Score Matching},
 url = {http://jmlr.org/papers/v21/19-383.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-407,
 abstract = {Given a response $Y$ and a vector $X = (X^1, \dots, X^d)$ of $d$ predictors, we investigate the problem of inferring direct causes of $Y$ among the vector $X$. Models for $Y$ that use all of its causal covariates as predictors enjoy the property of being invariant across different environments or interventional settings. Given data from such environments, this property has been exploited for causal discovery. Here, we extend this inference principle to situations in which some (discrete-valued) direct causes of $ Y $ are unobserved. Such cases naturally give rise to switching regression models. We provide sufficient conditions for the existence, consistency and asymptotic normality of the MLE in linear switching regression models with Gaussian noise, and construct a test for the equality of such models. These results allow us to prove that the proposed causal discovery method obtains asymptotic false discovery control under mild conditions. We provide an algorithm, make available code, and test our method on simulated data. It is robust against model violations and outperforms state-of-the-art approaches. We further apply our method to a real data set, where we show that it does not only output causal predictors, but also a process-based clustering of data points, which could be of additional interest to practitioners.},
 author = {Rune Christiansen and Jonas Peters},
 journal = {Journal of Machine Learning Research},
 number = {41},
 openalex = {W3013079341},
 pages = {1--46},
 title = {Switching Regression Models and Causal Inference in the Presence of Discrete Latent Variables},
 url = {http://jmlr.org/papers/v21/19-407.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-408,
 author = {Dimitris Bertsimas and Ivan Paskov},
 journal = {Journal of Machine Learning Research},
 number = {230},
 openalex = {W3116763499},
 pages = {1--25},
 title = {Stable Regression: On the Power of Optimization over Randomization},
 url = {http://jmlr.org/papers/v21/19-408.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-428,
 abstract = {This work was funded by the office of sponsored research at KAUST. Part of the results in this paper has been presented at the IEEE International Workshop on Signal Processing Advances in Wireless Communications, 2018.},
 author = {Houssem Sifaou and Abla Kammoun and Mohamed-Slim Alouini},
 journal = {Journal of Machine Learning Research},
 number = {112},
 openalex = {W3039874824},
 pages = {1--24},
 title = {High-dimensional Linear Discriminant Analysis Classifier for Spiked Covariance Model},
 url = {http://jmlr.org/papers/v21/19-428.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-429,
 abstract = {We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage.},
 author = {Jian Guo and He He and Tong He and Leonard Lausen and Mu Li and Haibin Lin and Xingjian Shi and Chenguang Wang and Junyuan Xie and Sheng Zha and Aston Zhang and Hang Zhang and Zhi Zhang and Zhongyue Zhang and Shuai Zheng and Yi Zhu},
 journal = {Journal of Machine Learning Research},
 number = {23},
 openalex = {W2956612537},
 pages = {1--7},
 title = {GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing},
 url = {http://jmlr.org/papers/v21/19-429.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-441,
 abstract = {Hamiltonian Monte Carlo (HMC) is a state-of-the-art Markov chain Monte Carlo
sampling algorithm for drawing samples from smooth probability densities over
continuous spaces. We study the variant most widely used in practice,
Metropolized HMC with the St\{o}rmer-Verlet or leapfrog integrator, and make
two primary contributions. First, we provide a non-asymptotic upper bound on
the mixing time of the Metropolized HMC with explicit choices of step-size and
number of leapfrog steps. This bound gives a precise quantification of the
faster convergence of Metropolized HMC relative to simpler MCMC algorithms such
as the Metropolized random walk, or Metropolized Langevin algorithm. Second, we
provide a general framework for sharpening mixing time bounds of Markov chains
initialized at a substantial distance from the target distribution over
continuous spaces. We apply this sharpening device to the Metropolized random
walk and Langevin algorithms, thereby obtaining improved mixing time bounds
from a non-warm initial distribution.},
 author = {Yuansi Chen and Raaz Dwivedi and Martin J. Wainwright and Bin Yu},
 journal = {Journal of Machine Learning Research},
 number = {92},
 openalex = {W3038331852},
 pages = {1--72},
 title = {Fast mixing of Metropolized Hamiltonian Monte Carlo: Benefits of multi-step gradients},
 url = {http://jmlr.org/papers/v21/19-441.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-447,
 abstract = {Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.},
 author = {Seyed Mehran Kazemi and Rishab Goel and Kshitij Jain and Ivan Kobyzev and Akshay Sethi and Peter Forsyth and Pascal Poupart},
 journal = {Journal of Machine Learning Research},
 number = {70},
 openalex = {W3026076535},
 pages = {1--73},
 title = {Representation Learning for Dynamic Graphs: A Survey},
 url = {http://jmlr.org/papers/v21/19-447.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-462,
 abstract = {Robust scatter estimation is a fundamental task in statistics. The recent discovery on the connection between robust estimation and generative adversarial nets (GANs) by Gao et al. (2018) suggests that it is possible to compute depth-like robust estimators using similar techniques that optimize GANs. In this paper, we introduce a general learning via classification framework based on the notion of proper scoring rules. This framework allows us to understand both matrix depth function and various GANs through the lens of variational approximations of $f$-divergences induced by proper scoring rules. We then propose a new class of robust scatter estimators in this framework by carefully constructing discriminators with appropriate neural network structures. These estimators are proved to achieve the minimax rate of scatter estimation under Huber's contamination model. Our numerical results demonstrate its good performance under various settings against competitors in the literature.},
 author = {Chao Gao and Yuan Yao and Weizhi Zhu},
 journal = {Journal of Machine Learning Research},
 number = {160},
 openalex = {W3085860822},
 pages = {1--48},
 title = {Generative Adversarial Nets for Robust Scatter Estimation: A Proper Scoring Rule Perspective},
 url = {http://jmlr.org/papers/v21/19-462.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-467,
 abstract = {We present apricot, an open source Python package for selecting representative subsets from large data sets using submodular optimization. The package implements an efficient greedy selection algorithm that offers strong theoretical guarantees on the quality of the selected set. Two submodular set functions are implemented in apricot: facility location, which is broadly applicable but requires memory quadratic in the number of examples in the data set, and a feature-based function that is less broadly applicable but can scale to millions of examples. Apricot is extremely efficient, using both algorithmic speedups such as the lazy greedy algorithm and code optimizers such as numba. We demonstrate the use of subset selection by training machine learning models to comparable accuracy using either the full data set or a representative subset thereof. This paper presents an explanation of submodular selection, an overview of the features in apricot, and an application to several data sets. The code and tutorial Jupyter notebooks are available at https://github.com/jmschrei/apricot},
 author = {Jacob Schreiber and Jeffrey Bilmes and William Stafford Noble},
 journal = {Journal of Machine Learning Research},
 number = {161},
 openalex = {W2948852427},
 pages = {1--6},
 title = {apricot: Submodular selection for data summarization in Python},
 url = {http://jmlr.org/papers/v21/19-467.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-468,
 abstract = {The success of Deep Learning and its potential use in many safety-critical applications has motivated research on formal verification of Neural Network (NN) models. In this context, verification involves proving or disproving that an NN model satisfies certain input-output properties. Despite the reputation of learned NN models as black boxes, and the theoretical hardness of proving useful properties about them, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure and taking insights from formal methods such as Satisifiability Modulo Theory. However, these methods are still far from scaling to realistic neural networks. To facilitate progress on this crucial area, we exploit the Mixed Integer Linear Programming (MIP) formulation of verification to propose a family of algorithms based on Branch-and-Bound (BaB). We show that our family contains previous verification methods as special cases. With the help of the BaB framework, we make three key contributions. Firstly, we identify new methods that combine the strengths of multiple existing approaches, accomplishing significant performance improvements over previous state of the art. Secondly, we introduce an effective branching strategy on ReLU non-linearities. This branching strategy allows us to efficiently and successfully deal with high input dimensional problems with convolutional network architecture, on which previous methods fail frequently. Finally, we propose comprehensive test data sets and benchmarks which includes a collection of previously released testcases. We use the data sets to conduct a thorough experimental comparison of existing and new algorithms and to provide an inclusive analysis of the factors impacting the hardness of verification problems.},
 author = {Rudy Bunel and Jingyue Lu and Ilker Turkaslan and Philip H.S. Torr and Pushmeet Kohli and M. Pawan Kumar},
 journal = {Journal of Machine Learning Research},
 number = {42},
 openalex = {W3012981624},
 pages = {1--39},
 title = {Branch and Bound for Piecewise Linear Neural Network Verification},
 url = {http://jmlr.org/papers/v21/19-468.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-470,
 author = {Davide Bacciu and Federico Errica and Alessio Micheli},
 journal = {Journal of Machine Learning Research},
 number = {134},
 openalex = {W3047991353},
 pages = {1--39},
 title = {Probabilistic Learning on Graphs via Contextual Architectures},
 url = {http://jmlr.org/papers/v21/19-470.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-471,
 abstract = {We formulate the problem of matrix completion with and without side information as a non-convex optimization problem. We design fastImpute based on non-convex gradient descent and show it converges to a global minimum that is guaranteed to recover closely the underlying matrix while it scales to matrices of sizes beyond $10^5 \times 10^5$. We report experiments on both synthetic and real-world datasets that show fastImpute is competitive in both the accuracy of the matrix recovered and the time needed across all cases. Furthermore, when a high number of entries are missing, fastImpute is over $75\%$ lower in MAPE and $15$ times faster than current state-of-the-art matrix completion methods in both the case with side information and without.},
 author = {Dimitris Bertsimas and Michael Lingzhi Li},
 journal = {Journal of Machine Learning Research},
 number = {231},
 openalex = {W4288090868},
 pages = {1--43},
 title = {Fast Exact Matrix Completion: A Unified Optimization Framework for Matrix Completion},
 url = {http://jmlr.org/papers/v21/19-471.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-496,
 abstract = {We consider the problem of estimating the latent structure of a social network based on the observed information diffusion events, or cascades, where the observations for a given cascade consist of only the timestamps of infection for infected nodes but not the source of the infection. Most of the existing work on this problem has focused on estimating a diffusion matrix without any structural assumptions on it. In this paper, we propose a novel model based on the intuition that an information is more likely to propagate among two nodes if they are interested in similar topics which are also prominent in the information content. In particular, our model endows each node with an influence vector (which measures how authoritative the node is on each topic) and a receptivity vector (which measures how susceptible the node is for each topic). We show how this node-topic structure can be estimated from the observed cascades, and prove the consistency of the estimator. Experiments on synthetic and real data demonstrate the improved performance and better interpretability of our model compared to existing state-of-the-art methods.},
 author = {Ming Yu and Varun Gupta and Mladen Kolar},
 journal = {Journal of Machine Learning Research},
 number = {71},
 openalex = {W3026581590},
 pages = {1--47},
 title = {Estimation of a Low-rank Topic-Based Model for Information Cascades},
 url = {http://jmlr.org/papers/v21/19-496.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-505,
 abstract = {In supervised learning, we typically leverage a fully labeled dataset to design methods for function estimation or prediction. In many practical situations, we are able to obtain alternative feedback, possibly at a low cost. A broad goal is to understand the usefulness of, and to design algorithms to exploit, this alternative feedback. In this paper, we consider a semi-supervised regression setting, where we obtain additional ordinal (or comparison) information for the unlabeled samples. We consider ordinal feedback of varying qualities where we have either a perfect ordering of the samples, a noisy ordering of the samples or noisy pairwise comparisons between the samples. We provide a precise quantification of the usefulness of these types of ordinal feedback in both nonparametric and linear regression, showing that in many cases it is possible to accurately estimate an underlying function with a very small labeled set, effectively \emph{escaping the curse of dimensionality}. We also present lower bounds, that establish fundamental limits for the task and show that our algorithms are optimal in a variety of settings. Finally, we present extensive experiments on new datasets that demonstrate the efficacy and practicality of our algorithms and investigate their robustness to various sources of noise and model misspecification.},
 author = {Yichong Xu and Sivaraman Balakrishnan and Aarti Singh and Artur Dubrawski},
 journal = {Journal of Machine Learning Research},
 number = {162},
 openalex = {W3085286395},
 pages = {1--54},
 title = {Regression with Comparisons: Escaping the Curse of Dimensionality with Ordinal Information},
 url = {http://jmlr.org/papers/v21/19-505.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-514,
 author = {Maxim Borisyak and Artem Ryzhikov and Andrey Ustyuzhanin and Denis Derkach and Fedor Ratnikov and Olga Mineeva},
 journal = {Journal of Machine Learning Research},
 number = {72},
 openalex = {W3028082542},
 pages = {1--22},
 title = {(1 + epsilon)-class Classification: an Anomaly Detection Method for Highly Imbalanced or Incomplete Data Sets},
 url = {http://jmlr.org/papers/v21/19-514.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-524,
 abstract = {Many methods for machine learning rely on approximate inference from intractable probability distributions. Variational inference approximates such distributions by tractable models that can be subsequently used for approximate inference. Learning sufficiently accurate approximations requires a rich model family and careful exploration of the relevant modes of the target distribution. We propose a method for learning accurate GMM approximations of intractable probability distributions based on insights from policy search by using information-geometric trust regions for principled exploration. For efficient improvement of the GMM approximation, we derive a lower bound on the corresponding optimization objective enabling us to update the components independently. Our use of the lower bound ensures convergence to a stationary point of the original objective. The number of components is adapted online by adding new components in promising regions and by deleting components with negligible weight. We demonstrate on several domains that we can learn approximations of complex, multimodal distributions with a quality that is unmet by previous variational inference methods, and that the GMM approximation can be used for drawing samples that are on par with samples created by state-of-theart MCMC samplers while requiring up to three orders of magnitude less computational resources.},
 author = {Oleg Arenz and Mingjun Zhong and Gerhard Neumann},
 journal = {Journal of Machine Learning Research},
 number = {163},
 openalex = {W3085775742},
 pages = {1--60},
 title = {Trust-Region Variational Inference with Gaussian Mixture Models},
 url = {http://jmlr.org/papers/v21/19-524.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-535,
 abstract = {Predict a new response from a covariate is a challenging task in regression, which raises new question since the era of high-dimensional data. In this paper, we are interested in the inverse regression method from a theoretical viewpoint. Theoretical results have already been derived for the well-known linear model, but recently, the curse of dimensionality has increased the interest of practitioners and theoreticians into generalization of those results for various estimators, calibrated for the high-dimension context. To deal with high-dimensional data, inverse regression is used in this paper. It is known to be a reliable and efficient approach when the number of features exceeds the number of observations. Indeed, under some conditions, dealing with the inverse regression problem associated to a forward regression problem drastically reduces the number of parameters to estimate and make the problem tractable. When both the responses and the covariates are multivariate, estimators constructed by the inverse regression are studied in this paper, the main result being explicit asymptotic prediction regions for the response. The performances of the proposed estimators and prediction regions are also analyzed through a simulation study and compared with usual estimators.},
 author = {Emilie Devijver and Emeline Perthame},
 journal = {Journal of Machine Learning Research},
 number = {113},
 openalex = {W2867982072},
 pages = {1--24},
 title = {Prediction regions through Inverse Regression},
 url = {http://jmlr.org/papers/v21/19-535.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-536,
 author = {James Johndrow and Paulo Orenstein and Anirban Bhattacharya},
 journal = {Journal of Machine Learning Research},
 number = {73},
 openalex = {W3028553139},
 pages = {1--61},
 title = {Scalable Approximate MCMC Algorithms for the Horseshoe Prior},
 url = {http://jmlr.org/papers/v21/19-536.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-537,
 abstract = {Feature screening is a powerful tool in the analysis of high dimensional data. When the sample size $N$ and the number of features $p$ are both large, the implementation of classic screening methods can be numerically challenging. In this paper, we propose a distributed screening framework for big data setup. In the spirit of "divide-and-conquer", the proposed framework expresses a correlation measure as a function of several component parameters, each of which can be distributively estimated using a natural U-statistic from data segments. With the component estimates aggregated, we obtain a final correlation estimate that can be readily used for screening features. This framework enables distributed storage and parallel computing and thus is computationally attractive. Due to the unbiased distributive estimation of the component parameters, the final aggregated estimate achieves a high accuracy that is insensitive to the number of data segments $m$ specified by the problem itself or to be chosen by users. Under mild conditions, we show that the aggregated correlation estimator is as efficient as the classic centralized estimator in terms of the probability convergence bound; the corresponding screening procedure enjoys sure screening property for a wide range of correlation measures. The promising performances of the new method are supported by extensive numerical examples.},
 author = {Xingxiang Li and Runze Li and Zhiming Xia and Chen Xu},
 journal = {Journal of Machine Learning Research},
 number = {24},
 openalex = {W2921467752},
 pages = {1--32},
 title = {Distributed Feature Screening via Componentwise Debiasing},
 url = {http://jmlr.org/papers/v21/19-537.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-540,
 abstract = {Mapper is an unsupervised machine learning algorithm generalising the notion of clustering to obtain a geometric description of a dataset. The procedure splits the data into possibly overlapping bins which are then clustered. The output of the algorithm is a graph where nodes represent clusters and edges represent the sharing of data points between two clusters. However, several parameters must be selected before applying Mapper and the resulting graph may vary dramatically with the choice of parameters. We define an intrinsic notion of Mapper instability that measures the variability of the output as a function of the choice of parameters required to construct a Mapper output. Our results and discussion are general and apply to all Mapper-type algorithms. We derive theoretical results that provide estimates for the instability and suggest practical ways to control it. We provide also experiments to illustrate our results and in particular we demonstrate that a reliable candidate Mapper output can be identified as a local minimum of instability regarded as a function of Mapper input parameters.},
 author = {Francisco Belchi and Jacek Brodzki and Matthew Burfitt and Mahesan Niranjan},
 journal = {Journal of Machine Learning Research},
 number = {202},
 openalex = {W2948336192},
 pages = {1--45},
 title = {A numerical measure of the instability of Mapper-type algorithms},
 url = {http://jmlr.org/papers/v21/19-540.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-545,
 abstract = {Standard high-dimensional regression methods assume that the underlying coefficient vector is sparse. This might not be true in some cases, in particular in presence of hidden, confounding variables. Such hidden confounding can be represented as a high-dimensional linear model where the sparse coefficient vector is perturbed. For this model, we develop and investigate a class of methods that are based on running the Lasso on preprocessed data. The preprocessing step consists of applying certain spectral transformations that change the singular values of the design matrix. We show that, under some assumptions, one can achieve the optimal $\ell_1$-error rate for estimating the underlying sparse coefficient vector. Our theory also covers the Lava estimator (Chernozhukov et al. [2017]) for a special model class. The performance of the method is illustrated on simulated data and a genomic dataset.},
 author = {Domagoj Ćevid and Peter Bühlmann and Nicolai Meinshausen},
 journal = {Journal of Machine Learning Research},
 number = {232},
 openalex = {W3117548431},
 pages = {1--41},
 title = {Spectral Deconfounding via Perturbed Sparse Linear Models},
 url = {http://jmlr.org/papers/v21/19-545.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-546,
 abstract = {The challenge in controlling stochastic systems in which low-probability events can set the system on catastrophic trajectories is to develop a robust ability to respond to such events without significantly compromising the optimality of the baseline control policy. This paper presents CelluDose, a stochastic simulation-trained deep reinforcement learning adaptive feedback control prototype for automated precision drug dosing targeting stochastic and heterogeneous cell proliferation. Drug resistance can emerge from random and variable mutations in targeted cell populations; in the absence of an appropriate dosing policy, emergent resistant subpopulations can proliferate and lead to treatment failure. Dynamic feedback dosage control holds promise in combatting this phenomenon, but the application of traditional control approaches to such systems is fraught with challenges due to the complexity of cell dynamics, uncertainty in model parameters, and the need in medical applications for a robust controller that can be trusted to properly handle unexpected outcomes. Here, training on a sample biological scenario identified single-drug and combination therapy policies that exhibit a 100% success rate at suppressing cell proliferation and responding to diverse system perturbations while establishing low-dose no-event baselines. These policies were found to be highly robust to variations in a key model parameter subject to significant uncertainty and unpredictable dynamical changes.},
 author = {Dalit Engelhardt},
 journal = {Journal of Machine Learning Research},
 number = {203},
 openalex = {W4288406763},
 pages = {1--30},
 title = {Dynamic Control of Stochastic Evolution: A Deep Reinforcement Learning Approach to Adaptively Targeting Emergent Drug Resistance},
 url = {http://jmlr.org/papers/v21/19-546.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-560,
 author = {Szymon Knop and Przemysław Spurek and Jacek Tabor and Igor Podolak and Marcin Mazur and Stanisław Jastrzębski},
 journal = {Journal of Machine Learning Research},
 number = {164},
 openalex = {W3085682849},
 pages = {1--28},
 title = {Cramer-Wold Auto-Encoder},
 url = {http://jmlr.org/papers/v21/19-560.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-562,
 abstract = {We present a framework for compactly summarizing many recent results in efficient and/or biologically plausible online training of recurrent neural networks (RNN). The framework organizes algorithms according to several criteria: (a) past vs. future facing, (b) tensor structure, (c) stochastic vs. deterministic, and (d) closed form vs. numerical. These axes reveal latent conceptual connections among several recent advances in online learning. Furthermore, we provide novel mathematical intuitions for their degree of success. Testing various algorithms on two synthetic tasks shows that performances cluster according to our criteria. Although a similar clustering is also observed for gradient alignment, alignment with exact methods does not alone explain ultimate performance, especially for stochastic algorithms. This suggests the need for better comparison metrics.},
 author = {Owen Marschall and Kyunghyun Cho and Cristina Savin},
 journal = {Journal of Machine Learning Research},
 number = {135},
 openalex = {W4288287658},
 pages = {1--34},
 title = {A Unified Framework of Online Learning Algorithms for Training Recurrent Neural Networks},
 url = {http://jmlr.org/papers/v21/19-562.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-563,
 author = {Tianxi Li and Cheng Qian and Elizaveta Levina and Ji Zhu},
 journal = {Journal of Machine Learning Research},
 number = {74},
 openalex = {W3027871065},
 pages = {1--45},
 title = {High-dimensional Gaussian graphical models on network-linked data},
 url = {http://jmlr.org/papers/v21/19-563.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-569,
 abstract = {We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As as example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.},
 author = {Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},
 journal = {Journal of Machine Learning Research},
 number = {43},
 openalex = {W3013371788},
 pages = {1--36},
 title = {Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},
 url = {http://jmlr.org/papers/v21/19-569.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-580,
 abstract = {We study the identity testing problem in the context of spin systems or undirected graphical models, where it takes the following form: given the parameter specification of the model $M$ and a sampling oracle for the distribution $\mu_{\hat{M}}$ of an unknown model $\hat{M}$, can we efficiently determine if the two models $M$ and $\hat{M}$ are the same? We consider identity testing for both soft-constraint and hard-constraint systems. In particular, we prove hardness results in two prototypical cases, the Ising model and proper colorings, and explore whether identity testing is any easier than structure learning. 
For the ferromagnetic (attractive) Ising model, Daskalakis et al. (2018) presented a polynomial time algorithm for identity testing. We prove hardness results in the antiferromagnetic (repulsive) setting in the same regime of parameters where structure learning is known to require a super-polynomial number of samples. In particular, for $n$-vertex graphs of maximum degree $d$, we prove that if $|\beta| d = \omega(\log{n})$ (where $\beta$ is the inverse temperature parameter), then there is no polynomial running time identity testing algorithm unless $RP=NP$. We also establish computational lower bounds for a broader set of parameters under the (randomized) exponential time hypothesis. Our proofs utilize insights into the design of gadgets using random graphs in recent works concerning the hardness of approximate counting by Sly (2010). In the hard-constraint setting, we present hardness results for identity testing for proper colorings. Our results are based on the presumed hardness of #BIS, the problem of (approximately) counting independent sets in bipartite graphs. In particular, we prove that identity testing is hard in the same range of parameters where structure learning is known to be hard.},
 author = {Ivona Bez{{\'a}}kov{{\'a}} and Antonio Blanca and Zongchen Chen and Daniel {\v{S}}tefankovi{\v{c}} and Eric Vigoda},
 journal = {Journal of Machine Learning Research},
 number = {25},
 openalex = {W3009206948},
 pages = {1--62},
 title = {Lower bounds for testing graphical models: colorings and antiferromagnetic Ising models},
 url = {http://jmlr.org/papers/v21/19-580.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-585,
 abstract = {We establish risk bounds for Regularized Empirical Risk Minimizers (RERM) when the loss is Lipschitz and convex and the regularization function is a norm. In a first part, we obtain these results in the i.i.d. setup under subgaussian assumptions on the design. In a second part, a more general framework where the design might have heavier tails and data may be corrupted by outliers both in the design and the response variables is considered. In this situation, RERM performs poorly in general. We analyse an alternative procedure based on median-of-means principles and called minmax MOM. We show optimal subgaussian deviation rates for these estimators in the relaxed setting. The main results are meta-theorems allowing a wide-range of applications to various problems in learning theory. To show a non-exhaustive sample of these potential applications, it is applied to classification problems with logistic loss functions regularized by LASSO and SLOPE, to regression problems with Huber loss regularized by Group LASSO and Total Variation. Another advantage of the minmax MOM formulation is that it suggests a systematic way to slightly modify descent based algorithms used in high-dimensional statistics to make them robust to outliers. We illustrate this principle in a Simulations section where a minmax MOM version of classical proximal descent algorithms are turned into robust to outliers algorithms.},
 author = {Chinot Geoffrey and Lecué Guillaume and Lerasle Matthieu},
 journal = {Journal of Machine Learning Research},
 number = {233},
 openalex = {W3117526597},
 pages = {1--47},
 title = {Robust high dimensional learning for Lipschitz and convex losses},
 url = {http://jmlr.org/papers/v21/19-585.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-587,
 abstract = {Generalized Linear Models (GLM) form a wide class of regression and classification models, where prediction is a function of a linear combination of the input variables. For statistical inference in high dimension, sparsity inducing regularizations have proven to be useful while offering statistical guarantees. However, solving the resulting optimization problems can be challenging: even for popular iterative algorithms such as coordinate descent, one needs to loop over a large number of variables. To mitigate this, techniques known as screening rules and working sets diminish the size of the optimization problem at hand, either by progressively removing variables, or by solving a growing sequence of smaller problems. For both techniques, significant variables are identified thanks to convex duality arguments. In this paper, we show that the dual iterates of a GLM exhibit a Vector AutoRegressive (VAR) behavior after sign identification, when the primal problem is solved with proximal gradient descent or cyclic coordinate descent. Exploiting this regularity, one can construct dual points that offer tighter certificates of optimality, enhancing the performance of screening rules and working set algorithms.},
 author = {Mathurin Massias and Samuel Vaiter and Alexandre Gramfort and Joseph Salmon},
 journal = {Journal of Machine Learning Research},
 number = {234},
 openalex = {W3113898612},
 pages = {1--33},
 title = {Dual Extrapolation for Sparse GLMs},
 url = {http://jmlr.org/papers/v21/19-587.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-589,
 abstract = {Parameterized state space models in the form of recurrent networks are often used in machine learning to learn from data streams exhibiting temporal dependencies. To break the black box nature of such models it is important to understand the dynamical features of the input driving time series that are formed in the state space. We propose a framework for rigorous analysis of such state representations in vanishing memory state space models such as echo state networks (ESN). In particular, we consider the state space a temporal feature space and the readout mapping from the state space a kernel machine operating in that feature space. We show that: (1) The usual ESN strategy of randomly generating input-to-state, as well as state coupling leads to shallow memory time series representations, corresponding to cross-correlation operator with fast exponentially decaying coefficients; (2) Imposing symmetry on dynamic coupling yields a constrained dynamic kernel matching the input time series with straightforward exponentially decaying motifs or exponentially decaying motifs of the highest frequency; (3) Simple cycle high-dimensional reservoir topology specified only through two free parameters can implement deep memory dynamic kernels with a rich variety of matching motifs. We quantify richness of feature representations imposed by dynamic kernels and demonstrate that for dynamic kernel associated with cycle reservoir topology, the kernel richness undergoes a phase transition close to the edge of stability.},
 author = {Peter Tino},
 journal = {Journal of Machine Learning Research},
 number = {44},
 openalex = {W3011544844},
 pages = {1--42},
 title = {Dynamical Systems as Temporal Feature Spaces},
 url = {http://jmlr.org/papers/v21/19-589.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-592,
 abstract = {This paper focuses on generalization performance analysis for distributed algorithms in the framework of learning theory. Taking distributed kernel ridge regression (DKRR) for example, we succeed in deriving its optimal learning rates in expectation and providing theoretically optimal ranges of the number of local processors. Due to the gap between theory and experiments, we also deduce optimal learning rates for DKRR in probability to essentially reflect the generalization performance and limitations of DKRR. Furthermore, we propose a communication strategy to improve the learning performance of DKRR and demonstrate the power of communications in DKRR via both theoretical assessments and numerical experiments.},
 author = {Shao-Bo Lin and Di Wang and Ding-Xuan Zhou},
 journal = {Journal of Machine Learning Research},
 number = {93},
 openalex = {W3038640622},
 pages = {1--38},
 title = {Distributed Kernel Ridge Regression with Communications},
 url = {http://jmlr.org/papers/v21/19-592.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-594,
 abstract = {The accuracy and complexity of kernel learning algorithms is determined by the set of kernels over which it is able to optimize. An ideal set of kernels should: admit a linear parameterization (tractability); be dense in the set of all kernels (accuracy); and every member should be universal so that the hypothesis space is infinite-dimensional (scalability). Currently, there is no class of kernel that meets all three criteria - e.g. Gaussians are not tractable or accurate; polynomials are not scalable. We propose a new class that meet all three criteria - the Tessellated Kernel (TK) class. Specifically, the TK class: admits a linear parameterization using positive matrices; is dense in all kernels; and every element in the class is universal. This implies that the use of TK kernels for learning the kernel can obviate the need for selecting candidate kernels in algorithms such as SimpleMKL and parameters such as the bandwidth. Numerical testing on soft margin Support Vector Machine (SVM) problems show that algorithms using TK kernels outperform other kernel learning algorithms and neural networks. Furthermore, our results show that when the ratio of the number of training data to features is high, the improvement of TK over MKL increases significantly.},
 author = {Brendon K. Colbert and Matthew M. Peet},
 journal = {Journal of Machine Learning Research},
 number = {45},
 openalex = {W3013114425},
 pages = {1--29},
 title = {A convex parametrization of a new class of universal kernel functions},
 url = {http://jmlr.org/papers/v21/19-594.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-636,
 abstract = {We prove the local convergence to minima and estimates on the rate of convergence for the stochastic gradient descent method in the case of not necessarily globally convex nor contracting objective functions. In particular, the results are applicable to simple objective functions arising in machine learning.},
 author = {Benjamin Fehrman and Benjamin Gess and Arnulf Jentzen},
 journal = {Journal of Machine Learning Research},
 number = {136},
 openalex = {W4288376397},
 pages = {1--48},
 title = {Convergence rates for the stochastic gradient descent method for non-convex objective functions},
 url = {http://jmlr.org/papers/v21/19-636.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-645,
 abstract = {A tacit assumption in linear regression is that (response, predictor)-pairs correspond to identical observational units. A series of recent works have studied scenarios in which this assumption is violated under terms such as ``Unlabeled Sensing and ``Regression with Unknown Permutation''. In this paper, we study the setup of multiple response variables and a notion of mismatches that generalizes permutations in order to allow for missing matches as well as for one-to-many matches. A two-stage method is proposed under the assumption that most pairs are correctly matched. In the first stage, the regression parameter is estimated by handling mismatches as contaminations, and subsequently the generalized permutation is estimated by a basic variant of matching. The approach is both computationally convenient and equipped with favorable statistical guarantees. Specifically, it is shown that the conditions for permutation recovery become considerably less stringent as the number of responses $m$ per observation increase. Particularly, for $m = Ω(\log n)$, the required signal-to-noise ratio no longer depends on the sample size $n$. Numerical results on synthetic and real data are presented to support the main findings of our analysis.},
 author = {Martin Slawski and Emanuel Ben-David and Ping Li},
 journal = {Journal of Machine Learning Research},
 number = {204},
 openalex = {W2958600879},
 pages = {1--42},
 title = {A Two-Stage Approach to Multivariate Linear Regression with Sparsely Mismatched Data},
 url = {http://jmlr.org/papers/v21/19-645.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-650,
 abstract = {We study contextual bandit learning with an abstract policy class and continuous action space. We obtain two qualitatively different regret bounds: one competes with a smoothed version of the policy class under no continuity assumptions, while the other requires standard Lipschitz assumptions. Both bounds exhibit data-dependent “zooming” behavior and, with no tuning, yield improved guarantees for benign problems. We also study adapting to unknown smoothness parameters, establishing a price-of-adaptivity and deriving optimal adaptive algorithms that require no additional information.},
 author = {Akshay Krishnamurthy and John Langford and Aleksandrs Slivkins and Chicheng Zhang},
 journal = {Journal of Machine Learning Research},
 number = {137},
 openalex = {W3082652141},
 pages = {1--45},
 title = {Contextual bandits with continuous actions: Smoothing, zooming, and adapting},
 url = {http://jmlr.org/papers/v21/19-650.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-664,
 author = {Gunwoong Park},
 journal = {Journal of Machine Learning Research},
 number = {75},
 openalex = {W3026916487},
 pages = {1--34},
 title = {Identifiability of Additive Noise Models Using Conditional Variances},
 url = {http://jmlr.org/papers/v21/19-664.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-671,
 abstract = {Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with molecular graphs due to their unique characteristics—their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we propose NeVAE, a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. In addition, by using masking, the decoder is able to guarantee a set of valid properties in the generated molecules. Experiments reveal that our model can discover plausible, diverse and novel molecules more effectively than several state of the art methods. Moreover, by utilizing Bayesian optimization over the continuous latent representation of molecules our model finds, we can also find molecules that maximize certain desirable properties more effectively than alternatives.},
 author = {Bidisha Samanta and Abir De and Gourhari Jana and Vicenç Gómez and Pratim Chattaraj and Niloy Ganguly and Manuel Gomez-Rodriguez},
 journal = {Journal of Machine Learning Research},
 number = {114},
 openalex = {W2965344674},
 pages = {1--33},
 title = {NeVAE: A Deep Generative Model for Molecular Graphs},
 url = {http://jmlr.org/papers/v21/19-671.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-678,
 abstract = {metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT licence.},
 author = {William de Vazelhes and CJ Carey and Yuan Tang and Nathalie Vauquier and Aurélien Bellet},
 journal = {Journal of Machine Learning Research},
 number = {138},
 openalex = {W3047712904},
 pages = {1--6},
 title = {metric-learn: Metric Learning Algorithms in Python},
 url = {http://jmlr.org/papers/v21/19-678.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-718,
 abstract = {When the data is distributed across multiple servers, lowering the communication cost between the servers (or workers) while solving the distributed learning problem is an important problem and is the focus of this paper. In particular, we propose a fast, and communication-efficient decentralized framework to solve the distributed machine learning (DML) problem. The proposed algorithm, Group Alternating Direction Method of Multipliers (GADMM) is based on the Alternating Direction Method of Multipliers (ADMM) framework. The key novelty in GADMM is that it solves the problem in a decentralized topology where at most half of the workers are competing for the limited communication resources at any given time. Moreover, each worker exchanges the locally trained model only with two neighboring workers, thereby training a global model with a lower amount of communication overhead in each exchange. We prove that GADMM converges to the optimal solution for convex loss functions, and numerically show that it converges faster and more communication-efficient than the state-of-the-art communication-efficient algorithms such as the Lazily Aggregated Gradient (LAG) and dual averaging, in linear and logistic regression tasks on synthetic and real datasets. Furthermore, we propose Dynamic GADMM (D-GADMM), a variant of GADMM, and prove its convergence under the time-varying network topology of the workers.},
 author = {Anis Elgabli and Jihong Park and Amrit S. Bedi and Mehdi Bennis and Vaneet Aggarwal},
 journal = {Journal of Machine Learning Research},
 number = {76},
 openalex = {W2971667645},
 pages = {1--39},
 title = {GADMM: Fast and Communication Efficient Framework for Distributed Machine Learning},
 url = {http://jmlr.org/papers/v21/19-718.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-723,
 abstract = {We propose a formulation for nonlinear recurrent models that includes simple parametric models of recurrent neural networks as a special case. The proposed formulation leads to a natural estimator in the form of a convex program. We provide a sample complexity for this estimator in the case of stable dynamics, where the nonlinear recursion has a certain contraction property, and under certain regularity conditions on the input distribution. We evaluate the performance of the estimator by simulation on synthetic data. These numerical experiments also suggest the extent at which the imposed theoretical assumptions may be relaxed.},
 author = {Sohail Bahmani and Justin Romberg},
 journal = {Journal of Machine Learning Research},
 number = {235},
 openalex = {W2970505641},
 pages = {1--20},
 title = {Convex Programming for Estimation in Nonlinear Recurrent Models},
 url = {http://jmlr.org/papers/v21/19-723.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-734,
 abstract = {Learning rates for least-squares regression are typically expressed in terms of $L_2$-norms. In this paper we extend these rates to norms stronger than the $L_2$-norm without requiring the regression function to be contained in the hypothesis space. In the special case of Sobolev reproducing kernel Hilbert spaces used as hypotheses spaces, these stronger norms coincide with fractional Sobolev norms between the used Sobolev space and $L_2$. As a consequence, not only the target function but also some of its derivatives can be estimated without changing the algorithm. From a technical point of view, we combine the well-known integral operator techniques with an embedding property, which so far has only been used in combination with empirical process arguments. This combination results in new finite sample bounds with respect to the stronger norms. From these finite sample bounds our rates easily follow. Finally, we prove the asymptotic optimality of our results in many cases.},
 author = {Simon Fischer and Ingo Steinwart},
 journal = {Journal of Machine Learning Research},
 number = {205},
 openalex = {W2594326342},
 pages = {1--38},
 title = {Sobolev Norm Learning Rates for Regularized Least-Squares Algorithm},
 url = {http://jmlr.org/papers/v21/19-734.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-737,
 abstract = {We consider the problem of learning high-dimensional, nonparametric and structured (e.g. Gaussian) distributions in distributed networks, where each node in the network observes an independent sample from the underlying distribution and can use $k$ bits to communicate its sample to a central processor. We consider three different models for communication. Under the independent model, each node communicates its sample to a central processor by independently encoding it into $k$ bits. Under the more general sequential or blackboard communication models, nodes can share information interactively but each node is restricted to write at most $k$ bits on the final transcript. We characterize the impact of the communication constraint $k$ on the minimax risk of estimating the underlying distribution under $\ell^2$ loss. We develop minimax lower bounds that apply in a unified way to many common statistical models and reveal that the impact of the communication constraint can be qualitatively different depending on the tail behavior of the score function associated with each model. A key ingredient in our proofs is a geometric characterization of Fisher information from quantized samples.},
 author = {Leighton Pate Barnes and Yanjun Han and Ayfer Ozgur},
 journal = {Journal of Machine Learning Research},
 number = {236},
 openalex = {W4288598403},
 pages = {1--30},
 title = {Lower Bounds for Learning Distributions under Communication Constraints via Fisher Information},
 url = {http://jmlr.org/papers/v21/19-737.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-748,
 author = {Sebastian U. Stich and Sai Praneeth Karimireddy},
 journal = {Journal of Machine Learning Research},
 number = {237},
 openalex = {W3117733161},
 pages = {1--36},
 title = {The Error-Feedback framework: SGD with Delayed Gradients},
 url = {http://jmlr.org/papers/v21/19-748.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-755,
 author = {Yuexiang Zhai and Zitong Yang and Zhenyu Liao and John Wright and Yi Ma},
 journal = {Journal of Machine Learning Research},
 number = {165},
 openalex = {W3086393698},
 pages = {1--68},
 title = {Complete Dictionary Learning via L4-Norm Maximization over the Orthogonal Group},
 url = {http://jmlr.org/papers/v21/19-755.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-763,
 abstract = {pyts is an open-source Python package for time series classification. This versatile toolbox provides implementations of many algorithms published in the literature, preprocessing functionalities, and data set loading utilities. pyts relies on the standard scientific Python packages numpy, scipy, scikit-learn, joblib, and numba, and is distributed under the BSD-3-Clause license. Documentation contains installation instructions, a detailed user guide, a full API description, and concrete self-contained examples. Source code and documentation can be downloaded from https://github.com/johannfaouzi/pyts.},
 author = {Johann Faouzi and Hicham Janati},
 journal = {Journal of Machine Learning Research},
 number = {46},
 openalex = {W3014052783},
 pages = {1--6},
 title = {pyts: A Python Package for Time Series Classification},
 url = {http://jmlr.org/papers/v21/19-763.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-764,
 abstract = {The DANE algorithm is an approximate Newton method popularly used for communication-efficient distributed machine learning. Reasons for the interest in DANE include scalability and versatility. Convergence of DANE, however, can be tricky; its appealing convergence rate is only rigorous for quadratic objective, and for more general convex functions the known results are no stronger than those of the classic first-order methods. To remedy these drawbacks, we propose in this paper some new alternatives of DANE which are more suitable for analysis. We first introduce a simple variant of DANE equipped with backtracking line search, for which global asymptotic convergence and sharper local non-asymptotic convergence rate guarantees can be proved for both quadratic and non-quadratic strongly convex functions. Then we propose a heavy-ball method to accelerate the convergence of DANE, showing that nearly tight local rate of convergence can be established for strongly convex functions, and with proper modification of algorithm the same result applies globally to linear prediction models. Numerical evidence is provided to confirm the theoretical and practical advantages of our methods.},
 author = {Xiao-Tong Yuan and Ping Li},
 journal = {Journal of Machine Learning Research},
 number = {206},
 openalex = {W3096659018},
 pages = {1--51},
 title = {On Convergence of Distributed Approximate Newton Methods: Globalization, Sharper Bounds and Beyond},
 url = {http://jmlr.org/papers/v21/19-764.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-771,
 abstract = {We study the question of whether parallelization in the exploration of the feasible set can be used to speed up convex optimization, in the local oracle model of computation. We show that the answer is negative for both deterministic and randomized algorithms applied to essentially any of the interesting geometries and nonsmooth, weakly-smooth, or smooth objective functions. In particular, we show that it is not possible to obtain a polylogarithmic (in the sequential complexity of the problem) number of parallel rounds with a polynomial (in the dimension) number of queries per round. In the majority of these settings and when the dimension of the space is polynomial in the inverse target accuracy, our lower bounds match the oracle complexity of sequential convex optimization, up to at most a logarithmic factor in the dimension, which makes them (nearly) tight. Prior to our work, lower bounds for parallel convex optimization algorithms were only known in a small fraction of the settings considered in this paper, mainly applying to Euclidean ($\ell_2$) and $\ell_\infty$ spaces. Our work provides a more general approach for proving lower bounds in the setting of parallel convex optimization.},
 author = {Jelena Diakonikolas and Crist{{\'o}}bal Guzm{{\'a}}n},
 journal = {Journal of Machine Learning Research},
 number = {5},
 openalex = {W3006839625},
 pages = {1--31},
 title = {Lower Bounds for Parallel and Randomized Convex Optimization.},
 url = {http://jmlr.org/papers/v21/19-771.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-773,
 abstract = {In this report we describe a tool for comparing the performance of graphical causal structure learning algorithms implemented in the TETRAD freeware suite of causal analysis methods. Currently the tool is available as package in the TETRAD source code (written in Java). Simulations can be done varying the number of runs, sample sizes, and data modalities. Performance on this simulated data can then be compared for a number of algorithms, with parameters varied and with performance statistics as selected, producing a publishable report. The package presented here may also be used to compare structure learning methods across platforms and programming languages, i.e., to compare algorithms implemented in TETRAD with those implemented in MATLAB, Python, or R.},
 author = {Joseph D. Ramsey and Daniel Malinsky and Kevin V. Bui},
 journal = {Journal of Machine Learning Research},
 number = {238},
 openalex = {W3116013208},
 pages = {1--6},
 title = {algcomparison: Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD},
 url = {http://jmlr.org/papers/v21/19-773.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-777,
 abstract = {Vector autoregression (VAR) is a fundamental tool for modeling multivariate time series. However, as the number of component series is increased, the VAR model becomes overparameterized. Several authors have addressed this issue by incorporating regularized approaches, such as the lasso in VAR estimation. Traditional approaches address overparameterization by selecting a low lag order, based on the assumption of short range dependence, assuming that a universal lag order applies to all components. Such an approach constrains the relationship between the components and impedes forecast performance. The lasso-based approaches work much better in high-dimensional situations but do not incorporate the notion of lag order selection. 
We propose a new class of hierarchical lag structures (HLag) that embed the notion of lag selection into a convex regularizer. The key modeling tool is a group lasso with nested groups which guarantees that the sparsity pattern of lag coefficients honors the VAR's ordered structure. The HLag framework offers three structures, which allow for varying levels of flexibility. A simulation study demonstrates improved performance in forecasting and lag order selection over previous approaches, and a macroeconomic application further highlights forecasting improvements as well as HLag's convenient, interpretable output.},
 author = {William B. Nicholson and Ines Wilms and Jacob Bien and David S. Matteson},
 journal = {Journal of Machine Learning Research},
 number = {166},
 openalex = {W3085120741},
 pages = {1--52},
 title = {High Dimensional Forecasting via Interpretable Vector Autoregression},
 url = {http://jmlr.org/papers/v21/19-777.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-786,
 abstract = {The Minimal Learning Machine (MLM) is a nonlinear supervised approach based on learning a linear mapping between distance matrices computed in the input and output data spaces, where distances are calculated using a subset of points called reference points. Its simple formulation has attracted several recent works on extensions and applications. In this paper, we aim to address some open questions related to the MLM. First, we detail theoretical aspects that assure the interpolation and universal approximation capabilities of the MLM, which were previously only empirically verified. Second, we identify the task of selecting reference points as having major importance for the MLM's generalization capability. Several clustering-based methods for reference point selection in regression scenarios are then proposed and analyzed. Based on an extensive empirical evaluation, we conclude that the evaluated methods are both scalable and useful. Specifically, for a small number of reference points, the clustering-based methods outperformed the standard random selection of the original MLM formulation.},
 author = {Joonas Hämäläinen and Alisson S. C. Alencar and Tommi Kärkkäinen and César L. C. Mattos and Amauri H. Souza Júnior and João P. P. Gomes},
 journal = {Journal of Machine Learning Research},
 number = {239},
 openalex = {W3113987434},
 pages = {1--29},
 title = {Minimal Learning Machine: Theoretical Results and Clustering-Based Reference Point Selection},
 url = {http://jmlr.org/papers/v21/19-786.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-794,
 author = {Amir R. Asadi and Emmanuel Abbe},
 journal = {Journal of Machine Learning Research},
 number = {139},
 openalex = {W3081982129},
 pages = {1--32},
 title = {Chaining Meets Chain Rule: Multilevel Entropic Regularization and Training of Neural Networks},
 url = {http://jmlr.org/papers/v21/19-794.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-800,
 abstract = {Testing the hypothesis of parallelism is a fundamental statistical problem arising from many applied sciences. In this paper, we develop a nonparametric parallelism test for inferring whether the trends are parallel in treatment and control groups. In particular, the proposed nonparametric parallelism test is a Wald type test based on a smoothing spline ANOVA (SSANOVA) model which can characterize the complex patterns of the data. We derive that the asymptotic null distribution of the test statistic is a Chi-square distribution, unveiling a new version of Wilks phenomenon. Notably, we establish the minimax sharp lower bound of the distinguishable rate for the nonparametric parallelism test by using the information theory, and further prove that the proposed test is minimax optimal. Simulation studies are conducted to investigate the empirical performance of the proposed test. DNA methylation and neuroimaging studies are presented to illustrate potential applications of the test. The software is available at \url{this https URL} .},
 author = {Xin Xing and Meimei Liu and Ping Ma and Wenxuan Zhong},
 journal = {Journal of Machine Learning Research},
 number = {94},
 openalex = {W3039540703},
 pages = {1--47},
 title = {Minimax Nonparametric Parallelism Test},
 url = {http://jmlr.org/papers/v21/19-800.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-802,
 abstract = {This paper considers hidden Markov models where the observations are given as the sum of a latent state which lies in a general state space and some independent noise with unknown distribution. It is shown that these fully nonparametric translation models are identifiable with respect to both the distribution of the latent variables and the distribution of the noise, under mostly a light tail assumption on the latent variables. Two nonparametric estimation methods are proposed and we prove that the corresponding estimators are consistent for the weak convergence topology. These results are illustrated with numerical experiments.},
 author = {Elisabeth Gassiat and Sylvain Le Corff and Luc Lehéricy},
 journal = {Journal of Machine Learning Research},
 number = {115},
 openalex = {W2912223888},
 pages = {1--40},
 title = {Identifiability and consistent estimation of nonparametric translation hidden Markov models with general state space},
 url = {http://jmlr.org/papers/v21/19-802.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-805,
 author = {Aghiles Salah and Quoc-Tuan Truong and Hady W. Lauw},
 journal = {Journal of Machine Learning Research},
 number = {95},
 openalex = {W3040516071},
 pages = {1--5},
 title = {Cornac: A Comparative Framework for Multimodal Recommender Systems},
 url = {http://jmlr.org/papers/v21/19-805.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-820,
 author = {Alexander Alexandrov and Konstantinos Benidis and Michael Bohlke-Schneider and Valentin Flunkert and Jan Gasthaus and Tim Januschowski and Danielle C. Maddix and Syama Rangapuram and David Salinas and Jasper Schulz and Lorenzo Stella and Ali Caner Türkmen and Yuyang Wang},
 journal = {Journal of Machine Learning Research},
 number = {116},
 openalex = {W3042623101},
 pages = {1--6},
 title = {GluonTS: Probabilistic and Neural Time Series Modeling in Python},
 url = {http://jmlr.org/papers/v21/19-820.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-827,
 abstract = {Off-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of $q$-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent. We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness.},
 author = {Nathan Kallus and Masatoshi Uehara},
 journal = {Journal of Machine Learning Research},
 number = {167},
 openalex = {W3085521361},
 pages = {1--63},
 title = {Double Reinforcement Learning for Efficient Off-Policy Evaluation in Markov Decision Processes},
 url = {http://jmlr.org/papers/v21/19-827.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-829,
 author = {Baihua He and Yanyan Liu and Yuanshan Wu and Guosheng Yin and Xingqiu Zhao},
 journal = {Journal of Machine Learning Research},
 number = {207},
 openalex = {W3095823571},
 pages = {1--37},
 title = {Functional Martingale Residual Process for High-Dimensional Cox Regression with Model Averaging},
 url = {http://jmlr.org/papers/v21/19-829.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-833,
 abstract = {We study the problem of estimation and testing in logistic regression with class-conditional noise in the observed labels, which has an important implication in the Positive-Unlabeled (PU) learning setting. With the key observation that the label noise problem belongs to a special sub-class of generalized linear models (GLM), we discuss convex and non-convex approaches that address this problem. A non-convex approach based on the maximum likelihood estimation produces an estimator with several optimal properties, but a convex approach has an obvious advantage in optimization. We demonstrate that in the low-dimensional setting, both estimators are consistent and asymptotically normal, where the asymptotic variance of the non-convex estimator is smaller than the convex counterpart. We also quantify the efficiency gap which provides insight into when the two methods are comparable. In the high-dimensional setting, we show that both estimation procedures achieve $\ell_2$-consistency at the minimax optimal $\sqrt{s\log p/n}$ rates under mild conditions. Finally, we propose an inference procedure using a de-biasing approach. We validate our theoretical findings through simulations and a real-data example.},
 author = {Hyebin Song and Ran Dai and Garvesh Raskutti and Rina Foygel Barber},
 journal = {Journal of Machine Learning Research},
 number = {168},
 openalex = {W4288094968},
 pages = {1--58},
 title = {Convex and Non-convex Approaches for Statistical Inference with Class-Conditional Noisy Labels},
 url = {http://jmlr.org/papers/v21/19-833.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-844,
 author = {Dmitry Kobak and Jonathan Lomond and Benoit Sanchez},
 journal = {Journal of Machine Learning Research},
 number = {169},
 openalex = {W3084611071},
 pages = {1--16},
 title = {The Optimal Ridge Penalty for Real-world High-dimensional Data Can Be Zero or Negative due to the Implicit Ridge Regularization},
 url = {http://jmlr.org/papers/v21/19-844.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-864,
 author = {Juan Luis Suárez and Salvador García and Francisco Herrera},
 journal = {Journal of Machine Learning Research},
 number = {96},
 openalex = {W3041284003},
 pages = {1--7},
 title = {pyDML: A Python Library for Distance Metric Learning},
 url = {http://jmlr.org/papers/v21/19-864.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-872,
 abstract = {We consider a novel application of inverse reinforcement learning with behavioral economics constraints to model, learn and predict the commenting behavior of YouTube viewers. Each group of users is modeled as a rationally inattentive Bayesian agent which solves a contextual bandit problem. Our methodology integrates three key components. First, to identify distinct commenting patterns, we use deep embedded clustering to estimate framing information (essential extrinsic features) that clusters users into distinct groups.Second, we present an inverse reinforcement learning algorithm that uses Bayesian revealed preferences to test for rationality: does there exist a utility function that rationalizes the given data, and if yes, can it be used to predict commenting behavior? Finally, we impose behavioral economics constraints stemming from rational inattention to characterize the attention span of groups of users. The test imposes a R{e}nyi mutual information cost constraint which impacts how the agent can select attention strategies to maximize their expected utility. After a careful analysis of a massive YouTube dataset, our surprising result is that in most YouTube user groups, the commenting behavior is consistent with optimizing a Bayesian utility with rationally inattentive constraints. The paper also highlights how the rational inattention model can accurately predict commenting behavior. The massive YouTube dataset and analysis used in this paper are available on GitHub and completely reproducible.},
 author = {William Hoiles and Vikram Krishnamurthy and Kunal Pattanayak},
 journal = {Journal of Machine Learning Research},
 number = {170},
 openalex = {W3085907897},
 pages = {1--39},
 title = {Rationally Inattentive Inverse Reinforcement Learning Explains YouTube Commenting Behavior},
 url = {http://jmlr.org/papers/v21/19-872.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-874,
 author = {Jiahe Lin and George Michailidis},
 journal = {Journal of Machine Learning Research},
 number = {117},
 openalex = {W4299990148},
 pages = {1--51},
 title = {Regularized Estimation of High-dimensional Factor-Augmented Vector Autoregressive (FAVAR) Models},
 url = {http://jmlr.org/papers/v21/19-874.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-900,
 author = {Fanghui Liu and Xiaolin Huang and Chen Gong and Jie Yang and Li Li},
 journal = {Journal of Machine Learning Research},
 number = {208},
 pages = {1--39},
 title = {Learning Data-adaptive Non-parametric Kernels},
 url = {http://jmlr.org/papers/v21/19-900.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-902,
 abstract = {We analyze the practices of reservoir computing in the framework of statistical learning theory. In particular, we derive finite sample upper bounds for the generalization error committed by specific families of reservoir computing systems when processing discrete-time inputs under various hypotheses on their dependence structure. Non-asymptotic bounds are explicitly written down in terms of the multivariate Rademacher complexities of the reservoir systems and the weak dependence structure of the signals that are being handled. This allows, in particular, to determine the minimal number of observations needed in order to guarantee a prescribed estimation accuracy with high probability for a given reservoir family. At the same time, the asymptotic behavior of the devised bounds guarantees the consistency of the empirical risk minimization procedure for various hypothesis classes of reservoir functionals.},
 author = {Lukas Gonon and Lyudmila Grigoryeva and Juan-Pablo Ortega},
 journal = {Journal of Machine Learning Research},
 number = {240},
 openalex = {W2982551457},
 pages = {1--61},
 title = {Risk bounds for reservoir computing},
 url = {http://jmlr.org/papers/v21/19-902.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-905,
 abstract = {Random forests remain among the most popular off-the-shelf supervised machine learning tools with a well-established track record of predictive accuracy in both regression and classification settings. Despite their empirical success as well as a bevy of recent work investigating their statistical properties, a full and satisfying explanation for their success has yet to be put forth. Here we aim to take a step forward in this direction by demonstrating that the additional randomness injected into individual trees serves as a form of implicit regularization, making random forests an ideal model in low signal-to-noise ratio (SNR) settings. Specifically, from a model-complexity perspective, we show that the mtry parameter in random forests serves much the same purpose as the shrinkage penalty in explicitly regularized regression procedures like lasso and ridge regression. To highlight this point, we design a randomized linear-model-based forward selection procedure intended as an analogue to tree-based random forests and demonstrate its surprisingly strong empirical performance. Numerous demonstrations on both real and synthetic data are provided.},
 author = {Lucas Mentch and Siyu Zhou},
 journal = {Journal of Machine Learning Research},
 number = {171},
 openalex = {W3086140324},
 pages = {1--36},
 title = {Randomization as Regularization: A Degrees of Freedom Explanation for Random Forest Success},
 url = {http://jmlr.org/papers/v21/19-905.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-912,
 abstract = {We consider a setting where multiple players sequentially choose among a common set of actions (arms). Motivated by a cognitive radio networks application, we assume that players incur a loss upon colliding, and that communication between players is not possible. Existing approaches assume that the system is stationary. Yet this assumption is often violated in practice, e.g., due to signal strength fluctuations. In this work, we design the first Multi-player Bandit algorithm that provably works in arbitrarily changing environments, where the losses of the arms may even be chosen by an adversary. This resolves an open problem posed by Rosenski, Shamir, and Szlak (2016).},
 author = {Pragnya Alatur and Kfir Y. Levy and Andreas Krause},
 journal = {Journal of Machine Learning Research},
 number = {77},
 openalex = {W3024642787},
 pages = {1--23},
 title = {Multi-Player Bandits: The Adversarial Case},
 url = {http://jmlr.org/papers/v21/19-912.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-930,
 author = {Feng Zhou and Zhidong Li and Xuhui Fan and Yang Wang and Arcot Sowmya and Fang Chen},
 journal = {Journal of Machine Learning Research},
 number = {241},
 openalex = {W3116417184},
 pages = {1--31},
 title = {Efficient Inference for Nonparametric Hawkes Processes Using Auxiliary Latent Variables},
 url = {http://jmlr.org/papers/v21/19-930.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-959,
 author = {Zhao-Rong Lai and Liming Tan and Xiaotian Wu and Liangda Fang},
 journal = {Journal of Machine Learning Research},
 number = {97},
 openalex = {W3040650268},
 pages = {1--37},
 title = {Loss Control with Rank-one Covariance Estimate for Short-term Portfolio Optimization},
 url = {http://jmlr.org/papers/v21/19-959.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-966,
 abstract = {Fairness of classification and regression has received much attention recently and various, partially non-compatible, criteria have been proposed. The fairness criteria can be enforced for a given classifier or, alternatively, the data can be adapated to ensure that every classifier trained on the data will adhere to desired fairness criteria. We present a practical data adaption method based on quantile preservation in causal structural equation models. The data adaptation is based on a presumed counterfactual model for the data. While the counterfactual model itself cannot be verified experimentally, we show that certain population notions of fairness are still guaranteed even if the counterfactual model is misspecified. The precise nature of the fulfilled non-causal fairness notion (such as demographic parity, separation or sufficiency) depends on the structure of the underlying causal model and the choice of resolving variables. We describe an implementation of the proposed data adaptation procedure based on Random Forests and demonstrate its practical use on simulated and real-world data.},
 author = {Drago Plečko and Nicolai Meinshausen},
 journal = {Journal of Machine Learning Research},
 number = {242},
 openalex = {W3117861986},
 pages = {1--44},
 title = {Fair Data Adaptation with Quantile Preservation},
 url = {http://jmlr.org/papers/v21/19-966.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-976,
 author = {Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar Raetsch and Sylvain Gelly and Bernhard Sch{{\"o}}lkopf and Olivier Bachem},
 journal = {Journal of Machine Learning Research},
 number = {209},
 openalex = {W3095169522},
 pages = {1--62},
 title = {A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation},
 url = {http://jmlr.org/papers/v21/19-976.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-985,
 abstract = {We develop ancestral Gumbel-Top-k sampling: a generic and efficient method for sampling without replacement from discrete-valued Bayesian networks, which includes multivariate discrete distributions, Markov chains and sequence models. The method uses an extension of the Gumbel-Max trick to sample without replacement by finding the top k of perturbed log-probabilities among all possible configurations of a Bayesian network. Despite the exponentially large domain, the algorithm has a complexity linear in the number of variables and sample size k. Our algorithm allows to set the number of parallel processors m, to trade off the number of iterations versus the total cost (iterations times m) of running the algorithm. For m=1 the algorithm has minimum total cost, whereas for m=k the number of iterations is minimized, and the resulting algorithm is known as Stochastic Beam Search. We provide extensions of the algorithm and discuss a number of related algorithms. We analyze the properties of ancestral Gumbel-Top-k sampling and compare against alternatives on randomly generated Bayesian networks with different levels of connectivity. In the context of (deep) sequence models, we show its use as a method to generate diverse but high-quality translations and statistical estimates of translation quality and entropy.},
 author = {Wouter Kool and Herke van Hoof and Max Welling},
 journal = {Journal of Machine Learning Research},
 number = {47},
 openalex = {W3013701218},
 pages = {1--36},
 title = {Ancestral Gumbel-Top-k Sampling for Sampling Without Replacement},
 url = {http://jmlr.org/papers/v21/19-985.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-993,
 abstract = {Operator-theoretic analysis of nonlinear dynamical systems has attracted much attention in a variety of engineering and scientific fields, endowed with practical estimation methods using data such as dynamic mode decomposition. In this paper, we address a lifted representation of nonlinear dynamical systems with random noise based on transfer operators, and develop a novel Krylov subspace method for estimating the operators using finite data, with consideration of the unboundedness of operators. For this purpose, we first consider Perron-Frobenius operators with kernel-mean embeddings for such systems. We then extend the Arnoldi method, which is the most classical type of Kryov subspace method, so that it can be applied to the current case. Meanwhile, the Arnoldi method requires the assumption that the operator is bounded, which is not necessarily satisfied for transfer operators on nonlinear systems. We accordingly develop the shift-invert Arnoldi method for Perron-Frobenius operators to avoid this problem. Also, we describe an approach of evaluating predictive accuracy by estimated operators on the basis of the maximum mean discrepancy, which is applicable, for example, to anomaly detection in complex systems. The empirical performance of our methods is investigated using synthetic and real-world healthcare data.},
 author = {Yuka Hashimoto and Isao Ishikawa and Masahiro Ikeda and Yoichi Matsuo and Yoshinobu Kawahara},
 journal = {Journal of Machine Learning Research},
 number = {172},
 openalex = {W3085597701},
 pages = {1--29},
 title = {Krylov Subspace Method for Nonlinear Dynamical Systems with Random Noise},
 url = {http://jmlr.org/papers/v21/19-993.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:19-996,
 abstract = {This paper presents a unified framework for supervised learning and inference procedures using the divide-and-conquer approach for high-dimensional correlated outcomes. We propose a general class of estimators that can be implemented in a fully distributed and parallelized computational scheme. Modelling, computational and theoretical challenges related to high-dimensional correlated outcomes are overcome by dividing data at both outcome and subject levels, estimating the parameter of interest from blocks of data using a broad class of supervised learning procedures, and combining block estimators in a closed-form meta-estimator asymptotically equivalent to estimates obtained by Hansen (1982)'s generalized method of moments (GMM) that does not require the entire data to be reloaded on a common server. We provide rigorous theoretical justifications for the use of distributed estimators with correlated outcomes by studying the asymptotic behaviour of the combined estimator with fixed and diverging number of data divisions. Simulations illustrate the finite sample performance of the proposed method, and we provide an R package for ease of implementation.},
 author = {Emily C. Hector and Peter X.-K. Song},
 journal = {Journal of Machine Learning Research},
 number = {173},
 openalex = {W3042623501},
 pages = {1--35},
 title = {Doubly Distributed Supervised Learning and Inference with High-Dimensional Correlated Outcomes},
 url = {http://jmlr.org/papers/v21/19-996.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-002,
 abstract = {In this study, we prove that an intrinsic low dimensionality of covariates is the main factor that determines the performance of deep neural networks (DNNs). DNNs generally provide outstanding empirical performance. Hence, numerous studies have actively investigated the theoretical properties of DNNs to understand their underlying mechanisms. In particular, the behavior of DNNs in terms of high-dimensional data is one of the most critical questions. However, this issue has not been sufficiently investigated from the aspect of covariates, although high-dimensional data have practically low intrinsic dimensionality. In this study, we derive bounds for an approximation error and a generalization error regarding DNNs with intrinsically low dimensional covariates. We apply the notion of the Minkowski dimension and develop a novel proof technique. Consequently, we show that convergence rates of the errors by DNNs do not depend on the nominal high dimensionality of data, but on its lower intrinsic dimension. We further prove that the rate is optimal in the minimax sense. We identify an advantage of DNNs by showing that DNNs can handle a broader class of intrinsic low dimensional data than other adaptive estimators. Finally, we conduct a numerical simulation to validate the theoretical results.},
 author = {Ryumei Nakada and Masaaki Imaizumi},
 journal = {Journal of Machine Learning Research},
 number = {174},
 openalex = {W3085710185},
 pages = {1--38},
 title = {Adaptive Approximation and Generalization of Deep Neural Network with Intrinsic Dimensionality},
 url = {http://jmlr.org/papers/v21/20-002.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-017,
 abstract = {There has recently been much work on the of neural networks, where Bayesian neural networks (BNNs) are shown to converge to a Gaussian (GP) as all hidden layers are sent to infinite width. However, these results do not apply to architectures that require one or more of the hidden layers to remain narrow. In this paper, we consider the wide limit of BNNs where some hidden layers, called bottlenecks, are held at finite width. The result is a composition of GPs that we term a neural network Gaussian process (bottleneck NNGP). Although intuitive, the subtlety of the proof is in showing that the wide limit of a composition of networks is in fact the composition of the limiting GPs. We also analyze theoretically a single-bottleneck NNGP, finding that the bottleneck induces dependence between the outputs of a multi-output network that persists through extreme post-bottleneck depths, and prevents the kernel of the network from losing discriminative power at extreme post-bottleneck depths.},
 author = {Devanshu Agrawal and Theodore Papamarkou and Jacob Hinkle},
 journal = {Journal of Machine Learning Research},
 number = {175},
 openalex = {W3086778125},
 pages = {1--66},
 title = {Wide Neural Networks with Bottlenecks are Deep Gaussian Processes},
 url = {http://jmlr.org/papers/v21/20-017.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-042,
 abstract = {We propose a novel inherently interpretable machine learning method that bases decisions on few relevant examples that we call prototypes. Our method, ProtoAttend, can be integrated into a wide range of neural network architectures including pre-trained models. It utilizes an attention mechanism that relates the encoded representations to samples in order to determine prototypes. The resulting model outperforms state of the art in three high impact problems without sacrificing accuracy of the original model: (1) it enables high-quality interpretability that outputs samples most relevant to the decision-making (i.e. a sample-based interpretability method); (2) it achieves state of the art confidence estimation by quantifying the mismatch across prototype labels; and (3) it obtains state of the art in distribution mismatch detection. All this can be achieved with minimal additional test time and a practically viable training time computational cost.},
 author = {Sercan O. Arik and Tomas Pfister},
 journal = {Journal of Machine Learning Research},
 number = {210},
 openalex = {W4288577486},
 pages = {1--35},
 title = {ProtoAttend: Attention-Based Prototypical Learning},
 url = {http://jmlr.org/papers/v21/20-042.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-056,
 abstract = {Finding a well-performing architecture is often tedious for both DL practitioners and researchers, leading to tremendous interest in the automation of this task by means of neural architecture search (NAS). Although the community has made major strides in developing better NAS methods, the quality of scientific empirical evaluations in the young field of NAS is still lacking behind that of other areas of machine learning. To address this issue, we describe a set of possible issues and ways to avoid them, leading to the NAS best practices checklist available at http://automl.org/nas_checklist.pdf.},
 author = {Marius Lindauer and Frank Hutter},
 journal = {Journal of Machine Learning Research},
 number = {243},
 openalex = {W2972268045},
 pages = {1--18},
 title = {Best Practices for Scientific Research on Neural Architecture Search},
 url = {http://jmlr.org/papers/v21/20-056.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-066,
 abstract = {Precision medicine is an emerging medical approach that allows physicians to select the treatment options based on individual patient information. The goal of precision medicine is to identify the optimal treatment regime (OTR) that yields the most favorable clinical outcome. Prior to adopting any OTR in clinical practice, it is crucial to know the impact of implementing such a policy. Although considerable research has been devoted to estimating the OTR in the literature, less attention has been paid to statistical inference of the OTR. Challenges arise in the nonregular cases where the OTR is not uniquely defined. To deal with nonregularity, we develop a novel inference method for the mean outcome under an OTR (the optimal value function) based on subsample aggregating (subagging). The proposed method can be applied to multi-stage studies where treatments are sequentially assigned over time. Bootstrap aggregating (bagging) and subagging have been recognized as effective variance reduction techniques to improve unstable estimators or classifiers (Buhlmann and Yu, 2002). However, it remains unknown whether these approaches can yield valid inference results. We show the proposed confidence interval (CI) for the optimal value function achieves nominal coverage. In addition, due to the variance reduction effect of subagging, our method enjoys certain statistical optimality. Specifically, we show that the mean squared error of the proposed value estimator is strictly smaller than that based on the simple sample-splitting estimator in the nonregular cases. Moreover, under certain conditions, the length of our proposed CI is shown to be on average shorter than CIs constructed based on the existing state-of-the-art method (Luedtke and van der Laan, 2016) and the \oraclemethod which works as well as if an OTR were known. Extensive numerical studies are conducted to back up our theoretical findings.},
 author = {Chengchun Shi and Wenbin Lu and Rui Song},
 journal = {Journal of Machine Learning Research},
 number = {176},
 openalex = {W3047597742},
 pages = {1--67},
 title = {Breaking the curse of nonregularity with subagging: inference of the mean outcome under optimal treatment regimes},
 url = {http://jmlr.org/papers/v21/20-066.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-074,
 abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
 author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
 journal = {Journal of Machine Learning Research},
 number = {140},
 openalex = {W4288089799},
 pages = {1--67},
 title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
 url = {http://jmlr.org/papers/v21/20-074.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-079,
 abstract = {Topic models have become popular tools for dimension reduction and exploratory analysis of text data which consists in observed frequencies of a vocabulary of $p$ words in $n$ documents, stored in a $p\times n$ matrix. The main premise is that the mean of this data matrix can be factorized into a product of two non-negative matrices: a $p\times K$ word-topic matrix $A$ and a $K\times n$ topic-document matrix $W$. This paper studies the estimation of $A$ that is possibly element-wise sparse, and the number of topics $K$ is unknown. In this under-explored context, we derive a new minimax lower bound for the estimation of such $A$ and propose a new computationally efficient algorithm for its recovery. We derive a finite sample upper bound for our estimator, and show that it matches the minimax lower bound in many scenarios. Our estimate adapts to the unknown sparsity of $A$ and our analysis is valid for any finite $n$, $p$, $K$ and document lengths. Empirical results on both synthetic data and semi-synthetic data show that our proposed estimator is a strong competitor of the existing state-of-the-art algorithms for both non-sparse $A$ and sparse $A$, and has superior performance is many scenarios of interest.},
 author = {Xin Bing and Florentina Bunea and Marten Wegkamp},
 journal = {Journal of Machine Learning Research},
 number = {177},
 openalex = {W3085197375},
 pages = {1--45},
 title = {Optimal estimation of sparse topic models},
 url = {http://jmlr.org/papers/v21/20-079.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-081,
 abstract = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.},
 author = {Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson},
 journal = {Journal of Machine Learning Research},
 number = {178},
 openalex = {W4295598622},
 pages = {1--51},
 title = {QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
 url = {http://jmlr.org/papers/v21/20-081.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-091,
 abstract = {tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn.},
 author = {Romain Tavenard and Johann Faouzi and Gilles Vandewiele and Felix Divo and Guillaume Androz and Chester Holtz and Marie Payne and Roman Yurchak and Marc Rußwurm and Kushal Kolar and Eli Woods},
 journal = {Journal of Machine Learning Research},
 number = {118},
 openalex = {W3039352388},
 pages = {1--6},
 title = {Tslearn, A Machine Learning Toolkit for Time Series Data},
 url = {http://jmlr.org/papers/v21/20-091.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-097,
 abstract = {We propose and analyze a novel theoretical and algorithmic framework for structured prediction. While so far the term has referred to discrete output spaces, here we consider more general settings, such as manifolds or spaces of probability measures. We define structured prediction as a problem where the output space lacks a vectorial structure. We identify and study a large class of loss functions that implicitly defines a suitable geometry on the problem. The latter is the key to develop an algorithmic framework amenable to a sharp statistical analysis and yielding efficient computations. When dealing with output spaces with infinite cardinality, a suitable implicit formulation of the estimator is shown to be crucial.},
 author = {Carlo Ciliberto and Lorenzo Rosasco and Alessandro Rudi},
 journal = {Journal of Machine Learning Research},
 number = {98},
 openalex = {W3040340324},
 pages = {1--67},
 title = {A General Framework for Consistent Structured Prediction with Implicit Loss Embeddings},
 url = {http://jmlr.org/papers/v21/20-097.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-120,
 abstract = {We consider the problem of identifying significant predictors in large data bases, where the response variable depends on the linear combination of explanatory variables through an unknown link function, corrupted with the noise from the unknown distribution. We utilize the natural, robust and efficient approach, which relies on replacing values of the response variables by their ranks and then identifying significant predictors by using well known Lasso. We provide new consistency results for the proposed procedure (called ,,RankLasso") and extend the scope of its applications by proposing its thresholded and adaptive versions. Our theoretical results show that these modifications can identify the set of relevant predictors under much wider range of data generating scenarios than regular RankLasso. Theoretical results are supported by the simulation study and the real data analysis, which show that our methods can properly identify relevant predictors, even when the error terms come from the Cauchy distribution and the link function is nonlinear. They also demonstrate the superiority of the modified versions of RankLasso in the case when predictors are substantially correlated. The numerical study shows also that RankLasso performs substantially better in model selection than LADLasso, which is a well established methodology for robust model selection.},
 author = {Wojciech Rejchel and Małgorzata Bogdan},
 journal = {Journal of Machine Learning Research},
 number = {244},
 openalex = {W3081920659},
 pages = {1--47},
 title = {Rank-based Lasso -- efficient methods for high-dimensional robust model selection},
 url = {http://jmlr.org/papers/v21/20-120.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-124,
 author = {Alberto Maria Metelli and Matteo Papini and Nico Montali and Marcello Restelli},
 journal = {Journal of Machine Learning Research},
 number = {141},
 openalex = {W3082245592},
 pages = {1--75},
 title = {Importance Sampling Techniques for Policy Optimization},
 url = {http://jmlr.org/papers/v21/20-124.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-151,
 abstract = {Machine learning methods for computational imaging require uncertainty estimation to be reliable in real settings. While Bayesian models offer a computationally tractable way of recovering uncertainty, they need large data volumes to be trained, which in imaging applications implicates prohibitively expensive collections with specific imaging instruments. This paper introduces a novel framework to train variational inference for inverse problems exploiting in combination few experimentally collected data, domain expertise and existing image data sets. In such a way, Bayesian machine learning models can solve imaging inverse problems with minimal data collection efforts. Extensive simulated experiments show the advantages of the proposed framework. The approach is then applied to two real experimental optics settings: holographic image reconstruction and imaging through highly scattering media. In both settings, state of the art reconstructions are achieved with little collection of training data.},
 author = {Francesco Tonolini and Jack Radford and Alex Turpin and Daniele Faccio and Roderick Murray-Smith},
 journal = {Journal of Machine Learning Research},
 number = {179},
 openalex = {W4288373378},
 pages = {1--46},
 title = {Variational Inference for Computational Imaging Inverse Problems},
 url = {http://jmlr.org/papers/v21/20-151.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-163,
 abstract = {Data augmentation is a widely used trick when training deep neural networks: in addition to the original data, properly transformed data are also added to the training set. However, to the best of our knowledge, a clear mathematical framework to explain the performance benefits of data augmentation is not available. In this paper, we develop such a theoretical framework. We show data augmentation is equivalent to an averaging operation over the orbits of a certain group that keeps the data distribution approximately invariant. We prove that it leads to variance reduction. We study empirical risk minimization, and the examples of exponential families, linear regression, and certain two-layer neural networks. We also discuss how data augmentation could be used in problems with symmetry where other approaches are prevalent, such as in cryo-electron microscopy (cryo-EM).},
 author = {Shuxiao Chen and Edgar Dobriban and Jane H. Lee},
 journal = {Journal of Machine Learning Research},
 number = {245},
 openalex = {W3023215041},
 pages = {1--71},
 title = {A Group-Theoretic Framework for Data Augmentation},
 url = {http://jmlr.org/papers/v21/20-163.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-175,
 abstract = {We consider estimation of a total causal effect from observational data via covariate adjustment. Ideally, adjustment sets are selected based on a given causal graph, reflecting knowledge of the underlying causal structure. Valid adjustment sets are, however, not unique. Recent research has introduced a graphical criterion for an 'optimal' valid adjustment set (O-set). For a given graph, adjustment by the O-set yields the smallest asymptotic variance compared to other adjustment sets in certain parametric and non-parametric models. In this paper, we provide three new results on the O-set. First, we give a novel, more intuitive graphical characterisation: We show that the O-set is the parent set of the outcome node(s) in a suitable latent projection graph, which we call the forbidden projection. An important property is that the forbidden projection preserves all information relevant to total causal effect estimation via covariate adjustment, making it a useful methodological tool in its own right. Second, we extend the existing IDA algorithm to use the O-set, and argue that the algorithm remains semi-local. This is implemented in the R-package pcalg. Third, we present assumptions under which the O-set can be viewed as the target set of popular non-graphical variable selection algorithms such as stepwise backward selection.},
 author = {Janine Witte and Leonard Henckel and Marloes H. Maathuis and Vanessa Didelez},
 journal = {Journal of Machine Learning Research},
 number = {246},
 openalex = {W3006529488},
 pages = {1--45},
 title = {On efficient adjustment in causal graphs},
 url = {http://jmlr.org/papers/v21/20-175.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-209,
 abstract = {We show a hardness result for random smoothing to achieve certified adversarial robustness against attacks in the $\ell_p$ ball of radius $ε$ when $p&gt;2$. Although random smoothing has been well understood for the $\ell_2$ case using the Gaussian distribution, much remains unknown concerning the existence of a noise distribution that works for the case of $p&gt;2$. This has been posed as an open problem by Cohen et al. (2019) and includes many significant paradigms such as the $\ell_\infty$ threat model. In this work, we show that any noise distribution $\mathcal{D}$ over $\mathbb{R}^d$ that provides $\ell_p$ robustness for all base classifiers with $p&gt;2$ must satisfy $\mathbb{E}η_i^2=Ω(d^{1-2/p}ε^2(1-δ)/δ^2)$ for 99% of the features (pixels) of vector $η\sim\mathcal{D}$, where $ε$ is the robust radius and $δ$ is the score gap between the highest-scored class and the runner-up. Therefore, for high-dimensional images with pixel values bounded in $[0,255]$, the required noise will eventually dominate the useful information in the images, leading to trivial smoothed classifiers.},
 author = {Avrim Blum and Travis Dick and Naren Manoj and Hongyang Zhang},
 journal = {Journal of Machine Learning Research},
 number = {211},
 openalex = {W3005089002},
 pages = {1--21},
 title = {Random Smoothing Might be Unable to Certify $\ell_\infty$ Robustness for High-Dimensional Images},
 url = {http://jmlr.org/papers/v21/20-209.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-210,
 abstract = {There is growing interest in large-scale machine learning and optimization over decentralized networks, e.g. in the context of multi-agent learning and federated learning. Due to the imminent need to alleviate the communication burden, the investigation of communication-efficient distributed optimization algorithms - particularly for empirical risk minimization - has flourished in recent years. A large fraction of these algorithms have been developed for the master/slave setting, relying on a central parameter server that can communicate with all agents. This paper focuses on distributed optimization over networks, or decentralized optimization, where each agent is only allowed to aggregate information from its neighbors. By properly adjusting the global gradient estimate via local averaging in conjunction with proper correction, we develop a communication-efficient approximate Newton-type method Network-DANE, which generalizes DANE to the decentralized scenarios. Our key ideas can be applied in a systematic manner to obtain decentralized versions of other master/slave distributed algorithms. A notable development is Network-SVRG/SARAH, which employs variance reduction to further accelerate local computation. We establish linear convergence of Network-DANE and Network-SVRG for strongly convex losses, and Network-SARAH for quadratic losses, which shed light on the impacts of data homogeneity, network connectivity, and local averaging upon the rate of convergence. We further extend Network-DANE to composite optimization by allowing a nonsmooth penalty term. Numerical evidence is provided to demonstrate the appealing performance of our algorithms over competitive baselines, in terms of both communication and computation efficiency. Our work suggests that performing a certain amount of local communications and computations per iteration can substantially improve the overall efficiency.},
 author = {Boyue Li and Shicong Cen and Yuxin Chen and Yuejie Chi},
 journal = {Journal of Machine Learning Research},
 number = {180},
 openalex = {W3086080324},
 pages = {1--51},
 title = {Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction},
 url = {http://jmlr.org/papers/v21/20-210.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-212,
 abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
 author = {Sanmit Narvekar and Bei Peng and Matteo Leonetti and Jivko Sinapov and Matthew E. Taylor and Peter Stone},
 journal = {Journal of Machine Learning Research},
 number = {181},
 openalex = {W3012544020},
 pages = {1--50},
 title = {Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey},
 url = {http://jmlr.org/papers/v21/20-212.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-297,
 abstract = {This paper studies distributed estimation and support recovery for high-dimensional linear regression model with heavy-tailed noise. To deal with heavy-tailed noise whose variance can be infinite, we adopt the quantile regression loss function instead of the commonly used squared loss. However, the non-smooth quantile loss poses new challenges to high-dimensional distributed estimation in both computation and theoretical development. To address the challenge, we transform the response variable and establish a new connection between quantile regression and ordinary linear regression. Then, we provide a distributed estimator that is both computationally and communicationally efficient, where only the gradient information is communicated at each iteration. Theoretically, we show that, after a constant number of iterations, the proposed estimator achieves a near-oracle convergence rate without any restriction on the number of machines. Moreover, we establish the theoretical guarantee for the support recovery. The simulation analysis is provided to demonstrate the effectiveness of our method.},
 author = {Xi Chen and Weidong Liu and Xiaojun Mao and Zhuoyi Yang},
 journal = {Journal of Machine Learning Research},
 number = {182},
 openalex = {W3086394069},
 pages = {1--43},
 title = {Distributed High-dimensional Regression Under a Quantile Loss Function},
 url = {http://jmlr.org/papers/v21/20-297.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-301,
 abstract = {We study the theoretical properties of image denoising via total variation penalized least-squares. We define the total vatiation in terms of the two-dimensional total discrete derivative of the image and show that it gives rise to denoised images that are piecewise constant on rectangular sets. We prove that, if the true image is piecewise constant on just a few rectangular sets, the denoised image converges to the true image at a parametric rate, up to a log factor. More generally, we show that the denoised image enjoys oracle properties, that is, it is almost as good as if some aspects of the true image were known. In other words, image denoising with total variation regularization leads to an adaptive reconstruction of the true image.},
 author = {Francesco Ortelli and Sara van de Geer},
 journal = {Journal of Machine Learning Research},
 number = {247},
 openalex = {W3114243412},
 pages = {1--38},
 title = {Adaptive Rates for Total Variation Image Denoising},
 url = {http://jmlr.org/papers/v21/20-301.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-312,
 abstract = {Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.},
 author = {Peter Henderson and Jieru Hu and Joshua Romoff and Emma Brunskill and Dan Jurafsky and Joelle Pineau},
 journal = {Journal of Machine Learning Research},
 number = {248},
 openalex = {W4310492983},
 pages = {1--43},
 title = {Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning},
 url = {http://jmlr.org/papers/v21/20-312.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-334,
 abstract = {The individualized treatment recommendation (ITR) is an important analytic framework for precision medicine. The goal of ITR is to assign the best treatments to patients based on their individual characteristics. From the machine learning perspective, the solution to the ITR problem can be formulated as a weighted classification problem to maximize the mean benefit from the recommended treatments given patients' characteristics. Several ITR methods have been proposed in both the binary setting and the multicategory setting. In practice, one may prefer a more flexible recommendation that includes multiple treatment options. This motivates us to develop methods to obtain a set of near-optimal individualized treatment recommendations alternative to each other, called alternative individualized treatment recommendations (A-ITR). We propose two methods to estimate the optimal A-ITR within the outcome weighted learning (OWL) framework. Simulation studies and a real data analysis for Type 2 diabetic patients with injectable antidiabetic treatments are conducted to show the usefulness of the proposed A-ITR framework. We also show the consistency of these methods and obtain an upper bound for the risk between the theoretically optimal recommendation and the estimated one. An R package aitr has been developed, found at https://github.com/menghaomiao/aitr.},
 author = {Haomiao Meng and Ying-Qi Zhao and Haoda Fu and Xingye Qiao},
 journal = {Journal of Machine Learning Research},
 number = {183},
 openalex = {W3084549129},
 pages = {1--28},
 title = {Near-optimal Individualized Treatment Recommendations},
 url = {http://jmlr.org/papers/v21/20-334.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-345,
 abstract = {We study how the topology of a data set $M = M_a \cup M_b \subseteq \mathbb{R}^d$, representing two classes $a$ and $b$ in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., with perfect accuracy on training set and near-zero generalization error ($\approx 0.01\%$). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrary well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of $M$ we begin with, when passed through a well-trained neural network $f : \mathbb{R}^d \to \mathbb{R}^p$, there is a vast reduction in the Betti numbers of both components $M_a$ and $M_b$; in fact they nearly always reduce to their lowest possible values: $\beta_k\bigl(f(M_i)\bigr) = 0$ for $k \ge 1$ and $\beta_0\bigl(f(M_i)\bigr) = 1$, $i =a, b$. Furthermore, (2) the reduction in Betti numbers is significantly faster for ReLU activation than hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. Lastly, (3) shallow and deep networks transform data sets differently -- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.},
 author = {Gregory Naitzat and Andrey Zhitnikov and Lek-Heng Lim},
 journal = {Journal of Machine Learning Research},
 number = {184},
 openalex = {W4287815523},
 pages = {1--40},
 title = {Topology of deep neural networks},
 url = {http://jmlr.org/papers/v21/20-345.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-365,
 author = {Can Zhou and Xiaofei Wang and Jianhua Guo},
 journal = {Journal of Machine Learning Research},
 number = {249},
 openalex = {W3113842873},
 pages = {1--35},
 title = {Learning Mixed Latent Tree Models},
 url = {http://jmlr.org/papers/v21/20-365.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-383,
 author = {Wenqi Lu and Zhongyi Zhu and Heng Lian},
 journal = {Journal of Machine Learning Research},
 number = {250},
 openalex = {W3113608201},
 pages = {1--31},
 title = {High-dimensional quantile tensor regression},
 url = {http://jmlr.org/papers/v21/20-383.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-412,
 abstract = {Scikit-network is a Python package inspired by scikit-learn for the analysis of large graphs. Graphs are represented by their adjacency matrix in the sparse CSR format of SciPy. The package provides state-of-the-art algorithms for ranking, clustering, classifying, embedding and visualizing the nodes of a graph. High performance is achieved through a mix of fast matrix-vector products (using SciPy), compiled code (using Cython) and parallel processing. The package is distributed under the BSD license, with dependencies limited to NumPy and SciPy. It is compatible with Python 3.6 and newer. Source code, documentation and installation instructions are available online.},
 author = {Thomas Bonald and Nathan de Lara and Quentin Lutz and Bertrand Charpentier},
 journal = {Journal of Machine Learning Research},
 number = {185},
 openalex = {W3086706245},
 pages = {1--6},
 title = {Scikit-network: Graph Analysis in Python},
 url = {http://jmlr.org/papers/v21/20-412.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-444,
 abstract = {Online Matrix Factorization (OMF) is a fundamental tool for dictionary learning problems, giving an approximate representation of complex data sets in terms of a reduced number of extracted features. Convergence guarantees for most of the OMF algorithms in the literature assume independence between data matrices, and the case of dependent data streams remains largely unexplored. In this paper, we show that a non-convex generalization of the well-known OMF algorithm for i.i.d. stream of data in \citep{mairal2010online} converges almost surely to the set of critical points of the expected loss function, even when the data matrices are functions of some underlying Markov chain satisfying a mild mixing condition. This allows one to extract features more efficiently from dependent data streams, as there is no need to subsample the data sequence to approximately satisfy the independence assumption. As the main application, by combining online non-negative matrix factorization and a recent MCMC algorithm for sampling motifs from networks, we propose a novel framework of Network Dictionary Learning, which extracts ``network dictionary patches' from a given network in an online manner that encodes main features of the network. We demonstrate this technique and its application to network denoising problems on real-world network data.},
 author = {Hanbaek Lyu and Deanna Needell and Laura Balzano},
 journal = {Journal of Machine Learning Research},
 number = {251},
 openalex = {W4288028785},
 pages = {1--49},
 title = {Online matrix factorization for Markovian data and applications to Network Dictionary Learning},
 url = {http://jmlr.org/papers/v21/20-444.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-729,
 author = {Sebastian Pölsterl},
 journal = {Journal of Machine Learning Research},
 number = {212},
 openalex = {W3097349486},
 pages = {1--6},
 title = {scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn},
 url = {http://jmlr.org/papers/v21/20-729.html},
 volume = {21},
 year = {2020}
}

@article{JMLR:v21:20-933,
 abstract = {Numerous researchers recently applied empirical spectral analysis to the study of modern deep learning classifiers. We identify and discuss an important formal class/cross-class structure and show how it lies at the origin of the many visually striking features observed in deepnet spectra, some of which were reported in recent articles, others are unveiled here for the first time. These include spectral outliers, spikes, and small but distinct continuous distributions, bumps, often seen beyond the edge of a main bulk. The significance of the cross-class structure is illustrated in three ways: (i) we prove the ratio of outliers to bulk in the spectrum of the Fisher information matrix is predictive of misclassification, in the context of multinomial logistic regression; (ii) we demonstrate how, gradually with depth, a network is able to separate class-distinctive information from class variability, all while orthogonalizing the class-distinctive information; and (iii) we propose a correction to KFAC, a well-known second-order optimization algorithm for training deepnets.},
 author = {Vardan Papyan},
 journal = {Journal of Machine Learning Research},
 number = {252},
 openalex = {W3116687755},
 pages = {1--64},
 title = {Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra},
 url = {http://jmlr.org/papers/v21/20-933.html},
 volume = {21},
 year = {2020}
}
